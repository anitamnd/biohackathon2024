<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8507389</article-id>
    <article-id pub-id-type="publisher-id">4419</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-021-04419-7</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>FoldHSphere: deep hyperspherical embeddings for protein fold recognition</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Villegas-Morcillo</surname>
          <given-names>Amelia</given-names>
        </name>
        <address>
          <email>ameliavm@ugr.es</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sanchez</surname>
          <given-names>Victoria</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gomez</surname>
          <given-names>Angel M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.4489.1</institution-id><institution-id institution-id-type="ISNI">0000000121678994</institution-id><institution>Department of Signal Theory, Telematics and Communications, </institution><institution>University of Granada, </institution></institution-wrap>Periodista Daniel Saucedo Aranda, 18071 Granada, Spain </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <elocation-id>490</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Current state-of-the-art deep learning approaches for protein fold recognition learn protein embeddings that improve prediction performance at the fold level. However, there still exists aperformance gap at the fold level and the (relatively easier) family level, suggesting that it might be possible to learn an embedding space that better represents the protein folds.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we propose the FoldHSphere method to learn a better fold embedding space through a two-stage training procedure. We first obtain prototype vectors for each fold class that are maximally separated in hyperspherical space. We then train a neural network by minimizing the angular large margin cosine loss to learn protein embeddings clustered around the corresponding hyperspherical fold prototypes. Our network architectures, ResCNN-GRU and ResCNN-BGRU, process the input protein sequences by applying several residual-convolutional blocks followed by a gated recurrent unit-based recurrent layer. Evaluation results on the LINDAHL dataset indicate that the use of our hyperspherical embeddings effectively bridges the performance gap at the family and fold levels. Furthermore, our FoldHSpherePro ensemble method yields an accuracy of 81.3% at the fold level, outperforming all the state-of-the-art methods.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">Our methodology is efficient in learning discriminative and fold-representative embeddings for the protein domains. The proposed hyperspherical embeddings are effective at identifying the protein fold class by pairwise comparison, even when amino acid sequence similarities are low.</p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-021-04419-7.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Protein fold recognition</kwd>
      <kwd>Deep neural networks</kwd>
      <kwd>Residual convolutions</kwd>
      <kwd>Embedding learning</kwd>
      <kwd>Hyperspherical space</kwd>
      <kwd>Thomson problem</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Spanish Ministry of Science, Innovation and Universities</institution>
        </funding-source>
        <award-id>Project No. PID2019-104206GB-I00 / SRA (State Research Agency) / 10.13039/501100011033</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par19">Protein structure prediction given the amino acid sequence is a challenging problem in structural bioinformatics. One of the key steps in the template-based modelling (TBM) of protein structures is the recognition of the protein fold [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR5">5</xref>]. The goal is to predict the fold type of a protein domain by comparison with template structures from the Protein Data Bank (PDB) [<xref ref-type="bibr" rid="CR6">6</xref>]. Solved structure domains from the PDB are classified into several levels according to structural and sequence similarities in databases as SCOP [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>] and CATH [<xref ref-type="bibr" rid="CR9">9</xref>]. The objective here is to identify proteins sharing the same <italic>fold class</italic>—with similar arrangement of structural elements but differing in the amino acid sequence.</p>
    <p id="Par20">Early computational approaches to recognizing proteins with similar structure and sequence (homology modelling) were based on sequence-to-sequence (BLAST [<xref ref-type="bibr" rid="CR10">10</xref>]) or profile-to-profile (HHpred [<xref ref-type="bibr" rid="CR11">11</xref>]) alignments, as well as Markov random fields (MRFAlign [<xref ref-type="bibr" rid="CR12">12</xref>]). In addition, threading methods aim to recognize distant-homologous proteins with low similarity in sequence by using structural properties instead. These methods include RAPTOR [<xref ref-type="bibr" rid="CR13">13</xref>], BoostThreader [<xref ref-type="bibr" rid="CR14">14</xref>], SPARKS-X [<xref ref-type="bibr" rid="CR15">15</xref>], conditional random field-based CNFpred [<xref ref-type="bibr" rid="CR16">16</xref>] and [<xref ref-type="bibr" rid="CR17">17</xref>], and more recently the EigenTHREADER [<xref ref-type="bibr" rid="CR18">18</xref>] and CEthreader [<xref ref-type="bibr" rid="CR19">19</xref>] methods, which use predicted contact map information.</p>
    <p id="Par21">In general, the protein fold recognition methods as the ones described above are derived from the template-based structure prediction problem. Unlike these, in the taxonomy-based fold classification approaches [<xref ref-type="bibr" rid="CR20">20</xref>] the protein sequences are directly mapped into fold classes. To this end, machine learning approaches such as FP-Pred [<xref ref-type="bibr" rid="CR21">21</xref>], ACCFold [<xref ref-type="bibr" rid="CR22">22</xref>], TAXFOLD [<xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR25">25</xref>], HMMFold [<xref ref-type="bibr" rid="CR26">26</xref>], ProFold [<xref ref-type="bibr" rid="CR27">27</xref>], and DKELM-LDA [<xref ref-type="bibr" rid="CR28">28</xref>], as well as the deep learning methods Conv-SXGbg-DeepFold [<xref ref-type="bibr" rid="CR29">29</xref>] and DeepFrag-k [<xref ref-type="bibr" rid="CR30">30</xref>], have been proposed to successfully classify into a pre-defined group of SCOP fold classes. However, the evaluated folds comprise a small set including only those folds with a higher amount of protein domains (27 or 30 folds), in contrast to the more than 1000 existing fold classes in the SCOP database.</p>
    <p id="Par22">Several machine learning algorithms have been also introduced for the protein fold recognition task [<xref ref-type="bibr" rid="CR31">31</xref>]. First attempts treated the task as a binary classification problem to decide whether two protein domains belonged to the same fold. Different techniques were applied here, such as support vector machines (FOLDpro [<xref ref-type="bibr" rid="CR32">32</xref>]), random forests (RF-Fold [<xref ref-type="bibr" rid="CR33">33</xref>]) and neural networks (DN-Fold [<xref ref-type="bibr" rid="CR34">34</xref>]). Moreover, ensemble methods enhance the recognition performance by combining multiple protein feature representations and prediction techniques. Examples are TA-Fold [<xref ref-type="bibr" rid="CR35">35</xref>] and the multi-view learning ensemble frameworks MT-fold [<xref ref-type="bibr" rid="CR36">36</xref>], EMfold [<xref ref-type="bibr" rid="CR37">37</xref>] and MLDH-Fold [<xref ref-type="bibr" rid="CR38">38</xref>]. On the other hand, the learning to rank methods, such as Fold-LTR-TCP [<xref ref-type="bibr" rid="CR39">39</xref>], FoldRec-C2C [<xref ref-type="bibr" rid="CR40">40</xref>], and ProtFold-DFG [<xref ref-type="bibr" rid="CR41">41</xref>], treat the problem as an information retrieval task and try to learn the relationship among proteins in the datasets.</p>
    <p id="Par23">Furthermore, deep learning-based methods have been recently proposed to identify the protein fold, such as DeepSF [<xref ref-type="bibr" rid="CR42">42</xref>], DeepFR [<xref ref-type="bibr" rid="CR43">43</xref>], DeepSVM-Fold [<xref ref-type="bibr" rid="CR44">44</xref>], MotifCNN-fold [<xref ref-type="bibr" rid="CR45">45</xref>], SelfAT-Fold [<xref ref-type="bibr" rid="CR46">46</xref>], VGGfold [<xref ref-type="bibr" rid="CR47">47</xref>], and CNN-BGRU [<xref ref-type="bibr" rid="CR48">48</xref>]. In these methods, a supervised neural network model is trained to classify the input protein domain into one of the possible fold classes. From the trained model, a fold-related embedding representation is extracted, which is then used to measure the similarity between each two protein domains. In this context, the learned embeddings constitute a <italic>d</italic>-dimensional space in which we can map high-dimensional protein representations such as evolutionary profiles [<xref ref-type="bibr" rid="CR48">48</xref>] (<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times 20$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq1.gif"/></alternatives></inline-formula>, where <italic>L</italic> is the protein sequence length) or contact maps [<xref ref-type="bibr" rid="CR43">43</xref>] (<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times L$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mi>L</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq2.gif"/></alternatives></inline-formula>). Moreover, these embeddings capture the fold information during training by placing inputs from the same fold close together in the embedding space. The model architecture for protein fold recognition usually contains a convolutional neural network (CNN) alone or in combination with recurrent layers—long-short term memory (LSTM) [<xref ref-type="bibr" rid="CR49">49</xref>] or gated recurrent unit (GRU) [<xref ref-type="bibr" rid="CR50">50</xref>] cells—or self-attention layers [<xref ref-type="bibr" rid="CR51">51</xref>]. Hence, in preceding works, most of the effort has been put into improving the neural network architectures and making them suitable to process different protein representations, such as predicted contact maps, evolutionary profiles or predicted secondary structure elements. In this work, we propose two architectures formed by several blocks of residual-convolutions [<xref ref-type="bibr" rid="CR52">52</xref>] and a recurrent layer, which we name ResCNN-GRU and ResCNN-BGRU. Here, the suffix ‘BGRU’ refers to bidirectional GRU, while ‘GRU’ indicates the use of a unidirectional GRU. These two architectures are derived from our previous CNN-GRU and CNN-BGRU models [<xref ref-type="bibr" rid="CR48">48</xref>], and can also process arbitrary length protein sequences represented by residue-level features.</p>
    <p id="Par24">However, unlike previous deep learning approaches, our main interest here is to improve the fold-related embedding vectors by modifying the neural network optimization criterion. While the softmax cross-entropy loss is commonly used for multi-class classification problems, it lacks sufficient discriminative power for classification [<xref ref-type="bibr" rid="CR53">53</xref>–<xref ref-type="bibr" rid="CR55">55</xref>]. In this regard, modifications on the loss function have been introduced, leading to improved functions such as the center loss [<xref ref-type="bibr" rid="CR53">53</xref>], the large margin softmax (L-Softmax) loss [<xref ref-type="bibr" rid="CR54">54</xref>], and the angular softmax (A-Softmax) loss [<xref ref-type="bibr" rid="CR55">55</xref>]. Thus, in this work, we propose to minimize an angular-based loss function, namely the <italic>large margin cosine loss</italic> (LMCL) [<xref ref-type="bibr" rid="CR56">56</xref>]. LMCL removes any vector norm dependencies by normalizing the input embedding and class weight vectors in the classification layer and therefore distributes them angularly on a high-dimensional sphere—or <italic>hypersphere</italic>. The function also introduces a class boundary margin to enlarge the inter-class angular separation while reducing the intra-class separation for embeddings within the same fold class.</p>
    <p id="Par25">We further improve the training of our neural network model by minimizing the LMCL with a fixed weight matrix in the last classification layer. Such a matrix contains a pre-defined set fold class vectors—<italic>hyperspherical prototypes</italic>—that are maximally separated on the surface of a hypersphere. To ensure maximum angular separation between prototypes, we draw inspiration from the well-known Thomson problem [<xref ref-type="bibr" rid="CR57">57</xref>]. Its goal is to determine the minimum energy configuration of <italic>K</italic> charged particles on the surface of a unit sphere. By minimizing a Thomson-based loss function, extended to a hypersphere of arbitrary number of dimensions, we optimize the angular distribution of our prototype vectors. Here we pre-train the prototype matrix separately and keep it fixed during the optimization of our neural network model. It must be noted that, unlike conventional transfer learning procedures in which the last layers of the network are fine-tuned, we pre-define the output embedding space given by a set of fold prototypes representing the cluster centroids for each fold class [<xref ref-type="bibr" rid="CR58">58</xref>]. In this way, during training, the model is forced to learn protein embeddings clustered around the corresponding hyperspherical fold prototypes.</p>
    <p id="Par26">In summary, our main contribution is a training procedure that provides hyperspherical protein embeddings, learned by minimizing the angular LMCL around pre-defined prototypes for the fold classes in a hyperspherical space. We obtain these embeddings by training the ResCNN-GRU and ResCNN-BGRU architectures that are effective at processing arbitrary length protein sequences. An overview of our approach is depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Our proposed methods, named FoldHSphere and FoldHSpherePro, significantly advance the state-of-the-art performance on well-known benchmark datasets.<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of the FoldHSphere approach for protein fold recognition. In the first stage <bold>a</bold> we train a neural network model to map the protein domains into <italic>K</italic> fold classes using the softmax cross-entropy as loss function. From this trained model, we extract fold class weight vectors <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {w}_k, k=1,\dots ,K$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq3.gif"/></alternatives></inline-formula> learned in the last classification layer. <bold>b</bold> We then optimize the position of the <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {w}_k$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq4.gif"/></alternatives></inline-formula> vectors by our proposed Thomson-based loss, so that they are maximally separated in the angular space. <bold>c</bold> The resulting hyperspherical prototypes are used as a fixed non-trainable classification matrix <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}$$\end{document}</tex-math><mml:math id="M10"><mml:mi mathvariant="bold">W</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq5.gif"/></alternatives></inline-formula> in the last layer of the neural network model, which is trained again, but now minimizing the LMCL. The final hyperspherical embeddings are extracted from the fully-connected part of this model. <bold>d</bold> Finally, the cosine similarity is computed between each two embeddings and a template ranking is performed for each query protein domain (FoldHSphere method). Moreover, template ranking is further improved by using enhanced scores provided by a random forest model trained with additional similarity measures as inputs (FoldHSpherePro method)</p></caption><graphic xlink:href="12859_2021_4419_Fig1_HTML" id="MO1"/></fig></p>
  </sec>
  <sec id="Sec2">
    <title>Materials and methods</title>
    <sec id="Sec3">
      <title>Datasets</title>
      <p id="Par27">The datasets were obtained from the public protein databases SCOP [<xref ref-type="bibr" rid="CR7">7</xref>] and the extended SCOPe [<xref ref-type="bibr" rid="CR8">8</xref>]. These databases contain a hierarchical structural classification of protein domains with solved structure. From the top-down view, such hierarchical levels are <italic>structural class, fold, superfamily</italic> and <italic>family</italic>, which group protein domains with increasing sequence similarity at each level.</p>
      <sec id="Sec4">
        <title>Training dataset</title>
        <p id="Par28">We trained our neural network models using the SCOPe 2.06 training set from [<xref ref-type="bibr" rid="CR43">43</xref>]. Such a training set was obtained after filtering out protein domains having a significant sequence similarity to those in the test set. To do so, the following similarity reduction methods were executed: MMseqs2 [<xref ref-type="bibr" rid="CR59">59</xref>] (sequence identity 25%, e-value <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-4}$$\end{document}</tex-math><mml:math id="M12"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq6.gif"/></alternatives></inline-formula>), CD-HIT-2D [<xref ref-type="bibr" rid="CR60">60</xref>] (sequence identity 40%) and BLAST+ [<xref ref-type="bibr" rid="CR61">61</xref>] (e-value <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-4}$$\end{document}</tex-math><mml:math id="M14"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq7.gif"/></alternatives></inline-formula>). The final dataset contains 16133 protein domains sharing at most 95% pairwise sequence identity, which are classified into <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K=1154$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1154</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq8.gif"/></alternatives></inline-formula> folds. For hyperparameter tuning, we performed a 5-stage cross-validation over the entire training set. Hence, we split the 16,133 protein domains into 5 groups, including domains from different families in each one (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: S1). This prevents having proteins in the validation subsets with similar amino acid sequence to those in the corresponding training subset.</p>
      </sec>
      <sec id="Sec5">
        <title>Benchmark datasets</title>
        <p id="Par29">We tested the effectiveness of our hyperspherical embeddings using both the well-known LINDAHL dataset [<xref ref-type="bibr" rid="CR3">3</xref>] and the updated LINDAHL_1.75 dataset we recently proposed in [<xref ref-type="bibr" rid="CR48">48</xref>]. The original LINDAHL dataset includes 976 domains from SCOP 1.37 covering 330 folds. Updated to SCOP 1.75, the LINDAHL_1.75 dataset contains the same number of proteins (976) but now classified into 323 folds. Protein domains within both test sets share a maximum sequence identity of 40%, as well as with respect to the training domains. Each dataset is paired and evaluated independently at three different levels—<italic>family, superfamily</italic> and <italic>fold</italic>. Thus, while the number of individual protein domains evaluated within the LINDAHL dataset are 555, 434 and 321 for the family, superfamily and fold levels, in LINDAHL_1.75 we evaluate 547, 431 and 356 domains, respectively.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Protein residue-level feature representation</title>
      <p id="Par30">In order to represent the protein amino acid sequence with variable length <italic>L</italic>, we considered 45 features for each amino acid as in previous works [<xref ref-type="bibr" rid="CR42">42</xref>, <xref ref-type="bibr" rid="CR48">48</xref>]. These 45 residue-level features contain the following information:<list list-type="bullet"><list-item><p id="Par31"><italic>Amino acid encoding:</italic> one-hot vector of size 20 representing the amino acid type.</p></list-item><list-item><p id="Par32"><italic>Position-specific scoring matrix (PSSM):</italic> 20 elements which contain the evolutionary profile information obtained from the multiple sequence alignment (MSA). We computed the PSSM matrix using PSI-BLAST [<xref ref-type="bibr" rid="CR10">10</xref>] and the non-redundant database ‘nr90’ for sequence homology searching.</p></list-item><list-item><p id="Par33"><italic>Secondary structure:</italic> one-hot vector of size 3 encoding the helix, strand and loop secondary structure elements. To predict the secondary structure we used the SSpro method from the SCRATCH suite [<xref ref-type="bibr" rid="CR62">62</xref>].</p></list-item><list-item><p id="Par34"><italic>Solvent accessibility:</italic> one-hot vector of size 2 encoding the exposed and buried states. Similar to before, we used the ACCpro method from SCRATCH to predict the solvent accessibility states.</p></list-item></list>These <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times 45$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq9.gif"/></alternatives></inline-formula> features are used as input to our neural network models, which are trained to predict the fold class for each protein domain.</p>
    </sec>
    <sec id="Sec7">
      <title>Residual-convolutional and recurrent neural network</title>
      <p id="Par35">In this study, we improve our previously proposed neural network models, CNN-GRU and CNN-BGRU [<xref ref-type="bibr" rid="CR48">48</xref>], with blocks of residual convolutions [<xref ref-type="bibr" rid="CR52">52</xref>]. As a result, the model architecture is formed by three main parts, as depicted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>: residual-convolutional (ResCNN), recurrent (RNN) and fully-connected (FC). We named these new models as ResCNN-GRU and ResCNN-BGRU, depending on the use of unidirectional or bidirectional layers of gated recurrent units (GRU) in the recurrent part.</p>
      <sec id="Sec8">
        <title>Residual-convolutional part</title>
        <p id="Par36">The convolutional neural network (CNN) aims to capture the local context of each residue in the protein domain and discover short-term patterns within the amino acid sequence. At each CNN layer, we apply a 1D-convolution operation along the sequence dimension, with several convolutional filters of specific length to be learned. Considering an input of size <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times 45$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq10.gif"/></alternatives></inline-formula>, the output of each 1D-convolutional layer is of size <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times N_l$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq11.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N_l$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>N</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq12.gif"/></alternatives></inline-formula> is the number of learned filters in the <italic>l</italic>-th layer. In our model, the 1D-convolutional layers are grouped into residual blocks [<xref ref-type="bibr" rid="CR52">52</xref>]. The output <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {R}(x_b, \mathcal {W}_b)$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq13.gif"/></alternatives></inline-formula> of each residual block is combined with its input <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_b$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mi>x</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq14.gif"/></alternatives></inline-formula> as <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{b+1} = x_b + \mathcal {R}(x_b, \mathcal {W}_b)$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq15.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {W}_b$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mi mathvariant="script">W</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq16.gif"/></alternatives></inline-formula> are the weights and biases associated to the <italic>b</italic>-th residual block, and <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {R}(\cdot )$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq17.gif"/></alternatives></inline-formula> is the mapping function performed by the block.</p>
        <p id="Par37">Figure <xref rid="Fig2" ref-type="fig">2</xref>a presents the ResCNN part of our model. We first apply an initial 1D-convolution to transform the <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times 45$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq18.gif"/></alternatives></inline-formula> input features into <inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times 256$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq19.gif"/></alternatives></inline-formula> outputs by using 256 filters of length 1. These are then processed by two residual blocks, each one formed by two layers with 64 and 256 filters of length 5. After each convolution, ReLU activation and Batch-Normalization [<xref ref-type="bibr" rid="CR63">63</xref>] are applied.<fig id="Fig2"><label>Fig. 2</label><caption><p>The proposed ResCNN-BGRU neural network model for fold-related embedding learning through protein fold classification. The model architecture contains three differentiated parts. The residual-convolutional network <bold>a</bold> processes the input <inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times 45$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>45</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq20.gif"/></alternatives></inline-formula> residue-level features and consists of two residual blocks with two 1D-convolutional layers each. Its output is passed through a bidirectional layer of gated recurrent units (<bold>b</bold>) to obtain a fixed size representation of the input domain, which is further processed by two fully-connected layers (<bold>c</bold>). The first FC layer learns a 512-dimensional embedding vector for each input, while the second one learns a class weight matrix <inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}$$\end{document}</tex-math><mml:math id="M42"><mml:mi mathvariant="bold">W</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq21.gif"/></alternatives></inline-formula> to perform the classification into <italic>K</italic> fold classes. The ResCNN-GRU model is identical but using a unidirectional GRU layer instead</p></caption><graphic xlink:href="12859_2021_4419_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
      <sec id="Sec9">
        <title>Recurrent part</title>
        <p id="Par38">The purpose of the recurrent neural network (RNN) is to exploit long-distance relations through all the amino acid sequence and generate a summary of the whole protein domain at its output. Here, the <inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times 256$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq22.gif"/></alternatives></inline-formula> outputs from the ResCNN are fed into a gated recurrent unit (GRU) [<xref ref-type="bibr" rid="CR50">50</xref>] based layer with 1024 state units.</p>
        <p id="Par39">As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b, instead of saving all the <inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L\times 1024$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq23.gif"/></alternatives></inline-formula> states of the GRU, we only consider the last state (<inline-formula id="IEq24"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{\mathbf {h}_L}$$\end{document}</tex-math><mml:math id="M48"><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq24.gif"/></alternatives></inline-formula>) as a summary vector of 1024 elements. In this way, our model architecture can process amino acid sequences of arbitrary length and extract a fixed-size vector representing the whole protein domain. We refer to this model as ResCNN-GRU. An alternative architecture is that based on a bidirectional GRU [<xref ref-type="bibr" rid="CR64">64</xref>] which also processes the sequence in reverse order. In such a case, last states from both forward (<inline-formula id="IEq25"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{\mathbf {h}_L}$$\end{document}</tex-math><mml:math id="M50"><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq25.gif"/></alternatives></inline-formula>) and backward (<inline-formula id="IEq26"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overleftarrow{\mathbf {h}_L}$$\end{document}</tex-math><mml:math id="M52"><mml:mover accent="true"><mml:msub><mml:mi mathvariant="bold">h</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">←</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq26.gif"/></alternatives></inline-formula>) GRU layers are concatenated into a vector of 2048 elements. We denote this model as ResCNN-BGRU.</p>
      </sec>
      <sec id="Sec10">
        <title>Fully-connected part</title>
        <p id="Par40">Finally, the fully-connected (FC) part combines the recurrent output to create a fold-related embedding for the whole protein domain, which is then used to perform a preliminary fold classification. The classification step guides the model during training to learn a meaningful embedding space, which is related to the protein folds. Then, these learned embeddings are used for pairwise fold recognition in the test phase.</p>
        <p id="Par41">In particular, the FC part (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c) consists of two dense layers. The first one, with 512 units, is used to learn a nonlinear combination of the GRU output vector (1024 or 2048 for the unidirectional and bidirectional architectures, respectively) which shapes the fold-related embedding. As nonlinearity, both the sigmoid and the hyperbolic tangent (tanh) activation functions have been tested in our experiments. The last layer performs a linear classification of the 512-dimensional embeddings using <italic>K</italic> output units. Here, <italic>K</italic> is the number of fold classes in which the input proteins are classified during training. In the following subsections we detail how this last classification layer can be modified to learn more discriminative embedding vectors by distributing the fold class vectors in hyperspherical space.</p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Neural network model optimization</title>
      <p id="Par42">We trained our neural network models with mini-batches of 64 protein domains. To process variable-length sequences, we applied zero-padding to the maximum length within each mini-batch. After the GRU layer, we kept the last state vector of each domain sample before the zero-padding, which corresponds to the last amino acid residue of each domain in the mini-batch. In the bidirectional GRU, the same GRU layers are used but the amino acid sequences were first reversed for the backward layer, so the last state (before zero-padding) corresponds to the first residue of each domain. The optimization process was performed in two different stages by comparing the model predictions with the true fold classes (ground truth). In the first one (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a), we optimized the models by minimizing the well-known softmax cross-entropy loss, while in the second stage (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c) we used the large margin cosine loss (LMCL) [<xref ref-type="bibr" rid="CR56">56</xref>], which is a normalized and margin discriminative version of the softmax loss. In this case, we also used a fixed (i.e. non-trainable) weight matrix in the classification layer (<inline-formula id="IEq27"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}$$\end{document}</tex-math><mml:math id="M54"><mml:mi mathvariant="bold">W</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq27.gif"/></alternatives></inline-formula> in Fig. <xref rid="Fig2" ref-type="fig">2</xref>c) which maximally separates fold class vectors in hyperspherical space (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b). We used the Adam optimizer [<xref ref-type="bibr" rid="CR65">65</xref>] with an initial learning rate of <inline-formula id="IEq28"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-3}$$\end{document}</tex-math><mml:math id="M56"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq28.gif"/></alternatives></inline-formula>, which we reduced by a factor of 10 at epoch number 40, whereas the whole optimization process was completed in 80 epochs. In order to prevent overfitting to the most populated fold classes, we applied <inline-formula id="IEq29"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq29.gif"/></alternatives></inline-formula> penalty with a small weight decay of <inline-formula id="IEq30"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5\times 10^{-4}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq30.gif"/></alternatives></inline-formula> and dropout [<xref ref-type="bibr" rid="CR66">66</xref>] with a drop probability of 0.2 in the convolutional and the first FC layers.</p>
    </sec>
    <sec id="Sec12">
      <title>Large margin cosine loss</title>
      <p id="Par43">The softmax cross-entropy loss (softmax loss for simplicity) is one of the most common loss functions for multi-class classification problems. It is defined as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{softmax} = - \frac{1}{N} \sum _{i=1}^N \log p_i = - \frac{1}{N} \sum _{i=1}^N \log \frac{ e^{f_{y_i}} }{ \sum _{k=1}^K e^{f_k} }, \end{aligned}$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">softmax</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>log</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4419_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq31"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_i$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq31.gif"/></alternatives></inline-formula> is the posterior probability of the <inline-formula id="IEq32"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {x}_i$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq32.gif"/></alternatives></inline-formula> embedding sample being classified into its ground-truth class <inline-formula id="IEq33"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M68"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq33.gif"/></alternatives></inline-formula>, <italic>N</italic> is the number of training samples in the mini-batch (<inline-formula id="IEq34"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i={1, \dots , N}$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq34.gif"/></alternatives></inline-formula>), <italic>K</italic> is the number of classes (<inline-formula id="IEq35"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k={1, \dots , K}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq35.gif"/></alternatives></inline-formula>), and <inline-formula id="IEq36"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_k$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq36.gif"/></alternatives></inline-formula> is the output of the last linear classification layer with weight matrix <inline-formula id="IEq37"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W} \in \mathbb {R}^{K\times d}$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq37.gif"/></alternatives></inline-formula> (the bias is set to zero for simplicity). For each input <inline-formula id="IEq38"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {x}_i$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq38.gif"/></alternatives></inline-formula>, the output corresponding to class <italic>k</italic> is computed as:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} f_k = \mathbf {w}_k^T \mathbf {x}_i = \left\| \mathbf {w}_k \right\| \left\| \mathbf {x}_i \right\| \cos (\theta _{k,i}), \end{aligned}$$\end{document}</tex-math><mml:math id="M80" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfenced><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4419_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>with <inline-formula id="IEq39"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta _{k,i}$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq39.gif"/></alternatives></inline-formula> being the angle between the vectors <inline-formula id="IEq40"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {w}_k$$\end{document}</tex-math><mml:math id="M84"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq40.gif"/></alternatives></inline-formula> and <inline-formula id="IEq41"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {x}_i$$\end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq41.gif"/></alternatives></inline-formula>. If we enforce that <inline-formula id="IEq42"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\| \mathbf {w}_k \right\| = 1$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq42.gif"/></alternatives></inline-formula> through <inline-formula id="IEq43"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M90"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq43.gif"/></alternatives></inline-formula> normalization, and <inline-formula id="IEq44"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\| \mathbf {x}_i \right\| = s$$\end{document}</tex-math><mml:math id="M92"><mml:mrow><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq44.gif"/></alternatives></inline-formula> by using a tunable scale hyperparameter, the posterior probability only depends on the cosine of the angle <inline-formula id="IEq45"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta _{k,i}$$\end{document}</tex-math><mml:math id="M94"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq45.gif"/></alternatives></inline-formula>. This results in the normalized softmax loss (NSL), defined as:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{ns} = - \frac{1}{N} \sum _{i=1}^N \log \frac{ e^{s \cos (\theta _{y_i,i}) } }{ \sum _{k=1}^K e^{s \cos (\theta _{k,i})} }. \end{aligned}$$\end{document}</tex-math><mml:math id="M96" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">ns</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>log</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4419_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>The feature embeddings learned by NSL are angularly distributed, but they are not necessarily more discriminative than the ones learned by softmax loss. In order to control the classification boundaries, two variants of the NSL, the angular softmax (A-Softmax) loss [<xref ref-type="bibr" rid="CR55">55</xref>] and the large margin cosine loss (LMCL) [<xref ref-type="bibr" rid="CR56">56</xref>], introduce a margin hyperparameter (<inline-formula id="IEq46"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m \ge 0$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq46.gif"/></alternatives></inline-formula>). The decision margin in LMCL is defined in cosine space rather than in angle space, which proved to be more beneficial when learning the classification boundaries [<xref ref-type="bibr" rid="CR56">56</xref>]. This is therefore the loss function we adopted to optimize our neural network models, and is formally defined as:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{lmc} = - \frac{1}{N} \sum _{i=1}^N \log \frac{ e^{s (\cos (\theta _{y_i,i}) - m)} }{ e^{s (\cos (\theta _{y_i,i}) - m)} + \sum _{k\ne y_i} e^{s \cos (\theta _{k,i})} }, \end{aligned}$$\end{document}</tex-math><mml:math id="M100" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">lmc</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>log</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>cos</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4419_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>subject to <inline-formula id="IEq47"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cos (\theta _{k,i}) = \hat{\mathbf {w}}_k^T \hat{\mathbf {x}}_i$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq47.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq48"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mathbf {w}}_k$$\end{document}</tex-math><mml:math id="M104"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq48.gif"/></alternatives></inline-formula> and <inline-formula id="IEq49"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mathbf {x}}_i$$\end{document}</tex-math><mml:math id="M106"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq49.gif"/></alternatives></inline-formula> are the <inline-formula id="IEq50"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M108"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq50.gif"/></alternatives></inline-formula> normalized vectors (<inline-formula id="IEq51"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mathbf {w}}_k = \mathbf {w}_k / \left\| \mathbf {w}_k \right\|$$\end{document}</tex-math><mml:math id="M110"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">w</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">/</mml:mo><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq51.gif"/></alternatives></inline-formula> and <inline-formula id="IEq52"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mathbf {x}}_i = \mathbf {x}_i / \left\| \mathbf {x}_i \right\|$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">/</mml:mo><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq52.gif"/></alternatives></inline-formula>).</p>
      <p id="Par44">As stated in the original paper [<xref ref-type="bibr" rid="CR56">56</xref>], by <inline-formula id="IEq53"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M114"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq53.gif"/></alternatives></inline-formula>-normalizing the embedding vectors <inline-formula id="IEq54"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {x}_i$$\end{document}</tex-math><mml:math id="M116"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq54.gif"/></alternatives></inline-formula>, we enforce them to be distributed on the surface of a <italic>d</italic>-dimensional hypersphere. Thus, the scaling hyperparameter <italic>s</italic> controls the radius of such hypersphere and its value increases with the number of classes. The margin hyperparameter <italic>m</italic> relates to the capacity of learning more discriminative embeddings. Possible values are in the range <inline-formula id="IEq55"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m \in [ 0, \frac{K}{K-1} )$$\end{document}</tex-math><mml:math id="M118"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mi>K</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq55.gif"/></alternatives></inline-formula>, although high values close to the upper-bound could cause failures in convergence. Having this in mind, we tuned the scale <italic>s</italic> and margin <italic>m</italic> hyperparameters for each neural network model through cross-validation.</p>
    </sec>
    <sec id="Sec13">
      <title>Thomson-derived hyperspherical prototypes</title>
      <p id="Par45">We hypothesize that by providing a non-trainable matrix <inline-formula id="IEq56"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W} \in \mathbb {R}^{K\times d}$$\end{document}</tex-math><mml:math id="M120"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq56.gif"/></alternatives></inline-formula> to the classification layer we can ease the training process. Such matrix contains <italic>K</italic> pre-defined prototype vectors representing each fold class, <inline-formula id="IEq57"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W} = \{ \mathbf {w}_1, \dots , \mathbf {w}_K \}$$\end{document}</tex-math><mml:math id="M122"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq57.gif"/></alternatives></inline-formula>. Thus, we can shape the embedding space to be representative of the protein folds, and so extract more meaningful fold-related embeddings for each protein during the training stage (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c). The use of such prototype networks was first proposed in [<xref ref-type="bibr" rid="CR58">58</xref>].</p>
      <sec id="Sec14">
        <title>Optimal distribution of prototypes</title>
        <p id="Par46">We argue that the optimal configuration of the <italic>K</italic> prototype vectors is that which provides maximal separation in the angular space. This can be achieved by placing the <italic>K</italic> points equidistant on the surface of a <italic>d</italic>-dimensional hypersphere, so <inline-formula id="IEq58"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {w}_k \in \mathbb {S}^{d-1}$$\end{document}</tex-math><mml:math id="M124"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">S</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq58.gif"/></alternatives></inline-formula>, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b. The Thomson problem [<xref ref-type="bibr" rid="CR57">57</xref>] addresses this by studying the distribution of <italic>K</italic> charged particles on the surface of a unit 3D-sphere. The minimum energy configuration can be optimized by measuring the Coulomb’s law. When using simplified units for electron charges and Coulomb’s constant, the formula for a pair of electrons reduces to <inline-formula id="IEq59"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E_{ij} = 1 / r_{ij}$$\end{document}</tex-math><mml:math id="M126"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq59.gif"/></alternatives></inline-formula>, relying only on the distance (<inline-formula id="IEq60"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_{ij}$$\end{document}</tex-math><mml:math id="M128"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq60.gif"/></alternatives></inline-formula>) between the two points.</p>
        <p id="Par47">This can be extended to points located on the surface of a hypersphere of <italic>d</italic> dimensions and computed for all possible pairs of points [<xref ref-type="bibr" rid="CR67">67</xref>]. We could therefore optimize the distribution of our <inline-formula id="IEq61"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {w}_k$$\end{document}</tex-math><mml:math id="M130"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq61.gif"/></alternatives></inline-formula> prototype vectors by minimizing the generalized Thomson loss (THL), defined as:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{th} = \sum _{k=1}^K \sum _{j=1}^{k-1} \frac{1}{\left\| \mathbf {w}_k - \mathbf {w}_j \right\| _2^2} + \frac{\lambda }{2} \sum _{k=1}^K (\left\| \mathbf {w}_k \right\| ^2 - 1)^2. \end{aligned}$$\end{document}</tex-math><mml:math id="M132" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4419_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>The hyperparameter <inline-formula id="IEq62"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M134"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq62.gif"/></alternatives></inline-formula> controls the weight of the norm constraint. Note that the Thomson loss uses the Euclidean distance between points, which is affected by the norm of each vector, while the cosine similarity is more adequate to measure the angular separation (independent of the norm). In order to remove the norm constraint from the loss function, we propose to directly maximize the Euclidean distance of the projected (<inline-formula id="IEq63"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_2$$\end{document}</tex-math><mml:math id="M136"><mml:msub><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq63.gif"/></alternatives></inline-formula>-normalized) vectors. Thus, we can remove the hyperparameter <inline-formula id="IEq64"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M138"><mml:mi>λ</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq64.gif"/></alternatives></inline-formula> from equation (<xref rid="Equ5" ref-type="">5</xref>), obtaining the following Thomson loss (THL–<italic>sum</italic>):<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{th\_sum} = \sum _{k=1}^K \sum _{j=1}^{k-1} \left\| \frac{\mathbf {w}_k}{\left\| \mathbf {w}_k \right\| } - \frac{\mathbf {w}_j}{\left\| \mathbf {w}_j \right\| } \right\| _2^{-2}. \end{aligned}$$\end{document}</tex-math><mml:math id="M140" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>_</mml:mi><mml:mi>s</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mfenced close="∥" open="∥"><mml:mfrac><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfenced></mml:mfrac><mml:mo>-</mml:mo><mml:mfrac><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mfrac></mml:mfenced><mml:mn>2</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4419_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>Alternatively, we can instead minimize the maximum cosine similarity computed for each prototype vector [<xref ref-type="bibr" rid="CR58">58</xref>], using the following loss function (THL–<italic>maxcos</italic>):<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{th\_maxcos} = \frac{1}{K} \sum _{k=1}^K \max _{j\ne k} \left( \frac{\mathbf {w}_k \cdot \mathbf {w}_j}{\left\| \mathbf {w}_k \right\| \left\| \mathbf {w}_j \right\| } \right) . \end{aligned}$$\end{document}</tex-math><mml:math id="M142" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>_</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfenced><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4419_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>Maximally separated prototype vectors are obtained by means of gradient descent over the proposed loss function (either THL–<italic>sum</italic> or THL–<italic>maxcos</italic>), where it must be noted that all possible pairs of points are taken to perform a single iteration step.</p>
      </sec>
      <sec id="Sec15">
        <title>Initial prototype vectors</title>
        <p id="Par48">As initial matrix of prototypes we can consider a set of <italic>K</italic> Gaussian random variables of dimension <italic>d</italic>, <inline-formula id="IEq65"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}^{random}$$\end{document}</tex-math><mml:math id="M144"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">random</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq65.gif"/></alternatives></inline-formula>. However, we found that the learned classification matrix from a model previously trained with the softmax cross-entropy loss (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a), <inline-formula id="IEq66"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}^{softmax}$$\end{document}</tex-math><mml:math id="M146"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">softmax</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq66.gif"/></alternatives></inline-formula>, provides better results. Unlike <inline-formula id="IEq67"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}^{random}$$\end{document}</tex-math><mml:math id="M148"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">random</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq67.gif"/></alternatives></inline-formula>, the matrix <inline-formula id="IEq68"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}^{softmax}$$\end{document}</tex-math><mml:math id="M150"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">softmax</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq68.gif"/></alternatives></inline-formula> has been trained to classify protein domains into folds, somehow preserving the arrangement of the structural classes within the learned space. To show this, we measured the intra- and inter-structural class prototype separation, as well as the angular Fisher score (AFS) [<xref ref-type="bibr" rid="CR55">55</xref>]. Further details can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: S2.</p>
      </sec>
    </sec>
    <sec id="Sec16">
      <title>Pairwise similarity scores</title>
      <sec id="Sec17">
        <title>Cosine similarity measures</title>
        <p id="Par49">The FoldHSphere method (Fig. <xref rid="Fig1" ref-type="fig">1</xref>d) uses the hyperspherical embeddings extracted from our neural network model to compute a fold similarity measure between each pair of protein domains. Following previous works [<xref ref-type="bibr" rid="CR43">43</xref>, <xref ref-type="bibr" rid="CR48">48</xref>], we used the cosine similarity between two embedding vectors <inline-formula id="IEq69"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[ \mathbf {x}_i, \mathbf {x}_j ] \in \mathbb {R}^{d}$$\end{document}</tex-math><mml:math id="M152"><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq69.gif"/></alternatives></inline-formula> as metric, computed as:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \cos (\mathbf {x}_i, \mathbf {x}_j) = \frac{\mathbf {x}_i \cdot \mathbf {x}_j}{\left\| \mathbf {x}_i \right\| \left\| \mathbf {x}_j \right\| }, \end{aligned}$$\end{document}</tex-math><mml:math id="M154" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mfenced close="∥" open="∥"><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4419_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>which is a measure of angular separation (in the range <inline-formula id="IEq70"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[-1, 1]$$\end{document}</tex-math><mml:math id="M156"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq70.gif"/></alternatives></inline-formula>) and independent of the norm of each embedding vector.</p>
      </sec>
      <sec id="Sec18">
        <title>Random forest enhanced scores</title>
        <p id="Par50">To obtain an improved fold similarity score (FoldHSpherePro in Fig. <xref rid="Fig1" ref-type="fig">1</xref>d), we trained a random forest (RF) model using our cosine similarity score along with the 84 pairwise similarity measures from [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>] and the DeepFR cosine similarity [<xref ref-type="bibr" rid="CR43">43</xref>]. Thus, each input vector is of size 86 and corresponds to a pair of protein domains. The RF model uses this information to determine whether the domains in such a pair share the same fold class (binary classification). We trained and evaluated the RF models in a 10-stage cross-validation setting for the LINDAHL and LINDAHL_1.75 test sets independently. The random forest models used 500 decision trees each as in [<xref ref-type="bibr" rid="CR43">43</xref>, <xref ref-type="bibr" rid="CR48">48</xref>].</p>
      </sec>
    </sec>
    <sec id="Sec19">
      <title>Evaluation</title>
      <sec id="Sec20">
        <title>Three-level rank performance accuracy</title>
        <p id="Par51">As originally proposed in [<xref ref-type="bibr" rid="CR3">3</xref>], we evaluated the test protein domains at three levels of increasing difficulty—<italic>family, superfamily</italic> and <italic>fold</italic>. At each level, we differentiated between positive and negative pairs of domains. A negative pair contains two protein domains from different fold classes, while in a positive pair both domains are from the same fold class. Each level includes all the negative pairs, while positive pairs are selected according to the SCOP hierarchy [<xref ref-type="bibr" rid="CR7">7</xref>]. That is, the <italic>family level</italic> contains pairs of domains that share the same family class, and therefore the same superfamily and fold classes. At the <italic>superfamily level</italic>, the domains in each pair share the same superfamily class—and therefore the same fold—but not the same family. Finally, domains in positive pairs at the <italic>fold level</italic> only share the same fold class, but neither share the same family nor superfamily.</p>
        <p id="Par52">At each of these levels, for every individual protein domain (query) we ranked the rest of domains (templates) according to their similarity scores. These can be either cosine similarities or random forest output scores. Then, we assigned the fold class of the most similar template to the query and computed the ratio of hits—<italic>top 1 accuracy</italic>. We also obtained the ratio of finding the correct fold class within the 5 first-ranked templates—<italic>top 5 accuracy</italic>. It must be noted that, instead of using the training set as the search database, in this evaluation we aim to find template domains inside the test set itself (either LINDAHL or LINDAHL_1.75).</p>
        <p id="Par53">In order to measure the statistical significance of our top 1 and top 5 accuracy results, we also provide standard errors estimated as the standard deviation of 1000 bootstrap samples. To do so, we sampled with replacement from the set of individual protein domains that are tested at each level (555, 434 and 321 domains respectively in the LINDAHL dataset). Then, for each sampled set we selected all negative pairs and positive pairs corresponding to the specific level, and proceeded with the evaluation as before.</p>
      </sec>
      <sec id="Sec21">
        <title>Fold-level LINDAHL cross-validation evaluation</title>
        <p id="Par54">In order to compare with some recent methods [<xref ref-type="bibr" rid="CR35">35</xref>–<xref ref-type="bibr" rid="CR41">41</xref>, <xref ref-type="bibr" rid="CR44">44</xref>–<xref ref-type="bibr" rid="CR46">46</xref>] we also provide results on a fold-level 2-stage cross-validation setting on the LINDAHL test set [<xref ref-type="bibr" rid="CR22">22</xref>]. Here, the 321 protein domains which form positive pairs at the fold level are separated into two subsets LE_a and LE_b, with 159 and 162 domains each. Note that the rest of domains within LINDAHL (up to 976) are not considered during this evaluation. When evaluating the protein domains in each subset (e.g. LE_a), the domains in the other subset (LE_b) act as templates for ranking. Thus, the random forest models are trained using pairs of protein domains from one subset, whereas the evaluation is performed on the other one. In this evaluation, we report the averaged performance accuracy over both cross-validation subsets.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec22">
    <title>Results</title>
    <p id="Par55">
      <fig id="Fig3">
        <label>Fig. 3</label>
        <caption>
          <p>Cross-validation fold classification accuracy (%) results for different LMCL margins and scales <inline-formula id="IEq71"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=\{30, 50\}$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq71.gif"/></alternatives></inline-formula>, using the SCOPe 2.06 training set. The results are provided separately for each neural network model: CNN-GRU, CNN-BGRU, ResCNN-GRU and ResCNN-BGRU, trained using different combinations of activation function (in the embedding layer) and loss function. These are: softmax loss with sigmoid activation (dash-dotted horizontal line), LMCL with sigmoid activation (blue lines), LMCL with tanh activation (magenta lines) and Thomson LMCL with tanh activation (green lines). For the LMCL and Thomson LMCL results, solid lines and dashed lines correspond to scale values 30 and 50, respectively</p>
        </caption>
        <graphic xlink:href="12859_2021_4419_Fig3_HTML" id="MO11"/>
      </fig>
    </p>
    <sec id="Sec23">
      <title>Learning fold-related embeddings with LMCL</title>
      <p id="Par56">We first assessed the performance of the different neural network models trained either with the softmax loss (<xref rid="Equ1" ref-type="">1</xref>) or the LMCL (<xref rid="Equ4" ref-type="">4</xref>) (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>a), by cross-validation over the SCOPe 2.06 training set. For the softmax loss, we used the sigmoid activation in the embedding layer (first FC layer in Fig. <xref rid="Fig2" ref-type="fig">2</xref>c), so that we can compare with the CNN-GRU and CNN-BGRU models from [<xref ref-type="bibr" rid="CR48">48</xref>]. Then, for each model trained with the LMCL function, we tuned the scale and margin hyperparameters through cross-validation. We considered two values for the scale <inline-formula id="IEq72"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=\{30, 50\}$$\end{document}</tex-math><mml:math id="M160"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq72.gif"/></alternatives></inline-formula> and margins in the range <inline-formula id="IEq73"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m=[0.1,0.9]$$\end{document}</tex-math><mml:math id="M162"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq73.gif"/></alternatives></inline-formula>. Here we tested two activation functions at the embedding layer: sigmoid as well as hyperbolic tangent (tanh). We argue that having negative and positive values ranging from <inline-formula id="IEq74"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-1$$\end{document}</tex-math><mml:math id="M164"><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq74.gif"/></alternatives></inline-formula> to 1 in the embedding vector (tanh activation) would better exploit the hyperspherical space than having only positive values (sigmoid activation, range [0, 1]).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Optimal set of hyperparameters for the LMCL function</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Model</th><th align="left" colspan="2">(a) LMCL</th><th align="left" colspan="3">(b) Thomson LMCL</th></tr><tr><th align="left">Scale</th><th align="left">Margin</th><th align="left">Iter THL-sum</th><th align="left">Scale</th><th align="left">Margin</th></tr></thead><tbody><tr><td align="left">CNN-GRU</td><td align="left">30</td><td align="left">0.25</td><td align="left">1130</td><td align="left">30</td><td align="left">0.25</td></tr><tr><td align="left">CNN-BGRU</td><td align="left">30</td><td align="left">0.55</td><td align="left">1172</td><td align="left">30</td><td align="left">0.45</td></tr><tr><td align="left">ResCNN-GRU</td><td align="left">30</td><td align="left">0.50</td><td align="left">1181</td><td align="left">30</td><td align="left">0.55</td></tr><tr><td align="left">ResCNN-BGRU</td><td align="left">30</td><td align="left">0.60</td><td align="left">1020</td><td align="left">30</td><td align="left">0.60</td></tr></tbody></table><table-wrap-foot><p>The scale and margin hyperparameters are provided for each neural network model and two approaches: (<bold>a</bold>) training the last classification layer end-to-end, (<bold>b</bold>) using the fixed prototype matrix by minimizing the Thomson loss THL–<italic>sum</italic>. We also include here the optimal iteration from the Thomson algorithm</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Effect of model architecture and loss function choice on FoldHSphere performance using the LINDAHL dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Model</th><th align="left" colspan="2">Family</th><th align="left" colspan="2">Superfamily</th><th align="left" colspan="2">Fold</th></tr><tr><th align="left">Top 1</th><th align="left">Top 5</th><th align="left">Top 1</th><th align="left">Top 5</th><th align="left">Top 1</th><th align="left">Top 5</th></tr></thead><tbody><tr><td align="left" colspan="7"><italic>(a) Softmax loss</italic></td></tr><tr><td align="left">CNN-GRU [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">68.6 (1.94)</td><td align="left">89.2 (1.37)</td><td align="left">56.2 (2.34)</td><td align="left">77.4 (1.96)</td><td align="left">56.7 (2.82)</td><td align="left">74.1 (2.46)</td></tr><tr><td align="left">CNN-BGRU [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">71.0 (1.92)</td><td align="left">87.7 (1.42)</td><td align="left">60.1 (2.30)</td><td align="left">77.2 (2.02)</td><td align="left">58.3 (2.83)</td><td align="left"><bold>78.8</bold> (2.27)</td></tr><tr><td align="left">ResCNN-GRU</td><td align="left">72.6 (1.87)</td><td align="left">90.3 (1.24)</td><td align="left">59.4 (2.32)</td><td align="left">77.0 (2.00)</td><td align="left">58.9 (2.88)</td><td align="left">75.1 (2.44)</td></tr><tr><td align="left">ResCNN-BGRU</td><td align="left"><bold>76.8</bold> (1.78)</td><td align="left"><bold>91.2</bold> (1.23)</td><td align="left"><bold>65.0</bold> (2.29)</td><td align="left"><bold>82.0</bold> (1.84)</td><td align="left"><bold>59.5</bold> (2.79)</td><td align="left">76.6 (2.35)</td></tr><tr><td align="left" colspan="7"><italic>(b) LMCL</italic></td></tr><tr><td align="left">CNN-GRU</td><td align="left"><bold>76.6</bold> (1.80)</td><td align="left"><bold>90.8</bold> (1.25)</td><td align="left">64.7 (2.21)</td><td align="left">80.2 (1.90)</td><td align="left">65.7 (2.69)</td><td align="left">79.8 (2.22)</td></tr><tr><td align="left">CNN-BGRU</td><td align="left">76.2 (1.79)</td><td align="left">89.4 (1.31)</td><td align="left"><bold>70.5</bold> (2.12)</td><td align="left">83.2 (1.80)</td><td align="left">72.0 (2.48)</td><td align="left">81.0 (2.21)</td></tr><tr><td align="left">ResCNN-GRU</td><td align="left">75.7 (1.77)</td><td align="left">89.7 (1.25)</td><td align="left">66.4 (2.29)</td><td align="left">81.1 (1.86)</td><td align="left">67.6 (2.63)</td><td align="left">80.1 (2.23)</td></tr><tr><td align="left">ResCNN-BGRU</td><td align="left">75.1 (1.84)</td><td align="left">89.5 (1.30)</td><td align="left">69.8 (2.25)</td><td align="left"><bold>85.3</bold> (1.67)</td><td align="left"><bold>74.1</bold> (2.42)</td><td align="left"><bold>82.2</bold> (2.12)</td></tr><tr><td align="left" colspan="7"><italic>(c) Thomson LMCL</italic></td></tr><tr><td align="left">CNN-GRU</td><td align="left"><bold>80.0</bold> (1.73)</td><td align="left">90.6 (1.24)</td><td align="left">66.8 (2.23)</td><td align="left">80.2 (1.94)</td><td align="left">66.0 (2.62)</td><td align="left">80.1 (2.22)</td></tr><tr><td align="left">CNN-BGRU</td><td align="left">77.5 (1.75)</td><td align="left"><bold>91.7</bold> (1.19)</td><td align="left">69.8 (2.09)</td><td align="left">85.3 (1.64)</td><td align="left">72.6 (2.48)</td><td align="left">82.2 (2.14)</td></tr><tr><td align="left">ResCNN-GRU</td><td align="left">76.9 (1.78)</td><td align="left">89.5 (1.28)</td><td align="left">69.1 (2.20)</td><td align="left">82.9 (1.77)</td><td align="left">69.5 (2.57)</td><td align="left">79.4 (2.26)</td></tr><tr><td align="left">ResCNN-BGRU</td><td align="left">76.4 (1.77)</td><td align="left">89.2 (1.30)</td><td align="left"><bold>72.8</bold> (2.15)</td><td align="left"><bold>86.4</bold> (1.63)</td><td align="left"><bold>75.1</bold> (2.47)</td><td align="left"><bold>84.1</bold> (2.12)</td></tr></tbody></table><table-wrap-foot><p>The fold recognition accuracy (%) results are provided at the family, superfamily and fold levels, considering both the top 1 and top 5 ranked templates. We compare the CNN-GRU, CNN-BGRU, ResCNN-GRU and ResCNN-BGRU neural network models, trained with different loss functions: <bold>(a)</bold> Softmax loss with sigmoid activation, <bold>(b)</bold> LMCL with tanh activation, and <bold>(c)</bold> Thomson LMCL with tanh activation. Optimal LMCL hyperparameters are in Table <xref rid="Tab1" ref-type="table">1</xref>. Boldface indicates the best performance per loss function. For each accuracy result, we also provide in parentheses the standard error estimated using 1000 bootstraps</p></table-wrap-foot></table-wrap></p>
      <p id="Par57">The cross-validation fold classification accuracy on the training set for the different models and loss functions is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. When using softmax loss, we can observe that the models applying residual convolutions (ResCNN-GRU and ResCNN-BGRU) perform better at fold classification than their counterparts (CNN-GRU and CNN-BGRU). We also observe that the tanh activation function yields better results than the sigmoid activation for all tested margin values in the LMCL function. In this case, the scale value <inline-formula id="IEq75"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=30$$\end{document}</tex-math><mml:math id="M166"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq75.gif"/></alternatives></inline-formula> outperforms <inline-formula id="IEq76"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=50$$\end{document}</tex-math><mml:math id="M168"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq76.gif"/></alternatives></inline-formula> for both activation functions. As for the margin, larger values seem to further benefit models applying bidirectional GRU (CNN-BGRU and ResCNN-BGRU), suggesting that these models have a higher discriminative capacity. The optimal LMCL hyperparameters for each model are summarized in Table <xref rid="Tab1" ref-type="table">1</xref>a.</p>
      <p id="Par58">In Table <xref rid="Tab2" ref-type="table">2</xref> we provide the fold recognition accuracy results on the LINDAHL test set (at the family, superfamily and fold levels), when using the cosine similarity (<xref rid="Equ8" ref-type="">8</xref>) as ranking metric. Here, we used the optimal LMCL hyperparameters to train each model on the whole training set, from which we extracted the fold-related embeddings. Table <xref rid="Tab2" ref-type="table">2</xref>a shows that the learned embeddings from the ResCNN-GRU and ResCNN-BGRU models using softmax loss yield slightly better fold recognition performance at the three levels than the CNN-GRU and CNN-BGRU models. On the other hand, in Table <xref rid="Tab2" ref-type="table">2</xref>b we observe a large performance boost at all levels when introducing the LMCL as loss function in comparison with softmax loss. At the fold level, we achieve performance gains of 9 or more percentage points for most of the models. More precisely, the CNN-BGRU and ResCNN-BGRU models stand out for their remarkable results at the fold level, with 72.0% and 74.1% top 1 accuracy values respectively.</p>
    </sec>
    <sec id="Sec24">
      <title>Enhancing embedding discrimination power through Thomson-derived hyperspherical prototypes</title>
      <p id="Par59">We then tested the performance of our neural network models trained with a fixed matrix of prototypes <inline-formula id="IEq77"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W} \in \mathbb {R}^{K\times d}$$\end{document}</tex-math><mml:math id="M170"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq77.gif"/></alternatives></inline-formula> in the classification layer (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c), being the number of fold classes <inline-formula id="IEq78"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K=1154$$\end{document}</tex-math><mml:math id="M172"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1154</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq78.gif"/></alternatives></inline-formula> and embedding dimension <inline-formula id="IEq79"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d=512$$\end{document}</tex-math><mml:math id="M174"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq79.gif"/></alternatives></inline-formula>. The fold prototype vectors have been maximally separated in the angular space by minimizing the THL–<italic>sum</italic> (<xref rid="Equ6" ref-type="">6</xref>), using the <inline-formula id="IEq80"><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}^{softmax}$$\end{document}</tex-math><mml:math id="M176"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">softmax</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq80.gif"/></alternatives></inline-formula> from each model as initial matrix (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b). A detailed study comparing the performance of the two variants for the Thomson loss (THL–<italic>sum</italic> and THL–<italic>maxcos</italic>) and two options for the initial matrix (<inline-formula id="IEq81"><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}^{softmax}$$\end{document}</tex-math><mml:math id="M178"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">softmax</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq81.gif"/></alternatives></inline-formula> and <inline-formula id="IEq82"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {W}^{random}$$\end{document}</tex-math><mml:math id="M180"><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">random</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4419_Article_IEq82.gif"/></alternatives></inline-formula>) can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: S2.</p>
      <p id="Par60">Given the optimized matrix of prototypes for each model, we tuned the LMCL scale and margin values by cross-validation over the SCOPe 2.06 training set, considering the tanh activation in the embedding layer. Results from this tuning are also shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. As can be observed, the Thomson LMCL achieves better fold classification results, specially for the models applying residual convolutions, and particularly in the case of ResCNN-BGRU.</p>
      <p id="Par61">Finally, we set the optimal LMCL hyperparameters for each model (Table <xref rid="Tab1" ref-type="table">1</xref>b) and trained them to extract fold-related hyperspherical embeddings. The fold recognition LINDAHL results in Table <xref rid="Tab2" ref-type="table">2</xref>c show that, at the fold level, all the models benefit from the Thomson LMCL. Our best model, the ResCNN-BGRU, achieves top 1 accuracy values of 76.4%, 72.8% and 75.1% at the family, superfamily and fold levels, and top 5 accuracy of 89.2%, 86.4% and 84.1% at each level, respectively.</p>
    </sec>
    <sec id="Sec25">
      <title>Analysis of the hyperspherical embeddings</title>
      <p id="Par62">
        <fig id="Fig4">
          <label>Fig. 4</label>
          <caption>
            <p>Cosine similarity probability histograms computed for all unique pairs within the LINDAHL test set (976 domains), grouping the negative pairs in blue color, and positive pairs in orange. To compute the cosine similarity scores, we used the embeddings extracted from the ResCNN-BGRU model trained with (<bold>a</bold>) softmax loss with sigmoid activation, (<bold>b</bold>) LMCL with tanh activation, or (<bold>c</bold>) Thomson LMCL with tanh activation</p>
          </caption>
          <graphic xlink:href="12859_2021_4419_Fig4_HTML" id="MO12"/>
        </fig>
      </p>
      <p id="Par63">The fold recognition results of our FoldHSphere method using the ResCNN-BGRU model trained with hyperspherical prototypes reflect the effectiveness and discrimination capability of the learned hyperspherical embeddings. To further illustrate this, we analyzed the 512-dimensional embeddings extracted from the 976 protein domains in the LINDAHL dataset. Figure <xref rid="Fig4" ref-type="fig">4</xref> compares the histogram of cosine similarities computed between each pair of embeddings for the softmax, LMCL and Thomson LMCL options. For each one, we plotted separately the histogram of negative pairs (different fold classes) and positive pairs (same fold class). It can be seen that the Thomson LMCL provides a better separation between positive and negative pairs, with a small overlap between the two groups. This directly contributes to a better performance in the pairwise fold recognition task. Additionally, we provide a two-dimensional visualization of the embedding space learned by the three loss functions in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S6, as well as a dendroheatmap of the hyperspherical embeddings obtained by the Thomson LMCL approach in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S7.</p>
    </sec>
    <sec id="Sec26">
      <title>FoldHSphere and FoldHSpherePro pairwise fold recognition performance results</title>
      <p id="Par64">Finally, we compare the results of our FoldHSphere and FoldHSpherePro approaches with several methods from the state-of-the-art, considering both the LINDAHL and LINDAHL_1.75 test sets. The FoldHSphere results correspond to those from the ResCNN-BGRU model trained with hyperspherical prototypes (Table <xref rid="Tab2" ref-type="table">2</xref>). The FoldHSpherePro results were obtained after conducting a 10-stage cross-validation on a random forest model using the FoldHSphere scores along with other pre-computed protein pairwise similarities as inputs.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Three-level LINDAHL fold recognition results of FoldHSphere and FoldHSpherePro in comparison with the state-of-the-art</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method</th><th align="left" colspan="2">Family</th><th align="left" colspan="2">Superfamily</th><th align="left" colspan="2">Fold</th></tr><tr><th align="left">Top 1</th><th align="left">Top 5</th><th align="left">Top 1</th><th align="left">Top 5</th><th align="left">Top 1</th><th align="left">Top 5</th></tr></thead><tbody><tr><td align="left">PSI-BLAST [<xref ref-type="bibr" rid="CR3">3</xref>]</td><td align="left">71.2</td><td align="left">72.3</td><td align="left">27.4</td><td align="left">27.9</td><td align="left">4.0</td><td align="left">4.7</td></tr><tr><td align="left">HHpred [<xref ref-type="bibr" rid="CR14">14</xref>]</td><td align="left">82.9</td><td align="left">87.1</td><td align="left">58.0</td><td align="left">70.0</td><td align="left">25.2</td><td align="left">39.4</td></tr><tr><td align="left">RAPTOR [<xref ref-type="bibr" rid="CR14">14</xref>]</td><td align="left"><bold>86.6</bold></td><td align="left">89.3</td><td align="left">56.3</td><td align="left">69.0</td><td align="left">38.2</td><td align="left">58.7</td></tr><tr><td align="left">BoostThreader [<xref ref-type="bibr" rid="CR14">14</xref>]</td><td align="left">86.5</td><td align="left">90.5</td><td align="left">66.1</td><td align="left">76.4</td><td align="left">42.6</td><td align="left">57.4</td></tr><tr><td align="left">SPARKS-X [<xref ref-type="bibr" rid="CR15">15</xref>]</td><td align="left">84.1</td><td align="left">90.3</td><td align="left">59.0</td><td align="left">76.3</td><td align="left">45.2</td><td align="left">67.0</td></tr><tr><td align="left">FOLDpro [<xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left">85.0</td><td align="left">89.9</td><td align="left">55.0</td><td align="left">70.0</td><td align="left">26.5</td><td align="left">48.3</td></tr><tr><td align="left">RF-Fold [<xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left">84.5</td><td align="left">91.5</td><td align="left">63.4</td><td align="left">79.3</td><td align="left">40.8</td><td align="left">58.3</td></tr><tr><td align="left">DN-Fold [<xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left">84.5</td><td align="left">91.2</td><td align="left">61.5</td><td align="left">76.5</td><td align="left">33.6</td><td align="left">60.7</td></tr><tr><td align="left">RFDN-Fold [<xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left">84.7</td><td align="left">91.5</td><td align="left">65.7</td><td align="left">78.8</td><td align="left">37.7</td><td align="left">61.7</td></tr><tr><td align="left">MRFalign [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">85.2</td><td align="left">90.8</td><td align="left">72.4</td><td align="left">80.9</td><td align="left">38.6</td><td align="left">56.7</td></tr><tr><td align="left">CEthreader [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">76.6</td><td align="left">87.2</td><td align="left">69.4</td><td align="left">81.8</td><td align="left">52.3</td><td align="left">70.4</td></tr><tr><td align="left">DeepFR (s2) [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">65.4</td><td align="left">83.4</td><td align="left">51.4</td><td align="left">67.1</td><td align="left">56.1</td><td align="left">70.1</td></tr><tr><td align="left">DeepFRpro (s2) [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td align="left">83.1</td><td align="left">92.3</td><td align="left">69.6</td><td align="left">82.5</td><td align="left">66.0</td><td align="left">78.8</td></tr><tr><td align="left">VGGfold [<xref ref-type="bibr" rid="CR47">47</xref>]</td><td align="left">67.9</td><td align="left">84.3</td><td align="left">53.2</td><td align="left">68.4</td><td align="left">58.3</td><td align="left">73.5</td></tr><tr><td align="left">CNN-BGRU [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">71.0</td><td align="left">87.7</td><td align="left">60.1</td><td align="left">77.2</td><td align="left">58.3</td><td align="left">78.8</td></tr><tr><td align="left">CNN-BGRU-RF+ [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">85.4</td><td align="left"><bold>93.5</bold></td><td align="left">73.3</td><td align="left">87.8</td><td align="left">76.3</td><td align="left">85.7</td></tr><tr><td align="left">FoldHSphere</td><td align="left">76.4</td><td align="left">89.2</td><td align="left">72.8</td><td align="left">86.4</td><td align="left">75.1</td><td align="left">84.1</td></tr><tr><td align="left">FoldHSpherePro</td><td align="left">85.2</td><td align="left">93.0</td><td align="left"><bold>79.0</bold></td><td align="left"><bold>89.2</bold></td><td align="left"><bold>81.3</bold></td><td align="left"><bold>90.3</bold></td></tr></tbody></table><table-wrap-foot><p>The accuracy (%) results are provided at the family, superfamily and fold levels, considering both the top 1 and top 5 ranked templates. Boldface indicates best performance</p></table-wrap-foot></table-wrap></p>
      <p id="Par65">The three-level LINDAHL fold recognition results are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. We can see that our FoldHSphere method yields better top 1 accuracy values, above 12 percentage points at the superfamily and fold levels compared to the state-of-the-art method CNN-BGRU [<xref ref-type="bibr" rid="CR48">48</xref>]. At the family level, we outperform all the deep learning methods. However, the alignment methods and approaches relying on pairwise similarities provide better results at this level. We include such information in the FoldHSpherePro method, which can be compared to DeepFRpro [<xref ref-type="bibr" rid="CR43">43</xref>] and CNN-BGRU-RF+ [<xref ref-type="bibr" rid="CR48">48</xref>] as all of them apply the same random forest ensemble approach. Our method provides a significant performance boost, obtaining remarkable top 1 accuracy results, with values of 79.0% at the superfamily level and 81.3% at the fold level. In terms of top 5 accuracy values, FoldHSpherePro also achieves the best performance, providing 89.2% and 90.3% at superfamily and fold levels respectively. On the other hand, at the family level we obtain on par results with the CNN-BGRU-RF+ method, being only outperformed by alignment and threading methods. This suggests that the performance of deep learning approaches might be saturating at this level. Similar conclusions can be drawn when evaluating the LINDAHL_1.75 test set (Table <xref rid="Tab4" ref-type="table">4</xref>). Here we only compare to the DeepFR and CNN-BGRU methods, as they have been previously tested on such a dataset. The results show that our FoldHSpherePro approach also performs the best in this dataset, yielding top 1 accuracy values of 87.9%, 81.2% and 80.9% at the three levels respectively.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Three-level LINDAHL_1.75 fold recognition results of FoldHSphere and FoldHSpherePro in comparison with the state-of-the-art</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method</th><th align="left" colspan="2">Family</th><th align="left" colspan="2">Superfamily</th><th align="left" colspan="2">Fold</th></tr><tr><th align="left">Top 1</th><th align="left">Top 5</th><th align="left">Top 1</th><th align="left">Top 5</th><th align="left">Top 1</th><th align="left">Top 5</th></tr></thead><tbody><tr><td align="left">DeepFR (s2) [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">72.2</td><td align="left">85.6</td><td align="left">46.6</td><td align="left">64.3</td><td align="left">50.8</td><td align="left">67.1</td></tr><tr><td align="left">DeepFRpro (s2) [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">87.0</td><td align="left">93.8</td><td align="left">71.9</td><td align="left">82.6</td><td align="left">63.5</td><td align="left">77.2</td></tr><tr><td align="left">CNN-BGRU [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left">73.1</td><td align="left">88.3</td><td align="left">60.1</td><td align="left">74.9</td><td align="left">60.1</td><td align="left">78.7</td></tr><tr><td align="left">CNN-BGRU-RF+ [<xref ref-type="bibr" rid="CR48">48</xref>]</td><td align="left"><bold>88.5</bold></td><td align="left"><bold>94.3</bold></td><td align="left">74.0</td><td align="left">86.3</td><td align="left">71.1</td><td align="left">86.8</td></tr><tr><td align="left">FoldHSphere</td><td align="left">77.9</td><td align="left">89.0</td><td align="left">72.4</td><td align="left">87.0</td><td align="left">75.8</td><td align="left">84.3</td></tr><tr><td align="left">FoldHSpherePro</td><td align="left">87.9</td><td align="left">94.1</td><td align="left"><bold>81.2</bold></td><td align="left"><bold>90.0</bold></td><td align="left"><bold>80.9</bold></td><td align="left"><bold>88.5</bold></td></tr></tbody></table><table-wrap-foot><p>The accuracy (%) results are provided at the family, superfamily and fold levels, considering both the top 1 and top 5 ranked templates. Boldface indicates best performance</p></table-wrap-foot></table-wrap></p>
      <p id="Par66">
        <fig id="Fig5">
          <label>Fig. 5</label>
          <caption>
            <p>Fold-level LINDAHL fold recognition accuracy (%) results of our proposed FoldHSpherePro method in comparison with other ensemble methods from the state-of-the-art. The results are averaged over the two cross-validated subsets (LE_a and LE_b)</p>
          </caption>
          <graphic xlink:href="12859_2021_4419_Fig5_HTML" id="MO13"/>
        </fig>
      </p>
      <p id="Par67">Figure <xref rid="Fig5" ref-type="fig">5</xref> includes the evaluation results of the fold-level 2-stage cross-validation setting on the LINDAHL dataset (over subsets LE_a and LE_b). In this case, we only compare to ensemble methods that have been assessed with such a methodology, namely TA-fold [<xref ref-type="bibr" rid="CR35">35</xref>], the multi-view learning frameworks MT-fold [<xref ref-type="bibr" rid="CR36">36</xref>], EMfold [<xref ref-type="bibr" rid="CR37">37</xref>] and MLDH-Fold [<xref ref-type="bibr" rid="CR38">38</xref>], the learning to rank approaches Fold-LTR-TCP [<xref ref-type="bibr" rid="CR39">39</xref>], FoldRec-C2C [<xref ref-type="bibr" rid="CR40">40</xref>] and ProtFold-DFG [<xref ref-type="bibr" rid="CR41">41</xref>], and the deep learning methods DeepSVM-fold [<xref ref-type="bibr" rid="CR44">44</xref>], MotifCNN-fold [<xref ref-type="bibr" rid="CR45">45</xref>] and SelfAT-Fold [<xref ref-type="bibr" rid="CR46">46</xref>] (residue and motif options). The results in Fig. <xref rid="Fig5" ref-type="fig">5</xref> show that our FoldHSpherePro method outperforms all of them yielding an accuracy of 85.6%.</p>
    </sec>
  </sec>
  <sec id="Sec27">
    <title>Discussion</title>
    <p id="Par68">In order to learn an embedding space that is representative of the protein folds, we have proposed a two-stage learning procedure. Our intuition here is that pre-defining the structure of the embedding space through fixed fold prototypes would ease the learning process for a neural network that embeds individual proteins into this space. The result of our experiments indicate that this intuition is well founded.</p>
    <p id="Par69">There are two important considerations when pre-defining the embedding space in our methodology: the initialization of the fold prototypes and how to obtain a suitable spatial distribution of these prototypes in the embedding space. Here we find that rather than initializing with random vectors in the hyperspherical space, better results are obtained by using the weight matrix from the classification layer of a neural network previously trained to map proteins to fold classes. Then, by maximally separating the weight vectors in this matrix in hyperspherical space, we obtain an effective configuration of the fold prototypes. We believe one of the main reasons is that such a matrix preserves the arrangement of the structural classes in the learned space, grouping related fold classes together and pushing them away from folds of unrelated structural classes (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>: S2).</p>
    <p id="Par70">Our experimental results in Table <xref rid="Tab2" ref-type="table">2</xref> display a large performance boost at the superfamily and fold levels when comparing our methodology (using LMCL) to previous approaches that use the softmax loss. Our initial intuition for the lower performance of the state-of-the-art at these levels is that since evaluation is done for pairs of proteins, it is possible that two proteins from different folds lying near the fold classification boundary are closer to each other than they are to proteins from their respective folds. This informs our choice for using the LMCL as loss function, which introduces a margin between fold classes to avoid these cases.</p>
    <p id="Par71">A further performance gain is seen when combining the LMCL margin with the pre-trained fold prototypes (Table <xref rid="Tab2" ref-type="table">2</xref>c). Here we use the fold prototypes optimized in the previous stage as a fixed (non-trainable) classification matrix for each neural network. We believe that the additional performance improvement is due to the simplified learning process that results from having this pre-defined organization of the folds in the embedding space, which is especially useful with limited and unbalanced training data. Stated differently, our models can focus on projecting protein embeddings closest to the corresponding fold prototypes without simultaneously learning where these prototypes should be.</p>
    <p id="Par72">We also observe from Fig. <xref rid="Fig3" ref-type="fig">3</xref> and Table <xref rid="Tab2" ref-type="table">2</xref> that the models applying residual convolutions benefit more from the use of pre-trained prototypes compared to only optimizing with LMCL. This suggests the residual connections might extract more robust features for each amino acid, which seems to be helpful for the recurrent layer to obtain a better fixed-size representation for the whole protein domain. In particular, our ResCNN-BGRU architecture provides the best results, which can be attributed to its greater flexibility compared to the other tested architectures.</p>
  </sec>
  <sec id="Sec28">
    <title>Conclusion</title>
    <p id="Par73">In this work we have proposed the FoldHSphere method to tackle the protein fold recognition problem. We described a neural network training procedure to learn fold-representative hyperspherical embeddings for the protein domains. The embeddings were extracted from a residual-convolutional and recurrent network architecture (ResCNN-BGRU), which is trained by minimizing the angular large margin cosine loss (LMCL) around pre-defined prototypes for the fold classes. We used a Thomson-based loss function to maximally separate the fold prototypes in hyperspherical space. This way, our embeddings proved to be more effective at identifying the fold class of each protein domain by pairwise comparison. When evaluating the LINDAHL dataset, FoldHSphere alone provided a remarkable performance boost at the superfamily and fold levels, being competitive even with previous ensemble methods. Furthermore, our FoldHSpherePro ensemble method significantly improved the state-of-the-art results, outperforming the best method CNN-BGRU-RF+ at these levels. Therefore, due to their discrimination capability, the hyperspherical embeddings could be used to find template proteins even when the amino acid sequence similarities are low and thus advance in the template-based modeling of protein structures.</p>
    <p id="Par74">As future work, we will explore the application of recently proposed embeddings from language models pre-trained using millions of unannotated protein sequences for the protein fold recognition task, as they have shown promising results in several downstream tasks, such as protein secondary structure prediction and subcellular localization prediction [<xref ref-type="bibr" rid="CR68">68</xref>–<xref ref-type="bibr" rid="CR70">70</xref>].</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec29">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2021_4419_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1.</bold> Supplementary file 1.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>BGRU</term>
        <def>
          <p id="Par4">Bidirectional gated-recurrent unit</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par5">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>FC</term>
        <def>
          <p id="Par6">Fully-connected</p>
        </def>
      </def-item>
      <def-item>
        <term>GRU</term>
        <def>
          <p id="Par7">Gated-recurrent unit</p>
        </def>
      </def-item>
      <def-item>
        <term>LMCL</term>
        <def>
          <p id="Par8">Large margin cosine loss</p>
        </def>
      </def-item>
      <def-item>
        <term>MSA</term>
        <def>
          <p id="Par9">Multiple sequence alignment</p>
        </def>
      </def-item>
      <def-item>
        <term>NSL</term>
        <def>
          <p id="Par10">Normalized softmax loss</p>
        </def>
      </def-item>
      <def-item>
        <term>PDB</term>
        <def>
          <p id="Par11">Protein Data Bank</p>
        </def>
      </def-item>
      <def-item>
        <term>PSI-BLAST</term>
        <def>
          <p id="Par12">Position-specific iterative basic local alignment search tool</p>
        </def>
      </def-item>
      <def-item>
        <term>PSSM</term>
        <def>
          <p id="Par13">Position-specific scoring matrix</p>
        </def>
      </def-item>
      <def-item>
        <term>ResCNN</term>
        <def>
          <p id="Par14">Residual-convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p id="Par15">Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p id="Par16">Recurrent neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>SCOP</term>
        <def>
          <p id="Par17">Structural Classification of Proteins</p>
        </def>
      </def-item>
      <def-item>
        <term>THL</term>
        <def>
          <p id="Par18">Thomson loss</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Amelia would like to thank Chirag Raman for his valuable input and feedback provided throughout the project.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>AVM proposed the use of LMCL to learn more discriminative protein embeddings. AG proposed to apply residual-convolutions in the network architecture and came up with the idea of prototype vectors for the fold classes. AVM implemented the system and designed the experiments with the guidance of AG. AVM wrote the manuscript text and created the figures. AG and VS supervised the project, participated in the discussions and reviewed the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work has been supported by the Spanish Ministry of Science, Innovation and Universities Project No. PID2019-104206GB-I00 / SRA (State Research Agency) / 10.13039/501100011033, as well as the FPI Grant BES-2017-079792.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>Source code, data and trained models can be found at <ext-link ext-link-type="uri" xlink:href="http://sigmat.ugr.es/%7eamelia/FoldHSphere">http://sigmat.ugr.es/~amelia/FoldHSphere</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar4">
      <title>Ethics approval and consent to participate</title>
      <p id="Par75">Not applicable.</p>
    </notes>
    <notes id="FPar5">
      <title>Consent for publication</title>
      <p id="Par76">Not applicable.</p>
    </notes>
    <notes id="FPar6" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par77">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chothia</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Finkelstein</surname>
            <given-names>AV</given-names>
          </name>
        </person-group>
        <article-title>The classification and origins of protein folding patterns</article-title>
        <source>Annu Rev Biochem</source>
        <year>1990</year>
        <volume>59</volume>
        <issue>1</issue>
        <fpage>1007</fpage>
        <lpage>1035</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev.bi.59.070190.005043</pub-id>
        <?supplied-pmid 2197975?>
        <pub-id pub-id-type="pmid">2197975</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>WR</given-names>
          </name>
          <name>
            <surname>Thornton</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>A new approach to protein fold recognition</article-title>
        <source>Nature</source>
        <year>1992</year>
        <volume>358</volume>
        <issue>6381</issue>
        <fpage>86</fpage>
        <pub-id pub-id-type="doi">10.1038/358086a0</pub-id>
        <?supplied-pmid 1614539?>
        <pub-id pub-id-type="pmid">1614539</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindahl</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Elofsson</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Identification of related proteins on family, superfamily and fold level</article-title>
        <source>J Mol Biol</source>
        <year>2000</year>
        <volume>295</volume>
        <issue>3</issue>
        <fpage>613</fpage>
        <lpage>625</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1999.3377</pub-id>
        <?supplied-pmid 10623551?>
        <pub-id pub-id-type="pmid">10623551</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schaeffer</surname>
            <given-names>RD</given-names>
          </name>
          <name>
            <surname>Daggett</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Protein folds and protein folding</article-title>
        <source>Protein Eng Des Sel</source>
        <year>2010</year>
        <volume>24</volume>
        <issue>1–2</issue>
        <fpage>11</fpage>
        <lpage>19</lpage>
        <?supplied-pmid 21051320?>
        <pub-id pub-id-type="pmid">21051320</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kolodny</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pereyaslavets</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Samson</surname>
            <given-names>AO</given-names>
          </name>
          <name>
            <surname>Levitt</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>On the universe of protein folds</article-title>
        <source>Annu Rev Biophys</source>
        <year>2013</year>
        <volume>42</volume>
        <fpage>559</fpage>
        <lpage>582</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-biophys-083012-130432</pub-id>
        <?supplied-pmid 23527781?>
        <pub-id pub-id-type="pmid">23527781</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berman</surname>
            <given-names>HM</given-names>
          </name>
          <name>
            <surname>Westbrook</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gilliland</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bhat</surname>
            <given-names>TN</given-names>
          </name>
          <name>
            <surname>Weissig</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shindyalov</surname>
            <given-names>IN</given-names>
          </name>
          <name>
            <surname>Bourne</surname>
            <given-names>PE</given-names>
          </name>
        </person-group>
        <article-title>The protein data bank</article-title>
        <source>Nucleic Acids Res</source>
        <year>2000</year>
        <volume>28</volume>
        <issue>1</issue>
        <fpage>235</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id>
        <?supplied-pmid 10592235?>
        <pub-id pub-id-type="pmid">10592235</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Murzin</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Brenner</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Hubbard</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chothia</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>SCOP: a structural classification of proteins database for the investigation of sequences and structures</article-title>
        <source>J Mol Biol</source>
        <year>1995</year>
        <volume>247</volume>
        <issue>4</issue>
        <fpage>536</fpage>
        <lpage>40</lpage>
        <?supplied-pmid 7723011?>
        <pub-id pub-id-type="pmid">7723011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fox</surname>
            <given-names>NK</given-names>
          </name>
          <name>
            <surname>Brenner</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Chandonia</surname>
            <given-names>J-M</given-names>
          </name>
        </person-group>
        <article-title>SCOPe: structural classification of proteins-extended, integrating SCOP and ASTRAL data and classification of new structures</article-title>
        <source>Nucleic Acids Res</source>
        <year>2014</year>
        <volume>42</volume>
        <issue>D1</issue>
        <fpage>304</fpage>
        <lpage>309</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt1240</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Orengo</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Michie</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Swindells</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Thornton</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>CATH—a hierarchic classification of protein domain structures</article-title>
        <source>Structure</source>
        <year>1997</year>
        <volume>5</volume>
        <issue>8</issue>
        <fpage>1093</fpage>
        <lpage>1109</lpage>
        <pub-id pub-id-type="doi">10.1016/S0969-2126(97)00260-8</pub-id>
        <?supplied-pmid 9309224?>
        <pub-id pub-id-type="pmid">9309224</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Gish</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>EW</given-names>
          </name>
          <name>
            <surname>Lipman</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Basic local alignment search tool</article-title>
        <source>J Mol Biol</source>
        <year>1990</year>
        <volume>215</volume>
        <issue>3</issue>
        <fpage>403</fpage>
        <lpage>410</lpage>
        <pub-id pub-id-type="doi">10.1016/S0022-2836(05)80360-2</pub-id>
        <pub-id pub-id-type="pmid">2231712</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Protein homology detection by HMM–HMM comparison</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <issue>7</issue>
        <fpage>951</fpage>
        <lpage>960</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti125</pub-id>
        <?supplied-pmid 15531603?>
        <pub-id pub-id-type="pmid">15531603</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>MRFalign: protein homology detection through alignment of Markov random fields</article-title>
        <source>PLoS Comput Biol</source>
        <year>2014</year>
        <volume>10</volume>
        <issue>3</issue>
        <fpage>1003500</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003500</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>RAPTOR: optimal protein threading by linear programming</article-title>
        <source>J Bioinform Comput Biol</source>
        <year>2003</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>95</fpage>
        <lpage>117</lpage>
        <pub-id pub-id-type="doi">10.1142/S0219720003000186</pub-id>
        <?supplied-pmid 15290783?>
        <pub-id pub-id-type="pmid">15290783</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Peng J, Xu J. Boosting protein threading accuracy. In: Annual international conference on research in computational molecular biology; 2009. pp. 31–45.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Faraggi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Improving protein fold recognition and template-based modeling by employing probabilistic-based matching between predicted one-dimensional structural properties of query and corresponding native properties of templates</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>27</volume>
        <issue>15</issue>
        <fpage>2076</fpage>
        <lpage>2082</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btr350</pub-id>
        <?supplied-pmid 21666270?>
        <pub-id pub-id-type="pmid">21666270</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A conditional neural fields model for protein threading</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>12</issue>
        <fpage>59</fpage>
        <lpage>66</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts213</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Morales-Cordovilla</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Sanchez</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ratajczak</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Protein alignment based on higher order conditional random fields for template-based modeling</article-title>
        <source>PLoS ONE</source>
        <year>2018</year>
        <volume>13</volume>
        <issue>6</issue>
        <fpage>0197912</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0197912</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Buchan</surname>
            <given-names>DWA</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>EigenTHREADER: analogous protein fold recognition by efficient contact map threading</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>17</issue>
        <fpage>2684</fpage>
        <lpage>2690</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx217</pub-id>
        <?supplied-pmid 28419258?>
        <pub-id pub-id-type="pmid">28419258</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wuyun</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Mortuza</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pearce</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Detecting distant-homology protein structures by aligning deep neural-network based contact maps</article-title>
        <source>PLoS Comput Biol</source>
        <year>2019</year>
        <volume>15</volume>
        <issue>10</issue>
        <fpage>1</fpage>
        <lpage>27</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007411</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Recent progress in machine learning-based methods for protein fold recognition</article-title>
        <source>Int J Mol Sci</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>12</issue>
        <fpage>2118</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms17122118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>H-B</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>Ensemble classifier for protein fold pattern recognition</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <issue>14</issue>
        <fpage>1717</fpage>
        <lpage>1722</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl170</pub-id>
        <?supplied-pmid 16672258?>
        <pub-id pub-id-type="pmid">16672258</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dong</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A new taxonomy-based protein fold recognition approach based on autocross-covariance transformation</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>20</issue>
        <fpage>2655</fpage>
        <lpage>2662</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp500</pub-id>
        <?supplied-pmid 19706744?>
        <pub-id pub-id-type="pmid">19706744</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J-Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Improving taxonomy-based protein fold recognition by using global and local features</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2011</year>
        <volume>79</volume>
        <issue>7</issue>
        <fpage>2053</fpage>
        <lpage>2064</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.23025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Lyons</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sattar</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A segmentation-based method to extract structural and evolutionary features for protein fold recognition</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <year>2014</year>
        <volume>11</volume>
        <issue>3</issue>
        <fpage>510</fpage>
        <lpage>519</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2013.2296317</pub-id>
        <?supplied-pmid 26356019?>
        <pub-id pub-id-type="pmid">26356019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paliwal</surname>
            <given-names>KK</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lyons</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Improving protein fold recognition using the amalgamation of evolutionary-based and structural based information</article-title>
        <source>BMC Bioinform</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>16</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lyons</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Heffernan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Advancing the accuracy of protein fold recognition by utilizing profiles from hidden Markov models</article-title>
        <source>IEEE Trans Nanobiosci</source>
        <year>2015</year>
        <volume>14</volume>
        <issue>7</issue>
        <fpage>761</fpage>
        <lpage>772</lpage>
        <pub-id pub-id-type="doi">10.1109/TNB.2015.2457906</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>ProFold: protein fold classification with additional structural features and a novel ensemble classifier</article-title>
        <source>BioMed Res Int</source>
        <year>2016</year>
        <volume>2016</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ibrahim</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Abadeh</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>Protein fold recognition using deep kernelized extreme learning machine and linear discriminant analysis</article-title>
        <source>Neural Comput Appl</source>
        <year>2019</year>
        <volume>31</volume>
        <issue>8</issue>
        <fpage>4201</fpage>
        <lpage>4214</lpage>
        <pub-id pub-id-type="doi">10.1007/s00521-018-3346-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bankapur</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Patil</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>An enhanced protein fold recognition for low similarity datasets using convolutional and skip-gram features with deep neural network</article-title>
        <source>IEEE Trans NanoBiosci</source>
        <year>2020</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>42</fpage>
        <lpage>49</lpage>
        <pub-id pub-id-type="doi">10.1109/TNB.2020.3022456</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Elhefnawy</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>DeepFrag-k: a fragment-based deep learning approach for protein fold recognition</article-title>
        <source>BMC Bioinform</source>
        <year>2020</year>
        <volume>21</volume>
        <issue>6</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Stapor, K., Roterman-Konieczna, I., Fabian, P.: Machine learning methods for the protein fold recognition problem. In: Machine learning paradigms, vol. 149. Springer; 2019. pp. 101–27.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>A machine learning information retrieval approach to protein fold recognition</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <issue>12</issue>
        <fpage>1456</fpage>
        <lpage>1463</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl102</pub-id>
        <?supplied-pmid 16547073?>
        <pub-id pub-id-type="pmid">16547073</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Improving protein fold recognition by random forest</article-title>
        <source>BMC Bioinform</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>11</issue>
        <fpage>14</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-15-S11-S14</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Eickholt</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Improving protein fold recognition by deep learning networks</article-title>
        <source>Sci Rep</source>
        <year>2015</year>
        <volume>5</volume>
        <fpage>17573</fpage>
        <pub-id pub-id-type="doi">10.1038/srep17573</pub-id>
        <?supplied-pmid 26634993?>
        <pub-id pub-id-type="pmid">26634993</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>An ensemble approach to protein fold classification by integration of template-based assignment and support vector machine classifier</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>33</volume>
        <issue>6</issue>
        <fpage>863</fpage>
        <lpage>870</lpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Protein fold recognition based on multi-view modeling</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>17</issue>
        <fpage>2982</fpage>
        <lpage>2990</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz040</pub-id>
        <?supplied-pmid 30668845?>
        <pub-id pub-id-type="pmid">30668845</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>An Yong Xu</surname>
            <given-names>JW</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Protein fold recognition based on auto-weighted multi-view graph embedding learning model</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform.</source>
        <year>2020</year>
        <volume>6</volume>
        <fpage>66</fpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>MLDH-Fold: protein fold recognition based on multi-view low-rank modeling</article-title>
        <source>Neurocomputing</source>
        <year>2021</year>
        <volume>421</volume>
        <fpage>127</fpage>
        <lpage>139</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2020.09.028</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Fold-LTR-TCP: protein fold recognition based on triadic closure principle</article-title>
        <source>Brief Bioinform</source>
        <year>2019</year>
        <volume>6</volume>
        <fpage>66</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbx095</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>FoldRec-C2C: protein fold recognition by combining cluster-to-cluster model and protein similarity network</article-title>
        <source>Brief Bioinform</source>
        <year>2020</year>
        <volume>6</volume>
        <fpage>66</fpage>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>ProtFold-DFG: protein fold recognition by combining Directed Fusion Graph and PageRank algorithm</article-title>
        <source>Brief Bioinform</source>
        <year>2020</year>
        <volume>6</volume>
        <fpage>66</fpage>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Adhikari</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>DeepSF: deep convolutional neural network for mapping protein sequences to folds</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>8</issue>
        <fpage>1295</fpage>
        <lpage>1303</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx780</pub-id>
        <?supplied-pmid 29228193?>
        <pub-id pub-id-type="pmid">29228193</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kong</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>W-M</given-names>
          </name>
          <name>
            <surname>Bu</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Improving protein fold recognition by extracting fold-specific features from predicted residue-residue contacts</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>23</issue>
        <fpage>3749</fpage>
        <lpage>3757</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx514</pub-id>
        <?supplied-pmid 28961795?>
        <pub-id pub-id-type="pmid">28961795</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>DeepSVM-fold: protein fold recognition by combining support vector machines and pairwise sequence similarity scores generated by deep learning networks</article-title>
        <source>Brief Bioinform</source>
        <year>2019</year>
        <volume>6</volume>
        <fpage>66</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbx095</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>MotifCNN-fold: protein fold recognition based on fold-specific features extracted by motif-based convolutional neural networks</article-title>
        <source>Brief Bioinform</source>
        <year>2019</year>
        <volume>6</volume>
        <fpage>66</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbx095</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>SelfAT-Fold: protein fold recognition based on residue-based and motif-based self-attention networks</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <year>2020</year>
        <volume>6</volume>
        <fpage>66</fpage>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y-H</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>D-J</given-names>
          </name>
        </person-group>
        <article-title>Why can deep convolutional neural networks improve protein fold recognition? A visual explanation by interpretation</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>6</volume>
        <fpage>66</fpage>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Villegas-Morcillo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gomez</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Morales-Cordovilla</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Sanchez</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Protein fold recognition from sequences using convolutional and recurrent neural networks</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <year>2020</year>
        <volume>6</volume>
        <fpage>66</fpage>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <?supplied-pmid 9377276?>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Chung J, Gulcehre C, Cho K, Bengio Y. Empirical evaluation of gated recurrent neural networks on sequence modeling; 2014. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.3555">arXiv:1412.3555</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vaswani</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shazeer</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Parmar</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Uszkoreit</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Gomez</surname>
            <given-names>AN</given-names>
          </name>
          <name>
            <surname>Kaiser</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Polosukhin</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Attention is all you need</article-title>
        <source>Adv Neural Inf Process Syst</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>5998</fpage>
        <lpage>6008</lpage>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Su J. Deep residual learning for image recognition. In: IEEE conference on computer vision and pattern recognition; 2016. pp. 770–8.</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Wen Y, Zhang K, Li Z, Qiao Y. A discriminative feature learning approach for deep face recognition. In: European conference on computer vision (ECCV); 2016. pp. 499–515.</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Liu W, Wen Y, Yu Z, Yang M. Large-margin softmax loss for convolutional neural networks. In: International conference on machine learning (ICML), vol. 2; 2016. p. 7.</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Liu W, Wen Y, Yu Z, Li M, Raj B, Song L. SphereFace: deep hypersphere embedding for face recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR); 2017. pp. 212–20.</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Wang H, Wang Y, Zhou Z, Ji X, Gong D, Zhou J, Li Z, Liu W. CosFace: large margin cosine loss for deep face recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR); 2018. pp. 5265–74.</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thomson</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>XXIV. On the structure of the atom: an investigation of the stability and periods of oscillation of a number of corpuscles arranged at equal intervals around the circumference of a circle; with application of the results to the theory of atomic structure</article-title>
        <source>Lond Edinb Dublin Philos Mag J Sci</source>
        <year>1904</year>
        <volume>7</volume>
        <issue>39</issue>
        <fpage>237</fpage>
        <lpage>265</lpage>
        <pub-id pub-id-type="doi">10.1080/14786440409463107</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Mettes P, van der Pol E, Snoek CGM. Hyperspherical prototype networks. In: Advances in neural information processing systems; 2019.</mixed-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Steinegger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title>
        <source>Nat Biotechnol</source>
        <year>2017</year>
        <volume>35</volume>
        <issue>11</issue>
        <fpage>1026</fpage>
        <lpage>1028</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3988</pub-id>
        <?supplied-pmid 29035372?>
        <pub-id pub-id-type="pmid">29035372</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <issue>13</issue>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id>
        <?supplied-pmid 16731699?>
        <pub-id pub-id-type="pmid">16731699</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Camacho</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Coulouris</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Avagyan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Papadopoulos</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bealer</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>TL</given-names>
          </name>
        </person-group>
        <article-title>BLAST+: architecture and applications</article-title>
        <source>BMC Bioinform</source>
        <year>2009</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-10-421</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Magnan</surname>
            <given-names>CN</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>SSpro/ACCpro 5: almost perfect prediction of protein secondary structure and relative solvent accessibility using profiles, machine learning and structural similarity</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>18</issue>
        <fpage>2592</fpage>
        <lpage>2597</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu352</pub-id>
        <?supplied-pmid 24860169?>
        <pub-id pub-id-type="pmid">24860169</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: International conference on machine learning; 2015. pp. 448–56.</mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schuster</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>KK</given-names>
          </name>
        </person-group>
        <article-title>Bidirectional recurrent neural networks</article-title>
        <source>IEEE Trans Signal Process</source>
        <year>1997</year>
        <volume>45</volume>
        <issue>11</issue>
        <fpage>2673</fpage>
        <lpage>2681</lpage>
        <pub-id pub-id-type="doi">10.1109/78.650093</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: a method for stochastic optimization; 2014. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1929</fpage>
        <lpage>1958</lpage>
      </element-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <mixed-citation publication-type="other">Raman P, Yang J. Optimization on the surface of the (hyper)-sphere; 2019. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1909.06463">arXiv:1909.06463</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heinzinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Elnaggar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Dallago</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Nechaev</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Matthes</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Modeling aspects of the language of life through transfer-learning protein sequences</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-3220-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rives</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sercu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Goyal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ott</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zitnick</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2021</year>
        <volume>118</volume>
        <issue>15</issue>
        <fpage>66</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Elnaggar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Heinzinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dallago</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Rehawi</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Gibbs</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Feher</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Angerer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Steinegger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bhowmik</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>ProtTrans: towards cracking the language of lifes code through self-supervised deep learning and high performance computing</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>2021</year>
        <volume>66</volume>
        <fpage>1</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2021.3095381</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
