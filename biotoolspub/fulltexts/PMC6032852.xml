<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v3.0 20080202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle3.dtd?>
<?SourceDTD.Version 3.0?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Microsc</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Microsc</journal-id>
    <journal-id journal-id-type="doi">10.1111/(ISSN)1365-2818</journal-id>
    <journal-id journal-id-type="publisher-id">JMI</journal-id>
    <journal-title-group>
      <journal-title>Journal of Microscopy</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0022-2720</issn>
    <issn pub-type="epub">1365-2818</issn>
    <publisher>
      <publisher-name>John Wiley and Sons Inc.</publisher-name>
      <publisher-loc>Hoboken</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6032852</article-id>
    <article-id pub-id-type="doi">10.1111/jmi.12700</article-id>
    <article-id pub-id-type="publisher-id">JMI12700</article-id>
    <article-categories>
      <subj-group subj-group-type="overline">
        <subject>Original Article</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Original Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Automated detection of fluorescent cells in in‐resin fluorescence sections for integrated light and electron microscopy</article-title>
      <alt-title alt-title-type="left-running-head">J. DELPIANO <italic>ET AL</italic>.</alt-title>
      <alt-title alt-title-type="right-running-head">AUTOMATED DETECTION OF FLUORESCENT CELLS FOR ILEM</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="jmi12700-cr-0001" contrib-type="author">
        <name>
          <surname>DELPIANO</surname>
          <given-names>J.</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9794-5649</contrib-id>
        <xref ref-type="aff" rid="jmi12700-aff-0001">
          <sup>1</sup>
        </xref>
        <xref ref-type="author-notes" rid="jmi12700-note-0002">
          <sup>†</sup>
        </xref>
      </contrib>
      <contrib id="jmi12700-cr-0002" contrib-type="author">
        <name>
          <surname>PIZARRO</surname>
          <given-names>L.</given-names>
        </name>
        <xref ref-type="aff" rid="jmi12700-aff-0002">
          <sup>2</sup>
        </xref>
        <xref ref-type="author-notes" rid="jmi12700-note-0002">
          <sup>†</sup>
        </xref>
      </contrib>
      <contrib id="jmi12700-cr-0003" contrib-type="author">
        <name>
          <surname>PEDDIE</surname>
          <given-names>C.J.</given-names>
        </name>
        <xref ref-type="aff" rid="jmi12700-aff-0003">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="jmi12700-cr-0004" contrib-type="author">
        <name>
          <surname>JONES</surname>
          <given-names>M.L.</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0994-5652</contrib-id>
        <xref ref-type="aff" rid="jmi12700-aff-0003">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="jmi12700-cr-0005" contrib-type="author">
        <name>
          <surname>GRIFFIN</surname>
          <given-names>L.D.</given-names>
        </name>
        <xref ref-type="aff" rid="jmi12700-aff-0002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="jmi12700-cr-0006" contrib-type="author" corresp="yes">
        <name>
          <surname>COLLINSON</surname>
          <given-names>L.M.</given-names>
        </name>
        <address>
          <email>Lucy.Collinson@crick.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="jmi12700-aff-0003">
          <sup>3</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="jmi12700-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">School of Engineering and Applied Sciences</named-content>
      <institution>Universidad de los Andes</institution>
      <named-content content-type="city">Santiago</named-content>
      <country country="CL">Chile</country>
    </aff>
    <aff id="jmi12700-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <named-content content-type="organisation-division">Department of Computer Science</named-content>
      <institution>University College London</institution>
      <named-content content-type="country-part">London</named-content>
      <country country="GB">United Kingdom</country>
    </aff>
    <aff id="jmi12700-aff-0003">
      <label>
        <sup>3</sup>
      </label>
      <named-content content-type="organisation-division">Electron Microscopy</named-content>
      <institution>The Francis Crick Institute</institution>
      <named-content content-type="country-part">London</named-content>
      <country country="GB">United Kingdom</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label>Correspondence to: Lucy M. Collinson, 1 Midland Road, London NW1 1AT, UK. Tel: +44 (0) 20 7269 3416; e‐mail: <email>Lucy.Collinson@crick.ac.uk</email></corresp>
      <fn id="jmi12700-note-0002">
        <label>†</label>
        <p>Equally contributing authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>26</day>
      <month>4</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2018</year>
    </pub-date>
    <volume>271</volume>
    <issue>1</issue>
    <issue-id pub-id-type="doi">10.1111/jmi.2018.271.issue-1</issue-id>
    <fpage>109</fpage>
    <lpage>119</lpage>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>9</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>3</month>
        <year>2018</year>
      </date>
    </history>
    <permissions>
      <!--<copyright-statement content-type="issue-copyright"> Journal compilation &#x000a9; 2018 Royal Microscopical Society <copyright-statement>-->
      <copyright-statement content-type="article-copyright">© 2018 The Authors. <italic>Journal of Microscopy</italic> published by JohnWiley &amp; Sons Ltd on behalf of Royal Microscopical Society.</copyright-statement>
      <license license-type="creativeCommonsBy">
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:type="simple" xlink:href="file:JMI-271-109.pdf"/>
    <abstract>
      <title>Summary</title>
      <p>Integrated array tomography combines fluorescence and electron imaging of ultrathin sections in one microscope, and enables accurate high‐resolution correlation of fluorescent proteins to cell organelles and membranes. Large numbers of serial sections can be imaged sequentially to produce aligned volumes from both imaging modalities, thus producing enormous amounts of data that must be handled and processed using novel techniques. Here, we present a scheme for automated detection of fluorescent cells within thin resin sections, which could then be used to drive automated electron image acquisition from target regions via ‘smart tracking’. The aim of this work is to aid in optimization of the data acquisition process through automation, freeing the operator to work on other tasks and speeding up the process, while reducing data rates by only acquiring images from regions of interest. This new method is shown to be robust against noise and able to deal with regions of low fluorescence.</p>
    </abstract>
    <trans-abstract xml:lang="pt" abstract-type="main">
      <title>Lay description</title>
      <p xml:lang="pt">Integrated Light and Electron Microscopy is a recent technique, which uses a light microscope inside an electron microscope to analyse the position of molecules in cells and tissues. The resulting data can give clues about the functions of those molecules in health and disease. However, this technique can produce huge amounts of image data, which has to be stored and analysed by scientists, with high computational and time costs. To streamline the process, we have designed a method whereby computers can automatically detect the position of the molecules of interest, using fluorescent labels, so that electron images are only acquired from these target regions. This method will form part of a workflow to automate the process of data collection, freeing scientists to work on other tasks.</p>
    </trans-abstract>
    <kwd-group kwd-group-type="author-generated">
      <kwd id="jmi12700-kwd-0001">Cell segmentation</kwd>
      <kwd id="jmi12700-kwd-0002">correlative light and electron microscopy</kwd>
      <kwd id="jmi12700-kwd-0003">image processing</kwd>
      <kwd id="jmi12700-kwd-0004">integrated light and electron microscopy</kwd>
      <kwd id="jmi12700-kwd-0005">in‐resin fluorescence</kwd>
      <kwd id="jmi12700-kwd-0006">watershed algorithm</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>Cancer Research UK</funding-source>
        <award-id>FC001999</award-id>
      </award-group>
      <award-group>
        <funding-source>UK Medical Research Council</funding-source>
        <award-id>FC001999</award-id>
      </award-group>
      <award-group>
        <funding-source>Wellcome Trust</funding-source>
        <award-id>FC001999</award-id>
      </award-group>
      <award-group>
        <funding-source>MRC</funding-source>
      </award-group>
      <award-group>
        <funding-source>BBSRC</funding-source>
      </award-group>
      <award-group>
        <funding-source>EPSRC</funding-source>
        <award-id>MR/K01580X/1</award-id>
      </award-group>
      <award-group>
        <funding-source>Fondo de Ayuda a la Investigacion (FAI), Universidad de los Andes</funding-source>
        <award-id>INV‐IN‐2017‐05</award-id>
      </award-group>
      <award-group>
        <funding-source>Francis Crick Institute</funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="3"/>
      <page-count count="11"/>
      <word-count count="5332"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>component-id</meta-name>
        <meta-value>jmi12700</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>July 2018</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_NLMPMC version:version=5.4.3 mode:remove_FC converted:05.07.2018</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="jmi12700-sec-0010">
    <title>Introduction</title>
    <p>Recent technological advances in electron microscopy have allowed the acquisition of extended volume data sets at high resolution (Peddie &amp; Collinson, <xref rid="jmi12700-bib-0017" ref-type="ref">2014</xref>). One of these methods is known as array tomography, whereby an array of ultrathin sections cut through resin‐embedded cells or tissues are imaged sequentially with a scanning electron microscope (SEM) to build up a 3D stack of images through the volume (Micheva &amp; Smith, <xref rid="jmi12700-bib-0015" ref-type="ref">2007</xref>; Wacker &amp; Schroeder, <xref rid="jmi12700-bib-0022" ref-type="ref">2013</xref>; Hayworth <italic>et al</italic>., <xref rid="jmi12700-bib-0009" ref-type="ref">2014</xref>). In parallel, the field of correlative light and electron microscopy has enabled the mapping of functional information onto high‐resolution ultrastructural electron microscopy data, by detecting fluorescent biomarkers in the context of cell structure (Kopek <italic>et al</italic>., <xref rid="jmi12700-bib-0011" ref-type="ref">2012</xref>; Bell <italic>et al</italic>., <xref rid="jmi12700-bib-0002" ref-type="ref">2013</xref>; Löschberger <italic>et al</italic>., <xref rid="jmi12700-bib-0013" ref-type="ref">2014</xref>; Johnson <italic>et al</italic>., <xref rid="jmi12700-bib-0010" ref-type="ref">2015</xref>; Bykov <italic>et al</italic>., <xref rid="jmi12700-bib-0003" ref-type="ref">2016</xref>; Mateos <italic>et al</italic>., <xref rid="jmi12700-bib-0014" ref-type="ref">2016</xref>; Wolff <italic>et al</italic>., <xref rid="jmi12700-bib-0024" ref-type="ref">2016</xref>). Integrated light and electron microscopy (ILEM) (Liv <italic>et al</italic>., <xref rid="jmi12700-bib-0012" ref-type="ref">2013</xref>) combines both microscopes in one device by placing a light microscope inside the vacuum chamber of an electron microscope. It is possible to perform integrated array tomography inside the ILEM using in‐resin fluorescence (IRF) sections, in which both fluorescent and electron signals have been preserved (Peddie <italic>et al</italic>., <xref rid="jmi12700-bib-0016" ref-type="ref">2014</xref>, <xref rid="jmi12700-bib-0018" ref-type="ref">2017</xref>). This technique delivers data from both modalities with almost perfect alignment.</p>
    <p>Both modalities – light and electron microscopy – produce enormous amounts of data that must be handled and processed using novel techniques. ILEM has the potential to reduce the volume of data acquired, by using the fluorescent signal to target regions of interest (ROI) for subsequent electron microscopy imaging, a process designated ‘smart tracking’. This kind of approach would save valuable time for researchers by automating the process of integrated array tomography.</p>
    <p>Software‐assisted array tomography can significantly help a microscopist in the procedures aimed at 3D digitization of a sample. A software package based on a multiscale approach has been developed (Hayworth <italic>et al</italic>., <xref rid="jmi12700-bib-0009" ref-type="ref">2014</xref>) for array tomography using SEM, directing the mapping and imaging of selected regions across a library of sections. This software package, WaferMapper, can manage the process of converting sections from an automatic tape‐collecting ultramicrotome tape, into an image volume. An initial low‐resolution mapping step is suggested, using either an optical image or an electron microscopy montage of the entire wafer. A second step requires more detailed low‐resolution images and is done automatically in the SEM. For use in ILEM, this and further steps should be revised, because the electron beam destroys the fluorescence signal and therefore cannot be used for mapping purposes.</p>
    <p>A recent review covered the free software tools for detection of fluorescence cells in light micrographs (Wiesmann <italic>et al</italic>., <xref rid="jmi12700-bib-0023" ref-type="ref">2015</xref>). The authors studied and tested 12 image analysis tools, including Icy (De Chaumont <italic>et al</italic>., <xref rid="jmi12700-bib-0006" ref-type="ref">2012</xref>), CellProfiler (Carpenter <italic>et al</italic>., <xref rid="jmi12700-bib-0004" ref-type="ref">2006</xref>), ImageJ/Fiji (Schneider <italic>et al</italic>., <xref rid="jmi12700-bib-0020" ref-type="ref">2012</xref>; Schindelin <italic>et al</italic>., <xref rid="jmi12700-bib-0019" ref-type="ref">2012</xref>) and Omero (Allan <italic>et al</italic>., <xref rid="jmi12700-bib-0001" ref-type="ref">2012</xref>). A test user with a life sciences background was asked to segment four fluorescence micrographs with all the tools. For two challenging data sets, they report that the best results were achieved using a seeded watershed approach. However, these tests were not performed using ultrathin IRF sections.</p>
    <p>Techniques for IRF yield fluorescence images with high variability in intensity in each cell. This is partially due to inherent variability in fluorescence expression levels, and partially as a result of ultrathin sectioning which reduces the number of fluorescent molecules available for detection per cell per section. Furthermore, the wide variability in cell shape due to the sectioning process makes the detection problem even harder.</p>
    <p>Here, we present a novel algorithm workflow for smart tracking of fluorescent cells in IRF sections in the ILEM for semiautomated ILEM.</p>
    <sec id="jmi12700-sec-0020">
      <title>Contributions</title>
      <p>
        <list list-type="bullet" id="jmi12700-list-0001">
          <list-item>
            <p>This paper presents a method for detection and localization of fluorescent cells in ultrathin IRF sections, and makes it available as a Matlab program with a graphical user interface.<xref ref-type="fn" rid="jmi12700-note-0001">1</xref> To the best of our knowledge, there are no other specialized methods that target this specific problem.</p>
          </list-item>
          <list-item>
            <p>There is no standard ground truth for this problem. Our results show that the manual segmentations by expert microscopists are highly subjective. Therefore, we show the results of measuring algorithm‐expert and inter‐expert variability.</p>
          </list-item>
          <list-item>
            <p>Our method is simple and uses standard tools that are available in most software packages and can be easily implemented.</p>
          </list-item>
          <list-item>
            <p>The method introduced in this paper is shown to be robust against noise and can deal with regions of low fluorescence intensity.</p>
          </list-item>
        </list>
      </p>
    </sec>
  </sec>
  <sec id="jmi12700-sec-0030">
    <title>Results</title>
    <sec id="jmi12700-sec-0040">
      <title>Image acquisition in the ILEM</title>
      <p>In ILEM, light and electron images are obtained in the same microscope, with the light and electron beams aligned to the same axis (Haring <italic>et al</italic>., <xref rid="jmi12700-bib-0008" ref-type="ref">2017</xref>) (Fig. <xref rid="jmi12700-fig-0001" ref-type="fig">1</xref>A). The resulting images need almost no postacquisition alignment (Fig. <xref rid="jmi12700-fig-0001" ref-type="fig">1</xref>B). Therefore, the fluorescence image can be used to locate cells for electron imaging in an automated correlative pipeline (Fig. <xref rid="jmi12700-fig-0001" ref-type="fig">1</xref>C). Serial ultrathin sections from IRF blocks containing HeLa cells expressing GFP‐C1 were imaged using the widefield fluorescence microscope inside the electron microscope chamber (Peddie <italic>et al</italic>., <xref rid="jmi12700-bib-0016" ref-type="ref">2014</xref>), giving three data sets for subsequent algorithm development (DS1, DS2 and DS3) (Fig. <xref rid="jmi12700-fig-0002" ref-type="fig">2</xref>).</p>
      <fig fig-type="Figure" xml:lang="en" id="jmi12700-fig-0001" orientation="portrait" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Workflow for correlative and integrated light and electron microscopy. (A) Close up of vacuum chamber inside an integrated light and electron microscope. Modified from Liv <italic>et al</italic>. (<xref rid="jmi12700-bib-0012" ref-type="ref">2013</xref>). (B) An example of an overlay of the fluorescence and EM data. Scale bar: 2<mml:math id="nlm-math-1"><mml:mi>μ</mml:mi></mml:math>m. (C) Suggested pipeline for automated light and electron microscopy.</p>
        </caption>
        <graphic id="nlm-graphic-1" xlink:href="JMI-271-109-g001"/>
      </fig>
      <fig fig-type="Figure" xml:lang="en" id="jmi12700-fig-0002" orientation="portrait" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Widefield fluorescence micrographs of fluorescent cells in ultrathin IRF sections. (A–C) Fluorescence micrographs for an ultrathin section in the data sets DS1, DS2, DS3, corresponding to three specimens. The scale bar in (C) is <mml:math id="nlm-math-2"><mml:mrow><mml:mn>20</mml:mn><mml:mspace width="3.33333pt"/><mml:mi>μ</mml:mi></mml:mrow></mml:math>m long. (D–F) Zoomed‐in cells from (A–C). Scale bar in (f): <mml:math id="nlm-math-3"><mml:mrow><mml:mn>5</mml:mn><mml:mspace width="3.33333pt"/><mml:mi>μ</mml:mi></mml:mrow></mml:math>m.</p>
        </caption>
        <graphic id="nlm-graphic-3" xlink:href="JMI-271-109-g002"/>
      </fig>
    </sec>
    <sec id="jmi12700-sec-0050">
      <title>Expert segmentation as ground truth</title>
      <p>To provide ground truth data, five expert microscopists were asked to segment the entirety of all GFP‐positive cells in two data sets (DS1 and DS2) by drawing around the edge of the fluorescent cytoplasmic signal in each image. The level of expertise of each microscopist varied across a wide spectrum; some were familiar with IRF images, whereas others were not. The expert microscopists found an average of 14.1 cells per slice image. The segmentation took 1.4 h per expert on average, with the group delivering more than 100 cell segmentations per hour (Fig. <xref rid="jmi12700-fig-0003" ref-type="fig">3</xref>A). It was immediately obvious that there was large variation in segmentations between experts (Fig. <xref rid="jmi12700-fig-0003" ref-type="fig">3</xref>B), and so comparisons were made of performance within the expert group before using the segmentations as ground truth to judge the performance of automated cell detection algorithms. The Dice index was used for numeric evaluation of the difference between expert segmentations (Fig. <xref rid="jmi12700-fig-0003" ref-type="fig">3</xref>C). The goal of this exercise was not to assess the segmentations of each expert, but to study the prior knowledge involved and the subjective component of their work. The mean interexpert DICE was only 67.7% on average (Fig. <xref rid="jmi12700-fig-0003" ref-type="fig">3</xref>D and Table <xref rid="jmi12700-tbl-0001" ref-type="table">1</xref>), demonstrating that the problem is difficult to tackle even for experts.</p>
      <fig fig-type="Figure" xml:lang="en" id="jmi12700-fig-0003" orientation="portrait" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Ground truth segmentation of cell images. (A) Free‐hand ROI segmentation of HeLa cells by Expert 1. (B) Bounding boxes for the cell segmentations by the five experts were superimposed, giving one colour to each of the experts, showing the variability and subjective nature of their work. (C) The Dice index (<italic>D</italic>, see definition in the main text) between two rectangular ROIs is a measure of how good a detection is, considering both a correct localization and size. (D) The Dice index allows for a quantification of how subjective two expert segmentations are. Here, if Expert 1 is considered as the ground truth, the average Dice between segmentations by Expert 1 and Expert 2 is <mml:math id="nlm-math-4"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi mathvariant="italic">GT</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>45</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math>.</p>
        </caption>
        <graphic id="nlm-graphic-5" xlink:href="JMI-271-109-g003"/>
      </fig>
      <table-wrap id="jmi12700-tbl-0001" xml:lang="en" orientation="portrait" position="float">
        <label>Table 1</label>
        <caption>
          <p>Average Dice when comparing segmentations by experts (DS2, image 1). Results under 60% are shown in bold text</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <col align="char" char="." span="1"/>
          <thead>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">Expert 1</th>
              <th align="center" rowspan="1" colspan="1">Expert 2</th>
              <th align="center" rowspan="1" colspan="1">Expert 3</th>
              <th align="center" rowspan="1" colspan="1">Expert 4</th>
              <th align="center" rowspan="1" colspan="1">Expert 5</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">Expert 1</td>
              <td align="char" rowspan="1" colspan="1">100%</td>
              <td align="char" rowspan="1" colspan="1">
                <bold>45%</bold>
              </td>
              <td align="char" rowspan="1" colspan="1">74%</td>
              <td align="char" rowspan="1" colspan="1">
                <bold>55%</bold>
              </td>
              <td align="char" rowspan="1" colspan="1">80%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Expert 2</td>
              <td align="char" rowspan="1" colspan="1">83%</td>
              <td align="char" rowspan="1" colspan="1">100%</td>
              <td align="char" rowspan="1" colspan="1">71%</td>
              <td align="char" rowspan="1" colspan="1">70%</td>
              <td align="char" rowspan="1" colspan="1">85%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Expert 3</td>
              <td align="char" rowspan="1" colspan="1">83%</td>
              <td align="char" rowspan="1" colspan="1">
                <bold>45%</bold>
              </td>
              <td align="char" rowspan="1" colspan="1">100%</td>
              <td align="char" rowspan="1" colspan="1">
                <bold>54%</bold>
              </td>
              <td align="char" rowspan="1" colspan="1">70%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Expert 4</td>
              <td align="char" rowspan="1" colspan="1">85%</td>
              <td align="char" rowspan="1" colspan="1">
                <bold>56%</bold>
              </td>
              <td align="char" rowspan="1" colspan="1">71%</td>
              <td align="char" rowspan="1" colspan="1">100%</td>
              <td align="char" rowspan="1" colspan="1">86%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Expert 5</td>
              <td align="char" rowspan="1" colspan="1">79%</td>
              <td align="char" rowspan="1" colspan="1">
                <bold>44%</bold>
              </td>
              <td align="char" rowspan="1" colspan="1">63%</td>
              <td align="char" rowspan="1" colspan="1">
                <bold>55%</bold>
              </td>
              <td align="char" rowspan="1" colspan="1">100%</td>
            </tr>
          </tbody>
        </table>
        <permissions>
          <copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder>
        </permissions>
      </table-wrap>
    </sec>
    <sec id="jmi12700-sec-0060">
      <title>Semi‐automated segmentation using Ilastik</title>
      <p>Ilastik (Sommer <italic>et al</italic>., <xref rid="jmi12700-bib-0021" ref-type="ref">2011</xref>) was used to segment the fluorescent cells, to test the performance of current state‐of‐the‐art software for semi‐automated image segmentation (Fig. <xref rid="jmi12700-fig-0004" ref-type="fig">4</xref>). For this experiment, the manual segmentation by one expert (see example in Fig. <xref rid="jmi12700-fig-0004" ref-type="fig">4</xref>B) was chosen as training data for pixel classification. Each segmented pixel was considered an example of the class ‘cell’, and all the other pixels in the slice were considered examples of the class ‘not a cell’. Although Ilastik was able to segment the cells in the images, there were two main classes of errors, where adjacent cells were merged and where some cell regions were marked as ‘not a cell’ (Fig. <xref rid="jmi12700-fig-0004" ref-type="fig">4</xref>C). Though these errors may be corrected by further refinement, the interaction required in seeding the segmentation and in correcting errors rules out the use of this semi‐automated detection method for automated on‐the‐fly detection during imaging as part of a smart‐tracking correlative pipeline. We therefore moved to develop an algorithm that would automatically detect fluorescent cells in IRF images without additional user interaction.</p>
      <fig fig-type="Figure" xml:lang="en" id="jmi12700-fig-0004" orientation="portrait" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Semiautomated segmentation of cells using Ilastik. All the pixels from the manual segmentation of one expert were used as training examples for Ilastik random forests. (A) Original image for DS1 ‐ slice 3. (B) Segmentation by Expert 1. (C) Ilastik result. White arrows show some examples of two types of errors: some groups of two or three close cells were merged as one and some cell parts were marked as ‘not a cell’ (green).</p>
        </caption>
        <graphic id="nlm-graphic-7" xlink:href="JMI-271-109-g004"/>
      </fig>
    </sec>
    <sec id="jmi12700-sec-0070">
      <title>Design and performance of new automated segmentation algorithm</title>
      <p>A workflow was designed for automated segmentation of fluorescent cells in IRF images (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>), consisting of pre‐processing steps to remove noise and artefacts, a watershed‐based algorithm to detect and segment the cells, and post‐processing steps to clean up the resulting segmentations (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>A). The raw image (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>B) was pre‐processed to remove noise (an optional step), and then uneven illumination was removed by background correction and enhancement of contrast (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>C). A linear filter was used as a feature to highlight the borders of the ring‐like objects that result from the selected sample and staining (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>D). Markers for the presence of cells were obtained as the brightest pixels in the feature image and saved (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>E). Two more results were fed as inputs for the main watershed step: the gradient magnitude of the image feature (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>F) and candidate ‘non cell’ pixels (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>G). The initial segmentation that results (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>H) is converted into its bounding boxes (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>I) and detections that are too large or small are filtered, whereas merging detections that correspond to the same cell (Fig. <xref rid="jmi12700-fig-0005" ref-type="fig">5</xref>J).</p>
      <fig fig-type="Figure" xml:lang="en" id="jmi12700-fig-0005" orientation="portrait" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Development of automated cell segmentation workflow, based on the watershed algorithm, and partial results of algorithm steps. (A) Steps of the algorithm. (B) Raw image. (C) Preprocessed image. (D) Laplacian of Gaussian (LoG) image feature. (E) Cell pixel markers. (F) Gradient of the image feature. (G) Background pixel markers. (H) The watershed transform of modified gradient, superimposed transparently on the image. (I) Bounding boxes for watershed labels. Arrows show two boxes corresponding to over segmentation in one cell. (J) Bounding boxes for watershed labels, after merging and size filtering. The arrow shows a box coming from merging of two labels.</p>
        </caption>
        <graphic id="nlm-graphic-9" xlink:href="JMI-271-109-g005"/>
      </fig>
      <p>The algorithm was applied to detect cells in all three data sets (Fig. <xref rid="jmi12700-fig-0006" ref-type="fig">6</xref>). The Dice index was used to compare the output of the automated segmentation algorithm (cell detections) to manual segmentations by the expert microscopists (ground truth). Figures <xref rid="jmi12700-fig-0006" ref-type="fig">6</xref>(A, D, G and J) show the results for data set DS1, with Dice calculated against one of the expert microscopists. The Dice index ranges from 39% to 75%. Figures <xref rid="jmi12700-fig-0006" ref-type="fig">6</xref>(B, E, H and K) show the same results for DS2 where the Dice index varies from 46% to 69%. Figures <xref rid="jmi12700-fig-0006" ref-type="fig">6</xref>(C, F, I and L) show the output of our method for DS3, where no ground truth is available. Therefore, no Dice values can be obtained. Whereas the mean interexpert DICE was 67.7%, the mean algorithm‐expert DICE was 68.08% (DS2, image 1), indicating that the algorithm performs at least as well as expert microscopists (Table <xref rid="jmi12700-tbl-0002" ref-type="table">2</xref>). The cell detections by our watershed‐based algorithm achieved an average Dice index of 58%. The global average recall of 67% means that our algorithm can find 67% of the objects in the ground truth, with <mml:math id="nlm-math-5"><mml:mrow><mml:mi>D</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>50</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math>. A global average precision of 63% is interpreted as 63% of our detections being correct and with <mml:math id="nlm-math-6"><mml:mrow><mml:mi>D</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>50</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math> (Table <xref rid="jmi12700-tbl-0003" ref-type="table">3</xref>).</p>
      <fig fig-type="Figure" xml:lang="en" id="jmi12700-fig-0006" orientation="portrait" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Results of detection for part of the HeLa cell data sets. Detections in solid magenta rectangles and ground truth in dashed green boxes. Left (A, D, G, J): Some slices of data set DS1. Centre (B, E, H, K): Slices of DS2. Right (C, F, I, L): DS3, no ground truth available.</p>
        </caption>
        <graphic id="nlm-graphic-11" xlink:href="JMI-271-109-g006"/>
      </fig>
      <table-wrap id="jmi12700-tbl-0002" xml:lang="en" orientation="portrait" position="float">
        <label>Table 2</label>
        <caption>
          <p>Average Dice for sample segmentation results. Results for data set DS2, image 1, are shown. See comments in main text</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <col align="center" span="1"/>
          <thead>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">Expert 1</th>
              <th align="center" rowspan="1" colspan="1">Expert 2</th>
              <th align="center" rowspan="1" colspan="1">Expert 3</th>
              <th align="center" rowspan="1" colspan="1">Expert 4</th>
              <th align="center" rowspan="1" colspan="1">Expert 5</th>
              <th align="center" rowspan="1" colspan="1">Avg.</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">Image 1</td>
              <td align="center" rowspan="1" colspan="1">64%</td>
              <td align="center" rowspan="1" colspan="1">73%</td>
              <td align="center" rowspan="1" colspan="1">64%</td>
              <td align="center" rowspan="1" colspan="1">77%</td>
              <td align="center" rowspan="1" colspan="1">62%</td>
              <td align="center" rowspan="1" colspan="1">68.08%</td>
            </tr>
          </tbody>
        </table>
        <permissions>
          <copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder>
        </permissions>
      </table-wrap>
      <table-wrap id="jmi12700-tbl-0003" xml:lang="en" orientation="portrait" position="float">
        <label>Table 3</label>
        <caption>
          <p>Average performance for our segmentation results</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="center" span="1"/>
          <thead>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" rowspan="1" colspan="1"/>
              <th align="center" rowspan="1" colspan="1">Global Avg.</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">Dice</td>
              <td align="center" rowspan="1" colspan="1">58%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Recall</td>
              <td align="center" rowspan="1" colspan="1">67%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Precision</td>
              <td align="center" rowspan="1" colspan="1">63%</td>
            </tr>
          </tbody>
        </table>
        <permissions>
          <copyright-holder>John Wiley &amp; Sons, Ltd.</copyright-holder>
        </permissions>
      </table-wrap>
    </sec>
    <sec id="jmi12700-sec-0080">
      <title>Development of a synthetic data set to model other fluorophore distributions</title>
      <p>As larger amounts of ILEM data become available, it will become feasible to start applying recent machine learning techniques, such as convolutional neural networks and deep learning. However, a large number of parameters implies a need for a large number of training examples. In the absence of real training data, a data set synthesized from a small amount of real data was developed, similar to the ‘flying chairs’ data set used for learning of optical flow (Dosovitskiy <italic>et al</italic>., <xref rid="jmi12700-bib-0007" ref-type="ref">2015</xref>). The synthetic cell data set developed for this purpose was a simplistic representation of the cell population. A representative slice from a simulated fluorescence data set modelling a cytoplasmic expression pattern with a slice thickness of 100 nm is shown (Figs. <xref rid="jmi12700-fig-0007" ref-type="fig">7</xref>A–C). Based on our prior knowledge of the ILEM images, slices were corrupted with Gaussian noise with standard deviations <mml:math id="nlm-math-7"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math> (Fig. <xref rid="jmi12700-fig-0007" ref-type="fig">7</xref>A), <mml:math id="nlm-math-8"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math> (Fig. <xref rid="jmi12700-fig-0007" ref-type="fig">7</xref>B) and <mml:math id="nlm-math-9"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math> (Fig. <xref rid="jmi12700-fig-0007" ref-type="fig">7</xref>C). We then applied our watershed‐based method to these images (Figs. <xref rid="jmi12700-fig-0007" ref-type="fig">7</xref>D–F) and compared the ground truth (green) and automated segmentation results (magenta) for the three images (Figs. <xref rid="jmi12700-fig-0007" ref-type="fig">7</xref>G–I). Recall (<italic>r</italic>) for the three noise levels was between 86% and 88%. The precision (<italic>p</italic>) range was 94–98%. This is a controlled experiment and the measures here are much better than for the real data, as expected for a simulated data set which is a simple representation of the real problem. However, the simulated data set will expedite further algorithm development against different fluorophore patterns (for example, nuclear or punctate fluorophore localizations), noise levels, cell shape and density.</p>
      <fig fig-type="Figure" xml:lang="en" id="jmi12700-fig-0007" orientation="portrait" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Watershed‐based detection steps and dependence on noise level for simulated data. (A–C) Simulated fluorescence images with Gaussian noise and <mml:math id="nlm-math-10"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math>, and magnitude of bell‐shaped background <mml:math id="nlm-math-11"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:math>. A rolling ball filter was used to level the background. (B) Same configuration, but Gaussian noise with <mml:math id="nlm-math-12"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:math>. (C) <mml:math id="nlm-math-13"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math>. (D–F) Watershed‐based segmentations, from the simulated images in (A–C). (G–I) Watershed‐based detections (solid magenta) and ground truth (dashed green).</p>
        </caption>
        <graphic id="nlm-graphic-13" xlink:href="JMI-271-109-g007"/>
      </fig>
    </sec>
  </sec>
  <sec id="jmi12700-sec-0090">
    <title>Discussion</title>
    <p>Our work shows that it is possible to automatically locate ROI in ultrathin IRF sections, using the fluorescence signal to identify cells for subsequent electron imaging.</p>
    <p>Though the current state‐of‐the‐art in shallow learning platforms, Ilastik, was able to identify fluorescent cells, it required ground truth training data from an expert microscopist as well as postcorrection to separate joined cells and to reclassify small dim regions of fluorescence as cells, making it unsuitable for on‐the‐fly detection of cells in an automated correlative workflow.</p>
    <p>For detection and definition of a region of interest for electron imaging in ILEM, it is better to err on the side of caution. Acquiring more data than needed is preferred to missing fluorescent cells, since subsequent exposure to the electron beam destroys the fluorescence signal in all cell in the field of view. This leads to some criteria to weight detection errors. A large region of interest that contains an actual cell should be preferred over a region of interest that misses part of a cell. Likewise, a false positive detection where there is no cell should be preferred over missing the detection of a cell. Regarding recall (<italic>r</italic>) and precision (<italic>p</italic>), we should aim at having <italic>r</italic> as close to 1 as possible, as a top priority. As a second priority, <italic>p</italic> should be as close to 1 as possible. Even operating with a conservative ‘if in doubt, image it’ policy, this could lead to a significant reduction in the total area to be imaged, with concomitant savings in data storage and processing requirements.</p>
    <p>The cell detections by our watershed‐based algorithm achieved an average Dice index of 58%, which was an improvement over the performance of some expert microscopists. Indeed, the interexpert Dice scores show that identification of fluorescent cells in ultrathin IRF sections is a difficult problem, even for humans. Though it is natural to expect some differences in ground truth segmentation results between experts, our results showed that agreement was surprisingly poor. The Dice scores revealed two groups of experts, which in postanalysis proved to be those with biology and physics backgrounds. The scientists with a biology background had a deeper understanding of the type of cells in the sample and were able to find cells that look very dim in the data, or that were smaller than expected due to being a glancing section through the edge of the cell.</p>
    <p>For individual images in the data sets, interexpert Dice scores reached an average of 67.7%, whereas mean algorithm‐expert Dice score reached 68.08%, indicating that the algorithm can perform as well as experts on some images. However, algorithm performance could undoubtedly be improved. As we gather IRF images from a greater variety of cell types and fluorescent labels, it will be possible to incorporate additional fluorescence patterns into the algorithm to increase robustness for multiple biological applications. In addition, 3D information gathered from sequential serial sections will allow us to identify the central slice for each cell, which would usually be the largest area for a cell with a roughly spherical shape, and use this position to track the cell outwards through the adjacent sections to capture and increase confidence in smaller, dimmer cell edges. Further development and implementation of the IRF cell simulation model will expedite this process in the absence of real data.</p>
    <p>Using the current algorithm, and future iterations that may include machine learning approaches, we foresee a workflow for automated integrated array tomography, whereby automated detection of fluorescent cells drives automated acquisition of electron images from cells of interest, which will speed up discovery research while minimizing the cost and compute resource required for 3D correlative microscopy by focusing data acquisition to specific ROI.</p>
  </sec>
  <sec id="jmi12700-sec-0100">
    <title>Materials and methods</title>
    <sec id="jmi12700-sec-0110">
      <title>Sample preparation</title>
      <p>HeLa cells expressing a cytoplasmic GFP‐C1 fluorescent tag were embedded as described previously (Peddie <italic>et al</italic>., <xref rid="jmi12700-bib-0016" ref-type="ref">2014</xref>). Sections of 200‐nm thickness were cut using an ultramicrotome and collected on indium‐tin oxide‐coated glass cover slips, which are conductive and optically transparent.</p>
    </sec>
    <sec id="jmi12700-sec-0120">
      <title>Data acquisition</title>
      <p>ILSEM was performed on the same day as sectioning using a SECOM light microscope platform (Delmic B.V., Delft) with Nikon Plan Apo 40x/0.95 objective, mounted on a Quanta 250 FEG SEM (FEI Company, Eindhoven). GFP fluorescence was stimulated by excitation with a 488‐nm laser light source and multiband filters (Di01‐R405/488/594 dichroic, FF01‐446/532/646‐25 emission; Semrock, Rochester, NY, USA). Individual fluorescence images at an XY pixel resolution of 178 nm were collected from matched areas of five serial sections using an EMCCD camera (iXon 897 Ultra; Andor Technology, Belfast, U.K.). The exposure time was 2 s, and power density was 0.5 W/cm<sup>2</sup> at the sample level. For fluorescence imaging, the chamber was maintained at a partial pressure of 200 Pa, created using water vapour. To collect matching SEM images of specific cells of interest at an XY pixel resolution of 16.5 nm, the system was pumped to high vacuum (<mml:math id="nlm-math-14"><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math> Pa). The vCD backscatter detector (FEI Company, Eindhoven) was used at a working distance of 5.8 mm, and inverted contrast images were acquired (2.5 keV, spot size 3.5, 30 μm aperture and pixel dwell time of 60 μs for a 1536 * 1103 pixel image frame).</p>
    </sec>
    <sec id="jmi12700-sec-0130">
      <title>Manual segmentation</title>
      <p>Manual segmentation was performed using a touchscreen interface (Wacom) in Fiji (Schindelin <italic>et al</italic>., <xref rid="jmi12700-bib-0019" ref-type="ref">2012</xref>; Schneider <italic>et al</italic>., <xref rid="jmi12700-bib-0020" ref-type="ref">2012</xref>) with the freehand selection tool, and exported as ROIs from the ROI manager. Five microscopists, with varied expertise in light and electron microscopy, segmented five images each from two data sets (DS1 and DS2).</p>
    </sec>
    <sec id="jmi12700-sec-0140">
      <title>Semiautomated segmentation using Ilastik</title>
      <p>For pixel classification in Ilastik, six features were selected: Gaussian smoothing, Laplacian of Gaussian, Gaussian gradient magnitude, difference of Gaussians, structure tensor eigenvalues and Hessian of Gaussian eigenvalues. Random forests were trained for those features with ground truth data from one of the expert microscopists.</p>
    </sec>
    <sec id="jmi12700-sec-0150">
      <title>Development of automated workflow</title>
      <p>Our workflow for automated detection of cells was implemented in Matlab and is available for download <ext-link ext-link-type="uri" xlink:href="https://github.com/jdelpiano/irfCellSegmentation">https://github.com/jdelpiano/irfCellSegmentation</ext-link>. A graphical user interface was developed for easy access of the workflow to users with no experience in programming. The workflow consisted of the following steps. First, an optional denoising step may be applied to images if required. Due to its excellent results in tests with simulated fluorescence, the Matlab implementation of the block‐matching and 3D filtering algorithm was used (Dabov <italic>et al</italic>., <xref rid="jmi12700-bib-0005" ref-type="ref">2007</xref>), assuming a good estimation of the noise level sigma. Widefield fluorescence microscopy images of IRF sections tend to have a Gaussian‐shaped background intensity, which was reliably corrected with the rolling ball filter. Due to the sample preparation procedures and fluorescent protein expression levels, some cells were very dim in the fluorescence image, and therefore contrast limited adaptive histogram equalization (CLAHE) (Zuiderveld, <xref rid="jmi12700-bib-0025" ref-type="ref">1994</xref>) was applied to increase the intensity of the dimmer cells. Three scalar fields were fed to the watershed transform as preparation for segmentation: (1) A threshold of 92% was used in the cumulative density function to obtain the 8% brightest pixels in the feature image and save them as candidate cell pixels, (2) the gradient magnitude for the feature image, which was the result of applying a Laplacian of Gaussian linear filter with parameter <mml:math id="nlm-math-15"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math> to the preprocessed image and (3) markers for background or absence of cells. The markers for background were obtained from the image complement of the cell markers. The distance transform was applied to that complement and then the watershed transform was used to find the skeleton of the structures observed in the distance transform.</p>
    </sec>
    <sec id="jmi12700-sec-0160">
      <title>Dice index for inter‐expert and algorithm‐expert comparisons</title>
      <p>To quantify comparison of cell segmentations, we defined the Dice index (<italic>D</italic>) between two segmentations as
<disp-formula id="jmi12700-disp-0001"><label>(1)</label><mml:math id="nlm-math-16"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>A</mml:mi><mml:mo>∩</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>A</italic> and <italic>B</italic> were the areas of the corresponding segmentations, and <mml:math id="nlm-math-17"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∩</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math> is the area of the intersection of both detections.</p>
      <p>Given a set of cell detections and a set of ground truth segmentations, a computer does not know which detected cell corresponds to which cell in the ground truth. Therefore, the calculation of the Dice index between them results in a matrix which may recall a confusion matrix. We need to define the Dice index for each element in that matrix as
<disp-formula id="jmi12700-disp-0002"><label>(2)</label><mml:math id="nlm-math-18"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <mml:math id="nlm-math-19"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math> and <mml:math id="nlm-math-20"><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math> are the areas of the <italic>i</italic>th detection and the <italic>j</italic>th ground truth detection, and <mml:math id="nlm-math-21"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math> is the area of the intersection of both detections. A Dice index matrix can be defined by <mml:math id="nlm-math-22"><mml:msub><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="italic">det</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="italic">GT</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math>, where <mml:math id="nlm-math-23"><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="italic">det</mml:mi></mml:msub></mml:math> is the number of detections and <mml:math id="nlm-math-24"><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="italic">GT</mml:mi></mml:msub></mml:math>, the number of objects in the ground truth set.</p>
      <p>There are two intuitive choices for an average Dice, given by Eqs <xref rid="jmi12700-disp-0003" ref-type="disp-formula">(3)</xref> and <xref rid="jmi12700-disp-0004" ref-type="disp-formula">(4)</xref>, one with respect to the detections that have been found <mml:math id="nlm-math-25"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi mathvariant="italic">det</mml:mi></mml:msub></mml:math> and one with respect to the ground truth <mml:math id="nlm-math-26"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi mathvariant="italic">GT</mml:mi></mml:msub></mml:math>. These choices are not equivalent nor symmetric.
<disp-formula id="jmi12700-disp-0003"><label>(3)</label><mml:math id="nlm-math-27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi mathvariant="italic">det</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="italic">det</mml:mi></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="italic">det</mml:mi></mml:msub></mml:munderover><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="jmi12700-disp-0004"><label>(4)</label><mml:math id="nlm-math-28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi mathvariant="italic">GT</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mstyle scriptlevel="0" displaystyle="true"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="italic">GT</mml:mi></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="italic">GT</mml:mi></mml:msub></mml:munderover><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi mathvariant="italic">ij</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p>
      <p>As we aim at defining the field of view for obtaining high‐resolution microscopy data, Dice indexes will be determined over the rectangular bounding boxes for each ground truth or estimated segmentation.</p>
      <p>A cell segmentation will be considered correct as a detection if the Dice index <italic>D</italic> between itself and a ground truth cell detection is greater than a threshold, which was defined for the experiments shown here as 0.5. To analyse the performance of cell detection, we define two more quantities: recall <italic>r</italic> and precision <italic>p</italic>, as
<disp-formula id="jmi12700-disp-0005"><label>(5)</label><mml:math id="nlm-math-29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TP</mml:mi><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
<disp-formula id="jmi12700-disp-0006"><label>(6)</label><mml:math id="nlm-math-30"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TP</mml:mi><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>TP</italic> is the number of true positives, which are the correct detections of cells; <italic>FN</italic>, the number of false negatives, corresponds to the ground truth objects that were not detected and <italic>FP</italic> is the number of false positives. <mml:math id="nlm-math-31"><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:math> is the number of objects in the ground truth data set.</p>
    </sec>
    <sec id="jmi12700-sec-0170">
      <title>Development of a synthetic data set to model other fluorophore distributions</title>
      <p>In order to test the detection of various fluorescent patterns, simulated cell data were generated with a custom MATLAB program that randomly placed cells within the volume. Each spherical cell had its size and brightness drawn from a Gaussian distribution, and a cytoplasmic staining was represented by a uniform intensity inside the sphere apart from an empty spherical region denoting the nucleus. On top of this, Gaussian noise was added at <mml:math id="nlm-math-32"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn><mml:mo>,</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math> (Figs. <xref rid="jmi12700-fig-0007" ref-type="fig">7</xref>A–C). The resulting volume was sliced into dimensions typical for this type of experiment.</p>
      <p>To obtain a ground truth, a bounding box was calculated for each simulated object, before adding Gaussian noise with standard deviation <mml:math id="nlm-math-33"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn><mml:mo>,</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math>. Figures <xref rid="jmi12700-fig-0007" ref-type="fig">7</xref>(A–C) show the result of preprocessing these images.</p>
    </sec>
  </sec>
</body>
<back>
  <ack id="jmi12700-sec-0180">
    <title>Acknowledgements</title>
    <p>This work was supported by the Francis Crick Institute which receives its core funding from Cancer Research UK (FC001999), the UK Medical Research Council (FC001999) and the Wellcome Trust (FC001999); by the MRC, BBSRC and EPSRC under grant award MR/K01580X/1 to LMC; and by Fondo de Ayuda a la Investigacion (FAI), Universidad de los Andes, under grant INV‐IN‐2017‐05 to J.D.</p>
  </ack>
  <fn-group>
    <fn id="jmi12700-note-0001">
      <label>1</label>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/jdelpiano/irfCellSegmentation">https://github.com/jdelpiano/irfCellSegmentation</ext-link>.</p>
    </fn>
  </fn-group>
  <ref-list id="jmi12700-bibl-0001">
    <title>References</title>
    <ref id="jmi12700-bib-0001">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0001"><string-name><surname>Allan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Burel</surname>, <given-names>J.‐M.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>J.</given-names></string-name><italic>et al</italic> (<year>2012</year>) <article-title>Omero: flexible, model‐driven data management for experimental biology</article-title>. <source xml:lang="en">Nat. Methods</source>
<volume>9</volume>, <fpage>245</fpage>–<lpage>253</lpage>.<pub-id pub-id-type="pmid">22373911</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0002">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0002"><string-name><surname>Bell</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Mitchell</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Paultre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Posch</surname>, <given-names>M.</given-names></string-name> &amp; <string-name><surname>Oparka</surname>, <given-names>K.</given-names></string-name> (<year>2013</year>) <article-title>Correlative imaging of fluorescent proteins in resin‐embedded plant material1</article-title>. <source xml:lang="en">Plant Physiol.</source>
<volume>161</volume>, <fpage>1595</fpage>–<lpage>1603</lpage>.<pub-id pub-id-type="pmid">23457228</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0003">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0003"><string-name><surname>Bykov</surname>, <given-names>Y.S.</given-names></string-name>, <string-name><surname>Cortese</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Briggs</surname>, <given-names>J.A.</given-names></string-name> &amp; <string-name><surname>Bartenschlager</surname>, <given-names>R.</given-names></string-name> (<year>2016</year>) <article-title>Correlative light and electron microscopy methods for the study of viruscell interactions</article-title>. <source xml:lang="en">FEBS Lett</source>. <volume>590</volume>, <fpage>1877</fpage>–<lpage>1895</lpage>.<pub-id pub-id-type="pmid">27008928</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0004">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0004"><string-name><surname>Carpenter</surname>, <given-names>A.E.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>T.R.</given-names></string-name>, <string-name><surname>Lamprecht</surname>, <given-names>M.R.</given-names></string-name><italic>et al</italic> (<year>2006</year>) <article-title>Cellprofiler: image analysis software for identifying and quantifying cell phenotypes</article-title>. <source xml:lang="en">Genome Biol.</source>
<volume>7</volume>, <fpage>R100</fpage>.<pub-id pub-id-type="pmid">17076895</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0005">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0005"><string-name><surname>Dabov</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Foi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Katkovnik</surname>, <given-names>V.</given-names></string-name> &amp; <string-name><surname>Egiazarian</surname>, <given-names>K.</given-names></string-name> (<year>2007</year>) <article-title>Image denoising by sparse 3‐D transform‐domain collaborative filtering</article-title>. <source xml:lang="en">IEEE Trans. Image Process.</source>
<volume>16</volume>, <fpage>2080</fpage>–<lpage>2095</lpage>.<pub-id pub-id-type="pmid">17688213</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0006">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0006"><string-name><surname>De Chaumont</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Dallongeville</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Chenouard</surname>, <given-names>N.</given-names></string-name><italic>et al</italic> (<year>2012</year>) <article-title>Icy: an open bioimage informatics platform for extended reproducible research</article-title>. <source xml:lang="en">Nat. Methods</source>
<volume>9</volume>, <fpage>690</fpage>–<lpage>696</lpage>.<pub-id pub-id-type="pmid">22743774</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0007">
      <mixed-citation publication-type="book" id="jmi12700-cit-0007"><string-name><surname>Dosovitskiy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fischer</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ilg</surname>, <given-names>E.</given-names></string-name><italic>et al</italic> (<year>2015</year>) <chapter-title>Flownet: learning optical flow with convolutional networks</chapter-title> In <source xml:lang="en">Proceedings of the IEEE International Conference on Computer Vision</source>, pp. <fpage>2758</fpage>–<lpage>2766</lpage>.</mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0008">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0008"><string-name><surname>Haring</surname>, <given-names>M.T.</given-names></string-name>, <string-name><surname>Liv</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Zonnevylle</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Narvaez</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Voortman</surname>, <given-names>L.M.</given-names></string-name>, <string-name><surname>Kruit</surname>, <given-names>P.</given-names></string-name> &amp; <string-name><surname>Hoogenboom</surname>, <given-names>J.P.</given-names></string-name> (<year>2017</year>) <article-title>Automated sub‐5 nm image registration in integrated correlative fluorescence and electron microscopy using cathodoluminescence pointers</article-title>. <source xml:lang="en">Sci. Rep.</source>
<volume>7</volume>, 43621.</mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0009">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0009"><string-name><surname>Hayworth</surname>, <given-names>K.J.</given-names></string-name>, <string-name><surname>Morgan</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Schalek</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Berger</surname>, <given-names>D.R.</given-names></string-name>, <string-name><surname>Hildebrand</surname>, <given-names>D.G.</given-names></string-name> &amp; <string-name><surname>Lichtman</surname>, <given-names>J.W.</given-names></string-name> (<year>2014</year>) <article-title>Imaging ATUM ultrathin section libraries with WaferMapper: a multi‐scale approach to EM reconstruction of neural circuits</article-title>. <source xml:lang="en">Front. Neural Circuits</source>
<volume>8</volume>, <fpage>68</fpage>.<pub-id pub-id-type="pmid">25018701</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0010">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0010"><string-name><surname>Johnson</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Seiradake</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>E.Y.</given-names></string-name>, <string-name><surname>Davis</surname>, <given-names>I.</given-names></string-name>, Grü<string-name><surname>newald</surname>, <given-names>K.</given-names></string-name> &amp; <string-name><surname>Kaufmann</surname>, <given-names>R.</given-names></string-name> (<year>2015</year>) <article-title>Correlative in‐resin super‐resolution and electron microscopy using standard fluorescent proteins</article-title>. <source xml:lang="en">Sci. Rep.</source>
<volume>5</volume>, <fpage>9583</fpage>.<pub-id pub-id-type="pmid">25823571</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0011">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0011"><string-name><surname>Kopek</surname>, <given-names>B.G.</given-names></string-name>, <string-name><surname>Shtengel</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>C.S.</given-names></string-name>, <string-name><surname>Clayton</surname>, <given-names>D.A.</given-names></string-name> &amp; <string-name><surname>Hess</surname>, <given-names>H.F.</given-names></string-name> (<year>2012</year>) <article-title>Correlative 3D superresolution fluorescence and electron microscopy reveal the relationship of mitochondrial nucleoids to membranes</article-title>. <source xml:lang="en">Proc. Natl. Acad. Sci.</source>
<volume>109</volume>, <fpage>6136</fpage>–<lpage>6141</lpage>.<pub-id pub-id-type="pmid">22474357</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0012">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0012"><string-name><surname>Liv</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Zonnevylle</surname>, <given-names>A.C.</given-names></string-name>, <string-name><surname>Narvaez</surname>, <given-names>A.C.</given-names></string-name><italic>et al</italic> (<year>2013</year>) <article-title>Simultaneous correlative scanning electron and high‐NA fluorescence microscopy</article-title>. <source xml:lang="en">PLoS One</source>
<volume>8</volume>, <fpage>e55707</fpage>.<pub-id pub-id-type="pmid">23409024</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0013">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0013">Lö<string-name><surname>schberger</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Franke</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Krohne</surname>, <given-names>G.</given-names></string-name>, van de Linde, S. &amp; <string-name><surname>Sauer</surname>, <given-names>M.</given-names></string-name> (<year>2014</year>) <article-title>Correlative super‐resolution fluorescence and electron microscopy of the nuclear pore complex with molecular resolution</article-title>. <source xml:lang="en">J. Cell Sci</source>. <volume>127</volume>, <fpage>4351</fpage>–<lpage>4355</lpage>.<pub-id pub-id-type="pmid">25146397</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0014">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0014"><string-name><surname>Mateos</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Guhl</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Doehner</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Barmettler</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kaech</surname>, <given-names>A.</given-names></string-name> &amp; <string-name><surname>Ziegler</surname>, <given-names>U.</given-names></string-name> (<year>2016</year>) <article-title>Topographic contrast of ultrathin cryosections for correlative super‐resolution light and electron microscopy</article-title>. <source xml:lang="en">Sci. Rep.</source>
<volume>6</volume>, <fpage>34062</fpage>.<pub-id pub-id-type="pmid">27666401</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0015">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0015"><string-name><surname>Micheva</surname>, <given-names>K.D.</given-names></string-name> &amp; <string-name><surname>Smith</surname>, <given-names>S.J.</given-names></string-name> (<year>2007</year>) <article-title>Array tomography: a new tool for imaging the molecular architecture and ultrastructure of neural circuits</article-title>. <source xml:lang="en">Neuron</source>
<volume>55</volume>, <fpage>25</fpage>–<lpage>36</lpage>.<pub-id pub-id-type="pmid">17610815</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0016">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0016"><string-name><surname>Peddie</surname>, <given-names>C.J.</given-names></string-name>, <string-name><surname>Blight</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>E.</given-names></string-name><italic>et al</italic> (<year>2014</year>) <article-title>Correlative and integrated light and electron microscopy of in‐resin GFP fluorescence, used to localise diacylglycerol in mammalian cells</article-title>. <source xml:lang="en">Ultramicroscopy</source>
<volume>143</volume>, <fpage>3</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">24637200</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0017">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0017"><string-name><surname>Peddie</surname>, <given-names>C.J.</given-names></string-name> &amp; <string-name><surname>Collinson</surname>, <given-names>L.M.</given-names></string-name> (<year>2014</year>) <article-title>Exploring the third dimension: volume electron microscopy comes of age</article-title>. <source xml:lang="en">Micron</source>
<volume>61</volume>, <fpage>9</fpage>–<lpage>19</lpage>.<pub-id pub-id-type="pmid">24792442</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0018">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0018"><string-name><surname>Peddie</surname>, <given-names>C.J.</given-names></string-name>, <string-name><surname>Domart</surname>, <given-names>M.‐C.</given-names></string-name>, <string-name><surname>Snetkov</surname>, <given-names>X.</given-names></string-name>, O<string-name><surname>Toole</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Larijani</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Way</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Cox</surname>, <given-names>S.</given-names></string-name> &amp; <string-name><surname>Collinson</surname>, <given-names>L.M.</given-names></string-name> (<year>2017</year>) <article-title>Correlative superresolution fluorescence and electron microscopy using conventional fluorescent proteins in vacuo</article-title>. <source xml:lang="en">J. Struct. Biol.</source>
<volume>199</volume>, <fpage>120</fpage>–<lpage>131</lpage>.<pub-id pub-id-type="pmid">28576556</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0019">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0019"><string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Arganda‐Carreras</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Frise</surname>, <given-names>E.</given-names></string-name><italic>et al</italic> (<year>2012</year>) <article-title>Fiji: an open‐source platform for biological‐image analysis</article-title>. <source xml:lang="en">Nat. Methods</source>
<volume>9</volume>, <fpage>676</fpage>–<lpage>682</lpage>.<pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0020">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0020"><string-name><surname>Schneider</surname>, <given-names>C.A.</given-names></string-name>, <string-name><surname>Rasband</surname>, <given-names>W. S.</given-names></string-name>, &amp; <string-name><surname>Eliceiri</surname>, <given-names>K.W.</given-names></string-name> (<year>2012</year>) <article-title>NIH Image to ImageJ: 25 years of image analysis</article-title>. <source xml:lang="en">Nat. Methods</source>
<volume>9</volume>, <fpage>671</fpage>–<lpage>675</lpage>.<pub-id pub-id-type="pmid">22930834</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0021">
      <mixed-citation publication-type="book" id="jmi12700-cit-0021"><string-name><surname>Sommer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Straehle</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Koethe</surname>, <given-names>U.</given-names></string-name> &amp; <string-name><surname>Hamprecht</surname>, <given-names>F.A.</given-names></string-name> (<year>2011</year>) <chapter-title>Ilastik: interactive learning and segmentation toolkit</chapter-title> In <source xml:lang="en">2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</source>, pp. <fpage>230</fpage>–<lpage>233</lpage>. <publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0022">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0022"><string-name><surname>Wacker</surname>, <given-names>I.</given-names></string-name> &amp; <string-name><surname>Schroeder</surname>, <given-names>R.</given-names></string-name> (<year>2013</year>) <article-title>Array tomography</article-title>. <source xml:lang="en">J. Microsc.</source>
<volume>252</volume>, <fpage>93</fpage>–<lpage>99</lpage>.<pub-id pub-id-type="pmid">24111814</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0023">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0023"><string-name><surname>Wiesmann</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Franz</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Held</surname>, <given-names>C.</given-names></string-name>, Mü<string-name><surname>nzenmayer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Palmisano</surname>, <given-names>R.</given-names></string-name> &amp; <string-name><surname>Wittenberg</surname>, <given-names>T.</given-names></string-name> (<year>2015</year>) <article-title>Review of free software tools for image analysis of fluorescence cell micrographs</article-title>. <source xml:lang="en">J. Microsc.</source>
<volume>257</volume>, <fpage>39</fpage>–<lpage>53</lpage>.<pub-id pub-id-type="pmid">25359577</pub-id></mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0024">
      <mixed-citation publication-type="journal" id="jmi12700-cit-0024"><string-name><surname>Wolff</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Hagen</surname>, <given-names>C.</given-names></string-name>, Grünewald, K. &amp; <string-name><surname>Kaufmann</surname>, <given-names>R.</given-names></string-name> (<year>2016</year>) <article-title>Towards correlative super‐resolution fluorescence and electron cryo‐microscopy</article-title>. <source xml:lang="en">Biol. Cell</source>
<volume>108</volume>, 245–258.</mixed-citation>
    </ref>
    <ref id="jmi12700-bib-0025">
      <mixed-citation publication-type="book" id="jmi12700-cit-0025"><string-name><surname>Zuiderveld</surname>, <given-names>K.</given-names></string-name> (<year>1994</year>) <chapter-title>Contrast limited adaptive histogram equalization</chapter-title>
<source xml:lang="en">Graphics Gems IV</source>, pp. <fpage>474</fpage>–<lpage>485</lpage>. <publisher-name>Academic Press Professional, Inc.</publisher-name>, <publisher-loc>San Diego, CA</publisher-loc>.</mixed-citation>
    </ref>
  </ref-list>
</back>
