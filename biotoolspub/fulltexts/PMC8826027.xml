<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8826027</article-id>
    <article-id pub-id-type="pmid">34888618</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab830</article-id>
    <article-id pub-id-type="publisher-id">btab830</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Unsupervised construction of computational graphs for gene expression data with explicit structural inductive biases</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2240-7501</contrib-id>
        <name>
          <surname>Scherer</surname>
          <given-names>Paul</given-names>
        </name>
        <xref rid="btab830-cor1" ref-type="corresp"/>
        <aff><institution>Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country country="GB">UK</country></aff>
        <!--paul.scherer@cl.cam.ac.uk-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Trębacz</surname>
          <given-names>Maja</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Simidjievski</surname>
          <given-names>Nikola</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2411-4478</contrib-id>
        <name>
          <surname>Viñas</surname>
          <given-names>Ramon</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shams</surname>
          <given-names>Zohreh</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Terre</surname>
          <given-names>Helena Andres</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jamnik</surname>
          <given-names>Mateja</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country country="GB">UK</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liò</surname>
          <given-names>Pietro</given-names>
        </name>
        <aff><institution>Department of Computer Science and Technology, University of Cambridge</institution>, Cambridge, CB3 0FD, <country country="GB">UK</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Martelli</surname>
          <given-names>Pier Luigi</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab830-cor1">To whom correspondence should be addressed. <email>paul.scherer@cl.cam.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-12-09">
      <day>09</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>09</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <volume>38</volume>
    <issue>5</issue>
    <fpage>1320</fpage>
    <lpage>1327</lpage>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>29</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>20</day>
        <month>11</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>03</day>
        <month>12</month>
        <year>2021</year>
      </date>
      <date date-type="corrected-typeset">
        <day>16</day>
        <month>12</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab830.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Gene expression data are commonly used at the intersection of cancer research and machine learning for better understanding of the molecular status of tumour tissue. Deep learning predictive models have been employed for gene expression data due to their ability to scale and remove the need for manual feature engineering. However, gene expression data are often very high dimensional, noisy and presented with a low number of samples. This poses significant problems for learning algorithms: models often overfit, learn noise and struggle to capture biologically relevant information. In this article, we utilize external biological knowledge embedded within structures of gene interaction graphs such as protein–protein interaction (PPI) networks to guide the construction of predictive models.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We present Gene Interaction Network Constrained Construction (GINCCo), an unsupervised method for automated construction of computational graph models for gene expression data that are structurally constrained by prior knowledge of gene interaction networks. We employ this methodology in a case study on incorporating a PPI network in cancer phenotype prediction tasks. Our computational graphs are structurally constructed using topological clustering algorithms on the PPI networks which incorporate inductive biases stemming from network biology research on protein complex discovery. Each of the entities in the GINCCo computational graph represents biological entities such as genes, candidate protein complexes and phenotypes instead of arbitrary hidden nodes of a neural network. This provides a biologically relevant mechanism for model regularization yielding strong predictive performance while drastically reducing the number of model parameters and enabling guided <italic toggle="yes">post-hoc</italic> enrichment analyses of influential gene sets with respect to target phenotypes. Our experiments analysing a variety of cancer phenotypes show that GINCCo often outperforms support vector machine, Fully Connected Multi-layer Perceptrons (MLP) and Randomly Connected MLPs despite greatly reduced model complexity.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p><ext-link xlink:href="https://github.com/paulmorio/gincco" ext-link-type="uri">https://github.com/paulmorio/gincco</ext-link> contains the source code for our approach. We also release a library with algorithms for protein complex discovery within PPI networks at <ext-link xlink:href="https://github.com/paulmorio/protclus" ext-link-type="uri">https://github.com/paulmorio/protclus</ext-link>. This repository contains implementations of the clustering algorithms used in this article.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Armstrong Fund from the School of Technology at the University of Cambridge</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>The Mark Foundation Institute for Integrated Cancer Medicine (MFICM)</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>University of Cambridge, with funding from The Mark Foundation for Cancer Research</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Cancer Research UK Cambridge Centre</institution>
          </institution-wrap>
        </funding-source>
        <award-id>C9685/A25177</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Gene expression data are commonly used at the intersection of cancer research and machine learning as it is seen as a crucial component towards understanding the molecular status of tumour tissue. In its most common form, an observation of gene expression data is presented as a <italic toggle="yes">k</italic>-dimensional feature vector of continuous values after normalization of the raw data where each element of the vector corresponds to the expression level of a particular gene in the sample. Classically, this representation is directly used to learn a prediction model for tasks such as cancer disease subtype classification or as part of a larger system integrating data from multiple modalities (<xref rid="btab830-B14" ref-type="bibr">Esteva <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab830-B34" ref-type="bibr">Simidjievski <italic toggle="yes">et al.</italic>, 2019</xref>).</p>
    <p>The high dimensionality and noisiness of the gene expression data pose significant problems to learning algorithms. Coupled with the comparatively low number of observations, this high dimensionality causes models to overfit, learn noise and struggle to capture any biologically relevant information (<xref rid="btab830-B14" ref-type="bibr">Esteva <italic toggle="yes">et al.</italic>, 2019</xref>). As a result, practitioners commonly aim to constrain model complexity by incorporating various approaches for regularization including dimensionality reduction and use of prior biological knowledge to inductively bias models towards learning representations with favourable characteristics (<xref rid="btab830-B7" ref-type="bibr">Cawley and Talbot, 2006</xref>; <xref rid="btab830-B12" ref-type="bibr">Dutil <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab830-B17" ref-type="bibr">Gustafsson <italic toggle="yes">et al.</italic>, 2005</xref>; <xref rid="btab830-B34" ref-type="bibr">Simidjievski <italic toggle="yes">et al.</italic>, 2019</xref>). Our research uses prior knowledge to focus on the incorporation of gene interaction networks as external priors into the predictive model in order to guide the learning process. The overall goal of applying network-based analysis to personal genomic profiles is to identify network modules that are both informative of cancer mechanisms and predictive of cancer phenotypes. A survey which describes some of these approaches can be found in <xref rid="btab830-B38" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2017)</xref>. However, many of these methods are handcrafted to address very specific case studies and typically they are not end-to-end differentiable which is the focus of this study.</p>
    <p>In this work, we propose a method for automated construction of predictive neural network models that build upon structures discovered within gene interaction networks. More specifically, we utilize topological clustering algorithms chiefly used for the discovery of protein complexes and functional modules within protein–protein interaction (PPI) networks to define the structure of factor graphs in an unsupervised manner. This deterministic procedure produces sparse computational graph models which relate genes to named protein complexes, structurally parameterizing individual functions for the ‘activity’ of each complex based on an input gene expression profile. Given such computation graphs, further connecting the complex activities to cancer phenotypes defines a supervised predictive model akin to a sparsely connected artificial neural network, which maps the activity patterns of higher level functional modules (protein complexes) to cancer phenotypes via the original gene expression data.</p>
    <p>Our approach effectively constrains the hypothesis space via explicit structural biases obtained through unsupervised analyses of network biology entities. As a result, this provides a biologically relevant mechanism for model regularization, resulting in structurally constrained models that yield competitive predictive performance with significantly lower number of model parameters and offer insights into the expression patterns of phenotype relevant complexes. <xref rid="btab830-F1" ref-type="fig">Figure 1</xref> features a simplified diagram of this process over an input genomic profile dataset and a toy interaction network used to construct the topology of the computational graph.</p>
    <fig position="float" id="btab830-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>An overview of our procedure for incorporating PPI network based protein complex discovery and constructing computational graphs for gene expression analysis. GINCCo’s procedure for model construction is best described in three stages: (i) induction of the case study specific sub-graph <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> common to the input gene expression dataset (for set of <italic toggle="yes">k</italic> genes <italic toggle="yes">K</italic>) and the external PPI network which will be used for the (ii) unsupervised discovery of the protein complexes that act as biologically relevant higher level modules of the inputs and (iii) the use of the clusterings <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to construct a bipartite factor graph between the gene expressions and the protein complexes and extending the use of the graph in the predictive model that transitively maps the gene expressions to phenotypes via the protein complex activities. In the final computational graph model, we can see blue genes which are excluded as a result of extracting the case specific study graph, and red genes which are excluded as a result of clustering process on <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p>
      </caption>
      <graphic xlink:href="btab830f1" position="float"/>
    </fig>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>The proposed method, which we will refer to as Gene Interaction Network Constrained Construction (<italic toggle="yes">GINCCo</italic>), incorporates prior biological knowledge embedded within the structure of external PPI networks and protein complexes discovered in these via topological clustering algorithms to construct a bipartite graph between gene expressions and functional modules. This bipartite factor graph serves as the structural foundation for computational graph models that will be further augmented into predictive models for cancer phenotypes. Crucially, this means that the structure of the computational graphs created by GINCCo is defined in a purely unsupervised and deterministic manner over external structured knowledge.</p>
    <p>GINCCo’s procedure for constructing the computational graphs is best described in three stages which also correspond to those shown in <xref rid="btab830-F1" ref-type="fig">Figure 1</xref>:
</p>
    <list list-type="bullet">
      <list-item>
        <p>Obtaining a case study specific sub-graph of an external PPI network with the input gene expression data.</p>
      </list-item>
      <list-item>
        <p>Discovering protein complexes that serve as higher level functional modules within the study specific sub-graph from Step 1.</p>
      </list-item>
      <list-item>
        <p>Constructing the factor and computational graphs for downstream modelling.</p>
      </list-item>
    </list>
    <sec>
      <title>2.1 Processing and generating case study PPI networks</title>
      <p>Let us assume an input gene expression dataset <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> describing <italic toggle="yes">m</italic> patient observations with <italic toggle="yes">k</italic>-dimensional vectors of gene expression values, and <italic toggle="yes">K</italic> represents the set of genes in this expression dataset. Furthermore, let us assume an external PPI network <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mtext>PPI</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>PPI</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>PPI</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, such as one from the STRING-DB 9606 <italic toggle="yes">Homo Sapiens</italic> PPI network (<xref rid="btab830-B36" ref-type="bibr">Szklarczyk <italic toggle="yes">et al.</italic>, 2019</xref>). For our purpose, this PPI network is an unweighted graph with nodes <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>PPI</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> labelled by the names of proteins, and no additional node or edge features. We induce a sub-graph of the input network <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo>⊆</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mtext>PPI</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The nodes of <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the intersection of the common genes in the input gene expression dataset <italic toggle="yes">K</italic> and their products in the PPI network; in other words <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>∩</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>PPI</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The induced sub-graph <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the graph whose vertex set is <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and whose edge set consists of all of the edges in <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>PPI</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that have both endpoints in <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This action is illustrated in the top row of actions in <xref rid="btab830-F1" ref-type="fig">Figure 1</xref>. We denote <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> our study PPI network since it is the ‘cut out’ of the external PPI network relevant to our case study.</p>
    </sec>
    <sec>
      <title>2.2 Protein complex discovery</title>
      <p>Given the induced study network, we use a topological clustering algorithm <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mi mathvariant="script">C</mml:mi></mml:math></inline-formula> (such as DPCLUS; <xref rid="btab830-B2" ref-type="bibr">Altaf-Ul-Amin <italic toggle="yes">et al.</italic>, 2006</xref>) to discover protein complexes within the study PPI network <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The aim of the clustering algorithms is to discover protein complexes represented as a set of induced sub-graphs <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">l</italic> is the number of complexes discovered by <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mi mathvariant="script">C</mml:mi></mml:math></inline-formula>. The number of protein complexes found, <italic toggle="yes">l</italic>, is not dependent on the user, but rather on the application of the clustering algorithm <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mi mathvariant="script">C</mml:mi></mml:math></inline-formula> upon the input study network. Any appropriate clustering algorithm can be used.</p>
      <p>It is worth noting that we specifically chose clustering algorithms that do not partition the graph. In other words, a single protein may be part of multiple complexes. This is to reflect the fact that proteins may be involved in several biological processes and complexes. Moreover, not all proteins in <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> will necessarily be assigned to clusters by <italic toggle="yes">C</italic>. We are not arbitrarily forcing all genes to be part of our constructed models, and this acts as a form of feature selection upon the input <bold>X</bold> by <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>2.3 Computational graph construction and predictive models</title>
      <p>The output of the clustering algorithm <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> enables the construction of a bipartite factor graph. Herein, each of the protein complexes is assigned a uniquely labelled node <italic toggle="yes">c<sub>i</sub></italic> and each protein within the set of proteins involved in one or more complexes is also given a labelled node by their name. Directed edges link proteins to complexes <italic toggle="yes">c<sub>i</sub></italic> they are a member of. This construction gives the factorization of a parametric function <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math></inline-formula> computed from the proteins involved in <italic toggle="yes">c<sub>i</sub></italic>. The function <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be set by the practitioner or learned as in a neural network.</p>
      <p>The parameterizations <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math></inline-formula> in our proposal are a stark contrast to arbitrarily chosen hidden-state activations <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>k</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math></inline-formula> found in conventional application of fully connected multi-layer perceptrons (FC MLPs). First, each of the <italic toggle="yes">c<sub>i</sub></italic> denotes a ‘protein complex activity’, a biologically relevant structure modelled through incorporation of external PPI and topological clustering algorithm, instead of an arbitrarily chosen hidden state node. The proteins that are members of <italic toggle="yes">c<sub>i</sub></italic>, and only those proteins, affect its activity level <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math></inline-formula>, instead of all input features. This is a strong and explicit inductive bias if <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is learned through a neural network. A visual comparison between the factor graphs of a FC model and that of GINCCo can be seen in <xref rid="btab830-F2" ref-type="fig">Figure 2</xref>.</p>
      <fig position="float" id="btab830-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>A visual comparison between the factor graphs produced using a FC computational graph as in a standard neural network and that produced by GINCCo using the toy example introduced in <xref rid="btab830-F1" ref-type="fig">Figure  1</xref></p>
        </caption>
        <graphic xlink:href="btab830f2" position="float"/>
      </fig>
      <p>We construct computational graph models for cancer phenotype prediction by further augmenting the current gene/protein to protein complex factor graph to include complete connections between the protein complexes <italic toggle="yes">c<sub>i</sub></italic> to target nodes gained when encoding the target observations <bold>Y</bold>. As such, each function <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math></inline-formula> computing the individual protein complex ‘activity’ is learned over minimizing the global cross-entropy loss between predicted and the target phenotypes.</p>
    </sec>
    <sec>
      <title>2.4 Experimental setup</title>
      <p>We hypothesized that the knowledge-driven construction of the computational graphs through incorporation of gene interaction networks as prior biological knowledge will yield sparser models and better predictive performance than FC baselines. We tested this hypothesis in parts: comparing model sparsity in terms of number of parameters, comparing predictive performance across datasets and subsequently checking whether GINCCo captures useful signals that cannot be found through random computational graph construction.</p>
      <p>In order to evaluate the proposed method for model construction, we used publicly available gene expression data from the METABRIC Breast Cancer Consortium (METABRIC; <xref rid="btab830-B10" ref-type="bibr">Curtis <italic toggle="yes">et al.</italic>, 2012</xref>) to predict cancer phenotypes with gene expression data. The dataset consists of the mRNA expression data and clinical data of breast cancer patient samples in the METABRIC cohort <xref rid="btab830-B10" ref-type="bibr">(Curtis <italic toggle="yes">et al.</italic>, 2012)</xref>. Herein, we tackle several classification tasks over the 1980 breast cancer patients, representing a particularly large dataset for cancer data research. Each observation is represented by a 24 368-dimensional vector corresponding to the continuous expression values of measured genes. The microarray data were normalized as described in <xref rid="btab830-B10" ref-type="bibr">Curtis <italic toggle="yes">et al.</italic> (2012)</xref>. We evaluate the predictive performance over the proposed methods ability to predict:
</p>
      <list list-type="bullet">
        <list-item>
          <p>Distance relapse, a binary classification task.</p>
        </list-item>
        <list-item>
          <p>IntegrativeCluster subtypes (IC10), a 11 class prediction task where observations belong to integrative clusters typified by copy number aberrations (<xref rid="btab830-B10" ref-type="bibr">Curtis <italic toggle="yes">et al.</italic>, 2012</xref>).</p>
        </list-item>
        <list-item>
          <p>PAM50 breast tumour cancer subtype (<xref rid="btab830-B30" ref-type="bibr">Prat <italic toggle="yes">et al.</italic>, 2010</xref>; PAM50), a five class prediction task (Basal, Her2, Luminal A, Luminal Band Normal).</p>
        </list-item>
      </list>
      <p>To show that GINCCo can operate across datasets, we also evaluate it on The Cancer Genome Atlas Head–Neck Squamous Cell Carcinoma (TCGA-HNSC) dataset (<xref rid="btab830-B29" ref-type="bibr">Cancer Genome Atlas Network, 2015</xref>; <xref rid="btab830-B31" ref-type="bibr">Rendleman <italic toggle="yes">et al.</italic>, 2019</xref>). The HT-Seq count expression data were normalized using the Fragments Per Kilobase of transcript per Million mapped reads method as made available through the National Cancer Institute Genomic Data Commons Data Portal, <ext-link xlink:href="https://portal.gdc.cancer.gov/" ext-link-type="uri">https://portal.gdc.cancer.gov/</ext-link> that have been as in <xref rid="btab830-B31" ref-type="bibr">Rendleman <italic toggle="yes">et al.</italic> (2019)</xref>. The dataset contains 528 TCGA-HNSC cases wherein we focus on the 20 501 mRNA expression features. The clinical targets include:
</p>
      <list list-type="bullet">
        <list-item>
          <p>tumour grade, wherein observations are classified into Grades I–IV based on standards set by the World Health Organization.</p>
        </list-item>
        <list-item>
          <p>2-year relapse-free survival, a binary prediction task.</p>
        </list-item>
      </list>
      <p>For all prediction tasks, tables of the exact class label distributions are presented in the <xref rid="sup1" ref-type="supplementary-material">supplementary materials</xref> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix SA</xref>).</p>
      <p>Amongst the considered methods are: majority class classifier (MajorityClass), a support vector machine (SVM) with RBF kernel, a FC two-layer neural network with 1600 hidden layer nodes (this number of hidden nodes was chosen to closely match the number of protein complexes used in GINCCo + DPCLUS, the best performing of the proposed methods), a network regularized FC network (<xref rid="btab830-B24" ref-type="bibr">Li and Li, 2008</xref>; GraphReg), and our proposed model constructor coupled with a variety of topological clustering algorithms. Each of our models is referred to as GINCCo + <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mi mathvariant="script">C</mml:mi></mml:math></inline-formula>, where C refers to one of: Molecular Complex Detection (MCODE) (<xref rid="btab830-B3" ref-type="bibr">Bader and Hogue, 2003</xref>), COre-AttaCHment-based method (COACH) (<xref rid="btab830-B37" ref-type="bibr">Wu <italic toggle="yes">et al.</italic>, 2009</xref>), IPCA (<xref rid="btab830-B25" ref-type="bibr">Li <italic toggle="yes">et al.</italic>, 2008</xref>) or DPCLUS (<xref rid="btab830-B2" ref-type="bibr">Altaf-Ul-Amin <italic toggle="yes">et al.</italic>, 2006</xref>) clustering algorithms. These clustering algorithms were chosen on the basis that they are well established, allow overlapping clusters and have deterministic implementations for reproducibility. We also release an open-source library of these implementations alongside this article as described in the availability statement.</p>
      <p>MCODE is an agglomerative clustering algorithm for identification of protein complexes given PPI graphs. COACH is an algorithm for identification of protein complexes based on core-attachment structure. DPCLUS is an iterative algorithm for protein-complex identification from interaction graphs. Similarly to MCODE, given a PPI graph, DPCLUS initializes the clusters with the node with the highest weight, identified by analysing node neighbourhoods. Once a cluster is initialized, the algorithm extends it by adding neighbouring nodes that meet predefined criteria of density and cluster-connectivity property. IPCA is a modification of DPCLUS. Similar to DPCLUS, IPCA grows the clusters based on the topological structure of the underlying interaction graph by searching for small-diameter sub-graphs that meet certain cluster connectivity-density property. In contrast to DPClus that re-computes the node weights each time a sub-graph is removed, IPCA computes these weights once at the beginning and uses them for the whole process. The hyperparameters of the clustering algorithms were set to their default values.</p>
      <p>The SVM’s hyperparameters were kept the same at <italic toggle="yes">C </italic>=<italic toggle="yes"> </italic>1.0 and a scaled <italic toggle="yes">γ</italic> value. The FC MLP and the computational graphs of GINCCo were trained through optimization of the cross-entropy loss. The loss was optimized using Adam (<xref rid="btab830-B22" ref-type="bibr">Kingma and Ba, 2014</xref>) with a mini batch size of 32 and 500 epochs and a learning rate of 0.0001. The weight parameters were initialized using the Xavier uniform initialization (<xref rid="btab830-B16" ref-type="bibr">Glorot and Bengio, 2010</xref>).</p>
      <p>For each task, we compare the methods over the average performance of five repeated class-stratified train and hold-out test splits with 80:20 train:test set ratios. We use a quarter of each training-set split to produce a validation set for early stopping. The performance of each model was compared with respect to average balanced classification accuracy (B-ACC) and weighted area under receiver operator characteristic (W-AUC) over each of the five splits in the tasks to account for any class imbalances. To compute the W-AUC, we averaged the one-versus-rest scores for each label weighted by the class label distribution. For completeness, we have included tables for the comparative analysis of unbalanced accuracy, weighted precision, weighted recall and weighted <italic toggle="yes">F</italic>-scores which can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix SC</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Factor graphs produced by GINCCo are considerably sparser than FC network models</title>
      <p>The computational graph models produced by GINCCo innately incorporate biological knowledge of the PPI network and the multi-protein modules discovered through <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> over the study network. The resulting bipartite factor graphs between the gene expressions and protein complex activities <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math></inline-formula> are considerably sparser than their FC counterparts as <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mo>∀</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>≤</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> by design and often <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>≪</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> as seen in <xref rid="btab830-T1" ref-type="table">Table 1</xref>. <xref rid="btab830-T1" ref-type="table">Table 1</xref> describes the number of edges (parameters) in the bipartite graph produced by GINCCo and a given clustering algorithm <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> on the study network created with STRING and the 24 368 genes in the METABRIC dataset. <xref rid="btab830-T2" ref-type="table">Table 2</xref> describes descriptive statistics of the clusters obtained by each clustering algorithm on the study network. This is compared against the number of edges formed in the FC counterpart with the equal number of hidden activities <italic toggle="yes">h<sub>i</sub></italic>; a visual comparison can be found in <xref rid="btab830-F2" ref-type="fig">Figure 2</xref>. <xref rid="btab830-T1" ref-type="table">Table 1</xref> shows how GINCCo models have orders of magnitude less parameters than their FC counterparts, and we will show that despite this the models still perform competitively in predictive tasks and bring additional benefits.</p>
      <table-wrap position="float" id="btab830-T1">
        <label>Table 1.</label>
        <caption>
          <p>Number of parameters used in equally dimensioned FC MLP network and the proposed method using different clustering methods to automatically discover protein complexes and their members on the STRING 9606 PPI network and the 24 368 genes measured in METABRIC</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">MCODE (40 clusters)</th>
              <th rowspan="1" colspan="1">COACH (4108 clusters)</th>
              <th rowspan="1" colspan="1">IPCA (5744 Clusters)</th>
              <th rowspan="1" colspan="1">DPCLUS (1562 clusters)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">FC MLP</td>
              <td rowspan="1" colspan="1">974 720</td>
              <td rowspan="1" colspan="1">100 103 744</td>
              <td rowspan="1" colspan="1">139 969 792</td>
              <td rowspan="1" colspan="1">38 062 816</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GINCCo</td>
              <td rowspan="1" colspan="1">14 537</td>
              <td rowspan="1" colspan="1">1 431 338</td>
              <td rowspan="1" colspan="1">2 800 267</td>
              <td rowspan="1" colspan="1">19 545</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 Empirical results show integration of prior biological knowledge yields strong predictive performance</title>
      <p>The main comparative results are summarized in <xref rid="btab830-T2" ref-type="table">Table 2</xref> for the METABRIC and TCGA-HNCS datasets. The results show that all variations of the computational graph models produced by GINCCo perform strongly against both the SVM and FC MLP baselines.</p>
      <table-wrap position="float" id="btab830-T2">
        <label>Table 2.</label>
        <caption>
          <p>Average percentage balanced accuracy (B-ACC) and W-AUC with SDs over five repeated train and holdout test evaluations using all of the gene expression features of METABRIC and TCGA-HNCS </p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="3" colspan="1">Method</th>
              <th colspan="6" rowspan="1">METABRIC<hr/></th>
              <th colspan="4" rowspan="1">TCGA-HNCS<hr/></th>
            </tr>
            <tr>
              <th colspan="2" rowspan="1">Distance relapse<hr/></th>
              <th colspan="2" rowspan="1">PAM50<hr/></th>
              <th colspan="2" rowspan="1">IC10<hr/></th>
              <th colspan="2" rowspan="1">Tumour grade<hr/></th>
              <th colspan="2" rowspan="1">2-year relapse-free survival<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">MajorityClass</td>
              <td rowspan="1" colspan="1">50.00 ± 0.00</td>
              <td rowspan="1" colspan="1">0.50 ± 0.00</td>
              <td rowspan="1" colspan="1">20.00 ± 0.00</td>
              <td rowspan="1" colspan="1">0.50 ± 0.00</td>
              <td rowspan="1" colspan="1">9.09 ± 0.00</td>
              <td rowspan="1" colspan="1">0.50 ± 0.00</td>
              <td rowspan="1" colspan="1">25.00 ± 0.00</td>
              <td rowspan="1" colspan="1">0.50 ± 0.00</td>
              <td rowspan="1" colspan="1">50.00 ± 0.00</td>
              <td rowspan="1" colspan="1">0.50 ± 0.00</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SVM</td>
              <td rowspan="1" colspan="1">54.43 ± 1.85</td>
              <td rowspan="1" colspan="1">0.54 ± 0.02</td>
              <td rowspan="1" colspan="1">72.21 ± 3.07</td>
              <td rowspan="1" colspan="1">0.94 ± 0.01</td>
              <td rowspan="1" colspan="1">55.72 ± 3.79</td>
              <td rowspan="1" colspan="1">0.95 ± 0.01</td>
              <td rowspan="1" colspan="1">39.35 ± 4.28</td>
              <td rowspan="1" colspan="1">0.67 ± 0.04</td>
              <td rowspan="1" colspan="1">56.59 ± 4.83</td>
              <td rowspan="1" colspan="1">0.57 ± 0.05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">FC MLP</td>
              <td rowspan="1" colspan="1">56.92 ± 2.65</td>
              <td rowspan="1" colspan="1">0.57 ± 0.03</td>
              <td rowspan="1" colspan="1">74.65 ± 3.60</td>
              <td rowspan="1" colspan="1">0.94 ± 0.01</td>
              <td rowspan="1" colspan="1">66.32 ± 1.99</td>
              <td rowspan="1" colspan="1">0.95 ± 0.01</td>
              <td rowspan="1" colspan="1">34.29 ± 3.53</td>
              <td rowspan="1" colspan="1">0.66 ± 0.04</td>
              <td rowspan="1" colspan="1">58.14 ± 4.23</td>
              <td rowspan="1" colspan="1">0.58 ± 0.05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GraphReg</td>
              <td rowspan="1" colspan="1">49.86 ± 1.05</td>
              <td rowspan="1" colspan="1">0.50 ± 0.01</td>
              <td rowspan="1" colspan="1">22.57 ± 2.71</td>
              <td rowspan="1" colspan="1">0.82 ± 0.01</td>
              <td rowspan="1" colspan="1">9.09 ± 0.00</td>
              <td rowspan="1" colspan="1">0.83 ± 0.01</td>
              <td rowspan="1" colspan="1">27.63 ± 3.25</td>
              <td rowspan="1" colspan="1">0.64 ± 0.02</td>
              <td rowspan="1" colspan="1">55.42 ± 2.35</td>
              <td rowspan="1" colspan="1">0.55 ± 0.02</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GINCCo + MCODE</td>
              <td rowspan="1" colspan="1">56.65 ± 1.86</td>
              <td rowspan="1" colspan="1">0.57 ± 0.02</td>
              <td rowspan="1" colspan="1">73.52 ± 2.71</td>
              <td rowspan="1" colspan="1">0.93 ± 0.01</td>
              <td rowspan="1" colspan="1">57.77 ± 1.73</td>
              <td rowspan="1" colspan="1">0.93 ± 0.01</td>
              <td rowspan="1" colspan="1">36.93 ± 10.14</td>
              <td rowspan="1" colspan="1">0.64 ± 0.03</td>
              <td rowspan="1" colspan="1">55.43 ± 2.87</td>
              <td rowspan="1" colspan="1">0.55 ± 0.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GINCCo + COACH</td>
              <td rowspan="1" colspan="1">56.73 ± 0.98</td>
              <td rowspan="1" colspan="1">0.57 ± 0.01</td>
              <td rowspan="1" colspan="1">74.97 ± 3.27</td>
              <td rowspan="1" colspan="1">0.95 ± 0.01</td>
              <td rowspan="1" colspan="1">63.04 ± 2.98</td>
              <td rowspan="1" colspan="1">0.95 ± 0.01</td>
              <td rowspan="1" colspan="1">39.38 ± 11.48</td>
              <td rowspan="1" colspan="1">0.65 ± 0.03</td>
              <td rowspan="1" colspan="1">56.79 ± 3.49</td>
              <td rowspan="1" colspan="1">0.57 ± 0.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GINCCo + IPCA</td>
              <td rowspan="1" colspan="1">57.13 ± 1.47</td>
              <td rowspan="1" colspan="1">0.57 ± 0.01</td>
              <td rowspan="1" colspan="1">74.62 ± 4.55</td>
              <td rowspan="1" colspan="1">0.94 ± 0.01</td>
              <td rowspan="1" colspan="1">62.26 ± 4.51</td>
              <td rowspan="1" colspan="1">0.94 ± 0.01</td>
              <td rowspan="1" colspan="1">37.36 ± 9.54</td>
              <td rowspan="1" colspan="1">0.63 ± 0.03</td>
              <td rowspan="1" colspan="1">55.56 ± 3.39</td>
              <td rowspan="1" colspan="1">0.55 ± 0.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GINCCo + DPCLUS</td>
              <td rowspan="1" colspan="1">57.27 ± 1.80</td>
              <td rowspan="1" colspan="1">0.57 ± 0.02</td>
              <td rowspan="1" colspan="1">75.97 ± 4.59</td>
              <td rowspan="1" colspan="1">0.97 ± 0.01</td>
              <td rowspan="1" colspan="1">70.43 ± 3.68</td>
              <td rowspan="1" colspan="1">0.97 ± 0.00</td>
              <td rowspan="1" colspan="1">39.09 ± 9.96</td>
              <td rowspan="1" colspan="1">0.67 ± 0.03</td>
              <td rowspan="1" colspan="1">57.17 ± 4.42</td>
              <td rowspan="1" colspan="1">0.57 ± 0.04</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>More specifically, GINCCo + DPCLUS performs competitively overall making an especially substantial gain in IC10 subtype prediction. Performing a pairwise frequentist correlated <italic toggle="yes">t</italic>-test (<xref rid="btab830-B5" ref-type="bibr">Benavoli <italic toggle="yes">et al.</italic>, 2017</xref>) shows that GINCCo + DPCLUS has statistically significant performance gains across all tasks compared with MajorityClass and GraphReg methods but is not significant against the other methods except on IC10 subtype prediction (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix SD</xref>). However, this result is still good as it comes in spite of the fact that the GINCCo + DPCLUS model contains &lt;0.05% of the number of parameters used in the FC MLP (see <xref rid="btab830-T1" ref-type="table">Table 1</xref>). Furthermore, GINCCo models provide additional features pertaining biologically relevant insights that are not possible with the other methods as we show in Section 3.3.</p>
      <p>We attribute the strong performance of GINCCo to two related advantages over FC networks. First, GINCCo’s sparser model complexity allows more ‘weight’ to be assigned to each of the input signals used. Similarly, the sparse connectivity also helps generalizability in a similar way to the dropout regularization method. However, in contrast, the connectivity of GINCCo graph is set, explicit and realized through incorporation of prior knowledge rather than being random and ephemeral. This brings us to the second advantage of GINCCo—the structure of the computational graphs, and thus the representations, explicitly incorporate biological knowledge of protein complex membership as intermediate states. In other words, they are not ‘hidden’ nodes with arbitrary meaning. The learned activities of the protein complexes are explicitly factorized to the gene expression measurements of the genes/proteins that have a membership in the complex. To show that GINCCo benefits from both of the previously mentioned advantages and not only from the first advantage of regularization via sparse connections, we demonstrate that the performance of GINCCo + DPCLUS outperform computational graphs constructed through random processes (RC MLP-R and RC MLP-M).</p>
      <p>The differing performances on the choice of clustering algorithm <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> reflect the different assumptions made by researchers on what topological structures within <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> contain protein complexes. MCODE and DPCLUS exhibit stricter rules on complex candidates with fewer, smaller and more tightly knit clusters than either COACH or IPCA as in <xref rid="btab830-T3" ref-type="table">Table 3</xref>. This may be interpreted as these two methods constraining the hypothesis space more and incorporating ‘more’ expert knowledge which is helpful to the classification tasks. Naturally, GINCCo is agnostic to the choice of <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, therefore various combinations or set complexes may be explored in further work.</p>
      <table-wrap position="float" id="btab830-T3">
        <label>Table 3.</label>
        <caption>
          <p>Descriptive statistics of the protein complexes discovered via the topological clustering of the study PPI network <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> induced from the STRING PPI network and METABRIC</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Statistic</th>
              <th rowspan="1" colspan="1">MCODE</th>
              <th rowspan="1" colspan="1">COACH</th>
              <th rowspan="1" colspan="1">IPCA</th>
              <th rowspan="1" colspan="1">DPCLUS</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Number of protein complex</td>
              <td rowspan="1" colspan="1">40</td>
              <td rowspan="1" colspan="1">4108</td>
              <td rowspan="1" colspan="1">5744</td>
              <td rowspan="1" colspan="1">1562</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Maximum cluster size</td>
              <td rowspan="1" colspan="1">1555</td>
              <td rowspan="1" colspan="1">2684</td>
              <td rowspan="1" colspan="1">639</td>
              <td rowspan="1" colspan="1">359</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Minimum cluster size</td>
              <td rowspan="1" colspan="1">3</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Average cluster size</td>
              <td rowspan="1" colspan="1">363.43</td>
              <td rowspan="1" colspan="1">348.43</td>
              <td rowspan="1" colspan="1">487.51</td>
              <td rowspan="1" colspan="1">12.51</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3 Experiments against randomly structured computational graphs show GINCCo models capture useful parameterizations</title>
      <p>As the structure of the computational graphs is driven largely by the structure of the external PPI network and the number/members of the protein complexes discovered, we check that GINCCo graphs actually capture biologically relevant information. Naturally, the structure of the PPI network itself is explained and justified by the maintainers/proposers/curators of the databases. Similarly, the biological relevance of the clustering algorithms used on the PPI networks is also reasoned and justified within each of the original papers. Hence, our task here is to find whether the computational graphs constructed through GINCCo obtain better scores than the SVMs and FC-MLP because the structure and learned activity functions capture meaningful biological relationships.</p>
      <p>We test this with two approaches to generate randomly connected computational graph models, referred to as RC MLP-R and RC MLP-M. For RC MLP-R, we construct computational graphs with a random number of ‘discovered protein complexes’ and a random number of connections attributing protein memberships to clusters. The random numbers are drawn from a uniform distribution between <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>6000</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> for the number of protein complexes (this range was chosen to roughly reflect the number of protein complexes found in the chosen clustering algorithms on the STRING-DB PPI network; see <xref rid="btab830-T3" ref-type="table">Table 3</xref>) and <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo>*</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> random protein to complex connections. For RC MLP-M models, we preserve the number of complexes and connections used in GINCCo + DPCLUS but perturb the connections. Hence, <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mtext>RCMLPM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mtext>DPCLUS</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mtext>RCMLPM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mtext>DPCLUS</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, translating to <italic toggle="yes">l </italic>=<italic toggle="yes"> </italic>1562 and <italic toggle="yes">u </italic>=<italic toggle="yes"> </italic>19 545 for METABRIC tasks. For an empirical evaluation, 100 instances of such random computational graphs were constructed to obtain a Monte-Carlo aggregate mean score across the same repeated train-test evaluation described in Section 2.4. Results are shown in <xref rid="btab830-T4" ref-type="table">Table 4</xref>.</p>
      <table-wrap position="float" id="btab830-T4">
        <label>Table 4.</label>
        <caption>
          <p>B-ACC and W-AUC with SDs over five repeated train/test evaluations using all of the gene expression features of METABRIC and TCGA-HNCS</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="3" colspan="1">Method</th>
              <th colspan="6" rowspan="1">METABRIC<hr/></th>
              <th colspan="4" rowspan="1">TCGA-HNCS<hr/></th>
            </tr>
            <tr>
              <th colspan="2" rowspan="1">Distance relapse<hr/></th>
              <th colspan="2" rowspan="1">PAM50<hr/></th>
              <th colspan="2" rowspan="1">IC10<hr/></th>
              <th colspan="2" rowspan="1">Tumour grade<hr/></th>
              <th colspan="2" rowspan="1">2-year relapse-free survival<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
              <th rowspan="1" colspan="1">B-ACC</th>
              <th rowspan="1" colspan="1">W-AUC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">FC MLP</td>
              <td rowspan="1" colspan="1">56.92 ± 2.65</td>
              <td rowspan="1" colspan="1">0.57 ± 0.03</td>
              <td rowspan="1" colspan="1">74.65 ± 3.60</td>
              <td rowspan="1" colspan="1">0.94 ± 0.01</td>
              <td rowspan="1" colspan="1">66.32 ± 1.99</td>
              <td rowspan="1" colspan="1">0.95 ± 0.01</td>
              <td rowspan="1" colspan="1">34.29 ± 3.53</td>
              <td rowspan="1" colspan="1">0.66 ± 0.04</td>
              <td rowspan="1" colspan="1">58.14 ± 4.23</td>
              <td rowspan="1" colspan="1">0.58 ± 0.05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RC MLP-R</td>
              <td rowspan="1" colspan="1">56.91 ± 0.78</td>
              <td rowspan="1" colspan="1">0.57 ± 0.01</td>
              <td rowspan="1" colspan="1">72.06 ± 6.55</td>
              <td rowspan="1" colspan="1">0.93 ± 0.04</td>
              <td rowspan="1" colspan="1">57.25 ± 10.03</td>
              <td rowspan="1" colspan="1">0.92 ± 0.06</td>
              <td rowspan="1" colspan="1">38.02 ± 3.26</td>
              <td rowspan="1" colspan="1">0.64 ± 0.05</td>
              <td rowspan="1" colspan="1">54.86 ± 1.58</td>
              <td rowspan="1" colspan="1">0.54 ± 0.02</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RC MLP-M</td>
              <td rowspan="1" colspan="1">55.25 ± 1.56</td>
              <td rowspan="1" colspan="1">0.55 ± 0.02</td>
              <td rowspan="1" colspan="1">64.87 ± 8.79</td>
              <td rowspan="1" colspan="1">0.92 ± 0.05</td>
              <td rowspan="1" colspan="1">54.10 ± 6.68</td>
              <td rowspan="1" colspan="1">0.91 ± 0.04</td>
              <td rowspan="1" colspan="1">35.45 ± 2.45</td>
              <td rowspan="1" colspan="1">0.66 ± 0.01</td>
              <td rowspan="1" colspan="1">54.15 ± 1.87</td>
              <td rowspan="1" colspan="1">0.54 ± 0.02</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GINCCo + DPCLUS</td>
              <td rowspan="1" colspan="1">57.27 ± 1.80</td>
              <td rowspan="1" colspan="1">0.57 ± 0.02</td>
              <td rowspan="1" colspan="1">75.97 ± 4.59</td>
              <td rowspan="1" colspan="1">0.97 ± 0.01</td>
              <td rowspan="1" colspan="1">70.43 ± 3.68</td>
              <td rowspan="1" colspan="1">0.97 ± 0.00</td>
              <td rowspan="1" colspan="1">39.09 ± 9.96</td>
              <td rowspan="1" colspan="1">0.67 ± 0.03</td>
              <td rowspan="1" colspan="1">57.17 ± 4.42</td>
              <td rowspan="1" colspan="1">0.57 ± 0.04</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>From RC MLP-R results, we can see how on average a sparse randomly structured instantiation of a computational graph model does not outperform the FC model or GINCCo + DPCLUS, often performing significantly worse on multi-label tasks and with highly variable outputs. This suggests that the unguided random sparsification does not lead to better results. This is further compounded by the results from RC MLP-M which show that despite the preservation of the number of ‘complexes’ and connections of GINCCO + DPCLUS, the randomizations of the connections hurt the performance. Moreover, this suggests that the inductive biases offered by explicit factorizations of genes and protein complexes via validated biologically inspired clustering algorithms drastically reduce the number of model parameters, perform competitively and also enable guided <italic toggle="yes">post-hoc</italic> enrichment studies of target relevant functional modules, as we show next.</p>
      <p>A benefit of the deterministic and explicit factorization of the parametric activity functions of named protein complexes (and potentially other higher level modules such as pathways) presents interesting opportunities for introspective analyses of the models. Each of the candidate protein complexes may be functionally analysed through gene set enrichment analyses that can provide insights into the patterns of ‘active’ functional modules with respect to the input gene expressions and the disease phenotypes. A preliminary <italic toggle="yes">post-hoc</italic> analysis to identify functionally relevant complex candidates with trained GINCCo models is presented in the <xref rid="sup1" ref-type="supplementary-material">supplementary materials</xref> (<xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix SB</xref>). In particular, we leveraged Integrated Gradients (<xref rid="btab830-B35" ref-type="bibr">Sundararajan et al., 2017</xref>), a gradient-based attribution method, to estimate the importance of intermediate protein complex nodes in the computation of the target values. We then ranked the protein complexes according to their importance to the prediction task and performed functional enrichment analysis using Enrichr (DisGeNET) to identify enriched pathways. For classification of PAM50 on the METABRIC dataset with GINCCo + DPCLUS, we found that the top enriched pathways for the most important complex candidates are (i) malignant neoplasm of the breast (<italic toggle="yes">q</italic>-value: 2.4e-21) and (ii) breast carcinoma (<italic toggle="yes">q</italic>-value: 8.35e-21). These results suggest that the protein complexes identified by DPCLUS are biologically meaningful and further support our choice for incorporating them as structural inductive biases in our model. More generally, this result shows the potential of GINCCo to help identify functionally relevant gene-sets given specific phenotype targets and to enable their study through functional enrichment analyses.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Related work and discussion</title>
    <p>This work is focussed on the utilization of prior biological knowledge embedded within the topologies of interaction networks to guide the construction of predictive models. Therefore, it is related to several other approaches that incorporate inductive biases from the topologies of external molecular networks into neural networks (and other modelling approaches) as well as end-to-end differentiable models. More closely, GINCCo relates to Knowledge-Primed Neural Networks (KPNNs; <xref rid="btab830-B15" ref-type="bibr">Fortelny and Bock, 2020</xref>), that explicitly incorporate biological networks in the design of the neural network architecture. Similarly to GINCCo, the input nodes correspond to genes (or proteins), but the hidden units of the neural network correspond to various signalling proteins and transcription factors. This, in turn, leads to an accurate and interpretable predictive model for single-cell analysis. However, in order to produce such models, KPNNs require topological data in the form of directed acyclic graphs with explicitly defined regulatory mechanisms. In contrast, GINCCo is more general in this respect, since it is not constrained by the type nor completeness of the structural prior. This allows for incorporating (and combining) different topological data for various applications including, but not limited to single-cell analysis, such as cancer sub-type identification/classification.</p>
    <p>Other similar approaches have been proposed recently exploiting knowledge of biological pathways to create sparse neural network models. PASNet (<xref rid="btab830-B19" ref-type="bibr">Hao <italic toggle="yes">et al.</italic>, 2018</xref>) and P-NET (<xref rid="btab830-B13" ref-type="bibr">Elmarakeby <italic toggle="yes">et al.</italic>, 2020</xref>) incorporate pathway information for survival prediction in glioblastoma multiforme and for stratification of prostate cancer patients, respectively. These approaches are all closely related to GINCCo. However, P-NET requires careful handcrafted construction of the architecture as well as manual curation of certain layers. In contrast, GINCCo is more general, fully automated and leads to substantially smaller models. Moreover, the clustering step in GINCCo is independent; therefore, it can handle various types of domain-knowledge (including pathways). Similarly, PASNet refers to a sparse neural network that also relies on knowledge-based structural biases, by incorporating pathway information. In that, it is similar to GINCCo, however, instead of ‘learning’ the second hidden layer from the constructed clusters (as in GINCCo), PASNet explicitly maps the pathways. Therefore, in that respect, GINCCo is more general, since it does not explicitly rely on known pathway sets.</p>
    <p>In broader terms, GINCCo follows a long tradition of methods that incorporate biological knowledge through feature selection and extraction. In particular, it relates to embedded techniques (<xref rid="btab830-B21" ref-type="bibr">Hira and Gillies, 2015</xref>) that simultaneously select subsets of the original gene features and build a predictive model such as SVM-RFE (<xref rid="btab830-B18" ref-type="bibr">Guyon <italic toggle="yes">et al.</italic>, 2002</xref>) and LASSO (<xref rid="btab830-B26" ref-type="bibr">Ma <italic toggle="yes">et al.</italic>, 2007)</xref>. GINCCo distinguishes itself here in that it performs the selection and model construction in a completely automated, deterministic and unsupervised manner; this can be seen as a pre-processing step allowing GINCCo to scale immensely and study factor graphs without the influence of task-specific optimization dictating the shape of the models.</p>
    <p>Incorporating topological inductive biases can also be performed with network regularization methods as seen in <xref rid="btab830-B17" ref-type="bibr">Gustafsson <italic toggle="yes">et al.</italic> (2005)</xref>, <xref rid="btab830-B24" ref-type="bibr">Li and Li (2008</xref>) and <xref rid="btab830-B28" ref-type="bibr">Min <italic toggle="yes">et al.</italic> (2018)</xref>. Herein, methods such as graph Laplacian regularization work on regularizing the coefficients of linear models such that they are similar for terms that are connected within the incorporated network. We have included the method proposed by <xref rid="btab830-B24" ref-type="bibr">Li and Li (2008)</xref> within our comparative analysis in the previous section. A benefit of graph regularization as a method for incorporating prior knowledge is that it does not require a separate clustering stage as in GINCCo. However, this comes at the cost of not being able to study the potential gene sets (in our case protein complexes) for functional relevance, such as post-training analysis using functional enrichment analysis in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix SB</xref>. Furthermore, there is a subtle but important difference in the aims of our method and graph regularization methods in terms of the inductive bias produced. The graph Laplacian regularization is a summation of the smoothness terms on the variables to encourage similar coefficients on the genes that are connected. In contrast, GINNCo models are inductively biased (quite explicitly) to produce representations based on the subnetworks extracted by the clustering algorithms. Naturally, as graph regularization methods are typically implemented as a regularization term, they can be trivially incorporated into the objective function of GINCCo models as well.</p>
    <p>More generally, variations operating on the general network propagation model have found increasing use within research involving network biology (<xref rid="btab830-B9" ref-type="bibr">Cowen <italic toggle="yes">et al.</italic>, 2017</xref>). Parallel research took place within machine learning communities on graph neural networks which impose a graph constrained inductive bias onto the representations learned in neural networks mostly on social networks (<xref rid="btab830-B4" ref-type="bibr">Belkin and Niyogi, 2001</xref>; <xref rid="btab830-B11" ref-type="bibr">Defferrard <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btab830-B23" ref-type="bibr">Kipf and Welling, 2017</xref>). Such neural networks are characterized by <italic toggle="yes">graph convolutional</italic> operators that serve as useful inductive biases for learning representations of nodes and other graph substructures.</p>
    <p>The clear biological motivations (<xref rid="btab830-B9" ref-type="bibr">Cowen <italic toggle="yes">et al.</italic>, 2017)</xref> behind the network propagation model and its parallels to graph neural network(GNN) models quickly inspired a succession of works aimed at using GNNs architectures on gene expression data. <xref rid="btab830-B32" ref-type="bibr">Rhee <italic toggle="yes">et al.</italic> (2018</xref>) use a ChebNet (<xref rid="btab830-B11" ref-type="bibr">Defferrard <italic toggle="yes">et al.</italic>, 2016)</xref> variant with a relation network (<xref rid="btab830-B33" ref-type="bibr">Santoro <italic toggle="yes">et al.</italic>, 2017</xref>) to impose a PPI network upon each of the genomic profiles. Here, each of the gene expression values is mapped onto a copy of the PPI structure. This was used to classify genomic profiles from the TCGA into PAM50 classifications for breast cancer subtype classifications. <xref rid="btab830-B8" ref-type="bibr">Chereda <italic toggle="yes">et al.</italic> (2019)</xref> provided a simpler architecture solely using a ChebNet on the gene expression values mapped on a PPI network to predict metastasis. The published results on metastasis show that their proposed method is marginally better (1–2%) than their random forest and FC neural network baselines. This naturally raises the question of whether the positive performance published in <xref rid="btab830-B32" ref-type="bibr">Rhee <italic toggle="yes">et al.</italic> (2018</xref>)’s hybrid model comes primarily from their GNN or relational network component or the combination of both.</p>
    <p>A series of closely related research (<xref rid="btab830-B6" ref-type="bibr">Bertin <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btab830-B12" ref-type="bibr">Dutil <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btab830-B20" ref-type="bibr">Hashir <italic toggle="yes">et al.</italic>, 2019</xref>) has studied integrating various <italic toggle="yes">gene interaction networks</italic> such as PPI, gene regulatory, transcription regulation, etc. as masking measures over the features to impose an inductive bias. Experiments were carried out on single-gene inference tasks (<xref rid="btab830-B12" ref-type="bibr">Dutil <italic toggle="yes">et al.</italic>, 2018</xref>) and a cancer phenotype prediction task (<xref rid="btab830-B6" ref-type="bibr">Bertin <italic toggle="yes">et al.</italic>, 2019</xref>). The usage of the network information was deemed useful for the single-gene inference task, but also important negative results in some experiments where the prior knowledge of a curated graph was about as useful as a randomly connected graph was also reported—highlighting the importance of choosing the ‘right’ graph as prior knowledge. On the phenotype prediction task, using graphs as a mask over the gene expressions as prior knowledge was unable to beat a baseline multilayer perceptron on the same task (<xref rid="btab830-B6" ref-type="bibr">Bertin <italic toggle="yes">et al.</italic>, 2019</xref>).</p>
    <p>The work on applying GNNs to incorporate prior network information to genomic data tasks is a nascent and valid general approach to the problem. However, the differential graph convolution and pooling operations as used in previous work, are not best suited to learn biologically useful subnetworks for the predictive model within the small datasets that are available now. The classic graph convolutional operations used in <xref rid="btab830-B32" ref-type="bibr">Rhee <italic toggle="yes">et al.</italic> (2018)</xref> and <xref rid="btab830-B8" ref-type="bibr">Chereda <italic toggle="yes">et al.</italic> (2019)</xref> consider higher level node aggregations of all its neighbours with equal weight. When the nodes of the GNNs are genes superimposed onto a gene interaction graph (let us say a PPI network) the resulting node feature only consists of the gene expression scalar. The feature propagation mechanism between neighbours creates a bottleneck when every node aggregates messages from its neighbours (<xref rid="btab830-B1" ref-type="bibr">Alon and Yahav, 2020</xref>). Each of the scalars is simply mixed into another scalar value through the aggregation. Differentiably learned pooling methods require an increasing number of samples to learn ‘useful’ higher level representations, which are not explicitly related to a biologically relevant entities. Furthermore, pooling methods have recently been shown to have inherent limitations in actually capturing local receptive fields better than random cluster assignments (<xref rid="btab830-B27" ref-type="bibr">Mesquita <italic toggle="yes">et al.</italic>, 2020)</xref>.</p>
    <p>In contrast, the models created through our proposed framework forego learning ‘hidden’ higher level representations by explicitly factorizing the transitive relationship between gene expressions, protein complex activity, and phenotypes using PPI networks and deterministic protein complex discovery algorithms. This is done specifically to constrain the hypothesis space of potential models and impose structure using domain knowledge on the scarce data in the gene expression datasets. It relates each gene expression to a named higher level entity, the protein complex and has a function specific weighting that is learned (or set based on the practitioner) through the global optimization scheme over this computational graph. As a result, the signal from each gene expression is not equally weighted, but specific to each complex activity function—signals are even dropped explicitly through the <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> function if they are not within the scope of study for the computational graph. This is unlike the GNN or a network regularized method which would include all of the input and try to learn something from it even if it were noise. Thus, our method is substantially different and additive on both existing approaches.</p>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>We presented GINCCo, a scalable unsupervised approach to incorporating biological knowledge embedded in the structure of gene interaction networks for automated construction of computational graphs for gene expression analysis. GINCCo has several distinguishing properties. First, it provides a biologically relevant mechanism for model regularization, resulting in structurally constrained models that often yield better predictive performance whilst drastically reducing model parameters and enabling <italic toggle="yes">post-hoc</italic> enrichment analyses. Secondly, GINCCo is scalable and applicable to other tasks beyond the case study presented where explicitly modelling the activities of subnetworks within networks describing prior knowledge can be beneficial to a data analysis task. For example, the computational graphs can be seamlessly incorporated into larger integrative frameworks handling multiple modalities such as the integrative variational auto-encoders in <xref rid="btab830-B34" ref-type="bibr">Simidjievski <italic toggle="yes">et al.</italic> (2019)</xref> to reduce the complexity of its hypothesis space. Finally, there is no arbitrary decision making on the number of hidden nodes or their biological relevance as in standard MLPs. Each node within our computational graphs is either a gene, a phenotype, or a candidate protein complex. The structure describes a knowledge-directed factorization of the parametric function for the activity of a protein complex based on the expression levels of its constituent gene/proteins. This makes introspective study into the individual contributions and functional roles of entities in the model and patterns as a whole more amenable.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The data underlying this article are publicly available for METABRIC at <ext-link xlink:href="https://www.cbioportal.org/study/summary?id=brca_metabric" ext-link-type="uri">https://www.cbioportal.org/study/summary?id=brca_metabric</ext-link> and for TCGA-HNCS at <ext-link xlink:href="https://portal.gdc.cancer.gov/" ext-link-type="uri">https://portal.gdc.cancer.gov/</ext-link>. The TCGA-HNCS dataset version used in this article was derived from <xref rid="btab830-B31" ref-type="bibr">Rendleman <italic toggle="yes">et al.</italic> (2019)</xref> available in the public domain: <ext-link xlink:href="https://github.com/mrendleman/" ext-link-type="uri">https://github.com/mrendleman/</ext-link> MachineLearningTCGAHNSC-BINF/. The STRING 9606 Human PPI network is publicly available at <ext-link xlink:href="https://string-db.org/cgi/download" ext-link-type="uri">https://string-db.org/cgi/download</ext-link>. Additionally, any data supporting the conclusions of this article will be shared on reasonable request to the corresponding author. Source code to implementations is made available as in the availability statement on the title page.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>P.S. was funded by the W.D. Armstrong Fund from the School of Technology at the University of Cambridge. N.S., H.A.T, Z.S., M.J. and P.L. were funded by The Mark Foundation Institute for Integrated Cancer Medicine (MFICM). MFICM is hosted at the University of Cambridge, with funding from The Mark Foundation for Cancer Research (NY, USA) and the Cancer Research UK Cambridge Centre [C9685/A25177] (UK). </p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared. </p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab830_Supplementary_Data</label>
      <media xlink:href="btab830_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab830-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alon</surname><given-names>U.</given-names></string-name>, <string-name><surname>Yahav</surname><given-names>E.</given-names></string-name></person-group> (<year>2020</year>) <article-title>On the bottleneck of graph neural networks and its practical implications</article-title>. <source>ArXiv</source>, abs/2006.05205.</mixed-citation>
    </ref>
    <ref id="btab830-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Altaf-Ul-Amin</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2006</year>) <article-title>Development and implementation of an algorithm for detection of protein complexes in large interaction networks</article-title>. <source>BMC Bioinformatics</source>, <volume>7</volume>, <fpage>207</fpage>–<lpage>207</lpage>.<pub-id pub-id-type="pmid">16613608</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bader</surname><given-names>G.D.</given-names></string-name>, <string-name><surname>Hogue</surname><given-names>C.W.V.</given-names></string-name></person-group> (<year>2003</year>) <article-title>An automated method for finding molecular complexes in large protein interaction networks</article-title>. <source>BMC Bioinformatics</source>, <volume>4</volume>, <fpage>2</fpage>.PMC149346[pmcid].<pub-id pub-id-type="pmid">12525261</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B4">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Belkin</surname><given-names>M.</given-names></string-name>, <string-name><surname>Niyogi</surname><given-names>P.</given-names></string-name></person-group> (<year>2001</year>). Laplacian eigenmaps and spectral techniques for embedding and clustering. In: Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, <italic toggle="yes">NeurIPS’01</italic>, pp. <fpage>585</fpage>–<lpage>591</lpage>. MIT Press, Cambridge, MA, USA.</mixed-citation>
    </ref>
    <ref id="btab830-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benavoli</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis</article-title>. <source>J. Mach. Learn. Res</source>., <volume>18</volume>, <fpage>2653</fpage>–<lpage>2688</lpage>.</mixed-citation>
    </ref>
    <ref id="btab830-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bertin</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Analysis of gene interaction graphs for biasing machine learning models</article-title>. <source>arXiv: Genomics</source>, abs/1905.02295.</mixed-citation>
    </ref>
    <ref id="btab830-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cawley</surname><given-names>G.C.</given-names></string-name>, <string-name><surname>Talbot</surname><given-names>N.L.C.</given-names></string-name></person-group> (<year>2006</year>) <article-title>Gene selection in cancer classification using sparse logistic regression with Bayesian regularization</article-title>. <source>Bioinformatics</source>, <volume>22</volume>, <fpage>2348</fpage>–<lpage>2355</lpage>.<pub-id pub-id-type="pmid">16844704</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chereda</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Utilizing molecular network information via graph convolutional neural networks to predict metastatic event in breast cancer</article-title>. <source>Stud Health Technol. Inform</source>., <volume>267</volume>, <fpage>181</fpage>–<lpage>186</lpage>.<pub-id pub-id-type="pmid">31483271</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cowen</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Network propagation: a universal amplifier of genetic associations</article-title>. <source>Nat. Rev. Genet</source>., <volume>18</volume>, <fpage>551</fpage>–<lpage>562</lpage>.<pub-id pub-id-type="pmid">28607512</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Curtis</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal>; METABRIC Group. (<year>2012</year>) <article-title>The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups</article-title>. <source>Nature</source>, <volume>486</volume>, <fpage>346</fpage>–<lpage>352</lpage>.<pub-id pub-id-type="pmid">22522925</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Defferrard</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>). Convolutional neural networks on graphs with fast localized spectral filtering. In: <italic toggle="yes">Proceedings of the 30th International Conference on Neural Information Processing Systems, NeurIPS’16</italic>, pp. 3844–3852. Curran Associates Inc, USA.</mixed-citation>
    </ref>
    <ref id="btab830-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Dutil</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>). Towards gene expression convolutions using gene interaction graphs. In: <italic toggle="yes">International Conference on Machine Learning 2017 (ICML'17) Workshop on Computational Biology (WCB'17)</italic></mixed-citation>
    </ref>
    <ref id="btab830-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elmarakeby</surname>  <given-names>H.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Biologically informed deep neural network for prostate cancer classification and discovery</article-title>. <source><italic toggle="yes">Nature </italic>598, 348–352 (2021). https://doi.org/10.1038/s41586-021-03922-4</source></mixed-citation>
    </ref>
    <ref id="btab830-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteva</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>A guide to deep learning in healthcare</article-title>. <source>Nat. Med</source>., <volume>25</volume>, <fpage>24</fpage>–<lpage>29</lpage>.<pub-id pub-id-type="pmid">30617335</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fortelny</surname><given-names>N.</given-names></string-name>, <string-name><surname>Bock</surname><given-names>C.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Knowledge-primed neural networks enable biologically interpretable deep learning on single-cell sequencing data</article-title>. <source>Genome Biol</source>., <volume>21</volume>, <fpage>190</fpage>.<pub-id pub-id-type="pmid">32746932</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glorot</surname><given-names>X.</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group> (<year>2010</year>) <article-title>Understanding the difficulty of training deep feedforward neural networks</article-title>. <source>J. Mach. Learn. Res. Proc. Track</source>, <volume>9</volume>, <fpage>249</fpage>–<lpage>256</lpage>.</mixed-citation>
    </ref>
    <ref id="btab830-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gustafsson</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2005</year>) <article-title>Constructing and analyzing a large-scale gene-to-gene regulatory network lasso-constrained inference and biological validation</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinformatics</source>, <volume>2</volume>, <fpage>254</fpage>–<lpage>261</lpage>.</mixed-citation>
    </ref>
    <ref id="btab830-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guyon</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2002</year>) <article-title>Gene selection for cancer classification using support vector machines</article-title>. <source>Mach. Learn</source>., <volume>46</volume>, <fpage>389</fpage>–<lpage>422</lpage>.</mixed-citation>
    </ref>
    <ref id="btab830-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hao</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Pasnet: pathway-associated sparse deep neural network for prognosis prediction from high-throughput data</article-title>. <source>BMC Bioinformatics</source>, <volume>19</volume>, <fpage>510</fpage>.<pub-id pub-id-type="pmid">30558539</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hashir</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Is graph biased feature selection of genes better than random?</article-title>  <source>ArXiv</source>, abs/1910.09600.</mixed-citation>
    </ref>
    <ref id="btab830-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hira</surname><given-names>Z.M.</given-names></string-name>, <string-name><surname>Gillies</surname><given-names>D.F.</given-names></string-name></person-group> (<year>2015</year>) <article-title>A review of feature selection and feature extraction methods applied on microarray data</article-title>. <source>Adv. Bioinformatics</source>, <volume>2015</volume>, <fpage>198363</fpage>.<pub-id pub-id-type="pmid">26170834</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J.</given-names></string-name></person-group> (<year>2015</year>). Adam: a method for stochastic optimization. <italic toggle="yes">International Conference on Learning Representations 2015 (ICLR'15), OpenReview.net, USA.</italic></mixed-citation>
    </ref>
    <ref id="btab830-B23">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kipf</surname><given-names>T.N.</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M.</given-names></string-name></person-group> (<year>2017</year>). Semi-supervised classification with graph convolutional networks. In: <italic toggle="yes">Proceedings of the 5th International Conference on Learning Representations</italic>(ICLR’17), OpenReview.net, France.</mixed-citation>
    </ref>
    <ref id="btab830-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>C.</given-names></string-name>, <string-name><surname>Li</surname><given-names>H.</given-names></string-name></person-group> (<year>2008</year>) <article-title>Network-constrained regularization and variable selection for analysis of genomic data</article-title>. <source>Bioinformatics</source>, <volume>24</volume>, <fpage>1175</fpage>–<lpage>1182</lpage>.<pub-id pub-id-type="pmid">18310618</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2008</year>) <article-title>Modifying the dpclus algorithm for identifying protein complexes based on new topological structures</article-title>. <source>BMC Bioinformatics</source>, <volume>9</volume>, <fpage>398</fpage>.<pub-id pub-id-type="pmid">18816408</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2007</year>) <article-title>Supervised group lasso with applications to microarray data analysis</article-title>. <source>BMC Bioinformatics</source>, <volume>8</volume>, <fpage>60</fpage>.<pub-id pub-id-type="pmid">17316436</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mesquita</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>). Rethinking pooling in graph neural networks. In: Advances in Neural Information Processing Systems (NeurIPS).</mixed-citation>
    </ref>
    <ref id="btab830-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Min</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Network-regularized sparse logistic regression models for clinical risk prediction and biomarker discovery</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinformatics</source>, <volume>15</volume>, <fpage>944</fpage>–<lpage>953</lpage>.</mixed-citation>
    </ref>
    <ref id="btab830-B29">
      <mixed-citation publication-type="journal">Cancer Genome Atlas Network. (<year>2015</year>) <article-title>Comprehensive genomic characterization of head and neck squamous cell carcinomas</article-title>. <source>Nature</source>, <volume>517</volume>, <fpage>576</fpage>–<lpage>582</lpage>.<pub-id pub-id-type="pmid">25631445</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prat</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>Phenotypic and molecular characterization of the claudin-low intrinsic subtype of breast cancer</article-title>. <source>Breast Cancer Res</source>., <volume>12</volume>, <fpage>R68</fpage>.<pub-id pub-id-type="pmid">20813035</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rendleman</surname><given-names>M.C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Machine learning with the TCGA-HNSC dataset: improving usability by addressing inconsistency, sparsity, and high-dimensionality</article-title>. <source>BMC Bioinformatics</source>, <volume>20</volume>, <fpage>339</fpage>.<pub-id pub-id-type="pmid">31208324</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B32">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rhee</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>). Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification. In: <italic toggle="yes">Proceedings of the 27th International Joint Conference on Artificial Intelligence</italic>, IJCAI’18, AAAI Press, USA. pp. <fpage>3527</fpage>–<lpage>3534</lpage>.</mixed-citation>
    </ref>
    <ref id="btab830-B33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Santoro</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>). <part-title>A simple neural network module for relational reasoning</part-title>. In <source><italic toggle="yes">Proceedings of the 31st International Conference on Neural Information Processing Systems</italic></source> (<italic toggle="yes">NIPS'17</italic>). Curran Associates Inc., Red Hook, NY, USA, 4974–4983.</mixed-citation>
    </ref>
    <ref id="btab830-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Simidjievski</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Variational autoencoders for cancer data integration: design principles and computational practice</article-title>. <source>Front. Genet</source>., <volume>10</volume>, <fpage>1205</fpage>.<pub-id pub-id-type="pmid">31921281</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sundararajan</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Axiomatic attribution for deep networks</article-title>. In <source><italic toggle="yes">Proceedings of the 34th International Conference on Machine Learning - Volume 70</italic></source> (<italic toggle="yes">ICML'17</italic>). pp. 3319–3328.</mixed-citation>
    </ref>
    <ref id="btab830-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szklarczyk</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>String v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D607</fpage>– <lpage>D613</lpage>.<pub-id pub-id-type="pmid">30476243</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) <article-title>A core-attachment based method to detect protein complexes in ppi networks</article-title>. <source>BMC Bioinformatics</source>, <volume>10</volume>, <fpage>169</fpage>–<lpage>169</lpage>. 1471-2105-10-169[PII].<pub-id pub-id-type="pmid">19486541</pub-id></mixed-citation>
    </ref>
    <ref id="btab830-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Network-based machine learning and graph theory algorithms for precision oncology</article-title>. <source>NPJ Precis. Oncol</source>., <volume>1</volume>, <fpage>25</fpage>.<pub-id pub-id-type="pmid">29872707</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
