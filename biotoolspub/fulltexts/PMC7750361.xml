<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Plant Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Plant Sci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Plant Sci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Plant Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-462X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7750361</article-id>
    <article-id pub-id-type="doi">10.3389/fpls.2020.541960</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Plant Science</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TasselNetV2+: A Fast Implementation for High-Throughput Plant Counting From High-Resolution RGB Imagery</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lu</surname>
          <given-names>Hao</given-names>
        </name>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1149459/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cao</surname>
          <given-names>Zhiguo</given-names>
        </name>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/769885/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology</institution>, <addr-line>Wuhan</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Roger Deal, Emory University, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Emanuel Peres, University of Trás-os-Montes and Alto Douro, Portugal; Jiwan Han Han, Aberystwyth University, United Kingdom</p>
      </fn>
      <corresp id="c001">*Correspondence: Zhiguo Cao <email>zgcao@hust.edu.cn</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Technical Advances in Plant Science, a section of the journal Frontiers in Plant Science</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>07</day>
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>541960</elocation-id>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>3</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>11</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2020 Lu and Cao.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Lu and Cao</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Plant counting runs through almost every stage of agricultural production from seed breeding, germination, cultivation, fertilization, pollination to yield estimation, and harvesting. With the prevalence of digital cameras, graphics processing units and deep learning-based computer vision technology, plant counting has gradually shifted from traditional manual observation to vision-based automated solutions. One of popular solutions is a state-of-the-art object detection technique called Faster R-CNN where plant counts can be estimated from the number of bounding boxes detected. It has become a standard configuration for many plant counting systems in plant phenotyping. Faster R-CNN, however, is expensive in computation, particularly when dealing with high-resolution images. Unfortunately high-resolution imagery is frequently used in modern plant phenotyping platforms such as unmanned aerial vehicles, engendering inefficient image analysis. Such inefficiency largely limits the throughput of a phenotyping system. The goal of this work hence is to provide an effective and efficient tool for high-throughput plant counting from high-resolution RGB imagery. In contrast to conventional object detection, we encourage another promising paradigm termed object counting where plant counts are directly regressed from images, without detecting bounding boxes. In this work, by profiling the computational bottleneck, we implement a fast version of a state-of-the-art plant counting model TasselNetV2 with several minor yet effective modifications. We also provide insights why these modifications make sense. This fast version, TasselNetV2+, runs an order of magnitude faster than TasselNetV2, achieving around 30 fps on image resolution of 1980 × 1080, while it still retains the same level of counting accuracy. We validate its effectiveness on three plant counting tasks, including wheat ears counting, maize tassels counting, and sorghum heads counting. To encourage the use of this tool, our implementation has been made available online at <ext-link ext-link-type="uri" xlink:href="https://tinyurl.com/TasselNetV2plus">https://tinyurl.com/TasselNetV2plus</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>plant counting</kwd>
      <kwd>real-time processing</kwd>
      <kwd>wheat ears</kwd>
      <kwd>maize tassels</kwd>
      <kwd>sorghum heads</kwd>
      <kwd>pytorch implementation</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">National Natural Science Foundation of China<named-content content-type="fundref-id">10.13039/501100001809</named-content></funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="11"/>
      <table-count count="4"/>
      <equation-count count="3"/>
      <ref-count count="63"/>
      <page-count count="15"/>
      <word-count count="8861"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Plant counting runs through almost every critical stage in agricultural production spreading from seed breeding (Wiles and Schweizer, <xref rid="B56" ref-type="bibr">1999</xref>; Mussadiq et al., <xref rid="B39" ref-type="bibr">2015</xref>; Guo et al., <xref rid="B13" ref-type="bibr">2018</xref>), germination (Baofeng et al., <xref rid="B2" ref-type="bibr">2016</xref>; Primicerio et al., <xref rid="B44" ref-type="bibr">2017</xref>), cultivation (Yu et al., <xref rid="B60" ref-type="bibr">2013</xref>; Liu et al., <xref rid="B26" ref-type="bibr">2018</xref>), fertilization (Vos and Frinking, <xref rid="B55" ref-type="bibr">1997</xref>; Boissard et al., <xref rid="B3" ref-type="bibr">2008</xref>), pollination (Guo et al., <xref rid="B12" ref-type="bibr">2015</xref>; Lu et al., <xref rid="B28" ref-type="bibr">2017a</xref>; Sadeghi-Tehran et al., <xref rid="B48" ref-type="bibr">2017</xref>), to yield estimation (Nuske et al., <xref rid="B41" ref-type="bibr">2014</xref>; Ghosal et al., <xref rid="B8" ref-type="bibr">2019</xref>; Zabawa et al., <xref rid="B61" ref-type="bibr">2019</xref>), and harvesting (Häni et al., <xref rid="B15" ref-type="bibr">2019</xref>; Jin et al., <xref rid="B19" ref-type="bibr">2019</xref>). It also plays an important role in phenotyping functional traits of plants because many traits of interest are quantity-related, such as density (Madec et al., <xref rid="B37" ref-type="bibr">2019</xref>) and the number of leaves (Giuffrida et al., <xref rid="B10" ref-type="bibr">2015</xref>). This task is typically addressed with manual efforts in traditional agriculture. Manual counting, however, is subjective, tedious, error-prone, labor-intensive and inefficient due to fatigue of humans. Indeed agricultural practitioners have tried to automate this task over past decades (McDonald and Chen, <xref rid="B38" ref-type="bibr">1990</xref>; Gomes and Leta, <xref rid="B11" ref-type="bibr">2012</xref>; Kamilaris and Prenafeta-Boldú, <xref rid="B20" ref-type="bibr">2018</xref>). Unfortunately this goal is not that easy to achieve due to versatile varieties of plants and intrinsic/extrinsic variations in reality. An automated plant counting system therefore is often limited to a controlled environment or a certain application scenario such that manual counting still takes place in most regions of the world.</p>
    <p>With the prevalence of low-end digital cameras, high-performance graphics processing units (GPUs) and effective deep learning-based technology, computer vision has received much attention in plant counting due to increased reliability and decreased costs. Plant counting has thus gradually shifted from traditional manual counting to vision-based automated solutions. The most popular solution in plant counting comes from the success of a widely-used object detection framework called Faster Region-based Convolutional Neural Network (Faster R-CNN) (Ren et al., <xref rid="B47" ref-type="bibr">2015</xref>). Faster R-CNN leverages a so-called region proposal network to identify potential object locations specified by bounding boxes, then passes these boxes into a classifier to assign object labels and confidence scores, and finally suppresses overlapped boxes per the confidence scores with a non-maximum suppression operator. The population of plants can be easily inferred from the number of bounding boxes detected. Faster R-CNN has been substantially applied to plant science and agriculture engineering communities to, for example, estimate ear density (Madec et al., <xref rid="B37" ref-type="bibr">2019</xref>), detect maize tassels (Liu et al., <xref rid="B27" ref-type="bibr">2020</xref>), localize sweet pepper (Halstead et al., <xref rid="B14" ref-type="bibr">2018</xref>), identify crop seedlings (Quan et al., <xref rid="B45" ref-type="bibr">2019</xref>), etc. However, it is expensive in computation due to the use of high-capacity ImageNet-pretrained models (Deng et al., <xref rid="B7" ref-type="bibr">2009</xref>), such as VGG-16 (Simonyan and Zisserman, <xref rid="B49" ref-type="bibr">2014</xref>) and ResNet (He et al., <xref rid="B16" ref-type="bibr">2016</xref>), especially when dealing with high-resolution images. To acquire sufficient spatial resolution, high-resolution imagery, unfortunately, cannot be avoided in modern plant phenotyping platforms such as unmanned aerial vehicles. The problem is that it is intractable to directly train/test high-resolution images with Faster R-CNN due to GPU memory limitation. It has been reported in Madec et al. (<xref rid="B37" ref-type="bibr">2019</xref>) that the maximum image size acceptable for training Faster R-CNN is about 500 × 500 pixels. To address this, pre-splitting images becomes a common practice during both training and inference, rendering inefficient image analysis. For instance, according to Madec et al. (<xref rid="B37" ref-type="bibr">2019</xref>), the inference of around 100 high-resolution images can take more than 1 h. Such inefficiency largely limits the throughput of phenotyping. In modern high-throughput plant phenotyping systems, it is important that an image analysis tool can process high-resolution images within a short period of time.</p>
    <p>In this paper, we advocate another promising plant counting paradigm—object counting. Instead of detecting object bounding boxes, object counting directly regresses object counts from an image. This is a much direct way when only the population of objects is concerned. Indeed the transductive principle suggests never to solve a harder problem than the target application necessities (Vapnik, <xref rid="B53" ref-type="bibr">1998</xref>)—estimating object counts does not have to localize where objects are. Compared with object detection, object counting has many appealing advantages, for instance: (i) cheap manual annotations: learning object counting models only requires dotted annotations, rather than more expensive bounding boxes annotations used in object detection; (ii) simplified network architectures: object detection generally builds on multi-scale architectures such as feature pyramid networks (Lin et al., <xref rid="B24" ref-type="bibr">2017</xref>; Tan et al., <xref rid="B51" ref-type="bibr">2019</xref>) that have extensive decoding stages, while object counting, especially for local count regression models (Lu et al., <xref rid="B32" ref-type="bibr">2017c</xref>; Xiong et al., <xref rid="B58" ref-type="bibr">2019a</xref>), only needs an encoder; (iii) robust to partially overlapping instances: object detection tends to under-estimate object counts due to the existence of non-maximum suppression where partially overlapping instances are likely to be suppressed, while object counting naturally takes overlapping instances into account during ground-truth generation; and (iv) light-weight computational requirement: a light-weight object counting model trained from scratch can deliver sufficiently accurate counting accuracy, while object detection models generally require ImageNet-pretrained models, with also large GPU memory consumption.</p>
    <p>In fact, object counting is a long-standing topic in computer vision. It can at least date back to early 2000s when counting is still a by-product of face/pedestrian detectors (Viola and Jones, <xref rid="B54" ref-type="bibr">2001</xref>; Dalal and Triggs, <xref rid="B6" ref-type="bibr">2005</xref>). Object counting then is gradually accepted as an independent research topic after the first counting-by-regression approach (Chan et al., <xref rid="B4" ref-type="bibr">2008</xref>) appears where the global object count can be regressed from an image. The idea of counting by regression is further amplified by Lempitsky and Zisserman (<xref rid="B21" ref-type="bibr">2010</xref>) who introduce the concept of the density map. The density map is generated from dotted annotations with Gaussian smoothing such that each pixel is assigned with a value that corresponds to the object density, which transforms counting into a dense prediction problem (Lu et al., <xref rid="B33" ref-type="bibr">2019</xref>, <xref rid="B34" ref-type="bibr">2020</xref>). It has become the basic building block for many object counting models (Chen et al., <xref rid="B5" ref-type="bibr">2013</xref>; Arteta et al., <xref rid="B1" ref-type="bibr">2014</xref>) including recent deep counting networks (Zhang et al., <xref rid="B62" ref-type="bibr">2015</xref>, <xref rid="B63" ref-type="bibr">2016</xref>; Sindagi and Patel, <xref rid="B50" ref-type="bibr">2017</xref>; Li et al., <xref rid="B22" ref-type="bibr">2018</xref>; Liu et al., <xref rid="B25" ref-type="bibr">2020</xref>; Ma et al., <xref rid="B36" ref-type="bibr">2019</xref>; Xiong et al., <xref rid="B59" ref-type="bibr">2019b</xref>). Most state-of-the-art counting networks, however, are also inefficient due to the use of pretrained VGG-16, which hinders their applicability in high-resolution imagery in plant counting. In plant science community, many attempts have also been made for direct counting by regression (Giuffrida et al., <xref rid="B10" ref-type="bibr">2015</xref>, <xref rid="B9" ref-type="bibr">2018</xref>; Rahnemoonfar and Sheppard, <xref rid="B46" ref-type="bibr">2017</xref>; Wu et al., <xref rid="B57" ref-type="bibr">2019</xref>). In particular, in our previous work we propose TasselNet (Lu et al., <xref rid="B32" ref-type="bibr">2017c</xref>), a counting network based on the idea of local count regression, to count in-field maize tassels, demonstrating that even a low-capacity network can achieve reasonably good counting accuracy. We remark that, the idea of local count regression is particularly suitable for counting plants, because this paradigm is robust to size variations of plants. Such robustness is important because a plant per se is a self-changing system such that its physical size varies over time. Xiong et al. (<xref rid="B58" ref-type="bibr">2019a</xref>) further extends TasselNet to TasselNetV2 and applies this new version to wheat spikes counting. We observe that TasselNetV2 turns out to be a generic tool for plant counting and even achieves comparable accuracy in crowd counting against state-of-the-art deep counting networks in computer vision. Unfortunately both TasselNet and TasselNetV2 are only implemented in a research-orientated software, i.e., MATLAB, making them infeasible for practical deployment<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>.</p>
    <p>In this work, we implement a fast version of TasselNetV2, TasselNetV2+, based on PyTorch (Paszke et al., <xref rid="B43" ref-type="bibr">2019</xref>). By profiling the computational bottleneck, we make several minor yet effective modifications to TasselNetV2 to improve its efficiency. These modifications are based on a novel framework view of TasselNetV2, which decomposes TasselNetV2 into an encoder, a counter and a normalizer, allowing module-specific optimization and diagnosis. In particular, we find the main computational bottleneck of TasselNetV2 lies in the poor implementation of the normalizer. We address this issue with a novel mathematically-equivalent reformulation that enables an efficient GPU-based implementation. In addition, we notice a large portion of model parameters are included in the first convolutional layer of the counter, which also introduces many floating-point calculations. Inspired by a common practice in image classification (Lin et al., <xref rid="B23" ref-type="bibr">2013</xref>; He et al., <xref rid="B16" ref-type="bibr">2016</xref>), we make the same observation that the first convolutional layer of the counter can be safely replaced with global average pooling without performance loss. This simple modification significantly reduces model parameters, improves efficiency, and more importantly, enables flexible adaptation to different object sizes. Further, we also slightly improve the efficiency of the encoder by moving forward the last downsampling layer. Such a modification enlarges the receptive field (RF) by 17% so that extra context can be seen by the network. Altogether these modifications significantly improve the efficiency of TasselNetV2 by more than an order of magnitude, achieving around 30 fps on image resolution of 1980 × 1080 (tested on a low-end GTX1070 GPU), as shown in <xref ref-type="fig" rid="F1">Figure 1</xref>. More importantly, these modifications have no negative effect on counting accuracy. To encourage the use of this tool, we has released our implementation online. We believe TasselNetV2+ will facilitate many counting-related tasks in plant phenotyping systems. In short, we make the following contributions:</p>
    <list list-type="bullet">
      <list-item>
        <p>TasselNetV2+: a fast version of TasselNetV2 with significant optimization in efficiency;</p>
      </list-item>
      <list-item>
        <p>A framework view of TasselNetV2 as a concatenation of an encoder, a counter and a normalizer, which allows module-specific optimization and diagnosis;</p>
      </list-item>
      <list-item>
        <p>A novel reformulation of local-count normalization that enables an efficient GPU-based implementation.</p>
      </list-item>
    </list>
    <fig id="F1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>The number of processed frames per second with different image resolution. TasselNetV2+ is an order of magnitude faster than TasselNetV2. Frames per second are averaged over 100 independent trials on random input tested on GTX 1070 GPU, i7-8700 CPU, and 16 GB RAM.</p>
      </caption>
      <graphic xlink:href="fpls-11-541960-g0001"/>
    </fig>
  </sec>
  <sec id="s2">
    <title>2. Datasets and Methods</title>
    <sec>
      <title>2.1. Plant Counting Datasets</title>
      <p>Since the focus of this work is on the methodology part, we leverage three publicly available plant counting datasets in our evaluation.</p>
      <p>The Wheat Ears Detection (WED) dataset was collected in France with a wheat field phenotyping platform using a Sony ILCE-6000 digital camera in 2017. Images were captured from a trial of 120 2 × 10 m microplots with 20 contrasting genotypes at 2.9 m distance to the ground. The image resolution was 6, 000 × 4, 000. The number of ears in each image varied from 80 to 170. The dataset included 236 images. 30, 729 wheat ears were identified and manually annotated with bounding boxes. More details about the dataset can be found in Madec et al. (<xref rid="B37" ref-type="bibr">2019</xref>).</p>
      <p>The Maize Tassels Counting (MTC) dataset was collected from four experimental fields across China between 2010 and 2015 with 6 different maize cultivars. The images were captured from a 5-meter-height imaging device with a CCD digital camera (E450 Olympus). The image resolutions were 3648 × 2736, 4272 × 2848 and 3456 × 2304. The dataset had 361 images, with 186 training images and 175 testing images. The number of maize tassels varied from 0 to around 100. Each maize tassel was manually annotated with a single dot. More details can be found in Lu et al. (<xref rid="B32" ref-type="bibr">2017c</xref>).</p>
      <p>The Sorghum Heads Counting (SHC) dataset was collected from a trail with 1440 plots in Australia during the 2015–2016 growing season. The images were captured using an unmanned aircraft vehicle at flight heights of 20 m and a flight speed of 3 m/s with a commercial RGB camera. The resolution of the camera was 5472 × 3648. In the released dataset, there were two subsets called “dataset1” and “dataset2” with 52 cropped images and 40 post-processed images, respectively. The cropped image resolution in dataset1 was 1154 × 1731. Forty processed images were of varied resolutions. These two subsets were chosen because only they were labeled with dotted annotations. More details can be found in Guo et al. (<xref rid="B13" ref-type="bibr">2018</xref>).</p>
      <p>Some example images of the three plant counting datasets are illustrated in <xref ref-type="fig" rid="F2">Figure 2</xref>.</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Example images on three plant counting datasets. Panels <bold>(A,B)</bold> are from the Wheat Ears Detection (WED) dataset, panels <bold>(C,D)</bold> are from the Maize Tassels Counting (MTC) dataset, and panels <bold>(E–H)</bold> are from the Sorghum Heads Counting (SHC) dataset.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0002"/>
      </fig>
    </sec>
    <sec>
      <title>2.2. Recapping TasselNetV2</title>
      <p>As the baseline of this work, here we first recap TasselNetV2 (Xiong et al., <xref rid="B58" ref-type="bibr">2019a</xref>). TasselNetV2 extends TasselNet (Lu et al., <xref rid="B32" ref-type="bibr">2017c</xref>)—the simplest implementation of local count regression, i.e., learning a mapping from local image features to local region counts. TasselNetV2 is inspired by an observation that the theoretical RF is wasted in TasselNet such that TasselNet is weak in modeling context. It addresses this issue by changing all fully-connected layers into convolutional ones to allow arbitrary sizes of input. Instead of sampling and operating on small image patches, TasselNetV2 processes full images. In this way, hidden RF can be freed to benefit some plant counting tasks where context is an important cue, such as wheat spikes counting (Xiong et al., <xref rid="B58" ref-type="bibr">2019a</xref>).</p>
      <p>The network architecture of TasselNetV2 is shown in <xref ref-type="fig" rid="F3">Figure 3</xref>. It includes 7 convolutional layers and 3 max pooling layers. Concretely, it is defined by <bold>C</bold><sub>3</sub>(16)-<bold>M</bold>-<bold>C</bold><sub>3</sub>(32)-<bold>M</bold>-<bold>C</bold><sub>3</sub>(64)-<bold>C</bold><sub>3</sub>(64)-<bold>C</bold><sub>3</sub>(64)-<bold>M</bold>-<bold>C</bold><sub>8</sub>(128)-<bold>C</bold><sub>1</sub>(128)-<bold>C</bold><sub>1</sub>(1), where <bold>C</bold><sub><italic>k</italic></sub>(<italic>m</italic>) denotes a 2D convolutional layer with <italic>m</italic>-channel <italic>k</italic> × <italic>k</italic> filters, followed by batch normalization (BN) (Ioffe and Szegedy, <xref rid="B18" ref-type="bibr">2015</xref>) and ReLU (Nair and Hinton, <xref rid="B40" ref-type="bibr">2010</xref>), and <bold>M</bold> is a 2-stride max pooling operator with 2 × 2 kernel size. The last <bold>C</bold>(1) is the prediction layer where BN and ReLU are not included.</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>A framework view of TasselNetV2. Given an input image, TasselNetV2 processes it through an encoder with a few convolutional and downsampling layers, passes it into a counter to regress local counts, and finally, normalizes the local counts with a normalizer to generate the final output.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0003"/>
      </fig>
      <p>In local count regression, an image is mapped to a (redundant) count map where each local count in the count map corresponds to a <italic>r</italic> × <italic>r</italic> local region. The relative order between <italic>r</italic> and the output stride <italic>s</italic> determines whether the count map is redundant. Note that <italic>r</italic> ≥ <italic>s</italic>. The count map is redundant when <italic>r</italic> &gt; <italic>s</italic>, because in this case every two adjacent local regions have a <inline-formula><mml:math id="M1"><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> overlap. Only when <italic>r</italic> = <italic>s</italic> that the overlap disappears. According to the network definition above, <italic>r</italic> = 64 and <italic>s</italic> = 8 in TasselNetV2, so the resulting count map is redundant. A normalizer must follow for de-redundancy such that the sum of the final normalized count map can reflect the image count exactly. We call <italic>r</italic> × <italic>r</italic> the <italic>base input size</italic> of the network. The base input size is only related to the network architecture. This is a different concept from the input image size that can be arbitrarily large in theory. For example, given an input <bold><italic>I</italic></bold> ∈ ℝ<sup><italic>r</italic>×<italic>r</italic>×3</sup>, TasselNetV2 defines a transformation <italic>f</italic> such that <italic>f</italic>(<bold><italic>I</italic></bold>):ℝ<sup><italic>r</italic>×<italic>r</italic>×3</sup> → ℝ; if <bold><italic>I</italic></bold>′ ∈ ℝ<sup><italic>H</italic>×<italic>W</italic>×3</sup> where <italic>H, W</italic> ≫ <italic>r</italic> and are assumed to be divisible by <italic>s</italic>, then <inline-formula><mml:math id="M2"><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>I</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math></inline-formula>. This suggests the output size of the count map is irrelevant to the base input size when the input image size is larger than the base input size. We will use this concept extensively throughout this paper.</p>
    </sec>
    <sec>
      <title>2.3. Profiling Computational Bottlenecks</title>
      <p>Despite TasselNetV2 exhibits remarkable counting performance on counting maize tassels and wheat spikes (Xiong et al., <xref rid="B58" ref-type="bibr">2019a</xref>), its efficiency does not meet the requirement of high-throughput high-resolution image analysis (<xref ref-type="fig" rid="F1">Figure 1</xref>). It is thus natural to consider whether there is room for efficiency improvement. Before optimization, a prerequisite is to figure out where the computational bottleneck is.</p>
      <p>From <xref ref-type="fig" rid="F3">Figure 3</xref>, an important insight of this work is that, by decomposing the architecture, TasselNetV2 can be viewed as a concatenation of an encoder, a counter and a normalizer: the encoder specializes in encoding the image representation; the counter maps the image representation to the local count; and the normalizer normalizes redundant local counts and outputs the final image-level count. Such decomposition is essential to allow module-specific diagnosis and profiling.</p>
      <p>Given the framework view of TasselNetV2, we profile the time usage of each module in detail. The profiling results are shown in <xref ref-type="fig" rid="F4">Figure 4</xref>. We surprisingly find that most of time consumption comes from the normalizer, and its occupancy even increases with increased image resolution. Since the bottleneck is found, the next step is to figure out why it wastes so much time. In what follows, we discuss this problem and our solution in detail.</p>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Time profiling of TasselNetV2. It is clear that the main computational bottleneck lies in the normalizer. From low resolution to high resolution, the normalizer takes up 91.72, 93.31, 93.62, 94.25% of the total processing time, respectively.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0004"/>
      </fig>
    </sec>
    <sec>
      <title>2.4. Reformulating Local-Count Normalizer</title>
      <p>Let us first elaborate on how the normalizer works. As aforementioned, given an input image <bold><italic>I</italic></bold> ∈ ℝ<sup><italic>H</italic>×<italic>W</italic>×3</sup>, TasselNetV2 produces a redundant count map <inline-formula><mml:math id="M3"><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>C</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math></inline-formula>. To remove redundancy, a normalizer is followed to generate a normalized count map <inline-formula><mml:math id="M4"><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>C</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. Notice that the spatial resolution is first reduced by <italic>s</italic> times and then recovered to the input resolution. TasselNetV2 achieves this by averaging each local count value <italic>c</italic> ∈ <bold><italic>C</italic></bold><sub><italic>r</italic></sub> into to a <italic>r</italic> × <italic>r</italic> region, i.e., each element of the <italic>r</italic> × <italic>r</italic> region is assigned with an averaged count of <inline-formula><mml:math id="M5"><mml:mfrac><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></inline-formula> (the sum of the local region still equals to <italic>c</italic>). By applying this rule to all local counts in <bold><italic>C</italic></bold><sub><italic>r</italic></sub> and rearranging them following the same spatial order and the output stride, an upsampled count map <inline-formula><mml:math id="M6"><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>C</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> can be acquired. <bold><italic>C</italic></bold><sub><italic>u</italic></sub> is still redundant. TasselNetV2 addresses this by constructing a reference map <bold><italic>P</italic></bold> ∈ ℝ<sup><italic>H</italic>×<italic>W</italic></sup> that records how many times each location is counted. <bold><italic>P</italic></bold> can be an indicator of redundancy, as visualized in <xref ref-type="fig" rid="F5">Figure 5</xref>. The final normalized count map <inline-formula><mml:math id="M7"><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>C</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> therefore can be computed by <bold><italic>C</italic></bold><sub><italic>n</italic></sub> = <bold><italic>C</italic></bold><sub><italic>u</italic></sub> ⊘ <bold><italic>P</italic></bold>, where ⊘ denotes the element-wise division operator. Finally, the image-level count <italic>c</italic><sub><italic>I</italic></sub> can be computed by aggregating <bold><italic>C</italic></bold><sub><italic>n</italic></sub>, i.e.,</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M8">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>I</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>x</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>W</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>H</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="bold-italic">
                      <mml:mi>C</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>n</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>y</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <bold><italic>C</italic></bold><sub><italic>n</italic></sub>(<italic>x, y</italic>) is the value of <bold><italic>C</italic></bold><sub><italic>n</italic></sub> indexed by <italic>x</italic> and <italic>y</italic>. The normalization process above can be implemented by Algorithm 1.</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Visualization of redundancy. The lighter the color is, the more redundant the regions are. The redundancy gradually grows from border to center and then remains constant in central areas.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0005"/>
      </fig>
      <table-wrap id="d39e1058" position="float">
        <label>Algorithm 1</label>
        <caption>
          <p>CPU implementation of redundant count normalization in TasselNetV2.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-i0001"/>
      </table-wrap>
      <p>Algorithm 1 is a CPU-based sequential implementation. It is easy to verify that most time consumption takes place in the two nested <monospace>for</monospace> loops, leading to inefficient normalization. One possible solution may be to parallel this process with additional computational resources, while a more elegant way may be to pose the question: <italic>Can we speed up the normalizer at the algorithmic level?</italic> Our answer is <italic>positive</italic>. Our solution comes from a mathematically-equivalent reformulation of Equation (1), which takes the form</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M9">
          <mml:mtable style="text-align:axis;" equalrows="false" columnlines="none" equalcolumns="false" class="array">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>I</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>x</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>W</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>H</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="bold-italic">
                      <mml:mi>C</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>n</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>y</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>    </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>H</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                      <mml:mo>×</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mi>y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mstyle mathvariant="bold-italic">
                          <mml:mi>c</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>r</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>r</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>×</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mstyle mathvariant="bold-italic">
                          <mml:mi>P</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>,</mml:mo>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>y</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
              </mml:mtd>
            </mml:mtr>
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>    </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>H</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                      <mml:mo>×</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="bold-italic">
                      <mml:mi>c</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>r</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mi>y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>r</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>×</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mstyle mathvariant="bold-italic">
                          <mml:mi>P</mml:mi>
                        </mml:mstyle>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>,</mml:mo>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>y</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>,</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <inline-formula><mml:math id="M10"><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>c</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is the vectorized version of <bold><italic>C</italic></bold><sub><italic>r</italic></sub>, <bold><italic>c</italic></bold><sub><italic>r</italic></sub>(<italic>i</italic>) denotes the <italic>i</italic>-th local count of <bold><italic>c</italic></bold><sub><italic>r</italic></sub>, and <bold><italic>P</italic></bold><sub><italic>i</italic></sub> indicates the <italic>r</italic> × <italic>r</italic> local region extracted from <bold><italic>P</italic></bold> that corresponds to <bold><italic>c</italic></bold><sub><italic>r</italic></sub>(<italic>i</italic>). The benefit of such a reformulation is that we can evade the explicit computation of <bold><italic>C</italic></bold><sub><italic>u</italic></sub> and achieve per-region normalization simultaneously. In addition, <bold><italic>P</italic></bold><sub><italic>i</italic></sub> can be efficiently constructed with modern image manipulation operators, such as <monospace>im2col</monospace> in MATLAB or <monospace>fold</monospace> in PyTorch. By defining another vector <inline-formula><mml:math id="M11"><mml:mstyle mathvariant="bold-italic"><mml:mi>q</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> where <inline-formula><mml:math id="M12"><mml:mstyle mathvariant="bold-italic"><mml:mi>q</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:munderover><mml:munderover accentunder="false" accent="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi>P</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></inline-formula>, Equation (2) can be further simplified to</p>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M13">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>I</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∑</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>H</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                      <mml:mo>×</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="bold-italic">
                      <mml:mi>c</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>r</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mstyle mathvariant="bold-italic">
                  <mml:mi>q</mml:mi>
                </mml:mstyle>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mstyle mathvariant="bold-italic">
                      <mml:mi>c</mml:mi>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>r</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:mrow>
                </mml:msubsup>
                <mml:mstyle mathvariant="bold-italic">
                  <mml:mi>q</mml:mi>
                </mml:mstyle>
                <mml:mo>.</mml:mo>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>This new formulation can be implemented by Algorithm 2. It is worth noting that Algorithm 2 is a full GPU-based implementation. We re-profile the time usage of TasselNetV2 with this new implementation. As shown in <xref ref-type="fig" rid="F6">Figure 6</xref>, the time consumption of the normalizer reduces significantly.</p>
      <table-wrap id="d39e1768" position="float">
        <label>Algorithm 2</label>
        <caption>
          <p>GPU implementation of redundant count normalization in TasselNetV2+.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-i0002"/>
      </table-wrap>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Time profiling of TasselNetV2 with the GPU-based normalizer. The GPU-based normalizer speeds up the inference significantly. From low resolution to high resolution, the normalizer only takes up 56.62, 52.36, 49.48, 49.15% of the total processing time, respectively.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0006"/>
      </fig>
    </sec>
    <sec>
      <title>2.5. Optimizing Encoder and Counter</title>
      <p>After addressing the main computational bottleneck, we also take a closer look at the encoder and the counter to examine their possibility for further optimization. Indeed we find such possibility. For the counter, we notice that the number of parameter of the first convolutional layer is 8 × 8 × 64 × 128 = 524, 288, while the total number parameters of the model is 638, 993. That is to say, this single layer takes up 82.05% of model parameters. This fact motivates us to investigate the necessity of reserving such a parameter-extensive layer. Inspired by a common practice in image classification (Lin et al., <xref rid="B23" ref-type="bibr">2013</xref>; He et al., <xref rid="B16" ref-type="bibr">2016</xref>) where fully-connected layers are replaced with a global average pooling (GAP) layer, we apply this modification to TasselNetV2 and surprisingly find that almost no performance loss is observed (we will justify this point in section 3), which suggests the first convolutional layer in the counter can be safely replaced by GAP. Note that, the sense of “global” in GAP is relative to the base input size, rather than the input image size. It is still implemented by a standard average pooling layer, with the same kernel size compared to the size used in convolution, i.e., 8 × 8 for <italic>r</italic> = 64.</p>
      <p>A very interesting property of introducing the GAP layer is that it allows flexible manipulation of the base input size <italic>r</italic> × <italic>r</italic> without changing the model complexity because GAP is a non-parametric layer. Allowing the change of <italic>r</italic> enables TasselNetV2+ to adapt to different object sizes in images. It is clear that, when resizing an image, object sizes change accordingly. <italic>r</italic> should also change to match the object size. For instance, if an image is upsampled by ×2, <italic>r</italic> also should be doubled. This is a hyper-parameter that needs to be tuned when choosing an appropriate image resolution in practice. Tuning <italic>r</italic> is easy in TasselNetV2+. Given the desired base input size <italic>r</italic> × <italic>r</italic> and the output stride <italic>s</italic>, one only needs to modify the kernel size of GAP to be <inline-formula><mml:math id="M14"><mml:mfrac><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>. Note that such a modification does not affect the model complexity. We will show later in section 3 how counting performance changes with changed base input sizes.</p>
      <p>Regarding the encoder, it is not immediately clear on how to improve its efficiency because its design is sufficiently clean. Despite there exist efficient convolutional operators such as depthwise convolution, such efficiency still stays in theory, e.g., “depthwise convolution + pointwise convolution” used in MobileNet (Howard et al., <xref rid="B17" ref-type="bibr">2017</xref>) is even less efficient than standard convolution in TasselNetV2 (23.76ms vs. 16.92ms for processing an 1920 × 1080 input with the encoder). Instead we find a simple trick that can improve the encoder efficiency. The trick is to move forward the last downsampling layer, right after the third convolutional layer. This simple modification leads to an efficiency improvement from 16.92 ms to 14.39 ms on an 1920 × 1080 input. The improvement can boil down to the early decrease of spatial resolution such that <italic>conv4</italic> and <italic>conv5</italic> are executed on low-resolution feature maps. The modification also increases the RF by 17%, from 94 to 110, as illustrated in <xref ref-type="fig" rid="F7">Figure 7</xref>. The importance of RF for plant counting has been highlighted in Xiong et al. (<xref rid="B58" ref-type="bibr">2019a</xref>). Such increment of RF hence allows additional context modeling.</p>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Receptive field comparison between TasselNetV2 and TasselNetV2+.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0007"/>
      </fig>
      <p>We remark that, since the improvements to the counter and the encoder are somewhat tricky and minor, we do not declare any novelty or contribution in this part.</p>
    </sec>
    <sec>
      <title>2.6. Meeting TasselNetV2+</title>
      <p>Altogether the efficient normalizer, the trimmed counter, and the improved encoder construct a fast version of TasselNetV2 we call TasselNetV2+. <xref ref-type="fig" rid="F8">Figure 8</xref> highlights the improvements of TasselNetV2+ over TasselNetV2. Following the same notation in section 2.2, the architecture of TasselNetV2+ is formally defined by <bold>C</bold><sub>3</sub>(16)-<bold>M</bold>-<bold>C</bold><sub>3</sub>(32)-<bold>M</bold>-<bold>C</bold><sub>3</sub>(64)-<bold>M</bold>-<bold>C</bold><sub>3</sub>(128)-<bold>C</bold><sub>3</sub>(128)-<bold>A</bold><sub>8</sub>-<bold>C</bold><sub>1</sub>(128)-<bold>C</bold><sub>1</sub>(1), where <bold>A</bold><sub>8</sub> is the average pooling operator with 8 × 8 kernel size so that each inferred local count is still learned from a region of the base input size.</p>
      <fig id="F8" position="float">
        <label>Figure 8</label>
        <caption>
          <p>A framework view of TasselNetV2+. Our modifications are in boldface, including changing the downsampling behavior in the encoder, aggregating encoder features with global average pooling in the counter, and implementing a GPU-based normalizer.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0008"/>
      </fig>
      <p>To showcase the overall effect in efficiency optimization, we again profile the time usage of TasselNetV2+ in <xref ref-type="fig" rid="F9">Figure 9</xref>. It can be observed that, compared with <xref ref-type="fig" rid="F6">Figure 6</xref>, the time consumption of the counter decreases significantly. Now TasselNetV2+ can process an 1920 × 1080 image in less than 40 ms. To give one a sense why TasselNetV2+ is significantly faster than TasselNetV2, we further summarize the number of parameters and GFLOPs (an indicator of the amount of floating-point operations) of two models. TasselNetV2 has 639K model parameters with the GFLOPs of 29.20, while TasselNetV2+ is with 262K and 12.42 GFLOPs (GFLOPs are based on an 1920 × 1080 input). Overall TasselNetV2+ is an order of magnitude faster than TasselNetV2 per <xref ref-type="fig" rid="F1">Figure 1</xref> with less parameters and GFLOPs. In section 3, we will show that the decrease of model parameters and GFLOPs does not imply the degradation of counting accuracy; instead TasselNetV2+ achieves almost the same counting accuracy compared to TasselNetV2.</p>
      <fig id="F9" position="float">
        <label>Figure 9</label>
        <caption>
          <p>Time profiling of TasselNetV2+.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0009"/>
      </fig>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Results and Discussions</title>
    <p>The goal of this work is to provide an easy-to-use tool for plant counting and to improve the efficiency of TasselNetV2. Since the efficiency issue has already been justified in the previous sections, here we mainly address the concern on whether the increased efficiency comes at the cost of decreased accuracy. We evaluate TasselNetV2+ on three plant counting tasks, wheat ears counting (Madec et al., <xref rid="B37" ref-type="bibr">2019</xref>), maize tassels counting (Lu et al., <xref rid="B32" ref-type="bibr">2017c</xref>), and sorghum heads counting (Guo et al., <xref rid="B13" ref-type="bibr">2018</xref>).</p>
    <sec>
      <title>3.1. Wheat Ears Counting</title>
      <p>Here we report results of TasselNetV2+, TasselNetV2 (Xiong et al., <xref rid="B58" ref-type="bibr">2019a</xref>), TasselNet (Lu et al., <xref rid="B32" ref-type="bibr">2017c</xref>), and Faster R-CNN (Ren et al., <xref rid="B47" ref-type="bibr">2015</xref>) on the WED dataset (Madec et al., <xref rid="B37" ref-type="bibr">2019</xref>). Since bounding boxes annotations are given, we only use the center points computed from bounding boxes to train TasselNetV2 and TasselNetV2+. We follow the same train/validation split used in Madec et al. (<xref rid="B37" ref-type="bibr">2019</xref>). We also follow (Madec et al., <xref rid="B37" ref-type="bibr">2019</xref>) that designs a series of experiments with different downsampling rates of <inline-formula><mml:math id="M15"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>, <inline-formula><mml:math id="M16"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>, <inline-formula><mml:math id="M17"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>, <inline-formula><mml:math id="M18"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>, and <inline-formula><mml:math id="M19"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> and different cropped image sizes. This allows us to directly compare TasselNetV2+ with the results of Faster R-CNN reported in Madec et al. (<xref rid="B37" ref-type="bibr">2019</xref>). Note that, since in high resolution, the average size of wheat ears will be larger than the RF of TasselNetV2+, we also build several variants of TasselNetV2+ with changed base input sizes.</p>
      <p>ℓ<sub>1</sub> loss is used for training TasselNetV2 and TasselNetV2+. 256 × 256 or 512 × 512 image patches are randomly cropped from each image with random horizontal flipping for data augmentation (only one patch is sampled from each image in each epoch). The network is trained from scratch with a batch size of 8. Model parameters are initialized from the normal distribution with a standard deviation of 0.01. The stochastic gradient descent (SGD) optimizer is used for optimization. Parameters are updated for 500 epochs, with 10, 000 iterations. The learning rate is initially set to 0.01 and reduced by 10 × at the 200-th and 400-th epoch, respectively. The mean absolute error (MAE), root mean square error (RMSE), relative RMSE, and the coefficient of determination (<italic>R</italic><sup>2</sup>) are reported.</p>
      <p>Results are listed in <xref rid="T1" ref-type="table">Table 1</xref>. We can make the following observations:</p>
      <list list-type="bullet">
        <list-item>
          <p>TasselNetV2+ achieves counting performance comparable to TasselNetV2 (5c vs. 5d);</p>
        </list-item>
        <list-item>
          <p>The best performance reported by TasselNetV2+ is slightly better than that reported by Faster R-CNN (4d vs. 1a), while TasselNetV2+ and Faster R-CNN achieve this at different resizing ratios (<inline-formula><mml:math id="M20"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> vs. <inline-formula><mml:math id="M21"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula>);</p>
        </list-item>
        <list-item>
          <p>Compared to Faster R-CNN (1a, 2a, 3b, and 4b), the performance of TasselNetV2+ is less sensitive to the change of image resolution (1b, 2b, 3d, and 4d). We believe the reason is that Faster R-CNN requires to encode sufficiently good appearance features to detect bounding boxes. In low image resolution, degraded appearance cues may lead to decreased performance of Faster R-CNN. By contrast, local count models like TasselNetV2 and TasselNetV2+ do not require detecting bounding boxes but work by counting repetitive visual patterns. Such repetitive patterns do not have to be the whole ear and instead can be any representative part of an ear. The patterns are not likely to change significantly with changed image resolution;</p>
        </list-item>
        <list-item>
          <p>Local regression models like TasselNetV2 and TasselNetV2+ generally work well when the ear size is small (4c, 4d, 5c, and 5d). This can be a valuable property in practice because these models make it possible for large-scale phenotyping from the sky, e.g., with unmanned aircraft vehicles, where the phenotyped plants often appear to be small in images;</p>
        </list-item>
        <list-item>
          <p>The counting performance of TasselNetV2+ improves when the base input size is larger than the average ear size (2b vs. 2c vs. 2d and 1b vs. 1c vs. 1d), which means the RF of the network should be large enough to cover the objects counted. In high resolution, the performance of TasselNetV2+ slightly decreases. We think the reason is that TasselNetV2+ is not sufficiently deep (with only 5 convolutional layers), the feature representation may not be encoded well at the high resolution (details of ears are rich in high resolution).</p>
        </list-item>
        <list-item>
          <p>Compared to Faster R-CNN, TasselNetV2+ is also efficient. It is reported in Madec et al. (<xref rid="B37" ref-type="bibr">2019</xref>) that the inference of Faster R-CNN on the <inline-formula><mml:math id="M22"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> resolution requires about 1 h to iterate over the validation set, while TasselNetV2+ only takes a few seconds.</p>
        </list-item>
      </list>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Performance on the Wheat Ears Detection dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Setting</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Resize</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Base input size</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Average ear size (pixels)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cropped image size</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MAE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>RMSE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>rRMSE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>R</italic>
                  <sup>2</sup>
                </bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1a</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Faster R-CNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">110.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">500 × 500</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>4.55</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.30%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.91</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1b</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">110.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">11.62</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.73%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.67</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1c</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128</td>
              <td valign="top" align="center" rowspan="1" colspan="1">110.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.47%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.79</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">1d</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">192</td>
              <td valign="top" align="center" rowspan="1" colspan="1">110.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.34%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.75</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2a</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Faster R-CNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">500 × 500</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.40%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.85</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2b</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.91%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.81</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2c</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.13</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.58</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.88%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.86</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">2d</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.97</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.62%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.86</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3a</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Faster R-CNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">250 × 250</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">11.20%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.83</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3b</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Faster R-CNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">500 × 500</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">24.70%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.87</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3c</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">256 × 256</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.03</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.71%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.87</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3d</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.65</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.52%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.88</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">3e</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.29</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.26%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.89</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">4a</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Faster R-CNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">250 × 250</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">11.20%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.75</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">4b</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Faster R-CNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">500 × 500</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38.50%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.33</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">4c</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">256 × 256</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.02</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.19</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.84%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.91</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">4d</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">512 × 512</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>5.66</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>4.24</bold>%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.92</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5a</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Faster R-CNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">250 × 250</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30.30%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.62</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5b</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNet</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">256 × 256</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.29</td>
              <td valign="top" align="center" rowspan="1" colspan="1">7.10%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.79</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5c</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">256 × 256</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.50%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.91</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5d</td>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1/8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">256 × 256</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.93</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6.08</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.63%</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.91</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The best performance is in boldface</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2. Maize Tassels Counting</title>
      <p>Here we evaluate TasselNetV2+ on the MTC dataset (Lu et al., <xref rid="B32" ref-type="bibr">2017c</xref>). Following the same practices in Lu et al. (<xref rid="B32" ref-type="bibr">2017c</xref>) and Xiong et al. (<xref rid="B58" ref-type="bibr">2019a</xref>), we downsample images to its <inline-formula><mml:math id="M23"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:mfrac></mml:math></inline-formula> resolution for a fair comparison. We also report performance of TasselNetV2 and other state-of-the-art methods that have reported their counting performance on this dataset.</p>
      <p>We follow the same training configuration used in the counting wheat ears except that, 256 × 256 image patches are randomly cropped, the batch size is set to 9 (with the same 10, 000 iterations). The MAE and RMSE are used as evaluation metrics. We also report <italic>R</italic><sup>2</sup> for TasselNetV2 and TasselNetV2+.</p>
      <p>Results are shown in <xref rid="T2" ref-type="table">Table 2</xref>. It is clear that TasselNetV2+ performs no worse than TasselNetV2 and other state-of-the-art methods, with the best MAE of 5.1 and a comparable RMSE of 9.0. The slightly improved performance compared to Xiong et al. (<xref rid="B58" ref-type="bibr">2019a</xref>) may boil down to the improved training protocol (we observe that mini-batch training leads to more stable training behavior than single-image training used in Xiong et al., <xref rid="B58" ref-type="bibr">2019a</xref>).</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Performance on the Maize Tassels Counting dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>MAE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>RMSE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>R</italic>
                  <sup>2</sup>
                </bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lu et al. (<xref rid="B30" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">24.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lu et al. (<xref rid="B29" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">19.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">26.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Tota and Idrees (<xref rid="B52" ref-type="bibr">2015</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">19.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lempitsky and Zisserman (<xref rid="B21" ref-type="bibr">2010</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">11.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Oñoro-Rubio and López-Sastre (<xref rid="B42" ref-type="bibr">2016</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">21.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">25.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Lu et al. (<xref rid="B32" ref-type="bibr">2017c</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">6.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Liu et al. (<xref rid="B25" ref-type="bibr">2020</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">5.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2 (Xiong et al., <xref rid="B58" ref-type="bibr">2019a</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">5.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>8.8</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2 (Our Re-implementation)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>5.1</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8870</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>5.1</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">9.0</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.8880</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The best performance is in boldface</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.3. Sorghum Heads Counting</title>
      <p>Here we evaluate TasselNetV2+ on the SHC dataset. The SHC dataset is introduced by Guo et al. (<xref rid="B13" ref-type="bibr">2018</xref>) where two subsets with 52 and 40 images are labeled with dotted annotations, respectively. Since two datasets are generated in different ways, we evaluate TasselNetV2+ on them independently. For the dataset1 with 52 images, 26 images are randomly sampled for training, and the rest for testing. For the dataset2 with 40 images, 20 images are randomly sampled for training, and the rest for testing. We do not downsample the images in both training and testing.</p>
      <p>We also follow the same training configuration used in counting wheat ears except that, 256 × 1024 image patches are randomly cropped, and the batch size is set to 5. We report MAE, RMSE and <italic>R</italic><sup>2</sup>.</p>
      <p>Results are shown in <xref rid="T3" ref-type="table">Table 3</xref>. Again TasselNetV2+ and TasselNetV2 achieve comparable counting performance. It is worth noting that, both models are trained with a limited number of training samples (no more than 30), which implies that TasselNetV2+ is applicable to small sample sizes. The <italic>R</italic><sup>2</sup> on the dataset2 is slightly poor, but we notice most inferred counts on this dataset are sufficiently accurate. Since the number of testing sample is limited, the computation of <italic>R</italic><sup>2</sup> may be biased by some outliers shown in <xref ref-type="fig" rid="F11">Figure 11</xref>.</p>
      <table-wrap id="T3" position="float">
        <label>Table 3</label>
        <caption>
          <p>Performance on the Sorghum Heads Counting dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Dataset1</bold>
              </th>
              <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>Dataset2</bold>
              </th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MAE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>RMSE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>R</italic>
                  <sup>2</sup>
                </bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MAE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>RMSE</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>
                  <italic>R</italic>
                  <sup>2</sup>
                </bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">17.96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9578</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>3.54</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.00</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.6115</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>17.53</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>20.60</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.9587</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.58</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>4.78</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.6767</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The best performance is in boldface</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.4. Further Discussions</title>
      <p>As a summary of experiments above, we compare merits and drawbacks of Faster R-CNN, TasselNet, TasselNetV2, and TasselNetV2+ in <xref rid="T4" ref-type="table">Table 4</xref>. Faster R-CNN is accurate and has good multi-scale adaptation, but it becomes slow when scaling to high-resolution images due to large model capacity and high GPU memory consumption. TasselNet is a prototype of the plant counting model with only dotted annotations required. It points out a promising plant counting paradigm under resource-constrained conditions, but also leaves many problems unsolved. TasselNetV2 improves the accuracy and efficiency of TasselNet with the same model capacity, but still cannot tackle high-resolution images well. TasselNetV2+ inherits all the advantages of TasselNet and TasselNetV2 and is also scalable to high resolution. Despite TasselNetV2+ may not generalize well to multiple scales, we consider it a good candidate for plant counting.</p>
      <table-wrap id="T4" position="float">
        <label>Table 4</label>
        <caption>
          <p>Characteristics of different methods.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Accuracy</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Speed</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Resolution scalability</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Scale adaptation</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>GPU memory consumption</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Model capacity</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Faster R-CNN</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Good</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Slow</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Poor</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Good</td>
              <td valign="top" align="left" rowspan="1" colspan="1">High</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Large</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNet</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Fair</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Relatively slow</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Poor</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Poor</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Low</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Small</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Good</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Relatively fast</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Fair</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Poor</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Low</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Small</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">TasselNetV2+</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Good</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Fast</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Good</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Poor</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Low</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Tiny</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Qualitative results of TasselNetV2+ on three plant counting tasks are shown in <xref ref-type="fig" rid="F10">Figure 10</xref>. TasselNetV2+ infers accurate counts with strong/weak responses on plant/non-plant regions. The resulting count map can be an useful auxiliary cue to benefit related tasks such as detection or segmentation. Note that, TasselNetV2+ are applied to these plant counting tasks with the same architecture and almost the same hyper-parameters (we only slightly vary the batch size to ensure the same number of iterations during parameters updating).</p>
      <fig id="F10" position="float">
        <label>Figure 10</label>
        <caption>
          <p>Qualitative results of TasselNetV2+ on three plant counting dataset. From top to bottom, the wheat ears detection dataset, maize tassels counting dataset, sorghum heads counting—dataset1, and sorghum heads counting—dataset2. Red points are manual annotations.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0010"/>
      </fig>
      <p>We further compare the manual counts and inferred counts of TasselNetV2+ on three counting tasks in <xref ref-type="fig" rid="F11">Figure 11</xref>. A strong correlation between manual counts and inferred counts is observed on the WED, MTC, and SHC-dataset1 datasets, with <italic>R</italic><sup>2</sup> of 0.9179, 0.8880 and 0.9587, respectively. On the SHC-dataset2, the <italic>R</italic><sup>2</sup> is slightly poor. We believe the reason is that the points are too sparse such that <italic>R</italic><sup>2</sup> can be easily affected by few outliers. Most predictions are sufficiently accurate. We also observe that on the MTC dataset, a set of samples are underestimated. This is because this dataset is the most challenging one with a large data shift between training and testing set. Models learned on the training set may not generalize well to the testing set with significant variations in plant cultivars, illumination changes, and poses. In this case, the idea of domain adaptation may be applied to fill the performance loss (Lu et al., <xref rid="B31" ref-type="bibr">2017b</xref>, <xref rid="B35" ref-type="bibr">2018</xref>).</p>
      <fig id="F11" position="float">
        <label>Figure 11</label>
        <caption>
          <p>Comparison between inferred counts and manual counts with TasselNetV2+ in four plant counting datasets. <bold>(A)</bold> Wheat ears counting, <bold>(B)</bold> maize tassels counting, <bold>(C)</bold> sorghum heads counting—dataset1, and <bold>(D)</bold> sorghum heads counting—dataset2.</p>
        </caption>
        <graphic xlink:href="fpls-11-541960-g0011"/>
      </fig>
      <p>All evaluation results above suggest the general applicability of TasselNetV2+ in plant counting, especially when only the count value is the output of interest. However, an application note is that, TasselNetV2+ may have limited adaptation to scale variations, e.g., for a model trained on images captured at 5 m height will significantly degrade when testing on images captured at 10 m height. This is because TasselNetV2+ is inherently not a multi-scale model. Fortunately practitioner often have consistent image capturing plans, so this may not be a problem to deploy TasselNetV2+ in reality.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s4">
    <title>4. Conclusion</title>
    <p>In high-throughput phenotyping systems, the term “throughput” is closely related to the efficiency of data analysis algorithms. Targeting plant counting, we present TasselNetV2+, a fast implementation of a state-of-the-art plant counting model TasselNetV2, to deal with high-throughput counting from high-resolution imagery. This new implementation is inspired by a time profiling that the computational bottleneck of TasselNetV2 lies in the normalizer. We therefore improve this part with a novel mathematically-equivalent formulation that enables a fast GPU implementation. TasselNetV2+ shows a clear advantage in efficiency on processing high-resolution images. Compared to Faster R-CNN, it also demonstrates its effectiveness and robustness in changed image resolution.</p>
    <p>We believe our new implementation will encourage many real-time applications in phenotyping plant counts. An interesting application scenario would be that, images are directly processed right after capturing on the unmanned aircraft vehicles, instead of being sent back for post-processing. It would also be interesting to see applications of TasselNetV2+ to other plant species. For future work, we plan to enhance the scale adaptation of the model.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>All datasets generated for this study are included in the article/supplementary material.</p>
  </sec>
  <sec id="s6">
    <title>Author Contributions</title>
    <p>HL proposed the idea of TasselNetV2+, implemented TasselNetV2 and TasselNetV2+ in PyTorch, conducted the experiments, analyzed the results, drafted, and revised the manuscript. ZC provided the funding and supervised the study. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <ack>
    <p>The authors would like to thank Liang Liu for organizing figures.</p>
  </ack>
  <fn-group>
    <fn id="fn0001">
      <p><sup>1</sup>The MATLAB implementation of TasselNetV2 can be found at <ext-link ext-link-type="uri" xlink:href="https://tinyurl.com/TasselNetV2">https://tinyurl.com/TasselNetV2</ext-link>.</p>
    </fn>
  </fn-group>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> This work was supported by the Natural Science Foundation of China under Grant No. 61876211.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Arteta</surname><given-names>C.</given-names></name><name><surname>Lempitsky</surname><given-names>V.</given-names></name><name><surname>Noble</surname><given-names>J. A.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Interactive object counting</article-title>, in <source>European Conference on Computer Vision</source> (<publisher-loc>Zurich</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>504</fpage>–<lpage>518</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-10578-9_33</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baofeng</surname><given-names>S.</given-names></name><name><surname>Jinru</surname><given-names>X.</given-names></name><name><surname>Chunyu</surname><given-names>X, Yuyang, S.</given-names></name><name><surname>Fuentes</surname><given-names>S.</given-names></name></person-group> (<year>2016</year>). <article-title>Digital surface model applied to unmanned aerial vehicle based photogrammetry to assess potential biotic or abiotic effects on grapevine canopies</article-title>. <source>Int. J. Agric. Biol. Eng</source>. <volume>9</volume>, <fpage>119</fpage>–<lpage>130</lpage>. <pub-id pub-id-type="doi">10.3965/j.ijabe.20160906.2908</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boissard</surname><given-names>P.</given-names></name><name><surname>Martin</surname><given-names>V.</given-names></name><name><surname>Moisan</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>). <article-title>A cognitive vision approach to early pest detection in greenhouse crops</article-title>. <source>Comput. Electron. Agric</source>. <volume>62</volume>, <fpage>81</fpage>–<lpage>93</lpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2007.11.009</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>A. B.</given-names></name><name><surname>Liang</surname><given-names>Z. S. J.</given-names></name><name><surname>Vasconcelos</surname><given-names>N.</given-names></name></person-group> (<year>2008</year>). <article-title>Privacy preserving crowd monitoring: counting people without people models or tracking</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<publisher-loc>Anchorage, AK</publisher-loc>), <fpage>1</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2008.4587569</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Gong</surname><given-names>S.</given-names></name><name><surname>Xiang</surname><given-names>T.</given-names></name><name><surname>Change Loy</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Cumulative attribute space for age and crowd density estimation</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Portland, OR</publisher-loc>), <fpage>2467</fpage>–<lpage>2474</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2013.319</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dalal</surname><given-names>N.</given-names></name><name><surname>Triggs</surname><given-names>B.</given-names></name></person-group> (<year>2005</year>). <article-title>Histograms of oriented gradients for human detection</article-title>, in <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>886</fpage>–<lpage>893</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2005.177</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Dong</surname><given-names>W.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Li</surname><given-names>L.-J.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Fei-Fei</surname><given-names>L.</given-names></name></person-group> (<year>2009</year>). <article-title>Imagenet: a large-scale hierarchical image database</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<publisher-loc>Miami, FL</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>248</fpage>–<lpage>255</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosal</surname><given-names>S.</given-names></name><name><surname>Zheng</surname><given-names>B.</given-names></name><name><surname>Chapman</surname><given-names>S. C.</given-names></name><name><surname>Potgieter</surname><given-names>A. B.</given-names></name><name><surname>Jordan</surname><given-names>D. R.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>A weakly supervised deep learning framework for sorghum head detection and counting</article-title>. <source>Plant Phenom</source>. <volume>2019</volume>:<fpage>1525874</fpage>
<pub-id pub-id-type="doi">10.34133/2019/1525874</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giuffrida</surname><given-names>M. V.</given-names></name><name><surname>Doerner</surname><given-names>P.</given-names></name><name><surname>Tsaftaris</surname><given-names>S. A.</given-names></name></person-group> (<year>2018</year>). <article-title>Pheno-deep counter: a unified and versatile deep learning architecture for leaf counting</article-title>. <source>Plant J</source>. <volume>96</volume>, <fpage>880</fpage>–<lpage>890</lpage>. <pub-id pub-id-type="doi">10.1111/tpj.14064</pub-id><?supplied-pmid 30101442?><pub-id pub-id-type="pmid">30101442</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Giuffrida</surname><given-names>M. V.</given-names></name><name><surname>Minervini</surname><given-names>M.</given-names></name><name><surname>Tsaftaris</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>). <article-title>Learning to count leaves in rosette plants</article-title>, in <source>Proc. Computer Vision Problems in Plant Phenotyping (CVPPP)</source> (<publisher-loc>Swansea, UK</publisher-loc>: <publisher-name>BMVA Press</publisher-name>), <fpage>1.1</fpage>–<lpage>1.13</lpage>. <pub-id pub-id-type="doi">10.5244/C.29.CVPPP.1</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomes</surname><given-names>J. F. S.</given-names></name><name><surname>Leta</surname><given-names>F. R.</given-names></name></person-group> (<year>2012</year>). <article-title>Applications of computer vision techniques in the agriculture and food industry: a review</article-title>. <source>Eur. Food Res. Technol</source>. <volume>235</volume>, <fpage>989</fpage>–<lpage>1000</lpage>. <pub-id pub-id-type="doi">10.1007/s00217-012-1844-2</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>W.</given-names></name><name><surname>Fukatsu</surname><given-names>T.</given-names></name><name><surname>Ninomiya</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>). <article-title>Automated characterization of flowering dynamics in rice using field-acquired time-series rgb images</article-title>. <source>Plant Methods</source>
<volume>11</volume>:<fpage>7</fpage>. <pub-id pub-id-type="doi">10.1186/s13007-015-0047-9</pub-id><?supplied-pmid 25705245?><pub-id pub-id-type="pmid">25705245</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>W.</given-names></name><name><surname>Zheng</surname><given-names>B.</given-names></name><name><surname>Potgieter</surname><given-names>A. B.</given-names></name><name><surname>Diot</surname><given-names>J.</given-names></name><name><surname>Watanabe</surname><given-names>K.</given-names></name><name><surname>Noshita</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Aerial imagery analysis-quantifying appearance and number of sorghum heads for applications in breeding and agronomy</article-title>. <source>Front. Plant Sci</source>. <volume>9</volume>:<fpage>1544</fpage>. <pub-id pub-id-type="doi">10.3389/fpls.2018.01544</pub-id><?supplied-pmid 30405675?><pub-id pub-id-type="pmid">30405675</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halstead</surname><given-names>M.</given-names></name><name><surname>McCool</surname><given-names>C.</given-names></name><name><surname>Denman</surname><given-names>S.</given-names></name><name><surname>Perez</surname><given-names>T.</given-names></name><name><surname>Fookes</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>). <article-title>Fruit quantity and ripeness estimation using a robotic vision system</article-title>. <source>IEEE Robot. Automat. Lett</source>. <volume>3</volume>, <fpage>2995</fpage>–<lpage>3002</lpage>. <pub-id pub-id-type="doi">10.1109/LRA.2018.2849514</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Häni</surname><given-names>N.</given-names></name><name><surname>Roy</surname><given-names>P.</given-names></name><name><surname>Isler</surname><given-names>V.</given-names></name></person-group> (<year>2019</year>). <article-title>A comparative study of fruit detection and counting methods for yield mapping in apple orchards</article-title>. <source>J. Field Robot</source>. <volume>37</volume>, <fpage>263</fpage>–<lpage>282</lpage>. <pub-id pub-id-type="doi">10.1002/rob.21902</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep residual learning for image recognition</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<publisher-loc>Las Vegas, NV</publisher-loc>), <fpage>770</fpage>–<lpage>778</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>A. G.</given-names></name><name><surname>Zhu</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name><name><surname>Kalenichenko</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>W.</given-names></name><name><surname>Weyand</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Mobilenets: efficient convolutional neural networks for mobile vision applications</article-title>. <source>arXiv [Preprint]. arXiv:1704.04861</source>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S.</given-names></name><name><surname>Szegedy</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title>, in <source>Proc. International Conference on Machine Learning (ICML)</source> (<publisher-loc>Lille</publisher-loc>), <fpage>448</fpage>–<lpage>456</lpage>.</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>X.</given-names></name><name><surname>Madec</surname><given-names>S.</given-names></name><name><surname>Dutartre</surname><given-names>D.</given-names></name><name><surname>de Solan</surname><given-names>B.</given-names></name><name><surname>Comar</surname><given-names>A.</given-names></name><name><surname>Baret</surname><given-names>F.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>High-throughput measurements of stem characteristics to estimate ear density and above-ground biomass</article-title>. <source>Plant Phenom</source>. <volume>2019</volume>:<fpage>4820305</fpage>
<pub-id pub-id-type="doi">10.34133/2019/4820305</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamilaris</surname><given-names>A.</given-names></name><name><surname>Prenafeta-Boldú</surname><given-names>F. X.</given-names></name></person-group> (<year>2018</year>). <article-title>Deep learning in agriculture: a survey</article-title>. <source>Comput. Electron. Agric</source>. <volume>147</volume>, <fpage>70</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2018.02.016</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lempitsky</surname><given-names>V.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Learning to count objects in images</article-title>, in <source>Advances in Neural Information Processing Systems (NIPS)</source> (<publisher-loc>Vancouver, BC</publisher-loc>), <fpage>1324</fpage>–<lpage>1332</lpage>. <?supplied-pmid 25980675?></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>). <article-title>CSRNet: dilated convolutional neural networks for understanding the highly congested scenes</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>), <fpage>1091</fpage>–<lpage>1100</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2018.00120</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>M.</given-names></name><name><surname>Chen</surname><given-names>Q.</given-names></name><name><surname>Yan</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>). <article-title>Network in network</article-title>, in <source>Proc. International Conference on Learning Representations (ICLR)</source> (<publisher-loc>Scottsdale, AZ</publisher-loc>), <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T.-Y.</given-names></name><name><surname>Dollár</surname><given-names>P.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Hariharan</surname><given-names>B.</given-names></name><name><surname>Belongie</surname><given-names>S.</given-names></name></person-group> (<year>2017</year>). <article-title>Feature pyramid networks for object detection</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<publisher-loc>Honolulu, HI</publisher-loc>), <fpage>2117</fpage>–<lpage>2125</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2017.106</pub-id><?supplied-pmid 27534393?></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Xiong</surname><given-names>H.</given-names></name><name><surname>Xian</surname><given-names>K.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name></person-group> (<year>2020</year>). <article-title>Counting objects by blockwise classification</article-title>. <source>IEEE Trans. Circ. Syst. Video Technol</source>. <volume>30</volume>, <fpage>3513</fpage>–<lpage>3527</lpage>.</mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>Yang</surname><given-names>T.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>R.</given-names></name><name><surname>Wu</surname><given-names>W.</given-names></name><name><surname>Zhong</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>A method to calculate the number of wheat seedlings in the 1st to the 3rd leaf growth stages</article-title>. <source>Plant Methods</source><volume>14</volume>, <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1186/s13007-018-0369-5</pub-id><?supplied-pmid 30473722?><pub-id pub-id-type="pmid">29321806</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Cen</surname><given-names>C.</given-names></name><name><surname>Che</surname><given-names>Y.</given-names></name><name><surname>Ke</surname><given-names>R.</given-names></name><name><surname>Ma</surname><given-names>Y.</given-names></name><name><surname>Ma</surname><given-names>Y.</given-names></name></person-group> (<year>2020</year>). <article-title>Detection of maize tassels from UAV RGB imagery with faster R-CNN</article-title>. <source>Remote Sens</source>. <volume>12</volume>:<fpage>338</fpage>
<pub-id pub-id-type="doi">10.3390/rs12020338</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Xiao</surname><given-names>Y.</given-names></name><name><surname>Fang</surname><given-names>Z.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name></person-group> (<year>2017a</year>). <article-title>Towards fine-grained maize tassel flowering status recognition: dataset, theory and practice</article-title>. <source>Appl. Soft Comput</source>. <volume>56</volume>, <fpage>34</fpage>–<lpage>45</lpage>. <pub-id pub-id-type="doi">10.1016/j.asoc.2017.02.026</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Xiao</surname><given-names>Y.</given-names></name><name><surname>Fang</surname><given-names>Z.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Xian</surname><given-names>K.</given-names></name></person-group> (<year>2015</year>). <article-title>Fine-grained maize tassel trait characterization with multi-view representations</article-title>. <source>Comput. Electron. Agric</source>. <volume>118</volume>, <fpage>143</fpage>–<lpage>158</lpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2015.08.027</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Xiao</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name></person-group> (<year>2016</year>). <article-title>Region-based colour modelling for joint crop and maize tassel segmentation</article-title>. <source>Biosyst. Eng</source>. <volume>147</volume>, <fpage>139</fpage>–<lpage>150</lpage>. <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2016.04.007</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Xiao</surname><given-names>Y.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name></person-group> (<year>2017b</year>). <article-title>Two-dimensional subspace alignment for convolutional activations adaptation</article-title>. <source>Pattern Recogn</source>. <volume>71</volume>, <fpage>320</fpage>–<lpage>336</lpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2017.06.010</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Xiao</surname><given-names>Y.</given-names></name><name><surname>Zhuang</surname><given-names>B.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name></person-group> (<year>2017c</year>). <article-title>TasselNet: counting maize tassels in the wild via local counts regression network</article-title>. <source>Plant Methods</source>
<volume>13</volume>, <fpage>79</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1186/s13007-017-0224-0</pub-id><?supplied-pmid 29118821?><pub-id pub-id-type="pmid">29118821</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Dai</surname><given-names>Y.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name><name><surname>Xu</surname><given-names>S.</given-names></name></person-group> (<year>2019</year>). <article-title>Indices matter: learning to index for deep image matting</article-title>, in <source>Proc. IEEE/CVF Conference on Computer Vision (ICCV)</source> (<publisher-loc>Seoul</publisher-loc>), <fpage>3266</fpage>–<lpage>3275</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2019.00336</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Dai</surname><given-names>Y.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name><name><surname>Xu</surname><given-names>S.</given-names></name></person-group> (<year>2020</year>). <article-title>Index networks</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>. <pub-id pub-id-type="doi">10.1109/TPAMI.2020.3004474</pub-id>. [Epub ahead of print].</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Xiao</surname><given-names>Y.</given-names></name><name><surname>van den Hengel</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>An embarrassingly simple approach to visual domain adaptation</article-title>. <source>IEEE Trans. Image Process</source>. <volume>27</volume>, <fpage>3403</fpage>–<lpage>3417</lpage>. <pub-id pub-id-type="doi">10.1109/TIP.2018.2819503</pub-id><?supplied-pmid 29671743?><pub-id pub-id-type="pmid">29671743</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>Z.</given-names></name><name><surname>Wei</surname><given-names>X.</given-names></name><name><surname>Hong</surname><given-names>X.</given-names></name><name><surname>Gong</surname><given-names>Y.</given-names></name></person-group> (<year>2019</year>). <article-title>Bayesian loss for crowd count estimation with point supervision</article-title>, in <source>Proc. IEEE International Conference on Computer Vision (ICCV)</source> (<publisher-loc>Seoul</publisher-loc>), <fpage>6142</fpage>–<lpage>6151</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2019.00624</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madec</surname><given-names>S.</given-names></name><name><surname>Jin</surname><given-names>X.</given-names></name><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>De Solan</surname><given-names>B.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><name><surname>Duyme</surname><given-names>F.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Ear density estimation from high resolution RGB imagery using deep learning technique</article-title>. <source>Agric. Forest Meteorol</source>. <volume>264</volume>, <fpage>225</fpage>–<lpage>234</lpage>. <pub-id pub-id-type="doi">10.1016/j.agrformet.2018.10.013</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDonald</surname><given-names>T.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name></person-group> (<year>1990</year>). <article-title>Application of morphological image processing in agriculture</article-title>. <source>Trans. ASAE</source>
<volume>33</volume>, <fpage>1346</fpage>–<lpage>1352</lpage>. <pub-id pub-id-type="doi">10.13031/2013.31479</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mussadiq</surname><given-names>Z.</given-names></name><name><surname>Laszlo</surname><given-names>B.</given-names></name><name><surname>Helyes</surname><given-names>L.</given-names></name><name><surname>Gyuricza</surname><given-names>C.</given-names></name></person-group> (<year>2015</year>). <article-title>Evaluation and comparison of open source program solutions for automatic seed counting on digital images</article-title>. <source>Comput. Electron. Agric</source>. <volume>117</volume>, <fpage>194</fpage>–<lpage>199</lpage>. <pub-id pub-id-type="doi">10.1016/j.compag.2015.08.010</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>V.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2010</year>). <article-title>Rectified linear units improve restricted Boltzmann machines</article-title>, in <source>Proc. International Conference on Machine Learning (ICML)</source> (<publisher-loc>Haifa</publisher-loc>), <fpage>807</fpage>–<lpage>814</lpage>.</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nuske</surname><given-names>S.</given-names></name><name><surname>Wilshusen</surname><given-names>K.</given-names></name><name><surname>Achar</surname><given-names>S.</given-names></name><name><surname>Yoder</surname><given-names>L.</given-names></name><name><surname>Narasimhan</surname><given-names>S.</given-names></name><name><surname>Singh</surname><given-names>S.</given-names></name></person-group> (<year>2014</year>). <article-title>Automated visual yield estimation in vineyards</article-title>. <source>J. Field Robot</source>. <volume>31</volume>, <fpage>837</fpage>–<lpage>860</lpage>. <pub-id pub-id-type="doi">10.1002/rob.21541</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oñoro-Rubio</surname><given-names>D.</given-names></name><name><surname>López-Sastre</surname><given-names>R. J.</given-names></name></person-group> (<year>2016</year>). <article-title>Towards perspective-free object counting with deep learning</article-title>, in <source>Proc. European Conference on Computer Vision (ECCV)</source> (<publisher-loc>Amsterdam</publisher-loc>), <fpage>615</fpage>–<lpage>629</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-46478-7_38</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lerer</surname><given-names>A.</given-names></name><name><surname>Bradbury</surname><given-names>J.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Pytorch: an imperative style, high-performance deep learning library</article-title>, in <source>Advances in Neural Information Processing Systems (NeurIPS)</source> (<publisher-loc>Vancouver, BC</publisher-loc>), <fpage>8024</fpage>–<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Primicerio</surname><given-names>J.</given-names></name><name><surname>Caruso</surname><given-names>G.</given-names></name><name><surname>Comba</surname><given-names>L.</given-names></name><name><surname>Crisci</surname><given-names>A.</given-names></name><name><surname>Gay</surname><given-names>P.</given-names></name><name><surname>Guidoni</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Individual plant definition and missing plant characterization in vineyards from high-resolution UAV imagery</article-title>. <source>Eur. J. Remote Sens</source>. <volume>50</volume>, <fpage>179</fpage>–<lpage>186</lpage>. <pub-id pub-id-type="doi">10.1080/22797254.2017.1308234</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quan</surname><given-names>L.</given-names></name><name><surname>Feng</surname><given-names>H.</given-names></name><name><surname>Lv</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Maize seedling detection under different growth stages and complex field environments based on an improved faster R-CNN</article-title>. <source>Biosyst. Eng</source>. <volume>184</volume>, <fpage>1</fpage>–<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2019.05.002</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahnemoonfar</surname><given-names>M.</given-names></name><name><surname>Sheppard</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>Deep count: fruit counting based on deep simulated learning</article-title>. <source>Sensors</source>
<volume>17</volume>:<fpage>905</fpage>. <pub-id pub-id-type="doi">10.3390/s17040905</pub-id><?supplied-pmid 28425947?><pub-id pub-id-type="pmid">28425947</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Faster R-CNN: towards real-time object detection with region proposal networks</article-title>, in <source>Advances in Neural Information Processing Systems (NIPS)</source> (<publisher-loc>Montréal, QC</publisher-loc>), <fpage>91</fpage>–<lpage>99</lpage>. <?supplied-pmid 27295650?></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeghi-Tehran</surname><given-names>P.</given-names></name><name><surname>Sabermanesh</surname><given-names>K.</given-names></name><name><surname>Virlet</surname><given-names>N.</given-names></name><name><surname>Hawkesford</surname><given-names>M. J.</given-names></name></person-group> (<year>2017</year>). <article-title>Automated method to determine two critical growth stages of wheat: heading and flowering</article-title>. <source>Front. Plant Sci</source>. <volume>8</volume>:<fpage>252</fpage>. <pub-id pub-id-type="doi">10.3389/fpls.2017.00252</pub-id><?supplied-pmid 28289423?><pub-id pub-id-type="pmid">28289423</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group> (<year>2014</year>). <article-title>Very deep convolutional networks for large-scale image recognition</article-title>, in <source>Proc. International Conference on Learning Representations (ICLR)</source> (<publisher-loc>Banff, AB</publisher-loc>), <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sindagi</surname><given-names>V. A.</given-names></name><name><surname>Patel</surname><given-names>V. M.</given-names></name></person-group> (<year>2017</year>). <article-title>Generating high-quality crowd density maps using contextual pyramid CNNs</article-title>, in <source>Proc. IEEE International Conference on Computer Vision (ICCV)</source> (<publisher-loc>Venice</publisher-loc>), <fpage>1879</fpage>–<lpage>1888</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2017.206</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>M.</given-names></name><name><surname>Pang</surname><given-names>R.</given-names></name><name><surname>Le</surname><given-names>Q. V.</given-names></name></person-group> (<year>2019</year>). <article-title>Efficientdet: scalable and efficient object detection</article-title>. <source>arXiv [Preprint]. arXiv:1911.09070</source>. <pub-id pub-id-type="doi">10.1109/CVPR42600.2020.01079</pub-id><?supplied-pmid 26761743?></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tota</surname><given-names>K.</given-names></name><name><surname>Idrees</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Counting in dense crowds using deep features</article-title>. <source>Proc. CRCV.</source></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vapnik</surname><given-names>V. N.</given-names></name></person-group> (<year>1998</year>). <source>Statistical Learning Theory</source>, Vol. <volume>1</volume>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>P.</given-names></name><name><surname>Jones</surname><given-names>M.</given-names></name></person-group> (<year>2001</year>). <article-title>Rapid object detection using a boosted cascade of simple features</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<publisher-loc>Kauai, HI</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>I-511</fpage>–<lpage>I-518</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2001.990517</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vos</surname><given-names>J.</given-names></name><name><surname>Frinking</surname><given-names>H.</given-names></name></person-group> (<year>1997</year>). <article-title>Nitrogen fertilization as a component of integrated crop management of hot pepper (<italic>Capsicum</italic> spp.) under tropical lowland conditions</article-title>. <source>Int. J. Pest Manage</source>. <volume>43</volume>, <fpage>1</fpage>–<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1080/096708797228915</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiles</surname><given-names>L. J.</given-names></name><name><surname>Schweizer</surname><given-names>E. E.</given-names></name></person-group> (<year>1999</year>). <article-title>The cost of counting and identifying weed seeds and seedlings</article-title>. <source>Weed Sci</source>. <volume>47</volume>, <fpage>667</fpage>–<lpage>673</lpage>. <pub-id pub-id-type="doi">10.1017/S0043174500091311</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Yang</surname><given-names>G.</given-names></name><name><surname>Yang</surname><given-names>X.</given-names></name><name><surname>Xu</surname><given-names>B.</given-names></name><name><surname>Han</surname><given-names>L.</given-names></name><name><surname>Zhu</surname><given-names>Y.</given-names></name></person-group> (<year>2019</year>). <article-title>Automatic counting of in situ rice seedlings from UAV images based on a deep fully convolutional neural network</article-title>. <source>Remote Sens</source>. <volume>11</volume>:<fpage>691</fpage>
<pub-id pub-id-type="doi">10.3390/rs11060691</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>H.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Madec</surname><given-names>S.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name></person-group> (<year>2019a</year>). <article-title>Tasselnetv2: in-field counting of wheat spikes with context-augmented local regression networks</article-title>. <source>Plant Methods</source>
<volume>15</volume>:<fpage>150</fpage>. <pub-id pub-id-type="doi">10.1186/s13007-019-0537-2</pub-id><?supplied-pmid 31857821?><pub-id pub-id-type="pmid">31857821</pub-id></mixed-citation>
    </ref>
    <ref id="B59">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>H.</given-names></name><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Liu</surname><given-names>C.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Shen</surname><given-names>C.</given-names></name></person-group> (<year>2019b</year>). <article-title>From open set to closed set: counting objects by spatial divide-and-conquer</article-title>, in <source>Proc. IEEE International Conference on Computer Vision (ICCV)</source> (<publisher-loc>Seoul</publisher-loc>), <fpage>8362</fpage>–<lpage>8371</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2019.00845</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Z.</given-names></name><name><surname>Cao</surname><given-names>Z.</given-names></name><name><surname>Wu</surname><given-names>X.</given-names></name><name><surname>Bai</surname><given-names>X.</given-names></name><name><surname>Qin</surname><given-names>Y.</given-names></name><name><surname>Zhuo</surname><given-names>W.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Automatic image-based detection technology for two critical growth stages of maize: emergence and three-leaf stage</article-title>. <source>Agric. Forest Meteorol</source>. <volume>174</volume>, <fpage>65</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1016/j.agrformet.2013.02.011</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zabawa</surname><given-names>L.</given-names></name><name><surname>Kicherer</surname><given-names>A.</given-names></name><name><surname>Klingbeil</surname><given-names>L.</given-names></name><name><surname>Milioto</surname><given-names>A.</given-names></name><name><surname>Topfer</surname><given-names>R.</given-names></name><name><surname>Kuhlmann</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Detection of single grapevine berries in images using fully convolutional neural networks</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</source> (<publisher-loc>Long Beach, CA</publisher-loc>), <fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1109/CVPRW.2019.00313</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>X.</given-names></name></person-group> (<year>2015</year>). <article-title>Cross-scene crowd counting via deep convolutional neural networks</article-title>, in <source>Proc. IEEE International Conference on Computer Vision (ICCV)</source> (<publisher-loc>Boston, MA</publisher-loc>), <fpage>833</fpage>–<lpage>841</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298684</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Zhou</surname><given-names>D.</given-names></name><name><surname>Chen</surname><given-names>S.</given-names></name><name><surname>Gao</surname><given-names>S.</given-names></name><name><surname>Ma</surname><given-names>Y.</given-names></name></person-group> (<year>2016</year>). <article-title>Single-image crowd counting via multi-column convolutional neural network</article-title>, in <source>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<publisher-loc>Las Vegas, NV</publisher-loc>), <fpage>589</fpage>–<lpage>597</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.70</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
