<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6929276</article-id>
    <article-id pub-id-type="publisher-id">3180</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3180-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DEEPSEN: a convolutional neural network based method for super-enhancer prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Bu</surname>
          <given-names>Hongda</given-names>
        </name>
        <address>
          <email>buhongda@163.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hao</surname>
          <given-names>Jiaqi</given-names>
        </name>
        <address>
          <email>1352450@tongji.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gan</surname>
          <given-names>Yanglan</given-names>
        </name>
        <address>
          <email>ylgan@dhu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Shuigeng</given-names>
        </name>
        <address>
          <email>sgzhou@fudan.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Guan</surname>
          <given-names>Jihong</given-names>
        </name>
        <address>
          <email>jhguan@tongji.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000000123704535</institution-id><institution-id institution-id-type="GRID">grid.24516.34</institution-id><institution>Department of Computer Science and Technology, Tongji University, </institution></institution-wrap>4800 Cao’an Road, Shanghai, 201804 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0125 2443</institution-id><institution-id institution-id-type="GRID">grid.8547.e</institution-id><institution>Shanghai Key Lab of Intelligent Information Processing, and School of Computer Science, Fudan University, </institution></institution-wrap>220 Handan Road, Shanghai, 200433 China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9141 4786</institution-id><institution-id institution-id-type="GRID">grid.255169.c</institution-id><institution>School of Computer Science and Technology, Donghua University, </institution></institution-wrap>2999 North Renmin Road, Shanghai, 201620 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 15</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>598</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>10</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Super-enhancers (SEs) are clusters of transcriptional active enhancers, which dictate the expression of genes defining cell identity and play an important role in the development and progression of tumors and other diseases. Many key cancer oncogenes are driven by super-enhancers, and the mutations associated with common diseases such as Alzheimer’s disease are significantly enriched with super-enhancers. Super-enhancers have shown great potential for the identification of key oncogenes and the discovery of disease-associated mutational sites.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we propose a new computational method called DEEPSEN for predicting super-enhancers based on convolutional neural network. The proposed method integrates 36 kinds of features. Compared with existing approaches, our method performs better and can be used for genome-wide prediction of super-enhancers. Besides, we screen important features for predicting super-enhancers.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">Convolutional neural network is effective in boosting the performance of super-enhancer prediction.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Super-enhancer prediction</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Convolutional neural network</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>No. 61772367</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Key Research and Development Program of China</institution>
        </funding-source>
        <award-id>No. 2016YFC0901704</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Shanghai Natural Science Foundation</institution>
        </funding-source>
        <award-id>No. 17ZR1400200</award-id>
      </award-group>
    </funding-group>
    <conference xlink:href="http://alan.cs.gsu.edu/isbra18/">
      <conf-name>14th International Symposium on Bioinformatics Research and Applications (ISBRA'18)</conf-name>
      <conf-acronym>ISBRA 2018</conf-acronym>
      <conf-loc>Beijing, China</conf-loc>
      <conf-date>8-11 June 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Numerous transcriptional factors combine with enhancers to regulate gene expression through recruiting transcriptional coactivator and RNA polymerase to target gene [<xref ref-type="bibr" rid="CR1">1</xref>]. The term ‘enhancer’ was first introduced to describe the effects of SV40 DNA on the ectopic expression of a cloned rabbit <italic>β</italic> globin gene. The SV40 DNA elements activated transcription at a distance and independently of their orientation concerning the target gene [<xref ref-type="bibr" rid="CR2">2</xref>]. Enhancer activation often coincides with DNase I hypersensitivity of these regions and with specific post-translational modifications of adjacent nucleosomes [<xref ref-type="bibr" rid="CR3">3</xref>]. Direct interaction or looping between enhancers and the promoters of target genes has been observed and might be critical to enhancer function [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. Recently, advances in DNA sequencing technology, such as Chromatin Immunoprecipitation sequencing(ChIP-seq) and DNase I hypersensitivity sites sequencing(DNase-seq) have enabled the discovery of putative mammalian enhancers on a genome-wide scale [<xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR10">10</xref>].</p>
    <p>The concept of super-enhancers was proposed by Richard A.Young based on the research on enhancers, which is described as a class of regulatory regions with unusually strong enrichment for the binding of transcriptional coactivators, specifically Mediator (Med1) [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]. In mouse embryonic stem cells (mESCs), super-enhancers were defined in the following way [<xref ref-type="bibr" rid="CR12">12</xref>]: 1) Sites bound by all three master regulators, Oct4, Sox2 and Nanog, according to ChIP-seq, were considered enhancers; 2) Enhancers within 12.5 kb of each other were stitched to define a single entity spanning a genomic region; 3) The stitched enhancer entities and the remaining individual enhancers (those without a neighboring enhancer within 12.5 kb) were then ranked by the total background-normalized level of the Med1 signal within the genomic region. A small proportion (less than 3%) of these enhancer regions contained Med1 levels above a cutoff was designated as super-enhancers. The remaining enhancer regions were considered ‘normal’ enhancers. Super-enhancers tend to span large genomic regions, whose median size generally an order of magnitude larger than that of normal enhancers (in mESCs, 8667 bp versus 703 bp) [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref>]. Relative to Med1, a number of factors generally associated with enhancer activity show enrichment at super-enhancers relative to normal enhancers. These factors include RNA polymerase II (Pol II), RNA from transcribed enhancer loci (eRNA), the histone acetyltransferases p300 and CBP, chromatin factors such as cohesin, the histone modifications H3K27ac, H3K4me2 and H3K4me1, and increased chromatin accessibility as measured by DNase-seq. Because of these cross-correlations, super-enhancers might be identified by many of these features [<xref ref-type="bibr" rid="CR11">11</xref>].</p>
    <p>Since super-enhancers influence various biological processes, the identification of super-enhancers becomes an urgent research issue. BRD4, a member of the BET protein family, was used to distinguish super-enhancers from typical enhancers as it is highly correlated with MED1 [<xref ref-type="bibr" rid="CR13">13</xref>]. H3K27ac was extensively used to create a catalog of super-enhancers across 86 different human cell-types and tissues due to its availability [<xref ref-type="bibr" rid="CR11">11</xref>]. Other studies used the coactivator protein P300 to define super-enhancers [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>] However, the knowledge about these factors’ ability to define a set of super-enhancers in a particular cell-type and their relative and combinatorial importance remains limited. Master transcriptional factors that might form super-enhancers domains are largely unknown for most cell-types, while performing ChIP-seq for the Mediator complex is difficult and costly. However, there are no predictive models that integrate various types of data to predict super-enhancers and their constituents (enhancers within super-enhancer). Besides, to what degree these features influence on super-enhancers remains unknown.</p>
    <p>Predicting super-enhancers based on machine learning remains nearly blank in the literature. The only work was done by Khan and Zhang [<xref ref-type="bibr" rid="CR16">16</xref>]. They used six different machine learning models, including Random Forest, linear SVM, KNN, AdaBoost, Naive Bayes and Decision Tree. Chromatin, transcription factors and sequence-specific features were used to train these models individually, which were evaluated by 10-fold cross-validation. With the rise of deep learning (DL) techniques, many researchers applied state-of-art DL methods to bioinformatics problems. In DEEPBIND [<xref ref-type="bibr" rid="CR17">17</xref>], Alipanahi et al. described the use of a deep learning strategy to calculate protein-nucleic acid interactions from diverse experimental data sets. Their results showed DL’s applicability in bioinformatics and improved prediction power over traditional methods. Besides, Zhou et al. developed a deep-learning based algorithmic framework, named DeepSEA, which learns a regulatory sequence code from large-scale chromatin-profiling data in order to predict the noncoding variants effects [<xref ref-type="bibr" rid="CR18">18</xref>].</p>
    <p>In this work, we proposed a novel approach to solving the problem of super-enhancer prediction based on convolutional neural networks (CNNs). This method is called DEEPSEN. We constructed different structures of CNN to discover which kind of structure is more appropriate for the problem. For each network structure, we did fine-tuning to find out the best parameter set and to avoid overfitting. Furthermore, we did feature ranking and found out the significance of features for super-enhancers prediction. Our experimental results demonstrate that DEEPSEN outperforms the existing super-enhancer prediction model.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Datasets</title>
      <p>Similar to Aziz Khan [<xref ref-type="bibr" rid="CR16">16</xref>], we obtained 32 publicly available ChIP-seq and DNase-seq datasets of mouse embryonic stem cells (mESC) from Gene Expression Ominibus (GEO). These data cover four histone modifications (H3K27ac, H3K4me1, H3K4me3 and H3K9me3), DNA hypersensitive site (DNaseI), RNA polymeraseII (Pol II), transcriptional co-activating proteins (p300 and CBP), P-TFEb subunit (Cdk9), sub-units of Mediator complex (Med1, Med12 and Cdk8), chromatin regulators (Brg1, Brd4 and Chd7), Cohesin (Smc1 and Nipbl), subunits of Lsd1-NuRD complex (Lsd1 and Mi2b) and 11 transcription factors (Oct4, Sox2, Nanog, Esrrb, Klf4, Tcfcp2l1, Prdm14, Nr5a2, Smad3, Stat3 and Tcf3). Table <xref rid="Tab1" ref-type="table">1</xref> shows the datasets used in this paper.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Datasets used in this paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data Type</th><th align="left">Data Name</th><th align="left">GEO ID</th></tr></thead><tbody><tr><td align="left">Transcription factors</td><td align="justify">Oct4, Sox2, Nanog, Esrrb, Klf4, Smad3, Tcfcp2l1, Prdm14, Stat3, Tcf3, Nr5a2</td><td align="justify">GSE44286, GSM288355, GSM288354, GSM623989, GSM53954</td></tr><tr><td align="left">Mediator complex</td><td align="justify">MED1</td><td align="justify">GSM560348,GSM560345</td></tr><tr><td align="left">Histone modifications</td><td align="justify">H3K27ac, H3K4me1, H3K4me3, H3K9me3</td><td align="justify">GSM594579, GSM281695, GSM307149, GSM18371</td></tr><tr><td align="left">RNA polymerase</td><td align="justify">RNA Pol</td><td align="justify">GSM318444</td></tr><tr><td align="left">Hypersensitive site</td><td align="justify">DNaseI</td><td align="justify">GSM1014154</td></tr><tr><td align="left">Co-activators</td><td align="justify">p300, CBP</td><td align="justify">GSM918750,GSM1246866</td></tr><tr><td align="left">Chromatin regulators</td><td align="justify">Brg1, Brd4, Chd7</td><td align="justify">GSM896923, GSM937540, GSM558674</td></tr><tr><td align="left">Cohesion</td><td align="justify">Nipbl, Smc1</td><td align="justify">GSM560350,GSM560342</td></tr><tr><td align="left">Mediator complex</td><td align="justify">MED12</td><td align="justify">GSM560348,GSM560345</td></tr><tr><td align="left">Lsd1-NuRD complex</td><td align="justify">Lsd1, Mi2b</td><td align="justify">GSM687282,GSM687284</td></tr></tbody></table></table-wrap>
</p>
      <p>We used MED1 signal to define super-enhancers as described in ROSE [<xref ref-type="bibr" rid="CR12">12</xref>]. We selected transcriptional enriched regions as the training samples. Thus, we obtained 11100 samples with 36 kinds of features. Among them, 1119 are positive samples and 9981 are negative ones.</p>
    </sec>
    <sec id="Sec4">
      <title>Pipeline of the dEEPSEN method</title>
      <p>Based on convolutional neural network (CNN), we proposed a novel approach named DEEPSEN to predict super enhancers on genome scale. Fig. <xref rid="Fig1" ref-type="fig">1</xref> illustrates the pipeline of the DEEPSEN method. It consists of three major steps:
<list list-type="order"><list-item><p>Data preprocessing and feature calculation. 36 kinds of features were used to represent super-enhancers, including DNA sequence compositional features, histone modifications, transcriptional factors, RNA polymeraseII, hypersensitive site, co-activators, chromatin regulators, cohesion, mediator complex, mediator complex, and Lsd1-NuRD complex.
</p></list-item><list-item><p>Constructing and training DEEPSEN. First, we built three models with different numbers of convolutional layers. Then, we trained each model using the back propagation (BP) algorithm [<xref ref-type="bibr" rid="CR19">19</xref>] and stochastic gradient descent optimization algorithm. Furthermore, we did parameter tuning and validated each model using 5-fold cross-validation.</p></list-item><list-item><p>Feature ranking. We evaluated each feature’s contribution to the identification of super-enhancers.</p></list-item></list>
<fig id="Fig1"><label>Fig. 1</label><caption><p>The pipeline of DEEPSEN. The data we used were from GEO. Firstly, we do data preprocessing and feature calculation. Secondly, we construct three models with different numbers of convolutional layers and train them. Thirdly, we evaluate Pearson correlation coefficient to rank the features for predicting super-enhancers. Finally, we do performance evaluation and analysis</p></caption><graphic xlink:href="12859_2019_3180_Fig1_HTML" id="MO1"/></fig>
</p>
      <p>In what follows, we elaborate the process of super-enhancer prediction step by step.</p>
    </sec>
    <sec id="Sec5">
      <title>Data preprocessing and feature selection</title>
      <p>Firstly, we aligned the original ChIP-seq reads to mouse genome-build mm9 with bowtie 0.12.9 [<xref ref-type="bibr" rid="CR20">20</xref>]. As a result, we got the start and end positions of each read. Secondly, with these positions and the help of bamtoGFF, we calculated the read densities of samples, including super-enhancers and normal enhancers, and normalized these densities. Thirdly, we evaluated the binding affinity scores of all the samples with DNA binding motif information. Finally, we combined the calculated read densities and the binding affinity scores to get the final training data.</p>
    </sec>
    <sec id="Sec6">
      <title>Constructing and training dEEPSEN</title>
      <sec id="Sec7">
        <title>The structure of dEEPSEN</title>
        <p>Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the architecture of a DEEPSEN classifier, which consists of the <italic>input layer</italic> (the 1st convolutional layer, including max-pooling), the <italic>2nd convolutional layer</italic> (including max-pooling),..., the <italic>fully connected layers</italic>, and the <italic>output layer</italic>.
<fig id="Fig2"><label>Fig. 2</label><caption><p>The architecture of DEEPSEN-2L. DEEPSEN-2L consists of the input layer, the 1st convolutional layer (including the 1st max-Pooling), the 2nd convolutional layer (including the 2nd max-Pooling), fully connected layer (including dropout), and output softmax layer</p></caption><graphic xlink:href="12859_2019_3180_Fig2_HTML" id="MO2"/></fig>
</p>
        <p>The convolutional layer contains two steps: convolution step and pooling step. The convolution step uses multiple convolutional kernels to do convolution operation on the input data. A max-pooling operation often follows a convolution step to output a local maximal value of the respective convolutional outputs. The convolution operation learns to recognize relevant patterns of the input. The function of max-pooling is to reduce parameters to abstract the features learned in the proceeding layers. An activation function is usually used after each layer, which is nonlinear to guarantee the nonlinearity of the whole model. Here, we used the rectified linear unit(ReLU) function:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ ReLU(x) = max(0,x)  $$ \end{document}</tex-math><mml:math id="M2"><mml:mtext mathvariant="italic">ReLU</mml:mtext><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">max</mml:mtext><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2019_3180_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>The subsequent convolutional layers capture the relationships of the features extracted from the proceeding layers to obtain high-level features. Finally, the fully connected layer with dropout transforms the input into probability distribution through the softmax function:
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ f_{i}(z) = \frac{\mathrm{e}^{z_{i}}}{\sum_{j}\mathrm{e}^{z_{j}}}  $$ \end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2019_3180_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>The parameter details of the architecture are described in Table 1. We take the model consisting of 2 convolutional layers as the example. The input layer is a N ×36×1 matrix, where <italic>N</italic> is the number of samples that is set to 11100 in our experiments. The first convolutional layer contains 32 kernels of shape 3 ×1 with the stride of 1 using the same padding so that the size does not change during convolution operation with. The output of the first layer includes 32 feature maps of size 36 ×1. Next is the first pooling layer of size 3 ×1, which means that we remain only the maximum value among every three values to reduce the dimensions and make the model robust. The second convolutional layer has 64 kernels, each of which is 3 ×1×32, and its output includes 64 feature maps of size 12 ×1. The 2nd pooling layer uses 3 ×1 max-pooling, and its output contains 64 feature maps of size 4 ×1, that is, 64*4=256 nodes. Following is the fully connected layer with 256 input nodes and 64 output nodes. We used dropout method [<xref ref-type="bibr" rid="CR21">21</xref>] in the fully connected layer to delete some nodes randomly for controlling over-fitting. The detailed structure of DEEPSEN that contains two convolutional layers is presented in Table <xref rid="Tab2" ref-type="table">2</xref>. Besides the DEEPSEN with two convolutional layers, we also constructed DEEPSEN predictors with three convolutional layers and four convolutional layers. The details are presented in Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref>, respectively.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>The structure of DEEPSEN-2L</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Layer</th><th align="left">Size</th><th align="left">Output Shape</th></tr></thead><tbody><tr><td align="left">Input</td><td align="left"/><td align="left">36 ×1</td></tr><tr><td align="left">Convo1</td><td align="left">32 ×3×1</td><td align="left">32 ×36×1</td></tr><tr><td align="left">Pool1</td><td align="left">1 ×3</td><td align="left">32 ×12×1</td></tr><tr><td align="left">Convo2</td><td align="left">64 ×3×1×32</td><td align="left">64 ×12×1</td></tr><tr><td align="left">Pool2</td><td align="left">1 ×3</td><td align="left">64 ×4×1</td></tr><tr><td align="left">Full-connected</td><td align="left">256</td><td align="left">64</td></tr><tr><td align="left">softmax</td><td align="left">64</td><td align="left">2</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>The structure of DEEPSEN-3L</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Layer</th><th align="left">Size</th><th align="left">Output Shape</th></tr></thead><tbody><tr><td align="left">Input</td><td align="left"/><td align="left">36 ×1</td></tr><tr><td align="left">Convo1</td><td align="left">32 ×3×1</td><td align="left">32 ×36×1</td></tr><tr><td align="left">Pool1</td><td align="left">1 ×3</td><td align="left">32 ×12×1</td></tr><tr><td align="left">Convo2</td><td align="left">64 ×3×1×32</td><td align="left">64 ×12×1</td></tr><tr><td align="left">Pool2</td><td align="left">1 ×3</td><td align="left">64 ×4×1</td></tr><tr><td align="left">Convo3</td><td align="left">128 ×3×1×64</td><td align="left">128 ×4×1</td></tr><tr><td align="left">Pool3</td><td align="left">1 ×2</td><td align="left">128 ×2×1</td></tr><tr><td align="left">Full-connected</td><td align="left">256</td><td align="left">64</td></tr><tr><td align="left">softmax</td><td align="left">64</td><td align="left">2</td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>The structure of DEEPSEN-4L</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Layer</th><th align="left">Size</th><th align="left">Output Shape</th></tr></thead><tbody><tr><td align="left">Input</td><td align="left"/><td align="left">36 ×1</td></tr><tr><td align="left">Convo1</td><td align="left">32 ×3×1</td><td align="left">32 ×36×1</td></tr><tr><td align="left">Pool1</td><td align="left">1 ×3</td><td align="left">32 ×12×1</td></tr><tr><td align="left">Convo2</td><td align="left">64 ×3×1×32</td><td align="left">64 ×12×1</td></tr><tr><td align="left">Pool2</td><td align="left">1 ×3</td><td align="left">64 ×4×1</td></tr><tr><td align="left">Convo3</td><td align="left">128 ×3×1×64</td><td align="left">128 ×4×1</td></tr><tr><td align="left">Pool3</td><td align="left">1 ×2</td><td align="left">128 ×2×1</td></tr><tr><td align="left">Convo4</td><td align="left">256 ×3×1×128</td><td align="left">256 ×2×1</td></tr><tr><td align="left">Pool4</td><td align="left">1 ×2</td><td align="left">256 ×1×1</td></tr><tr><td align="left">Full-connected</td><td align="left">256</td><td align="left">64</td></tr><tr><td align="left">softmax</td><td align="left">64</td><td align="left">2</td></tr></tbody></table></table-wrap>
</p>
        <p>The major difference between the CNN based models and previous models lies in that CNN can learn to recognize relevant patterns of input by updating the network during training. Therefore, the advantage of CNN based models is the ability to learn complicated features from large-scale datasets in an adaptive manner.</p>
      </sec>
      <sec id="Sec8">
        <title>The training of dEEPSEN</title>
        <p>We used the cross entropy loss function, which is as follows:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{i}\log(h_{\theta}(x^{i}))+(1-y^{i})\log(1-h_{\theta}(x^{i}))}  $$ \end{document}</tex-math><mml:math id="M6"><mml:mspace width="-10.0pt"/><mml:mi>J</mml:mi><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2019_3180_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <italic>θ</italic> is the parameter set, m is the amount of samples, y <sup><italic>i</italic></sup> is the label of x <sup><italic>i</italic></sup>, h <sub><italic>θ</italic></sub>(x <sup><italic>i</italic></sup>) is the predicted label of x <sup><italic>i</italic></sup>. Parameters were randomly initialized. The data was processed from the input layer to the output layer, and back propagation [<xref ref-type="bibr" rid="CR19">19</xref>] and stochastic gradient descent algorithms were used to update the network parameters to minimize the cost function. Each epoch contains forward propagation, loss calculation, back propagation and parameter refreshing. The detailed training steps are as follows:
<list list-type="order"><list-item><p>Initializing the parameters randomly.</p></list-item><list-item><p>Feeding the training data to the input layer.</p></list-item><list-item><p>Doing convolution operation and max-pooling operation in each conventional layer</p></list-item><list-item><p>Using the output of the last convolutional layer as the input of fully connected layer to obtain the result of the output layer</p></list-item><list-item><p>Evaluating the cost function and doing Adam optimization [<xref ref-type="bibr" rid="CR22">22</xref>] using the BP algorithm [<xref ref-type="bibr" rid="CR19">19</xref>] to refresh the parameters</p></list-item><list-item><p>Repeating step 2 to step 5 (one epoch) to recalculate the cost function until the desirable number of iterations is reached.</p></list-item></list></p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Feature ranking</title>
      <p>In our models, we integrated 36 different features to predict super enhancers, including H3K27ac, H3K4me1, H3K4me3, H3K9me3, Brd4, Cdk8, Cdk9, Med12, p300, CBP, Pol2, Lsd1, Brg1, Smc1, Nipbl, Mi2b, CHD7, H- DAC2, HDAC, DNaseI, 4-Oct, Sox2, Nanog, Smad3, Stat3, Tcf3, Esrrb, Klf4, Prdm14, Tcfcp2I1, Nr5a2, AT content, GC content, phastCons, phastConsP, re- peat fraction. To measure the predictive power of each feature, we computed the Pearson correlation coefficient between each feature vector and the output label vector of all test samples. Then, we ranked these features based on the calculated Pearson correlation coefficient.</p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Results and discussion</title>
    <sec id="Sec11">
      <title>Parameter tuning</title>
      <p>DEEPSEN was implemented on tensorflow [<xref ref-type="bibr" rid="CR23">23</xref>] with python. To investigate the impact of the number of convolutional layers on prediction performance, we constructed three models with different layers of convolutional neural networks, concretely, two, three and four convolutional layers. For simplification, these models are denoted as DEEPSEN-2L, DEEPSEN-3L and DEEPSEN-4L, respectively.</p>
      <p>For each model, although most parameters were tuned automatically in the training process of the convolutional neural networks, there are still some hyper-parameters to be determined. Here, the Adam optimization method [<xref ref-type="bibr" rid="CR22">22</xref>] was applied. We used grid search to tune the hyper-parameters, including learning rate, the number of epoches and the number of layers. Based on a number of preliminary experiments, we limit the parameters in the following ranges: the number of layers <italic>L</italic>: 2-4 (with stride 1); the number of epoches <italic>e</italic>: 50-150 (with stride 10); learning rate <italic>α</italic>: 10<sup>−5</sup>, 5 ×10<sup>−5</sup>, 10<sup>−4</sup>, 5 ×10<sup>−4</sup>, 10<sup>−3</sup>, 5 ×10<sup>−3</sup>, 10<sup>−2</sup>, 5 ×10<sup>−2</sup>.</p>
      <p>We used accuracy as evaluation metric to tune parameters. The results are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. For DEEPSEN-2L, when <italic>α</italic> is set between 0.00005 and 0.0001, it achieves better prediction accuracy. Generally, the accuracy increases with the number of epoches (for the number of epoches ≤ 140). We did not choose too large numbers of epoches for the reason of training efficiency. When <italic>α</italic> is set to between 0.01 to 0.05, the accuracy is fixed at 0.9 because <italic>α</italic> is so large that gradient descent algorithm can not perform well, and DEEPSEN-2L predicts all samples as negatives (note that the ratio of negatives over positives is 9). DEEPSEN-3L and DEEPSEN-4L show similar patterns on parameters tuning. Overall, the optimized learning rate is between 5* 10<sup>−4</sup> and 10<sup>−4</sup>, the optimized number of epoches is between 140-150. With such parameter setting, DEEPSEN-4L achieves a better overall performance. Thus, we chose DEEPSEN-4L as the final model to predict super-enhancers. In what follows, we compare our three models with existing methods in terms of evaluation metrics <italic>precision</italic>, <italic>recall</italic>, <italic>F1</italic> and <italic>AUC</italic>. The definitions of theses evaluation metrics is as follows. In classification task, TP denotes the true positives, FP denotes the false positives, TN denotes the true negatives and FN denotes the false negatives. ROC(Receiver Operating Characteristic) curve describe the relation between FP rate and TP rate, AUC is the area under curve.
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Precision = \frac{TP}{TP+FP}  $$ \end{document}</tex-math><mml:math id="M8"><mml:mtext mathvariant="italic">Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2019_3180_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>
<fig id="Fig3"><label>Fig. 3</label><caption><p>Accuracy results under different parameter sets. <bold>a</bold> Accuracy vs. epochs for different <italic>α</italic> (DEEPSEN-2L); <bold>b</bold> Accuracy vs. epochs for different <italic>α</italic> (DEEPSEN-3L); <bold>c</bold> Accuracy vs. epochs for different ? (DEEPSEN-4L)</p></caption><graphic xlink:href="12859_2019_3180_Fig3_HTML" id="MO3"/></fig>
</p>
      <p>
        <disp-formula id="Equ5">
          <label>5</label>
          <alternatives>
            <tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Recall = \frac{TP}{TP+FN}  $$ \end{document}</tex-math>
            <mml:math id="M10">
              <mml:mtext mathvariant="italic">Recall</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TP</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">TP</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">FN</mml:mtext>
                </mml:mrow>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12859_2019_3180_Article_Equ5.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ6">
          <label>6</label>
          <alternatives>
            <tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ F1 = \frac{2*Precision*Recall}{Precision+Recall}  $$ \end{document}</tex-math>
            <mml:math id="M12">
              <mml:mi>F</mml:mi>
              <mml:mn>1</mml:mn>
              <mml:mo>=</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                  <mml:mo>∗</mml:mo>
                  <mml:mtext mathvariant="italic">Precision</mml:mtext>
                  <mml:mo>∗</mml:mo>
                  <mml:mtext mathvariant="italic">Recall</mml:mtext>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">Precision</mml:mtext>
                  <mml:mo>+</mml:mo>
                  <mml:mtext mathvariant="italic">Recall</mml:mtext>
                </mml:mrow>
              </mml:mfrac>
            </mml:math>
            <graphic xlink:href="12859_2019_3180_Article_Equ6.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
    </sec>
    <sec id="Sec12">
      <title>Performance evaluation</title>
      <p>The <italic>F</italic>1 values of our three models under different hyper-parameter settings are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. For DEEPSEN-2L, the best performance is achieved with <italic>α</italic>=0.0001 and the number of epoches being 140. For DEEPSEN-3L, the best performance is obtained when <italic>α</italic>=0.00005 and the number of epoches is 140. As for DEEPSEN-4L, the best performance comes from <italic>α</italic>=0.00005 and the number of epoches being 130. So we can see that all the three models of DEEPSEN achieve the best <italic>F</italic>1 when <italic>α</italic> is between 0.00005 and 0.0001, and the number of epoches is between 130 and 140. This observation is also noticed on accuracy.
<fig id="Fig4"><label>Fig. 4</label><caption><p><italic>F1</italic> results under different parameter sets; <bold>a</bold> F1 vs. epochs for different <italic>α</italic> (DEEPSEN-2L). <bold>b</bold> F1 vs. epochs for different <italic>α</italic> (DEEPSEN-3L). <bold>c</bold> F1 vs. epochs for different <italic>α</italic> (DEEPSEN-4L)</p></caption><graphic xlink:href="12859_2019_3180_Fig4_HTML" id="MO4"/></fig>
</p>
      <p>The performance results of DEEPSEN with different structures are given in Table <xref rid="Tab5" ref-type="table">5</xref>, where the performance results of improse [<xref ref-type="bibr" rid="CR16">16</xref>] are listed for comparison. We can see that DEEPSEN-3L and DEEPSEN-4L perform better than improse in terms of precision, recall and F1. It demonstrates that the proposed DEEPSEN method outperforms the stat-of-the-art method improse. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows the performance comparison between our models and improse, and Fig. <xref rid="Fig6" ref-type="fig">6</xref> shows the best AUC of DEEPSEN-4L when <italic>α</italic>=0.00005 and the number of epoches is 110.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Performance comparison with improse</p></caption><graphic xlink:href="12859_2019_3180_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>The best ROC curve of DEEPSEN</p></caption><graphic xlink:href="12859_2019_3180_Fig6_HTML" id="MO6"/></fig><table-wrap id="Tab5"><label>Table 5</label><caption><p>Performance comparison with the state-of-the-art method</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th><th align="left">AUC</th></tr></thead><tbody><tr><td align="left">Improse</td><td align="left">0.88</td><td align="left">0.81</td><td align="left">0.84</td><td align="left">0.97</td></tr><tr><td align="left">DEEPSEN</td><td align="left">0.92</td><td align="left">0.88</td><td align="left">0.90</td><td align="left">0.97</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec13">
      <title>Performance comparison among different features</title>
      <p>The results of the first six correlated features are presented in Table <xref rid="Tab6" ref-type="table">6</xref>. The Pearson correlation coefficient indicates the contribution of each feature to prediction performance. For our method, the feature ranking according to Pearson correlation coefficient is: Med12, cdk8, Brd4, Cdk9, P300, H3K27ac, which is roughly similar to the findings of improse. The ranking given by improse is: Brd4, H3K27ac, Cdk8, Cdk9, Med12 and p300.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>The results of feature ranking</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">Features</td><td align="left">Med12</td><td align="left">Cdk8</td><td align="left">Brd4</td><td align="left">Cdk9</td><td align="left">p300</td><td align="left">H3K27ac</td></tr><tr><td align="left">Correlation</td><td align="left">0.746</td><td align="left">0.731</td><td align="left">0.684</td><td align="left">0.643</td><td align="left">0.618</td><td align="left">0.605</td></tr></tbody></table></table-wrap>
</p>
    </sec>
  </sec>
  <sec id="Sec14" sec-type="conclusion">
    <title>Conclusion</title>
    <p>In this paper, we proposed DEEPSEN, a new super-enhancer prediction method based on convolutional neural networks (CNNs). The data from GEO were used to train and test the proposed method. 36 kinds of features, including DNA sequence, histone modifications and TF bindings were integrated to train three models with 2, 3 and 4 convolutional layers. DEEPSEN uses a three-step scheme to construct and train CNN based classifiers. The first step is data preprocesing and feature calculation. The second step is to construct and train DEEPSEN. The third step is feature ranking. Our experimental results show that DEEPSEN outperforms the existing methods. DEEPSEN can be used with high-throughput experimental techniques to improve the accuracy of super-enhancer prediction.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>BP</term>
        <def>
          <p>Back-propagation</p>
        </def>
      </def-item>
      <def-item>
        <term>ChIP-seq</term>
        <def>
          <p>Chromatin Immunoprecipitation sequencing</p>
        </def>
      </def-item>
      <def-item>
        <term>CNNs</term>
        <def>
          <p>Convolutional neural networks</p>
        </def>
      </def-item>
      <def-item>
        <term>DL</term>
        <def>
          <p>Deep learning</p>
        </def>
      </def-item>
      <def-item>
        <term>DNase-seq</term>
        <def>
          <p>DNase I hypersensitive sites sequencing</p>
        </def>
      </def-item>
      <def-item>
        <term>eRNA</term>
        <def>
          <p>Enhancer RNA</p>
        </def>
      </def-item>
      <def-item>
        <term>GEO</term>
        <def>
          <p>Gene Expression O- minibus</p>
        </def>
      </def-item>
      <def-item>
        <term>KNN</term>
        <def>
          <p>K-nearest neighbors</p>
        </def>
      </def-item>
      <def-item>
        <term>mESCs</term>
        <def>
          <p>Mouse embryonic stem cells</p>
        </def>
      </def-item>
      <def-item>
        <term>SEs</term>
        <def>
          <p>Super-enhancers</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p>Support vector machine</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
    <sec id="d29e1770">
      <title>About this supplement</title>
      <p>This article has been published as part of <italic>BMC Bioinformatics, Volume 20 Supplement 15, 2019: Selected articles from the 14th International Symposium on Bioinformatics Research and Applications (ISBRA-18): bioinformatics</italic>. The full contents of the supplement are available at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-15">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-15</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>JH and SG designed the research and revised the manuscript. HD and JQ developed the method, carried out experiments, and drafted the manuscript. YL revised the manuscript. All authors have read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was funded by the National Natural Science Foundation of China (NSFC) (grant No. 61772367), which supported the collection, analysis and interpretation of data, the National Key Research and Development Program of China (grant No. 2016YFC0901704), which supported the publication costs, and the Shanghai Natural Science Foundation (grant No. 17ZR1400200), which supported the hardware and software device.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The data and materials are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/1991Troy/DEEPSEN">https://github.com/1991Troy/DEEPSEN</ext-link></p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pott</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lieb</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>What are super-enhancers?</article-title>
        <source>Nat Genet</source>
        <year>2015</year>
        <volume>47</volume>
        <issue>1</issue>
        <fpage>8</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1038/ng.3167</pub-id>
        <pub-id pub-id-type="pmid">25547603</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Banerji</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rusconi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schaffner</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Expression of a beta-globin gene is enhanced by remote sv40 dna sequences</article-title>
        <source>Cell</source>
        <year>1981</year>
        <volume>27</volume>
        <issue>2 Pt 1</issue>
        <fpage>299</fpage>
        <pub-id pub-id-type="doi">10.1016/0092-8674(81)90413-X</pub-id>
        <pub-id pub-id-type="pmid">6277502</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shlyueva</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Stampfel</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Stark</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Transcriptional enhancers: from properties to genome-wide predictions</article-title>
        <source>Nat Rev Genet</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>272</fpage>
        <pub-id pub-id-type="doi">10.1038/nrg3682</pub-id>
        <pub-id pub-id-type="pmid">24614317</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Deng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Reik</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gregory</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Blobel</surname>
            <given-names>GA</given-names>
          </name>
        </person-group>
        <article-title>Controlling long range genomic interactions at a native locus by targeted tethering of a looping factor</article-title>
        <source>Cell</source>
        <year>2012</year>
        <volume>149</volume>
        <issue>6</issue>
        <fpage>1233</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2012.03.051</pub-id>
        <pub-id pub-id-type="pmid">22682246</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tolhuis</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Palstra</surname>
            <given-names>R. J.</given-names>
          </name>
          <name>
            <surname>Splinter</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Grosveld</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>De</surname>
            <given-names>L. W.</given-names>
          </name>
        </person-group>
        <article-title>Looping and interaction between hypersensitive sites in the active beta-globin locus</article-title>
        <source>Mol Cell</source>
        <year>2002</year>
        <volume>10</volume>
        <issue>6</issue>
        <fpage>1453</fpage>
        <pub-id pub-id-type="doi">10.1016/S1097-2765(02)00781-5</pub-id>
        <pub-id pub-id-type="pmid">12504019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Visel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Blow</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Akiyama</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Holt</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Plajzer-Frick</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Shoukry</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wright</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Chip-seq accurately predicts tissue-specific activity of enhancers</article-title>
        <source>Nature</source>
        <year>2009</year>
        <volume>457</volume>
        <issue>7231</issue>
        <fpage>854</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1038/nature07730</pub-id>
        <pub-id pub-id-type="pmid">19212405</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thurman</surname>
            <given-names>RE</given-names>
          </name>
          <name>
            <surname>Rynes</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Humbert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Vierstra</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Maurano</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Haugen</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Sheffield</surname>
            <given-names>NC</given-names>
          </name>
          <name>
            <surname>Stergachis</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Vernot</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>The accessible chromatin landscape of the human genome</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>489</volume>
        <issue>7414</issue>
        <fpage>75</fpage>
        <lpage>82</lpage>
        <pub-id pub-id-type="doi">10.1038/nature11232</pub-id>
        <pub-id pub-id-type="pmid">22955617</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Consortium</surname>
            <given-names>TEP</given-names>
          </name>
        </person-group>
        <article-title>An integrated encyclopedia of dna elements in the human genome</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>489</volume>
        <issue>7414</issue>
        <fpage>57</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.1038/nature11247</pub-id>
        <pub-id pub-id-type="pmid">22955616</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Mortazavi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Wold</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide mapping of in vivo protein-dna interactions</article-title>
        <source>Science</source>
        <year>2007</year>
        <volume>316</volume>
        <issue>5830</issue>
        <fpage>1497</fpage>
        <lpage>502</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1141319</pub-id>
        <pub-id pub-id-type="pmid">17540862</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barski</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cuddapah</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Roh</surname>
            <given-names>TY</given-names>
          </name>
          <name>
            <surname>Schones</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chepelev</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>High-resolution profiling of histone methylations in the human genome</article-title>
        <source>Cell</source>
        <year>2007</year>
        <volume>129</volume>
        <issue>4</issue>
        <fpage>823</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2007.05.009</pub-id>
        <pub-id pub-id-type="pmid">17512414</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hnisz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Abraham</surname>
            <given-names>BJ</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>TI</given-names>
          </name>
          <name>
            <surname>Lau</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Saintandre</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Sigova</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Hoke</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>Super-enhancers in the control of cell identity and disease</article-title>
        <source>Cell</source>
        <year>2013</year>
        <volume>155</volume>
        <issue>4</issue>
        <fpage>934</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2013.09.053</pub-id>
        <pub-id pub-id-type="pmid">24119843</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Whyte</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Orlando</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hnisz</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Abraham</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kagey</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rahl</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>TI</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Master transcription factors and mediator establish super-enhancers at key cell identity genes</article-title>
        <source>Cell</source>
        <year>2013</year>
        <volume>153</volume>
        <issue>2</issue>
        <fpage>307</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2013.03.035</pub-id>
        <pub-id pub-id-type="pmid">23582322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Loven</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hoke</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>CY</given-names>
          </name>
          <name>
            <surname>Lau</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Orlando</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Vakoc</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>Bradner</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Tong</surname>
            <given-names>IL</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>Selective inhibition of tumor oncogenes by disruption of super-enhancers</article-title>
        <source>Cell</source>
        <year>2013</year>
        <volume>153</volume>
        <issue>2</issue>
        <fpage>320</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2013.03.036</pub-id>
        <pub-id pub-id-type="pmid">23582323</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vahedi</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kanno</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Furumoto</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Parker</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Erdos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Roychoudhuri</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Restifo</surname>
            <given-names>NP</given-names>
          </name>
          <name>
            <surname>Gadina</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Stretch-enhancers delineate disease-associated regulatory nodes in t cells</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>520</volume>
        <issue>7548</issue>
        <fpage>558</fpage>
        <lpage>62</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14154</pub-id>
        <pub-id pub-id-type="pmid">25686607</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Witte</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bradley</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Enright</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Muljo</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <article-title>High-density p300 enhancers control cell state transitions</article-title>
        <source>Bmc Genomics</source>
        <year>2015</year>
        <volume>16</volume>
        <issue>1</issue>
        <fpage>903</fpage>
        <pub-id pub-id-type="doi">10.1186/s12864-015-1905-6</pub-id>
        <pub-id pub-id-type="pmid">26546038</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Khan A, Zhang X (2017) Analysis and prediction of super-enhancers using sequence and chromatin signatures[J]. bioRxiv. 105262. 10.1101/105262. 10.1038/s41598-019-38979-9.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alipanahi</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Delong</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Weirauch</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Frey</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>Predicting the sequence specificities of dna- and rna-binding proteins by deep learning</article-title>
        <source>Nat Biotechnol</source>
        <year>2015</year>
        <volume>33</volume>
        <issue>8</issue>
        <fpage>831</fpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id>
        <pub-id pub-id-type="pmid">26213851</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Troyanskaya</surname>
            <given-names>OG</given-names>
          </name>
        </person-group>
        <article-title>Predicting effects of noncoding variants with deep learning" cbased sequence model</article-title>
        <source>Nat Methods</source>
        <year>2015</year>
        <volume>12</volume>
        <issue>10</issue>
        <fpage>931</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id>
        <pub-id pub-id-type="pmid">26301843</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rumelhart</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>RJ</given-names>
          </name>
        </person-group>
        <article-title>Learning representations by back-propagating errors</article-title>
        <source>Nature</source>
        <year>1986</year>
        <volume>323</volume>
        <issue>6088</issue>
        <fpage>533</fpage>
        <pub-id pub-id-type="doi">10.1038/323533a0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Langmead</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Trapnell</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pop</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Salzberg</surname>
            <given-names>SL</given-names>
          </name>
        </person-group>
        <article-title>Ultrafast and memory-efficient alignment of short dna sequences to the human genome</article-title>
        <source>Genome Biology</source>
        <year>2009</year>
        <volume>10</volume>
        <issue>3</issue>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2009-10-3-r25</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Salakhutdinov</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1929</fpage>
        <lpage>58</lpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J (2014) Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A, Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving G, Isard M, Jia Y, Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Mané D, Monga R, Moore S, Murray D, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar K, Tucker P, Vanhoucke V, Vasudevan V, Viégas F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X (2015) TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org <ext-link ext-link-type="uri" xlink:href="http://tensorflow.org/">http://tensorflow.org/</ext-link>. Accessed 1 Oct 2017.</mixed-citation>
    </ref>
  </ref-list>
</back>
