<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8050058</article-id>
    <article-id pub-id-type="publisher-id">87523</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-021-87523-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GraphCovidNet: A graph neural network based model for detecting COVID-19 from CT scans and X-rays of chest</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4177-6759</contrib-id>
        <name>
          <surname>Saha</surname>
          <given-names>Pritam</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6013-1067</contrib-id>
        <name>
          <surname>Mukherjee</surname>
          <given-names>Debadyuti</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9598-7981</contrib-id>
        <name>
          <surname>Singh</surname>
          <given-names>Pawan Kumar</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0106-7050</contrib-id>
        <name>
          <surname>Ahmadian</surname>
          <given-names>Ali</given-names>
        </name>
        <address>
          <email>ahmadian.hosseini@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3663-836X</contrib-id>
        <name>
          <surname>Ferrara</surname>
          <given-names>Massimiliano</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8813-4086</contrib-id>
        <name>
          <surname>Sarkar</surname>
          <given-names>Ram</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.216499.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0722 3459</institution-id><institution>Department of Electrical Engineering, </institution><institution>Jadavpur University, </institution></institution-wrap>Kolkata, 700032 India </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.216499.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0722 3459</institution-id><institution>Department of Computer Science and Engineering, </institution><institution>Jadavpur University, </institution></institution-wrap>Kolkata, 700032 India </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.216499.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0722 3459</institution-id><institution>Department of Information Technology, </institution><institution>Jadavpur University, </institution></institution-wrap>Kolkata, 700106 India </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.412113.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 1557</institution-id><institution>Institute of IR 4.0, </institution><institution>The National University of Malaysia, </institution></institution-wrap>Bangi, 43600 UKM Selangor Malaysia </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.507057.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 1779 9453</institution-id><institution>School of Mathematical Sciences, College of Science and Technology, </institution><institution>Wenzhou-Kean University, </institution></institution-wrap>Wenzhou, China </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.7945.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2165 6939</institution-id><institution>ICRIOS-The Invernizzi Centre for Research in Innovation, Organization, Strategy and Entrepreneurship, Department of Management and Technology, </institution><institution>Bocconi University, </institution></institution-wrap>Via Sarfatti, 25, 20136 Milan (MI), Italy </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>8304</elocation-id>
    <history>
      <date date-type="received">
        <day>15</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">COVID-19, a viral infection originated from Wuhan, China has spread across the world and it has currently affected over 115 million people. Although vaccination process has already started, reaching sufficient availability will take time. Considering the impact of this widespread disease, many research attempts have been made by the computer scientists to screen the COVID-19 from Chest X-Rays (CXRs) or Computed Tomography (CT) scans. To this end, we have proposed <bold>GraphCovidNet</bold>, a Graph Isomorphic Network (GIN) based model which is used to detect COVID-19 from CT-scans and CXRs of the affected patients. Our proposed model only accepts input data in the form of graph as we follow a GIN based architecture. Initially, pre-processing is performed to convert an image data into an undirected graph to consider only the edges instead of the whole image. Our proposed <bold>GraphCovidNet</bold> model is evaluated on four standard datasets: SARS-COV-2 Ct-Scan dataset, COVID-CT dataset, combination of covid-chestxray-dataset, Chest X-Ray Images (Pneumonia) dataset and CMSC-678-ML-Project dataset. The model shows an impressive accuracy of 99% for all the datasets and its prediction capability becomes 100% accurate for the binary classification problem of detecting COVID-19 scans. Source code of this work can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/debadyuti23/GraphCovidNet">GitHub-link</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Viral infection</kwd>
      <kwd>Computer science</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Recently, Coronavirus (COVID-19) disease has created an unprecedented situation across the world. Severe Acute Respiratory Syndrome coronavirus 2 (SARS-CoV-2), a novel virus enveloped with large single stranded RNA genome<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> is the root cause for this disease<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Although this virus is originated from Wuhan in China, in December 2019, later America and several other countries of Europe have severely affected in early days of the year 2020<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. According to recent statistics, both America and India have more number of confirmed cases than other affected countries. World Health Organization (WHO)<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> announced COVID-19 as a global health emergency on January 30, 2020 considering the adverse effects of this situation.</p>
    <p id="Par3">To diagnose the SARS-CoV-2, it has been observed that both CXRs as well as CT-scans are found to be beneficial<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. CXR images are more appreciated by the medical practitioners, since it can be obtained easily from the radiology departments. According to radiologists, CXR images help to understand the chest pathology clearly<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. However, CT scans provide high sensitivity, for example, 97% of the positive CT scans are confirmed in a case study in Wuhan<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Due to the exponential growth in cases, it is required to develop a automated and fast paced system which can identify COVID-19 from chest CT-scans or CXR images. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows some samples of these CT-scan and CXR images.<fig id="Fig1"><label>Figure 1</label><caption><p>Sample CT scans and CXRs of some patients (source: (i) CT-scan—SARS-COV-2 Ct-Scan Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, (ii) CXR—CMSC-678-ML-Project<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>).</p></caption><graphic xlink:href="41598_2021_87523_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par4">SARS-CoV-2 generally affects the lungs and turbid formation of cough around lungs can be detected from CT-scans and CXRs. The usual symptoms of COVID-19 are related to fever, dry cough and tiredness. The severity of COVID-19 symptoms can range from very mild to critical. Some people may show only a few symptoms, and sometimes no symptoms can be observed at all. In some cases, symptoms start worsen mere after a week and frequent shortness of breath and Pneumonia may happen. Elders and people with chronic medical conditions may possess a higher risk of serious illness from COVID-19. Now in case of mild COVID-19, CT-scans and CXR images may be inefficient as the cough clouds may not be prominent. Another failed case for CT-scan and CXR diagnosis may be any other kind of Pneumonia because of the presence of turbid lungs. So, an advanced classification model is needed to classify these CT-scans and CXR properly.</p>
    <p id="Par5">Due to this pandemic, a lot of people have been affected around the world and rapid tests are required with proper treatment and quarantine. Though Real Time Reverse Transcription Polymerase Chain Reaction (RT-PCR) test is the most common way to detect the virus, but the time required to get the results is around 1–2 days<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. So, an automatic and accurate classification method, where CT-scans or CXRs are used, can be considered as an alternative approach in order to deal with this pandemic whose turn around time is significantly less.</p>
    <p id="Par6">Although spread of COVID-19 has started recently, many research works have already been performed by the researchers during this short time span. Since the current problem consists of classifying COVID-19 images, various machine learning as well as deep learning methods have been proposed. In this section, a few works have been mentioned in brief.</p>
    <p id="Par7">Soares et al.<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> have used an explainable deep learning model called xDNN on the SARS-CoV-2 CT-scan dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> and have achieved 97.31% accuracy in the binary classification of scans between COVID and Non-COVID. Yang et al.<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> have introduced the COVID-CT dataset in their work. Since the total number of images are approximately 700 in the original dataset, they have used segmentation masks for lungs and lesion region to gain additional information from the original images. In their work, Contrastive Self-Supervised Learning (CSSL), an unsupervised learning approach<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> has been implemented to fine-tune the ImageNet pretrained models DenseNet-169<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and ResNet-50<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. Overall, they have achieved best accuracy as 89.1%. Pedro et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> have utilized the EfficientNet<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> model along with transfer learning citetranferlearning and have achieved accuracies 87.60% and 98.99% for COVID-CT dataset<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> and SARS-CoV-2 CT-scan dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> respectively. Sharma et al.<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> have applied ResNet<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> on the database consisting of datasets: (i) GitHub COVID-CT dataset<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, (ii) COVID dataset provided by Italian Society of Medical and Interventional Radiology<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, (iii) dataset provided by hospitals of Moscow, Russia<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, (iv) dataset provided by SAL Hospital, Ahmedabad, India<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> and have obtained almost 91% accuracy.</p>
    <p id="Par8">Elaziz et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> have used a modified version of Manta-Ray Foraging Optimization (MRFO) for feature selection and later have used K-Nearest neighbor (KNN)<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> for classification. They have considered two COVID datasets: (i) combined database of: covid-chestxray-dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> and ChestX-Ray Images (Pneumonia) dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, (ii) dataset provided by Chowdhury et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. They have achieved 96.09% and 98.09% accuracies on those two datasets respectively. Turkoglu et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> have proposed an COVIDetectioNet model where they have used transfer learning on a pre-trained Convolutional Neural Network (CNN)<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> called AlexNet. They have used Relief feature selection algorithm from all the layers of the architecture, and for the classification they have used Support Vector Machine (SVM)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. They have conducted their experiments on the combined dataset of: (i) dataset provided by Chowdhury et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, (ii) Chest X-RayImages (Pneumonia) dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> and (iii) COVID-19 Radiography Database<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Their proposed model has predicted 99.18%. accurate results on the combined dataset. Oh et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> have performed semantic segmentation by using an extended fully convolutional (FC)-DenseNet103<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> and later have used ResNet-18<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> on the combined database of: (i) JSRT dataset<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, (ii) SCR dataset<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, (iii) NLM(MC) dataset<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, (iv) covid-chestxray-dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, (v) CoronaHack dataset<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. In their work, 88.9% accurate results have been achieved.</p>
    <p id="Par9">Nour et al.<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> have proposed a five-layer CNN model on the COVID-19 radiology database<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. This dataset is composed of different benchmark datasets<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR35">35</xref></sup>. After extracting features from the proposed CNN model, basic machine learning algorithms KNN<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, SVM<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> and Decision Tree (DT)<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> are applied on the extracted features. State-of-the-art result is achieved using SVM with accuracy 98.97%. Chandra et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> have used majority voting based ensemble of five classifiers—SVM<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, KNN<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, DT<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, Artificial Neural Network (ANN)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, Naive Bayes (NB)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> on the database consisting of three publicly available CXR image datasets: covid-chestxray dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, Montgomery dataset<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, and NIH ChestX-ray14 dataset<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Among the total 8196 features extracted from all the pre-processed images, 8 are First Order Statistical Features (FOSF)<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, 88 are Grey Level Co-occurrence Matrix (GLCM)<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> based features and the rest 8100 are Histogram of Oriented Gradients (HOG)<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> features. The proposed classifier ensemble has predicted with 98.06% and 93.41% accuracy for 2 class (normal and abnormal) and 3 class (i.e., normal, COVID-19 and Pneumonia) classification problems respectively.Hemdam et al.<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> have used seven benchmark image classifier models: VGG19<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, DenseNet201<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, InceptionV3<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>, ResNetV2<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, Inception-ResNet-V2<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, Xception<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, MobileNetV2<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> on the dataset combined from covid-chestxray-dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> and dataset provided by Dr. Rosebrock<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. VGG19 and DenseNet201 have provided results with best accuracy as 90%.</p>
    <p id="Par10">Makris et al.<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> have used various existing CNN models along with transfer learning on the CXR images collected from sources: covid-chestxray dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> and Chest X-Ray Images dataset by Mooney et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Among all the used models, VGG16 and VGG19<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> have provided the best accuracy as 95%. Zhong et al.<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> have used a CNN model based on VGG16<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> architecture on the database consisted of- covid-chestxray-dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, ChestX-RayImages (Pneumonia) dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, Figure 1 COVID-19 Chest X-ray Dataset Initiative dataset<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> and ActualMed COVID-19 Chest X-ray Dataset Initiative dataset<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. Finally, they have obtained 87.3 % accurate results on their work. Sun et al.<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> have proposed an Adaptive Feature Selection guided Deep Forest (AFS-DF) algorithm and have achieved 91.79% accurate results on the CT scan database collected from the Third Hospital of Jilin University, Ruijin Hospital of Shanghai Jiao Tong University, Tongji Hospital of Huazhong University of Science and Technology, Shanghai Public Health Clinical Center of Fudan University, Hangzhou First People’s Hospital of Zhejiang University, and Sichuan University West China Hospital.</p>
    <p id="Par11">Chattopadhyay et al.<sup><xref ref-type="bibr" rid="CR57">57</xref></sup> have contributed in two-ways in their work in this domain. After extracting deep features from the original image dataset, they have applied a completely novel meta-heuristic feature selection approach named Clustering-based Golden Ratio Optimizer (CGRO). They have conducted the necessary experiments on the SARS-COV-2 Ct-Scan Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, COVID-CT dataset<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> and Chest X-Ray dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> and have achieved the state-of-the-art accuracies of 99.31%, 98.65%, 99.44% respectively.</p>
    <p id="Par12">Sen et al.<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> have proposed a CNN architecture and bi-stage Feature Selection (FS) approach to extract the most relevant features from the chest CT-scan images. Initially, they have applied a guided FS methodology by employing two filter procedures: (i) Mutual Information (MI), (ii)Relief-F. In the second stage, Dragonfly algorithm (DA) has been used for the further selection of the most relevant features. Finally, SVM has been applied to the overall feature set. The proposed model has been tested on two open-access datasets: SARS-CoV-2<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> CT images and COVID-CT<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> datasets and has got 98.39% and 90.0% accuracies on the said datasets respectively.</p>
    <p id="Par13">Besides classification of CT-scans and CXRs, there are other research fields related to COVID-19. One such field is mask detection. Loey et al.<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> have used first ResNet50<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> and then an ensemble of DT and SVM for the final classification. They have achieved best results for the SVM classifier with 99.64%, 99.49% and 100% accuracies for the three datasets: e Real-World Masked Face Dataset (RMFD)<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, the Simulated Masked Face Dataset (SMFD)<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>, and the Labeled Faces in the Wild (LFW)<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> respectively.</p>
    <p id="Par14">From the above mentioned works, it is clear that in most of the cases pre-existing or novel CNN<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> models are used as a classifier since this is basically an image classification problem. However, CNN has some limitations, for example, it can be overfitted when there is some class imbalance in the dataset<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. On the other hand, Graph Neural Network (GNN)<sup><xref ref-type="bibr" rid="CR64">64</xref></sup> based models can overcome the problems like: overfitting and class imbalance. From the experimental results found in other fields, it is evident that a GNN based model generally works fast<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. GNN, a relatively new approach in the field of deep learning domain, is applied for graph classification problems. So, GNN requires input data represented in the form of graph data structure. Whereas, any 2D-CNN model directly accepts a 2D image matrix as input. Therefore, we need a proper technique for mapping an image classification problem to a graph classification one. We have resolved this issue with the help of an appropriate pre-processing technique to convert an image into a graph data. Considering all the advantages and novelties of GNN approach, we have implemented our proposed <bold>GraphCovidNet</bold>, a Graph Isomorphism Network (GIN)<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> based model (a special category of GNN) called <bold>GraphCovidNet</bold>.</p>
    <p id="Par15">The experimental results show that our proposed model performs very well with respect to time-requirement by the model. Our architecture has also performed well for highly class imbalanced dataset due to the injective nature of the aggregation function. The architecture is able to map different graphs into different representations in the embedding space properly. Hence, the proposed model is able to identify the class with a lesser image count perfectly. We have used four publicly available datasets: (i) SARS-COV-2 Ct-Scan Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, (ii) COVID-CT dataset<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, (iii) 3-class and 4-class datasets under CMSC-678-ML-Project<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, (iv) combination of two datasets: (1) covid-chestxray-dataset available on GitHub<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, (2) Chest X-Ray Images (Pneumonia) dataset available on Kaggle<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. The main contributions of our work can be summarized as follows:<list list-type="bullet"><list-item><p id="Par16">In our work, we have introduced a new classification model, called <bold>GraphCovidNet</bold>, for screening COVID-19 CT-scan and CXR images.</p></list-item><list-item><p id="Par17">In the proposed model, we have used GIN as its backbone architecture which falls under a specialized category of GNN. Based on authors’ knowledge, any GNN based architecture has not been used previously in this domain.</p></list-item><list-item><p id="Par18">We have mapped image classification problem into a graph classification problem with proper pre-processing technique.</p></list-item><list-item><p id="Par19">We have also reduced the space complexity of our model by considering only the edges of an image instead of the whole image which, in turn, makes our approach computationally inexpensive.</p></list-item><list-item><p id="Par20">Our approach is not limited to a particular type of input as we have considered both CT-scan and CXR images and we have also worked binary to multi-class classification problem.</p></list-item><list-item><p id="Par21">Our model has also surpassed the existing state-of-the-art approaches.</p></list-item></list>Our proposed method is diagrammatically represented in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Figure 2</label><caption><p>Generic framework of our proposed <bold>GraphCovidNet</bold> model for COVID-19 detection from CT-scan or CXR images (Sample CT-scan image source: CT-scan—SARS-COV-2 Ct-Scan Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>).</p></caption><graphic xlink:href="41598_2021_87523_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par22">Our entire work has several sections that include: (1) <xref rid="Sec1" ref-type="sec">Introduction</xref>, (2) <xref rid="Sec2" ref-type="sec">Results and discussion</xref>, (3) <xref rid="Sec3" ref-type="sec">Methodology</xref>, (4) <xref rid="Sec12" ref-type="sec">Conclusion</xref>, (5) Data availability and finally, (6) Code availability.</p>
  </sec>
  <sec id="Sec2">
    <title>Results and discussion</title>
    <p id="Par23">In our experiments, we have used 5-fold cross-validation for evaluating the model. During each fold, the training is done for 10 epochs. We have used Adam optimizer and stochastic gradient descent (SGD) approach with a learning rate of 0.001 to train our model.</p>
    <p id="Par24">Here we have used five standard evaluation metrics such as Accuracy, Precision, Recall, F1 Score and Receiver Operating Characteristic (ROC) curve to assess our model performance. Table <xref rid="Tab1" ref-type="table">1</xref> shows the performance results as well as the average time taken for both training and testing in each fold given by our proposed <bold>GraphCovidNet</bold> model for all the four datasets.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Detailed results of the proposed <bold>GraphCovidNet</bold> model for all the four datasets in terms of some standard evaluation metrics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Category</th><th align="left">Accuracy (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1 Score (%)</th><th align="left">Training time (s)</th><th align="left">Testing time (s)</th></tr></thead><tbody><tr><td align="left">SARS-COV-2 Ct-Scan Dataset</td><td align="left">–</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">342.586</td><td align="left">2.328</td></tr><tr><td align="left">COVID-CT dataset</td><td align="left">–</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">146.365</td><td align="left">1.151</td></tr><tr><td align="left">covid-chestxray-dataset + Chest X-Ray Images (Pneumonia) dataset</td><td align="left">–</td><td align="left">99.84</td><td align="left">99.84</td><td align="left">99.84</td><td align="left">99.84</td><td align="left">1071.296</td><td align="left">7.138</td></tr><tr><td align="left" rowspan="2">CMSC-678-ML-Project GitHub</td><td align="left">3-class</td><td align="left">99.11</td><td align="left">99.11</td><td align="left">99.11</td><td align="left">99.11</td><td align="left">66.923</td><td align="left">0.6</td></tr><tr><td align="left">4-class</td><td align="left">99</td><td align="left">99</td><td align="left">99</td><td align="left">99</td><td align="left">73.697</td><td align="left">0.612</td></tr></tbody></table></table-wrap></p>
    <p id="Par25">From Table <xref rid="Tab1" ref-type="table">1</xref>, it is clear that the <bold>GraphCovidNet</bold> model has achieved at least 99% accuracy for all the datasets, whereas it gives 100% accuracy for the 2-class datasets. Generally, with increase in number of classes, our proposed model’s prediction capability drops from 100 to 99%. One notable point is that our proposed model provides nearly perfect (99.84%) accuracy for the heavily class imbalanced combined database of- covid-chestxray-dataset, Chest X-Ray Images (Pneumonia) dataset. Intuitively it can be said that a powerful GNN maps two nodes to the same location only if they have identical sub-trees with identical features on the corresponding nodes. Sub-tree structures are defined recursively via node neighborhoods. Thus, we can reduce our analysis to the question whether a GNN maps two neighborhoods (i.e., two multi-sets) to the same embedding or representation. A maximally powerful GNN would never map two different neighborhoods, i.e., multi-sets of feature vectors to the same representation. This means its aggregation scheme must be injective. Thus, it can be said that a powerful GNN’s aggregation scheme is able to represent injective multi-set functions.</p>
    <sec id="FPar1">
      <title>Theorem</title>
      <p id="Par26">Let <italic>A</italic> : <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G \rightarrow R^d$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq1.gif"/></alternatives></inline-formula> be a GNN. With a sufficient number of GNN layers, <italic>A</italic> maps any graphs, say, <italic>G</italic>1 and <italic>G</italic>2 such that the Weisfeiler–Lehman test of isomorphism decides as non-isomorphic, to different embeddings if the following conditions hold:<list list-type="bullet"><list-item><p id="Par27"><italic>A</italic> aggregates and updates node features iteratively with <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v^{(k)} = \phi (h_v^{(k-1)},f({h_u^{(k-1)} : u \in N(v)})$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq2.gif"/></alternatives></inline-formula>, where the function,<italic>f</italic>, which operates on multi-sets, and <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi$$\end{document}</tex-math><mml:math id="M6"><mml:mi>ϕ</mml:mi></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq3.gif"/></alternatives></inline-formula> are injective.</p></list-item><list-item><p id="Par28"><inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A's$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:msup><mml:mi>A</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>s</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq4.gif"/></alternatives></inline-formula> graph-level readout, which operates on the multi-set of node features, is injective.</p></list-item></list></p>
      <p>The mathematical proof of the above theorem is already reported in<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. The GIN follows this theorem. As this network is able to map any two different graphs into different embeddings, which helps to solve the challenging graph isomorphism problem. That is, isomorphic graphs are required to be mapped to the same representation, whereas the non-isomorphic ones to different representations. Due to these reasons, the proposed model even works well on heavily class imbalanced datasets. Based on the data from Table <xref rid="Tab1" ref-type="table">1</xref>, it is also notable that our proposed model takes considerably less time both in training (1–18 min) and testing (0.6–7 s) phases. Less number of epochs is also responsible for such low training time. But again, training loss becomes very less from the very beginning. So, there is no need to consider a large number of epochs for training purpose. We can visualize this low training loss from Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Figure 3</label><caption><p>Graphical representation of training loss and training accuracy vs epoch for each of the chosen datasets.</p></caption><graphic xlink:href="41598_2021_87523_Fig3_HTML" id="MO3"/></fig></p>
      <p>In Fig. <xref rid="Fig3" ref-type="fig">3</xref>, it is evident that at the first epoch, accuracy is at least 99%, whereas the loss is barely 0.4 for each of the datasets. Further, training reduces the loss value to almost 0, whereas the classification accuracy remains either almost the same or slightly increases with increasing epoch size. Since the change in loss is more prominent as compared to the change in overall accuracy, however, the accuracy seems constant as seen from Fig. <xref rid="Fig3" ref-type="fig">3</xref>. Due to proper pre-processing, the proposed architecture is able to understand the input graphs properly. Thus the loss becomes very low from beginning and training gets completed in at most 10 epochs. To verify more about the goodness of our classification model, we have generated Receiver Operating Characteristic (ROC) curves for each of the datasets which are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. Additionally, we have conducted experiments by varying the training to testing ratio from 10% to 90% with an interval of 10%. To have a better visualization, we have generated graphs of training and testing accuracies vs training to testing ratio for each of the datasets which are shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>ROC curves generated by our proposed <bold>GraphCovidNet</bold> model for each of the datasets.</p></caption><graphic xlink:href="41598_2021_87523_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Figure 5</label><caption><p>Graphical representation of training and testing accuracies vs training ratio for each of the chosen datasets.</p></caption><graphic xlink:href="41598_2021_87523_Fig5_HTML" id="MO5"/></fig></p>
      <p>So, from Fig. <xref rid="Fig4" ref-type="fig">4</xref>, it is evident that for all kind of training to testing ratios, the <bold>GraphCovidNet</bold> model predicts at least 95% samples correctly, which is a sign of its robustness. Figure <xref rid="Fig5" ref-type="fig">5</xref> further proves its success as a classifier because the Area Under the Curve (AUC) for each of the ROC curves is 0.97 units at worst. the AUC for both 2-class datasets is 1 unit and ROC is also perfect. In short, the <bold>GraphCovidNet</bold> model is able to deal with both of the 2-class datasets regardless of the training to testing ratio. We have also conducted experiments on different datasets having equal number of classes for both training and testing purposes. The results of all such training-testing combinations are enlisted in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Detailed results of the proposed <bold>GraphCovidNet</bold> model for all combination of different train-test datasets having same number of classes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Train data</th><th align="left">Test data</th><th align="left">Number of classes</th><th align="left">Accuracy (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1 Score (%)</th></tr></thead><tbody><tr><td align="left">SARS-COV-2 Ct-Scan Dataset</td><td align="left">COVID-CT dataset</td><td align="left">2</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">100</td></tr><tr><td align="left">COVID-CT dataset</td><td align="left">SARS-COV-2 Ct-Scan Dataset</td><td align="left">2</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">100</td></tr><tr><td align="left">covid-chestxray-dataset + Chest X-Ray Images (Pneumonia) dataset</td><td align="left">CMSC-678-ML-Project GitHub (3-class)</td><td align="left">3</td><td align="left">98.78</td><td align="left">99.02</td><td align="left">97.98</td><td align="left">98.54</td></tr><tr><td align="left">CMSC-678-ML-Project GitHub (3-class)</td><td align="left">covid-chestxray-dataset + Chest X-Ray Images (Pneumonia) dataset</td><td align="left">3</td><td align="left">99.32</td><td align="left">99.23</td><td align="left">99.45</td><td align="left">99.30</td></tr></tbody></table></table-wrap></p>
      <p>Table <xref rid="Tab2" ref-type="table">2</xref> shows that proposed model ensures accuracy above 98% even when training and testing data are from two different sources. Such highly accurate results further confirm the validity of <bold>GraphCovidNet</bold>.</p>
      <p>To further ensure the superiority of our proposed model, we have also compared its performance against some pretrained CNN models such as Inception-ResNet-V2<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, VGG19<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, ResNet152<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, DenseNet201<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, Xception<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, MobileNetV2<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> for both raw and edge-mapped images. Table <xref rid="Tab3" ref-type="table">3</xref> shows the accuracies (%) obtained in all the experiments considering the mentioned CNN models.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Accuracies(%) obtained by applying Inception-ResNet-V2, VGG19, ResNet152, DenseNet201, Xception, MobileNetV2 models for both raw and edge-mapped images.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Model</th><th align="left" colspan="2">SARS-COV-2 Ct-Scan Dataset</th><th align="left" colspan="2">COVID-CT dataset</th><th align="left" colspan="2">covid-chestxray-dataset + Chest X-Ray Images (Pneumonia) dataset</th><th align="left" colspan="2">CMSC-678-ML-Project GitHub (3-class)</th><th align="left" colspan="2">CMSC-678-ML-Project GitHub (4-class)</th></tr><tr><th align="left">Raw image</th><th align="left">Edge image</th><th align="left">Raw image</th><th align="left">Edge image</th><th align="left">Raw image</th><th align="left">Edge image</th><th align="left">Raw image</th><th align="left">Edge image</th><th align="left">Raw image</th><th align="left">Edge image</th></tr></thead><tbody><tr><td align="left">Inception-ResNet-V2</td><td align="left">77.85</td><td align="left">80.08</td><td align="left">74.35</td><td align="left">78.95</td><td align="left">98.22</td><td align="left">98.05</td><td char="." align="char">82.61</td><td align="left">91.3</td><td char="." align="char">77.56</td><td align="left">86.45</td></tr><tr><td align="left">VGG19</td><td align="left">78.27</td><td align="left">82.55</td><td align="left">79.60</td><td align="left">84.27</td><td align="left">98.45</td><td align="left">96.50</td><td char="." align="char">86.96</td><td align="left">97.83</td><td char="." align="char">79.65</td><td align="left">92.2</td></tr><tr><td align="left">ResNet152</td><td align="left">77.87</td><td align="left">84.58</td><td align="left">86.65</td><td align="left">87.97</td><td align="left">98.68</td><td align="left">97.82</td><td char="." align="char">91.31</td><td align="left">91.40</td><td char="." align="char">86.13</td><td align="left">85.88</td></tr><tr><td align="left">DenseNet201</td><td align="left">75.86</td><td align="left">85.69</td><td align="left">89.11</td><td align="left">90.21</td><td align="left">99.07</td><td align="left">97.35</td><td char="." align="char">95.65</td><td align="left">96.13</td><td char="." align="char">88.65</td><td align="left">90.44</td></tr><tr><td align="left">Xception</td><td align="left">83.30</td><td align="left">81.79</td><td align="left">82.01</td><td align="left">87.58</td><td align="left">96.74</td><td align="left">99.22</td><td char="." align="char">82.61</td><td align="left">86.96</td><td char="." align="char">82.15</td><td align="left">83.97</td></tr><tr><td align="left">MobileNetV2</td><td align="left">77.46</td><td align="left">80.48</td><td align="left">78.18</td><td align="left">76.97</td><td align="left">98.76</td><td align="left">98.52</td><td char="." align="char">93.48</td><td align="left">84.74</td><td char="." align="char">81.45</td><td align="left">82.25</td></tr></tbody></table></table-wrap></p>
      <p>Comparison between Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab3" ref-type="table">3</xref> validates that <bold>GraphCovidNet</bold> outperforms all these conventional CNN models which gives a more clear view about the robustness of our proposed model.</p>
      <p>We have also compared the results of our proposed <bold>GraphCovidNet</bold> model with some past works done on the chosen datasets. Table <xref rid="Tab4" ref-type="table">4</xref> demonstrates such comparative results.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison of our proposed <bold>GraphCovidNet</bold> model with some previous works on all the datasets (Oh et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, Chandra et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, Nour et al.<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, Hemdam et al.<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, Turkoglu et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> have combined other dataset; Oh et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, Chandra et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, Hemdam et al.<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> have considered the first dataset only; Nour et al.<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, Turkoglu et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> have considered the second dataset only).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Authors</th><th align="left">Methodology</th><th align="left">Accuracy (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">F1-score (%)</th></tr></thead><tbody><tr><td align="left" rowspan="3">SARS-COV-2 Ct-Scan Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup></td><td align="left">Soares et al.<sup><xref ref-type="bibr" rid="CR8">8</xref></sup></td><td align="left">xDNN</td><td align="left">97.38</td><td align="left">99.16</td><td align="left">95.53</td><td align="left">97.31</td></tr><tr><td align="left">Pedro et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup></td><td align="left">EfficientNet with transfer learning</td><td align="left">98.99</td><td align="left">99.20</td><td align="left">98.80</td><td align="left">99</td></tr><tr><td align="left">Proposed</td><td align="left">GraphCovidNet</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">100</td></tr><tr><td align="left" rowspan="3">COVID-CT dataset<sup><xref ref-type="bibr" rid="CR11">11</xref></sup></td><td align="left">Pedro et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup></td><td align="left">EfficientNet with transfer learning</td><td align="left">87.68</td><td align="left">93.98</td><td align="left">79.59</td><td align="left">86.19</td></tr><tr><td align="left">Yang et al.<sup><xref ref-type="bibr" rid="CR11">11</xref></sup></td><td align="left">Segmentation masks with CSSL</td><td align="left">89.1</td><td align="left">–</td><td align="left">–</td><td align="left">89.6</td></tr><tr><td align="left">Proposed</td><td align="left">GraphCovidNet</td><td align="left">100</td><td align="left">100</td><td align="left">100</td><td align="left">100</td></tr><tr><td align="left" rowspan="9">covid-chestxray-dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>+Chest X-Ray Images (Pneumonia) dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup></td><td align="left">Makris et al.<sup><xref ref-type="bibr" rid="CR53">53</xref></sup></td><td align="left">VGG16 and VGG19 with transfer learning</td><td align="left">95.88</td><td align="left">COVID-96 normal-95 Pneumonia-95</td><td align="left">COVID-96 normal-100 Pneumonia-91</td><td align="left">COVID-98 normal-98 Pneumonia-98</td></tr><tr><td align="left">Elaziz et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup></td><td align="left">MRFO + KNN</td><td align="left">96.09</td><td align="left">98.75</td><td align="left">98.75</td><td align="left">98.75</td></tr><tr><td align="left">Zhong et al.<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></td><td align="left">VGG16 based CNN model</td><td align="left">87.3</td><td align="left">89.67</td><td align="left">84.4</td><td align="left">86.96</td></tr><tr><td align="left">Oh et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup></td><td align="left">DenseNet103 for segmentation + ResNet-18</td><td align="left">88.9</td><td align="left">83.4</td><td align="left">85.9</td><td align="left">84.4</td></tr><tr><td align="left">Chandra et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup></td><td align="left">Majority voting of SVM, KNN, DT, ANN, NB</td><td align="left">93.41</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">Nour et al.<sup><xref ref-type="bibr" rid="CR3">3</xref></sup></td><td align="left">CNN for feature extraction + SVM</td><td align="left">98.97</td><td align="left">–</td><td align="left">89.39</td><td align="left">96.72 (F-score)</td></tr><tr><td align="left">Hemdam et al.<sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td><td align="left">VGG19 or DenseNet201</td><td align="left">90</td><td align="left">COVID-83 Normal-100</td><td align="left">COVID-100 Normal-80</td><td align="left">COVID-91 Normal-89</td></tr><tr><td align="left">Turkoglu et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup></td><td align="left">AlexNet+ Relief feature selection algorithm and SVM</td><td align="left">99.18</td><td align="left">99.48</td><td align="left">99.13</td><td align="left">99.30</td></tr><tr><td align="left">Proposed</td><td align="left">GraphCovidNet</td><td align="left">99.84</td><td align="left">99.84</td><td align="left">99.84</td><td align="left">99.84</td></tr></tbody></table></table-wrap></p>
      <p>From Table <xref rid="Tab4" ref-type="table">4</xref>, it is clear that our proposed approach surpasses all the previous works considered here for comparison in terms of accuracy. Although some of the listed previous works are done on database different or even larger than ours, the <bold>GraphCovidNet</bold> model still outperforms the ones on the same dataset. Based on our knowledge, there are no previous works performed on the CMSC-678-ML-Project GitHub dataset<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Still there are very few works previously done on a 4-class database in the domain of COVID-19 classification. So, we have considered to note down the results of CMSC-678-ML-Project GitHub dataset<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Not only that, any deep learning network generally is unable to achieve high accuracy for very less number of input samples such as CMSC-678-ML-Project GitHub dataset<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. But <bold>GraphCovidNet</bold> is able to predict with 99% and 99.11% accuracy for its 3-class and 4-class cases respectively as shown in Table <xref rid="Tab1" ref-type="table">1</xref>. So, our proposed model is able to perform very well even in case of datasets having very small number of samples.</p>
      <p>In a nutshell, we can say that our proposed model is very accurate, and robust with respect to other existing models.</p>
    </sec>
  </sec>
  <sec id="Sec3">
    <title>Methodology</title>
    <p id="Par38">In this section, we have discussed our proposed work along with the proper pre-processing required for COVID-19 image classification. We have also described the benchmark datasets briefly. This section consists of three subsections: (i) <xref rid="Sec4" ref-type="sec">Datasets used</xref>, (ii) <xref rid="Sec5" ref-type="sec">Pre-processing</xref>, and (iii) <xref rid="Sec8" ref-type="sec">Proposed model</xref>.</p>
    <sec id="Sec4">
      <title>Datasets used</title>
      <p id="Par39">In our work, we have selected the following four datasets to conduct the experiments individually- <list list-type="order"><list-item><p id="Par40">SARS-COV-2 Ct-Scan Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, a 2-class CT scan dataset collected by Plamen et al. available on Kaggle.</p></list-item><list-item><p id="Par41">COVID-CT dataset<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, a 2-class CT scan dataset introduced by Yang et al. available on GitHub.</p></list-item><list-item><p id="Par42">3-class dataset which is consisted of CXR from the two sources-<list list-type="bullet"><list-item><label/><p id="Par43">covid-chestxray-dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> collected by Cohen et al. available on GitHub.</p></list-item><list-item><label/><p id="Par44">Chest X-Ray Images (Pneumonia) dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> collected by Mooney et al. available on Kaggle.</p></list-item></list> For combining these two datasets, we have considered COVID-19 patients’ scans from the covid-chestxray-dataset and normal, Pneumonia patients’ scans from the Chest X-Ray Images (Pneumonia) dataset.</p></list-item><list-item><p id="Par45">Finally, 3-class and 4-class CXR datasets under the CMSC-678-ML-Project available on GitHub<sup><xref ref-type="bibr" rid="CR9">9</xref></sup></p></list-item></list>In this section, at first we look at the datasets used in the present work than training constraints alongside the detailed results of the experiments.</p>
      <p id="Par46">Basically, all the 2-class datasets contain classes of COVID and Non-COVID whereas the 3-class datasets contain: Normal, COVID, and Pneumonia. For the 4-class dataset of CMSC-678-ML Project, there are two separate classes of Pneumonia, which are: Bacterial Pneumonia and Viral Pneumonia. Table <xref rid="Tab5" ref-type="table">5</xref> illustrates the details of these datasets.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Statistical description of all the datasets used for experimentation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Dataset</th><th align="left" rowspan="2">Number of classes</th><th align="left" rowspan="2">Scan-type</th><th align="left" colspan="3">Total number of images</th></tr><tr><th align="left">Normal</th><th align="left">COVID-19</th><th align="left">Pneumonia</th></tr></thead><tbody><tr><td align="left">SARS-COV-2 Ct-Scan Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup></td><td align="left">2</td><td align="left">CT</td><td align="left">1229</td><td align="left">1252</td><td align="left">–</td></tr><tr><td align="left">COVID-CT dataset<sup><xref ref-type="bibr" rid="CR11">11</xref></sup></td><td align="left">2</td><td align="left">CT</td><td align="left">407</td><td align="left">349</td><td align="left">–</td></tr><tr><td align="left">covid-chestxray-dataset<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>+Chest X-Ray Images (Pneumonia) dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup></td><td align="left">3</td><td align="left">CXR</td><td align="left">1592</td><td align="left">504</td><td align="left">4343</td></tr><tr><td align="left" rowspan="2">CMSC-678-ML-Project GitHub dataset<sup><xref ref-type="bibr" rid="CR9">9</xref></sup></td><td align="left">3</td><td align="left">CXR</td><td align="left">79</td><td align="left">69</td><td align="left">79</td></tr><tr><td align="left">4</td><td align="left">CXR</td><td align="left">79</td><td align="left">69</td><td align="left">Bacterial: 79, Viral: 79</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec5">
      <title>Pre-processing</title>
      <p id="Par47">As mentioned earlier, the CT scans or CXRs are first pre-processed in order to apply our proposed <bold>GraphCovidNet</bold> model. We have considered two stages for pre-processing, which are illustrated as follows: <list list-type="order"><list-item><p id="Par48">Edge detection: First, the edges of the raw images are estimated using Prewitt filter<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>.</p></list-item><list-item><p id="Par49">Graph preparation: Next, these edge maps are converted into graph dataset by proper means.</p></list-item></list>Now these two stages are explained to have a better understanding of the whole pre-processing part.</p>
      <sec id="Sec6">
        <title>Edge detection</title>
        <p id="Par50">Basically, an edge is a region denoting a local change of intensity in an image which means that a local maxima or minima will occur for the change of intensity in the edge region. By applying proper filter on the original image, the edges can be prominent. In our work, we have convoluted the original image matrix with 3*3 Prewitt filter<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> for both horizontal and vertical edge detection which are defined as: <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{bmatrix} -1 &amp;{}&amp;{} -1 &amp;{}&amp;{} -1\\ 0 &amp;{}&amp;{} 0 &amp;{}&amp;{} 0\\ 1 &amp;{}&amp;{} 1 &amp;{}&amp;{} 1\\ \end{bmatrix}$$\end{document}</tex-math><mml:math id="M10"><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq5.gif"/></alternatives></inline-formula> and <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{bmatrix} -1 &amp;{}&amp;{} 0 &amp;{}&amp;{} 1\\ -1 &amp;{}&amp;{} 0 &amp;{}&amp;{} 1\\ -1 &amp;{}&amp;{} 0 &amp;{}&amp;{} 1\\ \end{bmatrix}$$\end{document}</tex-math><mml:math id="M12"><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>0</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mrow/><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq6.gif"/></alternatives></inline-formula> respectively. We have selected Prewitt operator for this experiment because it is easy to implement and it detects the edges quite efficiently<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>. Comparison among the three most popular edge filters: Canny, Sobel and Prewitt applied on a COVID-CT image is shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. Figure <xref rid="Fig6" ref-type="fig">6</xref> reveals that Sobel filter is the most noisy one, whereas Canny filter produces the least noisy image. Although image produced by Prewitt filter is more noisy than Canny, all edges have different pixel intensity in the case of Prewitt unlike Canny. So choosing pixel value as feature would be wiser for Prewitt filter.<fig id="Fig6"><label>Figure 6</label><caption><p>Comparison between original COVID-CT image and edge image after applying—Prewitt, Canny and Sobel filters respectively (raw image source: COVID-CT dataset<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>).</p></caption><graphic xlink:href="41598_2021_87523_Fig6_HTML" id="MO6"/></fig></p>
        <p id="Par51">After applying convolution on each 3 <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M14"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq7.gif"/></alternatives></inline-formula> 3 sub-matrix by both of the horizontal and vertical filters, gradient for each sub-matrix has been evaluated. Since all the images are in grayscale, we have considered that a pixel would be situated in an edge if the magnitude of the gradient crosses halfway i.e., the gradient value is greater than or equal to 128. We can get a more clear view of the edge-detection step from Fig. <xref rid="Fig7" ref-type="fig">7</xref>.<fig id="Fig7"><label>Figure 7</label><caption><p>Diagram representing flow of edge-detection method (raw image source: CT-scan—SARS-COV-2 Ct-Scan Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>).</p></caption><graphic xlink:href="41598_2021_87523_Fig7_HTML" id="MO7"/></fig></p>
      </sec>
      <sec id="Sec7">
        <title>Graph preparation</title>
        <p id="Par52">After the Prewitt filter<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> is applied on an image, each image is converted to graph. The graph preparation is done using a 3-step procedure which is discussed below: <list list-type="order"><list-item><p id="Par53">Each pixel having grayscale intensity value greater than or equal to 128 is qualified as a node or a graph vertex. This implies that nodes reside only on the prominent edges of the edge image. Feature of a node consists of the grayscale intensity of the corresponding pixel.</p></list-item><list-item><p id="Par54">Edge exists between the two nodes which represent neighboring pixels in the original image.</p></list-item><list-item><p id="Par55">For each image, one graph is formed. This means that all the nodes as well as the edges constructed from a single image belongs to the same graph. The node attributes, which are simply grayscale values, are normalized graph-wise. Finally, normalization is done by subtracting the mean of all attributes under a graph from the original value and then dividing it by the standard deviation.</p></list-item></list>Since nodes are formed only from edges present in an image instead of the whole image, so less memory is consumed to prepare such data. Since COVID-19 and any kind of Pneumonia scans contain cloudy region for coughs, detected edges would be different as well as the nature of the graph. This difference might be useful later for classification. Overall five kind of datasets are formed to represent the graph data of all the scans, which are- <list list-type="order"><list-item><p id="Par56">Node-attribute-dataset: Here the attribute value (in this case the normalized grayscale value) of each node is stored.</p></list-item><list-item><p id="Par57">Graph-indicator-dataset: Here the graph-id for each node is stored.</p></list-item><list-item><p id="Par58">Node-label-dataset: Here the class-label for each node is stored. Since this is a graph level classification, each node under same graph would have same label which is actually the class-label for the corresponding graph.</p></list-item><list-item><p id="Par59">Graph-label-dataset: Here the class-label for each graph is stored.</p></list-item><list-item><p id="Par60">Adjacency-dataset: Here the adjacency sparse matrix for all the graphs is stored.</p><p id="Par61">Figure <xref rid="Fig8" ref-type="fig">8</xref> summarizes the whole edge-preparation process.</p></list-item></list><fig id="Fig8"><label>Figure 8</label><caption><p>Diagram representing the flow of edge-preparation stage.</p></caption><graphic xlink:href="41598_2021_87523_Fig8_HTML" id="MO8"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec8">
      <title>Proposed model</title>
      <p id="Par62">We have introduced our novel approach named as <bold>GraphCovidNet</bold>, where we have implemented GIN for classification and prediction tasks. So, before we move deeper into the architecture we will briefly discuss about the graphs, GNN and GIN.</p>
      <sec id="Sec9">
        <title>Graph neural network</title>
        <p id="Par63">A graph <italic>g</italic> can be described by set of components, nodes (<italic>V</italic>) and edges (<italic>E</italic>) as <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g=(V,E)$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq8.gif"/></alternatives></inline-formula>, where <italic>V</italic> is the set of vertices and <italic>E</italic> is the set of edges. The GNN can be used to classify an unlabelled node in a graph, where some nodes in the graph are labeled using a supervised learning technique. Also, it can do graph classification tasks where each graph has its corresponding labels. Now here, we have formed one graph from each labelled image and have used supervised learning to classify these graphs.</p>
      </sec>
      <sec id="Sec10">
        <title>Embeddings and graph isomorphism network</title>
        <p id="Par64">In GNN, the nodes of a graph are embedded into a <italic>d</italic>-dimensional embedded space denoted as <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>h</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq9.gif"/></alternatives></inline-formula>. These nodes are encoded in such a way that the connected nodes or the nodes which have same neighbors are close to each other in embedded space and vice versa. Every node uses its own feature vector <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_v$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>f</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq10.gif"/></alternatives></inline-formula> and its neighborhood embedding vector <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{nev}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi mathvariant="italic">nev</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq11.gif"/></alternatives></inline-formula> to find out it own embedding vector <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>h</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq12.gif"/></alternatives></inline-formula>.</p>
        <p id="Par65">GNNs uses the graph structure and node features to learn a representation vector of a node, <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_v$$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mi>f</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq13.gif"/></alternatives></inline-formula>, where each node contains the feature vectors, <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_v$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mi>f</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq14.gif"/></alternatives></inline-formula>
<inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\forall$$\end{document}</tex-math><mml:math id="M30"><mml:mo>∀</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq15.gif"/></alternatives></inline-formula> v <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\in$$\end{document}</tex-math><mml:math id="M32"><mml:mo>∈</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq16.gif"/></alternatives></inline-formula> V and each edge contains the feature vectors, <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_e$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mi>f</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq17.gif"/></alternatives></inline-formula>, <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\forall$$\end{document}</tex-math><mml:math id="M36"><mml:mo>∀</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq18.gif"/></alternatives></inline-formula> e <inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\in$$\end{document}</tex-math><mml:math id="M38"><mml:mo>∈</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq19.gif"/></alternatives></inline-formula> E or the entire graph, <inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_g$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi>h</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq20.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_g$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>h</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq21.gif"/></alternatives></inline-formula> = <inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Readout({h_v, \forall v \in V})$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq22.gif"/></alternatives></inline-formula> , where <inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mi>h</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq23.gif"/></alternatives></inline-formula> is the final embeddings of the node V is set of all nodes in the graph g. Now every node defines a computation graph based on its neighborhood i.e., every node has its own neural network architecture<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. This is shown in Fig. <xref rid="Fig9" ref-type="fig">9</xref>.<fig id="Fig9"><label>Figure 9</label><caption><p>Diagram representing the computation graph of a node in an arbitrary graph.</p></caption><graphic xlink:href="41598_2021_87523_Fig9_HTML" id="MO9"/></fig></p>
        <p id="Par66">The model for each node can be of arbitrary length. GNN follows a neighborhood aggregation strategy, where we iteratively update the representation of a node by aggregating representations of its neighbors. Nodes have embeddings at each layer. First layer of node is the input feature of that node and after k iterations of aggregation, a node’s representation captures the structural information within its k-hop network neighborhood. Let <inline-formula id="IEq24"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_v$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>x</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq24.gif"/></alternatives></inline-formula> be the feature vector of the node and <inline-formula id="IEq25"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{v}^{0}$$\end{document}</tex-math><mml:math id="M50"><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq25.gif"/></alternatives></inline-formula> be the initial layer embedding. Now, <inline-formula id="IEq26"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{v}^{0}$$\end{document}</tex-math><mml:math id="M52"><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq26.gif"/></alternatives></inline-formula> = <inline-formula id="IEq27"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_v$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mi>x</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq27.gif"/></alternatives></inline-formula>, initial layer embeddings are equal to feature vectors. Formally, the k-th layer of a GNN is</p>
        <p id="Par67"><inline-formula id="IEq28"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a_v^{k} = Aggregate^{(k)}({h_u^{(k-1)} : u \in N(v)}) , Combine^{(k)}(h_v^{(k-1)},a_v^{k})$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq28.gif"/></alternatives></inline-formula> where, <inline-formula id="IEq29"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v^{(k)}$$\end{document}</tex-math><mml:math id="M58"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq29.gif"/></alternatives></inline-formula> is the feature vector of node v at the k-th layer and <inline-formula id="IEq30"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ha_v^{(k)}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi>h</mml:mi><mml:msubsup><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq30.gif"/></alternatives></inline-formula> is the aggregated message from its neighborhoods. <italic>N</italic>(<italic>v</italic>) is a set of nodes adjacent to <italic>v</italic>. The choice of <inline-formula id="IEq31"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Aggregate^{(k)}$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>A</mml:mi><mml:mi>g</mml:mi><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq31.gif"/></alternatives></inline-formula> (<inline-formula id="IEq32"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdot$$\end{document}</tex-math><mml:math id="M64"><mml:mo>·</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq32.gif"/></alternatives></inline-formula>) and <inline-formula id="IEq33"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Combine^{(k)}$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq33.gif"/></alternatives></inline-formula> (<inline-formula id="IEq34"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdot$$\end{document}</tex-math><mml:math id="M68"><mml:mo>·</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq34.gif"/></alternatives></inline-formula>) in GNNs are crucial.</p>
        <p id="Par68">Different architectures for <italic>Aggregate</italic> function have been proposed. In the pooling variant of GraphSAGE<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>, <italic>Aggregate</italic> has been calculated as <inline-formula id="IEq35"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a_v^{k} = Max({ReLU(b^{(k)} * h_u^{(k-1)}) : u \in N(v)})$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow/><mml:mo>∗</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq35.gif"/></alternatives></inline-formula> where <inline-formula id="IEq36"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b^{(k)}$$\end{document}</tex-math><mml:math id="M72"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq36.gif"/></alternatives></inline-formula> is a parameter metrics, and Max represents an element-wise max-pooling. The <italic>Combine</italic> step could be a concatenation of its neighborhood aggregation and its previous layer’s embedding<inline-formula id="IEq37"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cdot$$\end{document}</tex-math><mml:math id="M74"><mml:mo>·</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq37.gif"/></alternatives></inline-formula>
<inline-formula id="IEq38"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v^{(k)} = w^{(k)} * Concat(h_v^{(k)},a_v^{k})$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq38.gif"/></alternatives></inline-formula> as in GraphSAGE, here <inline-formula id="IEq39"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w^{(k)}$$\end{document}</tex-math><mml:math id="M78"><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq39.gif"/></alternatives></inline-formula> is a parameter metrics. In Graph Convolutional Networks (GCN)<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, the element-wise mean pooling is used instead, and the <italic>Aggregate</italic> and <italic>Combine</italic> steps are integrated as follows: <inline-formula id="IEq40"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v^{(k)} = ReLU(W^{(k)} * mean({h_u^{(k-1)}), \forall u \in N(v) })$$\end{document}</tex-math><mml:math id="M80"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq40.gif"/></alternatives></inline-formula> .</p>
        <p id="Par69">Mean and max-pooling aggregators are still well-defined multi-set (contains the feature vectors of adjacent nodes of a particular node) functions because they are permutation invariant. But, they are not injective. When performing neighborhood aggregation, the mean(GCN) or max(GraphSage) pooling always obtains the same node representation everywhere. Thus, in this case mean and max pooling aggregators fail to capture any structural information of the graph<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. GNNs and the Weisfeiler–Lehman (WL) graph isomorphism test<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>, a powerful test known to distinguish a broad class of graphs<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>, are very closely connected.</p>
        <p id="Par70">The WL test has aggregated the labels of nodes and their neighborhoods iteratively and then it hashed the aggregated labels into unique new labels. The algorithm decides that two graphs are non-isomorphic if at some iteration the labels of the nodes between the two graphs differ.Each iteration of WL test has been described as follows: FOR ALL vertices v <inline-formula id="IEq41"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\in$$\end{document}</tex-math><mml:math id="M82"><mml:mo>∈</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq41.gif"/></alternatives></inline-formula> g <list list-type="order"><list-item><p id="Par71">Compute a hash of <inline-formula id="IEq42"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(h_v,h_{v_1},\ldots ,h_{v_n})$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq42.gif"/></alternatives></inline-formula> where <inline-formula id="IEq43"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{v_i}$$\end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mi>h</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq43.gif"/></alternatives></inline-formula> are the attributes of the neighbors of vertex v.</p></list-item><list-item><p id="Par72">Use the computed hash as vertex attribute for v in the next iteration.</p></list-item></list>The algorithm will terminate when this iteration has converged in terms of unique assignments of hashes to vertices.</p>
        <p id="Par73">The WL test is so powerful due to its injective aggregation update that maps different node neighborhoods to different feature vectors. Our key insight is that a GNN can have as large distinguishable power as the WL test if the GNN’s aggregation scheme is highly expressive and can model injective functions. This task to map any two different graphs to different embedding have implied solving graph isomorphism problem. That is, we want isomorphic graphs to be mapped to the same representation and non-isomorphic ones to different representations. Now, the GIN that satisfies the conditions for WL test and generalizes it and hence achieves maximum discriminative power among GNNs. The k-th layer embedding of GIN is given by: <inline-formula id="IEq44"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v^{(k)} = MLP^{(k)}((1+\epsilon ^{(k)}) * h_v^{(k)} + \sum _{u \in N(v)} h_u^{(k-1)})$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow/><mml:mo>∗</mml:mo><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msubsup><mml:mi>h</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq44.gif"/></alternatives></inline-formula>, where MLP stands for Multi Layer Perception and <inline-formula id="IEq45"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon ^{(k)}$$\end{document}</tex-math><mml:math id="M90"><mml:msup><mml:mi>ϵ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq45.gif"/></alternatives></inline-formula> is a floating point value.</p>
        <p id="Par74">Now for node classification, the node representation <inline-formula id="IEq46"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_v^{(k)}$$\end{document}</tex-math><mml:math id="M92"><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq46.gif"/></alternatives></inline-formula> of the kth layer is used for prediction. For graph classification, the <italic>Readout</italic> function aggregates node features from the final iteration to obtain the entire graph’s embedding <inline-formula id="IEq47"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_g$$\end{document}</tex-math><mml:math id="M94"><mml:msub><mml:mi>h</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq47.gif"/></alternatives></inline-formula> that is given by the following equation :</p>
        <p id="Par75"><inline-formula id="IEq48"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_g = Readout({h_v^{(k}, \forall v \in V})$$\end{document}</tex-math><mml:math id="M96"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>∀</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq48.gif"/></alternatives></inline-formula>. After we have got the embedding of the final layer, a supervised learning for node or graph classification (in our case) needs to be performed.</p>
      </sec>
      <sec id="Sec11">
        <title>Architecture of our proposed <bold>GraphCovidNet</bold> model</title>
        <p id="Par76">Our architecture consists of a block of GINConv layer which uses MLP<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> in its subsequent layers for the neighborhood aggregation. In MLP, we have used a block of sequential layers which consist of a linear layer, then a Rectangular Linear Unit (ReLU) layer, followed by another linear layer. It is shown in Fig. <xref rid="Fig10" ref-type="fig">10</xref>.</p>
        <p id="Par77">GINConv layer basically takes two different inputs: <list list-type="order"><list-item><p id="Par78"><italic>x</italic> which is the feature matrix of each node with dimension v*d, where <italic>V</italic> is the total number of nodes in the graph and <italic>d</italic> is embedded dimension.</p></list-item><list-item><p id="Par79">The edge index <italic>E</italic> has a dimension of 2*L consisting of all edges present in the entire graph in the form of pair (<italic>v</italic>1, <italic>v</italic>2), where <italic>v</italic>1 and <italic>v</italic>2 are two nodes connected by an edge and L is the total number of edges in the entire graph.</p></list-item></list>The output of the GINConv layer is passed through ReLU activation function to introduce non-linearity and then we apply a dropout of 0.5 and it is followed by a normalization (<italic>norm</italic>) layer, which applies layer normalization over a mini-batch of inputs. This output (<italic>out</italic>1) is passed on to another block of the same GINConv-ReLU-dropout-norm layers whose output is <italic>out</italic>2. Now, this <italic>out</italic>2 is passed onto a block which consists of GINConv-ReLU-dropout layers and then it is followed by a global mean pooling layer. After that, a linear layer followed by a dropout layer with dropout rate is equal to 0.5, and then a linear layer with dimension is equal to that of the number of classes of the problem under consideration. Finally, we have used a Log Softmax as the activation function that is used to produce the final probability vector, <italic>z</italic>. The whole architecture is shown in Fig. <xref rid="Fig11" ref-type="fig">11</xref><disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} logsoftmax(z_i) = \log \left( \frac{e^{z_i}}{\sum _{j=1}^{c} e^{z_j}}\right) \end{aligned}$$\end{document}</tex-math><mml:math id="M98" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mfrac></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2021_87523_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where, <inline-formula id="IEq49"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_i$$\end{document}</tex-math><mml:math id="M100"><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq49.gif"/></alternatives></inline-formula> is the probability of the <italic>ith</italic> element in the last linear layer vector and <inline-formula id="IEq50"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum _{j=1}^{c} e^{z_j}$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq50.gif"/></alternatives></inline-formula> is the sum of all probability values of all the elements including in the vector for the number of classes. We have used negative log likelihood (<italic>nll</italic>) function as the objective function for classification which needs to be minimized and can be represented as follows: <italic>nll</italic>(<italic>z</italic>) <inline-formula id="IEq51"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$=$$\end{document}</tex-math><mml:math id="M104"><mml:mo>=</mml:mo></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq51.gif"/></alternatives></inline-formula> − <inline-formula id="IEq52"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum _{i=1}^{c} (y_i * logsoftmax(z_i))$$\end{document}</tex-math><mml:math id="M106"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq52.gif"/></alternatives></inline-formula> where, <inline-formula id="IEq53"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M108"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2021_87523_Article_IEq53.gif"/></alternatives></inline-formula> is the ground truth label of the ith graph.<fig id="Fig10"><label>Figure 10</label><caption><p>Diagram representing the MLP architecture of GINConv Layer.</p></caption><graphic xlink:href="41598_2021_87523_Fig10_HTML" id="MO11"/></fig><fig id="Fig11"><label>Figure 11</label><caption><p>Overall architecture of our proposed <bold>GraphCovidNet</bold> model.</p></caption><graphic xlink:href="41598_2021_87523_Fig11_HTML" id="MO12"/></fig></p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Conclusion</title>
    <p id="Par80">For the past one year, COVID-19 has affected our social and economical lives greatly. In this situation, researchers are focusing on CT scan and CXR images for screening COVID-19 cases of the affected persons. In this paper, we have proposed a novel model, named as <bold>GraphCovidNet</bold>, which basically deals with classification of COVID-19 or any kind of Pneumonia patients from healthy people. Prewitt filter<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> has been used in the pre-processing stage which produces the edges of an image. Thus our proposed approach utilizes the memory more optimally than the typical CNN based models. Proposed model performs impressively well over different dataset considered in the present work. For some cases, its prediction accuracy even reaches to 100% and it can easily overcome the problems like overfitting and class imbalance. The proposed model has also outperformed many past models in terms of accuracy, precision, recall and f1-score. In future, we can apply the proposed <bold>GraphCovidNet</bold> in other COVID-19 or other medical datasets having CT-scans or CXRs. To be precise, GNN based models are applicable in any kind of image classification problems. We have conducted the present experiments using only 10 epochs to build the training model. So in future, we shall try to improve our model’s speed so that it can be trained in very less time even for larger number of samples.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors contributed equally: Pritam Saha and Debadyuti Mukherjee.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We would like to thank the <italic>CMATER</italic> research laboratory of the Computer Science and Engineering Department, Jadavpur University, India for providing us the infrastructural support.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>P.K.S. and R.S. conceived the experiment(s); P.S. and D. M. conducted the experiment(s); A.A. and M.F. provided the funding acquisition, supervision and validation of the data; P.S., D.M., P.K.S. and R.S. analysed the results. All authors reviewed the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>No datasets are generated during the current study. The datasets analyzed during this work are made publicly available in this published article.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The codes used for this research work are made publicly available in the GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/debadyuti23/GraphCovidNet">https://github.com/debadyuti23/GraphCovidNet</ext-link>.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par85">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Goel, T., Murugan, R., Mirjalili, S. &amp; Chakrabartty, D. K. Optconet: An optimized convolutional neural network for an automatic diagnosis of covid-19. <italic>Appl. Intell.</italic>, 1–16 (2020).</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="https://covid19.who.int">https://covid19.who.int</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Nour, M., Cömert, Z. &amp; Polat, K. A novel medical diagnosis model for covid-19 infection detection based on deep features and bayesian optimization. <italic>Appl. Soft Comput.</italic><bold>106580</bold>. 10.1016/j.asoc.2020.106580 (2020).</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="https://www.who.int/">https://www.who.int/</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Chest CT findings in patients with coronavirus disease 2019 and its relationship with clinical features</article-title>
        <source>Investig. Radiol.</source>
        <year>2020</year>
        <volume>55</volume>
        <fpage>257</fpage>
        <pub-id pub-id-type="doi">10.1097/RLI.0000000000000670</pub-id>
        <pub-id pub-id-type="pmid">32091414</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Liang, S., Zhanghao, M. &amp; Fuhua, Y. <italic>Adaptive Feature Selection Guided Deep Forest for COVID-19 Classification With Chest CT</italic> (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hope</surname>
            <given-names>MD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A role for CT in COVID-19? What data really tell us so far</article-title>
        <source>Lancet (London, England)</source>
        <year>2020</year>
        <volume>395</volume>
        <fpage>1189</fpage>
        <lpage>1190</lpage>
        <pub-id pub-id-type="doi">10.1016/S0140-6736(20)30728-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Soares, E., Angelov, P., Biaso, S., Higa Froes, M. &amp; Kanda Abe, D. SARS-CoV-2 CT-scan dataset: A large dataset of real patients CT scans for SARS-CoV-2 identification. <italic>medRxiv</italic>. 10.1101/2020.04.24.20078584 (2020). <ext-link ext-link-type="uri" xlink:href="https://www.medrxiv.org/content/early/2020/05/14/2020.04.24.20078584.full.pdf">https://www.medrxiv.org/content/early/2020/05/14/2020.04.24.20078584.full.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Jamdade, V. CMSC-678-ML-Project. <ext-link ext-link-type="uri" xlink:href="https://github.com/vj2050/Transfer-Learning-COVID-19">https://github.com/vj2050/Transfer-Learning-COVID-19</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Perumal</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Narayanan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Rajasekar</surname>
            <given-names>SJS</given-names>
          </name>
        </person-group>
        <article-title>Detection of COVID-19 using CXR and CT images using transfer learning and Haralick features</article-title>
        <source>Appl. Intell.</source>
        <year>2021</year>
        <volume>51</volume>
        <fpage>341</fpage>
        <lpage>358</lpage>
        <pub-id pub-id-type="doi">10.1007/s10489-020-01831-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Yang, X. <italic>et al.</italic> Covid-CT-dataset: A CT scan dataset about COVID-19. <italic>ArXiv e-prints</italic> arXiv-2003 (2020).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">He, K., Fan, H., Wu, Y., Xie, S. &amp; Girshick, R. Momentum contrast for unsupervised visual representation learning. <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic>, 9729–9738 (2020).</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Jégou, S., Drozdzal, M., Vazquez, D., Romero, A. &amp; Bengio, Y. The one hundred layers tiramisu: Fully convolutional DenseNets for semantic segmentation. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</italic>, 11–19 (2017).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep residual learning for image recognition. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 770–778 (2016).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Silva</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>COVID-19 detection in CT images with deep learning: A voting-based scheme and cross-datasets analysis</article-title>
        <source>Inf. Med. Unlocked</source>
        <year>2020</year>
        <volume>20</volume>
        <fpage>100427</fpage>
        <pub-id pub-id-type="doi">10.1016/j.imu.2020.100427</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Tan, M. &amp; Le, Q. V. Efficientnet: Improving accuracy and efficiency through AutoML and model scaling. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1905.11946">arXiv:1905.11946</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sharma</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Drawing insights from COVID-19-infected patients using CT scan images and machine learning techniques: A study on 200 patients</article-title>
        <source>Environ. Sci. Pollut. Res.</source>
        <year>2020</year>
        <volume>27</volume>
        <fpage>37155</fpage>
        <lpage>37163</lpage>
        <pub-id pub-id-type="doi">10.1007/s11356-020-10133-3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Radiology IS of M and I. Italian society of medical and interventional radiology. <ext-link ext-link-type="uri" xlink:href="https://www.sirm.org/category/senza-categoria/covid-19/">https://www.sirm.org/category/senza-categoria/covid-19/</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="https://mosmed.ai/en/">https://mosmed.ai/en/</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="http://www.salhospital.com/">http://www.salhospital.com/</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Elaziz</surname>
            <given-names>MA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>New machine learning method for image-based diagnosis of COVID-19</article-title>
        <source>PLoS ONE</source>
        <year>2020</year>
        <volume>15</volume>
        <fpage>1</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0235187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Guo, G., Wang, H., Bell, D., Bi, Y. &amp; Greer, K. KNN model-based approach in classification. In <italic>OTM Confederated International Conferences ”On the Move to Meaningful Internet Systems”</italic>, 986–996 (Springer, 2003).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Cohen, J. P. <italic>et al.</italic> COVID-19 image data collection: Prospective predictions are the future (2020). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2006.11988">arXiv:2006.11988</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Mooney, P. Chest X-ray images (pneumonia) [Online]. <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia">https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia</ext-link>, tanggal akses (2018).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chowdhury</surname>
            <given-names>MEH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Can AI help in screening viral and COVID-19 pneumonia?</article-title>
        <source>IEEE Access</source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>132665</fpage>
        <lpage>132676</lpage>
        <pub-id pub-id-type="doi">10.1109/access.2020.3010287</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Turkoglu, M. Covidetectionet: COVID-19 diagnosis system based on X-ray images using features selected from pre-learned deep features ensemble. <italic>Appl. Intell.</italic>, 1–14 (2020).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">O’Shea, K. &amp; Nash, R. An introduction to convolutional neural networks (2015). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1511.08458">arXiv:1511.08458</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support-vector networks</article-title>
        <source>Mach. Learn.</source>
        <year>1995</year>
        <volume>20</volume>
        <fpage>273</fpage>
        <lpage>297</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Rahman, T. COVID-19 Radiography Database. <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/tawsifurrahman/covid19-radiography-database">https://www.kaggle.com/tawsifurrahman/covid19-radiography-database</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Oh, Y., Park, S. &amp; Ye, J. C. Deep learning COVID-19 features on CXR using limited training data sets. <italic>IEEE Trans. Med. Imaging</italic> (2020).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shiraishi</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Development of a digital image database for chest radiographs with and without a lung nodule: Receiver operating characteristic analysis of radiologists’ detection of pulmonary nodules</article-title>
        <source>Am. J. Roentgenol.</source>
        <year>2000</year>
        <volume>174</volume>
        <fpage>71</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.2214/ajr.174.1.1740071</pub-id>
        <pub-id pub-id-type="pmid">10628457</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Ginneken</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Stegmann</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Loog</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Segmentation of anatomical structures in chest radiographs using supervised methods: A comparative study on a public database</article-title>
        <source>Med. Image Anal.</source>
        <year>2006</year>
        <volume>10</volume>
        <fpage>19</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2005.02.002</pub-id>
        <pub-id pub-id-type="pmid">15919232</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jaeger</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Two public chest X-ray datasets for computer-aided screening of pulmonary diseases</article-title>
        <source>Quant. Imaging Med. Surg.</source>
        <year>2014</year>
        <volume>4</volume>
        <fpage>475</fpage>
        <?supplied-pmid 25525580?>
        <pub-id pub-id-type="pmid">25525580</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Praveen. CoronaHack—Chest X-ray-dataset. <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/praveengovi/coronahack-chest-xraydataset">https://www.kaggle.com/praveengovi/coronahack-chest-xraydataset</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kermany</surname>
            <given-names>DS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identifying medical diagnoses and treatable diseases by image-based deep learning</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>172</volume>
        <fpage>1122</fpage>
        <lpage>1131</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.02.010</pub-id>
        <pub-id pub-id-type="pmid">29474911</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Rokach, L. &amp; Maimon, O. Decision trees. In <italic>Data Mining and Knowledge Discovery Handbook</italic>, 165–192 (Springer, 2005).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chandra</surname>
            <given-names>TB</given-names>
          </name>
          <name>
            <surname>Verma</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>BK</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Netam</surname>
            <given-names>SS</given-names>
          </name>
        </person-group>
        <article-title>Coronavirus disease (COVID-19) detection in chest X-ray images using majority voting based classifier ensemble</article-title>
        <source>Expert Syst. Appl.</source>
        <year>2020</year>
        <volume>165</volume>
        <fpage>113909</fpage>
        <pub-id pub-id-type="doi">10.1016/j.eswa.2020.113909</pub-id>
        <pub-id pub-id-type="pmid">32868966</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Mishra, M. &amp; Srivastava, M. A view of artificial neural network. In <italic>2014 International Conference on Advances in Engineering Technology Research (ICAETR-2014)</italic>, 1–3. 10.1109/ICAETR.2014.7012785 (2014).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Webb</surname>
            <given-names>GI</given-names>
          </name>
        </person-group>
        <article-title>Naïve bayes</article-title>
        <source>Encycl. Mach. Learn.</source>
        <year>2010</year>
        <volume>15</volume>
        <fpage>713</fpage>
        <lpage>714</lpage>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Candemir</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2013</year>
        <volume>33</volume>
        <fpage>577</fpage>
        <lpage>590</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2013.2290491</pub-id>
        <pub-id pub-id-type="pmid">24239990</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Wang, X. <italic>et al.</italic> Chestx-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 2097–2106 (2017).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srinivasan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Shobha</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Statistical texture analysis</article-title>
        <source>Proc. World Acad. Sci. Eng. Technol.</source>
        <year>2008</year>
        <volume>36</volume>
        <fpage>1264</fpage>
        <lpage>1269</lpage>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gómez</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>WCA</given-names>
          </name>
          <name>
            <surname>Infantosi</surname>
            <given-names>AFC</given-names>
          </name>
        </person-group>
        <article-title>Analysis of co-occurrence texture statistics as a function of gray-level quantization for classifying breast ultrasound</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2012</year>
        <volume>31</volume>
        <fpage>1889</fpage>
        <lpage>1899</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2012.2206398</pub-id>
        <pub-id pub-id-type="pmid">22759441</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Dalal, N. &amp; Triggs, B. Histograms of oriented gradients for human detection. In <italic>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)</italic>, Vol. 1, 886–893. 10.1109/CVPR.2005.177 (2005).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Hemdan, E. E.-D., Shouman, M. A. &amp; Karar, M. E. Covidx-net: A framework of deep learning classifiers to diagnose COVID-19 in X-ray images. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2003.11055">arXiv:2003.11055</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556">arXiv:1409.1556</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Huang, G., Liu, Z., Van Der Maaten, L. &amp; Weinberger, K. Q. Densely connected convolutional networks. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 4700–4708 (2017).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. &amp; Wojna, Z. Rethinking the inception architecture for computer vision. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 2818–2826 (2016).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Too</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Yujian</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Njuki</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yingchun</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>A comparative study of fine-tuning deep learning models for plant disease identification</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2019</year>
        <volume>161</volume>
        <fpage>272</fpage>
        <lpage>279</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2018.03.032</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Szegedy, C., Ioffe, S., Vanhoucke, V. &amp; Alemi, A. Inception-v4, inception-resnet and the impact of residual connections on learning. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1602.07261">arXiv:1602.07261</ext-link> (2016).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Chollet, F. Xception: Deep learning with depthwise separable convolutions. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic>, 1251–1258 (2017).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Rosebrock, A. <ext-link ext-link-type="uri" xlink:href="https://www.pyimagesearch.com/category/medical/">https://www.pyimagesearch.com/category/medical/</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Makris, A., Kontopoulos, I. &amp; Tserpes, K. Covid-19 detection from chest X-ray images using deep learning and convolutional neural networks. In <italic>11th Hellenic Conference on Artificial Intelligence</italic>, 60–66 (2020).</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Zhong, Y. Using deep convolutional neural networks to diagnose COVID-19 from chest X-ray images. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2007.09695">arXiv:2007.09695</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Chung, A. Figure 1 COVID-19 chest X-ray data initiative. <ext-link ext-link-type="uri" xlink:href="https://github.com/agchung/Figure1-COVID-chestxray-dataset">https://github.com/agchung/Figure1-COVID-chestxray-dataset</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Chung, A. Actualmed COVID-19 chest X-ray data initiative. <ext-link ext-link-type="uri" xlink:href="https://github.com/agchung/Actualmed-COVID-chestxray-dataset">https://github.com/agchung/Actualmed-COVID-chestxray-dataset</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chattopadhyay</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dey</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Geem</surname>
            <given-names>ZW</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 detection by optimizing deep residual features with improved clustering-based golden ratio optimizer</article-title>
        <source>Diagnostics</source>
        <year>2021</year>
        <volume>11</volume>
        <fpage>315</fpage>
        <pub-id pub-id-type="doi">10.3390/diagnostics11020315</pub-id>
        <pub-id pub-id-type="pmid">33671992</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Sen, S., Saha, S., Chatterjee, S., Mirjalili, S. &amp; Sarkar, R. A bi-stage feature selection approach for COVID-19 prediction using chest CT images. <italic>Appl. Intell.</italic> (2021).</mixed-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Loey</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Manogaran</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Taha</surname>
            <given-names>MHN</given-names>
          </name>
          <name>
            <surname>Khalifa</surname>
            <given-names>NEM</given-names>
          </name>
        </person-group>
        <article-title>A hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic</article-title>
        <source>Measurement</source>
        <year>2020</year>
        <volume>167</volume>
        <fpage>108288</fpage>
        <pub-id pub-id-type="doi">10.1016/j.measurement.2020.108288</pub-id>
        <pub-id pub-id-type="pmid">32834324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <mixed-citation publication-type="other">Wang, Z. <italic>et al.</italic> Masked face recognition dataset and application. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2003.09093">arXiv:2003.09093</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <mixed-citation publication-type="other">prajnasb, “observations,” observations. <ext-link ext-link-type="uri" xlink:href="https://github.com/prajnasb/observations">https://github.com/prajnasb/observations</ext-link> (2020).</mixed-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <mixed-citation publication-type="other">Learned-Miller, E., Huang, G. B., RoyChowdhury, A., Li, H. &amp; Hua, G. Labeled faces in the wild: A survey. In <italic>Advances in Face Detection and Facial Image Analysis</italic>, 189–248 (Springer, 2016).</mixed-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Joshi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Verma</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Saxena</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Paraye</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Issues in training a convolutional neural network model for image classification</article-title>
        <source>Springer</source>
        <year>2019</year>
        <volume>1046</volume>
        <fpage>282</fpage>
        <lpage>293</lpage>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scarselli</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Gori</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tsoi</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Hagenbuchner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Monfardini</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>The graph neural network model</article-title>
        <source>IEEE Trans. Neural Netw.</source>
        <year>2009</year>
        <volume>20</volume>
        <fpage>61</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id>
        <?supplied-pmid 19068426?>
        <pub-id pub-id-type="pmid">19068426</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <mixed-citation publication-type="other">Mondal, R., Mukherjee, D., Singh, P. K., Bhateja, V. &amp; Sarkar, R. A new framework for smartphone sensor based human activity recognition using graph neural network. <italic>IEEE Sens. J.</italic> (2020).</mixed-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Xu, K., Hu, W., Leskovec, J. &amp; Jegelka, S. How powerful are graph neural networks? arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1810.00826">arXiv:1810.00826</ext-link> (2018).</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Prewitt</surname>
            <given-names>JMS</given-names>
          </name>
        </person-group>
        <source>Picture Processing and Psychopictorics</source>
        <year>1970</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Academic Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <mixed-citation publication-type="other">Priyam, P., Dey, D. &amp; Shreya, D. P. Edge detection by using canny and Prewitt. <italic>Int. J. Sci. Eng. Res.</italic><bold>7</bold> (2016).</mixed-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <mixed-citation publication-type="other">Hamilton, W., Ying, Z. &amp; Leskovec, J. Inductive representation learning on large graphs. <italic>Adv. Neural Inf. Process. Syst.</italic>, 1024–1034 (2017).</mixed-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <mixed-citation publication-type="other">Berg, R. V. D., Kipf, T. N. &amp; Welling, M. Graph convolutional matrix completion. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.02263">arXiv:1706.02263</ext-link> (2017).</mixed-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weisfeiler</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lehman</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <article-title>A reduction of a graph to a canonical form and an algebra arising during this reduction</article-title>
        <source>Nauchno-Technicheskaya Informatsia</source>
        <year>1968</year>
        <volume>2</volume>
        <fpage>12</fpage>
        <lpage>16</lpage>
      </element-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <mixed-citation publication-type="other">Babai, L. &amp; Kucera, L. Canonical labelling of graphs in linear average time. In <italic>20th Annual Symposium on Foundations of Computer Science (SFCS 1979)</italic>, 39–46 (IEEE, 1979).</mixed-citation>
    </ref>
  </ref-list>
</back>
