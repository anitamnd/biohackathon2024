<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brief Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">bib</journal-id>
    <journal-title-group>
      <journal-title>Briefings in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1467-5463</issn>
    <issn pub-type="epub">1477-4054</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10025436</article-id>
    <article-id pub-id-type="pmid">36738256</article-id>
    <article-id pub-id-type="doi">10.1093/bib/bbad029</article-id>
    <article-id pub-id-type="publisher-id">bbad029</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Problem Solving Protocol</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>netANOVA: novel graph clustering technique with significance assessment via hierarchical ANOVA</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Duroux</surname>
          <given-names>Diane</given-names>
        </name>
        <!--diane.duroux@uliege.be-->
        <aff><institution>BIO3 - Systems Genetics, GIGA-R Medical Genomics, University of Liege</institution>, <addr-line>Liege</addr-line>, <country country="BE">Belgium</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Van Steen</surname>
          <given-names>Kristel</given-names>
        </name>
        <aff><institution>BIO3 - Systems Genetics, GIGA-R Medical Genomics, University of Liege</institution>, <addr-line>Liege</addr-line>, <country country="BE">Belgium</country></aff>
        <aff><institution>BIO3 - Systems Medicine, Department of Human Genetics</institution>, <addr-line>KU Leuven, Leuven</addr-line>, <country country="BE">Belgium</country></aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Corresponding author. Postal address: GIGA – B34, Floor +5 (Scriptorium), Quartier Hôpital, Avenue de l’Hôpital, 11, 4000 Liège, Belgium; E-mail: <email>diane.duroux@uliege.be</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-02-04">
      <day>04</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>04</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>24</volume>
    <issue>2</issue>
    <elocation-id>bbad029</elocation-id>
    <history>
      <date date-type="received">
        <day>3</day>
        <month>8</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>2</day>
        <month>1</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bbad029.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Many problems in life sciences can be brought back to a comparison of graphs. Even though a multitude of such techniques exist, often, these assume prior knowledge about the partitioning or the number of clusters and fail to provide statistical significance of observed between-network heterogeneity. Addressing these issues, we developed an unsupervised workflow to identify groups of graphs from reliable network-based statistics. In particular, we first compute the similarity between networks via appropriate distance measures between graphs and use them in an unsupervised hierarchical algorithm to identify classes of similar networks. Then, to determine the optimal number of clusters, we recursively test for distances between two groups of networks. The test itself finds its inspiration in distance-wise ANOVA algorithms. Finally, we assess significance via the permutation of between-object distance matrices. Notably, the approach, which we will call netANOVA, is flexible since users can choose multiple options to adapt to specific contexts and network types. We demonstrate the benefits and pitfalls of our approach via extensive simulations and an application to two real-life datasets. NetANOVA achieved high performance in many simulation scenarios while controlling type I error. On non-synthetic data, comparison against state-of-the-art methods showed that netANOVA is often among the top performers. There are many application fields, including precision medicine, for which identifying disease subtypes via individual-level biological networks improves prevention programs, diagnosis and disease monitoring.</p>
    </abstract>
    <kwd-group>
      <kwd>Network clustering</kwd>
      <kwd>System medicine</kwd>
      <kwd>Stratified medicine</kwd>
      <kwd>Graph comparison</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Marie Sklodowska-Curie</institution>
          </institution-wrap>
        </funding-source>
        <award-id>813533</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p>Subjects or objects can often be linked to systems, and studying the differences between their corresponding system representations is of particular interest to precision medicine. Examples of systems in biology include the nervous system, the circulatory system and the respiratory system. Graphs lend themselves perfectly to visualize systems [<xref rid="ref1" ref-type="bibr">1</xref>]. A graph consists of nodes and edges as primary building blocks. Only the characteristics of these elements may differ since they can be labelled, attributed, weighted and directed (see Section <xref rid="sec2a" ref-type="sec">2.1</xref>). Sometimes, the term ‘graph’ may be reserved to describe an abstract data structure, whereas the term ‘network’ would refer to a concretization of a graph. Here, the terms graph and network are interchangeably used. Graph-based machine learning [<xref rid="ref2" ref-type="bibr">2</xref>] has already been used to disentangle complex diseases and improve personalized care. Lung cancer was predicted from a protein–protein interaction (PPI) network integrated with gene expression data using a combination of spectral clustering and deep learning methods [<xref rid="ref3" ref-type="bibr">3</xref>]. Breast cancer subtype classification was performed from PPI networks enriched with gene expression data via the integration of deep learning methods and a relational network [<xref rid="ref4" ref-type="bibr">4</xref>].</p>
    <p>Most of the graph analyses for complex diseases aggregate information across a whole cohort, failing to detect individual characteristics [<xref rid="ref5" ref-type="bibr">5</xref>]. Exploiting individual-specific interactions rather than population-level systems will capture the heterogeneity between individuals and enhance the identification of new biomarkers for precision medicine. This observation paves the way for developing individual networks, where nodes and/or edges are individual-specific. For each individual, nodes are variables (e.g. genes), and edges show the link between these variables for that individual.</p>
    <p>In this work, we want to understand what makes (individual) networks different. We aim at comparing entire networks to create groups of graphs that are homogeneous. In other words, we start from a set of graphs, and we are interested in finding sub-groups of graphs to learn about their different characteristics and examine the driving factors for similarity or dissimilarity. Unsupervised learning is required as in the medical context, grouping labels are not necessarily known and the goal is to discover discriminating properties among the data. The number of classes may not be known either, so the algorithm has to derive it. Finally, the method needs to include notions of statistics to assess that the groups are significantly different.</p>
    <p>The advancement of machine learning has led to the emergence of various network analytics tools and techniques [<xref rid="ref2" ref-type="bibr">2</xref>, <xref rid="ref6" ref-type="bibr">6</xref>]. In this work, we focus on the scenario where we have a list of graphs as input and aim to create groups of similar graphs. One option is to represent the edge weights as a vector and use these vectors as input to downstream analyses. This approach is easy to implement but ignores the topology of networks and is restricted to situations where networks have the same set of nodes. Another possibility is to derive graph summary statistics (e.g. average degree and path length). This method has proven successful ([<xref rid="ref7" ref-type="bibr">7</xref>, <xref rid="ref8" ref-type="bibr">8</xref>]) but tends to ignore local structures. To take into account local dissimilarities, an alternative is to apply network-specific distances [<xref rid="ref9" ref-type="bibr">9</xref>] or graph kernels [<xref rid="ref10" ref-type="bibr">10</xref>] to estimate the similarity/dissimilarity between networks and use the network similarities in kernel-based ML methods to identify groups of homogeneous graphs. However, the number of groups may not be known a priori, so there is a need to incorporate an algorithm that derives it. Also, this method often suffers from a high computational burden. Deep learning methods can help solve this scalability issue while bringing strong performance. Initially, these methods were constructed to work on vectors. Graph neural networks (GNNs) [<xref rid="ref11" ref-type="bibr">11</xref>] extend them to graphs. GNNs include graph embedding and graph convolutional networks (GCNs).</p>
    <p>Graph embedding aims at computing a fixed-size vector representation of a graph to decrease dimensionality. Structural properties in the embedding should correspond to the properties of the networks. For instance, InfoGraph [<xref rid="ref12" ref-type="bibr">12</xref>] maximizes the mutual information between the graph-level representation and the representations of substructures of different scales (e.g. nodes, edges, triangles). In the GraPHmax approach [<xref rid="ref13" ref-type="bibr">13</xref>], the concept of periphery representation of a graph into a single framework is introduced and combined with hierarchical GNNs and mutual information maximization. The graph2vec algorithm [<xref rid="ref14" ref-type="bibr">14</xref>] extends document embedding neural networks by considering an entire graph as a document and the rooted subgraphs (i.e. non-linear substructures) around every node in the graph as words, to create embeddings of entire graphs. With all these approaches, the derived representations in the embedding space can be used for classification (e.g. elastic net, SVM-L1, signal subgraph, dlda, lasso) or clustering (e.g. hierarchical clustering, <inline-formula><tex-math notation="LaTeX" id="ImEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-means, spectral clustering). Notably, classification has received a lot of attention. However, in many fields, group labels are not known, and unsupervised learning is required. Also, deriving the optimal number of clusters is often decoupled from the mainstream analysis [<xref rid="ref13" ref-type="bibr">13</xref>].</p>
    <p>GCNs adapt convolutional neural network methodologies for graph-structured data. To provide a network representation similar to the image convolution, GCN algorithms use a spectral [<xref rid="ref15" ref-type="bibr">15</xref>], or spatial-based [<xref rid="ref16" ref-type="bibr">16</xref>] convolution over the graph. GNNs’ drawbacks include their lack of interpretability which is an important issue for instance in biology where the goal is to understand the processes involved in the system studied [<xref rid="ref17" ref-type="bibr">17</xref>], and in precision medicine, where physicians will need to understand the prediction to trust it [<xref rid="ref18" ref-type="bibr">18</xref>]. However, some progress has been made recently [<xref rid="ref19" ref-type="bibr">19</xref>]. Furthermore, GNNs require a large amount of data to provide accurate predictions. It can be an issue in personalized medicine where it is complex to collect large samples for feasibility and privacy reasons [<xref rid="ref20" ref-type="bibr">20</xref>]. Deep learning methods for graph clustering have been shown to achieve high performance [<xref rid="ref21" ref-type="bibr">21–23</xref>]. As clustering methods are typically driven by particular characteristics of the data, no holy grail generic method is likely to prevail. Wu <italic toggle="yes">et al</italic>. [<xref rid="ref24" ref-type="bibr">24</xref>] showed that improved performance can be obtained with more traditional graph clustering approaches over deep learning ones in specific scenarios (in their work, the WL-CT kernel).</p>
    <p>In response to the illustrated shortcomings, with our novel netANOVA analysis workflow we aim to exploit information about structural and dynamical properties of networks to identify significantly different groups of similar networks. We do so by developing a novel group comparison testing workflow that sequentially evolves down a hierarchical tree. The netANOVA test statistic relies on additive partitioning rather than centroids; the latter is typical in traditional analysis of variance (ANOVA) hypothesis testing [<xref rid="ref25" ref-type="bibr">25</xref>]. Statistical significance is assessed empirically to avoid reliance on distributional assumptions. Furthermore, our flexible analysis workflow accommodates small datasets (smaller than <inline-formula><tex-math notation="LaTeX" id="ImEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$20$\end{document}</tex-math></inline-formula>) as well as larger ones (up to a few thousand), and can be used in multiple contexts via customizable hyperparameter settings, handling weighted, sparse or multi-layered networks.</p>
    <p>In summary, our analysis workflow can be used to identify and formally test for differences between objects that can be represented as graphs. Hence, application areas include, but are not restricted to, precision medicine and the challenging task of identifying endotypes for biomarker development.</p>
  </sec>
  <sec id="sec2">
    <title>Materials and methods</title>
    <sec id="sec2a">
      <title>Network and graphs</title>
      <p>A network is a data structure consisting of nodes and edges modelling the relations between two nodes. A network <inline-formula><tex-math notation="LaTeX" id="ImEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$G$\end{document}</tex-math></inline-formula> can be defined as <inline-formula><tex-math notation="LaTeX" id="ImEquation4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$G=(V, E)$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math notation="LaTeX" id="ImEquation5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$V$\end{document}</tex-math></inline-formula> is the set of nodes, and <inline-formula><tex-math notation="LaTeX" id="ImEquation6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$E$\end{document}</tex-math></inline-formula> are the edges between them. In biology, nodes can be genes, messenger RNAs, proteins or metabolites, and edges can represent molecular regulation, genetic interactions, co-localization or co-occurrence.</p>
      <p>For binary networks, a graph is completely described by its adjacency matrix <inline-formula><tex-math notation="LaTeX" id="ImEquation7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$A\in{0,1}_{n \times n}$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math notation="LaTeX" id="ImEquation8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$A(i,j) = 1$\end{document}</tex-math></inline-formula> if and only if the link <inline-formula><tex-math notation="LaTeX" id="ImEquation9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$(i,j)\in E$\end{document}</tex-math></inline-formula>. If matrix <inline-formula><tex-math notation="LaTeX" id="ImEquation10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$A$\end{document}</tex-math></inline-formula> is symmetric, then the graph is undirected, otherwise directed. For weighted networks, <inline-formula><tex-math notation="LaTeX" id="ImEquation11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$A(i,j)=w_{ij}$\end{document}</tex-math></inline-formula>, with <inline-formula><tex-math notation="LaTeX" id="ImEquation12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$i,j \in N$\end{document}</tex-math></inline-formula>. Attributed networks have labels and/or attributes on the nodes and/or edges. Attributes (resp. labels) are commonly expected to be real values (resp. alphabetic values).</p>
    </sec>
    <sec id="sec2b">
      <title>Distances and similarities between networks</title>
      <p>Distance and similarity are related concepts: when distance increases, similarity decreases. A ‘distance metric’ is a function that satisfies the non-negativity, identity, symmetry and triangle inequality properties [<xref rid="ref26" ref-type="bibr">26</xref>]. Often, some properties are not necessary, and a ‘distance measure’ may be used. The latter also captures how different two objects are but is a function that does not satisfy at least one of the four properties. A similarity function satisfies the non-negativity, boundedness, identity and symmetry properties. A distance can be calculated based on similarity and vice versa. NetANOVA is based on a distance matrix (<xref rid="sec2e" ref-type="sec">Algorithm 2</xref>). Hence, when the link between networks is directly computed using a distance measure (e.g. the edge difference distance or the hamming distance), no additional transformation is needed. However, when similarities are used to study the link between networks (e.g. with the shortest path kernel or the random walk kernel), we need to convert them into distances. Specifically, when a similarity is computed via a kernel, then the distance between two networks <inline-formula><tex-math notation="LaTeX" id="ImEquation13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$G_1$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$G_2$\end{document}</tex-math></inline-formula> can be calculated as the difference between the self-similarities <inline-formula><tex-math notation="LaTeX" id="ImEquation15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K(G_1, G_1) + K(G_2, G_2)$\end{document}</tex-math></inline-formula> and the cross-similarity <inline-formula><tex-math notation="LaTeX" id="ImEquation16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K(G_1, G_2)$\end{document}</tex-math></inline-formula> [<xref rid="ref27" ref-type="bibr">27</xref>]: <inline-formula><tex-math notation="LaTeX" id="ImEquation17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d(G_1, G_2) = K(G_1, G_1) + K(G_2, G_2) - 2K(G_1, G_2)$\end{document}</tex-math></inline-formula>. The multiplicative factor 2 is needed to ensure that d(G, G) = 0.</p>
      <p>The choice of distance and similarity measures is a critical step in clustering efforts. An extensive range of graph comparison measures exists. Requiring time-computational efficiency when clustering a large number of graphs dramatically reduces the options. Moreover, most of the remaining distances handle undirected [<xref rid="ref28" ref-type="bibr">28–30</xref>] and unweighted [<xref rid="ref31" ref-type="bibr">31–33</xref>] networks only. Hence, defining a distance between graphs is a cumbersome task, which requires seeking a context-dependent balance between computational efficiency, performance and interpretability. Following Tantardini <italic toggle="yes">et al</italic>. [<xref rid="ref9" ref-type="bibr">9</xref>], we group network-based distances into two main classes: Known Node-Correspondence (KNC) and Unknown Node-Correspondence (UNC) methods.</p>
      <p>In the KNC scenario, the networks have the same set of nodes or at least a common subset, and the pairwise correspondence between the networks nodes is known. In other words, a distance requires node correspondence when some meaningful mapping between the node sets of the graphs exists. Typically, there is Known Node-Correspondence when networks come from the same application field. KNC distances gather all the methods, such as Euclidean, Jaccard or DeltaCon distances, which require a priori to know the correspondence between the nodes of the compared networks. These methods allow comparing networks where nodes are labelled and hence not exchangeable.</p>
      <p>UNC approaches do not require knowledge of the correspondence between nodes. UNC methods, such as spectral distances, graphlet-based measures and Portrait Divergence, are suited for global structural comparison. They indicate how much the structures of graphs differ. We will pay special attention to graph kernel measures [<xref rid="ref10" ref-type="bibr">10</xref>]. A kernel is a measure of similarity between objects and must satisfy two mathematical requirements: it must be symmetric and positive semi-definite. Notably, there are much more UNC approaches than KNC ones.</p>
      <p>Our netANOVA workflow accommodates multiple measures: the edge difference distance [<xref rid="ref34" ref-type="bibr">34</xref>], a customized KNC version of <inline-formula><tex-math notation="LaTeX" id="ImEquation18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-step random walk kernel (see <xref rid="sup2" ref-type="supplementary-material">Supplementary</xref>) [<xref rid="ref35" ref-type="bibr">35</xref>], DeltaCon [<xref rid="ref36" ref-type="bibr">36</xref>], GTOM [<xref rid="ref37" ref-type="bibr">37</xref>] and the Gaussian kernel on the vectorized networks [<xref rid="ref38" ref-type="bibr">38</xref>] are proposed as KNC methods. The Hamming distance [<xref rid="ref39" ref-type="bibr">39</xref>], Shortest path kernel [<xref rid="ref40" ref-type="bibr">40</xref>], <inline-formula><tex-math notation="LaTeX" id="ImEquation19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-step random walk kernel and Graph Diffusion Distance [<xref rid="ref34" ref-type="bibr">34</xref>] are optional UNC methods. More details about these distance and similarity measures, and the reasoning behind these choices are given in <xref rid="sup2" ref-type="supplementary-material">Supplementary</xref>.</p>
    </sec>
    <sec id="sec2c">
      <title>Identification of homogeneous subgroups</title>
      <p>Distance-based clustering evolves around finding homogeneous subgroups of objects, where objects with minimal distances between them are assigned to the same cluster. The two most popular distance-based clustering approaches are hierarchical clustering and <inline-formula><tex-math notation="LaTeX" id="ImEquation20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-means clustering. The first clusters objects sequentially, via inter-cluster distances. The latter classifies objects into subgroups via inter-cluster variances that need to be minimized. Hierarchical clustering has the additional advantage that a tree (dendrogram) visualizes different granularities in the clustering process, which we will exploit in our workflow.</p>
      <p>NetANOVA is built around hierarchical distance-based clustering, with distance measures as in Section <xref rid="sec2b" ref-type="sec">2.2</xref>. We use the standard agglomerative clustering which first considers each object as a cluster and then merges clusters successively until one cluster contains all objects.</p>
    </sec>
    <sec id="sec2d">
      <title>Deriving the optimal number of clusters</title>
      <p>To determine the optimal number of clusters, we recursively test for distances between two groups of networks, progressing from the root node to the end nodes of the clustering dendrogram (<xref rid="f1" ref-type="fig">Figure 1<bold>(A)</bold></xref>). Many clustering methods require the user to pre-specify the number of clusters. However, this information is often not known. Incorrect estimation will prevent learning the real clustering structure. Here, the algorithm derives the number of classes. If the two groups created from a node of the dendrogram are statistically different, the algorithm to find the optimal number of clusters proceeds in the child nodes (<xref rid="sec2d" ref-type="sec">Algorithm 1</xref>). Details about the underlying formal hypothesis test are given next (Section <xref rid="sec2e" ref-type="sec">2.5</xref>). There are two stopping conditions: the two subgroups are too small or are not statistically significantly different. The first requires setting a threshold for the minimum allowable size of a subgroup. The result is a decision tree where the end leaves are the final clusters, and splitting is based on a formal group comparison test between network collections. Note that when one of the two groups tested (<inline-formula><tex-math notation="LaTeX" id="ImEquation21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b$\end{document}</tex-math></inline-formula>) has a size not surpassing the minimum size threshold (for example group <inline-formula><tex-math notation="LaTeX" id="ImEquation23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a$\end{document}</tex-math></inline-formula>), the statistical test is applied to the other group (group b giving rise to subgroups <inline-formula><tex-math notation="LaTeX" id="ImEquation24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_1$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_2$\end{document}</tex-math></inline-formula>). If subgroups <inline-formula><tex-math notation="LaTeX" id="ImEquation26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_1$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_2$\end{document}</tex-math></inline-formula> are statistically different, group <inline-formula><tex-math notation="LaTeX" id="ImEquation28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a$\end{document}</tex-math></inline-formula> is regarded to be outlying and hence an independent group.</p>
      <p>
        <inline-graphic xlink:href="bbad029fx1.jpg"/>
      </p>
      <fig position="float" id="f1">
        <label>Figure 1</label>
        <caption>
          <p><bold>(A)</bold> NetANOVA workflow. Starting from the list of networks, the pairwise distance between each pair of networks is computed. Then a hierarchical clustering is applied to the distance matrix to derive a dendrogram and identify preliminary groups. <xref rid="sec2e" ref-type="sec">Algorithm 2</xref> is applied to the two first groups (from the top to the bottom of the dendrogram). If the two first groups are statistically different, <xref rid="sec2e" ref-type="sec">Algorithm 2</xref> is applied to each of the two subgroups. Recursively (<xref rid="sec2d" ref-type="sec">Algorithm 1</xref>), a tree is built to derive the optimal number of clusters, with two stop conditions: the groups are too small, or the groups are not significantly different. <bold>(B)</bold> The sum of squared distances from individual points to their centroid is equal to the sum of squared interpoint distances divided by the number of points. <bold>(C)</bold> Simulation set-up. An original network with <inline-formula><tex-math notation="LaTeX" id="ImEquation29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$m$\end{document}</tex-math></inline-formula> nodes and density <inline-formula><tex-math notation="LaTeX" id="ImEquation30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d$\end{document}</tex-math></inline-formula> is generated. The baseline network has a random structure (simulated from the Erdos–Renyi model), a density of 0.05, 100 nodes and binary edges. Group networks are derived by perturbing (rewiring, adding or removing edges) the original graph. Individual networks are derived by perturbing the group networks.</p>
        </caption>
        <graphic xlink:href="bbad029f1" position="float"/>
      </fig>
      <p>With the aforementioned sequential procedure, false-positive control is a concern. We include two options to correct for multiple testing. First, we correct the <italic toggle="yes">P</italic>-values using the depth of the tree, i.e. no correction at the root node, <inline-formula><tex-math notation="LaTeX" id="ImEquation31">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$p_{adj}=p\times 2$\end{document}</tex-math></inline-formula> at level 2 of the dendrogram, <inline-formula><tex-math notation="LaTeX" id="ImEquation32">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$p_{adj}=p\times 3$\end{document}</tex-math></inline-formula> at level 3 of the dendrogram, and so forth. If at a node of the dendrogram, the difference between the two associated groups is not tested because one of the two groups is smaller than the minimum group size threshold, then the level (i.e. depth of the dendrogram) is not incremented. Also, we implement the correction developed by Meinshausen [<xref rid="ref41" ref-type="bibr">41</xref>] and created for variable selection. It controls the FWER at level <inline-formula><tex-math notation="LaTeX" id="ImEquation33">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\alpha \in (0, 1)$\end{document}</tex-math></inline-formula>, by performing the hypothesis test described in Section <xref rid="sec2e" ref-type="sec">2.5</xref> at each node <inline-formula><tex-math notation="LaTeX" id="ImEquation34">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j$\end{document}</tex-math></inline-formula>, with the significance threshold <inline-formula><tex-math notation="LaTeX" id="ImEquation35">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\alpha _{adj}=\alpha \times \frac{N_j-1}{N-1}$\end{document}</tex-math></inline-formula> with <inline-formula><tex-math notation="LaTeX" id="ImEquation36">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_j$\end{document}</tex-math></inline-formula> the number of networks clustered at node <inline-formula><tex-math notation="LaTeX" id="ImEquation37">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation38">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N$\end{document}</tex-math></inline-formula> the total number of networks. It gives increased power to the first nodes (near the root) of the tree. Also, we include the possibility of not correcting for multiple testing in the workflow. Strikingly, computing the total number of tests and applying a Bonferroni correction to each test to keep FWER under control would bypass the hierarchical structure of the analysis.</p>
      <p>In Section <xref rid="sec3a" ref-type="sec">3.1</xref>, we evaluate these multiple testing corrections for FWER control. We define FWER of the entire workflow as the probability of falsely rejecting the null hypothesis at least once when moving down the fixed hierarchical tree.</p>
    </sec>
    <sec id="sec2e">
      <title>A novel network-based empirical testing strategy</title>
      <p>The netANOVA compares the variation within a group of graphs and the variation between groups of graphs, using the ratio of the F-statistic [<xref rid="ref42" ref-type="bibr">42</xref>]. The higher the value of F, the more likely the null hypothesis <inline-formula><tex-math notation="LaTeX" id="ImEquation60">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$H0$\end{document}</tex-math></inline-formula> of no difference among the group means is false. In univariate ANOVA, the total sum of squares (SST) is computed from sums of squared differences between observations and their group mean (SSW), and between group means and the overall sample mean (SSA). A multivariate ANOVA is derived by adding up the sums of squares across all variables. Hence, a classical ANOVA test uses the concept of the mean of a group, which is complex for networks. To overcome this issue, we take advantage of the following property [<xref rid="ref25" ref-type="bibr">25</xref>]: the sum of squared distances between points and their centroid is equal to the sum of squared interpoint distances divided by the number of points (<xref rid="f1" ref-type="fig">Figure 1<bold>(B)</bold></xref>).</p>
      <p>Therefore, the total sum of square can be expressed as <disp-formula id="deqn01"><label>(1)</label><tex-math notation="LaTeX" id="DmEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{align*}&amp; SST=\frac{1}{N}\sum_{i=1}^{N-1}\sum_{j=i+1}^{N}d^2_{G_i,G_j}. \end{align*}\end{document}</tex-math></disp-formula></p>
      <p>The within-group sum of squares is <disp-formula id="deqn02"><label>(2)</label><tex-math notation="LaTeX" id="DmEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{align*}&amp; SSW=\sum_{l=1}^{k}\sum_{i=1}^{N-1}\sum_{j=i+1}^{N}\frac{1}{n_l}e_{ijl}\times d^2_{G_i,G_j}. \end{align*}\end{document}</tex-math></disp-formula></p>
      <p>The among-group sum of squares is <disp-formula id="deqn03"><label>(3)</label><tex-math notation="LaTeX" id="DmEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{align*}&amp; SSA=SST-SSW. \end{align*}\end{document}</tex-math></disp-formula></p>
      <p>Finally, the F-ratio is <disp-formula id="deqn04"><label>(4)</label><tex-math notation="LaTeX" id="DmEquation4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{align*}&amp; F=\frac{SSA/(k-1)}{SSW/(N-k)}, \end{align*}\end{document}</tex-math></disp-formula>
with <inline-formula><tex-math notation="LaTeX" id="ImEquation61">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N$\end{document}</tex-math></inline-formula> the total number of individuals, <inline-formula><tex-math notation="LaTeX" id="ImEquation62">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$n_l$\end{document}</tex-math></inline-formula> the number of networks in group <inline-formula><tex-math notation="LaTeX" id="ImEquation63">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$l$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation="LaTeX" id="ImEquation64">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula> the number of groups, <inline-formula><tex-math notation="LaTeX" id="ImEquation65">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d_{G_i,G_j}$\end{document}</tex-math></inline-formula> the distance between graph <inline-formula><tex-math notation="LaTeX" id="ImEquation66">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$i$\end{document}</tex-math></inline-formula> and graph <inline-formula><tex-math notation="LaTeX" id="ImEquation67">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j$\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math notation="LaTeX" id="ImEquation68">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$e_{ijl}$\end{document}</tex-math></inline-formula> takes the value 1 if network <inline-formula><tex-math notation="LaTeX" id="ImEquation69">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$i$\end{document}</tex-math></inline-formula> and network <inline-formula><tex-math notation="LaTeX" id="ImEquation70">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j$\end{document}</tex-math></inline-formula> are in the group <inline-formula><tex-math notation="LaTeX" id="ImEquation71">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$l$\end{document}</tex-math></inline-formula>, and 0 otherwise.</p>
      <p>Another benefit of not using a mean relates to the distance used. For Euclidean distances, the mean for each variable across observations within a group constitutes a measure of central location of the group. This is not true for many non-Euclidean distances. The statistic is also interesting in terms of the computational burden. Even though the distance between each pair of networks is required, it is computed only once. No additional computation is required on permutation replicates. In contrast, traditional ANOVA settings would require repetitive computation of network averages and distances to network averages.</p>
      <p>Since the actual statistic distribution may not have a closed form and distributional assumptions may not hold on large samples, significance is derived via permutation replicates. One critical assumption for this test is that the observations need to be exchangeable under a true null hypothesis. Thus, one needs to be careful regarding the interpretation of the significance assessment to ensure that the difference between groups is not due to differences in dispersion (i.e. difference in the distributions). Permutation tests in standard ANOVA settings typically rely on permuting known group labels. In our context, group labels are a priori unknown and inferred via a clustering procedure. Group label reshuffling, conditioning on two clusters in a clustering, will inflate overall type I error [<xref rid="ref43" ref-type="bibr">43</xref>]. To circumvent this we apply the following procedure to create appropriate null distributions of test statistics. Instead of permuting group labels at each dendrogram node, we permute the distances between the investigated graphs and re-apply hierarchical clustering to identify two groups. If both groups have a size surpassing the group size threshold, we compute the statistics described above. For instance, we repeat the procedure 99 times and compare the permuted statistics <inline-formula><tex-math notation="LaTeX" id="ImEquation72">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$F^\pi $\end{document}</tex-math></inline-formula> with the observed statistic <inline-formula><tex-math notation="LaTeX" id="ImEquation73">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$F$\end{document}</tex-math></inline-formula>: <disp-formula id="deqn05"><label>(5)</label><tex-math notation="LaTeX" id="DmEquation5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{align*}&amp; p-value=\frac{\# (F^\pi \geq F)+1}{Total \# F^\pi + 1} \end{align*}\end{document}</tex-math></disp-formula></p>
      <p>We emphasize that when permuting the values in the original distance matrix, the new matrix cannot be considered a distance matrix because the measure violates the triangular inequality. After applying the permutations, we can indeed obtain <inline-formula><tex-math notation="LaTeX" id="ImEquation85">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d_{G_i,G_l}&gt;d_{G_i,G_k} + d_{G_k,G_l}$\end{document}</tex-math></inline-formula>. The linkage criteria in the hierarchical clustering are then limited to methods requiring dissimilarities to be non-negative and symmetric only, such as complete and average linkage methods [<xref rid="ref44" ref-type="bibr">44</xref>]. The evaluation of the impact of this linkage criteria is shown in Section <xref rid="sec3a" ref-type="sec">3.1</xref>. In the available code, the user can select ‘complete’ (default) or ‘average’ linkage.</p>
      <p>In Section <xref rid="sec3a" ref-type="sec">3.1</xref>, we also compare different perturbation levels of the distance matrix and set the default amount of perturbation in the distance matrix to 20% and the default number of replicates to 99. These parameters are customizable, as is the significance threshold (default 0.05).</p>
    </sec>
    <sec id="sec2f">
      <title>Evaluation and application</title>
      <p>All the experiments are conducted on a Scientific Linux release 7.2 (Nitrogen) cluster.</p>
      <sec id="sec2f1">
        <title>Simulations—Type I error</title>
        <p>To evaluate the statistical relevance of the detected groups and the impact of our significance assessment, we study if the proposed workflow controls the Type I error. We perform a simulation analysis based on 1000 replicates for that purpose. First, we generate an original random graph with <inline-formula><tex-math notation="LaTeX" id="ImEquation86">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$m$\end{document}</tex-math></inline-formula> nodes and a density <inline-formula><tex-math notation="LaTeX" id="ImEquation87">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$d$\end{document}</tex-math></inline-formula>. For weighted networks, we simulate binary networks and replace the value of edges present by a random number from a normal distribution with a mean of 0.5 and standard distribution of 0.5*0.5. The edge values are scaled via the min–max scaling algorithm so that values of the adjacency matrix range from 0 and 1. Importantly, we consider the minimum and maximum values across all objects, so these boundaries are the for all networks.</p>
        <p>Then, in both the binary and the weighted contexts, we derive <inline-formula><tex-math notation="LaTeX" id="ImEquation88">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$n$\end{document}</tex-math></inline-formula> graphs by randomly rewiring the edges while preserving the original graph’s degree distribution [<xref rid="ref45" ref-type="bibr">45</xref>] of the original graph. Specifically, the algorithm chooses two arbitrary edges ((<inline-formula><tex-math notation="LaTeX" id="ImEquation89">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_a$\end{document}</tex-math></inline-formula>,<inline-formula><tex-math notation="LaTeX" id="ImEquation90">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_b$\end{document}</tex-math></inline-formula>) and (<inline-formula><tex-math notation="LaTeX" id="ImEquation91">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_c$\end{document}</tex-math></inline-formula>,<inline-formula><tex-math notation="LaTeX" id="ImEquation92">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_d$\end{document}</tex-math></inline-formula>)) and substitutes them with (<inline-formula><tex-math notation="LaTeX" id="ImEquation93">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_a$\end{document}</tex-math></inline-formula>,<inline-formula><tex-math notation="LaTeX" id="ImEquation94">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_d$\end{document}</tex-math></inline-formula>) and (<inline-formula><tex-math notation="LaTeX" id="ImEquation95">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_c$\end{document}</tex-math></inline-formula>,<inline-formula><tex-math notation="LaTeX" id="ImEquation96">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_b$\end{document}</tex-math></inline-formula>) if they do not yet exist.</p>
        <p>We evaluate the impact on the type I error of the level of perturbation, the number of graphs, the number of nodes, the graph density, the minimum group size, the distance used to compute dissimilarity between graphs and the minimum number of networks per group. In the baseline, the original network has a random structure (simulated from the Erdos–Renyi model), generated using the function erdos.renyi.game() from the R package igraph [<xref rid="ref45" ref-type="bibr">45</xref>]. It has a density of 0.05, 100 nodes and binary edges. When at least two groups are detected via netANOVA in a permutation, that permutation is considered a false positive (FP). This allows us to compute the type I error rate as <inline-formula><tex-math notation="LaTeX" id="ImEquation97">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\frac{ \text{\# FP}}{1000}$\end{document}</tex-math></inline-formula>.</p>
      </sec>
      <sec id="sec2f2">
        <title>Simulations—power</title>
        <p>We simulate the situation where each network represents its own individual (e.g. patients), the nodes are labelled and shared across all networks (e.g. genes) and several populations exist (e.g. disease sub-type). The goal is to identify and compare the different populations. To this end, the following experimental set-up (<xref rid="f1" ref-type="fig">Figure 1<bold>(C)</bold></xref>) is implemented. First, we generate an original network and perturb it to derive group networks. Then, we perturb each group network to create individual networks. The goal is to apply the unsupervised netANOVA to assign the individual networks to the correct groups. We validate the clustering via Jaccard similarity.</p>
        <p>We consider the same baseline original network as in Section <xref rid="sec2f1" ref-type="sec">2.6.1</xref>: a network with a random structure, a density of 0.05, 100 nodes and binary edges. In the baseline, we switch 40% of the original edges while preserving the degree distribution. This is done 10 times to create 10 group networks. Then, we switch 40% of the edges for each group network while preserving the degree distribution, 10 times to create 10 individual networks per group.</p>
        <p>We create 800 replicates, and we evaluate the impact of multiple parameters. Some parameters are associated with the network properties, such as network size or structure. Others are related to the method, such as the correction for multiple testing or the distance between graphs. The influence of the perturbation types and the minimum size of the groups are also studied.</p>
      </sec>
      <sec id="sec2f3">
        <title>Real-life data application</title>
        <p>We apply netANOVA to two real-life bioinformatics graph datasets. For both applications, we use datasets with known clusterings to be able to use these ‘true’ clusters to compute the performance of netANOVA. Hence, a supervised model could be used to define group membership. Still, the goal here is to evaluate the unsupervised procedure; we do not use any information about the groups in the netANOVA workflow.</p>
      </sec>
      <sec id="sec30a">
        <title>UNC scenario</title>
        <p>The graph dataset MUTAG [<xref rid="ref46" ref-type="bibr">46</xref>] contains collection of nitroaromatic compounds. The aim is usually to predict the mutagenicity of the compounds on Salmonella typhimurium. The nodes represent atoms, while edges are bonds between the corresponding atoms. The dataset includes 188 samples of chemical compounds. It is publicly available and commonly used to compare classification performances.</p>
      </sec>
      <sec id="sec30b">
        <title>KNC scenario</title>
        <p>We also apply netANOVA to graphs with known node correspondence, i.e. multiplex networks. Previous work [<xref rid="ref47" ref-type="bibr">47</xref>, <xref rid="ref48" ref-type="bibr">48</xref>] has shown the potential of brain networks to distinguish between various brain disorders. We selected the COBRE brain networks [<xref rid="ref47" ref-type="bibr">47</xref>, <xref rid="ref49" ref-type="bibr">49</xref>]. It contains 124 individual-specific networks: 70 controls and 54 schizophrenics. The brain networks are constructed from imaging data (resting state fMRI) to represent functional connectivity between regions of the brain. The graphs are composed of 263 nodes obtained from the Power parcellation [<xref rid="ref50" ref-type="bibr">50</xref>] and 34 453 edges. The edge weights are the Fisher-transformed correlation between the fMRI time series of the nodes. Nuisance covariates like age, gender, motion and handedness have been regressed out. For a description of the preprocessing steps to obtain the network edge weights, see [<xref rid="ref47" ref-type="bibr">47</xref>].</p>
        <p>The brain networks are fully connected. In their functional brain connectivity analysis (identification of controls versus autism spectrum disorder), Wills <italic toggle="yes">et al</italic>. [<xref rid="ref51" ref-type="bibr">51</xref>] found that only a subset of edges represents the structural differences between the two groups of graphs studied. The dissimilarities could not be identified with all the edges. Also, since the local changes in connectivity were of the same order of magnitude as the random local variations, a comparison using all the edges was ineffective. Similar findings were reported [<xref rid="ref52" ref-type="bibr">52</xref>, <xref rid="ref53" ref-type="bibr">53</xref>]. Therefore, we evaluated the impact of graphs sparsification using the method developed by Relión <italic toggle="yes">et al</italic>. [<xref rid="ref47" ref-type="bibr">47</xref>] to select edges. This method incorporates the network nature of the data via penalties to promote sparsity in the number of nodes, in addition to sparsity penalties that encourage the selection of edges. Specifically, to capture structural predictive edges, the authors focus on convex structured sparsity penalties that favour a small number of active nodes (nodes attached to at least one edge with a non-zero coefficient). To find a set of such nodes, they focus on convex formulations that encourage small active node sets indirectly. They penalize the number of active nodes by treating all edges connected to one node as a group. Then, eliminating this group is equivalent to de-activating a node.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Results</title>
    <p>All adopted simulation and real-life application parameters settings and choices are summarized in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table 1</xref>.</p>
    <sec id="sec3a">
      <title>Type I error</title>
      <p>We first investigated the influence of the network properties on the type I error (see <xref rid="TB1" ref-type="table">Table 1</xref>.). Some measures gave rise to a type I error under control in all experimental settings: edge difference, Hamming distance, shortest path kernel, <inline-formula><tex-math notation="LaTeX" id="ImEquation98">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-step random walk kernel, DeltaCon distance and Gaussian kernel. The graph diffusion distance was more prone to type I error. The network density had a high impact: a higher density produces more conservative results. In our simulation setting, we first generate an original random graph, and we derive 50 graphs by randomly rewiring 40% of the original graph’s edges while preserving the original graph’s degree distribution. Hence, increasing the density will provide more information but also more heterogeneity. Indeed, including more edges may induce more noise [<xref rid="ref54" ref-type="bibr">54</xref>]. We also evaluated the algorithm on weighted networks. Although some distances became highly conservative, most of them tended to behave as in the baseline.</p>
      <table-wrap position="float" id="TB1">
        <label>Table 1</label>
        <caption>
          <p>Type I error (%) of the netANOVA workflow depending on network properties, estimated over 1000 random replicates, as explained in Section <xref rid="sec2f1" ref-type="sec">2.6.1</xref>. The baseline corresponds to 50 networks, each one 100 nodes, a density of 0.05 and 40% of the edges switched. The minimum group size is 10, 20% of the distance matrix is shuffled in the netANOVA permutations and the linkage method in the hierarchical clustering is ‘complete’.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1" align="left"/>
            <col span="1" align="left"/>
            <col span="1" align="left"/>
            <col span="1" align="left"/>
            <col span="1" align="left"/>
            <col span="1" align="left"/>
            <col span="1" align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Measure</th>
              <th align="left" rowspan="1" colspan="1">Baseline</th>
              <th align="left" rowspan="1" colspan="1">Networks</th>
              <th align="left" rowspan="1" colspan="1">Nodes</th>
              <th align="left" rowspan="1" colspan="1">Density</th>
              <th align="left" rowspan="1" colspan="1">Perturbation</th>
              <th align="left" rowspan="1" colspan="1">Weighted</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">100</th>
              <th align="left" rowspan="1" colspan="1">500</th>
              <th align="left" rowspan="1" colspan="1">0.1</th>
              <th align="left" rowspan="1" colspan="1">60%</th>
              <th rowspan="1" colspan="1"/>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Edge difference distance [<xref rid="ref34" ref-type="bibr">34</xref>]</td>
              <td rowspan="1" colspan="1">4.0</td>
              <td rowspan="1" colspan="1">4.4</td>
              <td rowspan="1" colspan="1">1.1</td>
              <td rowspan="1" colspan="1">1.4</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">4.7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Hamming distance [<xref rid="ref39" ref-type="bibr">39</xref>]</td>
              <td rowspan="1" colspan="1">4.0</td>
              <td rowspan="1" colspan="1">4.2</td>
              <td rowspan="1" colspan="1">1.1</td>
              <td rowspan="1" colspan="1">1.5</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">NA</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Shortest path kernel [<xref rid="ref40" ref-type="bibr">40</xref>]</td>
              <td rowspan="1" colspan="1">4.0</td>
              <td rowspan="1" colspan="1">4.4</td>
              <td rowspan="1" colspan="1">1.1</td>
              <td rowspan="1" colspan="1">1.4</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><inline-formula><tex-math notation="LaTeX" id="ImEquation99">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-step random walk kernel [<xref rid="ref35" ref-type="bibr">35</xref>]</td>
              <td rowspan="1" colspan="1">4.0</td>
              <td rowspan="1" colspan="1">4.4</td>
              <td rowspan="1" colspan="1">1.1</td>
              <td rowspan="1" colspan="1">1.4</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><inline-formula><tex-math notation="LaTeX" id="ImEquation100">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-step random walk kernel KNC [<xref rid="ref35" ref-type="bibr">35</xref>]</td>
              <td rowspan="1" colspan="1">2.6</td>
              <td rowspan="1" colspan="1">4.3</td>
              <td rowspan="1" colspan="1">1.3</td>
              <td rowspan="1" colspan="1">1.6</td>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">5.2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeltaCon [<xref rid="ref36" ref-type="bibr">36</xref>]</td>
              <td rowspan="1" colspan="1">2.3</td>
              <td rowspan="1" colspan="1">3.4</td>
              <td rowspan="1" colspan="1">1.1</td>
              <td rowspan="1" colspan="1">1.8</td>
              <td rowspan="1" colspan="1">3.6</td>
              <td rowspan="1" colspan="1">3.0</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Graph Diffusion Distance [<xref rid="ref34" ref-type="bibr">34</xref>]</td>
              <td rowspan="1" colspan="1">7.2</td>
              <td rowspan="1" colspan="1">12.7</td>
              <td rowspan="1" colspan="1">0.6</td>
              <td rowspan="1" colspan="1">2.2</td>
              <td rowspan="1" colspan="1">7.6</td>
              <td rowspan="1" colspan="1">9.8</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Gaussian kernel [<xref rid="ref38" ref-type="bibr">38</xref>]</td>
              <td rowspan="1" colspan="1">4.0</td>
              <td rowspan="1" colspan="1">4.4</td>
              <td rowspan="1" colspan="1">1.1</td>
              <td rowspan="1" colspan="1">1.2</td>
              <td rowspan="1" colspan="1">4.1</td>
              <td rowspan="1" colspan="1">4.7</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GTOM [<xref rid="ref37" ref-type="bibr">37</xref>]</td>
              <td rowspan="1" colspan="1">4.7</td>
              <td rowspan="1" colspan="1">4.2</td>
              <td rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">1.4</td>
              <td rowspan="1" colspan="1">3.9</td>
              <td rowspan="1" colspan="1">5.1</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Then, we quantified the impact of the algorithm options (see <xref rid="TB2" ref-type="table">Table 2</xref>.). Overall, the type I error was still under control in almost all settings. The type I error tended to deflate when decreasing the minimum group size. Furthermore, the linkage criterium in the hierarchical clustering significantly impacted the false positive rate. The average linkage being highly conservative, the complete linkage was set as the default option. Finally, the higher the number of perturbations in the distance matrix in the netANOVA permutation procedure, the more conservative the test.</p>
      <table-wrap position="float" id="TB2">
        <label>Table 2</label>
        <caption>
          <p>Type I error (%) of the netANOVA workflow depending on netANOVA parameters, estimated over 1000 random replicates, as explained in Section <xref rid="sec2f1" ref-type="sec">2.6.1</xref> I error. The baseline corresponds to 50 networks, each one having 100 nodes, a density 0.05 and 40% of the edges switched. The minimum group size is 10, 20% of the distance matrix is shuffled in the netANOVA permutations and the linkage method in the hierarchical clustering is ‘complete’.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col span="1" align="left"/>
            <col span="1" align="left"/>
            <col span="1" align="left"/>
            <col span="1" align="left"/>
            <col span="1" align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Measure</th>
              <th align="left" rowspan="1" colspan="1">Min group size</th>
              <th align="left" rowspan="1" colspan="1">Linkage</th>
              <th align="left" rowspan="1" colspan="1">Perturbation of</th>
              <th align="left" rowspan="1" colspan="1">Perturbation of</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">5</th>
              <th align="left" rowspan="1" colspan="1">average</th>
              <th align="left" rowspan="1" colspan="1">distance matrix 10%</th>
              <th align="left" rowspan="1" colspan="1">distance matrix 50%</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Edge difference distance [<xref rid="ref34" ref-type="bibr">34</xref>]</td>
              <td rowspan="1" colspan="1">3.5</td>
              <td rowspan="1" colspan="1">0</td>
              <td rowspan="1" colspan="1">4.2</td>
              <td rowspan="1" colspan="1">3.2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Hamming distance [<xref rid="ref39" ref-type="bibr">39</xref>]</td>
              <td rowspan="1" colspan="1">3.5</td>
              <td rowspan="1" colspan="1">0.2</td>
              <td rowspan="1" colspan="1">4.4</td>
              <td rowspan="1" colspan="1">3.4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Shortest path kernel [<xref rid="ref40" ref-type="bibr">40</xref>]</td>
              <td rowspan="1" colspan="1">3.5</td>
              <td rowspan="1" colspan="1">0</td>
              <td rowspan="1" colspan="1">4.2</td>
              <td rowspan="1" colspan="1">3.2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><inline-formula><tex-math notation="LaTeX" id="ImEquation101">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-step random walk kernel [<xref rid="ref35" ref-type="bibr">35</xref>]</td>
              <td rowspan="1" colspan="1">3.5</td>
              <td rowspan="1" colspan="1">0</td>
              <td rowspan="1" colspan="1">4.2</td>
              <td rowspan="1" colspan="1">3.2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><inline-formula><tex-math notation="LaTeX" id="ImEquation102">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-step random walk kernel KNC [<xref rid="ref35" ref-type="bibr">35</xref>]</td>
              <td rowspan="1" colspan="1">2.1</td>
              <td rowspan="1" colspan="1">0.1</td>
              <td rowspan="1" colspan="1">2.5</td>
              <td rowspan="1" colspan="1">1.6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DeltaCon [<xref rid="ref36" ref-type="bibr">36</xref>]</td>
              <td rowspan="1" colspan="1">2</td>
              <td rowspan="1" colspan="1">0.2</td>
              <td rowspan="1" colspan="1">2.2</td>
              <td rowspan="1" colspan="1">1</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Graph Diffusion Distance [<xref rid="ref34" ref-type="bibr">34</xref>]</td>
              <td rowspan="1" colspan="1">6.2</td>
              <td rowspan="1" colspan="1">0.9</td>
              <td rowspan="1" colspan="1">4.7</td>
              <td rowspan="1" colspan="1">7.5</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Gaussian kernel [<xref rid="ref38" ref-type="bibr">38</xref>]</td>
              <td rowspan="1" colspan="1">3.4</td>
              <td rowspan="1" colspan="1">0</td>
              <td rowspan="1" colspan="1">4.4</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GTOM [<xref rid="ref37" ref-type="bibr">37</xref>]</td>
              <td rowspan="1" colspan="1">4.2</td>
              <td rowspan="1" colspan="1">0.7</td>
              <td rowspan="1" colspan="1">4.8</td>
              <td rowspan="1" colspan="1">4</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec3b">
      <title>Power</title>
      <p>The baseline scenario has an original network with a random structure, 100 nodes, a density of 0.05 and binary edges. It contains 10 groups and 10 networks per group obtained via degree preserving rewiring 40% of the edges. The hierarchical clustering is performed with complete-linkage clustering and the multiple testing correction is based on the depth of the dendrogram (Section <xref rid="sec2d" ref-type="sec">2.4</xref>). The minimum group size is set to 5. In the other scenarios, we altered one parameter at a time. The properties of networks and parameters used to derive results are described in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table 2</xref>. Overall, the baseline scenario gave good performance with a mean Jaccard index of 0.85 across all distances (see <xref rid="f2" ref-type="fig">Figure 2</xref>). The correction for multiple testing using the depth of the tree (see Section <xref rid="sec2d" ref-type="sec">2.4</xref>) was less conservative than the correction developed by Meinshausen [<xref rid="ref41" ref-type="bibr">41</xref>] and was, therefore, more optimal with the chosen baseline parameters. Indeed, the former detected nine groups on average across distances versus six groups for the latter. To validate the trends identified in the Section <xref rid="sec3a" ref-type="sec">3.1</xref>, we applied the average linkage in the hierarchical clustering. Here, it detected only seven groups on average. It confirmed that this linkage is more stringent than the complete one and makes the detection of the correct clusters more complex.</p>
      <fig position="float" id="f2">
        <label>Figure 2</label>
        <caption>
          <p>Average Jaccard index across multiple simulation scenarios for 800 replicates. The properties of networks and parameters used to derive results are described in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table 2</xref>. The baseline scenario has an original network with a random structure, 100 nodes, a density of 0.05 and binary edges. It contains 10 groups and 10 networks per group obtained via degree preserving rewiring 40% of the edges. The hierarchical clustering is performed with complete-linkage clustering and the multiple testing correction is based on the depth of the dendrogram (Section <xref rid="sec2d" ref-type="sec">2.4</xref>). In the other scenarios, we altered one parameter at a time. <italic toggle="yes">Density</italic> corresponds to an original network with a density 0.1. <italic toggle="yes">Cluster</italic> corresponds to an original network with a cluster structure and <italic toggle="yes">Scale-free</italic> to a scale-free original network (from Barabasi-Albert models). <italic toggle="yes">Weighted</italic> is for weighted networks (see Section <xref rid="sec2f2" ref-type="sec">2.6.2</xref>). <italic toggle="yes">Perturbation 60%</italic> means that group networks and individual networks are obtained with degree preserving rewiring 60% of the edges. Then, <italic toggle="yes">Add</italic>, <italic toggle="yes">Remove</italic> and <italic toggle="yes">switch</italic> correspond to addition, removal and random switching of edges instead of degree preserving rewiring. <italic toggle="yes">Linkage</italic> stands for average linkage in hierarchical clustering. The <italic toggle="yes">50 networks</italic> scenario has 10 groups of 50 networks, and the <italic toggle="yes">2 groups</italic> scenario has two groups of 10 networks. Finally, <italic toggle="yes">MT</italic> corresponds to the multiple testing correction developed by Meinshausen [<xref rid="ref41" ref-type="bibr">41</xref>] (Section <xref rid="sec2d" ref-type="sec">2.4</xref>), which controls the FWER at level <inline-formula><tex-math notation="LaTeX" id="ImEquation103">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\alpha \in (0, 1)$\end{document}</tex-math></inline-formula>, using the significance threshold <inline-formula><tex-math notation="LaTeX" id="ImEquation104">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\alpha _{adj}=\alpha \times \frac{N_j-1}{N-1}$\end{document}</tex-math></inline-formula> with <inline-formula><tex-math notation="LaTeX" id="ImEquation105">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N_j$\end{document}</tex-math></inline-formula> the number of networks clustered at node <inline-formula><tex-math notation="LaTeX" id="ImEquation106">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$j$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation107">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N$\end{document}</tex-math></inline-formula> the total number of networks.</p>
        </caption>
        <graphic xlink:href="bbad029f2" position="float"/>
      </fig>
      <p>We also compared various graph characteristics. With two groups only instead of 10, the classification was perfect for almost all distances. Also, when we simulated larger groups (50 graphs per cluster), the Jaccard index was comparable with the one obtained with 10 networks per group since it ranged between 0.79 and 0.89. Then, we tested multiple perturbation types: random switching, removal of edges and addition of edges. GTOM was less indicated when the perturbation was the removal of edges (resp. random switching) since the associated Jaccard index is 0.14 (resp. 0.1) on average. We also modified the original network structure and tested scale-free graphs using Barabasi-Albert models and cluster networks. With scale-free networks, across all distances except GTOM and graph diffusion, the average Jaccard index was again relatively high (0.83). The average Jaccard index per distance ranged from 0.79 to 0.86 across all distances with cluster networks.</p>
      <p>Moreover, we investigated the impact of perturbations within and between groups of networks by increasing this level up to 60%. The average Jaccard indexes were not highly different from those obtained with the baseline. Then, we increased the density of networks. With a density of 0.1 instead of 0.05, the average Jaccard index ranged from 0.85 to 0.88 and hence, groups were still detectable. We also tested weighted networks (see Section <xref rid="sec3a" ref-type="sec">3.1</xref>), and observed that distances based on random walk kernel did not perform as good as the other distances. In most settings, the graph diffusion distance tended to have difficulties clustering the graphs correctly. On the contrary, DeltaCon and the custom random walk kernel performed overall better than the other measures.</p>
    </sec>
    <sec id="sec3c">
      <title>Real-life data application</title>
      <sec id="sec30c">
        <title>UNC scenario</title>
        <p>The UNC application takes as input the list of 188 nitroaromatic compound networks. The goal is to create groups of networks to verify if we can identify the mutagenicity of the compounds on Salmonella typhimurium (2 groups). These group labels are not used to derive the clusterings, they are only used a posteriori to obtain the accuracy by comparison between the inferred groups and the ground truth. We compared the results of netANOVA with the methodologies DGI [<xref rid="ref55" ref-type="bibr">55</xref>], InfoGraph [<xref rid="ref12" ref-type="bibr">12</xref>], GraPHmax [<xref rid="ref13" ref-type="bibr">13</xref>] and its variants, and graph2vec. We also applied graph2vec embedding to convert variable-size graphs into a fixed-size representation of graphs and combined it with an autoencoder. The <inline-formula><tex-math notation="LaTeX" id="ImEquation108">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-means algorithm was used on the vector representations of the graphs obtained from the different algorithms with <inline-formula><tex-math notation="LaTeX" id="ImEquation109">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k=$\end{document}</tex-math></inline-formula># unique labels<inline-formula><tex-math notation="LaTeX" id="ImEquation110">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$=2$\end{document}</tex-math></inline-formula>. Moreover, we computed the pairwise distance between networks using the random walk kernel, and we used the inferred similarity matrix as input to a spectral clustering (with <inline-formula><tex-math notation="LaTeX" id="ImEquation111">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k=2$\end{document}</tex-math></inline-formula>) algorithm. We have included details on graph2vec and autoencoder parameters in the <xref rid="sup2" ref-type="supplementary-material">Supplementary</xref>.</p>
        <fig position="float" id="f3">
          <label>Figure 3</label>
          <caption>
            <p><bold>(A)</bold> netANOVA clustering on the MUTAG dataset. The pairwise distance between the 188 nitroaromatic compound networks is based on the random walk kernel. The minimum group size is <inline-formula><tex-math notation="LaTeX" id="ImEquation124">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$t=40$\end{document}</tex-math></inline-formula>. We set the other parameters to the default options. Two groups are identified by netANOVA: the red and the blue groups (<italic toggle="yes">P</italic>-value<inline-formula><tex-math notation="LaTeX" id="ImEquation125">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$=0.01$\end{document}</tex-math></inline-formula>). <bold>(B)</bold> netANOVA clustering on the pre-filtered COBRE dataset. The edge selection was performed according to Relión <italic toggle="yes">et al</italic>. [<xref rid="ref47" ref-type="bibr">47</xref>], with <inline-formula><tex-math notation="LaTeX" id="ImEquation126">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\rho =1$\end{document}</tex-math></inline-formula>. The pairwise edge difference distance between the 124 individual-specific brain networks is computed. The minimum group size is <inline-formula><tex-math notation="LaTeX" id="ImEquation127">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$t=10$\end{document}</tex-math></inline-formula>. We set the other parameters to the default options. Two groups are identified by netANOVA: the red and the blue groups (<italic toggle="yes">P</italic>-value<inline-formula><tex-math notation="LaTeX" id="ImEquation128">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$=0.01$\end{document}</tex-math></inline-formula>).</p>
          </caption>
          <graphic xlink:href="bbad029f3" position="float"/>
        </fig>
        <p>The default options are selected in netANOVA. We compute the pairwise distance between the 188 nitroaromatic compound networks based on the random walk kernel. We set the minimum group size to <inline-formula><tex-math notation="LaTeX" id="ImEquation112">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$t=40$\end{document}</tex-math></inline-formula>. Then, hierarchical clustering is applied to the distance matrix to derive a dendrogram and identify preliminary groups. We illustrate the procedure in <xref rid="f3" ref-type="fig">Figure 3<bold>(A)</bold></xref>. The first two groups are significantly different (<xref rid="sec2e" ref-type="sec">Algorithm 2</xref>), so we go to the next level of the dendrogram and re-apply <xref rid="sec2e" ref-type="sec">Algorithm 2</xref> to assess whether the corresponding subgroups are significantly different. We progress down the dendrogram tree, and stop when the groups are too small, or when the groups are not significantly different. Since subgroup <inline-formula><tex-math notation="LaTeX" id="ImEquation113">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a_1$\end{document}</tex-math></inline-formula> (<xref rid="f3" ref-type="fig">Figure 3<bold>(A)</bold></xref>) contains fewer networks than the minimum group size, we progress in the associated branch. Since the size of groups <inline-formula><tex-math notation="LaTeX" id="ImEquation114">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a_{21}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation115">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a_{22}$\end{document}</tex-math></inline-formula> are both smaller than <inline-formula><tex-math notation="LaTeX" id="ImEquation116">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$t$\end{document}</tex-math></inline-formula>, no significant subgroup is detected. We observe the same in group <inline-formula><tex-math notation="LaTeX" id="ImEquation117">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b$\end{document}</tex-math></inline-formula>: Since subgroup <inline-formula><tex-math notation="LaTeX" id="ImEquation118">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_1$\end{document}</tex-math></inline-formula> contains fewer networks than the minimum group size, we progress in the associated branch. The sizes of groups <inline-formula><tex-math notation="LaTeX" id="ImEquation119">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_{21}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation120">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_{22}$\end{document}</tex-math></inline-formula> are both smaller than <inline-formula><tex-math notation="LaTeX" id="ImEquation121">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$t$\end{document}</tex-math></inline-formula>, so no significant subgroup is detected. Thus, we identify two significant groups in the MUTAG dataset (<inline-formula><tex-math notation="LaTeX" id="ImEquation122">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation123">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b$\end{document}</tex-math></inline-formula>) with netANOVA.</p>
        <p>
          <inline-graphic xlink:href="bbad029fx2.jpg"/>
        </p>
        <p>The netANOVA algorithm detected the correct number of groups (2) and the associated accuracy is 79.8% (see <xref rid="TB3" ref-type="table">Table 3</xref>). Among the 10 comparative analyses, only two yielded improved performance (graPHmax and GraPHmax+NF) compared with netANOVA. Importantly, with all the methodologies except netANOVA, the correct number of groups was forced. Overall, results show that netANOVA was able to achieve competitive and in many cases superior performance while being able to determine the number of groups and assess statistical significance. </p>
        <table-wrap position="float" id="TB3">
          <label>Table 3</label>
          <caption>
            <p>UNC scenario: clustering accuracy of unsupervised graph-based algorithms applied to the MUTAG dataset. The different unsupervised algorithms take as input the list of 188 nitroaromatic compound networks. The mutagenicity (two groups) of the compounds on Salmonella typhimurium is considered as the ground truth. These group labels are not used to derive the clusterings, but only a posteriori to obtain the accuracy by comparison between the inferred groups and the ground truth.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1" align="left"/>
              <col span="1" align="left"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Method</th>
                <th align="left" rowspan="1" colspan="1">Accuracy (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">DGI [<xref rid="ref55" ref-type="bibr">55</xref>]</td>
                <td rowspan="1" colspan="1">72.34</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">InfoGraph [<xref rid="ref12" ref-type="bibr">12</xref>]</td>
                <td rowspan="1" colspan="1">77.65</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GraPHmax+NF [<xref rid="ref13" ref-type="bibr">13</xref>]</td>
                <td rowspan="1" colspan="1">84.10</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GraPHmax+EF [<xref rid="ref13" ref-type="bibr">13</xref>]</td>
                <td rowspan="1" colspan="1">68.08</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GraPHmax-P[<xref rid="ref13" ref-type="bibr">13</xref>]</td>
                <td rowspan="1" colspan="1">77.12</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GraPHmax-H [<xref rid="ref13" ref-type="bibr">13</xref>]</td>
                <td rowspan="1" colspan="1">76.59</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">GraPHmax [<xref rid="ref13" ref-type="bibr">13</xref>]</td>
                <td rowspan="1" colspan="1">85.04</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">graph2vec [<xref rid="ref14" ref-type="bibr">14</xref>]</td>
                <td rowspan="1" colspan="1">78.2</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">graph2vec + autoencoder [<xref rid="ref14" ref-type="bibr">14</xref>, <xref rid="ref56" ref-type="bibr">56</xref>]</td>
                <td rowspan="1" colspan="1">77.01</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">random walk kernel [<xref rid="ref57" ref-type="bibr">57</xref>]</td>
                <td rowspan="1" colspan="1">67.02</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">netANOVA</td>
                <td rowspan="1" colspan="1">79.8</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="sec30d">
        <title>KNC scenario</title>
        <p>The KNC application takes as input the list of 124 individual-specific brain networks. The goal is to see if we can differentiate the group of controls and the group of people with schizophrenia (two groups). We compared netANOVA performance with six approaches previously used on this dataset: graphclass [<xref rid="ref47" ref-type="bibr">47</xref>], Elastic net [<xref rid="ref58" ref-type="bibr">58</xref>], SVM-L1 [<xref rid="ref59" ref-type="bibr">59</xref>], Signal-subgraph [<xref rid="ref60" ref-type="bibr">60</xref>], DLDA and LASSO (see <xref rid="TB4" ref-type="table">Table 4</xref>).</p>
        <table-wrap position="float" id="TB4">
          <label>Table 4</label>
          <caption>
            <p>KNC scenario: accuracy for different classification and clustering methods with variable selection on the COBRE dataset. The list of 124 individual-specific brain networks is used as input. The status cases (schizophrenia) and controls are considered as the ground truth. Supervised results are reported from Relión <italic toggle="yes">et al</italic>. [<xref rid="ref47" ref-type="bibr">47</xref>]. The group labels are not used within the netANOVA clustering. They are used a priori to identify relevant graph substructures using the method developed by Relión <italic toggle="yes">et al</italic>. [<xref rid="ref47" ref-type="bibr">47</xref>] (Section <xref rid="sec2f" ref-type="sec">2.6.3</xref>). The group labels are also used a posteriori to obtain the accuracy by comparison between the inferred groups and the ground truth.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col span="1" align="left"/>
              <col span="1" align="left"/>
              <col span="1" align="left"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">Method</th>
                <th align="left" rowspan="1" colspan="1">Accuracy (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">
                  <bold>Supervised</bold>
                </td>
                <td rowspan="1" colspan="1">graphclass [<xref rid="ref47" ref-type="bibr">47</xref>]</td>
                <td rowspan="1" colspan="1">92.7(2.6)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Cross-validated accuracy (average</td>
                <td rowspan="1" colspan="1">Elastic net [<xref rid="ref58" ref-type="bibr">58</xref>]</td>
                <td rowspan="1" colspan="1">89.5 (1.8)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">and standard errors over 10 folds)</td>
                <td rowspan="1" colspan="1">SVM-L1 [<xref rid="ref59" ref-type="bibr">59</xref>]</td>
                <td rowspan="1" colspan="1">87.9 (2.2)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Signal-subgraph [<xref rid="ref60" ref-type="bibr">60</xref>]</td>
                <td rowspan="1" colspan="1">86.1 (3.3)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">DLDA</td>
                <td rowspan="1" colspan="1">84.6 (3.3)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">LASSO</td>
                <td rowspan="1" colspan="1">80.1 (5.6)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <bold>Unsupervised</bold>
                </td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">netANOVA <inline-formula><tex-math notation="LaTeX" id="ImEquation129">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\rho =1$\end{document}</tex-math></inline-formula> (3,766 edges)</td>
                <td rowspan="1" colspan="1">91.6</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">netANOVA <inline-formula><tex-math notation="LaTeX" id="ImEquation130">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\rho =0.8$\end{document}</tex-math></inline-formula> (4,817 edges)</td>
                <td rowspan="1" colspan="1">100</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">netANOVA <inline-formula><tex-math notation="LaTeX" id="ImEquation131">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\rho =0.6$\end{document}</tex-math></inline-formula> (6,283 edges)</td>
                <td rowspan="1" colspan="1">100</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">netANOVA <inline-formula><tex-math notation="LaTeX" id="ImEquation132">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\rho =0.4$\end{document}</tex-math></inline-formula> (9,606 edges)</td>
                <td rowspan="1" colspan="1">no group detected</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">netANOVA <inline-formula><tex-math notation="LaTeX" id="ImEquation133">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\rho =0.2$\end{document}</tex-math></inline-formula> (33,796 edges)</td>
                <td rowspan="1" colspan="1">no group detected</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">netANOVA <inline-formula><tex-math notation="LaTeX" id="ImEquation134">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\rho =0$\end{document}</tex-math></inline-formula> (34,453 edges)</td>
                <td rowspan="1" colspan="1">no group detected</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>In netANOVA, we compute the pairwise distance between the 124 brain networks using the edge difference distance because nodes are labelled and weighted. The minimum group size is s10 and the default options were used. Since networks are originally fully connected, we evaluated the impact of graph sparsification using the method developed by Relión <italic toggle="yes">et al</italic>. [<xref rid="ref47" ref-type="bibr">47</xref>] (Section <xref rid="sec2f" ref-type="sec">2.6.3</xref>). Hierarchical clustering is applied to the distance matrix, and the <xref rid="sec2d" ref-type="sec">Algorithm 1</xref> is recursively used to identify the final groups. We illustrate the procedure in <xref rid="f3" ref-type="fig">Figure 3<bold>(B)</bold></xref>. The first two groups are significantly different. Then, <inline-formula><tex-math notation="LaTeX" id="ImEquation135">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a_1$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math notation="LaTeX" id="ImEquation136">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a_{21}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation137">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a_{221}$\end{document}</tex-math></inline-formula> are too small to be tested. Groups <inline-formula><tex-math notation="LaTeX" id="ImEquation138">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a_{2221}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation139">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a_{2222}$\end{document}</tex-math></inline-formula> are not significantly different. Also, <inline-formula><tex-math notation="LaTeX" id="ImEquation140">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_1$\end{document}</tex-math></inline-formula> is too small to be tested, and groups <inline-formula><tex-math notation="LaTeX" id="ImEquation141">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_{21}$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation142">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b_{22}$\end{document}</tex-math></inline-formula> are not significantly different. Thus, we identify two significant groups in the COBRE dataset (<inline-formula><tex-math notation="LaTeX" id="ImEquation143">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$a$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math notation="LaTeX" id="ImEquation144">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$b$\end{document}</tex-math></inline-formula>).</p>
        <p>When focusing on 6000 edges or less, a minimum accuracy of 91.9% was obtained with netANOVA. However, when too many edges were considered, we could not distinguish between cases and controls. In the context of brain networks, it was already reported that feature selection is required to detect differences between groups (see <xref rid="sup2" ref-type="supplementary-material">Supplementary</xref>). When focusing on relevant edges, netANOVA was again among the top performers compared with supervised methods which usually lead to an inflated accuracy since the phenotype is used in the model. Hence, netANOVA was able to identify groups from a set of networks where nodes are labelled when proper edge selection was performed a priori.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec4">
    <title>Discussion</title>
    <p>In this article, we propose a novel workflow for statistical clustering of entire graphs, evaluate its properties (Sections <xref rid="sec3a" ref-type="sec">3.1</xref> and <xref rid="sec3b" ref-type="sec">3.2</xref>) and validate it via biological networks use cases (Section <xref rid="sec3c" ref-type="sec">3.3</xref> ). The extensive simulations show that netANOVA can reach high performance, both regarding type I error control and power, and show which option to prioritize depending on the context. The applications on real data reveal that the method achieves competitive results since netANOVA is often among the top approaches. This highlights the method’s potential in real-life situations.</p>
    <p>Most of the components in the procedure do not require a high computing time (<xref rid="sup2" ref-type="supplementary-material">Supplementary</xref>). The most influential aspects are the number of networks, their density and the distance chosen to compare the graphs. Distances for graphs with no node correspondence often require a longer processing time. Also, the computations of the first permutation-based significance assessments are the most intense due to the number of graphs compared.</p>
    <sec id="sec4a">
      <title>Novelty of the netANOVA strategy</title>
      <p>The workflow differs from generic non-parametric multivariate ANOVA [<xref rid="ref25" ref-type="bibr">25</xref>] and standard clustering methods in several respects. NetANOVA is a comprehensive graph-specific clustering workflow developed on strong statistics. It takes as input a set of networks, derives potential groups, determines the optimal number of groups without the need to set externally the number and assesses statistical significance while being completely unsupervised. Although this can be a great advantage for a user, it makes our workflow difficult to compare with baselines. Indeed, common methods often perform only one part of the analysis and there is a lack and a need for such complete approaches. For instance, a common strategy in the absence of graph labels and graph comparative analysis is to generate graph embedding, such as Graph2Vec [<xref rid="ref14" ref-type="bibr">14</xref>], and AWE [<xref rid="ref61" ref-type="bibr">61</xref>]. These are fed into downstream models, such as a <inline-formula><tex-math notation="LaTeX" id="ImEquation145">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$k$\end{document}</tex-math></inline-formula>-means clustering. However, deriving the optimal number of clusters is often decoupled from the mainstream analysis [<xref rid="ref13" ref-type="bibr">13</xref>], which is not the case in our proposed workflow. GCNs [<xref rid="ref62" ref-type="bibr">62</xref>] have also become a growing topic for supervised and unsupervised network clustering. We showed that netANOVA is able to compete and sometimes outperform GNNs approaches while bringing additional interpretability properties and being applicable to small datasets. Fraiman <italic toggle="yes">et al</italic>. [<xref rid="ref63" ref-type="bibr">63</xref>] outline another strategy. These authors examine network differences between groups with an ANOVA test explicitly developed for networks. They test whether the mean networks for predetermined groups are the same versus the alternative that at least one group has a deviating average network. Significance is derived by randomly distributing observations across groups in which no subgroup differences are to be expected. In contrast, we do not use the notion of an average network. The reason is that such a notion is not always meaningful. Also, our proposed workflow does not assume knowledge about group formation but identifies relevant partitions on the fly. The permutation procedure is also customized to handle the hierarchical structure of the workflow. In addition, the approach includes components to control for type I error, which improves the confidence in the detected groups.</p>
    </sec>
    <sec id="sec4b">
      <title>Significance assessment</title>
      <p>Several choices were made in the significance assessment procedure. The permutation-based significance assessment cannot be performed as in classical non-parametric distance-wise ANOVA [<xref rid="ref25" ref-type="bibr">25</xref>] because the clusters are derived via hierarchical clustering. Even if there are no actual groups, the clustering will create it by grouping the most similar networks, decreasing the within-group variance and increasing the across-group variance. Thus, a permutation of the graph labels to compute a <italic toggle="yes">P</italic>-value will bias towards false positives. Since the significance assessment is conditional on the two groups because of the hierarchical clustering, the same data must not be used to perform clustering and assess significant differences between groups. Multiple suggestions have been suggested to tackle this issue. In Gao <italic toggle="yes">et al</italic>. [<xref rid="ref43" ref-type="bibr">43</xref>], the authors propose a selective inference approach to test for a difference in means between two clusters. Kimes <italic toggle="yes">et al</italic>. [<xref rid="ref64" ref-type="bibr">64</xref>] developed a Monte Carlo based approach for statistical testing significance in hierarchical clustering. Suzuki and Shimodaira [<xref rid="ref65" ref-type="bibr">65</xref>] developed the R package pvclust where the hypothesis tests are based on bootstrapping procedures. Our approach also relies on randomization of the observed data, using permutations of the distances between the investigated graphs instead of the graph label. We re-apply the hierarchical clustering on these permuted sets to identify two groups and compare the obtained labels with the observed ones. Since the permutation of the distance has the additional impact that it no longer satisfies the triangular inequality, the linkage method in the hierarchical clustering is restricted.</p>
    </sec>
    <sec id="sec4c">
      <title>Userfriendliness of netANOVA</title>
      <p>There are multiple options in the workflow, such as the distance, the multiple testing correction method, the hierarchical linkage criteria, the minimum size of a group to be tested, the significance threshold, the number of permutations and the percentage of distances permuted in the distance matrix. It can therefore adapt to multiple scenarios and network types. Practical considerations on the minimum group size are presented in <xref rid="sup2" ref-type="supplementary-material">Supplementary</xref>. The customizable properties of netANOVA make it relevant to a larger range of users. For example, even though netANOVA has been developed for network analyses, it is generic in that it can accommodate any type of object. The only prerequisite is that a meaningful pairwise distances matrix can be computed.</p>
    </sec>
    <sec id="sec4d">
      <title>Future enhancements</title>
      <p>Our netANOVA workflow in the context of high-density networks can be improved. For now, edge selections may be required to select the most informative subnetworks and must be performed a priori. In our KNC application, the edge selection in COBRE networks is supervised and applied to the same dataset as the clustering (Section <xref rid="sec2f" ref-type="sec">2.6.3</xref>). Even if the clustering is then performed unsupervised, this could lead to overoptimistic performance estimates. This KNC application shows the importance of focusing on relevant interactions to improve interpretability and accuracy. Thresholding is typically adopted to cancel a percentage of the weakest connection, to turn fully connected and weighted brain networks into a useful sparse network. De Vico Fallani <italic toggle="yes">et al</italic>. [<xref rid="ref66" ref-type="bibr">66</xref>] indicate that the way to fix this threshold is still an open issue, and they introduce a criterion, the efficiency cost optimization (ECO), to select a threshold based on the optimization of the trade-off between the efficiency of a network and its wiring cost. ‘Informative’ parts can be also extracted in non-supervised ways [<xref rid="ref67" ref-type="bibr">67</xref>] for instance by looking for areas in the networks that exhibit a lot of variation between individuals, assuming that the more variation we have in ‘the input’, the more we will be able to explain with it. On the other hand, for weighted networks, even when we have a selection of nodes under consideration, the network will still be dense. Hence, some approaches based on multiple thresholds, such as filtration curves can be considered to capture a balance between hard thresholding and fully connected networks. The different thresholds reveal different structures in the graphs, and how these structures change from one threshold to another may be quite different from one network to another.</p>
      <boxed-text id="box01" position="float">
        <sec id="sec21b">
          <title>Key Points</title>
          <list list-type="bullet">
            <list-item>
              <p>The identification of homogeneous groups of networks is a common problem in system medicine. Often, the group labels are unknown, and there is no knowledge about the partitioning or the number of classes. Also, there is a need to know if the groups are significantly statistically different or not to enhance the belief in the discovered groups. We addressed these hurdles by developing an unsupervised approach based on reliable statistics that considers graphs’ specificities and derives groups of similar networks.</p>
            </list-item>
            <list-item>
              <p>Personalized screening before therapy enables improving diagnostic precision and treatment results. In network medicine, there is a trend to describe patients via individual-level biological networks, where edges are individual-specific. The tool developed in this paper paves the way towards exploiting individual networks to identify relevant disease subtypes and enhance stratified medicine.</p>
            </list-item>
            <list-item>
              <p>The method is flexible and user-friendly, making it relevant to a larger range of users. There are multiple options in the workflow, such as the distance between networks, the multiple testing correction method, the hierarchical linkage criteria, the minimum size of a group to be tested, the significance threshold or the number of permutations. It can therefore adapt to multiple scenarios and network types. In addition, even though netANOVA has been developed for network analyses, it can accommodate any type of object. The only prerequisite is that a meaningful pairwise distances matrix can be computed.</p>
            </list-item>
          </list>
        </sec>
      </boxed-text>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>netANOVA_supplementary_revised_bbad029</label>
      <media xlink:href="netanova_supplementary_revised_bbad029.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup2" position="float" content-type="local-data">
      <label>supplementary_bbad029</label>
      <media xlink:href="supplementary_bbad029.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgments</title>
    <p>We are grateful to Karsten Borgwardt and Leslie O’Bray for their advice and interesting discussions.</p>
  </ack>
  <sec sec-type="data-availability" id="sec5">
    <title>Data availability</title>
    <p>The code necessary to reproduce this article’s results and analyses is available on GitHub at <ext-link xlink:href="https://github.com/DianeDuroux/netANOVA" ext-link-type="uri">https://github.com/DianeDuroux/netANOVA</ext-link>. The MUTAG dataset is available at <ext-link xlink:href="https://networkrepository.com/Mutag.php" ext-link-type="uri">https://networkrepository.com/Mutag.php</ext-link> [<xref rid="ref68" ref-type="bibr">68</xref>]. The COBRE data was obtained from <ext-link xlink:href="http://fcon_1000.projects.nitrc.org/indi/retro/cobre.html" ext-link-type="uri">http://fcon_1000.projects.nitrc.org/indi/retro/cobre.html</ext-link>. It is available at <ext-link xlink:href="https://rdrr.io/github/jesusdaniel/graphclass/man/COBRE.data.html" ext-link-type="uri">https://rdrr.io/github/jesusdaniel/graphclass/man/COBRE.data.html</ext-link> [<xref rid="ref47" ref-type="bibr">47</xref>, <xref rid="ref49" ref-type="bibr">49</xref>].</p>
  </sec>
  <sec id="sec30e">
    <title>Funding</title>
    <p>This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 813533.</p>
  </sec>
  <notes id="bio3">
    <title>Author Biographies</title>
    <p><bold>Diane Duroux</bold> is a PhD student at the University of Liège, Belgium. Her research mainly focuses on network medicine. She is interested in detecting, understanding and reducing heterogeneity between networks to enhance stratified medicine.</p>
    <sec sec-type="author-bio" id="sec18b">
      <p><bold>Kristel Van Steen</bold> is a Professor at the University of Liège (Belgium). She also holds a part-time professorship position in Systems Medicine at the KU Leuven Department of Human Genetics. She has built up a recognized expertise in developing and applying methods to detect gene x gene and gene x environment interactions and in unifying biological and statistical evidence in genetic epidemiology.</p>
    </sec>
  </notes>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="ref1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>B</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>S</given-names></string-name>, <string-name><surname>Poleksic</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Heterogeneous multi-layered network model for omics data integration and analysis</article-title>. <source>Front Genet</source><year>2020</year>; <volume>10</volume>:<fpage>1381</fpage>.<pub-id pub-id-type="pmid">32063919</pub-id></mixed-citation>
    </ref>
    <ref id="ref2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Muzio</surname><given-names>G</given-names></string-name>, <string-name><surname>O’Bray</surname><given-names>L</given-names></string-name>, <string-name><surname>Borgwardt</surname><given-names>K</given-names></string-name></person-group>. <article-title>Biological network analysis with deep learning</article-title>. <source>Brief Bioinform</source><year>2021</year>; <volume>22</volume>(<issue>2</issue>): <fpage>1515</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">33169146</pub-id></mixed-citation>
    </ref>
    <ref id="ref3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Matsubara</surname><given-names>T</given-names></string-name>, <string-name><surname>Ochiai</surname><given-names>T</given-names></string-name>, <string-name><surname>Hayashida</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Convolutional neural network approach to lung cancer classification integrating protein interaction network and gene expression profiles</article-title>. <source>J Bioinform Comput Biol</source><year>2019</year>; <volume>17</volume>(<issue>03</issue>): <fpage>1940007</fpage>.<pub-id pub-id-type="pmid">31288636</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <label>4.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rhee</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Seo</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Kim</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2017</year>) <comment><italic toggle="yes">arXiv preprint arXiv:1711.05859</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gregorich</surname><given-names>M</given-names></string-name>, <string-name><surname>Melograna</surname><given-names>F</given-names></string-name>, <string-name><surname>Sunqvist</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Individual-specific networks for prediction modelling – a scoping review of methods</article-title>. <source>BMC Med Res Methodol</source><year>2022</year>; <volume>22</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">34991473</pub-id></mixed-citation>
    </ref>
    <ref id="ref6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Camacho</surname><given-names>DM</given-names></string-name>, <string-name><surname>Collins</surname><given-names>KM</given-names></string-name>, <string-name><surname>Powers</surname><given-names>RK</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Next-generation machine learning for biological networks</article-title>. <source>Cell</source><year>2018</year>; <volume>173</volume>(<issue>7</issue>): <fpage>1581</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">29887378</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Supekar</surname><given-names>K</given-names></string-name>, <string-name><surname>Menon</surname><given-names>V</given-names></string-name>, <string-name><surname>Rubin</surname><given-names>D</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Network analysis of intrinsic functional brain connectivity in Alzheimer’s disease</article-title>. <source>PLoS Comput Biol</source><year>2008</year>; <volume>4</volume>(<issue>6</issue>): <fpage>e1000100</fpage>.<pub-id pub-id-type="pmid">18584043</pub-id></mixed-citation>
    </ref>
    <ref id="ref8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Liang</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Disrupted small-world networks in schizophrenia</article-title>. <source>Brain</source><year>2008</year>; <volume>131</volume>(<issue>4</issue>): <fpage>945</fpage>–<lpage>61</lpage>.<pub-id pub-id-type="pmid">18299296</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tantardini</surname><given-names>M</given-names></string-name>, <string-name><surname>Ieva</surname><given-names>F</given-names></string-name>, <string-name><surname>Tajoli</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Comparing methods for comparing networks</article-title>. <source>Sci Rep</source><year>2019</year>; <volume>9</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>19</lpage>.<pub-id pub-id-type="pmid">30626917</pub-id></mixed-citation>
    </ref>
    <ref id="ref10">
      <label>10.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Borgwardt</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ghisu</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Llinares-López</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>O’Bray</surname>, <given-names>L.</given-names></string-name>, and <string-name><surname>Rieck</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2020</year>) <comment><italic toggle="yes">arXiv preprint arXiv:2011.03854</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>J</given-names></string-name>, <string-name><surname>Cui</surname><given-names>G</given-names></string-name>, <string-name><surname>Hu</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Graph neural networks: a review of methods and applications</article-title>. <source>AI Open</source><year>2020</year>; <volume>1</volume>:<fpage>57</fpage>–<lpage>81</lpage>.</mixed-citation>
    </ref>
    <ref id="ref12">
      <label>12.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>F.-Y.</given-names></string-name>, <string-name><surname>Hoffmann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Verma</surname>, <given-names>V.</given-names></string-name>, and <string-name><surname>Tang</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>) <comment><italic toggle="yes">arXiv preprint arXiv:1908.01000</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref13">
      <label>13.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bandyopadhyay</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Aggarwal</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Murty</surname>, <given-names>M. N.</given-names></string-name></person-group> (<year>2020</year>) <comment><italic toggle="yes">arXiv preprint arXiv:2006.04696</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref14">
      <label>14.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Narayanan</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chandramohan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Venkatesan</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Jaiswal</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2017</year>) <comment><italic toggle="yes">arXiv preprint arXiv:1707.05005</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Defferrard</surname><given-names>M</given-names></string-name>, <string-name><surname>Bresson</surname><given-names>X</given-names></string-name>, <string-name><surname>Vandergheynst</surname><given-names>P</given-names></string-name></person-group>. <source>Advances in neural information processing systems</source><year>2016</year>;<fpage>29</fpage>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <label>16.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kipf</surname>, <given-names>T. N.</given-names></string-name> and <string-name><surname>Welling</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2016</year>) <comment><italic toggle="yes">arXiv preprint arXiv:1609.02907</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zampieri</surname><given-names>G</given-names></string-name>, <string-name><surname>Vijayakumar</surname><given-names>S</given-names></string-name>, <string-name><surname>Yaneske</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Machine and deep learning meet genome-scale metabolic modeling</article-title>. <source>PLoS Comput Biol</source><year>2019</year>; <volume>15</volume>(<issue>7</issue>):e1007084.</mixed-citation>
    </ref>
    <ref id="ref18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miotto</surname><given-names>R</given-names></string-name>, <string-name><surname>Wang</surname><given-names>F</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Deep learning for healthcare: review, opportunities and challenges</article-title>. <source>Brief Bioinform</source><year>2018</year>; <volume>19</volume>(<issue>6</issue>): <fpage>1236</fpage>–<lpage>46</lpage>.<pub-id pub-id-type="pmid">28481991</pub-id></mixed-citation>
    </ref>
    <ref id="ref19">
      <label>19.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ribeiro</surname><given-names>MT</given-names></string-name>, <string-name><surname>Singh</surname><given-names>S</given-names></string-name>, <string-name><surname>Guestrin</surname><given-names>C</given-names></string-name></person-group>. <source>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</source>, <year>2016</year>, <fpage>1135</fpage>–<lpage>44</lpage>.</mixed-citation>
    </ref>
    <ref id="ref20">
      <label>20.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Malin</surname><given-names>BA</given-names></string-name>, <string-name><surname>Emam</surname><given-names>KE</given-names></string-name>, <string-name><surname>O’Keefe</surname><given-names>CM</given-names></string-name></person-group>. <source>Biomedical data privacy: problems, perspectives, and recent advances</source>, <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="ref21">
      <label>21.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Niepert</surname><given-names>M</given-names></string-name>, <string-name><surname>Ahmed</surname><given-names>M</given-names></string-name>, <string-name><surname>Kutzkov</surname><given-names>K</given-names></string-name></person-group>. <source>International conference on machine learning PMLR</source>, <year>2016</year>, <fpage>2014</fpage>–<lpage>23</lpage>.</mixed-citation>
    </ref>
    <ref id="ref22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>K</given-names></string-name>, <string-name><surname>Swanson</surname><given-names>K</given-names></string-name>, <string-name><surname>Jin</surname><given-names>W</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Analyzing learned molecular representations for property prediction</article-title>. <source>J Chem Inf Model</source><year>2019</year>; <volume>59</volume>(<issue>8</issue>): <fpage>3370</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="ref23">
      <label>23.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Nouranizadeh</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Matinkia</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rahmati</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Safabakhsh</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2021</year>) <comment><italic toggle="yes">arXiv preprint arXiv:2107.01410</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <label>24.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Pan</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Xu</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2022</year>) <comment><italic toggle="yes">arXiv preprint arXiv:2206.02404</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anderson</surname><given-names>MJ</given-names></string-name></person-group>. <article-title>A new method for non-parametric multivariate analysis of variance</article-title>. <source>Austral Ecol</source><year>2001</year>; <volume>26</volume>(<issue>1</issue>): <fpage>32</fpage>–<lpage>46</lpage>.</mixed-citation>
    </ref>
    <ref id="ref26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ontañón</surname><given-names>S</given-names></string-name></person-group>. <article-title>An overview of distance and similarity functions for structured data</article-title>. <source>Artif Intell Rev</source><year>2020</year>; <volume>53</volume>(<issue>7</issue>): <fpage>5309</fpage>–<lpage>51</lpage>.</mixed-citation>
    </ref>
    <ref id="ref27">
      <label>27.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Phillips</surname>, <given-names>J. M.</given-names></string-name> and <string-name><surname>Venkatasubramanian</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2011</year>) <comment><italic toggle="yes">arXiv preprint arXiv:1103.1625</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref28">
      <label>28.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bai</surname><given-names>L</given-names></string-name>, <string-name><surname>Hancock</surname><given-names>ER</given-names></string-name>, <string-name><surname>Torsello</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group><source>International Workshop on Graph-Based Representations in Pattern Recognition Springer</source>, <year>2013</year>, <fpage>121</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="ref29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kondor</surname><given-names>R</given-names></string-name>, <string-name><surname>Pan</surname><given-names>H</given-names></string-name></person-group>. <source>Advances in neural information processing systems</source><year>2016</year>;<fpage>29</fpage>.</mixed-citation>
    </ref>
    <ref id="ref30">
      <label>30.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Nikolentzos</surname><given-names>G</given-names></string-name>, <string-name><surname>Meladianos</surname><given-names>P</given-names></string-name>, <string-name><surname>Limnios</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group><article-title>In IJCAI</article-title>. <year>2018</year>;<fpage>2595</fpage>–<lpage>601</lpage>.</mixed-citation>
    </ref>
    <ref id="ref31">
      <label>31.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Shervashidze</surname><given-names>N</given-names></string-name>, <string-name><surname>Vishwanathan</surname><given-names>S</given-names></string-name>, <string-name><surname>Petri</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group><source>Artificial intelligence and statistics PMLR</source>, <year>2009</year>, <fpage>488</fpage>–<lpage>95</lpage>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <label>32.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yanardag</surname><given-names>P</given-names></string-name>, <string-name><surname>Vishwanathan</surname><given-names>S</given-names></string-name></person-group>. <source>Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</source>, <year>2015</year>, <fpage>1365</fpage>–<lpage>74</lpage>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shervashidze</surname><given-names>N</given-names></string-name>, <string-name><surname>Borgwardt</surname><given-names>K</given-names></string-name></person-group>. <source>Advances in neural information processing systems</source><year>2009</year>;<fpage>22</fpage>.</mixed-citation>
    </ref>
    <ref id="ref34">
      <label>34.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Hammond</surname><given-names>DK</given-names></string-name>, <string-name><surname>Gur</surname><given-names>Y</given-names></string-name>, <string-name><surname>Johnson</surname><given-names>CR</given-names></string-name></person-group>. <source>In 2013 IEEE Global Conference on Signal and Information Processing IEEE</source>, <year>2013</year>, <fpage>419</fpage>–<lpage>22</lpage>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sugiyama</surname><given-names>M</given-names></string-name>, <string-name><surname>Borgwardt</surname><given-names>K</given-names></string-name></person-group>. <source>Advances in neural information processing systems</source><year>2015</year>;<fpage>28</fpage>.</mixed-citation>
    </ref>
    <ref id="ref36">
      <label>36.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Koutra</surname><given-names>D</given-names></string-name>, <string-name><surname>Vogelstein</surname><given-names>JT</given-names></string-name>, <string-name><surname>Faloutsos</surname><given-names>C</given-names></string-name></person-group>. <source>Proceedings of the 2013 SIAM International Conference on Data Mining SIAM</source>, <year>2013</year>, <fpage>162</fpage>–<lpage>70</lpage>.</mixed-citation>
    </ref>
    <ref id="ref37">
      <label>37.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yip</surname><given-names>AM</given-names></string-name>, <string-name><surname>Horvath</surname><given-names>S</given-names></string-name></person-group>. <article-title>In BIOCOMP</article-title>. <year>2006</year>;<fpage>451</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="ref38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ferwerda</surname><given-names>J</given-names></string-name>, <string-name><surname>Hainmueller</surname><given-names>J</given-names></string-name>, <string-name><surname>Hazlett</surname><given-names>CJ</given-names></string-name></person-group>. <article-title>Kernel-based regularized least squares inR(KRLS) andStata(krls)</article-title>. <source>J Stat Softw</source><year>2017</year>; <volume>79</volume>(<issue>3</issue>): <fpage>1</fpage>–<lpage>26</lpage>.<pub-id pub-id-type="pmid">30220889</pub-id></mixed-citation>
    </ref>
    <ref id="ref39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamming</surname><given-names>RW</given-names></string-name></person-group>. <article-title>Error detecting and error correcting codes</article-title>. <source>Bell Syst Tech J</source><year>1950</year>; <volume>29</volume>(<issue>2</issue>): <fpage>147</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <label>40.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Borgwardt</surname><given-names>KM</given-names></string-name>, <string-name><surname>Kriegel</surname><given-names>H-P</given-names></string-name></person-group>. <source>Fifth IEEE international conference on data mining (ICDM’05) IEEE</source>, <year>2005</year>, <fpage>8</fpage>.</mixed-citation>
    </ref>
    <ref id="ref41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meinshausen</surname><given-names>N</given-names></string-name></person-group>. <article-title>Hierarchical testing of variable importance</article-title>. <source>Biometrika</source><year>2008</year>; <volume>95</volume>(<issue>2</issue>): <fpage>265</fpage>–<lpage>78</lpage>.</mixed-citation>
    </ref>
    <ref id="ref42">
      <label>42.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Girden</surname><given-names>ER</given-names></string-name></person-group>. <article-title>ANOVA: repeated measures, number 84sage</article-title>. <year>1992</year>.</mixed-citation>
    </ref>
    <ref id="ref43">
      <label>43.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gao</surname>, <given-names>L. L.</given-names></string-name>, <string-name><surname>Bien</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Witten</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2020</year>) <comment><italic toggle="yes">arXiv preprint arXiv:2012.02936</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ackermann</surname><given-names>MR</given-names></string-name>, <string-name><surname>Blömer</surname><given-names>J</given-names></string-name>, <string-name><surname>Sohler</surname><given-names>C</given-names></string-name></person-group>. <article-title>Clustering for metric and nonmetric distance measures</article-title>. <source>ACM Trans Algorithms</source><year>2010</year>; <volume>6</volume>(<issue>4</issue>): <fpage>1</fpage>–<lpage>26</lpage>.</mixed-citation>
    </ref>
    <ref id="ref45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Csardi</surname><given-names>G</given-names></string-name>, <string-name><surname>Nepusz</surname><given-names>T</given-names></string-name></person-group>. <source>Int J Complex Syst</source><year>2006</year>;<fpage>1695</fpage>.</mixed-citation>
    </ref>
    <ref id="ref46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Debnath</surname><given-names>AK</given-names></string-name>, <string-name><surname>Lopez</surname><given-names>RL</given-names></string-name>, <string-name><surname>Debnath</surname><given-names>G</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity</article-title>. <source>J Med Chem</source><year>1991</year>; <volume>34</volume>(<issue>2</issue>): <fpage>786</fpage>–<lpage>97</lpage>.<pub-id pub-id-type="pmid">1995902</pub-id></mixed-citation>
    </ref>
    <ref id="ref47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Relión</surname><given-names>JDA</given-names></string-name>, <string-name><surname>Kessler</surname><given-names>D</given-names></string-name>, <string-name><surname>Levina</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group><source>Ann Appl Stat</source><year>2019</year>; <volume>13</volume>(<issue>3</issue>): <fpage>1648</fpage>.<pub-id pub-id-type="pmid">33408802</pub-id></mixed-citation>
    </ref>
    <ref id="ref48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meng</surname><given-names>L</given-names></string-name>, <string-name><surname>Xiang</surname><given-names>J</given-names></string-name></person-group>. <article-title>Brain network analysis and classification based on convolutional neural network</article-title>. <source>Front Comput Neurosci</source><year>2018</year>; <volume>12</volume>:<fpage>95</fpage>.<pub-id pub-id-type="pmid">30618690</pub-id></mixed-citation>
    </ref>
    <ref id="ref49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aine</surname><given-names>C</given-names></string-name>, <string-name><surname>Bockholt</surname><given-names>HJ</given-names></string-name>, <string-name><surname>Bustillo</surname><given-names>JR</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Multimodal neuroimaging in schizophrenia: description and dissemination</article-title>. <source>Neuroinformatics</source><year>2017</year>; <volume>15</volume>(<issue>4</issue>): <fpage>343</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">28812221</pub-id></mixed-citation>
    </ref>
    <ref id="ref50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Power</surname><given-names>JD</given-names></string-name>, <string-name><surname>Cohen</surname><given-names>AL</given-names></string-name>, <string-name><surname>Nelson</surname><given-names>SM</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Functional network Organization of the Human Brain</article-title>. <source>Neuron</source><year>2011</year>; <volume>72</volume>(<issue>4</issue>): <fpage>665</fpage>–<lpage>78</lpage>.<pub-id pub-id-type="pmid">22099467</pub-id></mixed-citation>
    </ref>
    <ref id="ref51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wills</surname><given-names>P</given-names></string-name>, <string-name><surname>Meyer</surname><given-names>FG</given-names></string-name></person-group>. <article-title>Metrics for graph comparison: a practitioner’s guide</article-title>. <source>PloS one</source><year>2020</year>; <volume>15</volume>(<issue>2</issue>): <fpage>e0228728</fpage>.<pub-id pub-id-type="pmid">32050004</pub-id></mixed-citation>
    </ref>
    <ref id="ref52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Redcay</surname><given-names>E</given-names></string-name>, <string-name><surname>Moran</surname><given-names>JM</given-names></string-name>, <string-name><surname>Mavros</surname><given-names>PL</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Intrinsic functional network organization in high-functioning adolescents with autism spectrum disorder</article-title>. <source>Front Hum Neurosci</source><year>2013</year>; <volume>7</volume>:<fpage>573</fpage>.<pub-id pub-id-type="pmid">24062673</pub-id></mixed-citation>
    </ref>
    <ref id="ref53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hull</surname><given-names>JV</given-names></string-name>, <string-name><surname>Dokovna</surname><given-names>LB</given-names></string-name>, <string-name><surname>Jacokes</surname><given-names>ZJ</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Resting-state functional connectivity in autism Spectrum disorders: a review</article-title>. <source>Front Psych</source><year>2017</year>; <volume>7</volume>:<fpage>205</fpage>.</mixed-citation>
    </ref>
    <ref id="ref54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gupta</surname><given-names>S</given-names></string-name>, <string-name><surname>Gupta</surname><given-names>A</given-names></string-name></person-group>. <article-title>Dealing with noise problem in machine learning data-sets: a systematic review</article-title>. <source>Procedia Comput Sci</source><year>2019</year>; <volume>161</volume>:<fpage>466</fpage>–<lpage>74</lpage>.</mixed-citation>
    </ref>
    <ref id="ref55">
      <label>55.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Veličković</surname><given-names>P</given-names></string-name>, <string-name><surname>Fedus</surname><given-names>W</given-names></string-name>, <string-name><surname>Hamilton</surname><given-names>WL</given-names></string-name>, <etal>et al.</etal></person-group><article-title>arXiv preprint arXiv:1809.10341</article-title>. <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="ref56">
      <label>56.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rumelhart</surname><given-names>DE</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>GE</given-names></string-name>, <string-name><surname>Williams</surname><given-names>RJ</given-names></string-name></person-group>. <source>Learning internal representations by error propagation Technical report California Univ San Diego La Jolla Inst for Cognitive Science</source>, <year>1985</year>.</mixed-citation>
    </ref>
    <ref id="ref57">
      <label>57.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gärtner</surname><given-names>T</given-names></string-name>, <string-name><surname>Flach</surname><given-names>P</given-names></string-name>, <string-name><surname>Wrobel</surname><given-names>S</given-names></string-name></person-group>. <source>On graph kernels: Hardness results and efficient alternatives In Learning theory and kernel machines</source>.
<publisher-name>Springer</publisher-name>, <year>2003</year>, <fpage>129</fpage>–<lpage>43</lpage>.</mixed-citation>
    </ref>
    <ref id="ref58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J</given-names></string-name>, <string-name><surname>Hastie</surname><given-names>T</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>R</given-names></string-name></person-group>. <article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>J Stat Softw</source><year>2010</year>; <volume>33</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="ref59">
      <label>59.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhu</surname><given-names>J</given-names></string-name>, <string-name><surname>Rosset</surname><given-names>S</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group><source>Advances in neural information processing systems Citeseer</source>, <year>2003</year>, <comment>p. None</comment>.</mixed-citation>
    </ref>
    <ref id="ref60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vogelstein</surname><given-names>JT</given-names></string-name>, <string-name><surname>Roncal</surname><given-names>WG</given-names></string-name>, <string-name><surname>Vogelstein</surname><given-names>RJ</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Graph classification using signal-subgraphs: applications in statistical Connectomics</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source><year>2013</year>; <volume>35</volume>(<issue>7</issue>): <fpage>1539</fpage>–<lpage>51</lpage>.<pub-id pub-id-type="pmid">23681985</pub-id></mixed-citation>
    </ref>
    <ref id="ref61">
      <label>61.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ivanov</surname><given-names>S</given-names></string-name>, <string-name><surname>Burnaev</surname><given-names>E</given-names></string-name></person-group>. <source>International conference on machine learning PMLR</source>, <year>2018</year>, <fpage>2186</fpage>–<lpage>95</lpage>.</mixed-citation>
    </ref>
    <ref id="ref62">
      <label>62.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bai</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Qiao</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Marinovic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gu</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Wang</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2019</year>) <comment><italic toggle="yes">arXiv preprint arXiv:1904.01098</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fraiman</surname><given-names>D</given-names></string-name>, <string-name><surname>Fraiman</surname><given-names>R</given-names></string-name></person-group>. <article-title>An ANOVA approach for statistical comparisons of brain networks</article-title>. <source>Sci Rep</source><year>2018</year>; <volume>8</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">29311619</pub-id></mixed-citation>
    </ref>
    <ref id="ref64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kimes</surname><given-names>PK</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Neil Hayes</surname><given-names>D</given-names></string-name>, <etal>et al.</etal></person-group><article-title>Statistical significance for hierarchical clustering</article-title>. <source>Biometrics</source><year>2017</year>; <volume>73</volume>(<issue>3</issue>): <fpage>811</fpage>–<lpage>21</lpage>.<pub-id pub-id-type="pmid">28099990</pub-id></mixed-citation>
    </ref>
    <ref id="ref65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Suzuki</surname><given-names>R</given-names></string-name>, <string-name><surname>Shimodaira</surname><given-names>H</given-names></string-name></person-group>. <article-title>Pvclust: an R package for assessing the uncertainty in hierarchical clustering</article-title>. <source>Bioinformatics</source><year>2006</year>; <volume>22</volume>(<issue>12</issue>): <fpage>1540</fpage>–<lpage>2</lpage>.<pub-id pub-id-type="pmid">16595560</pub-id></mixed-citation>
    </ref>
    <ref id="ref66">
      <label>66.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Vico Fallani</surname><given-names>F</given-names></string-name>, <string-name><surname>Latora</surname><given-names>V</given-names></string-name>, <string-name><surname>Chavez</surname><given-names>M</given-names></string-name></person-group>. <article-title>A topological criterion for filtering information in complex brain networks</article-title>. <source>PLoS Comput Biol</source><year>2017</year>; <volume>13</volume>(<issue>1</issue>):e1005305.</mixed-citation>
    </ref>
    <ref id="ref67">
      <label>67.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Duroux</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Climente-Gonzáles</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Azencott</surname>, <given-names>C.-A.</given-names></string-name>, and <string-name><surname>Van Steen</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2020</year>) <comment><italic toggle="yes">bioRxiv</italic></comment>.</mixed-citation>
    </ref>
    <ref id="ref68">
      <label>68.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rossi</surname>, <given-names>R. A.</given-names></string-name> and <string-name><surname>Ahmed</surname>, <given-names>N. K.</given-names></string-name></person-group> (<year>2015</year>) <comment>In AAAI</comment></mixed-citation>
    </ref>
  </ref-list>
</back>
