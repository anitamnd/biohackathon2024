<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5977741</article-id>
    <article-id pub-id-type="publisher-id">2188</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-018-2188-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Rigorous optimisation of multilinear discriminant analysis with Tucker and PARAFAC structures</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0424-0533</contrib-id>
        <name>
          <surname>Frølich</surname>
          <given-names>Laura</given-names>
        </name>
        <address>
          <email>laura.frolich@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Andersen</surname>
          <given-names>Tobias Søren</given-names>
        </name>
        <address>
          <email>toban@dtu.dk</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mørup</surname>
          <given-names>Morten</given-names>
        </name>
        <address>
          <email>mmor@dtu.dk</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2181 8870</institution-id><institution-id institution-id-type="GRID">grid.5170.3</institution-id><institution>Department of Applied Mathematics and Computer Science, </institution><institution>Technical University of Denmark, </institution></institution-wrap>Building 324, Kongens Lyngby, 2800 Denmark </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>5</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>30</day>
      <month>5</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2018</year>
    </pub-date>
    <volume>19</volume>
    <elocation-id>197</elocation-id>
    <history>
      <date date-type="received">
        <day>9</day>
        <month>8</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>5</month>
        <year>2018</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2018</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>We propose rigorously optimised supervised feature extraction methods for multilinear data based on Multilinear Discriminant Analysis (MDA) and demonstrate their usage on Electroencephalography (EEG) and simulated data. While existing MDA methods use heuristic optimisation procedures based on an ambiguous Tucker structure, we propose a rigorous approach via optimisation on the cross-product of Stiefel manifolds. We also introduce MDA methods with the PARAFAC structure. We compare the proposed approaches to existing MDA methods and unsupervised multilinear decompositions.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We find that manifold optimisation substantially improves MDA objective functions relative to existing methods and on simulated data in general improve classification performance. However, we find similar classification performance when applied to the electroencephalography data. Furthermore, supervised approaches substantially outperform unsupervised mulitilinear methods whereas methods with the PARAFAC structure perform similarly to those with Tucker structures. Notably, despite applying the MDA procedures to raw Brain-Computer Interface data, their performances are on par with results employing ample pre-processing and they extract discriminatory patterns similar to the brain activity known to be elicited in the investigated EEG paradigms.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>The proposed usage of manifold optimisation constitutes the first rigorous and monotonous optimisation approach for MDA methods and allows for MDA with the PARAFAC structure. Our results show that MDA methods applied to raw EEG data can extract discriminatory patterns when compared to traditional unsupervised multilinear feature extraction approaches, whereas the proposed PARAFAC structured MDA models provide meaningful patterns of activity.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.1186/s12859-018-2188-0) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Multilinear discriminant analysis</kwd>
      <kwd>Electroencephalography</kwd>
      <kwd>EEG</kwd>
      <kwd>Tensor</kwd>
      <kwd>Classification</kwd>
      <kwd>Stiefel manifold</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Lundbeckfonden</institution>
        </funding-source>
        <award-id>R105-9813</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2018</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Linear Discriminant Analysis (LDA) is a widely used method for feature extraction, dimensionality reduction, and classification [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. When the number of observations is substantially larger than the number of observed variables, LDA often obtains high classification rates ([<xref ref-type="bibr" rid="CR2">2</xref>], p. 111), especially taking its relatively simple formulation and estimation into account. However, there are cases in which each observed entity is not a vector, but rather a matrix or a higher-order array (tensor), for example EEG data [<xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR6">6</xref>]. A tensor can be seen as a generalisation of a matrix such that a first-order tensor is a vector and a second-order tensor is a matrix. The term “mode” is important when describing a tensor, and the number of modes corresponds to the order of the tensor. In a matrix, i.e. a second-order tensor, the row number increases along the first mode while the column number increases over the second mode. The simplest way to handle higher order data is to vectorise it. However, this may lead to observation vectors longer than the number of observations. In such situations, LDA runs into singularity problems. Instead, the intrinsic multilinear structure can be retained and analysed.This is the aim of Multilinear Discriminant Analysis (MDA) methods which leverage the multilinear structure in order to find discriminatory subspaces. Unfortunately, current MDA approaches [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR14">14</xref>] are based on heuristic optimisation approaches that do not rigorously optimise the MDA objectives according to the imposed multilinear structure. In particular, they do not maintain the desired Tucker structure and constraints on interactions between modes throughout the optimisation, but resort to alternating heuristics.</p>
    <sec id="Sec2">
      <title>Contributions</title>
      <p>In this paper, we set out to investigate:</p>
      <p>
        <italic>What are the gains from optimising MDA rigorously over existing alternating heuristics?</italic>
      </p>
      <p>We investigate whether rigorous optimisation on the cross-product of Stiefel manifolds results in better solutions as quantified by the MDA objective function and classification performance than the existing heuristic optimization procedures. In particular, we consider trace-ratio optimisation of matrices and compare them to existing trace-ratio optimisation procedures that have been used for MDA. We note that other procedures for optimising the trace-ratio exist [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. However, none of these procedures incorporate the cross-product of Stiefel manifolds structure of matrices presently considered.</p>
      <p>
        <italic>Is the more flexible Tucker structure necessary in MDA or does the PARAFAC structure suffice?</italic>
      </p>
      <p>While the Tucker models are subject to rotational invariance, the PARAFAC structure is more constrained and may thereby provide unique representations, making interpretation of the PARAFAC model more meaningful. We consider MDA with the PARAFAC structure, which is not possible with the existing MDA optimisation methods. For completeness, we further consider the logistic regression framework proposed in [<xref ref-type="bibr" rid="CR3">3</xref>], both with the originally described PARAFAC structure and with the Tucker structure.</p>
      <p>
        <italic>How do classification performances using features extracted by MDA compare to features extracted using unsupervised multilinear decompositions?</italic>
      </p>
      <p>When extracting features via supervised methods, it is only possible to use observations whose class is known. On the other hand, unsupervised feature extraction methods learn from all available data, regardless of whether observations’ classes are known. Hence, if features extracted via unsupervised methods are as informative as those extracted in a supervised manner, then the features used for classification can be learned based on all data, making them more robust. This makes it relevant to investigate whether the use of labels during feature extraction yields substantially better classification results. To investigate the utility of MDA over existing unsupervised multilinear feature extraction approaches, we compare the performance of features extracted via MDA to the classification rates obtained when features are extracted using unsupervised multilinear decomposition approaches; PARAFAC [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>], PARAFAC2 [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>], Tucker, and Tucker2 [<xref ref-type="bibr" rid="CR21">21</xref>]. In effect, we compare to previously proposed approaches with an unsupervised step followed by a supervised step [<xref ref-type="bibr" rid="CR22">22</xref>–<xref ref-type="bibr" rid="CR30">30</xref>].</p>
    </sec>
  </sec>
  <sec id="Sec3">
    <title>Methods</title>
    <sec id="Sec4">
      <title>Multilinear Discriminant Analysis</title>
      <p>For clarity of exposition, we limit our presentation to matrix observations. Let <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\bar {\mathbf {X}}}$\end{document}</tex-math><mml:math id="M2"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq1.gif"/></alternatives></inline-formula> be the mean of all <italic>N</italic> observations and <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\bar {\mathbf {X}}}_{c}$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq2.gif"/></alternatives></inline-formula> be the mean of observations from class <italic>c</italic>. The operator <italic>v</italic><italic>e</italic><italic>c</italic>(<bold>X</bold>) vectorises the matrix <bold>X</bold> column-wise.</p>
      <p>Similar to the objective of LDA, that is, to find projections that optimally discriminate between vector observations from different classes, the objective of Multilinear Discriminant Analysis (MDA) is to find mode-specific projections that optimally separate tensor observations from different classes. Hence, MDA aims to find projection matrices that project tensor observations <inline-graphic xlink:href="12859_2018_2188_Figa_HTML.gif" id="d29e386"/> into a maximally discriminative lower dimensional representation, <inline-graphic xlink:href="12859_2018_2188_Figb_HTML.gif" id="d29e389"/> with <italic>K</italic><sub><italic>p</italic></sub>≤<italic>J</italic><sub><italic>p</italic></sub>, <italic>p</italic>=1,2,…,<italic>P</italic>. The projection matrix for mode <italic>p</italic> thus has the dimensions <italic>J</italic><sub><italic>p</italic></sub>×<italic>K</italic><sub><italic>p</italic></sub>.</p>
      <p>We generalise the within- and between-class scatter matrices from LDA to matrix observations, respectively: 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \mathbf{W} &amp;= \sum_{c=1}^{C}\sum_{n \in \mathcal{C}_{c}}vec\left(\mathbf{X}_{n}-{\bar{\mathbf{X}}}_{c}\right) vec\left(\mathbf{X}_{n}-{\bar{\mathbf{X}}}_{c}\right)^{\top}  \\ \mathbf{B} &amp;=\sum_{c=1}^{C} N_{c} vec \left({\bar{\mathbf{X}}}_{c} - {\bar{\mathbf{X}}}\right) vec\left({\bar{\mathbf{X}}}_{c} - {\bar{\mathbf{X}}}\right)^{\top}. \end{array} $$ \end{document}</tex-math><mml:math id="M6"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mi mathvariant="bold">W</mml:mi></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mtext mathvariant="italic">vec</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mtext mathvariant="italic">vec</mml:mtext><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"><mml:mi mathvariant="bold">B</mml:mi></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mtext mathvariant="italic">vec</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mtext mathvariant="italic">vec</mml:mtext><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mi>.</mml:mi><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>These can be generalised to general tensors, <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {X}_{n}$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq3.gif"/></alternatives></inline-formula>, by substituting all occurrences of the matrices <bold>X</bold><sub><italic>n</italic></sub>, <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\bar {\mathbf {X}}}_{c}$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq4.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\bar {\mathbf {X}}}$\end{document}</tex-math><mml:math id="M12"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq5.gif"/></alternatives></inline-formula> by their tensor counterparts <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {X}_{n}$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq6.gif"/></alternatives></inline-formula>, <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {\bar {X}}_{c}$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq7.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {\bar {X}}$\end{document}</tex-math><mml:math id="M18"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq8.gif"/></alternatives></inline-formula>.</p>
      <p>By substituting the projection matrix in standard LDA by the Kronecker product <bold>U</bold>=<bold>U</bold><sup>(2)</sup>⊗<bold>U</bold><sup>(1)</sup>, the objective function used in LDA becomes directly applicable to matrix observations. The Kronecker product repeats the second matrix as many times as there are elements in the first matrix, scaling each repetition by the corresponding element in the first matrix [<xref ref-type="bibr" rid="CR31">31</xref>]. A further generalisation to observations with <italic>P</italic> modes is straight-forward by defining <bold>U</bold>=<bold>U</bold><sup>(<italic>P</italic>)</sup>⊗<bold>U</bold><sup>(<italic>P</italic>−1)</sup>⊗…⊗<bold>U</bold><sup>(1)</sup>. This expression of the projection matrix <bold>U</bold> makes it clear that it lies on a cross-product manifold, with each mode-specific projection matrix corresponding to one of the manifold factors in the cross-product. These individual manifolds determine the constraints on each projection matrix. The Stiefel manifold contains the set of all matrices whose columns are mutually orthogonal, i.e. <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {U}^{(P)^{\top }}\mathbf {U}^{(P)}=\mathbf {I}$\end{document}</tex-math><mml:math id="M20"><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq9.gif"/></alternatives></inline-formula>. Hence, orthogonality constraints are enforced on all modes by optimising over a cross-product of Stiefel manifolds. Existing MDA methods [<xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR13">13</xref>] ignore this cross-product manifold structure, and most optimise mode-specific projection matrices one at a time using alternating optimisation heuristics between the modes.</p>
      <p>Once optimal projection matrices for each mode are found, an observation, <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {X}_{n}$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq10.gif"/></alternatives></inline-formula>, can be projected into the vector <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\pmb {y}_{n} = \left (\mathbf {U}^{(P)}\otimes \mathbf {U}^{(P-1)} \otimes \ldots \otimes \mathbf {U}^{(1)}\right)^{\top } vec(\mathcal {X}_{n})$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:mo>…</mml:mo><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mtext mathvariant="italic">vec</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq11.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\pmb {y}_{n} = vec(\mathcal {Y}_{n})$\end{document}</tex-math><mml:math id="M26"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">vec</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq12.gif"/></alternatives></inline-formula>. The elements in <italic>y</italic><sub><italic>n</italic></sub> may be given as input to a classification algorithm, e.g. logistic regression. In the case that we focus on, where each observation is a matrix, the projection to the lower-dimensional space can be written: <bold>Y</bold><sub><italic>n</italic></sub>=<bold>U</bold><sup>(1)⊤</sup>×<bold>X</bold><sub><italic>n</italic></sub><bold>U</bold><sup>(2)</sup>. Notice that the element in row <italic>i</italic> and column <italic>j</italic> of <bold>Y</bold><sub><italic>n</italic></sub> gives the strength of the interaction between factor (column) <italic>i</italic> from mode 1 (<bold>U</bold><sup>(1)</sup>) and factor (column) <italic>j</italic> from mode 2 (<bold>U</bold><sup>(2)</sup>). When all elements of <bold>Y</bold><sub><italic>n</italic></sub> are allowed to be non-zero, we refer to the MDA model as having the Tucker structure. It is natural to also consider a structure in which each factor only interacts with one factor in the other modes. This is enforced by only allowing diagonal elements of <bold>Y</bold><sub><italic>n</italic></sub> to be non-zero, and we refer to such MDA models as having the PARAFAC structure. In such models, the <italic>i</italic><sup><italic>t</italic><italic>h</italic></sup> columns of all projection matrices can be viewed as expressing how a discriminative pattern for classification is expressed in each mode. A consequence of an algebraic operation necessary for the existing heuristic optimisation methods is that the existing MDA methods implement the Tucker structure and do not allow for the PARAFAC structure.</p>
      <sec id="Sec5">
        <title>Heuristic solutions to multilinear discriminant analysis</title>
        <p>The methods Discriminant Analysis with TEnsor Representation (DATER) [<xref ref-type="bibr" rid="CR8">8</xref>] and Constrained Multilinear Discriminant Analysis (CMDA) [<xref ref-type="bibr" rid="CR11">11</xref>] aim to optimise the “scatter ratio” objective function [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR11">11</xref>] (see Eq. <xref rid="Equ5" ref-type="">5</xref>). Another existing MDA method [<xref ref-type="bibr" rid="CR13">13</xref>] is similar to DATER, but solves the Generalised Eigenvalue problem during optimisation instead of the standard formulation. We refer to this method as DATEReig. All three methods are based on an alternating optimisation procedure estimating each mode iteratively. When updating mode <italic>p</italic>, they project <bold>W</bold> and <bold>B</bold> unto all modes except mode <italic>p</italic>: 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \mathbf{W}_{proj}^{\tilde{p}} &amp;= \sum_{c=1}^{C}\sum_{n \in \mathcal{C}_{c}} \left(\mathbf{X}_{n}-{\bar{\mathbf{X}}}_{c}\right)_{(p)}\mathbf{U}^{\tilde{p}\top} \mathbf{U}^{\tilde{p}} \left(\mathbf{X}_{n}-{\bar{\mathbf{X}}}_{c}\right)_{(p)}^{\top}  \\ \mathbf{B}_{proj}^{\tilde{p}} &amp;=\sum_{c=1}^{C} N_{c} \left({\bar{\mathbf{X}}}_{c} - {\bar{\mathbf{X}}}\right)_{(p)}\mathbf{U}^{\tilde{p}\top} \mathbf{U}^{\tilde{p}} \left({\bar{\mathbf{X}}}_{c} - {\bar{\mathbf{X}}}\right)_{(p)}^{\top}, \end{array} $$ \end{document}</tex-math><mml:math id="M28"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:munder><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <inline-formula id="IEq13"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {U}^{\tilde {p}} = \mathbf {U}^{(P)}\otimes \ldots \mathbf {U}^{(p+1)} \otimes \mathbf {U}^{(p-1)} \ldots \mathbf {U}^{(1)}$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:mo>…</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>…</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq13.gif"/></alternatives></inline-formula>. Note, that <bold>X</bold><sub>(<italic>p</italic>)</sub> denotes matricisation along mode <italic>p</italic>.</p>
        <p>CMDA then updates <bold>U</bold><sup>(<italic>p</italic>)</sup> by setting it equal to the first <italic>K</italic><sub><italic>p</italic></sub> singular vectors of <inline-formula id="IEq14"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left (\mathbf {W}_{proj}^{\tilde {p}}\right)^{-1}\mathbf {B}_{proj}^{\tilde {p}}$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq14.gif"/></alternatives></inline-formula> which was proven in [<xref ref-type="bibr" rid="CR11">11</xref>] to result in an asymptotically bounded sequence of objective function values of the scatter-ratio objective function. Since a matrix defined by singular vectors is orthonormal, CMDA in effect uses the orthonormality constraint. DATER instead uses the first <italic>K</italic><sub><italic>p</italic></sub> generalised eigenvectors of the Generalised Eigenvalue Problem: <inline-formula id="IEq15"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {B}_{proj}^{\tilde {p}}\mathbf {U}^{(p)} = \mathbf {W}_{proj}^{\tilde {p}}\mathbf {U}^{(p)}\Lambda _{k}$\end{document}</tex-math><mml:math id="M34"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>Λ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq15.gif"/></alternatives></inline-formula>, which leads to <inline-formula id="IEq16"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {W}_{proj}^{\tilde {p}}$\end{document}</tex-math><mml:math id="M36"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq16.gif"/></alternatives></inline-formula>-orthogonality (<inline-formula id="IEq17"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {U}^{(p)^{\top }}\mathbf {W}_{proj}^{\tilde {p}}\mathbf {U}^{(p)}=\mathbf {\Lambda }$\end{document}</tex-math><mml:math id="M38"><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold">Λ</mml:mi></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq17.gif"/></alternatives></inline-formula>, where <bold>Λ</bold> is a diagonal matrix [<xref ref-type="bibr" rid="CR32">32</xref>]). Since the matrix <inline-formula id="IEq18"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {W}_{proj}^{\tilde {p}}$\end{document}</tex-math><mml:math id="M40"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq18.gif"/></alternatives></inline-formula> is different for each mode, this means that the projection matrices for the different modes are constrained differently by DATER. DATEReig instead solves the Standard Eigenvalue Problem, defined as: <inline-formula id="IEq19"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left (\mathbf {W}_{proj}^{\tilde {p}}\right)^{-1}\mathbf {B}_{proj}^{\tilde {p}}\mathbf {U}^{(p)} = \mathbf {D}\mathbf {U}^{(p)}$\end{document}</tex-math><mml:math id="M42"><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">proj</mml:mtext></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold">D</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq19.gif"/></alternatives></inline-formula>, where <bold>D</bold> is a diagonal matrix. Hence DATEReig is also subject to orthonormal constraints on the projection matrices. The algorithm Higher Order Discriminant Analysis (HODA) [<xref ref-type="bibr" rid="CR33">33</xref>] also iterates over modes in a similar fashion. HODA was seen to not be competitive on simulated data, and was not included in the comparisons on EEG data. Finally, the method Direct General Tensor Discriminant Analysis (DGTDA) [<xref ref-type="bibr" rid="CR11">11</xref>] optimises the difference between the scatter matrices. It does this by iterating over each mode once, independently for each mode without projection, setting <italic>ζ</italic> equal to the largest singular value of (<bold>W</bold><sub>(<italic>p</italic>)</sub>)<sup>−1</sup><bold>B</bold><sub>(<italic>p</italic>)</sub> when solving for mode <italic>p</italic>. The projection matrix for mode <italic>p</italic> is then set equal to the first <italic>K</italic><sub><italic>p</italic></sub> singular vectors of <bold>B</bold><sub>(<italic>p</italic>)</sub>−<italic>ζ</italic><bold>W</bold><sub>(<italic>p</italic>)</sub>.</p>
        <p>Rather than optimising a measure of class-separability, it may be advantageous to optimise classification performance directly. Bilinear Discriminant Component Analysis (BDCA) implements this idea through logistic regression with a PARAFAC structure [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. The log-likelihood for BDCA is: 
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} {}\sum_{n=1}^{N}y_{n}&amp;(w_{0}+\psi_{PARAFAC}(\mathbf{X}_{n})) \\ &amp;- \log(1+\exp(w_{0}+\psi_{PARAFAC}(\mathbf{X}_{n})), \end{aligned}  $$ \end{document}</tex-math><mml:math id="M44"><mml:mtable><mml:mtr><mml:mtd><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">PARAFAC</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>exp</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">PARAFAC</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>such that the probability that observation <bold>X</bold><sub><italic>n</italic></sub> belongs to class one is <inline-formula id="IEq20"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ \frac {1}{1+\exp \left (-\left (w_{0}+\psi _{PARAFAC}\left (\mathbf {X}_{n}\right) \right)\right)}$\end{document}</tex-math><mml:math id="M46"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>exp</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>−</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">PARAFAC</mml:mtext></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq20.gif"/></alternatives></inline-formula>, where 
<disp-formula id="Equa"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{aligned} {}\psi_{PARAFAC}(\mathbf{X}_{n})&amp;=Tr\left(\mathbf{U}^{(1)^{\top}} \mathbf{X}_{n}\mathbf{U}^{(2)}\right)\\ &amp;= \sum_{k=1}^{K_{1}}\left[\left(\mathbf{U}^{(1)}\odot \mathbf{U}^{(2)}\right)^{\top} vec(\mathbf{X}_{n})\right]_{k}. \end{aligned} $$ \end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">PARAFAC</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mtext mathvariant="italic">vec</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2018_2188_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>Thus, the number of components is the same in both modes (<italic>K</italic><sub>1</sub>=<italic>K</italic><sub>2</sub>) and there are no constraints on the projection matrices. Despite the PARAFAC type of structure, the model is not unique. For two square matrices satisfying <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\phantom {\dot {i}\!}\mathbf {Q}^{(2)}\mathbf {Q}^{(1)^{\top }}=\mathbf {I}$\end{document}</tex-math><mml:math id="M50"><mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold">I</mml:mi></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq21.gif"/></alternatives></inline-formula>, we have: 
<disp-formula id="Equb"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{aligned} {}&amp;Tr\left(\left(\mathbf{U}^{(1)}\mathbf{Q}^{(1)}\right)^{\top} \mathbf{X}_{n}\left(\mathbf{U}^{(2)}\mathbf{Q}^{(2)}\right)\right)\\&amp; =Tr\left(\mathbf{Q}^{(2)}\mathbf{Q}^{(1)^{\top}}\mathbf{U}^{(1)^{\top}} \mathbf{X}_{n}\mathbf{U}^{(2)}\right) = Tr\left(\mathbf{U}^{(1)^{\top}} \mathbf{X}_{n}\mathbf{U}^{(2)}\right), \end{aligned} $$ \end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2018_2188_Article_Equb.gif" position="anchor"/></alternatives></disp-formula> hampering model interpretation unless additional constraints are imposed.</p>
        <p>For comparison, we introduce a Tucker-structure version of the above logistic regression model, resulting in the following log-likelihood: 
<disp-formula id="Equc"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} {}\sum_{n=1}^{N}y_{n}\!\left(w_{0}+\psi_{Tucker}(\mathbf{X}_{n})\right)  \,-\, \log(1\,+\,\exp\!\left(w_{0}\,+\,\psi_{Tucker}\left(\mathbf{X}_{n}\right)\right), \end{array} $$ \end{document}</tex-math><mml:math id="M54"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">Tucker</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mspace width="0.3em"/><mml:mo>−</mml:mo><mml:mspace width="0.3em"/><mml:mo>log</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mspace width="0.3em"/><mml:mo>exp</mml:mo><mml:mspace width="0.3em"/><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">Tucker</mml:mtext></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equc.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where 
<disp-formula id="Equd"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \psi_{Tucker}(\mathbf{X}_{n}) = \sum_{k_{1}=1}^{K_{1}}\sum_{k_{2}=1}^{K_{2}} \left[\mathbf{U}^{(1)\top} \mathbf{X}_{n} \mathbf{U}^{(2)}\right]_{{k_{1}},{k_{2}}} \mathbf{V}_{k_{1},k_{2}}, \end{array} $$ \end{document}</tex-math><mml:math id="M56"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">Tucker</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equd.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>with <inline-formula id="IEq22"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {V}_{{k_{1}},{k_{2}}}=1$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mrow><mml:mi mathvariant="bold">V</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq22.gif"/></alternatives></inline-formula> for <italic>k</italic><sub>1</sub>=<italic>k</italic><sub>2</sub> to remove scaling ambiguities between the projection matrices and the matrix of interaction coefficients, <bold>V</bold>. As for BDCA, there are no constraints on <bold>U</bold><sup>(1)</sup> and <bold>U</bold><sup>(2)</sup>.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>MDA based on manifold optimisation with PARAFAC and Tucker structures</title>
      <p>The existing MDA approaches rely on heuristic optimisation procedures based on either eigenvalue or singular value decompositions. Instead, we propose to exploit the manifold optimisation in the recently released <italic>ManOpt</italic> toolbox [<xref ref-type="bibr" rid="CR35">35</xref>]. This toolbox implements rigorous optimisation of arbitrary objective functions on a variety of manifolds, as long as their gradients are known. Amongst others, the toolbox has implementations of optimisation over the Stiefel manifold, which consists of orthonormal matrices [<xref ref-type="bibr" rid="CR36">36</xref>]. By optimising over a cross-product of Stiefel manifolds, one for each mode, all projection matrices are optimised simultaneously under orthonormality constraints. Notably, other constraints can be enforced on some or all modes by changing the manifolds in the cross-product manifold.</p>
      <p>We propose four new MDA methods by optimising the scatter ratio objective [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR11">11</xref>] and three new MDA objective functions rigorously. We impose orthonormality constraints through optimisation on a cross-product of Stiefel manifolds and optimise the model parameters using the conjugate gradient method. The three new objective functions are a PARAFAC version of the scatter ratio objective and a PARAFAC and Tucker version of the trace-ratio objective [<xref ref-type="bibr" rid="CR1">1</xref>].</p>
      <p>The orthonormal projection matrices with the Tucker and PARAFAC structures are defined through the Kronecker and Khatri-Rao products, respectively: 
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \mathbf{U}_{Tucker} &amp;= \mathbf{U}^{(P)}\otimes \mathbf{U}^{(P-1)}\ldots \mathbf{U}^{(1)} \\ \mathbf{U}_{PARAFAC} &amp;= \mathbf{U}^{(P)}\odot \mathbf{U}^{(P-1)}\ldots \mathbf{U}^{(1)}. \end{array} $$ \end{document}</tex-math><mml:math id="M60"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">Tucker</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>…</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">PARAFAC</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>…</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mi>.</mml:mi><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>The Khatri-Rao product is the column-wise Kronecker product [<xref ref-type="bibr" rid="CR31">31</xref>]. The objective functions and the names we refer to the methods by are:</p>
      <p>Manifold Tucker/PARAFAC Discriminant Analysis with the scatter ratio objective (ManTDA_sr/ ManPDA_sr): 
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \frac{Tr\left(\mathbf{U}_{s}^{\top} \mathbf{B} \mathbf{U}_{s}\right)}{Tr\left(\mathbf{U}_{s}^{\top} \mathbf{W} \mathbf{U}_{s}\right)}. \end{array} $$ \end{document}</tex-math><mml:math id="M62"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">B</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Manifold Tucker/PARAFAC Discriminant Analysis with the trace of matrix ratio objective (ManTDA/ManPDA): 
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} Tr\left(\left(\mathbf{U}_{s}^{\top}\mathbf{W}\mathbf{U}_{s}\right)^{-1} \mathbf{U}_{s}^{\top} \mathbf{B} \mathbf{U}_{s}\right), \end{array} $$ \end{document}</tex-math><mml:math id="M64"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">B</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where the structure variable <italic>s</italic> is either <italic>Tucker</italic> or <italic>PARAFAC</italic>. Another proposed objective function uses determinants [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR37">37</xref>]. The solution to this objective has the same stationary points as (<xref rid="Equ6" ref-type="">6</xref>) (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Appendix A).</p>
      <p>While the scatter ratio objective (<xref rid="Equ5" ref-type="">5</xref>) maximises the ratio of energy in between-class observations relative to within-class observations, the trace of matrix ratio objective (<xref rid="Equ6" ref-type="">6</xref>) maximises the ratio of the volume spanned by between-class observations to the volume spanned by within-class observations.</p>
    </sec>
    <sec id="Sec7">
      <title>Logistic regression for classification</title>
      <p>For all methods, we use logistic regression for classification. For the MDA methods, discriminative projections are first found, and then used to project observations onto low-dimensional spaces, and the scalar values in these representations (matrices) are used for classification. For BDCA and BDCATucker, the logistic regression classification step is an integral part of the method. For the unsupervised methods (PARAFAC, PARAFAC2, Tucker, and Tucker2), data is decomposed and the estimated factors for the trial mode for each observation are used as features for logistic regression. While logistic regression is perhaps the most simple classifier, we use it to compare the degree of linear separability of classes obtained using each of the methods.</p>
    </sec>
    <sec id="Sec8">
      <title>Uniqueness of MDA</title>
      <p>MDA based on the Tucker structure is not unique when considering the objective functions given above. In fact, the projection matrix for each mode can separately be multiplied by any orthonormal matrix <bold><italic>R</italic></bold> without changing the value of the objective function, as shown in the (Additional file <xref rid="MOESM2" ref-type="media">2</xref>: Appendix B).</p>
      <p>For the PARAFAC version of MDA (for <italic>P</italic>=2) we can consider alternative representations of <bold>U</bold>=<bold>U</bold><sup>(2)</sup>⊙<bold>U</bold><sup>(1)</sup> by multiplying two orthonormal matrices <bold>R</bold><sup>(1)</sup> and <bold>R</bold><sup>(2)</sup> to form <inline-formula id="IEq23"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {\mathbf {U}}=\left (\mathbf {U}^{(2)} \mathbf {R}^{(2)}\right)\odot \left (\mathbf {U}^{(1)}\mathbf {R}^{(1)}\right)$\end{document}</tex-math><mml:math id="M66"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>⊙</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq23.gif"/></alternatives></inline-formula>. Exploiting the property [<xref ref-type="bibr" rid="CR38">38</xref>]: 
<disp-formula id="Eque"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} {}\left(\mathbf{U}^{(2)} \mathbf{R}^{(2)}\right)\odot \left(\mathbf{U}^{(1)}\mathbf{R}^{(1)}\right)=\left(\mathbf{U}^{(2)} \otimes \mathbf{U}^{(1)}\right)\left(\mathbf{R}^{(2)}\odot \mathbf{R}^{(1)}\right), \end{array} $$ \end{document}</tex-math><mml:math id="M68"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>⊙</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Eque.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>we obtain for the term used separately in the numerator and denominator of the scatter ratio objective function (<xref rid="Equ5" ref-type="">5</xref>): 
<disp-formula id="Equf"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \tilde{\mathbf{U}} \tilde{\mathbf{U}}^{\top}&amp;= \left(\mathbf{U}^{(2)} \otimes \mathbf{U}^{(1)}\right)\left(\mathbf{R}^{(2)}\odot \mathbf{R}^{(1)}\right)\\ &amp;\quad\,\,\left(\mathbf{R}^{(2)}\odot \mathbf{R}^{(1)}\right)^{\top}\left(\mathbf{U}^{(2)} \otimes \mathbf{U}^{(1)}\right)^{\top}, \end{array} $$ \end{document}</tex-math><mml:math id="M70"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mspace width="1em"/><mml:mspace width="0.3em"/><mml:mspace width="0.3em"/><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equf.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>and for the trace of matrix ratio objective (<xref rid="Equ6" ref-type="">6</xref>): 
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {\selectfont{\begin{aligned} {}Tr\left(\left(\tilde{\mathbf{U}}^{\top}\mathbf{W}\tilde{\mathbf{U}}\right)^{-1} \tilde{\mathbf{U}}^{\top} \mathbf{B} \tilde{\mathbf{U}}\right) \!=&amp; Tr\left(\left(\left(\mathbf{R}^{(2)}\!\odot\! \mathbf{R}^{(1)}\right)^{\top}\!\left(\mathbf{U}^{(2)} \!\otimes\! \mathbf{U}^{(1)}\right)^{\top}\!\mathbf{W}\right.\right.\\ &amp;\left.\left(\mathbf{U}^{(2)} \otimes \mathbf{U}^{(1)}\right)\left(\mathbf{R}^{(2)}\odot \mathbf{R}^{(1)}\right)\right)^{-1} \\ &amp; \left(\mathbf{R}^{(2)}\odot \mathbf{R}^{(1)}\right)^{\top}\left(\mathbf{U}^{(2)} \otimes \mathbf{U}^{(1)}\right)^{\top}\mathbf{B}\\ &amp; \left.\left(\mathbf{U}^{(2)} \otimes \mathbf{U}^{(1)}\right)\left(\mathbf{R}^{(2)}\odot \mathbf{R}^{(1)}\right)\right). \end{aligned}}}  $$ \end{document}</tex-math><mml:math id="M72"><mml:mtable><mml:mtr><mml:mtd><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mi mathvariant="bold">W</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mi mathvariant="bold">B</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:mfenced><mml:mspace width="0.3em"/><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mtext mathvariant="italic">Tr</mml:mtext><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.3em"/><mml:mo>⊙</mml:mo><mml:mspace width="0.3em"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.3em"/><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.3em"/><mml:mo>⊗</mml:mo><mml:mspace width="0.3em"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.3em"/><mml:mi mathvariant="bold">W</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mfenced close=")" open="" separators=""><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup><mml:mi mathvariant="bold">B</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced close=")" open="" separators=""><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊗</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2018_2188_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Due to the Khatri-Rao product structure it is no longer given that the above objective functions for <inline-formula id="IEq24"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {\mathbf {U}}$\end{document}</tex-math><mml:math id="M74"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq24.gif"/></alternatives></inline-formula> can be reduced to the objective functions based on <bold>U</bold> except for the trivial situation in which <bold>R</bold><sup>(2)</sup> and <bold>R</bold><sup>(1)</sup> are identical permutation matrices. We empirically tested the objective functions where <inline-formula id="IEq25"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathbf {R}^{(2)}=\mathbf {R}^{(1)}, \mathbf {R}^{(2)}\,=\,\mathbf {R}^{(1)^{\top }}\phantom {\dot {i}\!}$\end{document}</tex-math><mml:math id="M76"><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.3em"/><mml:mo>=</mml:mo><mml:mspace width="0.3em"/><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq25.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq26"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\phantom {\dot {i}\!}\mathbf {R}^{(2)}\neq \mathbf {R}^{(1)^{\top }}$\end{document}</tex-math><mml:math id="M78"><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>≠</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq26.gif"/></alternatives></inline-formula> and found that the random orthonormal matrices we generated indeed did not provide equivalent objective function values. Note that the case <bold>R</bold><sup>(2)</sup>=<bold>R</bold><sup>(1)</sup> would result in the same log-likelihood for <italic>BDCA</italic>.</p>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Data</title>
    <p>In data with a temporal and a spatial mode, such as EEG data, the PARAFAC structure assumes that each spatial pattern has one associated prototypical time series, and vice versa. On the other hand, the Tucker structure allows for each spatial pattern to be active according to any of the temporal patterns, and vice versa. Depending on the phenomenon under investigation and previous knowledge, one of these assumptions on interactions between spatial and temporal patterns is likely to be more probable than the other. Hence we expect tensor models to represent probable hypotheses of EEG data generation, and compared the methods on simulated data and on two EEG data sets.</p>
    <sec id="Sec10">
      <title>Simulated data</title>
      <p>We simulated one core with the Tucker structure for each of two classes. We then added noise to these cores when generating each observation. This was done by adding noise to the cores to simulate noisy realisations of the underlying cores, drawn from i.i.d. normal distributions. We then multiplied the noisy cores by simulated components to get observations in the observation space, for which we simulated observations with the dimensionionality of 10 rows and 80 columns. Finally, a non-discriminative core the same size as the discriminative core was simulated for each observation. These non-discriminative cores were multiplied by non-discriminative components, and added as structured noise consituting non-discriminative signal components shared across the two classes. The code we used for simulation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/laurafroelich/tensor_classification/tree/master/code/simulation">https://github.com/laurafroelich/tensor_classification/tree/master/code/simulation</ext-link>.</p>
    </sec>
    <sec id="Sec11">
      <title>Stekelenburg &amp; Vroomen data</title>
      <p>This data set consists of data from Experiment 2 in a set of three experiments performed and described by Stekelenburg and Vroomen [<xref ref-type="bibr" rid="CR39">39</xref>] containing data from 16 subjects. For our analyses, we used control trials (gray box shown on computer, no sound) and non-verbal auditory trials (clapping (103-107 ms) and tapping of spoon on cup (292-305 ms), gray box on screen). Trials containing values exceeding 150 <italic>μ</italic>V or lower than -150 <italic>μ</italic>V 200 ms prior to or 800 ms after stimulus onset were removed. The baseline of trials, defined as the mean of the 200 ms before stimulus onset, were subtracted. Trials were defined as lasting from stimulus onset until 500 ms after stimulus onset. These data were recorded at 512 Hz. We balanced the trials so that there were equally many from each class (2604 trials in total over all subjects and both classes). To make leave-one-subject-out cross-validation possible, we used 50 electrodes common to all subjects.</p>
    </sec>
    <sec id="Sec12">
      <title>BCI competition data</title>
      <p>This is Data Set II [<xref ref-type="bibr" rid="CR40">40</xref>] from BCI competition III [<xref ref-type="bibr" rid="CR41">41</xref>]<xref ref-type="fn" rid="Fn1">1</xref> from a P300 speller paradigm. These data were recorded from two subjects at 240 Hz from 64 electrodes and band-pass filtered during recording between 0.1-60 Hz. We extracted trials from stimulus onset until 667 ms after stimulus onset. For each subject, a training data set containing single-trial labels was available. The test data consisted of EEG recordings and the true spelled letters, but not single-trial labels.</p>
      <p>These two data sets represent different challenges. While there are many trials in the BCI data set (15,300 per subject for training), this data set is unbalanced, with one target trial for every five non-target trials. On the other hand, we balanced the Stekelenburg&amp;Vroomen data set but have far fewer trials for this data set.</p>
      <p>Since compression of the temporal mode extract the temporal signature relevant to classification, we avoid pre-processing steps such as down-sampling, band-pass filtering, and spectral decomposition.</p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Empirical analyses</title>
    <p>We compared the classification performance of logistic regression using features extracted by four existing supervised tensor methods (DATER [<xref ref-type="bibr" rid="CR8">8</xref>], DATEReig [<xref ref-type="bibr" rid="CR13">13</xref>], CMDA [<xref ref-type="bibr" rid="CR11">11</xref>], and DGTDA [<xref ref-type="bibr" rid="CR11">11</xref>]) and the proposed manifold MDA approaces (ManTDA_sr, ManPDA_sr, ManTDA, and ManPDA). Standard Linear Discriminant Analysis [<xref ref-type="bibr" rid="CR1">1</xref>] and HODA [<xref ref-type="bibr" rid="CR33">33</xref>] were also included in the simulation study. We used logistic regression to compare the performance of features extracted using these supervised methods to features extracted by the unsupervised methods Tucker, Tucker2 [<xref ref-type="bibr" rid="CR21">21</xref>], PARAFAC [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>], and PARAFAC2 [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]). For comparison, we further included BDCA [<xref ref-type="bibr" rid="CR3">3</xref>] as well as our extension of BDCA to the Tucker representation (BDCA_Tucker), both of which combine feature extraction and logistic regression in one step.</p>
    <sec id="Sec14">
      <title>Classification</title>
      <p>All classifications were performed within the logistic regression framework and the Area Under the Receiver Operating Curve (AUC) ([<xref ref-type="bibr" rid="CR2">2</xref>], Section 9.2) was used to quantify the classification performances when single-trial labels were available. To calculate the AUC, the probabilities predicted by the logistic regression models were compared to the true single-trial labels. For the BCI data, the final classification performance was evaluated as the proportion of letters spelled correctly, as in the original competition.</p>
      <p><bold>Simulated data</bold> We simulated data with three levels of signal and three components in each of two modes. The tensor decomposition methods (both the supervised and unsupervised methods) were estimated using three components.</p>
      <p><bold>Stekelenburg&amp;Vroomen data</bold> For the Stekelenburg&amp;Vroomen data, we used leave-one-subject-out cross-validation (CV) to estimate the between-subject performances of the models. Each subject was left out in turn to serve as test data for model evaluation, and the models were trained on the remaining 15 subjects. To see how well each model fits the training data, we inspected classification performances when the models classified trials from the 15 CV folds that they were trained on.</p>
      <p><bold>BCI data</bold> For each of the two subjects from the BCI data, we performed 5-fold CV using the training data containing single-trial labels. Each of the following steps were performed for each subject. We inspected the models’ performance both on training data (classifying trials form the four CV folds used for training) and on validation data (classifying the trials from the CV fold left out during training). We used the CV performance to choose the number of components for each model. Each model was then trained on the entire training data set using this number of components. The resulting model was applied to the test data for which single-trial labels were not available. In a final step, these single-trial classifications were used to predict the letters spelled, and these were compared to the correct letters. Hence, our results on the letter classification task are comparable to those from the competition since we did not use the test data to choose or train models, which was also the procedure in the competition.</p>
    </sec>
    <sec id="Sec18">
      <title>Number of components</title>
      <p>The supervised tensor classification methods find projection matrices that compress multilinear observations into lower-dimensional representations. With <italic>K</italic> components in each mode, the size of the lower-dimensional space becomes <italic>K</italic>×<italic>K</italic> for matrix observations <inline-formula id="IEq27"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left (\mathbf {U}^{(1)^{\top }}\mathbf {X}_{n}\mathbf {U}^{(2)}\right)$\end{document}</tex-math><mml:math id="M80"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq27.gif"/></alternatives></inline-formula>, as for our data sets. Hence, each observation leads to <italic>K</italic><sup>2</sup> features in the lower-dimensional discriminative space. We investigated performances for one, three, and five components for the Tucker-structure projection methods (Tucker2, CMDA, DATER, DATEReig, DGTDA, ManTDA, ManTDA_sr, and BDCA_Tucker). For the PARAFAC variants of the projection methods, only the diagonal elements are used, i.e. diag <inline-formula id="IEq28"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left (\mathbf {U}^{(1)^{\top }}\mathbf {X}_{n}\mathbf {U}^{(2)}\right)$\end{document}</tex-math><mml:math id="M82"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2018_2188_Article_IEq28.gif"/></alternatives></inline-formula>. Hence, to get the same number of features as input to logistic regression for all methods, we also included 9 and 25 components for the PARAFAC-structure methods. Likewise, the methods PARAFAC, PARAFAC2, and Tucker only yield one feature for each mode-3 component. Hence, we also estimated these models with 9 and 25 components. Note that a Tucker structured model with a core of size <italic>K</italic> in all <italic>p</italic> modes could equivalently be written as a PARAFAC structured model of rank <italic>K</italic><sup><italic>p</italic></sup>. However, a PARAFAC model with rank <italic>K</italic><sup><italic>p</italic></sup> cannot be guaranteed to have an equivalent Tucker structure representation with core of size <italic>K</italic><sup><italic>p</italic></sup>. By including the higher number of components for PARAFAC structure models, we quantify the effect of allowing the model to be at least as flexible as the Tucker representation also passing the same number of features to the classifier.</p>
    </sec>
    <sec id="Sec19">
      <title>Model implementations</title>
      <p>We used the <italic>nway</italic> [<xref ref-type="bibr" rid="CR42">42</xref>] toolbox to estimate the PARAFAC, PARAFAC2, Tucker, and Tucker2 models. These models were initialised with the best of 10 short runs, which were themselves initialised with random matrices. The BDCA methods were initialised with random normal values. The components for the trial mode (i.e., mode 3) were constrained to be orthogonal for PARAFAC and PARAFAC2. For Tucker and Tucker2, all projection matrices were constrained to be orthogonal. Due to the rotational ambiguity between the core and the projection matrices in the Tucker, the Tucker model’s fit is not impacted by these constraints. In principle, constraints are not necessary on the PARAFAC model. However, in practice, degeneracy can be an issue, which constraints preempt. Since we do not believe orthogonality constraints imposed on the spatial mode (scalp maps) or temporal mode are plausible, we chose to constrain the trial mode to be orthogonal.</p>
      <p>The existing MDA methods (DATER, DATEReig, CMDA, HODA, and DGTDA) were optimised by Matlab code that we wrote based on the pseudo-code in the papers describing these methods [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR33">33</xref>]. CMDA, DATER, HODA, and DATEReig were initialised with random orthogonal matrices while DGTDA does not need initialisation.</p>
      <p>To avoid the log-likelihood from overflowing in the first iteration for the BDCA methods, the standard deviation of the initial random values for the Stekelenburg&amp;Vroomen data was set to 0.01 while a lower value, 10<sup>−5</sup>, was necessary to avoid overflow for the BCI data.</p>
      <p>Our proposed MDA methods were optimised using the <italic>ManOpt</italic> [<xref ref-type="bibr" rid="CR35">35</xref>] toolbox for Matlab. The models were initialised both with random orthonormal matrices and with projection matrices obtained from short runs of CMDA. Results from the two initialisation methods were similar, so we only show the results from random initialisation.</p>
      <p>It was originally recommended to use the Damped Newton procedure in the <italic>immoptibox</italic> [<xref ref-type="bibr" rid="CR43">43</xref>] to optimise the BDCA log-likelihood objective [<xref ref-type="bibr" rid="CR3">3</xref>]. We optimised BDCA using both the suggested Damped Newton method and the Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimisation, also available in the <italic>immoptibox</italic>. These two optimisation methods achieved very similar classification rates. The BFGS method was slightly faster despite it only requiring gradients. We therefore used BFGS optimisation to optimise the BDCA methods.</p>
      <p>All iterative methods were started three times and run for up to 5000 iterations or until convergence for the Stekelenburg&amp;Vroomen data and for 1000 iterations for the BCI data. The best of the three solutions was chosen for further analysis to minimise the risk of analysing solutions from local minima. The convergence criteria used for CMDA, DATER and DATEReig were those originally proposed for CMDA and DATER [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR11">11</xref>].</p>
    </sec>
    <sec id="Sec20">
      <title>Visualisation</title>
      <p>The projection matrices found by the supervised methods act as dimension-reducing filters that maximise the class-discriminative information in the filtered data. However, such filters are not suited for visualisation for model interpretation purposes [<xref ref-type="bibr" rid="CR44">44</xref>]. Instead, the interesting spatial properties of the estimated sources consist of how their activity is expressed on the scalp. This can be derived from the filters by pre-multiplying the data covariance matrix of electrodes onto the filter (projection) matrix if sources are assumed uncorrelated. We extrapolated this visualisation approach established for the spatial domain to the temporal domain by pre-multiplying the data covariance of temporal samples onto the temporal filter matrices to visualise the time courses of the sources. Since the MDA models with Tucker structure and BDCA are rotationally invariant, they do not have straight-forward interpretations, except in the one-component case.</p>
      <p>On the other hand, each column in a projection matrix can only interact with one column from projection matrices for the other modes when using the PARAFAC structure. Also, we empirically observed that the PARAFAC formulations of MDA objectives were not invariant to rotations via random orthogonal matrices, making their interpretation more intuitive. For these reasons, we limit visualisations to one-component Tucker models and PARAFAC-structure MDA models.</p>
    </sec>
  </sec>
  <sec id="Sec21" sec-type="results">
    <title>Results</title>
    <sec id="Sec22">
      <title>Classification performance on simulated data</title>
      <p>Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the classification performances of the tensor decomposition methods on simulated data with the medium level of signal strength that we simulated. The figure shows the mean AUC plus/minus the standard deviation of the mean across 25 simulations. As on the EEG data, we observe low performances from the unsupervised methods. While standard LDA outperforms the unsupervised tensor methods, the supervised tensor decomposition methods (except HODA and DGTDA) obtain higher AUCs than LDA. As expected, all methods improve with more training observations. Both DATER and DATEReig outperform LDA. CMDA is comparable in performance to the manifold methods for most numbers of training observations, but there seems to be a trend that the manifold method ManTDA is better able to leverage the addition of more training observations for large numbers of training observations. Additional plots are given in Additional file <xref rid="MOESM3" ref-type="media">3</xref>: Appendix C, for each of three noise levels.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Performance on simulated data. Classification performance obtained through the tensor decomposition methods on simulated data with the medium level of simulated signal as a function of the number of training observations. Vertical lines denote plus/minus the standard deviation of the mean of 25 simulations</p></caption><graphic xlink:href="12859_2018_2188_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec23">
      <title>Objective function values on EEG data</title>
      <p>Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the objective function values obtained by CMDA, DATER, DATEReig, and our proposed manifold optimisation of the scatter-ratio objective with Tucker structure, the objective function these four methods aim to optimise. The values obtained for the scatter-ratio objective are shown as full lines. Objective function values for the trace of matrix ratio objective are also shown since the heuristic methods, during optimisation, use this objective as an approximation to the scatter-ratio objective. CMDA, DATEReig, and the manifold methods share the same constraints on the projection matrices and are hence directly comparable. Each iteration for DATER, DATEReig, and CMDA corresponds to an update of the projection matrix for one of the modes. Each iteration for the manifold optimisation corresponds to one update in all modes since all modes are optimised simultaneously in this approach.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Objective function values. Objective function values for one, three, and five components. Scatter ratio objective function (<xref rid="Equ5" ref-type="">5</xref>) values are shown as full lines while the matrix ratio objective (<xref rid="Equ6" ref-type="">6</xref>) is shown as dashed lines for three random initialisations. <italic>Top</italic>: Stekelenburg&amp;Vroomen data for the CV fold with subject 5 left out. <italic>Bottom</italic>: subject B from the BCI data. Note the log scale of the y-axis in the upper row and the linear scale in the bottom row</p></caption><graphic xlink:href="12859_2018_2188_Fig2_HTML" id="MO2"/></fig></p>
      <p>The top of Fig. <xref rid="Fig2" ref-type="fig">2</xref> shows a randomly chosen case of the optimisation for the CV fold with subject 5 left out in the Stekelenburg&amp;Vroomen data. All optimisation runs were very similar to the example shown here. The bottom part of the figure shows the optimisation for CV fold number 1 for subject B. This is similar to the other CV folds, including those for subject A.</p>
      <p>One observation from this figure is that the convergence of CMDA and DATEReig is not monotone, increasing rapidly to begin with, followed by a decline before stabilising. The alternation between optimising the two modes is seen as a sawtooth pattern of objective function values rising and falling between iterations in the initial part of the optimisation. Although more difficult to see, DATER also exhibits these characteristics. This shows that CMDA, DATEReig, and DATER do not optimise the scatter-ratio objective consistently.</p>
      <p>Secondly, we observe that the manifold methods obtain the highest values. That is, the dashed line for ManTDA dominates the other dashed lines, while the full line for ManTDA_sr dominates the other full lines, from a certain number of iterations and onwards. DATEReig and CMDA reach the same value of the matrix ratio objective, with DATER also reaching a similar value. Since the matrix ratio is a simple, but inexact, approximation to the scatter ratio objective, it is reassuring that the iterative methods reach similar values for the inexact problem. However, their differences on the exact scatter ratio objective reveal that the inexact approximation combined with iterating over modes to optimise does not suffice to obtain the best solution to the exact problem.</p>
    </sec>
    <sec id="Sec24">
      <title>Cross-validated classification performance on EEG data</title>
      <p>Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the AUC when evaluating on training data for Stekelenburg&amp;Vroomen data (top) and for the two BCI subjects (A in the middle and B at the bottom). When evaluating on training data, all methods improved with more components, as expected.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Performances on training data. Testing on training data (data from each CV fold that was also used to train on). <italic>Top</italic>: Stekelenburg&amp;Vroomen data. <italic>Middle</italic>: BCI data, subjet A. <italic>Bottom</italic>: BCI data, subjet B. The methods are grouped by type such that first four methods plotted are the unsupervised decomposition methods, followed by the four heuristic supervised decomposition methods. The next four methods are the supervised manifold methods, which are followed by the two methods performing decomposition and classification in one step. Finally, the six methods that produce fewer features for classification are plotted again with 9 and 25 components</p></caption><graphic xlink:href="12859_2018_2188_Fig3_HTML" id="MO3"/></fig></p>
      <p>On the Stekelenburg&amp;Vroomen training data, ManTDA, BDCA and BDCA_Tucker outperform the other methods, even obtaining perfect classification performances (AUC value of one) whereas the other MDA methods, except DGTDA, are very close to these best performances. The PARAFAC-structure and Tucker-structure formulations of the objective functions have very similar performances but the PARAFAC-structure versions of MDA do not improve to perfection, as BDCA does for the largest component numbers. The performances are nearly identical, and low, for the unsupervised PARAFAC and Tucker models, even when allowed a large number of components. The Tucker2 method, which projects each trial into a lower dimensional space analogously to the MDA methods, performs substantially better than the other unsupervised methods, even outperforming DGTDA.</p>
      <p>On the BCI training data, the two BDCA methods also outperform ManTDA. Here, the performance of BDCA is substantially higher than all other methods. With 25 components, BDCA again obtains AUC values of one, for both BCI subjects. On the BCI data, we observe some performance differences between ManPDA and ManTDA, with ManTDA performing best. For subject A, Tucker2 again outperforms DGTDA while it is on the same (low) level as PARAFAC and Tucker for subject B.</p>
      <p>Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the classification performances obtained when evaluating on test data. Again, the results from the Stekelenburg&amp;Vroomen data are shown in the top of the figure, with BCI subjects A and B in the middle and bottom, respectively.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Performances on test data. Testing on validation data (data left out from each CV fold). <italic>Top</italic>: Stekelenburg&amp;Vroomen data. <italic>Middle</italic>: BCI data, subjet A. <italic>Bottom</italic>: BCI data, subjet B</p></caption><graphic xlink:href="12859_2018_2188_Fig4_HTML" id="MO4"/></fig></p>
      <p>When evaluating on Stekelenburg&amp;Vroomen test data, ManTDA and the BDCA methods perform worse than the other supervised methods, especially for high component numbers. With five components, they and DGTDA are even outperformed by Tucker2. The other MDA methods still obtain the highest performances, with Tucker, PARAFAC, and PARAFAC2 only obtaining low AUCs until 25 components. At this point, Tucker and PARAFAC approach the MDA performances.</p>
      <p>On the BCI data, ManTDA and the BDCA methods perform at the same level as the MDA methods while the unsupervised feature extraction methods do not reach this level, with any component number. With four and five components (also with three for subject A), DGTDA is somewhat better than the unsupervised methods without coming close to the other supervised methods. While the performances of CMDA, DATER, and ManTDA are slightly better, all the MDA methods perform similarly.</p>
    </sec>
    <sec id="Sec25">
      <title>BCI data letter classification performance</title>
      <p>Table <xref rid="Tab1" ref-type="table">1</xref> shows average classification rates of letters across the two subjects in the BCI data. The first column gives the classification rates when each row/column was flashed 15 times to spell a character. The second column shows the results for five flashes. The average classification rates obtained by the five teams with highest performances in the competition are also shown, reproduced from the competition website<xref ref-type="fn" rid="Fn2">2</xref>. DATEReig obtains the best performance, closely followed by CMDA, DATER and ManTDA, with only small differences between the PARAFAC and Tucker structures of the MDA methods.</p>
    </sec>
    <sec id="Sec26">
      <title>Model interpretation</title>
      <p>We now show the temporal and spatial patterns of several of the fitted models. The components were derived and arranged in no particular order. Since the performances of the unsupervised methods are very low, we focus on visualising the supervised methods.</p>
      <p>Figure <xref rid="Fig5" ref-type="fig">5</xref> shows the scalp maps and corresponding temporal signatures extracted by one-component models of the Stekelenburg&amp;Vroomen data. With only one component, the PARAFAC and Tucker versions of each objective function are identical, making BDCA and BDCA_Tucker equivalent. Also, the trace of matrix ratio is the same as the scatter ratio in this case, making all the methods optimised on manifolds equivalent. We included one-component models from each of the equivalent models in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. Except for different scaling in DATER, the components fitted by CMDA, DATER, and ManTDA are identical. This is reflected in the nearly identical logistic regression coefficients (shown above the spatial patterns) found for CMDA and ManTDA. The magnitude of the temporal pattern found by DATER is lower than that in CMDA and ManTDA, which is accounted for by the higher logistic regression coefficient. Since the BDCA model uses the projection into a lower dimensional space directly in the logistic regression model, no coefficient is displayed for this model. Although the patterns found by BDCA are not identical to those found by the other methods, they are very similar. The temporal pattern of the component is very similar to the difference wave found by Stekelenburg and Vroomen between the two conditions [<xref ref-type="bibr" rid="CR39">39</xref>]. The centrally located scalp map is also in good accordance with their analysis of the central Cz electrode [<xref ref-type="bibr" rid="CR39">39</xref>]. The logistic regression model was trained to predict probabilities for the auditory class. All shown components are well in line with this training since the positive logistic regression coefficients means that centrally located scalp activity with temporal activity like the difference wave in [<xref ref-type="bibr" rid="CR39">39</xref>] indicates that a trial is from the auditory class.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Interpretation of components from the Stekelenburg&amp;Vroomen experiment. Spatial and temporal patterns corresponding to the extracted spatial and temporal filters found from the training data without subject 5 in the Stekelenburg&amp;Vroomen data by the following (from top to bottom) one component models: CMDA, DATER, ManTDA, BDCA. Logistic regression coefficients are shown above the spatial patterns. <bold>a</bold> CMDA <bold>b</bold> DATER <bold>c</bold> ManTDA <bold>d</bold> BDCA</p></caption><graphic xlink:href="12859_2018_2188_Fig5_HTML" id="MO5"/></fig></p>
      <p>Figure <xref rid="Fig6" ref-type="fig">6</xref> shows the spatial and temporal patterns corresponding to the extracted filters for supervised MDA PARAFAC-structure models with three components, trained on four of the five CV folds from subject A’s training data. All models extract a waveform similar to the P300 ERP, which is the theoretical foundation of P300 BCI systems. The three components extracted by ManPDA look almost identical, with the characteristic P300 temporal signature and a centrally focused scalp pattern. The component on the right for ManPDA_sr has the same characteristics as the ManPDA components. The logistic regression model for the BCI data was trained to predict the probability of the target class, i.e. the class that should contain the P300 response. The estimated components’ logistic regression coefficients are in line with this since the estimates show that central scalp activity exhibiting the P300-like waveform increases the probability of an observation being from the target class.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Interpretation of components from the BCI data. Spatial and temporal patterns corresponding to the extracted filters from the PARAFAC models ManPDA and ManPDA_sr, trained on four of five CV folds from subject A’s BCI data. Fitted logistic regression coefficients are shown above the spatial patterns. <bold>a</bold> ManPDA <bold>b</bold> ManPDA_sr</p></caption><graphic xlink:href="12859_2018_2188_Fig6_HTML" id="MO6"/></fig></p>
      <p>The two components shown on the left for ManPDA_sr are difficult to interpret since their spatial patterns are not smooth and their temporal patterns are very high frequent. These two components might represent random noise in the data. Hence, we would not expect these components to contribute to classification performance. This is aligned with the observation from Fig. <xref rid="Fig3" ref-type="fig">3</xref> that classification performance is not improved substantially for higher component numbers.</p>
    </sec>
  </sec>
  <sec id="Sec27" sec-type="discussion">
    <title>Discussion</title>
    <p>We saw that supervising the feature extraction step resulted in better classification rates. When feature extraction is not supervised, some directions of the data space that contain class-discriminative information but have low variance, and so explain only a small data proportion, may be lost since unsupervised feature extraction focuses on data directions that best explain data variance. Even when including a large number of components, the unsupervised methods did not obtain competitive classification performances, emphasising the need for supervised feature extraction methods.</p>
    <p>Although the manifold optimisation approach obtained substantially higher objective function values than existing heuristic optimisation provides, we did not observe large classification performance differences between the supervised methods on the EEG data. With the same number of components, the Tucker and PARAFAC versions of the methods performed similarly. On the simulated data, though, it was evident that the manifold optimisation approach delivered better classification performance, both for the PARAFAC and Tucker objective functions. Keeping in mind that the simulated data was endowed with a Tucker structure, this seems to indicate that the PARAFAC structured models are robust to deviations from the PARAFAC structure assumption. Both CMDA and the PARAFAC structured manifold methods outperformed the Tucker structured trace-of-ratio manifold method (ManTDA) when low numbers of training observations were used. However, it seems that ManTDA was better able to learn from the available data, overfitting when too few observations were available, but performing better with sufficient training data, whereas the other methods’ performances plateaued and were not able to further improve. Notably, the PARAFAC-versions proposed for MDA are also attractive due to their interpretability.</p>
    <p>Combining feature extraction and learning the classifier in one step by BDCA led to the best performance on training data. However, as was also a problem for the ManTDA method, the performance dropped on Stekelenburg&amp;Vroomen test data, especially with many components. This pattern is a sign of overfitting. On BCI data, the performance of ManTDA and BDCA did not drop on the test data as these data sets had substantially more trials.</p>
    <p>As was originally recommended, regularising the BDCA methods would probably improve their performance [<xref ref-type="bibr" rid="CR3">3</xref>]. Regularisation could be done in an unsupervised manner by using a Tucker2 compression of the temporal and spatial modes before applying the supervised methods. The regularisation originally recommended was a smoothing function [<xref ref-type="bibr" rid="CR3">3</xref>], making the estimated spatial and temporal filters smoother. Alternatively, such a smoothing constraint could be applied to the patterns to make them resemble expressions of neural activity more. Other regularisation options are also possible. For example, L1 or L2 regularisation could be incorporated in the logistic regression model in the BDCA methods.</p>
    <p>Likewise, regularisation of the manifold-optimised MDA methods would also protect against the problem with overfitting, that became apparent with the Stekelenburg&amp;Vroomen data. One explanation for the high performance of the existing MDA methods could be that their sub-optimal optimisation induces regularisation, albeit uncontrolled. By optimising over manifolds rigorously, the manner and degree of regularisation can potentially be controlled systematically.</p>
    <p>For our manifold optimisation, we used conjugate gradient as provided in the <italic>ManOpt</italic> toolbox [<xref ref-type="bibr" rid="CR35">35</xref>]. However, more efficient optimisation using newer, more advanced manifold optimisation methods [<xref ref-type="bibr" rid="CR45">45</xref>, <xref ref-type="bibr" rid="CR46">46</xref>] might be beneficial. In order to minimise the amount of pre-processing, we used the raw EEG trial data as input to the compared methods. In view of the lack of pre-processing, the high classification rates are surprising and indicates that the tensor methods are able to extract the temporal, as well as spatial, characteristics of data. Hence, these methods might also be useful for extracting neural phenomena without prior knowledge.</p>
  </sec>
  <sec id="Sec28" sec-type="conclusion">
    <title>Conclusion</title>
    <p>We set out to investigate whether the performance of Multilinear Discriminant Analysis (MDA) methods could be improved through rigorous optimisation instead of existing optimisation heuristics in the context of single-trial EEG classification. We found that rigorous optimisation does obtain substantially higher objective function values than the existing optimisation procedures. This, however, did not lead to better classification performance on EEG data, while performance improvements were seen on simulated data. Additionally, we compared PARAFAC- and Tucker formulations of objective functions and did not observe large differences between these formulations. However, for model interpretation, we found that the proposed PARAFAC MDA models, which the existing MDA methods do not allow for, are attractive. Finally, we investigate whether it is necessary to use supervised methods when searching for subspaces suitable for classification. Our results showed that supervised feature extraction methods perform substantially better than unsupervised methods, indicating that it is advisable to use observation labels when performing feature extraction. For interpretability, PARAFAC formulations are preferred. Using these methods, we were able to extract spatial and temporal patterns resembling spatial activation and waveforms known to be characteristic of the investigated paradigms. This was achieved without usual pre-processing steps such as filtering. That is, the MDA methods were directed solely by data and the labels of trials when extracting these patterns. Our Matlab implementations are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/laurafroelich/tensor_classification">https://github.com/laurafroelich/tensor_classification</ext-link>.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Additional files</title>
    <sec id="Sec29">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2018_2188_MOESM1_ESM.pdf">
            <label>Additional file 1</label>
            <caption>
              <p>Appendix A - Stationary points. Proof that the stationary points of the trace of matrix ratio and ratio of deteriminants objectives are the same. (PDF 92.7 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
      <p>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="12859_2018_2188_MOESM2_ESM.pdf">
            <label>Additional file 2</label>
            <caption>
              <p>Appendix B - non-uniqueness of the tucker structure. Proof that the Tucker structure leads to non-unique solutions. (PDF 106 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
      <p>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="12859_2018_2188_MOESM3_ESM.pdf">
            <label>Additional file 3</label>
            <caption>
              <p>Appendix C - figures from simulation study. Figures comparing the performances of the different methods on data simulated with three different amounts of noise. (PDF 3318 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://www.bbci.de/competition/iii/">http://www.bbci.de/competition/iii/</ext-link>
      </p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p>
        <ext-link ext-link-type="uri" xlink:href="http://www.bbci.de/competition/iii/results/index.html">http://www.bbci.de/competition/iii/results/index.html</ext-link>
        <table-wrap id="Tab1">
          <label>Table 1</label>
          <caption>
            <p>Mean letter classification rates for data set II from BCI competition III from the compared methods (top) and best five competition participants (bottom), copied from <ext-link ext-link-type="uri" xlink:href="http://www.bbci.de/competition/iii/results/index.html">http://www.bbci.de/competition/iii/results/ index.html</ext-link></p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left"/>
                <th align="left">15 flashes</th>
                <th align="left">5 flashes</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">DATEReig</td>
                <td align="left">0.930</td>
                <td align="left">0.695</td>
              </tr>
              <tr>
                <td align="left">CMDA</td>
                <td align="left">0.925</td>
                <td align="left">0.695</td>
              </tr>
              <tr>
                <td align="left">DATER</td>
                <td align="left">0.925</td>
                <td align="left">0.670</td>
              </tr>
              <tr>
                <td align="left">ManTDA</td>
                <td align="left">0.915</td>
                <td align="left">0.660</td>
              </tr>
              <tr>
                <td align="left">ManPDA</td>
                <td align="left">0.910</td>
                <td align="left">0.645</td>
              </tr>
              <tr>
                <td align="left">BDCA</td>
                <td align="left">0.895</td>
                <td align="left">0.645</td>
              </tr>
              <tr>
                <td align="left">ManPDA_sr</td>
                <td align="left">0.890</td>
                <td align="left">0.555</td>
              </tr>
              <tr>
                <td align="left">BDCATucker</td>
                <td align="left">0.890</td>
                <td align="left">0.655</td>
              </tr>
              <tr>
                <td align="left">ManTDA_sr</td>
                <td align="left">0.880</td>
                <td align="left">0.555</td>
              </tr>
              <tr>
                <td align="left">Tucker2</td>
                <td align="left">0.605</td>
                <td align="left">0.260</td>
              </tr>
              <tr>
                <td align="left">DGTDA</td>
                <td align="left">0.595</td>
                <td align="left">0.320</td>
              </tr>
              <tr>
                <td align="left">Parafac</td>
                <td align="left">0.315</td>
                <td align="left">0.105</td>
              </tr>
              <tr>
                <td align="left">Tucker</td>
                <td align="left">0.300</td>
                <td align="left">0.090</td>
              </tr>
              <tr>
                <td align="left">Parafac2</td>
                <td align="left">0.025</td>
                <td align="left">0.005</td>
              </tr>
              <tr>
                <td align="left">Alain Rakotomamonjy</td>
                <td align="left">0.965</td>
                <td align="left">0.735</td>
              </tr>
              <tr>
                <td align="left">Li Yandong</td>
                <td align="left">0.905</td>
                <td align="left">0.550</td>
              </tr>
              <tr>
                <td align="left">Zhou Zongtan</td>
                <td align="left">0.900</td>
                <td align="left">0.595</td>
              </tr>
              <tr>
                <td align="left">Ulrich Hoffmann</td>
                <td align="left">0.895</td>
                <td align="left">0.530</td>
              </tr>
              <tr>
                <td align="left">Lin Zhonglin</td>
                <td align="left">0.875</td>
                <td align="left">0.575</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
    </fn>
    <fn>
      <p>
        <bold>Electronic supplementary material</bold>
      </p>
      <p>The online version of this article (10.1186/s12859-018-2188-0) contains supplementary material, which is available to authorized users.</p>
    </fn>
  </fn-group>
  <ack>
    <p>The authors would like to thank Jeroen J. Stekelenburg and Jean Vroomen for kindly letting us analyse their data [<xref ref-type="bibr" rid="CR39">39</xref>].</p>
    <sec id="d29e4678">
      <title>Funding</title>
      <p>Morten Mørup was supported by the Lundbeck Foundation (grant nr. R105-9813).</p>
    </sec>
    <sec id="d29e4683">
      <title>Availability of data and materials</title>
      <p>The BCI Competition data is available as Data Set II from BCI competition III: <ext-link ext-link-type="uri" xlink:href="http://www.bbci.de/competition/iii/">http://www.bbci.de/competition/iii/</ext-link>. The Stekelenburg&amp;Vroomen data will be available upon reasonable request to the authors. Code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/laurafroelich/tensor_classification">https://github.com/laurafroelich/tensor_classification</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>LF, MM, and TA wrote the manuscript. LF implemented the models and executed the experiments with assistance from MM. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="d29e4704">
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e4709">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="d29e4714">
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bishop</surname>
            <given-names>CM</given-names>
          </name>
        </person-group>
        <source>Pattern Recognition and Machine Learning.</source>
        <year>2006</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Friedman</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</source>
        <year>2011</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer-Verlag</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dyrholm</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Christoforou</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Parra</surname>
            <given-names>LC</given-names>
          </name>
        </person-group>
        <article-title>Bilinear discriminant component analysis</article-title>
        <source>J Mach Learn Res</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>1097</fpage>
        <lpage>111</lpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Higashi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Rutkowski</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Tanaka</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tanaka</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Multilinear discriminant analysis with subspace constraints for single-trial classification of event-related potentials</article-title>
        <source>IEEE J Selected Topics Signal Process</source>
        <year>2016</year>
        <volume>10</volume>
        <issue>7</issue>
        <fpage>1295</fpage>
        <lpage>305</lpage>
        <pub-id pub-id-type="doi">10.1109/JSTSP.2016.2599297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Onishi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Phan</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Matsuoka</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Cichocki</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Tensor classification for p300-based brain computer interface</article-title>
        <source>Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference On</source>
        <year>2012</year>
        <publisher-loc>Kyoto</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <mixed-citation publication-type="other">Liu Y, Zhao Q, Zhan L. Uncorrelated multiway discriminant analysis for motor imagery eeg classification. Int J Neural Syst. 2015;25(4).</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>2d-lda: A statistical linear discriminant analysis for image matrix</article-title>
        <source>Pattern Recognit Lett</source>
        <year>2005</year>
        <volume>26</volume>
        <issue>5</issue>
        <fpage>527</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patrec.2004.09.007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H-J</given-names>
          </name>
        </person-group>
        <article-title>Discriminant analysis with tensor representation</article-title>
        <source>Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference On</source>
        <year>2005</year>
        <publisher-loc>San Diego</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Maybank</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Tensor rank one discriminant analysis, a convergent method for discriminative multilinear subspace selection</article-title>
        <source>Neurocomputing</source>
        <year>2008</year>
        <volume>71</volume>
        <issue>10</issue>
        <fpage>1866</fpage>
        <lpage>82</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2007.08.036</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Tien</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Tensor regression based on linked multiway parameter analysis</article-title>
        <source>Data Mining (ICDM), 2014 IEEE International Conference On</source>
        <year>2014</year>
        <publisher-loc>Shenzhen</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Schonfeld</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Multilinear discriminant analysis for higher-order tensor data classification</article-title>
        <source>Pattern Anal Mach Intell IEEE Trans</source>
        <year>2014</year>
        <volume>36</volume>
        <issue>12</issue>
        <fpage>2524</fpage>
        <lpage>37</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2014.2342214</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Cardiology knowledge free ecg feature extraction using generalized tensor rank one discriminant analysis</article-title>
        <source>EURASIP J Adv Signal Process</source>
        <year>2014</year>
        <volume>2014</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1186/1687-6180-2014-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Visani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jolion</surname>
            <given-names>J-M</given-names>
          </name>
        </person-group>
        <article-title>Normalized radial basis function networks and bilinear discriminant analysis for face recognition</article-title>
        <source>Advanced Video and Signal Based Surveillance, 2005. AVSS 2005. IEEE Conference On</source>
        <year>2005</year>
        <publisher-loc>Como</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Phan</surname>
            <given-names>AH</given-names>
          </name>
          <name>
            <surname>Cichocki</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vu-Dinh</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>A tensorial approach to single trial recognition for brain computer interface</article-title>
        <source>Advanced Technologies for Communications (ATC), 2010 International Conference On</source>
        <year>2010</year>
        <publisher-loc>Ho Chi Minh City</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ngo</surname>
            <given-names>TT</given-names>
          </name>
          <name>
            <surname>Bellalij</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Saad</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>The trace ratio optimization problem for dimensionality reduction</article-title>
        <source>SIAM J Matrix Anal Appl</source>
        <year>2010</year>
        <volume>31</volume>
        <issue>5</issue>
        <fpage>2950</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1137/090776603</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ngo</surname>
            <given-names>TT</given-names>
          </name>
          <name>
            <surname>Bellalij</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Saad</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>The trace ratio optimization problem</article-title>
        <source>SIAM Rev</source>
        <year>2012</year>
        <volume>54</volume>
        <issue>3</issue>
        <fpage>545</fpage>
        <lpage>69</lpage>
        <pub-id pub-id-type="doi">10.1137/120864799</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carroll</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>J-J</given-names>
          </name>
        </person-group>
        <article-title>Analysis of individual differences in multidimensional scaling via an n-way generalization of eckart-young decomposition</article-title>
        <source>Psychometrika</source>
        <year>1970</year>
        <volume>35</volume>
        <issue>3</issue>
        <fpage>283</fpage>
        <lpage>319</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02310791</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harshman</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>Foundations of the PARAFAC procedure: Models and conditions for an "explanatory" multi-modal factor analysis</article-title>
        <source>UCLA Working Papers in Phonetics</source>
        <year>1970</year>
        <volume>16</volume>
        <issue>1</issue>
        <fpage>84</fpage>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <mixed-citation publication-type="other">Harshman R. Parafac2: Extensions of a procedure for explanatory factor analysis and multidimensional scaling. J Acoust Soc Am. 1972; 51(1A):111. 10.1121/1.1981298.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kiers</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Ten Berge</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Bro</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Parafac2-part i. a direct fitting algorithm for the parafac2 model</article-title>
        <source>J Chemometr</source>
        <year>1999</year>
        <volume>13</volume>
        <issue>3-4</issue>
        <fpage>275</fpage>
        <lpage>94</lpage>
        <pub-id pub-id-type="doi">10.1002/(SICI)1099-128X(199905/08)13:3/4&lt;275::AID-CEM543&gt;3.0.CO;2-B</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tucker</surname>
            <given-names>LR</given-names>
          </name>
        </person-group>
        <article-title>Some mathematical notes on three-mode factor analysis</article-title>
        <source>Psychometrika</source>
        <year>1966</year>
        <volume>31</volume>
        <issue>3</issue>
        <fpage>279</fpage>
        <lpage>311</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02289464</pub-id>
        <?supplied-pmid 5221127?>
        <pub-id pub-id-type="pmid">5221127</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Tao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Maybank</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Supervised tensor learning</article-title>
        <source>Data Mining, Fifth IEEE International Conference On</source>
        <year>2005</year>
        <publisher-loc>Houston</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ridgway</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Irfanoglu</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Machiraju</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Image segmentation with tensor-based classification of n-point correlation functions</article-title>
        <source>MICCAI Workshop on Medical Image Analysis with Applications in Biology, vol. 1.</source>
        <year>2006</year>
        <publisher-loc>Amsterdam</publisher-loc>
        <publisher-name>Elsevier</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bourennane</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fossati</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>About classification methods based on tensor modelling for hyperspectral images</article-title>
        <source>Signal Processing, Image Processing and Pattern Recognition</source>
        <year>2009</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Smalter</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Huan</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lushington</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Feature selection in the tensor product feature space</article-title>
        <source>Data Mining, 2009. ICDM’09. Ninth IEEE International Conference On</source>
        <year>2009</year>
        <publisher-loc>Miami</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Velasco-Forero</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Angulo</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Classification of hyperspectral images by tensor modeling and additive morphological decomposition</article-title>
        <source>Pattern Recognit</source>
        <year>2013</year>
        <volume>46</volume>
        <issue>2</issue>
        <fpage>566</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2012.08.011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kong</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>P. S</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ragin</surname>
            <given-names>AB</given-names>
          </name>
        </person-group>
        <article-title>Tensor-based multi-view feature selection with applications to brain diseases</article-title>
        <source>Data Mining (ICDM), 2014 IEEE International Conference On</source>
        <year>2014</year>
        <publisher-loc>Shenzhen</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kong</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Philip</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Ragin</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Dusk: A dual structure-preserving kernel for supervised tensor learning with applications to neuroimages</article-title>
        <source>Matrix</source>
        <year>2014</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>2</fpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Learning tensor-based features for whole-brain fmri classification</article-title>
        <source>Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015</source>
        <year>2015</year>
        <publisher-loc>Munich</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vo</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tran</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Tensor decomposition and application in image classification with histogram of oriented gradients</article-title>
        <source>Neurocomputing</source>
        <year>2015</year>
        <volume>165</volume>
        <fpage>38</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neucom.2014.06.093</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mørup</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Applications of tensor (multiway array) factorizations and decompositions in data mining</article-title>
        <source>Wiley Interdiscip Rev Data Mining Knowl Discov</source>
        <year>2011</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>24</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1002/widm.1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Petschow</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Peise</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bientinesi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>High-performance solvers for dense hermitian eigenproblems</article-title>
        <source>SIAM J Sci Comput</source>
        <year>2013</year>
        <volume>35</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.1137/110848803</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Phan</surname>
            <given-names>A. H</given-names>
          </name>
          <name>
            <surname>Cichocki</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Tensor decompositions for feature extraction and classification of high dimensional datasets</article-title>
        <source>Nonlinear Theory Appl IEICE</source>
        <year>2010</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>37</fpage>
        <lpage>68</lpage>
        <pub-id pub-id-type="doi">10.1587/nolta.1.37</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Dyrholm</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Parra</surname>
            <given-names>LC</given-names>
          </name>
        </person-group>
        <article-title>Smooth bilinear classification of eeg</article-title>
        <source>Engineering in Medicine and Biology Society, 2006. EMBS’06. 28th Annual International Conference of the IEEE</source>
        <year>2006</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boumal</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Mishra</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Absil</surname>
            <given-names>P-A</given-names>
          </name>
          <name>
            <surname>Sepulchre</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Manopt, a matlab toolbox for optimization on manifolds</article-title>
        <source>J Mach Learn Res</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1455</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Absil</surname>
            <given-names>P-A</given-names>
          </name>
          <name>
            <surname>Mahony</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sepulchre</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>Optimization Algorithms on Matrix Manifolds.</source>
        <year>2009</year>
        <publisher-loc>Princeton</publisher-loc>
        <publisher-name>Princeton University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Belhumeur</surname>
            <given-names>PN</given-names>
          </name>
          <name>
            <surname>Hespanha</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Kriegman</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</article-title>
        <source>Pattern Anal Mach Intell IEEE Trans</source>
        <year>1997</year>
        <volume>19</volume>
        <issue>7</issue>
        <fpage>711</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1109/34.598228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Trenkler</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Hadamard, khatri-rao, kronecker and other matrix products</article-title>
        <source>Int J Inform Syst Sci</source>
        <year>2008</year>
        <volume>4</volume>
        <issue>1</issue>
        <fpage>160</fpage>
        <lpage>77</lpage>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stekelenburg</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Vroomen</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Neural correlates of multisensory integration of ecologically valid audiovisual events</article-title>
        <source>J Cognit Neurosci</source>
        <year>2007</year>
        <volume>19</volume>
        <issue>12</issue>
        <fpage>1964</fpage>
        <lpage>73</lpage>
        <pub-id pub-id-type="doi">10.1162/jocn.2007.19.12.1964</pub-id>
        <pub-id pub-id-type="pmid">17892381</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schalk</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>McFarland</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Hinterberger</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Birbaumer</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wolpaw</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>Bci2000: a general-purpose brain-computer interface (bci) system</article-title>
        <source>Biomed Eng IEEE Trans</source>
        <year>2004</year>
        <volume>51</volume>
        <issue>6</issue>
        <fpage>1034</fpage>
        <lpage>43</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2004.827072</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blankertz</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>K-R</given-names>
          </name>
          <name>
            <surname>Krusienski</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Schalk</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wolpaw</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Schlögl</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pfurtscheller</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Millan</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Schröder</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Birbaumer</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>The bci competition iii: Validating alternative approaches to actual bci problems</article-title>
        <source>Neural Syst Rehabil Eng IEEE Trans</source>
        <year>2006</year>
        <volume>14</volume>
        <issue>2</issue>
        <fpage>153</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1109/TNSRE.2006.875642</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Andersson</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Bro</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>The n-way toolbox for matlab</article-title>
        <source>Chemom Intell Lab Syst</source>
        <year>2000</year>
        <volume>1</volume>
        <issue>52</issue>
        <fpage>1</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1016/S0169-7439(00)00071-X</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <mixed-citation publication-type="other">Nielsen HB, Carsten V. IMMOPTIBOX: A Matlab Toolbox for Optimization and Data Fitting: Technical University of Denmark, DTU Informatics, Building 321; 2010.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haufe</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Meinecke</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Görgen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Dähne</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Haynes</surname>
            <given-names>J. -D</given-names>
          </name>
          <name>
            <surname>Blankertz</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bießmann</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title>
        <source>NeuroImage</source>
        <year>2014</year>
        <volume>87</volume>
        <fpage>96</fpage>
        <lpage>110</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.10.067</pub-id>
        <?supplied-pmid 24239590?>
        <pub-id pub-id-type="pmid">24239590</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <mixed-citation publication-type="other">Wen Z, Yin W. A feasible method for optimization with orthogonality constraints. Math Program. 2013; 142(1-2):397–434. 10.1007/s10107-012-0584-1.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>Y-H</given-names>
          </name>
        </person-group>
        <article-title>A framework of constraint preserving update schemes for optimization on stiefel manifold</article-title>
        <source>Math Program</source>
        <year>2015</year>
        <volume>153</volume>
        <issue>2</issue>
        <fpage>535</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1007/s10107-014-0816-7</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
