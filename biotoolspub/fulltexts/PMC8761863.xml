<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Digit Health</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Digit Health</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Digit. Health</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Digital Health</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2673-253X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8761863</article-id>
    <article-id pub-id-type="doi">10.3389/fdgth.2021.799067</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Digital Health</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CovNet: A Transfer Learning Framework for Automatic COVID-19 Detection From Crowd-Sourced Cough Sounds</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chang</surname>
          <given-names>Yi</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1395108/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jing</surname>
          <given-names>Xin</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1601467/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ren</surname>
          <given-names>Zhao</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/760805/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schuller</surname>
          <given-names>Björn W.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/419411/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Group on Language, Audio, and Music, Imperial College London</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg</institution>, <addr-line>Augsburg</addr-line>, <country>Germany</country></aff>
    <aff id="aff3"><sup>3</sup><institution>L3S Research Center</institution>, <addr-line>Hannover</addr-line>, <country>Germany</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Harry Hochheiser, University of Pittsburgh, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Samrat Kumar Dey, Bangladesh Open University, Bangladesh; K. C. Santosh, University of South Dakota, United States</p>
      </fn>
      <corresp id="c001">*Correspondence: Yi Chang <email>y.chang20@imperial.ac.uk</email></corresp>
      <corresp id="c002">Zhao Ren <email>zren@l3s.de</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Health Informatics, a section of the journal Frontiers in Digital Health</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>03</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <!--PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>.-->
    <volume>3</volume>
    <elocation-id>799067</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>11</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Chang, Jing, Ren and Schuller.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Chang, Jing, Ren and Schuller</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Since the COronaVIrus Disease 2019 (COVID-19) outbreak, developing a digital diagnostic tool to detect COVID-19 from respiratory sounds with computer audition has become an essential topic due to its advantages of being swift, low-cost, and eco-friendly. However, prior studies mainly focused on small-scale COVID-19 datasets. To build a robust model, the large-scale multi-sound FluSense dataset is utilised to help detect COVID-19 from cough sounds in this study. Due to the gap between FluSense and the COVID-19-related datasets consisting of cough only, the transfer learning framework (namely CovNet) is proposed and applied rather than simply augmenting the training data with FluSense. The CovNet contains (i) a parameter transferring strategy and (ii) an embedding incorporation strategy. Specifically, to validate the CovNet's effectiveness, it is used to transfer knowledge from FluSense to COUGHVID, a large-scale cough sound database of COVID-19 negative and COVID-19 positive individuals. The trained model on FluSense and COUGHVID is further applied under the CovNet to another two small-scale cough datasets for COVID-19 detection, the COVID-19 cough sub-challenge (CCS) database in the INTERSPEECH Computational Paralinguistics challengE (ComParE) challenge and the DiCOVA Track-1 database. By training four simple convolutional neural networks (CNNs) in the transfer learning framework, our approach achieves an absolute improvement of 3.57% over the baseline of DiCOVA Track-1 validation of the area under the receiver operating characteristic curve (ROC AUC) and an absolute improvement of 1.73% over the baseline of ComParE CCS test unweighted average recall (UAR).</p>
    </abstract>
    <kwd-group>
      <kwd>transfer learning</kwd>
      <kwd>COVID-19</kwd>
      <kwd>cough</kwd>
      <kwd>FluSense</kwd>
      <kwd>COUGHVID</kwd>
    </kwd-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="6"/>
      <equation-count count="0"/>
      <ref-count count="58"/>
      <page-count count="11"/>
      <word-count count="8405"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Since the year 2019, the coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has become a global pandemic<xref rid="fn0001" ref-type="fn"><sup>1</sup></xref>. As of August 2021, there have been more than 202, 000, 000 confirmed cases of COVID-19 worldwide, including more than 4, 000, 000 deaths, reported by the World Health Organization (WHO)<xref rid="fn0002" ref-type="fn"><sup>2</sup></xref>. The daily <xref rid="fn0001" ref-type="fn"><sup>1</sup></xref>
<xref rid="fn0002" ref-type="fn"><sup>2</sup></xref> increasing COVID-19 cases and deaths have resulted in global lockdown, quarantine, and many restrictions (<xref rid="B1" ref-type="bibr">1</xref>). Along with the above measures, a set of following problems have appeared, including the economic downturn (<xref rid="B2" ref-type="bibr">2</xref>) and mental health problems (e.g., depression and stress) (<xref rid="B1" ref-type="bibr">1</xref>).</p>
    <p>Swift and accurate diagnosis of COVID-19 is essential to give patients appropriate treatments and effectively control its transmission (<xref rid="B3" ref-type="bibr">3</xref>). The reverse transcription PCR (RT-PCR) from oral-nasopharyngeal swabs identifies viral RNA and is a commonly used instrument for the diagnosis of COVID-19. Nevertheless, high false negative rate and stability issues have been reported (<xref rid="B4" ref-type="bibr">4</xref>). In contrast to RT-PCR, chest CT was proven to have high sensitivity and be expedited for diagnosing COVID-19(<xref rid="B4" ref-type="bibr">4</xref>). Serological instruments are utilised to diagnose/confirm late COVID-19 cases by measuring antibody responses to the corresponding infection (<xref rid="B5" ref-type="bibr">5</xref>). Compared to the above laboratory instruments, which require professionals and special medical equipment, rapid antigen and molecular tests using nasopharyngeal swabs are commercially available due to their swift and simple test procedures, reduced mortality of COVID-19 patients, internal hospital costs, and in-hospital transmission (<xref rid="B6" ref-type="bibr">6</xref>). However, rapid tests are still hard-to-follow for non-specialists and are not environment-friendly.</p>
    <p>Artificial intelligence has been widely applied to respiratory sounds in the healthcare area (<xref rid="B7" ref-type="bibr">7</xref>–<xref rid="B9" ref-type="bibr">9</xref>). In a study by (<xref rid="B8" ref-type="bibr">8</xref>), a multilayer perceptron based classifier was developed on features extracted from respiratory sounds to screen lung health. Random forests are applied on the filter bank energy-based features to pre-screen the lung health abnormalities (<xref rid="B9" ref-type="bibr">9</xref>). COVID-19 patients were reported to have seven common symptoms, including fever, cough, sore throat, headache, myalgia, nausea/vomiting, and diarrhea (<xref rid="B10" ref-type="bibr">10</xref>). Among these symptoms, the first two symptoms of COVID-19 are fever and cough (<xref rid="B10" ref-type="bibr">10</xref>). As a fast and non-invasive way to detect potential infections in public areas, body temperature measurement has been commonly employed (<xref rid="B11" ref-type="bibr">11</xref>). Traditional body temperature measurement with a thermometer usually requires relatively close contact with potential COVID-19 positive individuals (<xref rid="B12" ref-type="bibr">12</xref>). Although infrared (IR) thermal cameras provide a non-contact way for mass fever detection, they may not be valid because of the absence of calibration, non-homogeneous devices/protocols, and poor correlation between skin temperature and core body temperature (<xref rid="B11" ref-type="bibr">11</xref>). The reading of IR thermal cameras could also be affected by the environmental temperature (<xref rid="B11" ref-type="bibr">11</xref>). On the other hand, cough, as a common symptom in many respiratory diseases, is a worthwhile consideration when diagnosing a disease (<xref rid="B13" ref-type="bibr">13</xref>). Cough sounds have been used to diagnose asthma, bronchitis, pertussis, pneumonia, etc. (<xref rid="B13" ref-type="bibr">13</xref>). Recent studies have also investigated the feasibility of detecting COVID-19 infections from cough sounds. For instance, cough sounds were shown to contain latent features distinguishable between COVID-19 positive individuals and COVID-19 negative individuals (i.e., normal, bronchitis, and pertussis) (<xref rid="B14" ref-type="bibr">14</xref>). In Brown et al.'s study (<xref rid="B15" ref-type="bibr">15</xref>), cough sounds from COVID-19 positive individuals were reported to have a longer duration, more onsets, higher periods, lower RMS, and MFCC features with fewer outliers. Due to the development of the internet-of-things (IoT), the algorithms for detecting potential COVID-19 positive individuals from cough sounds can be integrated into mobile phones, wearable devices, and robots. Such a rapid, easy-to-use, and environment-friendly instrument will be helpful for real-time and remote pre-screening of COVID-19 infections, thereby supplementing clinical diagnosis and reducing the medical burden.</p>
    <p>Since the outbreak of COVID-19, several studies have collected cough samples from COVID-19 positive patients (and COVID-19 negative individuals) to detect COVID-19 infections. Coswara (<xref rid="B16" ref-type="bibr">16</xref>) is a crowd-sourced database consisting of various kinds of sounds, including breathing (shallow and deep), coughing (shallow and deep), sustained vowel phonation (/ey/ as in made, /i/ as in beet, /u:/ as in cool), and number counting from one to twenty (normal and fast-paced). Another crowd-sourced database, COUGHVID with cough sounds only (<xref rid="B17" ref-type="bibr">17</xref>), was collected via a web interface. To date, the latest version of COUGHVID is publically released with 27, 550 cough recordings.<xref rid="fn0003" ref-type="fn"><sup>3</sup></xref> The crowd-sourced University of Cambridge COVID database was reported to have more than 400 cough and breathing recordings (<xref rid="B15" ref-type="bibr">15</xref>). The Virufy datasets consist of a Latin American crowd-sourced dataset (31 individuals) and two South Asian clinical datasets (362 and 63 individuals, respectively). Due to the difficulty of collecting cough sounds of confirmed COVID-19 patients and multi-sound (non-cough)/noise in crowd-sourced datasets, most of the above databases are small-scale, leading to a challenge for training robust machine learning models.</p>
    <p>With this in mind, we propose a hybrid transfer learning framework for robust COVID-19 detection, where several convolutional neural networks (CNNs) are trained on large-scale databases and fine-tuned on several small-scale cough sound databases for verification. Note that the focus of this paper is not to outperform the state-of-the-art neural networks models for COVID-19 detection from cough sounds; rather, the aim of this study is to provide a framework for mitigating the effect of noise or irrelevant sounds in the crowd-sourcing datasets applied to COVID-19 by training robust CNN models with the transferred knowledge from Flusense and/or COUGHVID. The workflow of this study is indicated in <xref rid="F1" ref-type="fig">Figure 1</xref>. The code of this paper is publicly available on GitHub<xref rid="fn0004" ref-type="fn"><sup>4</sup></xref>.</p>
    <list list-type="bullet">
      <list-item>
        <p>The FluSense database (<xref rid="B18" ref-type="bibr">18</xref>) was collected in a platform to track influenza-related indicators, such as cough, sneeze, sniffle, and speech. Since it contains various types of sounds existing in crowd-sourced cough datasets, the FluSense dataset is applied in this study.</p>
      </list-item>
      <list-item>
        <p>Due to the gap in sound type between FluSense and databases with cough sounds only, the COUGHVID database is considered as the target data when CNNs are trained on FluSense as the source data. The trained models on COUGHVID are further adapted to the other two smaller test databases, i.e., Computational Paralinguistics challengE (ComParE) 2021 COVID-19 cough sub-challenge (CCS) (<xref rid="B19" ref-type="bibr">19</xref>) and DiCOVA 2021 Track-1 (<xref rid="B20" ref-type="bibr">20</xref>).</p>
      </list-item>
      <list-item>
        <p>We propose two transfer learning pipelines, i.e., transferring parameters from the source database to the target database for fine-tuning models and incorporating embeddings for expanding models' capability of extracting useful features.</p>
      </list-item>
    </list>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p>The workflow of this study. CovNet is the proposed transfer learning framework, which includes transferring parameters and incorporating embeddings. CovNet is first applied on the Flusense as the source data, COUGHVID as the target data. Afterwards, to further validate the effectiveness of CovNet, the CovNet based pre-trained COUGHVID models are applied on two smaller Computational Paralinguistics challengE (ComParE) 2021 COVID-19 cough sub-challenge (CCS) dataset and DiCOVA 2021 Track-1 dataset.</p>
      </caption>
      <graphic xlink:href="fdgth-03-799067-g0001" position="float"/>
    </fig>
    <p>In the following sections, the transfer learning framework is first introduced in section 2, followed by the architecture of the models for COVID-19 detection in section 3. Next, the experimental details are described, and the results are presented and discussed in section 4. Finally, our study is summarised, and the outlook is given in section 5.</p>
  </sec>
  <sec id="s2">
    <title>2. Transfer Learning Frameworks</title>
    <p>Transfer learning aims at applying the knowledge learnt from source data to different but related target data and achieving better performance in a cost-effective way (<xref rid="B21" ref-type="bibr">21</xref>–<xref rid="B23" ref-type="bibr">23</xref>). The source data and target data should be similar, otherwise negative transfer may happen (<xref rid="B22" ref-type="bibr">22</xref>, <xref rid="B24" ref-type="bibr">24</xref>). Transfer learning has been successfully applied to COVID-19 detection based on acoustic data (<xref rid="B14" ref-type="bibr">14</xref>, <xref rid="B15" ref-type="bibr">15</xref>). In Imran et al.'s study (<xref rid="B14" ref-type="bibr">14</xref>), the knowledge was transferred from the cough detection model to the COVID-19 diagnosis model. Brown et al. (<xref rid="B15" ref-type="bibr">15</xref>) discovered that VGGish pre-trained on a large-scale YouTube dataset was utilised to extract audio features from raw audio samples for COVID-19 diagnosis.</p>
    <p>In this study, two ways of transfer learning are applied. One is to fine-tune the parameters of the networks with the target data. The other is extracting the embeddings from the pre-trained network and applying the embeddings when training the new network for the target dataset. Since the crowd-sourced cough recordings usually contain non-cough audio signals other than cough sounds, such as speech and breathing, the FluSense dataset and the COUGHVID dataset contain similar sound types. Therefore, the knowledge learnt from FluSense data can be employed to improve the performance of models trained on the COUGHDVID dataset. In <xref rid="F2" ref-type="fig">Figure 2</xref>, D<sub><italic toggle="yes">FluSense</italic></sub> is the FluSense dataset, and D<sub><italic toggle="yes">COUGHVID</italic></sub> means the COUGHVID dataset; convs0 and convs1 represent the convolutional layers/blocks in the neural networks on the FluSense dataset and the COUGHVID dataset, respectively; FC<sub><italic toggle="yes">FluSense</italic></sub> and FC<sub><italic toggle="yes">COUGHVID</italic></sub> denotes the fully-connected (FC) layer of corresponding models. When separating the left part with the right part in <xref rid="F2" ref-type="fig">Figures 2A,B</xref>, with the training data (<italic toggle="yes">x</italic><sub>0</sub>, <italic toggle="yes">y</italic><sub>0</sub>) and (<italic toggle="yes">x</italic><sub>1</sub>, <italic toggle="yes">y</italic><sub>1</sub>), we separately train the CNNs on the FluSense and COUGHVID datasets to produce the predicted values <inline-formula><mml:math id="M1" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mtext>y</mml:mtext><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M2" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mtext>y</mml:mtext><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, respectively.</p>
    <fig position="float" id="F2">
      <label>Figure 2</label>
      <caption>
        <p>The proposed transfer learning framework :CovNet. <bold>(A)</bold> Parameters of the first <italic toggle="yes">n</italic> convolutional layers/blocks (convs1) of the current COUGHVID model are frozen and initialised by the corresponding first <italic toggle="yes">n</italic> convolutional layers/blocks (convs0) of the pre-trained FluSense model. <bold>(B)</bold> Embeddings are extracted after the <italic toggle="yes">n</italic>-th convs0 of the pre-trained FluSense model. The extracted embeddings are concatenated or added to the current embeddings generated after the <italic toggle="yes">n</italic>-th convs1 of the COUGHVID model.</p>
      </caption>
      <graphic xlink:href="fdgth-03-799067-g0002" position="float"/>
    </fig>
    <p>With the parameters and embeddings from the pre-trained FluSense models, as highlighted in blue in <xref rid="F2" ref-type="fig">Figure 2</xref>, the COUGHVID models are given the potential to discriminate between the various audio signals, which further helps its COVID-19 detection from crowd-sourced cough signals. Notably, the predicted value <inline-formula><mml:math id="M3" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mtext>y</mml:mtext><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the final output of the proposed transfer learning framework.</p>
    <p>To further investigate the generalisation ability of CovNet, we apply it to some other small-scale crowd-sourced datasets for COVID-19 detection. In the following, we introduce the two transfer learning methods in greater detail.</p>
    <sec>
      <title>2.1. Transferring Parameters</title>
      <p>Fine-tuning pre-trained models is an effective transfer learning method by sharing some parameters across tasks (<xref rid="B21" ref-type="bibr">21</xref>, <xref rid="B22" ref-type="bibr">22</xref>). In the computer vision area, parameters of pre-trained models on ImageNet (<xref rid="B25" ref-type="bibr">25</xref>) are often applied for transfer learning on a wide range of image-related tasks (<xref rid="B26" ref-type="bibr">26</xref>–<xref rid="B29" ref-type="bibr">29</xref>). Similarly, parameters of pre-trained models on the Audio Set are transferred to many audio-related tasks (<xref rid="B30" ref-type="bibr">30</xref>–<xref rid="B32" ref-type="bibr">32</xref>). Parameters of pre-trained CNN models on the Audio Set are transferred to the adapting networks for acoustic event recognition (<xref rid="B30" ref-type="bibr">30</xref>, <xref rid="B31" ref-type="bibr">31</xref>). Several pre-trained audio neural networks trained on the Audio Set dataset were proposed for other audio pattern recognition tasks (<xref rid="B32" ref-type="bibr">32</xref>).</p>
      <p>In this study, as indicated in <xref rid="F2" ref-type="fig">Figure 2A</xref>, the parameters of the first <italic toggle="yes">n</italic> convolutional layers/blocks, convs1<sub>1, 2,..., <italic toggle="yes">n</italic></sub>, of models trained on the COUGHVID dataset, are initialised by the corresponding layers/blocks convs0<sub>1, 2,..., <italic toggle="yes">n</italic></sub> of models pre-trained on FluSense dataset. The parameters of convs1<sub>1, 2,..., <italic toggle="yes">n</italic></sub> are frozen and not trained, and only the remaining randomly initialised parameters of convs1<sub><italic toggle="yes">n</italic>+1, <italic toggle="yes">n</italic>+2,..., <italic toggle="yes">N</italic></sub> and FC<sub><italic toggle="yes">COUGHVID</italic></sub> are updated during the training procedure.</p>
    </sec>
    <sec>
      <title>2.2. Incorporating Embeddings</title>
      <p>The embeddings generated by the convolutional layers carry either low-level edge information or high-level discrimination-related features (<xref rid="B22" ref-type="bibr">22</xref>, <xref rid="B23" ref-type="bibr">23</xref>). Moreover, the performance of embeddings appears to be highly scalable with the amount of training data (<xref rid="B33" ref-type="bibr">33</xref>). In this study, the pre-trained FluSense models produce embeddings representing high-level or low-level characteristics of various audio types, which can be applied as an additional input to help develop the target model.</p>
      <p>Specifically, we feed the crowd-sourced cough recordings from the COUGHVID into the pre-trained Flusnese model and extract the embeddings after certain convolutional layers/blocks. <xref rid="F2" ref-type="fig">Figure 2B</xref> exhibits this strategy. Data-point (<italic toggle="yes">x</italic><sub>1</sub>, <italic toggle="yes">y</italic><sub>1</sub>) enters the pre-trained FluSense model, and the output embeddings of the <italic toggle="yes">n</italic>-th convolutional layer/block convs0<sub><italic toggle="yes">n</italic></sub> are extracted to be concatenated (on the channel dimension) or added with the embeddings generated by the corresponding convs1<sub><italic toggle="yes">n</italic></sub>. The concatenated or added embeddings enter the next convolutional layer/block convs1<sub><italic toggle="yes">n</italic>+1</sub> for the task of COVID-19 detection.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Automatic COVID-19 Detection</title>
    <p>Convolutional neural networks have been successfully applied in image-related areas, such as image classification (<xref rid="B34" ref-type="bibr">34</xref>–<xref rid="B37" ref-type="bibr">37</xref>). When processing audio signals, CNNs have demonstrated their capabilities in extracting effective representations from the log Mel spectrograms (<xref rid="B38" ref-type="bibr">38</xref>, <xref rid="B39" ref-type="bibr">39</xref>). In this study, we choose four typical CNN models: base CNN (<xref rid="B34" ref-type="bibr">34</xref>), VGG (<xref rid="B40" ref-type="bibr">40</xref>), residual network (ResNet) (<xref rid="B41" ref-type="bibr">41</xref>), and MobileNet (<xref rid="B42" ref-type="bibr">42</xref>). We focus on the proposed transfer learning framework, CovNet, instead of competing with the state-of-the-art models on COVID-19 detection. Therefore, in order to highlight the effectiveness of CovNet, we construct four simple CNN models (i.e., CNN-4, VGG-7, ResNet-6, and MobileNet-6), each of which only has three convolutional layers/blocks. A detailed description of each model is given and analysed in the following subsections.</p>
    <p>The log Mel spectrograms are calculated by Mel filter banks and logarithmic operation worked on the spectrograms, which are produced by the Short-Time Fourier Transforms (STFTs) on the original waveforms. In this section, to better evaluate the effectiveness of the proposed transfer learning framework and compare the performance differences among different CNN architectures, four CNNs are employed to deal with the extracted log Mel spectrograms: CNN-4, VGG-7, ResNet-6, and MobileNet-6. Log Mel spectrograms (<italic toggle="yes">T</italic>,<italic toggle="yes">F</italic>) are extracted from the audio signals as the input to the CNNs, where <italic toggle="yes">T</italic> represents the sequence length, and <italic toggle="yes">F</italic> denotes the log Mel frequency. Before entering the final FC layer, the matrix has the dimension (<italic toggle="yes">C</italic><sub><italic toggle="yes">N</italic></sub>, <italic toggle="yes">N</italic>), where <italic toggle="yes">C</italic><sub><italic toggle="yes">N</italic></sub> is the output channel number of the last convolutional layer, and <italic toggle="yes">N</italic> is the class number. Specifically, for the FluSense database, <italic toggle="yes">N</italic> is set to be 9; for the other datasets used in this study, <italic toggle="yes">N</italic> equals 2. For comparison convenience, we regard the convolutional layers and blocks equally when ordering them in a specific model. In this notation, ResNet-6 and MobileNet-6 have “block2” following the first convolutional layer.</p>
    <sec>
      <title>3.1. CNN-4</title>
      <p>As shown <xref rid="F3" ref-type="fig">Figure 3A</xref>, we propose a simple 4-layer CNN, CNN-4, constructed by three 5 × 5 convolutional layers. To speed up and stabilise the training procedure, each convolutional layer is followed by batch normalisation (<xref rid="B43" ref-type="bibr">43</xref>) and the Rectified Linear Unit (ReLU) activation function (<xref rid="B44" ref-type="bibr">44</xref>). Afterwards, we apply max pooling for downsampling. The first three local max pooling operations are conducted over a 2 × 2 kernel, and the last max pooling is a global one to summarise the features along the dimension of the sequence length and frequency. Before the final FC layer for the final predicted result, a dropout (<xref rid="B45" ref-type="bibr">45</xref>) layer is utilised to address the overfitting issue.</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>Models' architecture: <bold>(A)</bold> Convolutional neural network-4 (CNN-4), <bold>(B)</bold> VGG-7, <bold>(C)</bold> residual network-6 (ResNet-6), <bold>(D)</bold> MobileNet-6. “conv” stands for the convolutional layer, and “block” indicates the convolutional block. The number before the “conv” is the kernel size; the number after the “conv” is the output channel number. The number after “FC” is the input neurons' size.</p>
        </caption>
        <graphic xlink:href="fdgth-03-799067-g0003" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. VGG-7</title>
      <p>Very deep CNN, known as VGG, were originally designed with up to 19 weight layers and achieved great performance on the large-scale image classification task (<xref rid="B40" ref-type="bibr">40</xref>, <xref rid="B46" ref-type="bibr">46</xref>). VGG or VGG-like architectures were applied to extract audio features from respiratory sound data for COVID-19 detection and obtained good performances (<xref rid="B15" ref-type="bibr">15</xref>, <xref rid="B47" ref-type="bibr">47</xref>).</p>
      <p>As indicated in <xref rid="F3" ref-type="fig">Figure 3B</xref>, we adapt the VGG (<xref rid="B40" ref-type="bibr">40</xref>) with 7 layers, VGG-7, which is composed of three convolutional blocks and a final FC layer. Although the VGG-7 is simple, different from its original “deep” design, it is still worthwhile to include it for fair comparison with other CNNs in this study. Each block contains two 3 × 3 convolutional layers, each of which is followed by batch normalisation (<xref rid="B43" ref-type="bibr">43</xref>) and the ReLU function (<xref rid="B44" ref-type="bibr">44</xref>) to stabilise and speed up the training process. Afterwards, a local max pooling layer with a kernel size of 2 × 2 is applied. Following the three blocks, there is also a global max pooling layer working on the sequence length and log Mel frequency dimensions. Before the FC layer, a dropout (<xref rid="B45" ref-type="bibr">45</xref>) layer is applied.</p>
    </sec>
    <sec>
      <title>3.3. ResNet-6</title>
      <p>The Deep ResNet is proposed to address the degradation problem existing in training deeper networks (<xref rid="B41" ref-type="bibr">41</xref>) by incorporating shortcut connections between convolutional layers. In Hershey et al.'s (<xref rid="B48" ref-type="bibr">48</xref>) study,ResNet has outperformed other CNNs for audio classification on the Audio Set (<xref rid="B49" ref-type="bibr">49</xref>). A ResNet based model is constructed for COVID-19 detection from breath and cough audio signals (<xref rid="B50" ref-type="bibr">50</xref>).</p>
      <p>In this study, we mainly adopt the above mentioned shortcut connections to construct a 6-layer ResNet, ResNet-6. In <xref rid="F3" ref-type="fig">Figure 3C</xref>, after the first convolutional layer with a kernel size of 7 × 7 followed by batch normalisation (<xref rid="B43" ref-type="bibr">43</xref>) and the ReLU function (<xref rid="B44" ref-type="bibr">44</xref>), we apply two convolutional blocks, each of which contains the “shortcut connections” to add the identity mapping with the outputs of two stacked 3 × 3 convolutional layers.</p>
      <p>Inside “block2” and “block3,” after the first 3 × 3 convolutional layer, the batch normalisation (<xref rid="B43" ref-type="bibr">43</xref>) and ReLU function (<xref rid="B44" ref-type="bibr">44</xref>) are applied, whereas only the batch normalisation is utilised after the second 3 × 3 convolutional layer. For the channel number consistency, the identity is processed by a 1 × 1 convolutional layer followed by batch normalisation (<xref rid="B43" ref-type="bibr">43</xref>); after the addition of the identity and the output of two stacked convolutional layers, we apply the ReLU function (<xref rid="B44" ref-type="bibr">44</xref>). The max pooling after the 7 × 7 convolutional layer is a local one with a kernel size of 3 × 3 and the max pooling layers in “block2” and “block3” are also local with a kernel size of 2 × 2; similarly, the last max pooling is a global one, followed by a dropout (<xref rid="B45" ref-type="bibr">45</xref>) layer and the FC layer.</p>
    </sec>
    <sec>
      <title>3.4. MobileNet-6</title>
      <p>Based on depthwise separable convolutions, light-weight MobileNets have been widely applied in mobile and embedded image related applications (<xref rid="B42" ref-type="bibr">42</xref>, <xref rid="B51" ref-type="bibr">51</xref>). MobileNets are cost-effective and are explored herein for potential solutions embedded in mobile devices for COVID-19 detection.</p>
      <p>We adapt the MobileNet with 6 layers only. As shown in <xref rid="F3" ref-type="fig">Figure 3D</xref>, after the first 3 × 3 convolutional layer followed by batch normalisation (<xref rid="B43" ref-type="bibr">43</xref>) and the ReLU function (<xref rid="B44" ref-type="bibr">44</xref>), each of “block2” and “block3” contains a 3 × 3 depthwise convolutional layer and a 1 × 1 pointwise convolutional layer, respectively. Similarly, batch normalisation (<xref rid="B43" ref-type="bibr">43</xref>) and ReLU function (<xref rid="B44" ref-type="bibr">44</xref>) are applied after each convolutional layer. Similar to the original MobileNet architecture, we only set one global max pooling layer before the dropout (<xref rid="B45" ref-type="bibr">45</xref>) layer and the final FC layer.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Experimental Results</title>
    <p>With the aforementioned transfer learning framework, the experiments will be presented in this section, including the databases, experimental setup, results, and discussions.</p>
    <sec>
      <title>4.1. Databases</title>
      <p>To verify the proposed transfer learning framework in this study, the following four datasets are employed.</p>
      <sec>
        <title>4.1.1. FluSense</title>
        <p>The FluSense (<xref rid="B18" ref-type="bibr">18</xref>) project applied a part of the original Audio Set dataset (<xref rid="B49" ref-type="bibr">49</xref>), which includes weakly labelled 10-s audio clips from YouTube. After the re-annotation by two human raters for more precise labels in the FluSense (<xref rid="B18" ref-type="bibr">18</xref>) project, there are a total of 45, 550 seconds samples in Audio Set that are considered in this study, and they are labelled with the classes of <italic toggle="yes">breathe, burp, cough, gasp, hiccup, other, silence, sneeze, sniffle, snore, speech, throat-clearing, vomit</italic>, and <italic toggle="yes">wheeze</italic>. To mitigate the effect of data imbalance on the classification performance, those classes with a number of samples less than 100 are not considered in our experiments. Therefore, the audio samples labelled with the following nine classes are employed: <italic toggle="yes">breathe, cough, gasp, other, silence, sneeze, sniffle, speech</italic>, and <italic toggle="yes">throat-clearing</italic>. For all audio recordings in the above nine classes, we first re-sampled them into 16 kHz. Second, as the audio samples have various time lengths, we split the original samples with a length of greater than or equal to 0.5 s into one or more 1 s segment(s). In particular, for audio samples with a length between 0.5 and 1 s, the audio repeats itself until a full 1 s segment is reached. For those samples with a length greater than 1 s, after a certain number of 1 s segments are split, the remaining signals repeat themselves until a full segment is reached if the remaining one has a length of greater than or equal to 0.5 s; otherwise, the remaining signals are simply abandoned. Furthermore, we split the segments into train/val subsets with a ratio of 0.8/0.2 in a stratified manner. The data distribution of FluSense before and after the pre-processing is shown in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
        <table-wrap position="float" id="T1">
          <label>Table 1</label>
          <caption>
            <p>Data distribution of the FluSense data.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Original</bold>
                </th>
                <th valign="top" align="center" colspan="3" style="border-bottom: thin solid #000000;" rowspan="1">
                  <bold>Pre-Processing</bold>
                </th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>#</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Train</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Val</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>∑</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Breathe</td>
                <td valign="top" align="center" rowspan="1" colspan="1">167</td>
                <td valign="top" align="center" rowspan="1" colspan="1">238</td>
                <td valign="top" align="center" rowspan="1" colspan="1">58</td>
                <td valign="top" align="center" rowspan="1" colspan="1">297</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Cough</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2,486</td>
                <td valign="top" align="center" rowspan="1" colspan="1">6,148</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1,537</td>
                <td valign="top" align="center" rowspan="1" colspan="1">7,685</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Gasp</td>
                <td valign="top" align="center" rowspan="1" colspan="1">337</td>
                <td valign="top" align="center" rowspan="1" colspan="1">315</td>
                <td valign="top" align="center" rowspan="1" colspan="1">79</td>
                <td valign="top" align="center" rowspan="1" colspan="1">394</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Other</td>
                <td valign="top" align="center" rowspan="1" colspan="1">3,863</td>
                <td valign="top" align="center" rowspan="1" colspan="1">15,059</td>
                <td valign="top" align="center" rowspan="1" colspan="1">3,765</td>
                <td valign="top" align="center" rowspan="1" colspan="1">18,824</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Silence</td>
                <td valign="top" align="center" rowspan="1" colspan="1">832</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1,116</td>
                <td valign="top" align="center" rowspan="1" colspan="1">279</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1,395</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Sneeze</td>
                <td valign="top" align="center" rowspan="1" colspan="1">611</td>
                <td valign="top" align="center" rowspan="1" colspan="1">540</td>
                <td valign="top" align="center" rowspan="1" colspan="1">135</td>
                <td valign="top" align="center" rowspan="1" colspan="1">675</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Sniffle</td>
                <td valign="top" align="center" rowspan="1" colspan="1">589</td>
                <td valign="top" align="center" rowspan="1" colspan="1">604</td>
                <td valign="top" align="center" rowspan="1" colspan="1">151</td>
                <td valign="top" align="center" rowspan="1" colspan="1">755</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Speech</td>
                <td valign="top" align="center" rowspan="1" colspan="1">2,615</td>
                <td valign="top" align="center" rowspan="1" colspan="1">16,614</td>
                <td valign="top" align="center" rowspan="1" colspan="1">4,154</td>
                <td valign="top" align="center" rowspan="1" colspan="1">20,768</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Throat clearing</td>
                <td valign="top" align="center" rowspan="1" colspan="1">102</td>
                <td valign="top" align="center" rowspan="1" colspan="1">118</td>
                <td valign="top" align="center" rowspan="1" colspan="1">29</td>
                <td valign="top" align="center" rowspan="1" colspan="1">147</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">∑</td>
                <td valign="top" align="center" rowspan="1" colspan="1">11,602</td>
                <td valign="top" align="center" rowspan="1" colspan="1">40,752</td>
                <td valign="top" align="center" rowspan="1" colspan="1">10,188</td>
                <td valign="top" align="center" rowspan="1" colspan="1">50,940</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p><italic toggle="yes">The “original” column indicates the number of audio samples; whereas the “pre-processing” columns show the number of segments with unified length of 1 s</italic>.</p>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <title>4.1.2. COUGHVID</title>
        <p>The on-going crowd-sourced COUGHVID dataset (<xref rid="B17" ref-type="bibr">17</xref>) is collected via a web interface<xref rid="fn0005" ref-type="fn"><sup>5</sup></xref>. All participants voluntarily record and upload their cough sounds lasting for up to 10 s. In the meantime, the COVID-19 status of each cough sample is self-reported by each participant: <italic toggle="yes">healthy, symptomatic without COVID-19 diagnosis</italic>, and <italic toggle="yes">COVID-19</italic>. The information of each participant is optionally self-reported, including the geographic location (latitude, longitude), age, gender, and whether she/he has other pre-existing respiratory conditions, and muscle pain/fever symptoms. As there might be some low-quality audio samples (e.g., noise, speech, etc.), the data collectors trained an extreme gradient boosting (XBG) classifier on 215 audio samples (121 cough and 94 non-cough) to predict the probability of a recording containing cough sounds. For all audio recordings, the sampling frequency is 48 kHz.</p>
        <p>In this study, only the classes of <italic toggle="yes">healthy</italic> (i.e., COVID-19 negative) and <italic toggle="yes">COVID-19</italic> (i.e., COVID-19 positive) are considered, as the audio samples with symptomatic status were not explicitly reported by the participants as to whether they were diagnosed with COVID-19 or not. Furthermore, only audio samples with cough sound probabilities greater than 0.9 are included to ensure each audio sample contains cough sounds. Finally, 7, 774 audio samples (COVID-19 negative: 7, 075, COVID-19 positive: 699) are selected for our experiments. Similarly, we split the selected samples into train/test subsets with a ratio of 0.8/0.2, respectively in a stratified manner. <xref rid="T2" ref-type="table">Table 2</xref> shows the data distribution of COUGHVID.</p>
        <table-wrap position="float" id="T2">
          <label>Table 2</label>
          <caption>
            <p>Data distribution of the COUGHVID data.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>#</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Train</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Test</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>∑</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Negative</td>
                <td valign="top" align="center" rowspan="1" colspan="1">5,660</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1,415</td>
                <td valign="top" align="center" rowspan="1" colspan="1">7,075</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Positive</td>
                <td valign="top" align="center" rowspan="1" colspan="1">559</td>
                <td valign="top" align="center" rowspan="1" colspan="1">140</td>
                <td valign="top" align="center" rowspan="1" colspan="1">699</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">∑</td>
                <td valign="top" align="center" rowspan="1" colspan="1">6,219</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1,555</td>
                <td valign="top" align="center" rowspan="1" colspan="1">7,774</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>4.1.3. ComParE 2021 CCS</title>
        <p>In the INTERPSEECH 2021 ComParE (<xref rid="B19" ref-type="bibr">19</xref>), the CCS provides a dataset from the crowd-sourced Cambridge COVID-19 Sound database (<xref rid="B15" ref-type="bibr">15</xref>). The participants are asked to provide one to three forced coughs in each recording via one of the following multiple platforms: A web interface, an Android app, and an iOS app.<xref rid="fn0006" ref-type="fn"><sup>6</sup></xref> The CCS dataset consists of 929 cough recordings (1.63 h) from 397 participants. The data distribution of CCS is shown in <xref rid="T3" ref-type="table">Table 3</xref>. All recordings from the CCS dataset were resampled and converted into 16 kHz. The official training, validation, and test sets in the ComParE challenge are used in this study.</p>
        <table-wrap position="float" id="T3">
          <label>Table 3</label>
          <caption>
            <p>Data distribution of the Computational Paralinguistics challengE (ComParE) COVID-19 cough sub-challenge (CCS) data.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>#</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Train</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Val</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Test</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>∑</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Negative</td>
                <td valign="top" align="center" rowspan="1" colspan="1">215</td>
                <td valign="top" align="center" rowspan="1" colspan="1">183</td>
                <td valign="top" align="center" rowspan="1" colspan="1">169</td>
                <td valign="top" align="center" rowspan="1" colspan="1">567</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Positive</td>
                <td valign="top" align="center" rowspan="1" colspan="1">71</td>
                <td valign="top" align="center" rowspan="1" colspan="1">48</td>
                <td valign="top" align="center" rowspan="1" colspan="1">39</td>
                <td valign="top" align="center" rowspan="1" colspan="1">158</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">∑</td>
                <td valign="top" align="center" rowspan="1" colspan="1">286</td>
                <td valign="top" align="center" rowspan="1" colspan="1">231</td>
                <td valign="top" align="center" rowspan="1" colspan="1">208</td>
                <td valign="top" align="center" rowspan="1" colspan="1">725</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>4.1.4. DiCOVA 2021 Track-1</title>
        <p>The Track-1 of the DiCOVA challenge 2021 (<xref rid="B20" ref-type="bibr">20</xref>) provides cough recordings from 1, 040 participants (COVID-19 negative: 965, COVID-19 positive 75). In the challenge, the dataset was split into five train-validation folds. Each training set consists of 822 cough samples (COVID-19 negative: 772, COVID-19 positive: 50), and each validation set contains 218 cough samples (COVID-19 negative: 193, COVID-19 positive: 25). The additional test set is not used in this study, as it is blind. All cough recordings are sampled at 44.1 kHz. The data distribution of DiCOVA 2021 Track-1 is indicated in <xref rid="T4" ref-type="table">Table 4</xref>.</p>
        <table-wrap position="float" id="T4">
          <label>Table 4</label>
          <caption>
            <p>DiCOVA Track-1 data distribution of each fold of cross-validation.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>#</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Train</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Val</bold>
                </th>
                <th valign="top" align="center" rowspan="1" colspan="1">
                  <bold>∑</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Negative</td>
                <td valign="top" align="center" rowspan="1" colspan="1">772</td>
                <td valign="top" align="center" rowspan="1" colspan="1">193</td>
                <td valign="top" align="center" rowspan="1" colspan="1">965</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Positive</td>
                <td valign="top" align="center" rowspan="1" colspan="1">50</td>
                <td valign="top" align="center" rowspan="1" colspan="1">25</td>
                <td valign="top" align="center" rowspan="1" colspan="1">75</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">∑</td>
                <td valign="top" align="center" rowspan="1" colspan="1">822</td>
                <td valign="top" align="center" rowspan="1" colspan="1">218</td>
                <td valign="top" align="center" rowspan="1" colspan="1">1040</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
    </sec>
    <sec>
      <title>4.2. Experimental Setup</title>
      <p>For faster progress (<xref rid="B38" ref-type="bibr">38</xref>), all audio files in the four datasets are re-sampled into 16 kHz. The log Mel spectrograms are extracted with a sliding window size of 512, an overlap of 256 units, and 64 Mel bins.</p>
      <p>As for the evaluation metrics, we mainly use unweighted average recall (UAR), since it is more adequate for evaluating the classification performance on imbalanced datasets than accuracy,—the weighted average recall (<xref rid="B52" ref-type="bibr">52</xref>, <xref rid="B53" ref-type="bibr">53</xref>). Apart from the UAR, we also calculate the area under the receiver operating characteristic curve (ROC AUC) score.</p>
      <p>The proposed CNNs consist of three convolutional layers/blocks. The number of output channels for the three convolutional layers/blocks is 64, 128, and 256, respectively. During the training procedure of the neural networks, the cross-entropy loss is utilised as the loss function. To overcome the class imbalance issue, we re-scale the weight parameter for each class in the loss function. Since this study focuses on the transfer learning framework, we do not further mitigate the class imbalance issue through down-/up-sampling.</p>
      <p>For single learning (i.e., training from scratch) on the FluSense and the COUGHVID datasets, the optimiser is set to “Adam” with an initial learning rate of 0.001, which is scheduled to be reduced by a factor of 0.4 when there is less than 0.01 improvement of the UAR after every 4 of 30 epochs in total. When transferring parameters, we set the initial learning rate as 0.0001; for incorporating embeddings, the initial learning rate is set to be 0.001.</p>
      <p>When applying the strategy of transferring parameters introduced in section 2.1 to training the COUGHVID model, we experiment with only setting the following layer(s) trainable: the FC layer, the convolutional layer/block (conv/block) 3 &amp; FC layer, conv/block 2−3 &amp; FC layer, and conv/block 1−3 &amp; FC layer, respectively. The remaining layer(s)/block(s) are initialised based on the pre-trained FluSense models' corresponding parameters and are frozen during the whole training procedure. As for the incorporating embeddings strategy described in section 2.2, we investigate the concatenation and addition of two embeddings generated from the conv/block 3, conv/block 2, and conv/block 1, respectively. One embedding is from the pre-trained FluSense model, and the other one is the COUGHVID model trained from scratch.</p>
      <p>To further validate the effectiveness of the CovNet, we apply the pre-trained COUGHVID models on the ComParE CCS dataset and the DiCOVA Track-1 dataset. Specifically, we train the four CNNs introduced in section 3 from scratch. Afterwards, we choose up to two COUGHVID models with the best performance (best AUC or best UAR) as the pre-trained models. With the chosen pre-trained COUGHVID models and their strategies (layer(s)/block(s) number and transfer learning strategies), we transfer the parameters or embeddings of the above chosen COUGHVID models to the current train-from-scratch models on the ComParE and DiCOVA datasets during the training. Finally, we choose the best results to compete with official baselines: the average validation AUC 68.81% (<xref rid="B20" ref-type="bibr">20</xref>) for the DiCOVA Track-1 dataset, and test UAR without fusion 64.7% (<xref rid="B19" ref-type="bibr">19</xref>) for ComParE CCS. Similarly, when training models from scratch or applying the incorporating embeddings method, we set the initial learning rate as 0.001, whereas if the transferring parameters are utilised, the initial learning rate is set as 0.0001.</p>
    </sec>
    <sec>
      <title>4.3. Results</title>
      <p>In <xref rid="T5" ref-type="table">Table 5</xref>, we focus on performance differences on the COUGHVID test dataset between single learning (training from scratch) models and the models produced by the proposed transfer learning strategies in section 2. For convenience, the best test AUC and test UAR of every model under three transfer learning strategies are shown in bold face. We can see that there are some improvements in test AUC/UAR, especially for the VGG-7 and MobileNet-6. In the following analysis, we compare the absolute difference between performances. On the COUGHVID test dataset, with the transfer learning, the VGG-7 obtains an improvement of 2.62% AUC (<italic toggle="yes">p</italic> &lt; 0.1 in a one-tailed <italic toggle="yes">z</italic>-test) and an improvement of 3.75% UAR (<italic toggle="yes">p</italic> &lt; 0.05 in a one-tailed <italic toggle="yes">z</italic>-test); the MobileNet-6 achieves 3.77% improvement in AUC (<italic toggle="yes">p</italic> &lt; 0.05 in a one-tailed <italic toggle="yes">z</italic>-test) and 4.88% improvement in UAR (<italic toggle="yes">p</italic> &lt; 0.005 in a one-tailed <italic toggle="yes">z</italic>-test). Moreover, for all constructed CNN models, only setting the FC layer trainable and freezing other layers with parameters transferred from pre-trained FluSense models achieves almost the lowest AUC/UAR among all transfer learning settings.</p>
      <table-wrap position="float" id="T5">
        <label>Table 5</label>
        <caption>
          <p>Models' performances [AUC/UAR %] on FluSense and COUGHVID test datasets.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Layers</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CNN-4</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ResNet-6</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>VGG-7</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MobileNet-6</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Single Learning</td>
              <td valign="top" align="center" rowspan="1" colspan="1">FluSense</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.55/65.27</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.91/64.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">93.23/63.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.26/58.24</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">COUGHVID</td>
              <td valign="top" align="center" rowspan="1" colspan="1">—</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.14/59.43</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.86/60.43</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.15/56.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.17/54.83</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Transfer Learning</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Parameters</td>
              <td valign="top" align="center" rowspan="1" colspan="1">FC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.59/53.68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.35/57.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.68/54.14</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.91/53.93</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 3 &amp; FC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.04/57.04</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.01/57.97</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.97/57.15</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.88/<bold>59.71</bold></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 2-3 &amp; FC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.05/<bold>60.98</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>67.89</bold>/<bold>59.25</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.92/<bold>59.79</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>67.94</bold>/58.93</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 1-3 &amp; FC</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>69.43</bold>/55.54</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.23/56.31</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>67.31</bold>/56.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.21/55.64</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">Embeddings Cat</td>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 3</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>67.73</bold>/<bold>60.65</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>67.21</bold>/59.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>65.85</bold>/<bold>58.27</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.32/<bold>56.46</bold></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.30/57.81</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.17/55.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.58/52.30</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>67.36</bold>/52.31</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.15/59.30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.35/<bold>59.77</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.67/51.92</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.37/53.77</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">Embeddings Add</td>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 3</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>66.76</bold>/<bold>59.30</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.27/<bold>58.88</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.08/<bold>60.17</bold></td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>65.94</bold>/<bold>58.24</bold></td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.39/58.82</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.55/57.27</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>67.77</bold>/58.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.37/57.19</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">conv/block 1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.91/57.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>64.63</bold>/58.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.85/58.97</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.17/56.60</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic toggle="yes">Single learning indicates training from scratch and transfer learning includes “Parameters” (transferring parameters), “Embeddings Cat,” and “Embeddings Add” (incorporating emebdddings). The Models' performances with transfer learning are based on the COUGHVID dataset. For “Parameters,” the “Layers” column indicates the layers that are randomly initialised and trainable during the training procedure, and the remaining layers are frozen and initialised by the pre-trained FluSense models; for “Embeddings Cat,” “Embeddings Add,” and “Layers,” the column lists the convolutional layer/block (conv/block), after which embeddings incorporation happens. For convenience, the best test AUC and test UAR of every model under three transfer learning strategies are shown in bold face</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>For the transferring parameters strategy, we can see that most best test AUC/UAR cases are obtained by only setting the convolutional layer/block (conv/block) 2−3 &amp; FC layer trainable or the conv/block 1−3 &amp; FC layer trainable. With the embeddings cat method, models' performances are mostly better than single learning models' and the most best results are achieved by concatenating the embeddings output by the conv/block 3. With the embeddings addition method, models also mostly outperform the single learning ones, and similarly, most best results are obtained by adding embeddings after the conv/block 3.</p>
      <p>In <xref rid="T6" ref-type="table">Table 6</xref>, first, we can see that with the proposed transfer learning strategies on the pre-trained COUGHVID models generated by the CovNet, most of the models' performances improve a lot compared with the single learning models' performance. Specifically, transferring parameters improves the test UAR on ComParE by 9.05% for the VGG-7 (<italic toggle="yes">p</italic> &lt; 0.05 in a one-tailed <italic toggle="yes">z</italic>-test); the transferring parameters improves the validation AUC on DiCOVA by 1.12, 3.86, and 5.22 % for the CNN-4, ResNet-6, and VGG-7, respectively (in a one-tailed <italic toggle="yes">z</italic>-test, not significant, <italic toggle="yes">p</italic> &lt; 0.05, and <italic toggle="yes">p</italic> &lt; 0.005, respectively). The incorporating embeddings improves the test UAR on ComParE data by 1.47, and 1.11% for the CNN-4, and VGG-7, respectively; the incorporating embeddings improves the validation AUC of DiCOVA by 3.62%, 8.85, 7.46, and 2.20% for the CNN-4, ResNet-6, VGG-7, and MobileNet-6, respectively (in a one-tailed <italic toggle="yes">z</italic>-test, <italic toggle="yes">p</italic> &lt; 0.05, <italic toggle="yes">p</italic> &lt; 0.001, <italic toggle="yes">p</italic> &lt; 0.001 and not significant, respectively).</p>
      <table-wrap position="float" id="T6">
        <label>Table 6</label>
        <caption>
          <p>Models' performances [ %], validation AUC on the DiCOVA Track-1 dataset, and test UAR on the ComParE dataset, with single learning (train from scratch), and the proposed transfer learning strategies.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Dataset</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Baseline</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>CNN-4</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ResNet-6</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>VGG-7</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MobileNet-6</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Single Learning</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">ComParE</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.70</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.35</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.78</td>
              <td valign="top" align="center" rowspan="1" colspan="1">57.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.80</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">DiCOVA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.81</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">62.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.88</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64.27</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Transfer Learning</td>
              <td valign="top" align="center" rowspan="1" colspan="1">Parameters</td>
              <td valign="top" align="center" rowspan="1" colspan="1">ComParE</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.24</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.01</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>66.43</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">57.22</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">DiCOVA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>69.88</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.39</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>70.10</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.29</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">Embeddings</td>
              <td valign="top" align="center" rowspan="1" colspan="1">ComParE</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>64.82</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.67</td>
              <td valign="top" align="center" rowspan="1" colspan="1">58.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.37</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" rowspan="1" colspan="1">DiCOVA</td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>72.38</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>71.38</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>72.34</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.47</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic toggle="yes">Pre-trained COUGHVID models and their corresponding transfer learning settings are chosen based on the best performance in <xref rid="T5" ref-type="table">Table 5</xref>. “Embeddings” here include addition/concatenation. The numbers in bold are higher than the baseline</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Second, as the numbers in bold indicate better performance than the baseline, we can see that most models learnt through the transfer learning framework outperform the official baselines, even though the models here are quite simple. Notably, the best test UAR 66.43% on ComParE CCS data is achieved by the VGG-7 with transferring parameters, which is 1.73% above the official baseline; the CNN-4 with incorporating embeddings the achieves the best validation AUC 72.38% on the DiCOVA Track-1, which is 3.57% higher than the baseline (<italic toggle="yes">p</italic> &lt; 0.05 in a one-tailed <italic toggle="yes">z</italic>-test). <xref rid="F4" ref-type="fig">Figure 4</xref> displays the confusion matrices for above-mentioned best UAR on the ComParE CCS dataset and best validation AUC on the DiCOVA Track-1 dataset. We can see that the models recognise negative samples very well, but the positive ones are frequently confused with the negative ones.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>Confusion matrices for the best performance on the ComParE CCS test set and the DiCOVA validation set. For the DiCOVA dataset, since its test dataset is not accessible, the numbers are averaged over the five cross-validation folds.</p>
        </caption>
        <graphic xlink:href="fdgth-03-799067-g0004" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>4.4. Discussion</title>
      <p>In <xref rid="T5" ref-type="table">Table 5</xref>, if comparing the performance of single learning CNNs and transfer learning CNNs, we find that there is no improvement or even slightly worse performance of transfer learning methods on the ResNet-6 model. ResNet gains accuracy from increased neural network depth (<xref rid="B41" ref-type="bibr">41</xref>), which may explain the performance of the simple ResNet-6 in this study. Apart from fine-tuning the parameters of FC layers only, almost all other CNN models obtain better performance after the transfer learning, proving the usefulness of the knowledge transferred from the FluSense dataset for recognising COVID-19 on the COUGHVID dataset. Setting FC layers trainable only limits the generalisation of the pre-trained FluSense models.</p>
      <p>For fine-tuning parameters of different layers, fine-tuning the weights of the convolutional layers/blocks 2−3 &amp; FC layer obtains better performance. Since the target dataset COUGHVID is not large-scale enough compared with the FluSense one, fine-tuning the entire network (convolutional layers/blocks 1−3 &amp; FC layer) might encounter an overfitting issue (<xref rid="B23" ref-type="bibr">23</xref>). Specifically, earlier layers/blocks generate low-level, generic features, which do not change significantly during the training procedure (<xref rid="B23" ref-type="bibr">23</xref>). Conversely, the convolutional layer/block 3 herein generates more high-level, domain-dependent representations. As for the embeddings incorporation, concatenation and addition of the embeddings achieve similar results, which indicates that both operations equally transfer the knowledge learnt from the FluSense dataset. Furthermore, we find that incorporating the embeddings after the convolutional layer/block 3 mostly outperforms the operations on other layers/blocks. This can be caused by more discrimination power obtained by applying the pre-trained FluSense models.</p>
      <p>From <xref rid="T6" ref-type="table">Table 6</xref>, we further validate the generalisation ability of the proposed CovNet with the DiCOVA Track-1 and ComParE CCS datasets. By competing with the official baselines, even simple CNNs can also achieve better performance with the proposed transfer learning methods. Therefore, the considered CovNet appears robust and can provide useful knowledge when detecting COVID-19 from crowd-sourced cough recordings. However, the performance improvement over the ComParE CCS baseline by incorporating the embeddings method is not obvious, which might be caused by the inherent data difference between the FluSense and COUGHVID datasets and the ComParE CCS dataset. Moreover, the CovNet works very well on the DiCOVA track-1 dataset, especially the incorporating embeddings. Perhaps, the embeddings from the pre-trained COUGHVID models carry more beneficial knowledge compared with parameters of convolutional layers on the DiCOVA dataset.</p>
      <p>The main purpose of this study is to introduce and prove the usefulness of the transfer learning framework CovNet, instead of competing with the state-of-the-art performance on the DiCOVA Track-1 dataset (<xref rid="B54" ref-type="bibr">54</xref>–<xref rid="B56" ref-type="bibr">56</xref>) and ComParE CCS dataset (<xref rid="B19" ref-type="bibr">19</xref>). The constructed four CNN models are so simple that each of them only contains three convolutional layers/blocks; we do not apply any data augmentation techniques and the only input to the networks are the original log Mel spectrograms.</p>
    </sec>
  </sec>
  <sec id="s5">
    <title>5. Conclusions and Future Work</title>
    <p>In this study, we proposed a transfer learning framework, CovNet, containing transferring parameters and incorporating embeddings. Transferring parameters indicate fine-tuning the models by initialising and freezing some parameters with the pre-trained model; incorporating embeddings describe concatenating or adding the embeddings generated by a pre-trained model with the embeddings produced by the current model.</p>
    <p>The effectiveness and generalisation ability of the proposed transfer learning framework was demonstrated when developing simple CNNs for COVID-19 detection from crowd-sourced cough sounds. In the future, one should consider deeper neural networks to further improve performance through transfer learning. Moreover, other knowledge transfer architectures, such as multi-task learning (<xref rid="B57" ref-type="bibr">57</xref>) and domain adaption (<xref rid="B58" ref-type="bibr">58</xref>) can be explored.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author/s.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>YC contributed to the study design, experimenting, manuscript drafting, and editing. XJ contributed to the experimenting and manuscript editing. ZR contributed to the study design, manuscript drafting, and editing. BS supervised the whole process, from study design, overall implementation, to manuscript drafting, and editing. All authors approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s8">
    <title>Funding</title>
    <p>This study was partially supported by the Horizon H2020 Marie Skłodowska-Curie Actions Initial Training Network European Training Network (MSCA-ITN-ETN) project under grant agreement No. 766287 (TAPAS), the DFG's Reinhart Koselleck project No. 442218748 (AUDI0NOMOUS), and the Federal Ministry of Education and Research (BMBF), Germany under the project LeibnizKILabor (Grant No. 01DD20003).</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ack>
    <p>The authors would like to express their gratitude to the holders of the FluSense, COUGHVID, ComParE 2021 CCS, and DiCOVA challenge 2021 Track-1 datasets for providing collected data for research purposes. Moreover, thanks to all the participants for their data points.</p>
  </ack>
  <fn-group>
    <fn id="fn0001">
      <p><sup>1</sup><ext-link xlink:href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/naming-the-coronavirus-disease-(covid-2019)-and-the-virus-that-causes-it" ext-link-type="uri">https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/naming-the-coronavirus-disease-(covid-2019)-and-the-virus-that-causes-it</ext-link>; retrieved 10 August 2021.</p>
    </fn>
    <fn id="fn0002">
      <p><sup>2</sup><ext-link xlink:href="https://covid19.who.int/" ext-link-type="uri">https://covid19.who.int/</ext-link>; retrieved 10 August 2021.</p>
    </fn>
    <fn id="fn0003">
      <p>
        <sup>3</sup>
        <ext-link xlink:href="https://zenodo.org/record/4498364#.YRKa3IgzbD4" ext-link-type="uri">https://zenodo.org/record/4498364#.YRKa3IgzbD4</ext-link>
      </p>
    </fn>
    <fn id="fn0004">
      <p>
        <sup>4</sup>
        <ext-link xlink:href="https://github.com/ychang74/CovNet" ext-link-type="uri">https://github.com/ychang74/CovNet</ext-link>
      </p>
    </fn>
    <fn id="fn0005">
      <p><sup>5</sup><ext-link xlink:href="https://COUGHVID.epfl.ch/" ext-link-type="uri">https://COUGHVID.epfl.ch/</ext-link>; retrieved 09 July 2021.</p>
    </fn>
    <fn id="fn0006">
      <p><sup>6</sup><ext-link xlink:href="https://www.covid-19-sounds.org/" ext-link-type="uri">https://www.covid-19-sounds.org/</ext-link>; retrieved 15 July 2021</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atalan</surname><given-names>A</given-names></name></person-group>. <article-title>Is the lockdown important to prevent the COVID-19 pandemic? Effects on psychology, environment and economy-perspective</article-title>. <source>Ann Med Surg</source>. (<year>2020</year>) <volume>56</volume>:<fpage>38</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1016/j.amsu.2020.06.010</pub-id><?supplied-pmid 33824722?><pub-id pub-id-type="pmid">33824722</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inoue</surname><given-names>K</given-names></name><name><surname>Hashioka</surname><given-names>S</given-names></name><name><surname>Kawano</surname><given-names>N</given-names></name></person-group>. <article-title>Risk of an increase in suicide rates associated with economic downturn due to COVID-19 pandemic</article-title>. <source>Asia Pac J Public Health</source>. (<year>2020</year>) <volume>32</volume>:<fpage>367</fpage>. <pub-id pub-id-type="doi">10.1177/1010539520940893</pub-id><?supplied-pmid 32639165?><pub-id pub-id-type="pmid">32639165</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuller</surname><given-names>BW</given-names></name><name><surname>Schuller</surname><given-names>DM</given-names></name><name><surname>Qian</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Zheng</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group>. <article-title>COVID-19 and computer audition: an overview on what speech &amp; sound analysis could contribute in the SARS-CoV-2 corona crisis</article-title>. <source>Front Digit Health</source>. (<year>2021</year>) <volume>3</volume>:<fpage>14</fpage>. <pub-id pub-id-type="doi">10.3389/fdgth.2021.564906</pub-id><?supplied-pmid 34713079?><pub-id pub-id-type="pmid">34713079</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Cai</surname><given-names>Z</given-names></name><etal/></person-group>. <article-title>Stability issues of RT-PCR testing of SARS-CoV-2 for hospitalized patients clinically diagnosed with COVID-19</article-title>. <source>J Med Virol</source>. (<year>2020</year>) <volume>92</volume>:<fpage>903</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1002/jmv.25786</pub-id><?supplied-pmid 32219885?><pub-id pub-id-type="pmid">32219885</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>YW</given-names></name><name><surname>Schmitz</surname><given-names>JE</given-names></name><name><surname>Persing</surname><given-names>DH</given-names></name><name><surname>Stratton</surname><given-names>CW</given-names></name></person-group>. <article-title>Laboratory diagnosis of COVID-19: Current issues and challenges</article-title>. <source>J Clin Microbiol</source>. (<year>2020</year>) <volume>58</volume>:<fpage>e00512</fpage>-<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1128/JCM.00512-20</pub-id><?supplied-pmid 32245835?><pub-id pub-id-type="pmid">32245835</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dinnes</surname><given-names>J</given-names></name><name><surname>Deeks</surname><given-names>JJ</given-names></name><name><surname>Berhane</surname><given-names>S</given-names></name><name><surname>Taylor</surname><given-names>M</given-names></name><name><surname>Adriano</surname><given-names>A</given-names></name><name><surname>Davenport</surname><given-names>C</given-names></name><etal/></person-group>. <article-title>Rapid, point-of-care antigen and molecular-based tests for diagnosis of SARS-CoV-2 infection</article-title>. <source>Cochrane Database Syst Rev</source>. (<year>2021</year>) <volume>3</volume>:<fpage>1</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1002/14651858.CD013705.pub2</pub-id><?supplied-pmid 33760236?><pub-id pub-id-type="pmid">33760236</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Santosh</surname><given-names>KC</given-names></name></person-group>. <article-title>Chapter 1: Speech processing in healthcare: can we integrate?</article-title> In: <person-group person-group-type="editor"><name><surname>Dey</surname><given-names>N</given-names></name></person-group>, editor. <source>Intelligent Speech Signal Processing</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Academic Press</publisher-name> (<year>2019</year>). p. <fpage>1</fpage>–<lpage>4</lpage>. <pub-id pub-id-type="doi">10.1016/B978-0-12-818130-0.00001-5</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukherjee</surname><given-names>H</given-names></name><name><surname>Sreerama</surname><given-names>P</given-names></name><name><surname>Dhar</surname><given-names>A</given-names></name><name><surname>Obaidullah</surname><given-names>SM</given-names></name><name><surname>Roy</surname><given-names>K</given-names></name><name><surname>Santosh</surname><given-names>KC</given-names></name><etal/></person-group>. <article-title>Automatic lung health screening using respiratory sounds</article-title>. <source>J Med Syst</source>. (<year>2021</year>) <volume>45</volume>:<fpage>19</fpage>. <pub-id pub-id-type="doi">10.1007/s10916-020-01681-9</pub-id><?supplied-pmid 33426615?><pub-id pub-id-type="pmid">33426615</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukherjee</surname><given-names>H</given-names></name><name><surname>Salam</surname><given-names>H</given-names></name><name><surname>Santosh</surname><given-names>K</given-names></name></person-group>. <article-title>Lung health analysis: adventitious respiratory sound classification using filterbank energies</article-title>. <source>Int J Pattern Recogn Artif Intell</source>. (<year>2021</year>) <volume>2021</volume>:<fpage>2157008</fpage>. <pub-id pub-id-type="doi">10.1142/S0218001421570081</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>JR</given-names></name><name><surname>Martin</surname><given-names>MR</given-names></name><name><surname>Martin</surname><given-names>JD</given-names></name><name><surname>Kuhn</surname><given-names>P</given-names></name><name><surname>Hicks</surname><given-names>JB</given-names></name></person-group>. <article-title>Modeling the onset of symptoms of COVID-19</article-title>. <source>Front Public Health</source>. (<year>2020</year>) <volume>8</volume>:<fpage>473</fpage>. <pub-id pub-id-type="doi">10.3389/fpubh.2020.00473</pub-id><?supplied-pmid 34914688?><pub-id pub-id-type="pmid">32903584</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buoite Stella</surname><given-names>A</given-names></name><name><surname>Manganotti</surname><given-names>P</given-names></name><name><surname>Furlanis</surname><given-names>G</given-names></name><name><surname>Accardo</surname><given-names>A</given-names></name><name><surname>Ajčević</surname><given-names>M</given-names></name></person-group>. <article-title>Return to school in the COVID-19 era: considerations for temperature measurement</article-title>. <source>J Med Eng Technol</source>. (<year>2020</year>) <volume>44</volume>:<fpage>468</fpage>–<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1080/03091902.2020.1822941</pub-id><?supplied-pmid 32990119?><pub-id pub-id-type="pmid">32990119</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Cheng</surname><given-names>N</given-names></name><name><surname>Xiao</surname><given-names>J</given-names></name></person-group>. <article-title>A real-time robot-based auxiliary system for risk evaluation of COVID-19 infection</article-title>. In: <source>Proc. Interspeech</source>. <publisher-loc>Shanghai</publisher-loc> (<year>2020</year>). p. <fpage>701</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2020-2105</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alqudaihi</surname><given-names>KS</given-names></name><name><surname>Aslam</surname><given-names>N</given-names></name><name><surname>Khan</surname><given-names>IU</given-names></name><name><surname>Almuhaideb</surname><given-names>AM</given-names></name><name><surname>Alsunaidi</surname><given-names>SJ</given-names></name><name><surname>Ibrahim</surname><given-names>NM</given-names></name><etal/></person-group>. <article-title>Cough sound detection and diagnosis using artificial intelligence techniques: challenges and opportunities</article-title>. <source>IEEE Access</source>. (<year>2021</year>). <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3097559</pub-id><?supplied-pmid 34786317?><pub-id pub-id-type="pmid">34786317</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imran</surname><given-names>A</given-names></name><name><surname>Posokhova</surname><given-names>I</given-names></name><name><surname>Qureshi</surname><given-names>HN</given-names></name><name><surname>Masood</surname><given-names>U</given-names></name><name><surname>Riaz</surname><given-names>MS</given-names></name><name><surname>Ali</surname><given-names>K</given-names></name><etal/></person-group>. <article-title>AI4COVID-19: AI enabled preliminary diagnosis for COVID-19 from cough samples via an app</article-title>. <source>Inform Med Unlocked</source>. (<year>2020</year>) <volume>20</volume>:<fpage>100378</fpage>. <pub-id pub-id-type="doi">10.1016/j.imu.2020.100378</pub-id><?supplied-pmid 32839734?><pub-id pub-id-type="pmid">32839734</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>C</given-names></name><name><surname>Chauhan</surname><given-names>J</given-names></name><name><surname>Grammenos</surname><given-names>A</given-names></name><name><surname>Han</surname><given-names>J</given-names></name><name><surname>Hasthanasombat</surname><given-names>A</given-names></name><name><surname>Spathis</surname><given-names>D</given-names></name><etal/></person-group>. <article-title>Exploring automatic diagnosis of COVID-19 from crowdsourced respiratory sound data</article-title>. In: <source>Proc. ACM SIGKDD</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name> (<year>2020</year>). p. <fpage>3474</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1145/3394486.3412865</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>N</given-names></name><name><surname>Krishnan</surname><given-names>P</given-names></name><name><surname>Kumar</surname><given-names>R</given-names></name><name><surname>Ramoji</surname><given-names>S</given-names></name><name><surname>Chetupalli</surname><given-names>SR</given-names></name><name><surname>R</surname><given-names>N</given-names></name><etal/></person-group>. <article-title>Coswara–a database of breathing, cough, and voice sounds for COVID-19 diagnosis</article-title>. In: <source>Proc. Interspeech</source>. <publisher-loc>Shanghai</publisher-loc>: <publisher-name>Interspeech</publisher-name> (<year>2020</year>). p. <fpage>4811</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2020-2768</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orlandic</surname><given-names>L</given-names></name><name><surname>Teijeiro</surname><given-names>T</given-names></name><name><surname>Atienza</surname><given-names>D</given-names></name></person-group>. <article-title>The COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms</article-title>. <source>Sci Data</source>. (<year>2021</year>) <volume>8</volume>:<fpage>1</fpage>–<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1038/s41597-021-00937-4</pub-id><?supplied-pmid 34162883?><pub-id pub-id-type="pmid">33414438</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Al Hossain</surname><given-names>F</given-names></name><name><surname>Lover</surname><given-names>AA</given-names></name><name><surname>Corey</surname><given-names>GA</given-names></name><name><surname>Reich</surname><given-names>NG</given-names></name><name><surname>Rahman</surname><given-names>T</given-names></name></person-group>. <article-title>FluSense: a contactless syndromic surveillance platform for influenza-like illness in hospital waiting areas</article-title>. In: <source>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name> (<year>2020</year>). p. <fpage>1</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1145/3381014</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schuller</surname><given-names>B</given-names></name><name><surname>Batliner</surname><given-names>A</given-names></name><name><surname>Bergler</surname><given-names>C</given-names></name><name><surname>Mascolo</surname><given-names>C</given-names></name><name><surname>Han</surname><given-names>J</given-names></name><name><surname>Lefter</surname><given-names>I</given-names></name><etal/></person-group>. <article-title>The INTERSPEECH 2021 computational paralinguistics challenge: COVID-19 cough, COVID-19 speech, escalation &amp; primates</article-title>. In: <source>Proc. Interspeech</source>. <publisher-loc>Brno</publisher-loc> (<year>2021</year>). p. <fpage>431</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2021-19</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Muguli</surname><given-names>A</given-names></name><name><surname>Pinto</surname><given-names>L</given-names></name><name><surname>R</surname><given-names>N</given-names></name><name><surname>Sharma</surname><given-names>N</given-names></name><name><surname>Krishnan</surname><given-names>P</given-names></name><name><surname>Ghosh</surname><given-names>PK</given-names></name><etal/></person-group>. <article-title>DiCOVA challenge: dataset, task, and baseline system for COVID-19 diagnosis using acoustics</article-title>. In: <source>Proc. Interspeech</source>. <publisher-loc>Brno</publisher-loc> (<year>2021</year>). p. <fpage>901</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2021-74</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Torrey</surname><given-names>L</given-names></name><name><surname>Shavlik</surname><given-names>J</given-names></name></person-group>. <article-title>Transfer learning</article-title>. In: <person-group person-group-type="editor"><name><surname>Ganchev</surname><given-names>T</given-names></name><name><surname>Sokolova</surname><given-names>M</given-names></name><name><surname>Rada</surname><given-names>R</given-names></name><name><surname>Garcia-Laencina</surname><given-names>PJ</given-names></name><name><surname>Ravi</surname><given-names>V</given-names></name></person-group>, editors. <source>Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques</source>. <publisher-loc>Hershey, PA</publisher-loc>: <publisher-name>IGI Publishing</publisher-name> (<year>2010</year>) p. <fpage>242</fpage>–<lpage>64</lpage>. <pub-id pub-id-type="doi">10.4018/978-1-60566-766-9.ch011</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>SJ</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name></person-group>. <article-title>A survey on transfer learning</article-title>. <source>IEEE Trans Knowledge Data Eng</source>. (<year>2010</year>) <volume>22</volume>:<fpage>1345</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1109/TKDE.2009.191</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehdipour Ghazi</surname><given-names>M</given-names></name><name><surname>Yanikoglu</surname><given-names>B</given-names></name><name><surname>Aptoula</surname><given-names>E</given-names></name></person-group>. <article-title>Plant identification using deep neural networks via optimization of transfer learning parameters</article-title>. <source>Neurocomputing</source>. (<year>2017</year>) <volume>235</volume>:<fpage>228</fpage>–<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2017.01.018</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>B</given-names></name><name><surname>Pan</surname><given-names>SJ</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Yeung</surname><given-names>DY</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name></person-group>. <article-title>Adaptive transfer learning</article-title>. In: <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>. <publisher-loc>Atlanta, AL</publisher-loc>: <publisher-name>AAAI</publisher-name> (<year>2010</year>). p. <fpage>407</fpage>–<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group>. <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>Commun ACM</source>. (<year>2017</year>) <volume>60</volume>:<fpage>84</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1145/3065386</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name></person-group>. <article-title>Do better imagenet models transfer better? In: <italic toggle="yes">Proc</italic></article-title>. <source>CVPR</source>. <publisher-name>Long Beach, CA</publisher-name> (<year>2019</year>). <pub-id pub-id-type="doi">10.1109/CVPR.2019.00277</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morid</surname><given-names>MA</given-names></name><name><surname>Borjali</surname><given-names>A</given-names></name><name><surname>Del Fiol</surname><given-names>G</given-names></name></person-group>. <article-title>A scoping review of transfer learning research on medical image analysis using ImageNet</article-title>. <source>Comput Biol Med</source>. (<year>2021</year>) <volume>128</volume>:<fpage>104115</fpage>. <pub-id pub-id-type="doi">10.1016/j.compbiomed.2020.104115</pub-id><?supplied-pmid 33227578?><pub-id pub-id-type="pmid">33227578</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Raghu</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Kleinberg</surname><given-names>J</given-names></name><name><surname>Bengio</surname><given-names>S</given-names></name></person-group>. <source>Transfusion: Understanding Transfer Learning for Medical Imaging</source>. <publisher-loc>Red Hook, NY</publisher-loc>: <publisher-name>Curran Associates Inc</publisher-name>. (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>HC</given-names></name><name><surname>Roth</surname><given-names>HR</given-names></name><name><surname>Gao</surname><given-names>M</given-names></name><name><surname>Lu</surname><given-names>L</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>Nogues</surname><given-names>I</given-names></name><etal/></person-group>. <article-title>Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</article-title>. <source>IEEE Trans Med Imaging</source>. (<year>2016</year>) <volume>35</volume>:<fpage>1285</fpage>–<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2528162</pub-id><?supplied-pmid 26886976?><pub-id pub-id-type="pmid">26886976</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pons</surname><given-names>J</given-names></name><name><surname>Serra</surname><given-names>J</given-names></name><name><surname>Serra</surname><given-names>X</given-names></name></person-group>. <article-title>Training neural audio classifiers with few data</article-title>. In: <source>Proc. ICASSP</source>. <publisher-loc>Brighton</publisher-loc> (<year>2019</year>). p. <fpage>16</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2019.8682591</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Khadkevich</surname><given-names>M</given-names></name><name><surname>Fagen</surname><given-names>C</given-names></name></person-group>. <article-title>Knowledge transfer from weakly labeled audio using convolutional neural network for sound events and scenes</article-title>. In: <source>Proc. ICASSP</source>. <publisher-loc>Calgary, AB</publisher-loc> (<year>2018</year>). p. <fpage>326</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2018.8462200</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>Q</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Iqbal</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Plumbley</surname><given-names>MD</given-names></name></person-group>. <article-title>PANNs: Large-scale pretrained audio neural networks for audio pattern recognition</article-title>. <source>IEEE/ACM Trans Audio Speech Lang Process</source>. (<year>2020</year>) <volume>28</volume>:<fpage>2880</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1109/TASLP.2020.3030497</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Snyder</surname><given-names>D</given-names></name><name><surname>Garcia-Romero</surname><given-names>D</given-names></name><name><surname>Sell</surname><given-names>G</given-names></name><name><surname>Povey</surname><given-names>D</given-names></name><name><surname>Khudanpur</surname><given-names>S</given-names></name></person-group>. <article-title>X-vectors: robust DNN embeddings for speaker recognition</article-title>. In: <source>Proc. ICASSP</source>. <publisher-loc>Calgary, AB</publisher-loc> (<year>2018</year>). p. <fpage>5329</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2018.8461375</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>MD</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group>. <article-title>Visualizing and understanding convolutional networks</article-title>. In: <person-group person-group-type="editor"><name><surname>Fleet</surname><given-names>D</given-names></name><name><surname>Pajdla</surname><given-names>T</given-names></name><name><surname>Schiele</surname><given-names>B</given-names></name><name><surname>Tuytelaars</surname><given-names>T</given-names></name></person-group>, editors. <source>Computer Vision-ECCV 2014</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name> (<year>2014</year>). p. <fpage>818</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-10590-1_53</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Mao</surname><given-names>J</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Huang</surname><given-names>C</given-names></name><name><surname>Xu</surname><given-names>W</given-names></name></person-group>. <article-title>CNN-RNN: a unified framework for multi-label image classification</article-title>. In: <source>Proc. CVPR</source>. <publisher-loc>Las Vegas, NV</publisher-loc> (<year>2016</year>). <pub-id pub-id-type="doi">10.1109/CVPR.2016.251</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Y</given-names></name><name><surname>Xia</surname><given-names>W</given-names></name><name><surname>Lin</surname><given-names>M</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Ni</surname><given-names>B</given-names></name><name><surname>Dong</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>HCP: a flexible CNN framework for multi-label image classification</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. (<year>2016</year>) <volume>38</volume>:<fpage>1901</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2015.2491929</pub-id><?supplied-pmid 26513778?><pub-id pub-id-type="pmid">26513778</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Cai</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Feng</surname><given-names>DD</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name></person-group>. <article-title>Medical image classification with convolutional neural network</article-title>. In: <source>Proc. ICARCV</source>. <publisher-name>Marina Bay Sands</publisher-name> (<year>2014</year>). p. <fpage>844</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1109/ICARCV.2014.7064414</pub-id><?supplied-pmid 34115822?></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>Z</given-names></name><name><surname>Baird</surname><given-names>A</given-names></name><name><surname>Han</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Schuller</surname><given-names>B</given-names></name></person-group>. <article-title>Generating and protecting against adversarial attacks for deep speech-based emotion recognition models</article-title>. In: <source>Proc. ICASSP</source>. <publisher-loc>Barcelona</publisher-loc> (<year>2020</year>). p. <fpage>7184</fpage>–<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP40776.2020.9054087</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>Q</given-names></name><name><surname>Yu</surname><given-names>C</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Iqbal</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Plumbley</surname><given-names>MD</given-names></name></person-group>. <article-title>Weakly labelled AudioSet tagging with attention neural networks</article-title>. <source>IEEE/ACM Trans Audio Speech Lang Process</source>. (<year>2019</year>) <volume>27</volume>:<fpage>1791</fpage>–<lpage>802</lpage>. <pub-id pub-id-type="doi">10.1109/TASLP.2019.2930913</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group>. <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. In: <source>Proc. ICLR</source>. <publisher-loc>San Diego, CA</publisher-loc> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group>. <article-title>Deep residual learning for image recognition</article-title>. In: <source>Proc. CVPR</source>. <publisher-loc>Las Vegas, NV</publisher-loc> (<year>2016</year>). p. <fpage>770</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id><?supplied-pmid 32166560?></mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>AG</given-names></name><name><surname>Zhu</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>B</given-names></name><name><surname>Kalenichenko</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Weyand</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>MobileNets: efficient convolutional neural networks for mobile vision applications</article-title> (<year>2017</year>). <source>arXiv [Preprint]. arXiv</source>: 1704.04861. Available Online at: <ext-link xlink:href="https://dblp.uni-trier.de/rec/journals/corr/HowardZCKWWAA17.html?view=bibtex" ext-link-type="uri">https://dblp.uni-trier.de/rec/journals/corr/HowardZCKWWAA17.html?view=bibtex</ext-link></mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group>. <article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title>. In: <source>Proc. ICML.</source><publisher-loc>Lille</publisher-loc>: <publisher-name>ICML</publisher-name> (<year>2015</year>). p. <fpage>448</fpage>–<lpage>56</lpage>.</mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>V</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group>. <article-title>Rectified linear units improve restricted Boltzmann machines</article-title>. In: <source>Proc. ICML</source>. <publisher-loc>Madison, WI</publisher-loc> (<year>2010</year>). p. <fpage>807</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group>. <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J Mach Learn Res</source>. (<year>2014</year>) <volume>15</volume>:<fpage>1929</fpage>–<lpage>58</lpage>. Available online at: <ext-link xlink:href="https://jmlr.org/papers/v15/srivastava14a.html" ext-link-type="uri">https://jmlr.org/papers/v15/srivastava14a.html</ext-link></mixed-citation>
    </ref>
    <ref id="B46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sitaula</surname><given-names>C</given-names></name><name><surname>Belayet Hossain</surname><given-names>M</given-names></name></person-group>. <article-title>Attention-based VGG-16 model for COVID-19 chest X-ray image classification</article-title>. <source>Appl Intell</source>. (<year>2021</year>) <volume>51</volume>:<fpage>2850</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1007/s10489-020-02055-x</pub-id><?supplied-pmid 34764568?><pub-id pub-id-type="pmid">34764568</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lella</surname><given-names>KK</given-names></name><name><surname>Pja</surname><given-names>A</given-names></name></person-group>. <article-title>Automatic diagnosis of COVID-19 disease using deep convolutional neural network with multi-feature channel from respiratory sound data: cough, voice, and breath</article-title>. <source>Alexandria Eng J</source>. (<year>2021</year>) <volume>61</volume>:<fpage>1319</fpage>–<lpage>34</lpage>. <pub-id pub-id-type="doi">10.1016/j.aej.2021.06.024</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hershey</surname><given-names>S</given-names></name><name><surname>Chaudhuri</surname><given-names>S</given-names></name><name><surname>Ellis</surname><given-names>DPW</given-names></name><name><surname>Gemmeke</surname><given-names>JF</given-names></name><name><surname>Jansen</surname><given-names>A</given-names></name><name><surname>Moore</surname><given-names>RC</given-names></name><etal/></person-group>. <article-title>CNN architectures for large-scale audio classification</article-title>. In: <source>Proc. ICASSP</source>. <publisher-loc>New Orleans, LA</publisher-loc> (<year>2017</year>). p. <fpage>131</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2017.7952132</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B49">
      <label>49.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gemmeke</surname><given-names>JF</given-names></name><name><surname>Ellis</surname><given-names>DPW</given-names></name><name><surname>Freedman</surname><given-names>D</given-names></name><name><surname>Jansen</surname><given-names>A</given-names></name><name><surname>Lawrence</surname><given-names>W</given-names></name><name><surname>Moore</surname><given-names>RC</given-names></name><etal/></person-group>. <article-title>Audio Set: An ontology and human-labeled dataset for audio events</article-title>. In: <source>Proc. ICASSP</source>. <publisher-loc>New Orleans, LA</publisher-loc> (<year>2017</year>). p. <fpage>776</fpage>–<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2017.7952261</pub-id><?supplied-pmid 27295638?></mixed-citation>
    </ref>
    <ref id="B50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coppock</surname><given-names>H</given-names></name><name><surname>Gaskell</surname><given-names>A</given-names></name><name><surname>Tzirakis</surname><given-names>P</given-names></name><name><surname>Baird</surname><given-names>A</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Schuller</surname><given-names>B</given-names></name></person-group>. <article-title>End-to-end convolutional neural network enables COVID-19 detection from breath and cough audio: a pilot study</article-title>. <source>BMJ Innovations</source>. (<year>2021</year>) <volume>7</volume>:<fpage>356</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1136/bmjinnov-2021-000668</pub-id><?supplied-pmid 34192022?><pub-id pub-id-type="pmid">34192022</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nayak</surname><given-names>SR</given-names></name><name><surname>Nayak</surname><given-names>DR</given-names></name><name><surname>Sinha</surname><given-names>U</given-names></name><name><surname>Arora</surname><given-names>V</given-names></name><name><surname>Pachori</surname><given-names>RB</given-names></name></person-group>. <article-title>Application of deep learning techniques for detection of COVID-19 cases using chest X-ray images: a comprehensive study</article-title>. <source>Biomed Signal Process Control</source>. (<year>2021</year>) <volume>64</volume>:<fpage>102365</fpage>. <pub-id pub-id-type="doi">10.1016/j.bspc.2020.102365</pub-id><?supplied-pmid 33230398?><pub-id pub-id-type="pmid">33230398</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <label>52.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schuller</surname><given-names>B</given-names></name><name><surname>Batliner</surname><given-names>A</given-names></name></person-group>. <source>Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing</source>. <edition>1st ed</edition>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley Publishing</publisher-name> (<year>2013</year>). <pub-id pub-id-type="doi">10.1002/9781118706664</pub-id><?supplied-pmid 25855820?></mixed-citation>
    </ref>
    <ref id="B53">
      <label>53.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>A</given-names></name></person-group>. <article-title>Classifying skewed data: importance weighting to optimize average recall</article-title>. In: <source>Proc. Interspeech</source>. <publisher-loc>Portland, OR</publisher-loc> (<year>2012</year>). p. <fpage>2242</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2012-131</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <label>54.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sodergren</surname><given-names>I</given-names></name><name><surname>Nodeh</surname><given-names>MP</given-names></name><name><surname>Chhipa</surname><given-names>PC</given-names></name><name><surname>Nikolaidou</surname><given-names>K</given-names></name><name><surname>Kovacs</surname><given-names>G</given-names></name></person-group>. <article-title>Detecting COVID-19 from audio recording of coughs using random forests and support vector machines</article-title>. In: <source>Proc. Interspeech</source>. <publisher-loc>Brno</publisher-loc>: <publisher-name>Interspeech</publisher-name> (<year>2021</year>) p. <fpage>916</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2021-2191</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <label>55.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Das</surname><given-names>RK</given-names></name><name><surname>Madhavi</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group>. <article-title>Diagnosis of COVID-19 using auditory acoustic cues</article-title>. In: <source>Proc. Interspeech</source>. <publisher-loc>Brno</publisher-loc>: <publisher-name>Interspeech</publisher-name> (<year>2021</year>) p. <fpage>921</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2021-497</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvill</surname><given-names>J</given-names></name><name><surname>Wani</surname><given-names>YR</given-names></name><name><surname>Hasegawa-Johnson</surname><given-names>M</given-names></name><name><surname>Ahuja</surname><given-names>N</given-names></name><name><surname>Beiser</surname><given-names>D</given-names></name><name><surname>Chestek</surname><given-names>D</given-names></name></person-group>. <article-title>Classification of COVID-19 from cough using autoregressive predictive coding pretraining and spectral data augmentation</article-title>. In: <source>Proc. Interspeech</source>. (<year>2021</year>) p. <fpage>926</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.21437/Interspeech.2021-799</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caruana</surname><given-names>R</given-names></name></person-group>. <article-title>Multitask learning</article-title>. <source>Mach Learn</source>. (<year>1997</year>) <volume>28</volume>:<fpage>41</fpage>–<lpage>75</lpage>. <pub-id pub-id-type="doi">10.1023/A:1007379606734</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Deng</surname><given-names>W</given-names></name></person-group>. <article-title>Deep visual domain adaptation: a survey</article-title>. <source>Neurocomputing</source>. (<year>2018</year>) <volume>312</volume>:<fpage>135</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2018.05.083</pub-id><?supplied-pmid 30683543?><pub-id pub-id-type="pmid">30683543</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
