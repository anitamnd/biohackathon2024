<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8170782</article-id>
    <article-id pub-id-type="publisher-id">4022</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-021-04022-w</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PIKE-R2P: Protein–protein interaction network-based knowledge embedding with graph neural network for single-cell RNA to protein prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Dai</surname>
          <given-names>Xinnan</given-names>
        </name>
        <address>
          <email>daixn@shanghaitech.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xu</surname>
          <given-names>Fan</given-names>
        </name>
        <address>
          <email>xunfan1@shanghaitech.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Shike</given-names>
        </name>
        <address>
          <email>wangshk@shanghaitech.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mundra</surname>
          <given-names>Piyushkumar A.</given-names>
        </name>
        <address>
          <email>mpiyush21@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6774-9786</contrib-id>
        <name>
          <surname>Zheng</surname>
          <given-names>Jie</given-names>
        </name>
        <address>
          <email>zhengjie@shanghaitech.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.440637.2</institution-id><institution-id institution-id-type="ISNI">0000 0004 4657 8879</institution-id><institution>School of Information Science and Technology, </institution><institution>ShanghaiTech University, </institution></institution-wrap>393 Middle Huaxia Road, Pudong District, Shanghai, 201210 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5379.8</institution-id><institution-id institution-id-type="ISNI">0000000121662407</institution-id><institution>Molecular Oncology Group, Cancer Research UK Manchester Institute, </institution><institution>The University of Manchester, </institution></institution-wrap>Alderley Park, Manchester, UK </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>2</day>
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>2</day>
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <issue>Suppl 6</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. Supplement Editors were not involved in the review of any supplement article they had co-authored. No other competing interests were declared.</issue-sponsor>
    <elocation-id>139</elocation-id>
    <history>
      <date date-type="received">
        <day>8</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>2</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Recent advances in simultaneous measurement of RNA and protein abundances at single-cell level provide a unique opportunity to predict protein abundance from scRNA-seq data using machine learning models. However, existing machine learning methods have not considered relationship among the proteins sufficiently.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We formulate this task in a multi-label prediction framework where multiple proteins are linked to each other at the single-cell level. Then, we propose a novel method for single-cell RNA to protein prediction named PIKE-R2P, which incorporates protein–protein interactions (PPI) and prior knowledge embedding into a graph neural network. Compared with existing methods, PIKE-R2P could significantly improve prediction performance in terms of smaller errors and higher correlations with the gold standard measurements.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">The superior performance of PIKE-R2P indicates that adding the prior knowledge of PPI to graph neural networks can be a powerful strategy for cross-modality prediction of protein abundances at the single-cell level.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Single-cell</kwd>
      <kwd>Protein prediction</kwd>
      <kwd>Graph neural network</kwd>
      <kwd>Knowledge embedding</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100012600</institution-id>
            <institution>ShanghaiTech University</institution>
          </institution-wrap>
        </funding-source>
        <award-id>Startup Grant</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zheng</surname>
            <given-names>Jie</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <conference xlink:href="https://incob.apbionet.org/incob20/">
      <conf-name>19th International Conference on Bioinformatics 2020 (InCoB2020)</conf-name>
      <conf-loc>Virtual</conf-loc>
      <conf-date>25-29 November 2020</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par12">The state of a cell can be described from different perspectives by using a variety of omics data, such as genomic, transcriptomic, and proteomic data [<xref ref-type="bibr" rid="CR1">1</xref>]. Simultaneous measurement of RNA and protein abundances in the same cells is conducive to the elucidation of cell states [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. Moreover, there is a correlation between the abundances of RNAs and proteins [<xref ref-type="bibr" rid="CR4">4</xref>]. According to [<xref ref-type="bibr" rid="CR5">5</xref>], to some extent, RNAs can guide the expression of proteins. Recently, machine learning methods have been proposed to predict protein abundances from transcriptomic data at the single-cell level. Because the same set of RNAs are used to predict multiple proteins, the task can be formulated in a multi-label machine learning framework. These multi-label models reduce some cost of computation by extracting the general features from input data [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>].</p>
    <p id="Par13">Multi-label modeling, which uses one model to predict multiple labels at the same time, has been widely used in machine learning applications, such as image recognition [<xref ref-type="bibr" rid="CR8">8</xref>] and text classification [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. Moreover, the multi-label models have been adopted for the prediction of the biological quantities such as the abundances of proteins and RNAs. For example, Liang et al. [<xref ref-type="bibr" rid="CR11">11</xref>] uses the Gaussian method to identify disease-associated candidate miRNAs; Chou [<xref ref-type="bibr" rid="CR12">12</xref>] proposes a feature merging method to improve the multiple protein prediction by genomic data; Zou et al. [<xref ref-type="bibr" rid="CR13">13</xref>] employs a hierarchical neural network for enzyme function prediction. In recent years, graph neural network (GNN) has been one of the most popular core frameworks of the multi-label models [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
    <p id="Par14">Graph neural networks have been widely applied to different fields, such as natural language processing [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], computer vision [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>], and drug discovery [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]. Knowledge graph is a particular application of GNN which introduces knowledge-based information into predictions, boosting performance of GNN on various tasks, such as image classification [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>], recommendation systems [<xref ref-type="bibr" rid="CR23">23</xref>], and dialogue systems [<xref ref-type="bibr" rid="CR24">24</xref>].</p>
    <p id="Par15">Protein abundance is closely related to other types of molecules in cells, especially RNAs [<xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR27">27</xref>]. A variety of data sources have been used to predict protein abundance [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR29">29</xref>]. With the published CITE-seq dataset, machine learning methods have been used to predict protein abundances from RNA expression levels, e.g. [<xref ref-type="bibr" rid="CR6">6</xref>] proposed a toolkit to study the correlation between the abundances of RNAs and proteins.</p>
    <p id="Par16">Machine learning methods for RNA to protein abundance prediction based on CITE-seq dataset include cTP-net [<xref ref-type="bibr" rid="CR7">7</xref>] and Random Forest [<xref ref-type="bibr" rid="CR30">30</xref>]. Zhou et al. proposed cTP-net, using transfer learning to construct a multi-branch model, which predicts the abundances of multiple proteins using the same parameter values [<xref ref-type="bibr" rid="CR7">7</xref>]. After extracting RNA features, Xu et al. applied the Random Forest models with different parameters for each protein [<xref ref-type="bibr" rid="CR30">30</xref>] . They found that the Random Forest model achieved higher prediction performance than neural network methods (including cTP-net) on small datasets.</p>
    <p id="Par17">In this work, we propose a novel method called PIKE-R2P (Protein–protein Interaction network-based Knowledge Embedding with graph neural network for single-cell RNA to Protein prediction). Given a sample of scRNA-seq data, the model predicts the abundances of multiple proteins. Our model mainly comprises two parts: a PPI-based GNN and prior knowledge embedding. We use the GNN to capture the relationships among target proteins in sharing some mechanisms of gene expression regulation from transcription to translation. Besides, we integrate the prior knowledge from the STRING database [<xref ref-type="bibr" rid="CR31">31</xref>] with the model to constrain the protein correlations. PIKE-R2P performs better than existing methods for the protein abundance prediction, especially in terms of accuracy.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <sec id="Sec3">
      <title>Dataset</title>
      <p id="Par18">To demonstrate the efficacy of the proposed PIKE-R2P model, we applied it on two CITE-seq datasets available from NCBI GEO database (GSE100866) [<xref ref-type="bibr" rid="CR4">4</xref>]. The first dataset includes single-cell gene expression of 36,280 mRNAs in 8617 cord blood mononuclear cells (CBMC) with simultaneous measurement of 13 surface proteins. The second dataset contains the expression levels of 29,929 mRNAs and 10 proteins in 7985 peripheral blood mononuclear cells (PBMC).</p>
      <p id="Par19">As these datasets are inherently noisy, we did quality control and noise reduction for them. First, we filtered out cells whose mitochondrial read rates are at least 20%. Then, cells with at most 250 genes expressed were deleted, following the guide of Seurat v3.0 [<xref ref-type="bibr" rid="CR6">6</xref>]. Then, to denoise the data, we fed the data to SAVER-X, a toolkit implementing an autoencoder combined with a Bayesian method for denoising cross-species data by transfer learning [<xref ref-type="bibr" rid="CR32">32</xref>]. As a result, the final CBMC dataset contains 8552 cells with 20,501 genes, while the PBMC dataset contains 7947 cells with 17,114 genes.</p>
      <p id="Par20">To train and test the machine learning models, we randomly divided the cells into two disjoint subsets with a 70:30 split for training and testing respectively. Thus, the CBMC training dataset has 5991 cells while the remaining 2561 cells are in the test set. Similarly, the PBMC training and test datasets contain 5567 and 2380 cells respectively. Details of the data are summarized in Table <xref rid="Tab1" ref-type="table">1</xref>.</p>
      <p id="Par21">To incorporate PPI information in the GNN, we selected several PPI features from the STRING database [<xref ref-type="bibr" rid="CR31">31</xref>] as prior knowledge, including empirically determined interaction, annotated database, automated text mining, combined score, and gene co-occurrence. These features are encoded as floating point numbers.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Data summary after noise reduction</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">CBMC</th><th align="left">PBMC</th></tr></thead><tbody><tr><td align="left" colspan="3">Number of molecular species</td></tr><tr><td align="left"> RNA</td><td char="." align="char">20,501</td><td char="." align="char">17,114</td></tr><tr><td align="left"> Protein</td><td char="." align="char">13</td><td char="." align="char">10</td></tr><tr><td align="left" colspan="3">Number of cells</td></tr><tr><td align="left"> Training set</td><td char="." align="char">5991</td><td char="." align="char">5567</td></tr><tr><td align="left"> Testing set</td><td char="." align="char">2561</td><td char="." align="char">2380</td></tr><tr><td align="left"> Total</td><td char="." align="char">8552</td><td char="." align="char">7947</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Analysis of model prediction results</title>
      <p id="Par22">We compared the performance of the proposed PIKE-R2P method with cTP-net [<xref ref-type="bibr" rid="CR7">7</xref>] and Random Forest [<xref ref-type="bibr" rid="CR33">33</xref>]. We used the Random Forest available from the Scikit-learn (0.23.1) Python package [<xref ref-type="bibr" rid="CR34">34</xref>], and the R code of cTP-net. Both PIKE-R2P and Random Forest were trained and tested on the data as summarized in Table <xref rid="Tab1" ref-type="table">1</xref> with the same input features. However, cTP-net does not provide any training API. Thus, we used the pre-trained cTP-net model with a reduced number of gene expression features <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n=12{,}363$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mn>363</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq1.gif"/></alternatives></inline-formula>, and the performance of cTP-net was evaluated on the testing set only. In addition, cTP-net only predicts 10 proteins in the CBMC dataset, excluding three proteins (CCR7, CCR5, and CD10). Thus, in this section, we also analyzed these 10 proteins only. The performance of the models were evaluated using mean squared error (MSE) and Pearson Correlation Coefficient (PCC) between the ground truth values and the predicted values. For each protein, we picked the best result (i.e. smallest MSE and highest PCC) out of 5 runs. We calculated the means and standard deviations (SDs) for the values of MSE and PCC of the 10 proteins to show the stability of the model.</p>
      <p id="Par23">Table <xref rid="Tab2" ref-type="table">2</xref> shows the performance of the models on the two datasets. In general, all the models had lower mean MSE and PCC scores on the CBMC dataset than the corresponding scores on the PBMC dataset (except that PIKE-R2P achieved a higher PCC on CBMC than on PBMC). Among the three models, PIKE-R2P got the lowest MSEs on both datasets, the highest PCC on CBMC, and the second highest PCC on PBMC.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance of different models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="4">CBMC</th><th align="left" colspan="4">PBMC</th></tr><tr><th align="left">MSE</th><th align="left">MSE SD</th><th align="left">PCC</th><th align="left">PCC SD</th><th align="left">MSE</th><th align="left">MSE SD</th><th align="left">PCC</th><th align="left">PCC SD</th></tr></thead><tbody><tr><td align="left">Random forest</td><td char="." align="char">0.6608</td><td char="." align="char">0.3844</td><td char="." align="char">0.5045</td><td char="." align="char">0.2675</td><td char="." align="char">1.1670</td><td char="." align="char">0.9187</td><td char="." align="char">0.7459</td><td char="." align="char">0.1391</td></tr><tr><td align="left">cTP-net</td><td char="." align="char">3.1963</td><td char="." align="char">1.3963</td><td char="." align="char">0.4893</td><td char="." align="char">0.4675</td><td char="." align="char">3.5971</td><td char="." align="char">1.522</td><td char="." align="char"><bold>0.8294</bold></td><td char="." align="char">0.1091</td></tr><tr><td align="left">PIKE-R2P</td><td char="." align="char"><bold>0.2446</bold></td><td char="." align="char"><bold>0.1703</bold></td><td char="." align="char"><bold>0.8640</bold></td><td char="." align="char"><bold>0.0636</bold></td><td char="." align="char"><bold>0.4397</bold></td><td char="." align="char"><bold>0.3360</bold></td><td char="." align="char">0.8144</td><td char="." align="char"><bold>0.0999</bold></td></tr></tbody></table><table-wrap-foot><p>The bold numbers represent the best performance among the compared models</p></table-wrap-foot></table-wrap></p>
      <p id="Par24">When the PCC scores are similar, a lower MSE score means the model prediction is closer to ground truth measurement. For example, let us look at the performance of cTP-net and PIKE-R2P on proteins CD14 and CD11c in PBMC. Interestingly, both models agreed that the PCC score of CD14 is 0.77 and that of CD11c is 0.91. However, for CD14, the MSE scores of PIKE-R2P and cTP-net are 0.19 and 4.43 respectively and similarly for CD11c. As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, while the PCC scores are equal between the two models, the predictions of cTP-net deviate from the diagonal, which means the predicted abundance is higher than the ground truth. Using Seurat v3.0 [<xref ref-type="bibr" rid="CR6">6</xref>], we divided the cells into different cell types based on RNA expression levels as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b. Furthermore, Fig. <xref rid="Fig1" ref-type="fig">1</xref>c, d show that CD14 and CD11c have high abundance values in Monocytes in the real measurement, which has been successfully captured by PIKE-R2P. However, the predictions by cTP-net have high values for the two proteins in almost all of the cells.<fig id="Fig1"><label>Fig. 1</label><caption><p>Visualization and comparison of results from cTP-net and PIKE-R2P on PBMC. The visualization results show that the lower the MSE scores, the closer the predicted protein abundances are to the ground truth. In <bold>c</bold> and <bold>d</bold>, from left to right are shown the ground truth, results of PIKE-R2P, and results of cTP-net of protein levels on RNA-based cell clusters in <bold>b</bold></p></caption><graphic xlink:href="12859_2021_4022_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par25">To test whether clustering based on the protein data can distinguish cell types more accurately than that based on RNA data, we compared cell clustering results based on the protein abundance values both of ground truth and predicted by PIKE-R2P to RNA-based clustering, and the results are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. To cluster the cell types, we used the method of UMAP as implemented in the Seurat v3.0 package. UMAP reduces the dimensionality of data to visualize clustering results [<xref ref-type="bibr" rid="CR35">35</xref>]. Besides, we calculated the Silhouette Coefficient (SC) scores as a quantitative metric to evaluate the performance of clustering. In Fig. <xref rid="Fig2" ref-type="fig">2</xref>a, we find that, when using the RNA data to cluster the cells, CD8<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{+}$$\end{document}</tex-math><mml:math id="M4"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq2.gif"/></alternatives></inline-formula> T cells and CD4<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{+}$$\end{document}</tex-math><mml:math id="M6"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq3.gif"/></alternatives></inline-formula> T cells are mixed in the same cluster, but when using the ground truth protein data to cluster the cells in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b, CD8<inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{+}$$\end{document}</tex-math><mml:math id="M8"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq4.gif"/></alternatives></inline-formula> T cells and CD4<inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{+}$$\end{document}</tex-math><mml:math id="M10"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq5.gif"/></alternatives></inline-formula> T cells are in two different groups. Moreover, NK cells, Monocytes, and Pre-B cells in the CBMC dataset are difficult to distinguish with RNA-based clustering as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a. By contrasts, in the clustering result based on the ground truth protein data as in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b, those three cell types are well separated. Using the protein abundances predicted by PIKE-R2P, the cell types can also be easily distinguished from each other, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>c. Using the protein abundances predicted by cTP-net, however, CD8<inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{+}$$\end{document}</tex-math><mml:math id="M12"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq6.gif"/></alternatives></inline-formula> T cells and CD4<inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{+}$$\end{document}</tex-math><mml:math id="M14"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq7.gif"/></alternatives></inline-formula> T cells in CBMC are still mixed, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>d.<fig id="Fig2"><label>Fig. 2</label><caption><p>Visualizations of cell clustering results based on different data. The data predicted by PIKE-R2P disperses different cell clusters almost equally well as the ground truth protein data on the CBMC dataset. <bold>a</bold> cell clustering result based on RNA, SC = 0.069; <bold>b</bold> cell clustering result based on ground truth measurement of protein abundance, SC = 0.305; <bold>c</bold> cell clustering result based on PIKE-R2P prediction, SC = 0.309; <bold>d</bold> cell clustering result based on cTP-net prediction, SC = 0.135</p></caption><graphic xlink:href="12859_2021_4022_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par26">Protein abundance levels from the ground truth and the predictions of two models are visualized on RNA-based cell clustering in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. We find that, for most proteins predicted by PIKE-R2P, the distribution of protein levels across the cell clusters is similar to the ground truth. Each protein is highly expressed in its corresponding cell type annotated based on RNAs. For example, in the ground truth, CD3 is highly expressed in T cells and monocytes, and CD8 is highly expressed in CD8<inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{+}$$\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq8.gif"/></alternatives></inline-formula> T cells and NK cells. In this regard, our PIKE-R2P model is able to make predictions similar to the ground truth. However, it is not the case for cTP-net. For instance, cTP-net predicts that CD3 is highly expressed in NK cells and Pre-B cells, and so is CD8 in monocytes. The protein abundances predicted by cTP-net tend to be high on most cell types, which makes it difficult to distinguish the cell types by the predicted protein abundances.<fig id="Fig3"><label>Fig. 3</label><caption><p>Protein levels on RNA-based cell clustering results on CBMC data. The results predicted by PIKE-R2P are more similar to the ground truth than cTP-net</p></caption><graphic xlink:href="12859_2021_4022_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Module analysis</title>
      <p id="Par27">For noise reduction, we used the pre-trained model of SAVER-X to process the original data. SAVER-X is a self-supervised learning model based on auto-encoder. The pre-trained model of SAVER-X has somehow captured the distributions of RNAs among single cells, and thereby it could filter out some noise that could have made the data not fit the distributions well. Compared with the results without using SAVER-X, we found that the data pre-processing using SAVER-X significantly improved the performance of our model, and made our model converge faster (data not shown).</p>
      <p id="Par28">We further investigated the influence of prior knowledge on the PIKE-R2P model. Our experiment included seven conditions, i.e. no prior knowledge, adding empirically determined interaction, database annotated, automated text mining, combined score, gene co-occurrence, and merging with these five kinds of prior knowledge. To even out the fluctuations of result due to random initialization of the parameter values, we did 5 repeated experiments in each case. Besides, to reduce the effect of overfitting, we ran 450 epochs in each case, and keep the minimum MSE value among the epochs as defined in Eq. <xref rid="Equ9" ref-type="">9</xref>. For all the experimental results of each group, we calculated the average between the maximum and the minimum values of the scores among the 5 runs and gave the difference between the maximum score and the average in each group of experiments.</p>
      <p id="Par29">The results are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. In general, adding prior knowledge can slightly improve the model performance. For different features, if the prior knowledge reflects biological characteristics, such as combined score, empirically determined interaction, and gene co-occurrence, the model improves more than others. When merging all the 5 types of prior knowledge features, the performance of the model improves the most. However, the scores are very close to each other among the conditions in Table <xref rid="Tab3" ref-type="table">3</xref>. One reason could be that the knowledge information is far less rich than the RNA data, and thus the RNA data are in a dominant position.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Impact of prior knowledge embedding on model performance of PIKE-R2P</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="2">CBMC</th><th align="left" colspan="2">PBMC</th></tr><tr><th align="left">PCC</th><th align="left">MSE</th><th align="left">PCC</th><th align="left">MSE</th></tr></thead><tbody><tr><td align="left">No prior knowledge</td><td char="±" align="char">0.8452 ± 0.0020</td><td char="±" align="char">0.1960 ± 0.0022</td><td char="±" align="char">0.8119 ± 0.0049</td><td char="±" align="char">0.4432 ± 0.0043</td></tr><tr><td align="left">Empirically determined interaction</td><td char="±" align="char"><bold>0.8464</bold> ± 0.0011</td><td char="±" align="char">0.1958 ± 0.0018</td><td char="±" align="char">0.8159 ± 0.0038</td><td char="±" align="char">0.4306 ± 0.0073</td></tr><tr><td align="left">Automated text mining</td><td char="±" align="char">0.8456 ± 0.0011</td><td char="±" align="char">0.1953 ± 0.0014</td><td char="±" align="char">0.8165 ± 0.0012</td><td char="±" align="char">0.4337 ± 0.0055</td></tr><tr><td align="left">Database annotated</td><td char="±" align="char">0.8460 ± 0.0031</td><td char="±" align="char">0.1957 ± 0.0018</td><td char="±" align="char">0.8163 ± 0.0030</td><td char="±" align="char">0.4320 ± 0.0068</td></tr><tr><td align="left">Combined score</td><td char="±" align="char">0.8459 ± 0.0029</td><td char="±" align="char">0.1952 ± 0.0060</td><td char="±" align="char">0.8162 ± 0.0020</td><td char="±" align="char">0.4333 ± 0.0072</td></tr><tr><td align="left">Gene co-occurrence</td><td char="±" align="char">0.8442 ± 0.0012</td><td char="±" align="char"><bold>0.1944</bold> ± 0.0027</td><td char="±" align="char">0.8165 ± 0.0019</td><td char="±" align="char">0.4329 ± 0.0039</td></tr><tr><td align="left">Merge 5 features</td><td char="±" align="char"><bold>0.8462</bold> ± 0.0037</td><td char="±" align="char"><bold>0.1944</bold> ± 0.0035</td><td char="±" align="char"><bold>0.8181</bold> ± 0.0013</td><td char="±" align="char"><bold>0.4303</bold> ± 0.0083</td></tr></tbody></table><table-wrap-foot><p>The bold numbers represent the best performance. Note that on the CBMC dataset, for either PCC or MSE, the best and the second best scores are very close to each other, so both results are in bold</p></table-wrap-foot></table-wrap></p>
      <p id="Par30">To further illustrate the power of adding the prior knowledge, we conducted an experiment by merging the two datasets (i.e. CBMC and PBMC) into one artificial dataset, comprising 16,603 types of RNA that overlap between CBMC and PBMC (i.e. the intersection). Then, we added the training sets from CBMC and PBMC together to get 11,558 cells in the merged training set; likewise, we got 4941 cells in the merged test set. We ran PIKE-R2P 15 times for both the condition of using no prior knowledge and the condition of adding prior knowledge with all the 5 features. The box plots in Fig. <xref rid="Fig4" ref-type="fig">4</xref> show that adding prior knowledge can significantly improve the performance of our model on the merged dataset. The results also show that the variances of both PCC and MSE of the model without prior knowledge are larger than the model with knowledge embedding.<fig id="Fig4"><label>Fig. 4</label><caption><p>The effect of the knowledge on the performance of PIKE-R2P on the artificial dataset merging CBMC and PBMC. <bold>a</bold> The PCC scores. The median and mean PCC scores are 0.8170 and 0.8214 with knowledge but they are 0.8008 and 0.7784 without knowledge. <bold>b</bold> The MSE scores. The median and mean are 0.5471 and 0.5494 with knowledge but are 0.5631 and 0.6254 without knowledge. In each boxplot, the green triangle marks the position of the average value, and the orange line makes the median value</p></caption><graphic xlink:href="12859_2021_4022_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec6">
    <title>Discussion</title>
    <p id="Par31">In our experiments, Random Forest was more computationally expensive than the neural network-based models (data not shown). This could be due to the sharing of RNA features among different proteins which are reused by neural network models so that some of the model retraining can be avoided, whereas the Random Forest method does the whole feature engineering for every target protein.</p>
    <p id="Par32">We have used the PPI network as prior knowledge. Similarly, several other sources of prior information are available in the literature, including gene ontologies and text mining databases. Each data source could provide additional information while reducing inherent noise in the data. As a future extension, the incorporation of multiple data sources in the model may provide a better prediction framework.</p>
    <p id="Par33">In our work, we predicted proteins using the CITE-seq dataset, where the measurements were performed on blood samples. It has been shown that single-cell gene expression patterns tend to be tissue specific [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]. A transfer learning framework may help train a model from a large known dataset of one tissue while predicting gene expressions in other tissues. A similar approach of transfer learning could also be used to compare different sequencing platforms (e.g. CITE-seq and REAP-seq). In both cases, a model based on graph neural networks incorporating prior knowledge may provide good model performance and biological insights.</p>
  </sec>
  <sec id="Sec7">
    <title>Conclusion</title>
    <p id="Par34">Recently emerging single-cell multi-omics techniques can measure RNA and protein abundances simultaneously in the same cells. Based on such data, machine learning models have been proposed to predict protein abundances based on RNA abundances at the single-cell level. However, their performances can be further improved.</p>
    <p id="Par35">In this paper, we proposed PIKE-R2P, a machine learning method based on graph neural network (GNN) and knowledge embedding. The key idea is that target proteins often share mechanisms of gene expression regulation from transcription to translation. PIKE-R2P captures such relations by embedding the prior knowledge of protein–protein interactions into a GNN. Through information propagation among nodes of the GNN, the model can make better use of information from the RNA-seq data, and thereby improve its prediction performance. Our results on real CITE-seq data demonstrated that PIKE-R2P significantly out-performed existing methods, indicating the value of adding knowledge to neural network models. In the future, more sources of knowledge and more modalities of single-cell data can be integrated through GNN, not only improving prediction performance, but also paving the way for interpretable machine learning in bioinformatics.</p>
  </sec>
  <sec id="Sec8">
    <title>Methods</title>
    <sec id="Sec9">
      <title>Overview</title>
      <p id="Par36">
        <fig id="Fig5">
          <label>Fig. 5</label>
          <caption>
            <p>The method of PIKE-R2P. <bold>a</bold> is the whole pipeline and the model structure. The pipeline includes data denoising, model training, and testing. The green matrix represents denoised RNA data; the blue matrix is the high-dimensional representation of the RNA data; the orange vectors are the features of proteins and the orange lines correspond to the edges in the PPI network; the purple vectors are representations of the prior knowledge. <bold>b</bold> The knowledge embedding structure. The prior knowledge of each protein is mapped into a high-dimensional feature matrix. Then the attention mechanism is used to select the features. After that, the weights of protein interaction pairs linked to the same protein are adjusted. Finally, these feature vectors are concatenated together into one matrix as the prior knowledge embedding</p>
          </caption>
          <graphic xlink:href="12859_2021_4022_Fig5_HTML" id="MO5"/>
        </fig>
      </p>
      <p id="Par37">The main idea of our method is to integrate the PPI-based information as prior knowledge into a graph neural network, to capture the relationships between proteins and RNAs as well as among proteins, and thereby to improve the accuracy of protein abundance prediction. The whole pipeline is described in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a and Algorithm 1. After noise reduction by SAVER-X, we divide the cells into two disjoint datasets, i.e. a training set and a test set. For training, we feed the training set to the model for parameter estimation and save the parameter values that correspond to the minimum MSE loss among all the epochs that have been computed. During the test, the model loads these parameters, and predicts the protein abundances of the cells in the test set directly.</p>
      <graphic position="anchor" xlink:href="12859_2021_4022_Figa_HTML" id="MO6"/>
      <p id="Par38">Our model mainly consists of two modules. The first one is adding the PPI-based graph neural network to the dataset, shown as the “PPI-based graph neural network part” in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a. These protein–protein interactions provide a way for information transmission between proteins, which means the proteins jointly promote specific biological functions, e.g. by inhibiting or promoting each other [<xref ref-type="bibr" rid="CR31">31</xref>]. Intuitively, we encode the PPIs with a graph structure, where the nodes are proteins, and edges represent the interactions. Thus, we use the graph neural network to compute the result of information transmission through these interactions between proteins. The other module is the embedding of prior knowledge, such as co-expression and gene co-occurence, etc., which is described in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a. Since PPI relationships tend to be conserved across different cell types [<xref ref-type="bibr" rid="CR31">31</xref>], the PPI in large-scale databases such as STRING can be used for the knowledge embedding.</p>
      <p id="Par39">The whole structure of the model is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a. The input is the denoised data from SAVER-X. Then, similar to cTP-net [<xref ref-type="bibr" rid="CR7">7</xref>], we extract the RNA representation from the input RNA data using a neural network for feature extraction, which includes two fully-connected layers, shown as the blue part in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a. After that, to represent the features of <italic>N</italic> proteins in the high-dimensional space independently, we used <italic>N</italic> 1-layer forward networks to map the RNA representation to <italic>N</italic> protein feature vectors, and combined all the feature vectors of the proteins into matrix <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V_r\in {\mathbb {R}}^{N\times {d_r}}$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq9.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_r$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq10.gif"/></alternatives></inline-formula> is the number of dimensions of the protein representations, shown as the orange vectors in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a. Besides, the prior knowledge from different sources is embedded into matrix <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V_k\in {\mathbb {R}}^{N\times {d_k}}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq11.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_k$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq12.gif"/></alternatives></inline-formula> is the number of dimensions of the target vector space of the knowledge embedding, shown as the purple matrices in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a. By concatenating the column vectors from the two matrices that correspond to the same protein, the high-dimensional representation of each protein is<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} v_i=v_{r_i}\oplus {v_{k_i}}, \end{aligned}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_i\in {\mathbb {R}}^{1\times {d}}, i=1,2,\dots ,N$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq13.gif"/></alternatives></inline-formula>, <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d=d_r+d_k$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq14.gif"/></alternatives></inline-formula> and <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\oplus$$\end{document}</tex-math><mml:math id="M32"><mml:mo>⊕</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq15.gif"/></alternatives></inline-formula> is the concatenation operation. Thus, the PPI network has the set of nodes <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V=\{v_1,v_2,\dots ,v_N\}$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq16.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V\in {\mathbb {R}}^{N\times {d}}$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq17.gif"/></alternatives></inline-formula>. Moreover, the interactions between the proteins are represented as the set of edges <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E\subseteq {V\times {V}}$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⊆</mml:mo><mml:mrow><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq18.gif"/></alternatives></inline-formula>. Therefore, graph <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G= (V, E)$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq19.gif"/></alternatives></inline-formula> represents the PPI network, as shown in the PPI-based Graph Neural Network part in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a. To model the information transmission in the PPI network, we apply algorithms of graph neural network on <italic>G</italic>. After that, to map the <italic>N</italic> representations in <italic>d</italic> dimensions to the abundance values <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{Y}\in {\mathbb {R}}^{N\times {1}}$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq20.gif"/></alternatives></inline-formula>, we reduce the dimensions of the node vectors from <italic>d</italic> to 1 through the predictor which is a 1-layer feed-forward network.</p>
      <graphic position="anchor" xlink:href="12859_2021_4022_Figb_HTML" id="MO8"/>
    </sec>
    <sec id="Sec10">
      <title>PPI-based graph neural network</title>
      <p id="Par40">In this paper, we assume that the proteins whose abundances are to be predicted have some relations with each other. Such relations could be due to physical interactions, crosstalk between signaling pathways, shared mechanisms of gene regulation from transcription to translation, or some other functional relationships. For convenience, we consider such relations as “protein–protein interactions” (PPIs) in the general sense, i.e. the PPIs include both direct and indirect interactions. A PPI network is naturally represented as an undirected graph denoted by <inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G = (V, E)$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq21.gif"/></alternatives></inline-formula>, where each node in <italic>V</italic> corresponds to a protein and each edge in <italic>E</italic> corresponds to the interaction between two proteins.</p>
      <p id="Par41">To represent the edges in set <italic>E</italic>, we use a weight matrix <inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W\in {\mathbb {R}}^{d\times {d}}$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mi>W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq22.gif"/></alternatives></inline-formula> to capture the relations among the features of the proteins and we use an adjacency matrix <inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A\in {\mathbb {R}}^{N\times {N}}$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq23.gif"/></alternatives></inline-formula>containing edge weights to describe the connectivity among the proteins. The values in both matrices are initialized randomly and will be adjusted when the model is trained, according to the definition of graph neural network in [<xref ref-type="bibr" rid="CR36">36</xref>]. During the training, the nodes transmit feature information to each other, and the result is:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} V^{e}=\sigma (AVW), \end{aligned}$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>V</mml:mi><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where matrix <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V^e\in {\mathbb {R}}^{N\times {d}}$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq24.gif"/></alternatives></inline-formula> contains the node vectors transformed from the node vectors in <italic>V</italic> through <italic>A</italic>, <italic>W</italic> and the sigmoid function <inline-formula id="IEq25"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma (x)=\frac{1}{1+e^{-x}}$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq25.gif"/></alternatives></inline-formula>, which is applied to each element of matrix <italic>AVW</italic>. After that, we use a Feed-Forward (FF) layer to reduce the dimensions of the node features from <inline-formula id="IEq26"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\times {d}$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq26.gif"/></alternatives></inline-formula> to <inline-formula id="IEq27"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\times {1}$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq27.gif"/></alternatives></inline-formula>, where <italic>N</italic> is the number of proteins. Different from cTP-net [<xref ref-type="bibr" rid="CR7">7</xref>], which fits the Centered Log-ratio Range of protein abundance [<xref ref-type="bibr" rid="CR4">4</xref>] by the ReLu function <inline-formula id="IEq28"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ReLu(x)=max(0,x)$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq28.gif"/></alternatives></inline-formula>, we use the PReLu function <inline-formula id="IEq29"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PReLu(x)=max(0,x)+0.25\times min(0,x)$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.25</mml:mn><mml:mo>×</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq29.gif"/></alternatives></inline-formula> in the last layer to ensure that the model can predict values less than 0. Note that, in the CITE-seq data, the protein abundance values are log-transformed and thus could be negative sometimes. Thus, the output is<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{Y}=PReLu(FF(V^{e})). \end{aligned}$$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec11">
      <title>Prior knowledge</title>
      <p id="Par42">In the previous section we mainly built a PPI network from a specific dataset, but there is additional prior knowledge about PPI from other datasets. The STRING database collects information on PPI from different anngles such as co-expression and gene co-occurrence, etc. Therefore, we use this superset of PPI information to improve the model performance. To represent these features, we embed this prior knowledge into <inline-formula id="IEq30"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_k$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq30.gif"/></alternatives></inline-formula> dimensions, which adds constraints to the protein predictions in the graph neural network. The structure is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b and the algorithm is described in Algorithm 3.</p>
      <graphic position="anchor" xlink:href="12859_2021_4022_Figc_HTML" id="MO11"/>
      <p id="Par43">We use <italic>M</italic> independent features <inline-formula id="IEq31"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C=\{C_1,C_2,\dots , C_M\}$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq31.gif"/></alternatives></inline-formula> of the PPIs in the STRING database [<xref ref-type="bibr" rid="CR31">31</xref>]. Each feature <inline-formula id="IEq32"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_i$$\end{document}</tex-math><mml:math id="M70"><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq32.gif"/></alternatives></inline-formula> is represented by a graph with <italic>N</italic> protein nodes and <inline-formula id="IEq33"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N\times {N}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq33.gif"/></alternatives></inline-formula> edges represented by the interaction scores, where <italic>N</italic> is the number of proteins. We transform every <inline-formula id="IEq34"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_i$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq34.gif"/></alternatives></inline-formula> into an <inline-formula id="IEq35"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N \times N$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq35.gif"/></alternatives></inline-formula> adjacency matrix <inline-formula id="IEq36"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${C_i}'\in {\mathbb {R}}^{N\times {N}\times {1}}$$\end{document}</tex-math><mml:math id="M78"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq36.gif"/></alternatives></inline-formula>. When a protein is missing in the prior knowledge database, which means the connections of the protein with others are absent. We set the weights of the connections to 0. In order to obtain the high-dimensional features of each adjacent matrix, each column vector in matrix <inline-formula id="IEq37"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${C_i}'$$\end{document}</tex-math><mml:math id="M80"><mml:msup><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq37.gif"/></alternatives></inline-formula> is encoded by <italic>N</italic> 1-layer fully-connected networks with <inline-formula id="IEq38"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_c$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq38.gif"/></alternatives></inline-formula> dimensions and the result is <inline-formula id="IEq39"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{c_i}\in {\mathbb {R}}^{N\times {N}\times {d_c}}$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq39.gif"/></alternatives></inline-formula>. Then, through the attention mechanism defined in [<xref ref-type="bibr" rid="CR37">37</xref>], the importance scores of the features are merged into matrix <inline-formula id="IEq40"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_c\in {\mathbb {R}}^{N\times {N}\times {d_c}}$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq40.gif"/></alternatives></inline-formula>,<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} A_c &amp;= elu\left( \frac{1}{M}\sum _{i=1}^{M}(a_{c_i}W_{a_i}A_{c_i})\right) , \end{aligned}$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} a_{c_i}&amp;= \frac{\mathrm{exp}{(elu(A_{c_i}))}}{\sum _{e=1}^{N}\mathrm{exp}{(elu(A_{c_e}))}}, \end{aligned}$$\end{document}</tex-math><mml:math id="M90" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>a</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq41"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a_{c_i}$$\end{document}</tex-math><mml:math id="M92"><mml:msub><mml:mi>a</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq41.gif"/></alternatives></inline-formula> is the normalized attention coefficient, <inline-formula id="IEq42"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{a_i}$$\end{document}</tex-math><mml:math id="M94"><mml:msub><mml:mi>W</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq42.gif"/></alternatives></inline-formula> is the weighted matrix for the <italic>i</italic>-th coefficient, and <inline-formula id="IEq43"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$elu(x)=max(0,x)+min(0,\mathrm{exp}(x)-1)$$\end{document}</tex-math><mml:math id="M96"><mml:mrow><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq43.gif"/></alternatives></inline-formula>.</p>
      <p id="Par44">To combine the prior knowledge with each protein node to constrain the information transmission, we divide <inline-formula id="IEq44"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_c$$\end{document}</tex-math><mml:math id="M98"><mml:msub><mml:mi>A</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq44.gif"/></alternatives></inline-formula> into <italic>N</italic> submatrices <inline-formula id="IEq45"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{c_j}\in {\mathbb {R}}^{N\times {d_c}}$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq45.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq46"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0&lt;j\le {N}$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq46.gif"/></alternatives></inline-formula>, and each submatrix corresponds to one of the <italic>N</italic> proteins. To reflect different degrees of importance of the protein pairs, we need to re-weight all the relationships. In the following, <inline-formula id="IEq47"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_{k_j}\in {\mathbb {R}}^{N\times {d_c}}$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq47.gif"/></alternatives></inline-formula> represents the re-weighted relationships:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} a_{k_j}&amp;= \frac{\mathrm{exp}{(elu(A_{k_j}))}}{\sum _{e=1}^{N}\mathrm{exp}{(elu(A_{k_e}))}}, \end{aligned}$$\end{document}</tex-math><mml:math id="M106" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>a</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mi mathvariant="normal">exp</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} A_{k_j} &amp; = elu(a_{k_j}W_{k_j}A_{c_j}), \end{aligned}$$\end{document}</tex-math><mml:math id="M108" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq48"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a_{k_j}$$\end{document}</tex-math><mml:math id="M110"><mml:msub><mml:mi>a</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq48.gif"/></alternatives></inline-formula> is the normalized attention coefficient for the different constrained features. Because a pair of proteins may be influenced by multiple intermediate proteins, we concatenate all the prior knowledge of protein interactions for each node into a feature vector, as follows:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} V_k=A_{k_1}\oplus {A_{k_2}}\oplus \dots \oplus {A_{k_N}}, \end{aligned}$$\end{document}</tex-math><mml:math id="M112" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>⊕</mml:mo><mml:mo>⋯</mml:mo><mml:mo>⊕</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq49"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V_k\in {\mathbb {R}}^{N\times {d_k}}, d_k=N\times {d_c}$$\end{document}</tex-math><mml:math id="M114"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq49.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq50"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\oplus$$\end{document}</tex-math><mml:math id="M116"><mml:mo>⊕</mml:mo></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq50.gif"/></alternatives></inline-formula> is the concatenation operation.</p>
    </sec>
    <sec id="Sec12">
      <title>Model training</title>
      <p id="Par45">Before training, we set the parameters for the model. In the fully connected layers, the hidden sizes are 1024 and 128 for the numbers of output neurons of the two hidden layers for the RNA representation and 32 hidden neurons in the connected layer for the prior knowledge embedding. In the feed-forward network, we set <inline-formula id="IEq51"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_r$$\end{document}</tex-math><mml:math id="M118"><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq51.gif"/></alternatives></inline-formula> to 64, <inline-formula id="IEq52"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_c$$\end{document}</tex-math><mml:math id="M120"><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq52.gif"/></alternatives></inline-formula> to 32 and <inline-formula id="IEq53"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_k$$\end{document}</tex-math><mml:math id="M122"><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq53.gif"/></alternatives></inline-formula> to <inline-formula id="IEq54"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_c \times N$$\end{document}</tex-math><mml:math id="M124"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq54.gif"/></alternatives></inline-formula>. The number of nodes <italic>N</italic> in our graph neural network depends on the dataset, i.e., <inline-formula id="IEq55"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=10$$\end{document}</tex-math><mml:math id="M126"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq55.gif"/></alternatives></inline-formula> for PBMC and <inline-formula id="IEq56"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=13$$\end{document}</tex-math><mml:math id="M128"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq56.gif"/></alternatives></inline-formula> for CBMC. Thus, <inline-formula id="IEq57"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_k=320$$\end{document}</tex-math><mml:math id="M130"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>320</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq57.gif"/></alternatives></inline-formula>, <inline-formula id="IEq58"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d=d_r+d_k=384$$\end{document}</tex-math><mml:math id="M132"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>384</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq58.gif"/></alternatives></inline-formula> for PBMC, and <inline-formula id="IEq59"><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_k=416$$\end{document}</tex-math><mml:math id="M134"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>416</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq59.gif"/></alternatives></inline-formula>, <inline-formula id="IEq60"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d=d_r+d_k=480$$\end{document}</tex-math><mml:math id="M136"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>480</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq60.gif"/></alternatives></inline-formula> for CBMC.</p>
      <p id="Par46">For the training, we set the number of epochs to 350 and batch size to 32. For the optimization of loss function based on mean squared error (MSE), we first set the global <inline-formula id="IEq61"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MSE_{loss}'$$\end{document}</tex-math><mml:math id="M138"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">loss</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq61.gif"/></alternatives></inline-formula> to an infinite value. In each epoch, if the current <inline-formula id="IEq62"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MSE_{loss}$$\end{document}</tex-math><mml:math id="M140"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">loss</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq62.gif"/></alternatives></inline-formula> is smaller than the global <inline-formula id="IEq63"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MSE_{loss}'$$\end{document}</tex-math><mml:math id="M142"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">loss</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq63.gif"/></alternatives></inline-formula>, we update <inline-formula id="IEq64"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MSE_{loss}'$$\end{document}</tex-math><mml:math id="M144"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msubsup><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">loss</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq64.gif"/></alternatives></inline-formula> to <inline-formula id="IEq65"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MSE_{loss}$$\end{document}</tex-math><mml:math id="M146"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">loss</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq65.gif"/></alternatives></inline-formula>, and save the model parameters of this epoch. We assume that all proteins have equal weights in the MSE loss:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MSE_{loss}(Y,\hat{Y})=\sum _{i=1}^{N}{(\hat{y}_i-y_i)}^2, \end{aligned}$$\end{document}</tex-math><mml:math id="M148" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="italic">loss</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4022_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <italic>Y</italic> contains the ground truth measurements and <inline-formula id="IEq66"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{Y}$$\end{document}</tex-math><mml:math id="M150"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq66.gif"/></alternatives></inline-formula> is the set of the predicted protein abundances. The initial learning rate is set to <inline-formula id="IEq67"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-6}$$\end{document}</tex-math><mml:math id="M152"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4022_Article_IEq67.gif"/></alternatives></inline-formula>. The model parameters are estimated based on the minimization of MSE loss and the Adam optimizer by back propagation.</p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>PIKE-R2P</term>
        <def>
          <p id="Par4">Protein–protein interaction network-based knowledge embedding with graph neural network for single-cell RNA to protein prediction</p>
        </def>
      </def-item>
      <def-item>
        <term>PPI</term>
        <def>
          <p id="Par5">Protein–protein interaction</p>
        </def>
      </def-item>
      <def-item>
        <term>CBMC</term>
        <def>
          <p id="Par6">Cord blood mononuclear cells</p>
        </def>
      </def-item>
      <def-item>
        <term>PBMC</term>
        <def>
          <p id="Par7">Peripheral blood mononuclear cells</p>
        </def>
      </def-item>
      <def-item>
        <term>PCC</term>
        <def>
          <p id="Par8">Pearson correlation coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>MSE</term>
        <def>
          <p id="Par9">Mean squared error</p>
        </def>
      </def-item>
      <def-item>
        <term>SD</term>
        <def>
          <p id="Par10">Standard deviation</p>
        </def>
      </def-item>
      <def-item>
        <term>GNN</term>
        <def>
          <p id="Par11">Graph neural network</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
    <sec id="d32e3373">
      <title>About this supplement</title>
      <p id="Par47">This article has been published as part of BMC Bioinformatics Volume 22 Supplement 6, 2021: 19th International Conference on Bioinformatics 2020 (InCoB2020). The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-22-supplement-6">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-22-supplement-6</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>XD and JZ conceived the project. XD developed the algorithm. XD and JZ wrote the manuscript. FX, SW, and PAM helped collect the data and revised the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This research was supported by a startup grant from the ShanghaiTech University. Publication costs are funded by the same startup grant. The funding body had no role in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The raw data is at <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE100866">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE100866</ext-link>. The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/JieZheng-ShanghaiTech/PIKE-R2P">https://github.com/JieZheng-ShanghaiTech/PIKE-R2P</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar2">
      <title>Ethics approval and consent to participate</title>
      <p id="Par48">Not applicable.</p>
    </notes>
    <notes id="FPar3">
      <title>Consent for publication</title>
      <p id="Par49">Not applicable.</p>
    </notes>
    <notes id="FPar4" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par50">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Yong</surname>
            <given-names>KW</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>JY</given-names>
          </name>
          <name>
            <surname>Cowie</surname>
            <given-names>AC</given-names>
          </name>
        </person-group>
        <article-title>Single-cell RNA sequencing and its combination with protein and DNA analyses</article-title>
        <source>Cells</source>
        <year>2020</year>
        <volume>9</volume>
        <issue>5</issue>
        <fpage>1130</fpage>
        <pub-id pub-id-type="doi">10.3390/cells9051130</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Patterson</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Aebersold</surname>
            <given-names>RH</given-names>
          </name>
        </person-group>
        <article-title>Proteomics: the first decade and beyond</article-title>
        <source>Nat Genet</source>
        <year>2003</year>
        <volume>33</volume>
        <issue>3</issue>
        <fpage>311</fpage>
        <lpage>323</lpage>
        <pub-id pub-id-type="doi">10.1038/ng1106</pub-id>
        <pub-id pub-id-type="pmid">12610541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McManus</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Vogel</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Next-generation analysis of gene expression regulation-comparing the roles of synthesis and degradation</article-title>
        <source>Mol Biosyst</source>
        <year>2015</year>
        <volume>11</volume>
        <issue>10</issue>
        <fpage>2680</fpage>
        <lpage>2689</lpage>
        <pub-id pub-id-type="doi">10.1039/C5MB00310E</pub-id>
        <pub-id pub-id-type="pmid">26259698</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stoeckius</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hafemeister</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Stephenson</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Houck-Loomis</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chattopadhyay</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Swerdlow</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Satija</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Smibert</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Simultaneous epitope and transcriptome measurement in single cells</article-title>
        <source>Nat Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <issue>9</issue>
        <fpage>865</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4380</pub-id>
        <pub-id pub-id-type="pmid">28759029</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Beyer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Aebersold</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>On the dependency of cellular protein levels on mRNA abundance</article-title>
        <source>Cell</source>
        <year>2016</year>
        <volume>165</volume>
        <issue>3</issue>
        <fpage>535</fpage>
        <lpage>550</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2016.03.014</pub-id>
        <pub-id pub-id-type="pmid">27104977</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stuart</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Butler</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hoffman</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Hafemeister</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Papalexi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Mauck</surname>
            <given-names>WM</given-names>
            <suffix>III</suffix>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Stoeckius</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Smibert</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Satija</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive integration of single-cell data</article-title>
        <source>Cell</source>
        <year>2019</year>
        <volume>177</volume>
        <issue>7</issue>
        <fpage>1888</fpage>
        <lpage>1902</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2019.05.031</pub-id>
        <pub-id pub-id-type="pmid">31178118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>NR</given-names>
          </name>
        </person-group>
        <article-title>Surface protein imputation from single cell transcriptomes by deep neural networks</article-title>
        <source>Nat Commun</source>
        <year>2020</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1038/s41467-019-13993-7</pub-id>
        <pub-id pub-id-type="pmid">31911652</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Alfassy A, Karlinsky L, Aides A, Shtok J, Harary S, Feris R, Giryes R, Bronstein AM. Laso: label-set operations networks for multi-label few-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition, 2019. p. 6548–57.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Du</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xiang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Ml-net: multi-label classification of biomedical texts with deep neural networks</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2019</year>
        <volume>26</volume>
        <issue>11</issue>
        <fpage>1279</fpage>
        <lpage>1285</lpage>
        <pub-id pub-id-type="doi">10.1093/jamia/ocz085</pub-id>
        <pub-id pub-id-type="pmid">31233120</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Liu J, Chang W-C, Wu Y, Yang Y. Deep learning for extreme multi-label text classification. In: Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, 2017. p. 115–24.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Adaptive multi-view multi-label learning for identifying disease-associated candidate miRNAs</article-title>
        <source>PLoS Comput Biol</source>
        <year>2019</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>1006931</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006931</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>Advances in predicting subcellular localization of multi-label proteins and its implication for developing multi-target drugs</article-title>
        <source>Curr Med Chem</source>
        <year>2019</year>
        <volume>26</volume>
        <issue>26</issue>
        <fpage>4918</fpage>
        <lpage>4943</lpage>
        <pub-id pub-id-type="doi">10.2174/0929867326666190507082559</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>mldeepre: multi-functional enzyme function prediction with hierarchical multi-label deep learning</article-title>
        <source>Front Genet</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>714</fpage>
        <pub-id pub-id-type="doi">10.3389/fgene.2018.00714</pub-id>
        <pub-id pub-id-type="pmid">30723495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Chen Z-M, Wei X-S, Wang P, Guo Y. Multi-label image recognition with graph convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition, 2019. p. 5177–86.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Nguyen TH, Grishman R. Graph convolutional networks with argument-aware pooling for event detection. In: 32nd AAAI conference on artificial intelligence, 2018.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Fernandes P, Allamanis M, Brockschmidt M. Structured neural summarization, 2018.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Norcliffe-Brown W, Vafeias S, Parisot S. Learning conditioned graph structures for interpretable visual question answering. In: Advances in neural information processing systems, 2018. p. 8334–8343.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Yan S, Xiong Y, Lin D. Spatial temporal graph convolutional networks for skeleton-based action recognition. In: 232nd AAAI conference on artificial intelligence, 2018.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Fout A, Byrd J, Shariat B, Ben-Hur A. Protein interface prediction using graph convolutional networks. In: Advances in neural information processing systems, 2017. p. 6530–9.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lim</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ryu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Choe</surname>
            <given-names>YJ</given-names>
          </name>
          <name>
            <surname>Ham</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>WY</given-names>
          </name>
        </person-group>
        <article-title>Predicting drug-target interaction using a novel graph neural network with 3d structure-embedded graph representation</article-title>
        <source>J Chem Inf Model</source>
        <year>2019</year>
        <volume>59</volume>
        <issue>9</issue>
        <fpage>3981</fpage>
        <lpage>3988</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00387</pub-id>
        <pub-id pub-id-type="pmid">31443612</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Marino K, Salakhutdinov R, Gupta A. The more you know: using knowledge graphs for image classification. 2016. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1612.04844">arXiv:1612.04844</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Gong K, Gao Y, Liang X, Shen X, Wang M, Lin L. Graphonomy: universal human parsing via graph transfer learning, 2019. p. 7450–7459.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Wang X, He X, Cao Y, Liu M, Chua T-S. Kgat: knowledge graph attention network for recommendation. In: Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery and data mining, 2019. p. 950–8.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Huang X, Zhang J, Li D, Li P. Knowledge graph embedding based question answering. In: Proceedings of the 12th ACM international conference on web search and data mining, 2019. p. 105–13.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Sousa Abreu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Penalva</surname>
            <given-names>LO</given-names>
          </name>
          <name>
            <surname>Marcotte</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Vogel</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Global signatures of protein and mRNA expression levels</article-title>
        <source>Mol BioSyst</source>
        <year>2009</year>
        <volume>5</volume>
        <issue>12</issue>
        <fpage>1512</fpage>
        <lpage>1526</lpage>
        <?supplied-pmid 20023718?>
        <pub-id pub-id-type="pmid">20023718</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reuveni</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Meilijson</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Kupiec</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ruppin</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Tuller</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Genome-scale analysis of translation elongation with a ribosome flow model</article-title>
        <source>PLoS Comput Biol</source>
        <year>2011</year>
        <volume>7</volume>
        <issue>9</issue>
        <fpage>1002127</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002127</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frith</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Pheasant</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mattick</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>The amazing complexity of the human transcriptome</article-title>
        <source>Eur J Human Genet</source>
        <year>2005</year>
        <volume>13</volume>
        <issue>8</issue>
        <fpage>894</fpage>
        <pub-id pub-id-type="doi">10.1038/sj.ejhg.5201459</pub-id>
        <pub-id pub-id-type="pmid">15970949</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mehdi</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Patrick</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bailey</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Boden</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Predicting the dynamics of protein abundance</article-title>
        <source>Mol Cell Proteomics</source>
        <year>2014</year>
        <volume>13</volume>
        <issue>5</issue>
        <fpage>1330</fpage>
        <lpage>1340</lpage>
        <pub-id pub-id-type="doi">10.1074/mcp.M113.033076</pub-id>
        <pub-id pub-id-type="pmid">24532840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Siddiqui</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Joint learning improves protein abundance prediction in cancers</article-title>
        <source>BMC Biol</source>
        <year>2019</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1186/s12915-018-0614-4</pub-id>
        <pub-id pub-id-type="pmid">30616566</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Mundra</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Ensemble learning models that predict surface protein abundance from single-cell multimodal omics data</article-title>
        <source>Methods.</source>
        <year>2021</year>
        <volume>189</volume>
        <fpage>65</fpage>
        <lpage>73</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2020.10.001</pub-id>
        <pub-id pub-id-type="pmid">33039573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Szklarczyk</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Gable</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Lyon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Junge</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wyder</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huerta-Cepas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Simonovic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Doncheva</surname>
            <given-names>NT</given-names>
          </name>
          <name>
            <surname>Morris</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Bork</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>String v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets</article-title>
        <source>Nucleic Acids Res</source>
        <year>2019</year>
        <volume>47</volume>
        <issue>D1</issue>
        <fpage>607</fpage>
        <lpage>613</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky1131</pub-id>
        <pub-id pub-id-type="pmid">30335158</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Wang J, Agarwal D, Huang M, Hu G, Zhou Z, Conley V, MacMullan H, Zhang NR. Transfer learning in single-cell transcriptomics improves data denoising and pattern discovery. 2018. bioRxiv, 457879</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <year>2001</year>
        <volume>45</volume>
        <issue>1</issue>
        <fpage>5</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vanderplas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Passos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Brucher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Perrot</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Duchesnay</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Scikit-learn: machine learning in python</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McInnes</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Healy</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Saul</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Großberger</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Umap: uniform manifold approximation and projection for dimension reduction</article-title>
        <source>J Open Source Softw</source>
        <year>2018</year>
        <volume>3</volume>
        <issue>29</issue>
        <fpage>861</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.00861</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Kipf TN, Welling M. Semi-supervised classification with graph convolutional networks. In: International conference on learning representations, 2017.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Advances in neural information processing systems, 2017. p. 5998–6008.</mixed-citation>
    </ref>
  </ref-list>
</back>
