<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7280338</article-id>
    <article-id pub-id-type="publisher-id">1314</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-019-01314-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GlassesViewer: Open-source software for viewing and analyzing data from the Tobii Pro Glasses 2 eye tracker</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4672-8756</contrib-id>
        <name>
          <surname>Niehorster</surname>
          <given-names>Diederick C.</given-names>
        </name>
        <address>
          <email>diederick_c.niehorster@humlab.lu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hessels</surname>
          <given-names>Roy S.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Benjamins</surname>
          <given-names>Jeroen S.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.4514.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0930 2361</institution-id><institution>Lund University Humanities Laboratory and Department of Psychology, </institution><institution>Lund University, </institution></institution-wrap>Lund, Sweden </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5477.1</institution-id><institution-id institution-id-type="ISNI">0000000120346234</institution-id><institution>Experimental Psychology, Helmholtz Institute and Developmental Psychology, </institution><institution>Utrecht University, </institution></institution-wrap>Utrecht, The Netherlands </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.5477.1</institution-id><institution-id institution-id-type="ISNI">0000000120346234</institution-id><institution>Experimental Psychology, Helmholtz Institute and Social, Health and Organisational Psychology, </institution><institution>Utrecht University, </institution></institution-wrap>Utrecht, The Netherlands </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>2</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>2</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2020</year>
    </pub-date>
    <volume>52</volume>
    <issue>3</issue>
    <fpage>1244</fpage>
    <lpage>1253</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">We present GlassesViewer, open-source software for viewing and analyzing eye-tracking data of the Tobii Pro Glasses 2 head-mounted eye tracker as well as the scene and eye videos and other data streams (pupil size, gyroscope, accelerometer, and TTL input) that this headset can record. The software provides the following functionality written in MATLAB: (1) a graphical interface for navigating the study- and recording structure produced by the Tobii Glasses 2; (2) functionality to unpack, parse, and synchronize the various data and video streams comprising a Glasses 2 recording; and (3) a graphical interface for viewing the Glasses 2’s gaze direction, pupil size, gyroscope and accelerometer time-series data, along with the recorded scene and eye camera videos. In this latter interface, segments of data can furthermore be labeled through user-provided event classification algorithms or by means of manual annotation. Lastly, the toolbox provides integration with the GazeCode tool by Benjamins et al. (<xref ref-type="bibr" rid="CR2">2018</xref>), enabling a completely open-source workflow for analyzing Tobii Pro Glasses 2 recordings.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Head-mounted eye tracking</kwd>
      <kwd>Wearable eye tracking</kwd>
      <kwd>Mobile eye tracking</kwd>
      <kwd>Eye movements</kwd>
      <kwd>Data analysis</kwd>
      <kwd>Event classification</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">In the past several decades, mobile (head-worn) eye tracking has become a popular research method that has found widespread use across a range of fields. The opportunity afforded by head-worn eye-tracking setups to acquire data on the visual behavior of participants who freely move around has enabled researchers to conduct studies in fields such as vision science (Land, <xref ref-type="bibr" rid="CR23">1992</xref>; Land &amp; Lee, <xref ref-type="bibr" rid="CR24">1994</xref>; Ballard et al., <xref ref-type="bibr" rid="CR1">1995</xref>; Pelz &amp; Canosa, <xref ref-type="bibr" rid="CR36">2001</xref>; Matthis et al., <xref ref-type="bibr" rid="CR29">2018</xref>), social interaction between adults (Ho et al., <xref ref-type="bibr" rid="CR16">2015</xref>; Rogers et al., <xref ref-type="bibr" rid="CR39">2018</xref>; Macdonald &amp; Tatler, <xref ref-type="bibr" rid="CR27">2018</xref>; Rogers et al., <xref ref-type="bibr" rid="CR38">2019</xref>) or children and their parents (Yu &amp; Smith, <xref ref-type="bibr" rid="CR43">2017</xref>; Suarez-Rivera et al., <xref ref-type="bibr" rid="CR41">2019</xref>), usability (Masood &amp; Thigambaram, <xref ref-type="bibr" rid="CR28">2015</xref>; Bergstrom &amp; Schall, <xref ref-type="bibr" rid="CR3">2014</xref>), marketing (Harwood &amp; Jones, <xref ref-type="bibr" rid="CR10">2014</xref>), decision making (Gidlöf et al., <xref ref-type="bibr" rid="CR8">2013</xref>; Gidlöf et al., <xref ref-type="bibr" rid="CR7">2017</xref>), surgery (Dik et al., <xref ref-type="bibr" rid="CR5">2016</xref>; Harrison et al., <xref ref-type="bibr" rid="CR9">2016</xref>), navigation and wayfinding (Kiefer et al., <xref ref-type="bibr" rid="CR20">2014</xref>; Koletsis et al., <xref ref-type="bibr" rid="CR21">2017</xref>) and education (McIntyre et al., <xref ref-type="bibr" rid="CR31">2017</xref>; McIntyre &amp; Foulsham, <xref ref-type="bibr" rid="CR30">2018</xref>).</p>
    <p id="Par3">Head-worn eye trackers typically consist of some form of headset or glasses on which multiple cameras are mounted. First, there is a scene camera that is pointed forward and films the world in front of the participant. Second, there are one or more cameras that film one or both eyes of the participant. The images from one or more eye cameras are processed by firmware or software—the headset and the gaze processing code together form an eye-tracking setup. The typical output of a head-worn eye-tracking setup consists of the video of the scene camera along with gaze direction, usually reported in the video frame of the scene camera. The Tobii Pro Glasses 2, a system that records binocular gaze direction, furthermore provides pupil size, the 3D orientation of each eye ball in a coordinate system fixed to the headset, and gyroscope and accelerometer data indicating movement of the headset.</p>
    <p id="Par4">Analysis of head-worn eye-tracking data often happens in multiple steps. First, event classification (the labeling of parts of, e.g., the eye-tracking data as, e.g., “fixations” and “saccades”, see Hessels et al.,, <xref ref-type="bibr" rid="CR14">2018</xref>) is commonly performed to extract episodes of the recording for further analysis. Important to note here is that only few event classification algorithms are available that are developed for head-worn eye tracker signals (Hooge &amp; Camps, <xref ref-type="bibr" rid="CR18">2013</xref>; Hessels et al., in press; Larsson &amp; et al. <xref ref-type="bibr" rid="CR26">2016</xref>; Kothari et al., <xref ref-type="bibr" rid="CR22">2019</xref>), and that none of these appear to be implemented in commercial software for head-worn eye-tracking data analysis.</p>
    <p id="Par5">In many fields, such as psychology, researchers using head-worn eye trackers are predominantly not interested in how a participant’s eyes move in their head, but instead in questions such as which objects in the world a person looked at, in what order, and how long each object was looked at (see Hessels et al.,, <xref ref-type="bibr" rid="CR14">2018</xref>, for a discussion of coordinate systems in which gaze direction can be measured). As such, after event classification, a common step is to use the classified fixations to determine what objects an observer looked at. This is however not straightforward for most head-mounted eye-tracking research. Unlike in screen-based eye tracking where the experimenter often has control over the stimulus and knows what object was presented where and when, the positions of objects in the scene video of a head-worn eye tracker are usually unknown. Since the scene video provided by the head-worn eye tracker is recorded from the perspective of a moving observer, objects of interest continuously change position when the observer moves, or even disappear from view altogether. Analysis of head-mounted eye-tracking data therefore requires mapping the gaze direction provided by the eye tracker in the reference frame of the scene video to specific objects in the world. Unless the position and orientation of the headset and the objects of interest in the world are known, or if the location of objects in the scene video can be recovered by automatic means (e.g., Brône et al.,, <xref ref-type="bibr" rid="CR4">2011</xref>), mapping gaze to the object in the world is often carried out manually. We will refer to this as <italic>manual mapping</italic> in this paper. While some researchers perform manual mapping for individual video frames or gaze samples (e.g., Land et al.,, <xref ref-type="bibr" rid="CR25">1999</xref>; Gidlöf et al.,, <xref ref-type="bibr" rid="CR8">2013</xref>; Gidlöf et al.,, <xref ref-type="bibr" rid="CR7">2017</xref>), frequently researchers first obtain episodes in the recording where gaze is fixed on an object in the world (often called fixation classification or event classification), and then code these episodes one at a time (see, e.g., Hessels et al.,, in press).</p>
    <p id="Par6">The Tobii Pro Glasses 2 is a recent head-worn eye tracker that is commonly used across a range of studies (e.g., Harrison et al.,, <xref ref-type="bibr" rid="CR9">2016</xref>; Topolšek et al.,, <xref ref-type="bibr" rid="CR42">2016</xref>; Koletsis et al.,, <xref ref-type="bibr" rid="CR21">2017</xref>; Rogers et al.,, <xref ref-type="bibr" rid="CR39">2018</xref>; Raptis et al.,, <xref ref-type="bibr" rid="CR37">2018</xref>). However, analysis of the recordings is currently limited to Tobii Pro Lab, a software package sold by the manufacturer of this eye tracker, and other closed commercial software packages such as offered by iMotions. Tobii Pro Lab aims to provide the analysis options that a majority of Tobii’s academic and industry clients want. For many researchers, such as the authors of this paper, Tobii Pro Lab and the other commercial packages offer insufficient flexibility and insufficient control over the various analysis steps. Specifically, Tobii Pro Lab only visualizes a limited amount of the data available in a Glasses 2 recording (for instance, the gyroscope and accelerometer data which may be helpful to classify participant movement are not featured in the interface at all), is limited in functionality (for instance, only one fixation classifier can be chosen and its exact algorithm is not known given that the implementation’s source code cannot be inspected), is not extensible and provides a workflow for manual mapping gaze data to objects-of-interest that has been found to be inefficient (Benjamins et al., <xref ref-type="bibr" rid="CR2">2018</xref>).</p>
    <p id="Par7">In this paper, we present GlassesViewer, an open-source tool that enables easily navigating the study- and recording structure produced by the Tobii Pro Glasses 2 and provides tools for converting the recorded data to an easily readable format and synchronizing all data streams and videos together. It furthermore provides a graphical user interface that can visualize and replay all data streams and both scene and eye videos in a recording, contains an interface for manual or automatic event classification, as well as for viewing and adjusting the output of a manual mapping procedure. Via integration with GazeCode (Benjamins et al., <xref ref-type="bibr" rid="CR2">2018</xref>), users can manually map the events that were classified with GlassesViewer’s tools (e.g., fixations) to objects of interest in an intuitive and efficient manner, and then view these mapped events on the recording’s timeline in the GlassesViewer tool. In contrast to Tobii Pro Lab, the GlassesViewer software presented here has extensive built-in visualization capabilities, is built to be highly configurable and extensible and uses data input and output formats that allow for easy interoperability with other software tools, such as the featured integration with GazeCode. We consider GlassesViewer and GazeCode together to offer a free and open-source<xref ref-type="fn" rid="Fn1">1</xref> replacement for most of the functionality offered by the manufacturer software. For a feature comparison, please refer to Table <xref rid="Tab1" ref-type="table">1</xref>. GlassesViewer is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/dcnieho/GlassesViewer">https://github.com/dcnieho/GlassesViewer</ext-link> and GazeCode from <ext-link ext-link-type="uri" xlink:href="https://github.com/jsbenjamins/gazecode">https://github.com/jsbenjamins/gazecode</ext-link>. GlassesViewer has been tested with recordings made with Tobii Pro Glasses 2 firmware versions 1.22.0-zucchinipaj, 1.25.0-citronkola and 1.25.3-citronkola.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>A comparison of the features offered by the GlassesViewer and GazeCode combination, and the Tobii Pro Lab Analysis module</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">GlassesViewer + GazeCode</th><th align="left">Tobii Pro Lab Analysis module</th></tr></thead><tbody><tr><td align="left">Scene video playback</td><td align="left">yes</td><td align="left">yes</td></tr><tr><td align="left">Eye video playback</td><td align="left">yes</td><td align="left">no</td></tr><tr><td align="left">Data stream timeseries plots</td><td align="left">horizontal and vertical eye orientation (per eye), pupil size, eye angular velocity, gyroscope, accelerometer</td><td align="left">horizontal and vertical gaze position on the video (binocular only), gaze velocity</td></tr><tr><td align="left">TTL signal visualization on timeline</td><td align="left">yes</td><td align="left">yes</td></tr><tr><td align="left">Coding by selecting intervals on timeline</td><td align="left">yes</td><td align="left">no</td></tr><tr><td align="left">Interactive event classifier parameter adjustments</td><td align="left">yes</td><td align="left">yes</td></tr><tr><td align="left">Manual event mapping</td><td align="left">yes, using coding buttons or keyboard keys</td><td align="left">yes, using manually defined events coupled to keyboard keys</td></tr><tr><td align="left">Supported event types for manual mapping</td><td align="left">any</td><td align="left">fixations</td></tr><tr><td align="left">Automatic event mapping</td><td align="left">no</td><td align="left">yes, for 2D planes</td></tr><tr><td align="left">AOI analyses</td><td align="left">no</td><td align="left">yes</td></tr><tr><td align="left">Extensible event classifiers</td><td align="left">yes</td><td align="left">no</td></tr><tr><td align="left">Video export with overlaid visualization</td><td align="left">no</td><td align="left">yes</td></tr><tr><td align="left">Data export</td><td align="left">yes, to MATLAB file</td><td align="left">yes, to csv file</td></tr></tbody></table></table-wrap></p>
  </sec>
  <sec id="Sec2">
    <title>The GlassesViewer tool</title>
    <p id="Par9">The GlassesViewer toolbox consists of tools for (1) selecting, (2) parsing and (3) viewing and analyzing Tobii Pro Glasses 2 recordings. Below we provide a detailed description of the toolbox’s functionality. For the reader who prefers to try out the tools directly, we however recommend starting by following the steps described in the quick start manual manual.md that is available in the GlassesViewer repository on GitHub. The first two tools described in the Sections “<xref rid="Sec3" ref-type="sec">Recording selector</xref>” and “<xref rid="Sec4" ref-type="sec">Recording parser</xref>” are building blocks for the glassesViewer graphical user interface described in the Section “<xref rid="Sec5" ref-type="sec">Recording viewer and analyzer graphical user interface</xref>”. These first two tools can however also be used stand-alone.
</p>
    <sec id="Sec3">
      <title>Recording selector</title>
      <p id="Par10">The Tobii Pro Glasses 2 stores studies and recordings in a complex directory structure on its SD card. The names of studies and recordings in this directory structure are random strings (e.g. recording gzz7stc, which is part of project raoscyb), and as such not human-readable. This makes it hard to locate a specific study or recording. GlassesViewer therefore provides two functions to easily navigate this structure. First, the function G2ProjectParser determines what studies (e.g., DemoIntegratie) and recordings (e.g., Recording011) are contained in a directory and stores it in a Microsoft Excel file, lookup.xls. This file contains information about all the studies in the directory, and all the recordings in each study. Second, the function recordingSelector reads this lookup file and presents a graphical study and recording selection interface (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). When a recording is selected, the tool provides the path to this recording in the directory structure, so the recording can be located.
<fig id="Fig1"><label>Fig. 1</label><caption><p>The recording selector interface. When provided with the directory structure created on the SD card by the Tobii Pro Glasses 2, this dialogue shows all the projects contained in the directory structure. For each of the projects, it shows all the participants from whom data was recorded (<italic>left panel</italic>), and for each participant all their recordings. Once a recording is selected, the dialogue shows whether the recording was preceded by a successful calibration (<italic>right panel</italic>)</p></caption><graphic xlink:href="13428_2019_1314_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Recording parser</title>
      <p id="Par11">Each recording made by the Tobii Pro Glasses 2 is comprised of a scene video file, an optional eye video file, and a gzipped text file containing the headset’s data streams encoded in packets of json-formatted text data. To make the data in these files accessible, the GlassesViewer toolbox contains functionality to parse, organize and synchronize these data streams. It can furthermore extract the timestamp for each frame in the video streams, and then synchronize these frames to the recording’s data streams. Parsing of a recording is performed by the function getTobiiDataFromGlasses which provides the processed data streams and video frame timestamps to the caller in a MATLAB structure, and also stores this structure to a file livedata.mat in the recording’s directory. During parsing, getTobiiDataFromGlasses cleans up the data streams by checking if a set of conditions is met for each gaze sample—such as whether data for all gaze data streams is available for a given sample. Gaps sometimes occur in the gaze data recorded by the Glasses 2 and can complicate the analysis of eye-tracking data (Hessels et al., <xref ref-type="bibr" rid="CR12">2015</xref>). getTobiiDataFromGlasses detects these gaps and fills them with “missing” gaze samples in such a way that the recording’s gaze data sampling interval is preserved. getTobiiDataFromGlasses furthermore provides support for parsing recordings consisting of multiple segments, which occur when the Tobii Pro Glasses 2 splits recordings longer than approximately an hour.</p>
    </sec>
    <sec id="Sec5">
      <title>Recording viewer and analyzer graphical user interface</title>
      <p id="Par12">The function glassesViewer selects and loads a recording using the functionality provided by the two tools described above and shows a flexible graphical user interface in which all data streams along with the video files can be viewed and annotated (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). glassesViewer furthermore supports loading annotations (denoting, e.g., fixation episodes or walking episodes) from text files, and the automatic generation of annotations through running built-in or user-provided event classifiers. The glassesViewer tool’s settings can be configured through a JSON file, or provided as a MATLAB struct when invoking the function. Example default settings are provided in the defaults.json file included with the glassesViewer tool. A complete manual for the glassesViewer interface as well as all the settings in the JSON file is found in the readme.md file included with the glassesViewer tool.
<fig id="Fig2"><label>Fig. 2</label><caption><p>The GlassesViewer recording viewer. On the left of the interface, raw data streams are plotted along with a scarf plot showing multiple annotation streams. To the right of these data stream plots, the scene and eye videos are shown, along with a media player toolbar for playback of the recording, and to allow selecting the time window for which data are plotted in the data stream plots. Furthermore shown when a recording is first opened is a panel indicating the data quality (RMS-S2S precision and data loss) of the recording</p></caption><graphic xlink:href="13428_2019_1314_Fig2_HTML" id="MO2"/></fig></p>
      <sec id="Sec6">
        <title>Viewing a recording</title>
        <p id="Par13">Figure <xref rid="Fig2" ref-type="fig">2</xref> shows a Tobii Pro Glasses 2 recording loaded in the glassesViewer interface. The left side of the interface shows a series of data streams plotting the raw gaze direction vectors decomposed into Fick angles (Fick, <xref ref-type="bibr" rid="CR6">1854</xref>; Haslwanter, <xref ref-type="bibr" rid="CR11">1995</xref>) and plotted as azimuthal and elevational eye orientations; the corresponding eye velocity as computed from the eye orientation time series; pupil size; and the raw gyroscope and accelerometer time series that are available in a recording. Furthermore shown is a scarf plot denoting manually or algorithmically created event streams that will be discussed in the Section “<xref rid="Sec7" ref-type="sec">Manual and algorithmic event annotation</xref>” section below. Which of the data stream plots are shown is configurable, both in the settings JSON file, and in a settings panel in the glassesViewer interface itself (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). The eye velocity is computed in the following manner. First, smoothed azimuthal (<italic>𝜃</italic>) and elevational (<italic>φ</italic>) eye velocity time series are computed using a Savitky–Golay differentiation filter (Savitzky &amp; Golay, <xref ref-type="bibr" rid="CR40">1964</xref>). The eye velocity (<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\dot {\omega }$\end{document}</tex-math><mml:math id="M2"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="13428_2019_1314_Article_IEq1.gif"/></alternatives></inline-formula>) is then computed by <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\dot {\omega }=\sqrt {\dot {\theta }^{2}\cos \limits ^{2}\varphi +\dot {\varphi }^{2}}$\end{document}</tex-math><mml:math id="M4"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>𝜃</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>cos</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>φ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt></mml:math><inline-graphic xlink:href="13428_2019_1314_Article_IEq2.gif"/></alternatives></inline-formula> (Niehorster et al., <xref ref-type="bibr" rid="CR34">2015</xref>).
<fig id="Fig3"><label>Fig. 3</label><caption><p>The settings panel of the glassesViewer recording viewer enables changing various settings while a recording is loaded in the glassesViewer interface. The Savitzky–Golay window setting changes the number of filter taps used for calculating instantaneous eye velocity, and the two list boxes underneath denote which time series are visible in the interface and in which order (<italic>left box</italic>). Data stream plots can be hidden from view by moving them to the right box, allowing maximum screen space for the data stream plots that the user is interested in viewing</p></caption><graphic xlink:href="13428_2019_1314_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>The glassesViewer recording viewer showing an event coding. When an event stream containing coding for the current time window is selected, the coded episodes are shaded on each of the data stream plots. Furthermore shown is an open coding panel allowing one to change an already coded episode to another label, or to place a new code in each of the event streams. The currently active event stream is indicated by the <italic>red arrow</italic> next to the scarf plot</p></caption><graphic xlink:href="13428_2019_1314_Fig4_HTML" id="MO4"/></fig></p>
        <p id="Par14">To the right of the data stream plots, the scene video and, if available, eye videos are shown. Underneath the scene video, a media player toolbar is provided, allowing to play and pause a recording, as well as jump forward and backward in time, adjust the time window that is shown in the data stream plots, and loop the playback during that time window. On top of the scene video, the recorded gaze position is shown as a green dot when the Glasses 2 headset based its reported gaze position on data from both eyes, and as a red dot when the reported gaze position is based on data from a single eye.<xref ref-type="fn" rid="Fn2">2</xref></p>
        <p id="Par16">Underneath the media player toolbar, a panel indicating the data quality of the recording is shown. To ensure that a recording’s data quality is sufficient to allow for a valid analysis, it is of utmost importance that the user is aware of these statistics (Holmqvist et al., <xref ref-type="bibr" rid="CR17">2012</xref>; Niehorster et al., in press; Nyström et al., <xref ref-type="bibr" rid="CR35">2013</xref>; Hessels et al., <xref ref-type="bibr" rid="CR12">2015</xref>; Niehorster et al., <xref ref-type="bibr" rid="CR33">2018</xref>). We have therefore decided to display them prominently in the interface. Specifically, two aspects of data quality are reported, separately for the azimuth and elevation gaze direction signals of the left and right eyes. First, a measure of the random variation in the gaze direction signal (often referred to as its precision) is provided by means of the root-mean-square of the distance between the gaze directions of adjacent gaze samples (RMS-S2S). The RMS-S2S measure was calculated with a moving window (default length 300 ms), yielding a list of RMS-S2S values. To get an estimate of the RMS-S2S during periods where the eyes move slowly (fixations on static or moving objects while the participant’s head is also either static or moving), the median of all the RMS-S2S values was computed and displayed in the interface. This RMS-S2S measure is intended to provide a value that can be used to compare various recordings performed with the same system setup, making the user aware of recordings that are of significantly worse quality than others in their set. Second, data loss, the number of invalid gaze samples as a percentage of all gaze samples in the recording is presented.</p>
      </sec>
      <sec id="Sec7">
        <title>Manual and algorithmic event annotation</title>
        <p id="Par17">The glassesViewer interface provides annotation functionality, allowing users to manually label episodes in the recording, or to have episodes labeled algorithmically. For this functionality, glassesViewer uses event streams. Each event stream is a separate, user-configured contiguous series of labels denoting specific episodes in the recording. Users could for instance in one stream code whether the eyes are rotating slowly (slow phase; ‘fixation’ or ‘smooth pursuit’) or rapidly (fast phase; ‘saccade’) in the head of the participant (Hessels et al., <xref ref-type="bibr" rid="CR14">2018</xref>), while in another stream coding whether the participant is walking or standing still and use a third stream to view a coding of which object-of-interest the participant is looking at. Event streams and their annotation categories are set up in the JSON settings file, which is documented in the readme.md file. The produced coding is stored in a file coding.mat alongside a recording’s livedata.mat file.</p>
        <p id="Par18">At any time, one of the coding streams is active, meaning that its codes are displayed through highlighting in each of the data stream plots (see Fig. <xref rid="Fig4" ref-type="fig">4</xref>). By clicking on any of the data stream plots, a new annotation can be added ranging from the end of the previous annotation until the clicked time. When clicking an existing annotation, the labeled category can be changed or the annotation can be removed. The start- and end time of annotations can be changed by dragging and annotations can be split by holding down the shift key while clicking on the time where a new annotation should be inserted. Lastly, users can set up their annotation scheme such that flags can be applied to them. For instance, when coding for fixations and saccades, users can set up a flag to augment the saccade annotation with information about whether the saccade has a vergence component or not.
</p>
        <p id="Par19">Besides manually produced annotations, event streams can also come from multiple other sources:</p>
        <p id="Par20">
          <def-list>
            <def-item>
              <term>
                <bold>
                  <italic>TTL signals:</italic>
                </bold>
              </term>
              <def>
                <p id="Par21">The Tobii Pro Glasses 2 includes a TTL port through which events, such as sync signals or button presses, can be timestamped and recorded. glassesViewer can be set up such that an event stream is automatically generated from the TTL activity.</p>
              </def>
            </def-item>
            <def-item>
              <term>
                <bold>
                  <italic>Text files:</italic>
                </bold>
              </term>
              <def>
                <p id="Par22">To provide simple interoperability with other tools, an event stream can be loaded from a text file. If the user manually adjusts the loaded annotations, they can be reset to those contained in the text file with a button in the interface. A full description of how to set this up is provided in the readme.md file that comes with the GlassesViewer toolbox.</p>
              </def>
            </def-item>
            <def-item>
              <term>
                <bold>
                  <italic>Classification algorithms:</italic>
                </bold>
              </term>
              <def>
                <p id="Par23">Lastly, glassesViewer can produce annotations by calling user-provided algorithms. These algorithms are provided with a recording’s data streams and are expected to return an event stream. The MATLAB function to call is defined in the JSON settings file, along with parameter settings. These parameters can be marked as user-settable, in which case a dialogue becomes available in the interface (Fig. <xref rid="Fig5" ref-type="fig">5</xref>) allowing the user to change the parameter settings of the algorithm and rerun it to produce an updated event stream. glassesViewer comes with two classifiers (Hooge and Camps, <xref ref-type="bibr" rid="CR18">2013</xref>; Hessels et al., in press) that split the gaze data into episodes where the eye moves fast, and episodes where the eye moves slowly. Shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref> is the classification produced by the Hessels et al. (in press) algorithm, and in Fig. <xref rid="Fig5" ref-type="fig">5</xref> the settings for this algorithm are shown.</p>
              </def>
            </def-item>
          </def-list>
          <fig id="Fig5">
            <label>Fig. 5</label>
            <caption>
              <p>Classifier settings for the glassesViewer recording viewer. Shown are the settings for the Hessels et al., (in press) slow and fast phase classifier. For each setting, a textbox is available through which the value can be changed, and besides the textbox, the range of allowed values is indicated. The recalculate button performs event classification with the selected settings, and the restore defaults button resets the parameter values to their initial values as defined in the JSON settings file</p>
            </caption>
            <graphic xlink:href="13428_2019_1314_Fig5_HTML" id="MO5"/>
          </fig>
        </p>
        <p id="Par24">Regarding the use of event classification, it is important to realize that use of the event classification algorithms provided by GlassesViewer comes with the same caveats as use of those provided by Tobii Pro Lab or any other software package. Specifically, the user has to assure themselves that the algorithm and its settings produce classifications (e.g., “fixations”) that have the properties desired for further analysis. Example properties that may be relevant for specific use cases are that all events are at least of a specific minimum duration, that eye velocity during a fixation episode does not exceed a given threshold, or conversely that the slow eye-in-head gaze shifts associated with pursuit or VOR activity do not break up the classified event. These are properties of the event classifier output that have to be determined a-priori by the researcher (see, e.g., Hessels et al.,, <xref ref-type="bibr" rid="CR13">2017</xref>) and have to be checked by the researchers through inspection of the classified events using, e.g., GlassesViewer’s timeline (see Fig. <xref rid="Fig4" ref-type="fig">4</xref>). Failure to do either of these steps and unthinkingly accepting a default algorithm or its default settings may lead to improper event classifications and thereby invalid study conclusions.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Integration with GazeCode</title>
    <p id="Par25">To provide researchers with a complete, more flexible and fully transparent replacement to Tobii’s software for viewing and analyzing recordings made with the Tobii Pro Glasses 2, a close but optional integration between the GlassesViewer toolbox and GazeCode (Benjamins et al., <xref ref-type="bibr" rid="CR2">2018</xref>) has been developed. GazeCode,<xref ref-type="fn" rid="Fn3">3</xref> available from <ext-link ext-link-type="uri" xlink:href="https://github.com/jsbenjamins/gazecode">https://github.com/jsbenjamins/gazecode</ext-link>, is a tool developed for efficient manual mapping of participant gaze onto the visual stimulus by assigning each look to a predefined category (such as “teapot”, “cup”, “spoon”). GazeCode was designed with a minimalistic interface that only shows the information necessary for this task and allows for straightforward keyboard-based operation. As such, a test performed by Benjamins et al., (<xref ref-type="bibr" rid="CR2">2018</xref>) found that coders were able to annotate the same gaze data with GazeCode at double the rate they obtained when using the Tobii Pro Lab software.</p>
    <p id="Par27">By means of the integration between GazeCode and GlassesViewer, GazeCode can now use GlassesViewer’s functionality to parse and load a Tobii Pro Glasses 2 recording. It can furthermore load any of the event coding streams produced with the GlassesViewer tool, or invoke GlassesViewer’s functionality to produce these event streams automatically. Finally, after the researcher is done manually mapping, e.g., a participant’s gaze to objects in the world, the new event stream containing the researcher’s annotations is stored in the coding.mat file, allowing it to be viewed on the timeline provided by the glassesViewer interface and potentially manually adjusted.</p>
  </sec>
  <sec id="Sec9">
    <title>Example use and workflow</title>
    <p id="Par28">To demonstrate the workflow using the GlassesViewer and GazeCode tools, we have made a brief example recording where a participant was asked to search a notice board for red pushpins used to attach posters to the board, among pushpins with different colors. The participant was instructed to press a button on a custom-built response box connected to the Glasses 2’s recording unit (further described in <xref rid="Sec11" ref-type="sec">Appendix</xref>) for each red push pin that they found. When loading the example recording from the demo_data directory included with the glassesViewer tool, these button presses are visualized as an event stream, and they can be chosen in GazeCode as the stream for which to conduct manual mapping.</p>
    <p id="Par29">The complete example workflow including all steps one may wish to perform in the glassesViewer interface, further manual mapping in GazeCode and then reviewing this mapping in glassesViewer is provided in the manual.md file that is included with the glassesViewer tool. The user is referred to this manual for an up-to-date walk-through of the glassesViewer tool.</p>
  </sec>
  <sec id="Sec10">
    <title>Conclusions</title>
    <p id="Par30">In this article, we presented GlassesViewer, a toolbox for viewing and analyzing recordings made with the Tobii Pro Glasses 2 head-mounted eye tracker. It provides tools for selecting, parsing, viewing and manually or automatically annotating such recordings. It is a flexible and fully transparent tool which, together with the efficient GazeCode manual mapper, provides a replacement for most of the functionality offered by the analysis module of the commercial Tobii Pro Lab software.</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec11">
        <title>Appendix: A button box for use with the Tobii Pro Glasses 2</title>
        <p id="Par31">A button box (see Fig. <xref rid="Fig6" ref-type="fig">6</xref>) was built by our technician at Utrecht University. It is a one-button box that sends a TTL signal as long as the button is pressed. It operates on one CR2032 battery, and can be plugged directly into the Tobii Pro Glasses 2 recording unit. It is possible that similar products could be bought off-the-shelf.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Button box for Tobii Pro Glasses 2. As long as the button is pressed, this box sends a TTL signal over a 3.5 mm jack that can be plugged directly into the sync port on the Tobii Pro Glasses 2 recording unit</p></caption><graphic position="anchor" xlink:href="13428_2019_1314_Fig6_HTML" id="MO6"/></fig></p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par8">Note however that both require MATLAB to run, which is not open-source nor free software.</p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par15">The Tobii Pro Glasses 2 determines the gaze position in the scene video from binocular data. If data for only one eye is available, gaze position on the video is determined under the assumption that the vergence distance of the two eyes did not change since binocular data was last available. In some experimental situations this is a valid assumption, but not in all. As such, we color-code the gaze marker in the video to make the researcher aware that they must decide whether to use gaze position data on the video originating from only a single eye. For a note of caution regarding using vergence in pupil-based eye trackers, see Hooge et al., (<xref ref-type="bibr" rid="CR19">2019</xref>).</p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par26">Note that besides data from the Tobii Pro Glasses 2, GazeCode also supports data from SMI, Pupil-Labs and Positive Science head-worn eye trackers.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Pieter Schiphorst for building the button box. The data and materials for the experiment are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/dcnieho/GlassesViewer">https://github.com/dcnieho/GlassesViewer</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/jsbenjamins/gazecode">https://github.com/jsbenjamins/gazecode</ext-link>, and the data collection was not preregistered. Open access funding provided by Lund University.</p>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ballard</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Hayhoe</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Pelz</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <article-title>Memory representations in natural tasks</article-title>
        <source>Journal of Cognitive Neuroscience</source>
        <year>1995</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>66</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1162/jocn.1995.7.1.66</pub-id>
        <?supplied-pmid 23961754?>
        <pub-id pub-id-type="pmid">23961754</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <mixed-citation publication-type="other">Benjamins, J.S., Hessels, R.S., &amp; Hooge, I.T.C. (2018). GazeCode: Open-source software for manual mapping of mobile eye-tracking data. In <italic>Proceedings of the 2018 ACM symposium on eye tracking research &amp; applications</italic>. 10.1145/3204493.3204568 (pp. 54:1–54:4). New York: ACM.</mixed-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bergstrom</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Schall</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Eye tracking in user experience design</source>
        <year>2014</year>
        <publisher-loc>Boston</publisher-loc>
        <publisher-name>Morgan Kaufmann</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR4">
      <mixed-citation publication-type="other">Brône, G., Oben, B., &amp; Goedemé, T. (2011). Towards a more effective method for analyzing mobile eye-tracking data: Integrating gaze data with object recognition algorithms. In <italic>Proceedings of the 1st international workshop on pervasive eye tracking &amp; mobile eye-based interaction</italic>. 10.1145/2029956.2029971 (pp. 53–56). New York: ACM.</mixed-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dik</surname>
            <given-names>VK</given-names>
          </name>
          <name>
            <surname>Hooge</surname>
            <given-names>ITC</given-names>
          </name>
          <name>
            <surname>van Oijen</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Siersema</surname>
            <given-names>PD</given-names>
          </name>
        </person-group>
        <article-title>Measuring gaze patterns during colonoscopy: A useful tool to evaluate colon inspection?</article-title>
        <source>European Journal of Gastroenterology &amp; Hepatology</source>
        <year>2016</year>
        <volume>28</volume>
        <issue>12</issue>
        <fpage>1400</fpage>
        <lpage>1406</lpage>
        <pub-id pub-id-type="doi">10.1097/MEG.0000000000000717</pub-id>
        <pub-id pub-id-type="pmid">27769078</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fick</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Die bewegungen des menschlichen augapfels</article-title>
        <source>Zeitschrift für rationelle Medicin</source>
        <year>1854</year>
        <volume>4</volume>
        <fpage>101</fpage>
        <lpage>128</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gidlöf</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Anikin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lingonblad</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wallin</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Looking is buying. How visual attention and choice are affected by consumer preferences and properties of the supermarket shelf</article-title>
        <source>Appetite</source>
        <year>2017</year>
        <volume>116</volume>
        <fpage>29</fpage>
        <lpage>38</lpage>
        <pub-id pub-id-type="doi">10.1016/j.appet.2017.04.020</pub-id>
        <pub-id pub-id-type="pmid">28433775</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <mixed-citation publication-type="other">Gidlöf, K., Wallin, A., Dewhurst, R., &amp; Holmqvist, K. (2013). Using eye tracking to trace a cognitive process: Gaze behaviour during decision making in a natural environment. <italic>Journal of Eye Movement Research, 6</italic>(1).</mixed-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harrison</surname>
            <given-names>TK</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>TE</given-names>
          </name>
          <name>
            <surname>Kou</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shum</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mariano</surname>
            <given-names>ER</given-names>
          </name>
          <name>
            <surname>Howard</surname>
            <given-names>SK</given-names>
          </name>
          <collab>The ADAPT (Anesthesiology-Directed Advanced Procedural Training) Research Group</collab>
        </person-group>
        <article-title>Feasibility of eye-tracking technology to quantify expertise in ultrasound-guided regional anesthesia</article-title>
        <source>Journal of Anesthesia</source>
        <year>2016</year>
        <volume>30</volume>
        <issue>3</issue>
        <fpage>530</fpage>
        <lpage>533</lpage>
        <pub-id pub-id-type="doi">10.1007/s00540-016-2157-6</pub-id>
        <?supplied-pmid 26980475?>
        <pub-id pub-id-type="pmid">26980475</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <mixed-citation publication-type="other">Harwood, T., &amp; Jones, M. (2014). Mobile eye-tracking in retail research. In M. Horsley, M. Eliot, B.A. Knight, &amp; R. Reilly (Eds.) <italic>Current trends in eye tracking research</italic> (pp. 183–199). Cham: Springer International Publishing.</mixed-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haslwanter</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Mathematics of three-dimensional eye rotations</article-title>
        <source>Vision Research</source>
        <year>1995</year>
        <volume>35</volume>
        <issue>12</issue>
        <fpage>1727</fpage>
        <lpage>1739</lpage>
        <pub-id pub-id-type="doi">10.1016/0042-6989(94)00257-M</pub-id>
        <?supplied-pmid 7660581?>
        <pub-id pub-id-type="pmid">7660581</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Cornelissen</surname>
            <given-names>THW</given-names>
          </name>
          <name>
            <surname>Kemner</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hooge</surname>
            <given-names>ITC</given-names>
          </name>
        </person-group>
        <article-title>Qualitative tests of remote eyetracker recovery and performance during head rotation</article-title>
        <source>Behavior Research Methods</source>
        <year>2015</year>
        <volume>47</volume>
        <issue>3</issue>
        <fpage>848</fpage>
        <lpage>859</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-014-0507-6</pub-id>
        <pub-id pub-id-type="pmid">25033759</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Niehorster</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Kemner</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hooge</surname>
            <given-names>ITC</given-names>
          </name>
        </person-group>
        <article-title>Noise-robust fixation detection in eye movement data: Identification by two-means clustering (I2MC)</article-title>
        <source>Behavior Research Methods</source>
        <year>2017</year>
        <volume>49</volume>
        <issue>5</issue>
        <fpage>1802</fpage>
        <lpage>1823</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-016-0822-1</pub-id>
        <?supplied-pmid 27800582?>
        <pub-id pub-id-type="pmid">27800582</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Niehorster</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hooge</surname>
            <given-names>ITC</given-names>
          </name>
        </person-group>
        <article-title>Is the eye-movement field confused about fixations and saccades? a survey among 124 researchers</article-title>
        <source>Royal Society Open Science</source>
        <year>2018</year>
        <volume>5</volume>
        <issue>8</issue>
        <fpage>180502</fpage>
        <pub-id pub-id-type="doi">10.1098/rsos.180502</pub-id>
        <?supplied-pmid 30225041?>
        <pub-id pub-id-type="pmid">30225041</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <mixed-citation publication-type="other">Hessels, R.S., van Doorn, A.J., Benjamins, J.S., Holleman, G.A., &amp; Hooge, I.T.C. (in press). Task-related gaze control in human crowd navigation. <italic>Attention, Perception, &amp; Psychophysics</italic>.</mixed-citation>
    </ref>
    <ref id="CR16">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ho</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Foulsham</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kingstone</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Speaking and listening with the eyes: Gaze signaling during dyadic interactions</article-title>
        <source>PloS one</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>8</issue>
        <fpage>e0136905</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0136905</pub-id>
        <pub-id pub-id-type="pmid">26309216</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <mixed-citation publication-type="other">Holmqvist, K., Nyström, M., &amp; Mulvey, F. (2012). Eye tracker data quality: What it is and how to measure it. In <italic>Proceedings of the symposium on eye tracking research and applications</italic>. 10.1145/2168556.2168563(pp. 45–52). New York: ACM.</mixed-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hooge</surname>
            <given-names>ITC</given-names>
          </name>
          <name>
            <surname>Camps</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Scan path entropy and arrow plots: Capturing scanning behavior of multiple observers</article-title>
        <source>Frontiers in Psychology</source>
        <year>2013</year>
        <volume>4</volume>
        <fpage>996</fpage>
        <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00996</pub-id>
        <?supplied-pmid 24399993?>
        <pub-id pub-id-type="pmid">24399993</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hooge</surname>
            <given-names>ITC</given-names>
          </name>
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Do pupil-based binocular video eye trackers reliably measure vergence?</article-title>
        <source>Vision Research</source>
        <year>2019</year>
        <volume>156</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1016/j.visres.2019.01.004</pub-id>
        <?supplied-pmid 30641092?>
        <pub-id pub-id-type="pmid">30641092</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kiefer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Giannopoulos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Raubal</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Where am I? Investigating map matching during self-localization with mobile eye tracking in an urban environment</article-title>
        <source>Transactions in GIS</source>
        <year>2014</year>
        <volume>18</volume>
        <issue>5</issue>
        <fpage>660</fpage>
        <lpage>686</lpage>
        <pub-id pub-id-type="doi">10.1111/tgis.12067</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koletsis</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>van Elzakker</surname>
            <given-names>CPJM</given-names>
          </name>
          <name>
            <surname>Kraak</surname>
            <given-names>M-J</given-names>
          </name>
          <name>
            <surname>Cartwright</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Arrowsmith</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Field</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>An investigation into challenges experienced when route planning, navigating and wayfinding</article-title>
        <source>International Journal of Cartography</source>
        <year>2017</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>4</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1080/23729333.2017.1300996</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <mixed-citation publication-type="other">Kothari, R., Yang, Z., Kanan, C., Bailey, R., Pelz, J., &amp; Diaz, G. (2019). Gaze-in-wild: A dataset for studying eye and head coordination in everyday activities. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1905.13146">1905.13146</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Land</surname>
            <given-names>MF</given-names>
          </name>
        </person-group>
        <article-title>Predictable eye-head coordination during driving</article-title>
        <source>Nature</source>
        <year>1992</year>
        <volume>359</volume>
        <issue>6393</issue>
        <fpage>318</fpage>
        <lpage>320</lpage>
        <pub-id pub-id-type="doi">10.1038/359318a0</pub-id>
        <?supplied-pmid 1406934?>
        <pub-id pub-id-type="pmid">1406934</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Land</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>DN</given-names>
          </name>
        </person-group>
        <article-title>Where we look when we steer</article-title>
        <source>Nature</source>
        <year>1994</year>
        <volume>369</volume>
        <issue>6483</issue>
        <fpage>742</fpage>
        <lpage>744</lpage>
        <pub-id pub-id-type="doi">10.1038/369742a0</pub-id>
        <?supplied-pmid 8008066?>
        <pub-id pub-id-type="pmid">8008066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Land</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Mennie</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Rusted</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The roles of vision and eye movements in the control of activities of daily living</article-title>
        <source>Perception</source>
        <year>1999</year>
        <volume>28</volume>
        <issue>11</issue>
        <fpage>1311</fpage>
        <lpage>1328</lpage>
        <pub-id pub-id-type="doi">10.1068/p2935</pub-id>
        <?supplied-pmid 10755142?>
        <pub-id pub-id-type="pmid">10755142</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <mixed-citation publication-type="other">Larsson, L., et al. (2016). Head movement compensation and multi-modal event detection in eye-tracking data for unconstrained head movements. <italic>Journal of Neuroscience Methods</italic>.</mixed-citation>
    </ref>
    <ref id="CR27">
      <mixed-citation publication-type="other">Macdonald, R.G., &amp; Tatler, B.W. (2018). Gaze in a real-world social interaction: A dual eye-tracking study. <italic>Quarterly Journal of Experimental Psychology</italic>, 1747021817739221.</mixed-citation>
    </ref>
    <ref id="CR28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Masood</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Thigambaram</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The usability of mobile applications for pre-schoolers</article-title>
        <source>Procedia - Social and Behavioral Sciences</source>
        <year>2015</year>
        <volume>197</volume>
        <fpage>1818</fpage>
        <lpage>1826</lpage>
        <pub-id pub-id-type="doi">10.1016/j.sbspro.2015.07.241</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Matthis</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Yates</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Hayhoe</surname>
            <given-names>MM</given-names>
          </name>
        </person-group>
        <article-title>Gaze and the control of foot placement when walking in natural terrain</article-title>
        <source>Current Biology</source>
        <year>2018</year>
        <volume>28</volume>
        <issue>8</issue>
        <fpage>1224</fpage>
        <lpage>1233.e5</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cub.2018.03.008</pub-id>
        <?supplied-pmid 29657116?>
        <pub-id pub-id-type="pmid">29657116</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McIntyre</surname>
            <given-names>NA</given-names>
          </name>
          <name>
            <surname>Foulsham</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Scanpath analysis of expertise and culture in teacher gaze in real-world classrooms</article-title>
        <source>Instructional Science</source>
        <year>2018</year>
        <volume>46</volume>
        <issue>3</issue>
        <fpage>435</fpage>
        <lpage>455</lpage>
        <pub-id pub-id-type="doi">10.1007/s11251-017-9445-x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <mixed-citation publication-type="other">McIntyre, N.A., Jarodzka, H., &amp; Klassen, R.M. (2017). Capturing teacher priorities: Using real-world eye-tracking to investigate expert teacher priorities across two cultures. <italic>Learning and Instruction</italic>.</mixed-citation>
    </ref>
    <ref id="CR32">
      <mixed-citation publication-type="other">Niehorster, D.C., Santini, T., Hessels, R.S., Hooge, I.T.C., Kasneci, E., &amp; Nyström, M. (in press). The impact of slippage on the data quality of head-worn eye trackers. <italic>Behavior Research Methods</italic>10.3758/s13428-019-01307-0.</mixed-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Niehorster</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Cornelissen</surname>
            <given-names>THW</given-names>
          </name>
          <name>
            <surname>Holmqvist</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hooge</surname>
            <given-names>ITC</given-names>
          </name>
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
        </person-group>
        <article-title>What to expect from your remote eye-tracker when participants are unrestrained</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>1</issue>
        <fpage>213</fpage>
        <lpage>227</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-017-0863-0</pub-id>
        <pub-id pub-id-type="pmid">28205131</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Niehorster</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Siu</surname>
            <given-names>WWF</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Manual tracking enhances smooth pursuit eye movements</article-title>
        <source>Journal of Vision</source>
        <year>2015</year>
        <volume>15</volume>
        <issue>15</issue>
        <fpage>11</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1167/15.15.11</pub-id>
        <?supplied-pmid 26605840?>
        <pub-id pub-id-type="pmid">26605840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <mixed-citation publication-type="other">Nyström, M., Andersson, R., Holmqvist, K., &amp; van de Weijer, J. (2013). The influence of calibration method and eye physiology on eyetracking data quality. <italic>Behavior Research Methods</italic>, <italic>45</italic>(1), 272–288. 10.3758/s13428-012-0247-4</mixed-citation>
    </ref>
    <ref id="CR36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pelz</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Canosa</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Oculomotor behavior and perceptual strategies in complex tasks</article-title>
        <source>Vision Research</source>
        <year>2001</year>
        <volume>41</volume>
        <issue>25</issue>
        <fpage>3587</fpage>
        <lpage>3596</lpage>
        <pub-id pub-id-type="doi">10.1016/S0042-6989(01)00245-0</pub-id>
        <?supplied-pmid 11718797?>
        <pub-id pub-id-type="pmid">11718797</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raptis</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Fidas</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Avouris</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Effects of mixed-reality on players’ behaviour and immersion in a cultural tourism game: A cognitive processing perspective</article-title>
        <source>International Journal of Human-Computer Studies</source>
        <year>2018</year>
        <volume>114</volume>
        <fpage>69</fpage>
        <lpage>79</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ijhcs.2018.02.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rogers</surname>
            <given-names>SL</given-names>
          </name>
          <name>
            <surname>Guidetti</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Speelman</surname>
            <given-names>CP</given-names>
          </name>
          <name>
            <surname>Longmuir</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Contact is in the eye of the beholder: The eye contact illusion</article-title>
        <source>Perception</source>
        <year>2019</year>
        <volume>48</volume>
        <issue>3</issue>
        <fpage>248</fpage>
        <lpage>252</lpage>
        <pub-id pub-id-type="doi">10.1177/0301006619827486</pub-id>
        <?supplied-pmid 30714488?>
        <pub-id pub-id-type="pmid">30714488</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rogers</surname>
            <given-names>SL</given-names>
          </name>
          <name>
            <surname>Speelman</surname>
            <given-names>CP</given-names>
          </name>
          <name>
            <surname>Guidetti</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Longmuir</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Using dual eye tracking to uncover personal gaze patterns during social interaction</article-title>
        <source>Scientific Reports</source>
        <year>2018</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>4271</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-22726-7</pub-id>
        <pub-id pub-id-type="pmid">29523822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Savitzky</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Golay</surname>
            <given-names>MJE</given-names>
          </name>
        </person-group>
        <article-title>Smoothing and differentiation of data by simplified least squares procedures</article-title>
        <source>Analytical Chemistry</source>
        <year>1964</year>
        <volume>36</volume>
        <issue>8</issue>
        <fpage>1627</fpage>
        <lpage>1639</lpage>
        <pub-id pub-id-type="doi">10.1021/ac60214a047</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Suarez-Rivera</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>LB</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Multimodal parent behaviors within joint attention support sustained attention in infants</article-title>
        <source>Developmental Psychology</source>
        <year>2019</year>
        <volume>55</volume>
        <issue>1</issue>
        <fpage>96</fpage>
        <pub-id pub-id-type="doi">10.1037/dev0000628</pub-id>
        <pub-id pub-id-type="pmid">30489136</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Topolšek</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Areh</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Cvahte</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Examination of driver detection of roadside traffic signs and advertisements using eye tracking</article-title>
        <source>Transportation Research Part F: Traffic Psychology and Behaviour</source>
        <year>2016</year>
        <volume>43</volume>
        <fpage>212</fpage>
        <lpage>224</lpage>
        <pub-id pub-id-type="doi">10.1016/j.trf.2016.10.002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>LB</given-names>
          </name>
        </person-group>
        <article-title>Hand–eye coordination predicts joint attention</article-title>
        <source>Child Development</source>
        <year>2017</year>
        <volume>88</volume>
        <issue>6</issue>
        <fpage>2060</fpage>
        <lpage>2078</lpage>
        <pub-id pub-id-type="doi">10.1111/cdev.12730</pub-id>
        <pub-id pub-id-type="pmid">28186339</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
