<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Oncol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Oncol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Oncol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Oncology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2234-943X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9646829</article-id>
    <article-id pub-id-type="doi">10.3389/fonc.2022.971871</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Oncology</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BgNet: Classification of benign and malignant tumors with MRI multi-plane attention learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Liu</surname>
          <given-names>Hong</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="fn001" ref-type="author-notes">
          <sup>*</sup>
        </xref>
        <xref rid="fn003" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1648576"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jiao</surname>
          <given-names>Meng-Lei</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="fn003" ref-type="author-notes">
          <sup>†</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Xing</surname>
          <given-names>Xiao-Ying</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="fn003" ref-type="author-notes">
          <sup>†</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ou-Yang</surname>
          <given-names>Han-Qiang</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
        <xref rid="fn003" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1696770"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yuan</surname>
          <given-names>Yuan</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1697127"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Jian-Fang</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1306286"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Yuan</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Chun-Jie</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1700891"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lang</surname>
          <given-names>Ning</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/930886"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Qian</surname>
          <given-names>Yue-Liang</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Jiang</surname>
          <given-names>Liang</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
        <xref rid="fn001" ref-type="author-notes">
          <sup>*</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1370279"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Yuan</surname>
          <given-names>Hui-Shu</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="fn001" ref-type="author-notes">
          <sup>*</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Xiang-Dong</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1748964"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>University of Chinese Academy of Sciences</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Department of Radiology, Peking University Third Hospital</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Department of Orthopaedics, Peking University Third Hospital</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Engineering Research Center of Bone and Joint Precision Medicine</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff>
    <aff id="aff6"><sup>6</sup><institution>Beijing Key Laboratory of Spinal Disease Research</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Tonghe Wang, Emory University, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Lichi Zhang, Shanghai Jiao Tong University, China; Jie Wu, Hubei Normal University, China</p>
      </fn>
      <corresp id="fn001">*Correspondence: Hong Liu, <email xlink:href="mailto:hliu@ict.ac.cn">hliu@ict.ac.cn</email>; Liang Jiang, <email xlink:href="mailto:jiangliang@bjmu.edu.cn">jiangliang@bjmu.edu.cn</email>; Hui-Shu Yuan, <email xlink:href="mailto:huishuy@vip.163.com">huishuy@vip.163.com</email>
</corresp>
      <fn fn-type="equal" id="fn003">
        <p>†These authors have contributed equally to this work</p>
      </fn>
      <fn fn-type="other" id="fn002">
        <p>This article was submitted to Radiation Oncology, a section of the journal Frontiers in Oncology</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>971871</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Liu, Jiao, Xing, Ou-Yang, Yuan, Liu, Li, Wang, Lang, Qian, Jiang, Yuan and Wang</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Liu, Jiao, Xing, Ou-Yang, Yuan, Liu, Li, Wang, Lang, Qian, Jiang, Yuan and Wang</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Objectives</title>
        <p>To propose a deep learning-based classification framework, which can carry out patient-level benign and malignant tumors classification according to the patient’s multi-plane images and clinical information.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>A total of 430 cases of spinal tumor, including axial and sagittal plane images by MRI, of which 297 cases for training (14072 images), and 133 cases for testing (6161 images) were included. Based on the bipartite graph and attention learning, this study proposed a multi-plane attention learning framework, BgNet, for benign and malignant tumor diagnosis. In a bipartite graph structure, the tumor area in each plane is used as the vertex of the graph, and the matching between different planes is used as the edge of the graph. The tumor areas from different plane images are spliced at the input layer. And based on the convolutional neural network ResNet and visual attention learning model Swin-Transformer, this study proposed a feature fusion model named ResNetST for combining both global and local information to extract the correlation features of multiple planes. The proposed BgNet consists of five modules including a multi-plane fusion module based on the bipartite graph, input layer fusion module, feature layer fusion module, decision layer fusion module, and output module. These modules are respectively used for multi-level fusion of patient multi-plane image data to realize the comprehensive diagnosis of benign and malignant tumors at the patient level.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>The accuracy (ACC: 79.7%) of the proposed BgNet with multi-plane was higher than that with a single plane, and higher than or equal to the four doctors’ ACC (D1: 70.7%, p=0.219; D2: 54.1%, p&lt;0.005; D3: 79.7%, p=0.006; D4: 72.9%, p=0.178). Moreover, the diagnostic accuracy and speed of doctors can be further improved with the aid of BgNet, the ACC of D1, D2, D3, and D4 improved by 4.5%, 21.8%, 0.8%, and 3.8%, respectively.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>The proposed deep learning framework BgNet can classify benign and malignant tumors effectively, and can help doctors improve their diagnostic efficiency and accuracy. The code is available at <uri xlink:href="https://github.com/research-med/BgNet">https://github.com/research-med/BgNet</uri>.</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>tumor classification</kwd>
      <kwd>deep learning</kwd>
      <kwd>multi-plane fusion</kwd>
      <kwd>benign</kwd>
      <kwd>malignant</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Natural Science Foundation of China
</institution>
            <institution-id institution-id-type="doi">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn001">81871326, 81971578, 82102638</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>Beijing Municipal Natural Science Foundation
</institution>
            <institution-id institution-id-type="doi">10.13039/501100005089</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn002">Z190020</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="4"/>
      <equation-count count="3"/>
      <ref-count count="35"/>
      <page-count count="12"/>
      <word-count count="6783"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>The tumor is one of the main causes of human health problems. According to the statistics of the World Health Organization, millions of people die of cancer every year. Tumors can be divided into benign and malignant. Benign tumors may cause local destruction and even invasive growth of other surrounding tissues, if not detected early. Malignant tumors may cause systemic multisystem metastasis and threaten life. The diagnosis of benign and malignant tumors through patient image data, such as X-ray, CT, or MRI, in the early stage, can guide the formulation of the clinical treatment plan and have important clinical significance.</p>
    <p>With the development of computer technology, the use of artificial intelligence (AI) technology for medical image-aided diagnosis has increasingly attracted attention. Traditional image analysis methods generally extract manually designed features and then use support vector machines (<xref rid="B1" ref-type="bibr">1</xref>), clustering, decision trees, or artificial neural networks (<xref rid="B2" ref-type="bibr">2</xref>) for classification. However, this effect is not ideal in the face of complex data. In recent years, an increasing number of researchers have used deep learning methods from natural images to medical images to assist doctors in diagnosis. The deep learning model can automatically extract and classify the multilevel deep features of the images. Using AI technology to classify benign and malignant tumors in the early stage can help doctors formulate corresponding treatment plans for patients in time and prevent deterioration of the disease, which have very important clinical and research values.</p>
    <p>Professionally trained doctors can easily locate tumor regions in images, but it is difficult to classify benign or malignant for the complex appearance of tumors. Some tumors usually have the characteristics of high heterogeneity, diverse location, unclear boundary, and unclear visual characteristics. In the clinic, radiologists usually need to observe multiple plane images, such as axial, sagittal, or coronal planes for comprehensive judgment. Existing AI analysis of medical images can be divided into two categories according to the data modality. One is single-modality data, which is used to analyze medical images through data processing and model improvement (<xref rid="B3" ref-type="bibr">3</xref>–<xref rid="B6" ref-type="bibr">6</xref>). Due to the single source of single-modality data, it may have limitations in some tasks. The other is multi-modal data, which carries out the corresponding tasks by constructing multi-modal models including cross-modal analysis, such as CT and MRI (<xref rid="B7" ref-type="bibr">7</xref>), MRI and ultrasound (<xref rid="B8" ref-type="bibr">8</xref>), MRI and PET (<xref rid="B9" ref-type="bibr">9</xref>), and also different sequences of CT (<xref rid="B10" ref-type="bibr">10</xref>, <xref rid="B11" ref-type="bibr">11</xref>) or MRI (<xref rid="B12" ref-type="bibr">12</xref>). While the aforementioned methods are focused on the same single planes, such as axial or sagittal. The same tumor can demonstrate various shapes in different planes, which has certain internal relevance and need to be explored from multi-plane image data. In addition, the patient’s clinical information has a great reference value for diagnosis. Existing methods lack the effective combination of different planes or even patients’ clinical information.</p>
    <p>To solve the aforementioned problems, this study proposed a multi-plane fusion framework named BgNet for tumor benign and malignant diagnosis at the patient level, which used a bipartite graph to splice different plane images at the input layer, and an attention mechanism model to carry out feature association mining and multilevel fusion on different plane images. BgNet can integrate different plane images and clinical age information of the same patient for patient-level tumor benign and malignant diagnosis. In addition, with the help of BgNet, the diagnostic accuracy and speed of doctors can be improved. Compared with other methods for processing single frame images, the main innovation of this paper is to apply bipartite graph to medical images, and use the proposed ResNetST model for multi-plane fusion, which can make the model more comprehensive in observing patients’ images.</p>
  </sec>
  <sec sec-type="materials|methods" id="s2">
    <title>Materials and methods</title>
    <sec id="s2_1">
      <title>Dataset</title>
      <p>In the clinic, bone tumors are usually diagnosed by observing multiple planes of medical images. The performance of the proposed method was evaluated on the classification task of benign and malignant spine tumors, which is challenging due to the complex appearance of images arising from tumor heterogeneity and varying locations. This study collected the MRI data of primary spinal tumors from 2006 to 2019 from the participating hospital, and it was approved by the Medical Science Research Ethics Committee review board. A total of 430 patients or cases with both axial and sagittal sequences and pathological reports were selected for experiments. Radiologists marked the tumor region using a rectangular box, which was checked by each of them for reliability. The benign and malignant classifications of these regions were based on the patients’ pathological reports. Detailed information of the dataset is shown in <xref rid="T1" ref-type="table"><bold>Table 1</bold></xref>, and the distribution of subtypes in the dataset is shown in <xref rid="f1" ref-type="fig"><bold>Figure 1</bold></xref>; the age distribution was large, some tumors were multiple, and the locations of the tumor varied, such as cervical, thoracic, or lumbar vertebrae, which are complex and challenging for AI model.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>The specific information of the training set and testing set.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">Training Set (n = 297)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Testing Set (n = 133)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Age (years, mean±SD)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40.2± 20.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38.8± 19.1</td>
            </tr>
            <tr>
              <td valign="top" rowspan="2" align="left" colspan="1">Gender</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Female</td>
              <td valign="top" align="center" rowspan="1" colspan="1">152 (51.2%)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">48 (36.1%)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Male</td>
              <td valign="top" align="center" rowspan="1" colspan="1">145 (48.8%)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85 (63.9%)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Slice thickness</td>
              <td valign="top" align="left" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.0mm~8.0mm</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3.0mm~6.0mm</td>
            </tr>
            <tr>
              <td valign="top" rowspan="4" align="left" colspan="1">Location</td>
              <td valign="top" align="left" rowspan="1" colspan="1">cervical vertebrae</td>
              <td valign="top" align="center" rowspan="1" colspan="1">167</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">thoracic vertebrae</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89</td>
              <td valign="top" align="center" rowspan="1" colspan="1">48</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">lumbar vertebrae</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">sacral vertebrae</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5</td>
            </tr>
            <tr>
              <td valign="top" rowspan="2" align="left" colspan="1">Number of Sequences</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Axial</td>
              <td valign="top" align="center" rowspan="1" colspan="1">491</td>
              <td valign="top" align="center" rowspan="1" colspan="1">230</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sagittal</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1089</td>
              <td valign="top" align="center" rowspan="1" colspan="1">491</td>
            </tr>
            <tr>
              <td valign="top" rowspan="2" align="left" colspan="1">Number of Images</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Axial</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4049</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1597</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sagittal</td>
              <td valign="top" align="center" rowspan="1" colspan="1">10023</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4564</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Labeled Images</td>
              <td valign="top" align="left" rowspan="1" colspan="1">–</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14072</td>
              <td valign="top" align="center" rowspan="1" colspan="1">6161</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>SD, standard deviation.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <fig position="float" id="f1">
        <label>Figure 1</label>
        <caption>
          <p>Distribution of subtypes in the dataset.</p>
        </caption>
        <graphic xlink:href="fonc-12-971871-g001" position="float"/>
      </fig>
    </sec>
    <sec id="s2_2">
      <title>Method</title>
      <p>Most existing AI methods are based on single plane analysis. In the clinic, radiologists usually need to observe multiple plane images and sequences of the same patient for comprehensive judgment. Inspired by this processing, this study proposes a comprehensive diagnostic AI framework, BgNet, which used a bipartite graph to fuse the data of the two planes, the tumor area in each plane is used as the vertex of the graph, and the matching between different planes is used as the edge of the graph. By integrating all edges in the bipartite graph, the patient-level diagnosis results of benign and malignant tumors can be obtained. The framework of BgNet can be seen in <xref rid="f2" ref-type="fig"><bold>Figure 2</bold></xref>, which consists of five parts. In the first part, the matching pairs between different planes are constructed through the bipartite graph. In the second part, the tumor areas on different planes represented by each matching pair are fused in the input layer. The tumor areas of axial and sagittal images were scaled, and combined up and down to form a single image in the input layer. In the third part, the proposed model named ResNetST to extract and fuse the features of different planes from the input layer, with the convolutional neural network ResNet50 (<xref rid="B13" ref-type="bibr">13</xref>) as the feature extraction module and the Swin-Transformer (<xref rid="B14" ref-type="bibr">14</xref>) model based on the attention learning as the global feature fusion module. Finally, all matching edges between different planes of patients are integrated through a trusted edge-set screening strategy to obtain the final patient-level diagnosis results. The patients’ biopsy-confirmed labels were used to train and test the model. This paper focuses on the fusion of different planes and the same plane can contain different sequences, e.g. T1, T2.</p>
      <fig position="float" id="f2">
        <label>Figure 2</label>
        <caption>
          <p>The framework of BgNet is divided into five modules, including a multi-plane fusion module based on the bipartite graph, an input layer fusion module, a feature layer fusion module, a decision layer fusion module, and an output module. In the first part, the matching pairs between different planes are constructed through the bipartite graph. In the second part, the tumor regions on different planes represented by each matching pair are fused in the input layer. In the third part, the proposed model named ResNetST is used to extract and fuse the features of different planes. Finally, all the matching edges between the different planes of the patients are integrated through a trusted edge set screening strategy to obtain the final patient-level classification results.</p>
        </caption>
        <graphic xlink:href="fonc-12-971871-g002" position="float"/>
      </fig>
      <sec id="s2_2_1">
        <title>Multi-plane fusion module based on the bipartite graph</title>
        <p>Bipartite graph is a special model in the graph theory (<xref rid="B15" ref-type="bibr">15</xref>). Let <italic>G</italic>(<italic>V</italic>, <italic>E</italic>) be an undirected graph. If vertex <italic>v</italic> can be divided into two disjoint subsets <italic>A</italic> and <italic>B</italic> , and the two vertices <italic>i</italic> and <italic>j</italic> associated with each edge <italic>e</italic>(<italic>i</italic>, <italic>j</italic>) in the graph belong to these two different vertex sets (<italic>i</italic>∈<italic>A</italic>,<italic>j</italic>∈<italic>B</italic>) , then the graph <italic>G</italic> is called a bipartite graph.</p>
        <p>Referring to the radiologists’ reading process, we proposed a multi-plane fusion strategy based on a bipartite graph. First, we constructed a bipartite graph, as shown in <xref rid="f2" ref-type="fig"><bold>Figure 2</bold></xref>. Here, we consider MRI axial and sagittal images and divide images of the same patient into two plane sets. The axial images were set <italic>A</italic> , and the sagittal images were set <italic>B</italic> . Each image in the set was used as the vertex to connect the graphs to form a complete bipartite graph. These connections are called matching edges. Assuming that there are <italic>n</italic> images in set <italic>A</italic> and <italic>m</italic> images in set <italic>B</italic>, we obtain <italic>n</italic>×<italic>m</italic> matching edges between <italic>A</italic> and <italic>B</italic> . The edges in the bipartite graph will be fused in the input layer fusion phase described below, and then the common features will be extracted using the deep learning model.</p>
        <p>In the training stage, the edge <italic>e</italic>(<italic>i</italic>,<italic>j</italic>) between set <italic>A</italic> and set <italic>B</italic> was randomly activated. The model takes the activated edge as the input to perform the input layer fusion and feature layer fusion and calculates the loss function and backpropagation of the gradient. In the test stage, all edges of <italic>E</italic> in sets <italic>A</italic> and set <italic>B</italic> were activated. The model takes all edges as inputs to perform input layer fusion, feature layer fusion, and decision layer fusion.</p>
      </sec>
      <sec id="s2_2_2">
        <title>Input layer fusion module</title>
        <p>The input layer fusion module is shown in <xref rid="f2" ref-type="fig"><bold>Figure 2</bold></xref>. Two vertices <italic>i</italic> and <italic>j</italic> of the activated edge <italic>e</italic>(<italic>i</italic>,<italic>j</italic>) , which are a frame of the axial image and a frame of the sagittal image, are fused in the input layer. The tumor areas of axial and sagittal images were scaled, combined up and down to form a single image, in which the axial is above and sagittal is below as shown in <xref rid="f2" ref-type="fig"><bold>Figure 2</bold></xref>, and resized to form a 224×224×3 image. Then, the feature-layer fusion model was used to extract the common features.</p>
        <p>Before sending the image to the feature-layer fusion model, a series of data augmentation techniques will be done to prevent the model from overfitting, which will be introduced next. In the training stage, we did not extract the tumor area marked by the doctors strictly, but expanded 40 to 60 pixels around the marked tumor area, so that the model can also learn the information around the tumor. In addition, we randomly flipped the tumor area, including the up and down direction, left and right direction. In the stage of training and testing, we used <inline-formula><mml:math id="im1" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> to standardize the image, where <italic>μ</italic> is the mean value of all pixels of the image, and <italic>σ</italic> is the standard deviation of all pixels of the image.</p>
      </sec>
      <sec id="s2_2_3">
        <title>Feature layer fusion module</title>
        <p>Convolutional neural network (CNN) can adaptively learn the features and spatial hierarchy of images (<xref rid="B16" ref-type="bibr">16</xref>), and the attention mechanism can correlate the global features well (<xref rid="B17" ref-type="bibr">17</xref>), Based on the above technologies, we proposed ResNetST as the feature layer fusion model by feature extraction and attention learning, and the network structure is shown in <xref rid="f3" ref-type="fig"><bold>Figure 3</bold></xref>, including five parts. The first part contains conv, norm, relu, and maxpool operations to reduce the dimension of the input image and reduce the computational complexity. The second part is ResNet Stage 1, which performs feature extraction at 1/4 resolution of the image. The third part is ResNet Stage 2, which performs feature extraction at 1/8 resolution of the image. The fourth part is the feature fusion module, which uses the attention mechanism to correlate the extracted features globally and extracts deeper features. The fifth part includes the global average pooling and output, which uses fully connected networks as classifiers to get the benign and malignant probabilities of these edges. In the training stage, cross-entropy was used to calculate the loss, and the BP algorithm (<xref rid="B18" ref-type="bibr">18</xref>) was used for gradient backpropagation and model parameter updating. In the testing stage, the benign and malignant probabilities of the edge are directly output.</p>
        <fig position="float" id="f3">
          <label>Figure 3</label>
          <caption>
            <p>The structure of the feature layer fusion module. It includes five parts. The first part contains Conv, Norm, Relu, and max pool operations to reduce the dimension of the input image and reduce the computational complexity. The second part is ResNet Stage 1, which performs feature extraction at 1/4 the resolution of the image. The third part is ResNet Stage 2, which performs feature extraction at 1/8 the resolution of the image. The fourth part is the feature fusion module, which uses the attention mechanism to correlate the extracted features globally and extracts deeper features. The fifth part includes the global average pooling and output, which employs fully connected networks as a classifier to obtain the benign and malignant probabilities of these edges.</p>
          </caption>
          <graphic xlink:href="fonc-12-971871-g003" position="float"/>
        </fig>
      </sec>
      <sec id="s2_2_4">
        <title>Decision layer fusion module and output module</title>
        <p>Decision-layer fusion was only used in the testing (or inference) stage. All edges of the above bipartite graph are activated and predict these matching edges one by one to obtain the probability that they are benign or malignant. For the same patient, set <italic>C</italic> is obtained in the decision layer fusion module, in which each element is the benign and malignant probability of the corresponding matching edge. The purpose of decision layer fusion is to judge the overall benign and malignant category of the patient according to set <italic>C</italic>.</p>
        <p>In clinical practice, the age information of patients has a significant reference for diagnosis. First, we obtained the relationship set <italic>K</italic> between the patient’s age and the probability of benign and malignant tumors according to the statistical information of all patients in the training set. Each element in set <italic>C</italic> and the probability of benign and malignant tumors at the patient’s current age are weighted and summed, and the weights are <italic>λ</italic>
<sub>1</sub> and <italic>λ</italic>
<sub>2</sub> , wherein <italic>C</italic>
<sub><italic>i</italic></sub>=<italic>λ</italic>
<sub>1</sub>×<italic>C</italic>
<sub><italic>i</italic></sub>+<italic>λ</italic>
<sub>2</sub>×<italic>K</italic>
<sub><italic>i</italic></sub> . Then, each element in set <italic>C</italic> is sorted from large to small, and the <italic>topk</italic> elements with the largest probability are taken out to form a set <italic>S</italic> , that is, the <italic>topk</italic> matching edges that the model considers to be more accurate in prediction. Then, taking <italic>T</italic> as the threshold, the matching edge with probability greater than <italic>T</italic> in set <italic>S</italic> will be judged as malignant, less than <italic>T</italic> will be judged as benign, and equal to <italic>T</italic> will be discarded. Next, we obtain the number <italic>q</italic> of benign matching edges and the number <italic>w</italic> of malignant matching edges in the set <italic>S</italic> , and calculate the probability <inline-formula><mml:math id="im2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>q</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> for predicting benign tumors and <inline-formula><mml:math id="im3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>w</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> for predicting malignant tumors for the current patient. If <italic>p</italic>
<sub><italic>b</italic></sub>&gt;<italic>p</italic>
<sub><italic>m</italic></sub> , the model predicts that the current patient has a benign tumor, and if <italic>p</italic>
<sub><italic>b</italic></sub>≤<italic>p</italic>
<sub><italic>m</italic></sub> , the model predicts the current patient as having a malignancy.</p>
      </sec>
    </sec>
    <sec id="s2_3">
      <title>Experimental design</title>
      <p>In our experiments, MRI axial and sagittal images of 430 cases with primary spine tumors from 2006 to 2019 from the participating hospital were used, with 297 cases included for training and 133 cases for testing, and all cases were approved by the Medical Science Research Ethics Committee review board. We design the following experiments to verify the effectiveness of the proposed multi-plane fusion framework.</p>
      <p>Firstly, we conducted experiments based on a single plane, such as using only axial or sagittal images. That is, we use ResNetST mentioned above as the classification model and use the patient’s axial or sagittal image data for training and testing. In the test stage, according to all the axial or sagittal data of the patient, we will get set <italic>C</italic> , where each element is the probability that each image is benign or malignant. Like the decision layer fusion mentioned above, we can get the patient-level benign and malignant diagnosis results through set <italic>C</italic> .</p>
      <p>Secondly, we conduct mixed training on an axial plane and sagittal plane and finally make decision layer fusion. This is, we also use ResNetST mentioned above as the classification model, but mixed the axial and sagittal image data for training and testing. The mean of “mixed” is that during training and testing, the axial and sagittal planes are still single frame independent images. If the patient has <italic>n</italic> axial images and <italic>m</italic> sagittal images, the set <italic>C</italic> containing <italic>n</italic>+<italic>m</italic> elements will be obtained in the test stage. Similarly, patient-level fusion is performed according to set <italic>C</italic> to obtain the overall benign and malignant diagnosis result of the patient.</p>
      <p>Third, we used multi-plane fusion based on a bipartite graph. That is, different from the previous two experiments, we matched the axial and sagittal images of patients and then fused the input layer and feature layer. Assuming that the patient has <italic>n</italic> axial images and <italic>m</italic> sagittal images, in the test stage, a set C containing <italic>n</italic>×<italic>m</italic> elements will be obtained. Similarly, the patient-level diagnosis results are obtained according to set <italic>C</italic> .</p>
      <p>Finally, we evaluated the performance of referring to age information based on the first three experiments. In our experiments, the proposed ResNetST was better than the original ResNet (<xref rid="B13" ref-type="bibr">13</xref>) and Swin-Transformer (<xref rid="B14" ref-type="bibr">14</xref>) as the feature-layer fusion module. ResNetST can combine the advantages of ResNet and Swin-Transformer for this task. All experiments used ResNetST as a feature layer fusion model, but this does not mean that in BgNet, the feature layer fusion model must be ResNetST, it can be changed according to different tasks.</p>
      <p>To ensure the fairness of comparison, all the above experiments use the same feature layer fusion model, decision layer fusion module, and super parameters except for the different data (single-plane or multi-plane). The results of AI methods were also compared with four doctors, including one spine surgeon (D2: 8 years experience) and three radiologists (D1: 3 years experience; D3: 11 years experience; D4: 18 years experience). Four doctors independently predicted benign and malignant tumors by checking axial and sagittal MRI images with age information on the testing set. All doctors had trained in neuroradiology or musculoskeletal fellowship. In addition, we also evaluated the results of doctors assisted by an artificial intelligence model.</p>
    </sec>
    <sec id="s2_4">
      <title>Metrics</title>
      <p>In this study, samples of malignant tumors were considered positive samples, and the area under the curve (AUC), accuracy (ACC), sensitivity (SE), specificity (SP), and average time spent to predict each case were used as evaluation metrics. AUC is obtained by drawing a ROC curve for the probability of benign and malignant tumors diagnosed by each patient according to the model and taking the area under the curve. Since the best classification threshold of the model cannot be known in advance, in order to be fair, we take 0.5 as the classification threshold <italic>T</italic> to calculate ACC, SE, and SP. Then, we get the confusion matrix including TP, FP, TN, and FN. The calculation method of ACC, SE, and SP is as shown in equations (1), (2), and (3). We take the average time of each patient in the model and doctor diagnostic test set as the metrics of diagnostic efficiency. It should be noted again that we calculate all the above metrics based on the patient-level diagnostic results obtained from all the patient’s image data.</p>
      <disp-formula>
        <label>(1)</label>
        <mml:math id="M1" display="block" overflow="scroll">
          <mml:mrow>
            <mml:mi>A</mml:mi>
            <mml:mi>C</mml:mi>
            <mml:mi>C</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mi>T</mml:mi>
                <mml:mi>N</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mi>T</mml:mi>
                <mml:mi>N</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mi>F</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mi>F</mml:mi>
                <mml:mi>N</mml:mi>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <disp-formula>
        <label>(2)</label>
        <mml:math id="M2" display="block" overflow="scroll">
          <mml:mrow>
            <mml:mi>S</mml:mi>
            <mml:mi>E</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mi>F</mml:mi>
                <mml:mi>N</mml:mi>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <disp-formula>
        <label>(3)</label>
        <mml:math id="M3" display="block" overflow="scroll">
          <mml:mrow>
            <mml:mi>S</mml:mi>
            <mml:mi>P</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>N</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>F</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mi>T</mml:mi>
                <mml:mi>N</mml:mi>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </mml:math>
      </disp-formula>
    </sec>
    <sec id="s2_5">
      <title>Training setting</title>
      <p>We evaluated our artificial intelligence model using the testing set containing 133 cases or patients. In the training stage, all methods used the same super parameters, with a learning rate of 0.0002. Stochastic gradient descent was used as the optimizer (<xref rid="B19" ref-type="bibr">19</xref>) and the number of iterations was 20 epochs. The super parameters of ResNetST are the same as ResNet50 and Swin-Transformer Tiny. The ResNet Stage 1 and Stage 2 in ResNetST are initialized using the pre-trained model on ImageNet. All methods use four 12G 1080ti GPUs for distributed training. In the decision layer fusion, <inline-formula><mml:math id="im4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.4</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>300</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. All experiments used the same decision layer fusion method to obtain the patient-level results.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <sec id="s3_1">
      <title>Comparison of the different methods</title>
      <p>In <xref rid="T2" ref-type="table"><bold>Table 2</bold></xref>, the AUC of the model with only reference to the sagittal image (Sag: 73.1%) is 1.0% higher than that of the axial image (Axi: 72.1%). The AUC with mixed axial and sagittal images (Axi_Sag: 76.7%) is 4.6% and 3.6% higher than that of single plane Axi and Sag. Based on the BG, the AUC of BG_Axi_Sag (81.8%) was further improved by 9.7%, 8.7%, and 5.1% compared to Axi, Sag, and Axi_Sag, respectively.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Results of the different methods in the classification of benign and malignant tumors based on the primary spinal tumors dataset (95% CI).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">Age</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Plane</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Method</th>
              <th valign="top" align="center" rowspan="1" colspan="1">AUC (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">ACC (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">SE (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">SP (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" rowspan="4" align="left" colspan="1">✘</td>
              <td valign="top" rowspan="2" align="left" colspan="1">single</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Axi</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.1±7.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.4±7.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.2±8.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.6±7.6</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sag</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.1±7.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.7±7.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">49.0±8.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.6±7.0</td>
            </tr>
            <tr>
              <td valign="top" rowspan="2" align="left" colspan="1">multi</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Axi_Sag</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.7±7.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">70.7±7.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.2±8.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.2±7.2</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>BG_Axi_Sag</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>81.8</bold>±6.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>70.7</bold>±7.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>87.8</bold>±5.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.7±8.3</td>
            </tr>
            <tr>
              <td valign="top" rowspan="4" align="left" colspan="1">✔</td>
              <td valign="top" rowspan="2" align="left" colspan="1">single</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Axi_Age</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.3±7.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.2±7.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.2±8.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.0±7.4</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sag_Age</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.4±7.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.4±8.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">44.9±8.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.4±7.1</td>
            </tr>
            <tr>
              <td valign="top" rowspan="2" align="left" colspan="1">multi</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Axi_Sag_Age</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.0±6.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.2±7.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.2±8.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.8±6.8</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>BG_Axi_Sag_Age (BgNet)</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>84.3</bold>±6.2</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>79.7</bold>±6.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>91.8</bold>±4.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.6±7.6</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p><bold>Axi</bold> represents the axial images. Sag represents the sagittal images. BG represents the bipartite image-fusion strategy. Age represents a reference to age. CI, confidence interval; AUC, area under the curve; ACC, accuracy; SE, sensitivity; SP, specificity.</p>
            <p>Bold indicates the result with the best result.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>After referring to the age information, the AUC of all the AI models improved. The AUC of Sag_Age was 4.1% higher than that of Axi_Age. The AUC of the mixed axial and sagittal images was 7.7% and 3.6% higher than that of Axi_Age and Sag_Age. BgNet can obtain the highest AUC (84.3%) and ACC (79.7%) after referencing the age. The AUC of BgNet is improved by 2.5% to 13.0%, and the ACC is improved by 7.5% to 14.3% with age information compared to the other methods.</p>
    </sec>
    <sec id="s3_2">
      <title>Comparison of the model and doctors</title>
      <p>In the case of reference the age, the comparison results with doctors are shown in <xref rid="T3" ref-type="table"><bold>Table 3</bold></xref>. The ACC of BgNet was equal to D3 and was 9.0%, 25.6%, and 6.8% higher than those of D1, D2, and D4, respectively. The SE of BgNet was 2.0%, 22.4%, and 20.4% higher than those of D1, D3, and D4, respectively. Although the SE of D2 (95.9%) was higher than that of BgNet, the corresponding SP (29.8%) was low. The SP of BgNet (72.6%) was 13.1% and 42.8% higher than that of D1 and D2, respectively, which approached D4 (73.8%). Although the SP of D3 (85.7%) was higher than that of BgNet, its SE (69.4%) was low. The ACC of D3 (79.7%) was equal to that of BgNet; however, the average time for predicting a patient was 74.9 s, while it was only 0.7 s for BgNet.</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Comparison of the results between the model and the doctors (95% CI).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" colspan="2" align="left" rowspan="1">Method</th>
              <th valign="top" align="center" rowspan="1" colspan="1">ACC (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">SE (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">SP (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Avg. Time (s)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">p-value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" rowspan="4" align="left" colspan="1">Doctors</td>
              <td valign="top" align="center" rowspan="1" colspan="1">D1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">70.7±7.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.8±5.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.5±8.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">29.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">p=0.219</td>
            </tr>
            <tr>
              <td valign="top" align="center" rowspan="1" colspan="1">D2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.1±8.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.9±3.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">29.8±7.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">18.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">p&lt;0.005</td>
            </tr>
            <tr>
              <td valign="top" align="center" rowspan="1" colspan="1">D3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.7±6.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.4±7.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">85.7±5.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">p=0.006</td>
            </tr>
            <tr>
              <td valign="top" align="center" rowspan="1" colspan="1">D4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">72.9±7.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">71.4±7.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">73.8±7.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">p=0.178</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>AI Model</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>BgNet</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>79.7</bold>±6.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>91.8</bold>±4.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1"><bold>72.6</bold>±7.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.7</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">–</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>Three radiologists D1, D3 and D4 and one spine surgeon D2. D1: 3 years’ experience; D2: 8 years’ experience; D3: 11 years’ experience; D4: 18 years’ experience CI, confidence interval; AUC, area under the curve; ACC, accuracy; SE, sensitivity; SP, specificity.</p>
            <p>Bold indicates the result with the best result.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="s3_3">
      <title>Comparison of the different vertebral locations</title>
      <p>To specifically analyze the results of BgNet and doctors, we counted the number of patients with incorrect prediction and error rates in different vertebral locations, as well as the distribution of vertebral locations in the testing set, as shown in <xref rid="f4" ref-type="fig"><bold>Figure 4</bold></xref>.</p>
      <fig position="float" id="f4">
        <label>Figure 4</label>
        <caption>
          <p>Comparison of the different vertebral locations. <bold>(A)</bold> Number of cases with wrong prediction in different vertebral locations. <bold>(B)</bold> Vertebral location of the distribution of cases in the testing set. <bold>(C)</bold> Error rates in the different locations.</p>
        </caption>
        <graphic xlink:href="fonc-12-971871-g004" position="float"/>
      </fig>
      <p>The number of cases with incorrect prediction and error rates by BgNet in each location was lower than those of the doctors. As shown in <xref rid="f4" ref-type="fig"><bold>Figure 4A</bold></xref>, BgNet, D2, and D4 had the largest incorrect predictions at the cervical vertebra, and D1 and D3 had the largest number in the thoracic vertebra, while both BgNet and doctors had the lowest number in the sacral vertebra. By observing the number distribution in different vertebral locations in <xref rid="f4" ref-type="fig"><bold>Figure 4B</bold></xref>, the number of cases in the cervical and thoracic vertebrae is large, and the misprediction trend of BgNet and doctors is consistent with the location distribution.</p>
      <p>However, as shown in <xref rid="f4" ref-type="fig"><bold>Figure 4C</bold></xref>, the error rate trend of BgNet and doctors is different from that of <xref rid="f4" ref-type="fig"><bold>Figure 4B</bold></xref>. For example, most doctors and BgNet have a lower error rate in the cervical vertebrae and the highest error rate in the lumbar vertebrae. The reason for this phenomenon is that both BgNet and doctors need to accumulate experience from a large number of cases. The more cases, the richer the experience, and the lower the error rate. Our testing set generally reflects the actual distribution of patients in cooperative hospitals. With the largest number of cases in the cervical vertebra, the number of patients with incorrect predictions may be higher, while the error rate is lower.</p>
    </sec>
    <sec id="s3_4">
      <title>Comparison of the different age groups</title>
      <p>In addition to vertebral locations, we also specifically analyzed the results according to different age groups, as shown in <xref rid="f5" ref-type="fig"><bold>Figure 5</bold></xref>. From <xref rid="f5" ref-type="fig"><bold>Figure 5A</bold></xref> and <xref rid="f5" ref-type="fig"><bold>Figure 5C</bold></xref>, the number of cases with incorrect prediction and error rate by BgNet in each age group is lower than most doctors, and the error rate in the two age groups of 20–39 and 60–79 is lower than that of the other two age groups.</p>
      <fig position="float" id="f5">
        <label>Figure 5</label>
        <caption>
          <p>Comparison of the different age groups. <bold>(A)</bold> Number of cases with the wrong prediction in different age groups. <bold>(B)</bold> The age distribution of the cases in the test set. <bold>(C)</bold> Error rates in the different age groups.</p>
        </caption>
        <graphic xlink:href="fonc-12-971871-g005" position="float"/>
      </fig>
      <p>The number of wrong cases in the 40–59 age group was the largest for both doctors and BgNet. The 40–59 age group included 31 cases of cervical vertebrae, 17 cases of thoracic vertebrae, nine cases of lumbar vertebrae, and one case of sacral vertebrae. In this age group, the BgNet mispredicted nine cervical cases, five thoracic cases, and five lumbar cases. Overall, the error rate of BgNet in this age group was 29.0%, 29.4%, 55.6%, and 0% for the cervical, thoracic, lumbar, and sacral vertebrae, respectively. Combined with <xref rid="f4" ref-type="fig"><bold>Figure 4</bold></xref>, it can be seen that the increase in the error rate of the thoracic and lumbar vertebrae caused an increase in the overall error rate in this age group. In addition, the error rate trends of D1 and D2 are relatively consistent, and the error rate of D3 in the age group 0–19 is much lower than that of BgNet, but in the age group 20–39 is much higher than that of BgNet.</p>
    </sec>
    <sec sec-type="results" id="s3_5">
      <title>Results of the classification by doctors assisted with the AI model</title>
      <p>To evaluate whether the AI model can assist doctors in improving their diagnosis accuracy and efficiency, we again invited those 4 doctors to label the testing cases. Based on MRI data, age information and additional malignant probability of each case given by BgNet model, doctors can make their final comprehensive diagnosis for benign or malignant tumor. The results are shown in <xref rid="T4" ref-type="table"><bold>Table 4</bold></xref>, the diagnostic accuracy and speed of all four doctors have been significantly improved. For example, with the highest diagnostic accuracy in <xref rid="T4" ref-type="table"><bold>Table 4</bold></xref>, D3’s ACC increased from 79.7% to 80.5%. D4’s ACC increased from 72.9% to 76.7%, and his diagnostic time for each case decreased from 31.4 s to 15.6 s. So with the help of BgNet, doctors can improve their diagnostic accuracy and diagnose twice as fast as before.</p>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>Classification results of the doctors assisted by BgNet.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">Doctors</th>
              <th valign="top" align="center" rowspan="1" colspan="1">ACC (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">SE (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">SP (%)</th>
              <th valign="top" align="center" rowspan="1" colspan="1">Avg. Time (s)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D1</td>
              <td valign="top" align="char" char="±" rowspan="1" colspan="1">75.2+4.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95.9+6.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">63.1+3.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">17.0-12.5</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.9+21.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.8-8.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">69.1+39.3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.5-5.3</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D3</td>
              <td valign="top" align="char" char="±" rowspan="1" colspan="1">80.5+0.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">89.8+20.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.0-10.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">34.3-40.6</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D4</td>
              <td valign="top" align="char" char="±" rowspan="1" colspan="1">76.7+3.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">75.5+4.1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">77.4+3.6</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15.6-15.8</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn>
            <p>Based on the MRI data, age information, and additional malignant probability of each case given by the BgNet model, doctors made their final comprehensive classification for benign or malignant tumors. D1, D3, and D4, and one spine surgeon D2. D1: 3 year experience; D2: 8 year experience; D3: 11 year experience; D4: 18 year experience. AI, artificial intelligence; AUC, area under the curve; ACC, accuracy; SE, sensitivity; SP, specificity.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="s3_6">
      <title>Heat maps analysis</title>
      <p><xref rid="f6" ref-type="fig"><bold>Figure 6</bold></xref> shows some heat maps of BgNet, which can represent the information focused on the model in the inferencing process. The images of each line in <xref rid="f6" ref-type="fig"><bold>Figure 6</bold></xref> are from different cases, including three benign and three malignant cases, and the ground truth is listed in the first column. The axial and sagittal images in the left two columns contain a rectangular box marked by a doctor. The third column is one of the edges of the bipartite graph used as the input of the feature-layer fusion model. The fifth column shows the diagnosis results of cases obtained by BgNet from all edges of the bipartite graph of the case and the results obtained by the four doctors. The blue square indicates that the case was diagnosed as benign, and the red square indicates that it was diagnosed as malignant. Heat maps are generated based on Grad-CAM++ (<xref rid="B20" ref-type="bibr">20</xref>), which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps concerning a specific class score as weights to generate a visual explanation for the corresponding class label. The redder the color, the more inclined the model is to predict it as malignant, and the bluer the color, the more inclined the model is to predict it as benign.</p>
      <fig position="float" id="f6">
        <label>Figure 6</label>
        <caption>
          <p>Part heat maps of BgNet. The redder the color, the more inclined the model is to predict it as malignant, and the bluer the color, the more inclined the model is to predict it as benign. The images of each line are from different cases, including three benign and three malignant cases, and the ground truth is listed in the first column. The axial and sagittal images in the left two columns contain a rectangular box marked by doctors. The third column is one of the edges of the bipartite graph used as the input of the feature-layer fusion model. The fifth column shows the classification results of the cases obtained by BgNet from all the edges of the bipartite graph of the case and the results obtained by the four doctors. The blue square indicates that the case was classified as benign, and the red square indicates that it was classified as malignant.</p>
        </caption>
        <graphic xlink:href="fonc-12-971871-g006" position="float"/>
      </fig>
      <p>It can be seen that BgNet obtains the correct diagnosis results for these six cases. For cases in lines 1 and 4, both BgNet and doctors make a correct diagnosis. From the heat map of one edge of the bipartite graph represented by these lines, BgNet can correct the response to benign and malignant regions. For the cases in lines 2 and 5, the prediction results of D2 and D1 were incorrect. It can be seen from line 2 that although BgNet focused on a suspected malignant area in the sagittal image, it finally made a correct judgment at the case level from the integration of the axial information, which shows the effectiveness of taking the edge of the bipartite graph as input. As predicted, the BgNet, will not be 100% correct when predicting each edge of the bipartite graph of the case. As shown in lines 3 and 6, BgNet tends to predict the edge represented by line 3 as malignant and the edge represented by line 6 as benign, which is contrary to the pathological results. However, the final diagnosis result for BgNet was correct. This is because, in the constructed bipartite graph, a single edge cannot represent the final diagnosis result. BgNet fuses the results of all edges at the decision layer and finally makes a judgment. In addition, for the cases represented by lines 3 and 6, two doctors made a misjudgment, which shows that the proposed BgNet has better adaptability to complex samples.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>Predicting benign and malignant tumors through patient images at an early stage is important for clinical treatment plans. Taking spine tumors as an example, inspired by the doctors’ diagnosis processing by checking multiple plane images, we proposed a novel framework, BgNet, to combine the patient’s axial and sagittal images with a bipartite graph strategy at the input layer. Then, the feature layer fusion model ResNetST fuses the axial and sagittal tumor areas represented by the edges in the bipartite graph at the feature layer. Finally, the results of all edges of the bipartite graph and clinical age information are integrated to obtain the final patient-level diagnosis results for the benign and malignant diagnoses of tumors.</p>
    <p>In recent years, deep learning methods (<xref rid="B21" ref-type="bibr">21</xref>–<xref rid="B23" ref-type="bibr">23</xref>) have gradually become the main methods in the field of pattern recognition and computer vision since 2012. The tumor diagnosis in the medical image refers to the use of a deep learning model, such as ResNet or VGG (<xref rid="B24" ref-type="bibr">24</xref>) to extract the features and classify the corresponding image or tumor area. Hong et al. (<xref rid="B25" ref-type="bibr">25</xref>) proposed a multi-information fusion framework based on MRI sagittal images, which can integrate a detection model and a sequence classification model for patient-level diagnosis. In recent years, the attention mechanism-based model Transformer (<xref rid="B26" ref-type="bibr">26</xref>) can better extract and correlate global information, which can be used for medical imaging.</p>
    <p>Multi-modal fusion of medical images has received increasing attention in the fields of medicine and computers, which can be divided into input-level fusion, feature-level fusion, and decision-level fusion (<xref rid="B27" ref-type="bibr">27</xref>). At the data level, it can be divided into image fusion of the same modality with different parameters, multi-sequence fusion, cross-modality, multiple planes, images, and clinical information fusion. For input level fusion, Yang et al. (<xref rid="B28" ref-type="bibr">28</xref>) created a channel splice for the four different scanning images, T1, T1c, T2, and flair in the input stage, and then sent them to the deep learning model for feature extraction and fusion. For feature-level fusion, Dolz et al. (<xref rid="B29" ref-type="bibr">29</xref>) proposed HyperDenseNet, a 3D full convolution neural network based on DenseNet (<xref rid="B30" ref-type="bibr">30</xref>), which extends the definition of dense connectivity to the multimodal segmentation model, receiving T1 and T2 sequences of MRI as inputs and achieving great results for brain tumor segmentation. Chen et al. (<xref rid="B31" ref-type="bibr">31</xref>) proposed a dual-branch multimodal brain tumor segmentation network, which uses two branches to extract the features of T1 and T2, respectively, and fused them at the end of these branches. Zhou et al. (<xref rid="B9" ref-type="bibr">9</xref>) proposed a multi-modality framework based on a deep non-negative matrix factorization model, which can fuse MRI and PET images for the diagnosis of dementia. Zhang et al. (<xref rid="B10" ref-type="bibr">10</xref>) proposed a modality-aware mutual learning method, which can fuse the arterial and venous phases of CT images for tumor segmentation. Most methods usually take decision level fusion as a part of the whole method, mainly including majority voting, averaging (<xref rid="B32" ref-type="bibr">32</xref>–<xref rid="B34" ref-type="bibr">34</xref>), etc. In addition, Reda et al. (<xref rid="B35" ref-type="bibr">35</xref>) built an additional classifier, which used the prediction probability of different modal models as input to classify the decision level, and the performance was improved compared with the model with a single modal only. Most of the existing methods are aimed at the fusion of single-plane data, and lack a multi plane fusion method.</p>
    <p>In contrast to the above method, we adopted a bipartite graph strategy to directly combine the different plane data, which can be used for further associated feature extraction and also greatly expands the number of training samples for deep learning model training. The attention mechanism in the ResNetST model can further extract the local and associated feature expressions from patient multi-plane data. Age information can improve final diagnosis performance. The experimental results show that the proposed BgNet can approach or exceed four medical experts, and with the help of BgNet, the diagnostic accuracy and speed of doctors have been significantly improved, which has clinical significance in medical image-aided diagnosis. Although we have carried out experiments on spine tumors and achieved certain results, the method is universal and can also be used in the classification of other tumors.</p>
    <p>The proposed BgNet still has room for improvement. In the test stage, we used all matching pairs between different planes. For example, an image of the axial corresponds to all sagittal images and this is effective in our experiment. While there may be some matching pairs that ultimately have a significant impact on the patient-level diagnosis. How to select these matching pairs in the early stage when constructing the bipartite graph? In addition, the certain correlation for the tumor areas of cross-modality, such as CT and MRI, will be existed, which should be studied and optimized on the cross-modal data based on our framework in the future.</p>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>Conclusions</title>
    <p>Due to the complex data of tumors, it is difficult to accurately diagnose patients only through a single plane. This paper proposed a novel multi-plane fusion framework BgNet, which fuses axial and sagittal MR images through an attention mechanism based on a bipartite graph and makes patient-level diagnosis combined with clinical age information. Experimental results showed that BgNet can efficiently identify benign and malignant tumors, and with the help of the AI model, the diagnostic accuracy and speed of doctors can be significantly improved.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are included in the article/supplementary material. Further inquiries can be directed to the corresponding authors.</p>
  </sec>
  <sec sec-type="author-contributions" id="s7">
    <title>Author contributions</title>
    <p>HL and M-LJ processed data, proposed methods, designed experiments, analyzed results, wrote, and modified the manuscript. X-YX, H-QO-Y, YY, J-FL, YL, C-JW, NL, LJ, and H-SY collected original data, labeled tumors, provided clinical suggestions, and reviewed the manuscript. Y-LQ and X-DW gave suggestions on methods. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s8">
    <title>Funding</title>
    <p>This work was supported by the Beijing Natural Science Foundation (Z190020), National Natural Science Foundation of China (62276250, 82171927, 81971578), Capital's Funds for Health Improvement and Research (2020-4-40916).</p>
  </sec>
  <sec sec-type="COI-statement" id="s9">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher’s note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <label>1</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hearst</surname><given-names>MA</given-names></name><name><surname>Dumais</surname><given-names>ST</given-names></name><name><surname>Osuna</surname><given-names>E</given-names></name><name><surname>Platt</surname><given-names>J</given-names></name><name><surname>Scholkopf</surname><given-names>B</given-names></name></person-group>. <article-title>Support vector machines</article-title>. <source>IEEE Intelligent Syst their Appl</source> (<year>1998</year>) <volume>13(4)</volume>:<fpage>18</fpage>–<lpage>28</lpage>. doi: <pub-id pub-id-type="doi">10.1109/5254.708428</pub-id>
</mixed-citation>
    </ref>
    <ref id="B2">
      <label>2</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chianca</surname><given-names>V</given-names></name><name><surname>Cuocolo</surname><given-names>R</given-names></name><name><surname>Gitto</surname><given-names>S</given-names></name><name><surname>Albano</surname><given-names>D</given-names></name><name><surname>Merli</surname><given-names>I</given-names></name><name><surname>Badalyan</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>Radiomic machine learning classifiers in spine bone tumors: a multi-software, multi-scanner study</article-title>. <source>Eur J Radiol</source> (<year>2021</year>) <volume>137</volume>:<fpage>109586</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.ejrad.2021.109586</pub-id>
<pub-id pub-id-type="pmid">33610852</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ben-Cohen</surname><given-names>A</given-names></name><name><surname>Diamant</surname><given-names>I</given-names></name><name><surname>Klang</surname><given-names>E</given-names></name><name><surname>Amitai</surname><given-names>M</given-names></name><name><surname>Greenspan</surname><given-names>H</given-names></name></person-group>. <article-title>Fully convolutional network for liver segmentation and lesions detection</article-title>. In: <source>Deep learning and data labeling for medical applications</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2016</year>). p. <page-range>(pp. 77–85)</page-range>.</mixed-citation>
    </ref>
    <ref id="B4">
      <label>4</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Christ</surname><given-names>PF</given-names></name><name><surname>Ettlinger</surname><given-names>F</given-names></name><name><surname>Grün</surname><given-names>F</given-names></name><name><surname>Elshaera</surname><given-names>MEA</given-names></name><name><surname>Lipkova</surname><given-names>J</given-names></name><name><surname>Schlecht</surname><given-names>S</given-names></name><etal/></person-group>. <source>Automatic liver and tumor segmentation of CT and MRI volumes using cascaded fully convolutional neural networks</source>. (<year>2017</year>), arXiv preprint arXiv:1702.05970.</mixed-citation>
    </ref>
    <ref id="B5">
      <label>5</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>Q</given-names></name><name><surname>Xia</surname><given-names>Y</given-names></name></person-group>. <article-title>Medical image classification using synergic deep learning</article-title>. <source>Med image Anal</source> (<year>2019</year>) <volume>54</volume>:<page-range>10–9</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.media.2019.02.010</pub-id>
</mixed-citation>
    </ref>
    <ref id="B6">
      <label>6</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isensee</surname><given-names>F</given-names></name><name><surname>Petersen</surname><given-names>J</given-names></name><name><surname>Kohl</surname><given-names>SA</given-names></name><name><surname>Jäger</surname><given-names>PF</given-names></name><name><surname>Maier-Hein</surname><given-names>KH</given-names></name></person-group>. <source>Nnu-net: Breaking the spell on successful medical image segmentation</source>, Vol. <volume>1</volume>. (<year>2019</year>). pp. <fpage>1</fpage>–<lpage>8</lpage>, arXiv preprint arXiv:1904.08128.</mixed-citation>
    </ref>
    <ref id="B7">
      <label>7</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>D</given-names></name><name><surname>Ahmad</surname><given-names>S</given-names></name><name><surname>Huo</surname><given-names>J</given-names></name><name><surname>Peng</surname><given-names>W</given-names></name><name><surname>Ge</surname><given-names>Y</given-names></name><name><surname>Xue</surname><given-names>Z</given-names></name><etal/></person-group>. <article-title>Synthesis and inpainting-based MR-CT registration for image-guided thermal ablation of liver tumors</article-title>. In: <source>International conference on medical image computing and computer-assisted intervention</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2019</year>). <page-range>pp. 512–20</page-range>.</mixed-citation>
    </ref>
    <ref id="B8">
      <label>8</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Song</surname><given-names>X</given-names></name><name><surname>Guo</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Chao</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Turkbey</surname><given-names>B</given-names></name><etal/></person-group>. <article-title>Cross-modal attention for MRI and ultrasound volume registration</article-title>. In: <source>International conference on medical image computing and computer-assisted intervention</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2021</year>). <page-range>pp. 66–75</page-range>.</mixed-citation>
    </ref>
    <ref id="B9">
      <label>9</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>T</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Fu</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Shen</surname><given-names>J</given-names></name><name><surname>Shao</surname><given-names>L</given-names></name><etal/></person-group>. <article-title>Deep multi-modal latent representation learning for automated dementia diagnosis</article-title>. In: <source>International conference on medical image computing and computer-assisted intervention</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2019</year>). <page-range>pp. 629–38</page-range>.</mixed-citation>
    </ref>
    <ref id="B10">
      <label>10</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Tian</surname><given-names>J</given-names></name><name><surname>Shi</surname><given-names>Z</given-names></name><name><surname>Zhong</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><etal/></person-group>. <article-title>Modality-aware mutual learning for multi-modal medical image segmentation</article-title>. In: <source>International conference on medical image computing and computer-assisted intervention</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2021</year>). <page-range>pp. 589–99</page-range>.</mixed-citation>
    </ref>
    <ref id="B11">
      <label>11</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Peng</surname><given-names>C</given-names></name><name><surname>Peng</surname><given-names>L</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Tong</surname><given-names>R</given-names></name><name><surname>Lin</surname><given-names>L</given-names></name><etal/></person-group>. <article-title>Multi-phase liver tumor segmentation with spatial aggregation and uncertain region inpainting</article-title>. In: <source>International conference on medical image computing and computer-assisted intervention</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2021</year>). <page-range>pp. 68–77</page-range>.</mixed-citation>
    </ref>
    <ref id="B12">
      <label>12</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Syazwany</surname><given-names>NS</given-names></name><name><surname>Nam</surname><given-names>JH</given-names></name><name><surname>Lee</surname><given-names>SC</given-names></name></person-group>. <article-title>MM-BiFPN: Multi-modality fusion network with bi-FPN for MRI brain tumor segmentation</article-title>. <source>IEEE Access</source> (<year>2021</year>) <volume>9</volume>:<page-range>160708–20</page-range>. doi: <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3132050</pub-id>
</mixed-citation>
    </ref>
    <ref id="B13">
      <label>13</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group>. <source>Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition</source>. (<year>2016</year>). pp. <page-range>770–8</page-range>.</mixed-citation>
    </ref>
    <ref id="B14">
      <label>14</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>H</given-names></name><name><surname>Wei</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><etal/></person-group>. <article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title> 2021 IEEE/CVF International Conference on Computer Vision (ICCV). (<year>2021</year>). <page-range>9992–10002</page-range>. doi: <pub-id pub-id-type="doi">10.1109/ICCV48922.2021.00986</pub-id>
</mixed-citation>
    </ref>
    <ref id="B15">
      <label>15</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zha</surname><given-names>H</given-names></name><name><surname>He</surname><given-names>X</given-names></name><name><surname>Ding</surname><given-names>C</given-names></name><name><surname>Simon</surname><given-names>H</given-names></name><name><surname>Gu</surname><given-names>M</given-names></name></person-group>. <article-title>Bipartite graph partitioning and data clustering</article-title>. <source>Proc tenth Int Conf Inf knowledge Manage</source> (<year>2001</year>), <volume>2001</volume>:<page-range>25–32</page-range>. doi: <pub-id pub-id-type="doi">10.2172/816202</pub-id>
</mixed-citation>
    </ref>
    <ref id="B16">
      <label>16</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamashita</surname><given-names>R</given-names></name><name><surname>Nishio</surname><given-names>M</given-names></name><name><surname>Do</surname><given-names>RKG</given-names></name><name><surname>Togashi</surname><given-names>K</given-names></name></person-group>. <article-title>Convolutional neural networks: an overview and application in radiology</article-title>. <source>Insights into Imaging</source> (<year>2018</year>) <volume>9</volume>(<issue>4</issue>):<page-range>611–29</page-range>. doi: <pub-id pub-id-type="doi">10.1007/s13244-018-0639-9</pub-id>
</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><etal/></person-group>. <article-title>A survey on visual transformer</article-title>. (<year>2012</year>) <volume>2020</volume>:<fpage>12556</fpage>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.2012.12556</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname><given-names>DE</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Williams</surname><given-names>RJ</given-names></name></person-group>. <article-title>Learning representations by back-propagating errors[J]</article-title>. <source>nature</source> (<year>1986</year>) <volume>323</volume>(<issue>6088</issue>):<page-range>533–6</page-range>. doi: <pub-id pub-id-type="doi">10.1038/323533a0</pub-id>
</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Physica-Verlag</surname><given-names>HD</given-names></name></person-group>. <article-title>Large-Scale machine learning with stochastic gradient descent</article-title>. <source>Proc COMPSTAT'2010</source> (<year>2010</year>), <volume>2010</volume>: <page-range>177–86</page-range>. doi: <pub-id pub-id-type="doi">10.1007/978-3-7908-2604-3_16</pub-id>
</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chattopadhay</surname><given-names>A</given-names></name><name><surname>Sarkar</surname><given-names>A</given-names></name><name><surname>Howlader</surname><given-names>P</given-names></name><name><surname>Balasubramanian</surname><given-names>VN</given-names></name></person-group>. <article-title>Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks[C]//2018 IEEE winter conference on applications of computer vision (WACV)</article-title>. <source>IEEE</source> (<year>2018</year>), <page-range>839–47</page-range>. doi: <pub-id pub-id-type="doi">10.1109/WACV.2018.00097</pub-id>
</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litjens</surname><given-names>G</given-names></name><name><surname>Kooi</surname><given-names>T</given-names></name><name><surname>Bejnordi</surname><given-names>BE</given-names></name><name><surname>Setio</surname><given-names>AAA</given-names></name><name><surname>Ciompi</surname><given-names>F.</given-names></name><name><surname>Ghafoorian</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>A survey on deep learning in medical image analysis</article-title>. <source>Med image Anal</source> (<year>2017</year>) <volume>42</volume>:<fpage>60</fpage>–<lpage>88</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id>
<pub-id pub-id-type="pmid">28778026</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>Imagenet large scale visual recognition challenge</article-title>. <source>Int J Comput Vision</source> (<year>2015</year>) <volume>115</volume>(<issue>3</issue>):<page-range>211–52</page-range>. doi: <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id>
</mixed-citation>
    </ref>
    <ref id="B23">
      <label>23</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>TY</given-names></name><name><surname>Maire</surname><given-names>M</given-names></name><name><surname>Belongie</surname><given-names>S</given-names></name><name><surname>Hays</surname><given-names>J</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name><name><surname>Ramanan</surname><given-names>D</given-names></name><etal/></person-group>. <article-title>Microsoft Coco: Common objects in context</article-title>. In: <source>European Conference on computer vision</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2014</year>). p. <page-range>740–55</page-range>.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group>. <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. (<year>2014</year>) <volume>1409</volume>:<fpage>1556</fpage>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Jiao</surname><given-names>M</given-names></name><name><surname>Yuan</surname><given-names>H</given-names></name><name><surname>Jiang</surname><given-names>L</given-names></name><name><surname>Lang</surname><given-names>N</given-names></name><name><surname>Ouyang</surname><given-names>H</given-names></name><etal/></person-group>. <article-title>A deep learning method based on multi-model weighted fusion to distinguish benign and malignant spinal tumors with magnetic resonance imaging</article-title>. <source>Radiological Soc North America (RSNA) Chicago America</source> (<year>2021</year>) <volume>11</volume>:<fpage>28</fpage>–<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="B26">
      <label>26</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><etal/></person-group>. <source>Attention is all you need[C]//Advances in neural information processing systems</source>. (<year>2017</year>). pp. <fpage>5998</fpage>–<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="B27">
      <label>27</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>SC</given-names></name><name><surname>Pareek</surname><given-names>A</given-names></name><name><surname>Seyyedi</surname><given-names>S</given-names></name><name><surname>Banerjee</surname><given-names>I</given-names></name><name><surname>Lungren</surname><given-names>MP</given-names></name></person-group>. <article-title>Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines</article-title>. <source>NPJ digital Med</source> (<year>2020</year>) <volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41746-020-00341-z</pub-id>
</mixed-citation>
    </ref>
    <ref id="B28">
      <label>28</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>HY</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name></person-group>. <source>Automatic brain tumor segmentation with contour aware residual network and adversarial training[C]//International MICCAI brainlesion workshop</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2018</year>) p. <page-range>267–78</page-range>.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolz</surname><given-names>J</given-names></name><name><surname>Gopinath</surname><given-names>K</given-names></name><name><surname>Yuan</surname><given-names>J</given-names></name><name><surname>Lombaert</surname><given-names>H</given-names></name><name><surname>Desrosiers</surname><given-names>C</given-names></name><name><surname>Ayed</surname><given-names>IB</given-names></name><etal/></person-group>. <article-title>HyperDense-net: A hyper-densely connected CNN for multi-modal image segmentation</article-title>. <source>IEEE Trans Med Imaging</source> (<year>2018</year>) <volume>38</volume>(<issue>5</issue>):<page-range>1116–26</page-range>. doi: <pub-id pub-id-type="doi">10.1109/TMI.2018.2878669</pub-id>
</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iandola</surname><given-names>F</given-names></name><name><surname>Moskewicz</surname><given-names>M</given-names></name><name><surname>Karayev</surname><given-names>S</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Keutzer</surname><given-names>K</given-names></name><etal/></person-group>. <article-title>Densenet: Implementing efficient convnet descriptor pyramids</article-title>. (<year>2014</year>) <volume>1404</volume>:<elocation-id>1869</elocation-id>. doi: <pub-id pub-id-type="doi">10.48550/arXiv.1404.1869</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <label>31</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>DSouza</surname><given-names>AM</given-names></name><name><surname>Abidin</surname><given-names>AZ</given-names></name><name><surname>Wismüller</surname><given-names>A</given-names></name><name><surname>Xu</surname><given-names>C</given-names></name><etal/></person-group>. <article-title>MRI Tumor segmentation with densely connected 3D CNN[C]//Medical imaging 2018: Image processing</article-title>. <source>Int Soc Optics Photonics</source> (<year>2018</year>) <volume>10574</volume>:<page-range>105741F</page-range>. doi: <pub-id pub-id-type="doi">10.1117/12.2293394</pub-id>
</mixed-citation>
    </ref>
    <ref id="B32">
      <label>32</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kamnitsas</surname><given-names>K</given-names></name><name><surname>Bai</surname><given-names>W</given-names></name><name><surname>Ferrante</surname><given-names>E</given-names></name><name><surname>McDonagh</surname><given-names>S</given-names></name><name><surname>Sinclair</surname><given-names>M</given-names></name><name><surname>Pawlowski</surname><given-names>N</given-names></name><etal/></person-group>. <article-title>Ensembles of multiple models and architectures for robust brain tumour segmentation</article-title>. In: <source>International MICCAI brainlesion workshop</source>. <publisher-name>Springer, Cham</publisher-name> (<year>2017</year>). p. <page-range>450–62</page-range>.</mixed-citation>
    </ref>
    <ref id="B33">
      <label>33</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>Y</given-names></name><name><surname>Tang</surname><given-names>LY</given-names></name><name><surname>Li</surname><given-names>DK</given-names></name><name><surname>Metz</surname><given-names>L</given-names></name><name><surname>Kolind</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>Deep learning of brain lesion patterns and user-defined clinical and MRI features for predicting conversion to multiple sclerosis from clinically isolated syndrome</article-title>. <source>Comput Methods Biomechanics Biomed Engineering: Imaging Visualization</source> (<year>2019</year>) <volume>7</volume>(<issue>3</issue>):<page-range>250–9</page-range>. doi: <pub-id pub-id-type="doi">10.1080/21681163.2017.1356750</pub-id>
</mixed-citation>
    </ref>
    <ref id="B34">
      <label>34</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>S</given-names></name><name><surname>Chang</surname><given-names>GH</given-names></name><name><surname>Panagia</surname><given-names>M</given-names></name><name><surname>Gopal</surname><given-names>DM</given-names></name><name><surname>Au</surname><given-names>R</given-names></name><name><surname>Kolachalama</surname><given-names>VB</given-names></name></person-group>. <article-title>Fusion of deep learning models of MRI scans, mini–mental state examination, and logical memory test enhances diagnosis of mild cognitive impairment</article-title>. <source>Alzheimer's Dementia: Diagnosis Assess Dis Monit</source> (<year>2018</year>) <volume>10</volume>:<page-range>737–49</page-range>. doi: <pub-id pub-id-type="doi">10.1016/j.dadm.2018.08.013</pub-id>
</mixed-citation>
    </ref>
    <ref id="B35">
      <label>35</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reda</surname><given-names>I</given-names></name><name><surname>Khalil</surname><given-names>A</given-names></name><name><surname>Elmogy</surname><given-names>M</given-names></name><name><surname>Abou El-Fetouh</surname><given-names>A</given-names></name><name><surname>Shalaby</surname><given-names>A</given-names></name><name><surname>Abou El-Ghar</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>Deep learning role in early diagnosis of prostate cancer</article-title>. <source>Technol Cancer Res Treat</source> (<year>2018</year>) <volume>17</volume>:<fpage>1533034618775530</fpage>. doi: <pub-id pub-id-type="doi">10.1177/1533034618775530</pub-id>
<pub-id pub-id-type="pmid">29804518</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
