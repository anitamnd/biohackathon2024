<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10844231</article-id>
    <article-id pub-id-type="pmid">38316843</article-id>
    <article-id pub-id-type="publisher-id">52653</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-024-52653-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep-WET: a deep learning-based approach for predicting DNA-binding proteins using word embedding techniques with weighted features</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Mahmud</surname>
          <given-names>S. M. Hasan</given-names>
        </name>
        <address>
          <email>hasan.swe@aiub.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Goh</surname>
          <given-names>Kah Ong Michael</given-names>
        </name>
        <address>
          <email>michael.goh@mmu.edu.my</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hosen</surname>
          <given-names>Md. Faruk</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nandi</surname>
          <given-names>Dip</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shoombuatong</surname>
          <given-names>Watshara</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02j8ga255</institution-id><institution-id institution-id-type="GRID">grid.442972.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2218 5390</institution-id><institution>Department of Computer Science, </institution><institution>American International University-Bangladesh (AIUB), </institution></institution-wrap>Kuratoli, Dhaka, 1229 Bangladesh </aff>
      <aff id="Aff2"><label>2</label>Centre for Advanced Machine Learning and Applications (CAMLAs), Dhaka, 1229 Bangladesh </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04zrbnc33</institution-id><institution-id institution-id-type="GRID">grid.411865.f</institution-id><institution-id institution-id-type="ISNI">0000 0000 8610 6308</institution-id><institution>Faculty of Information Science &amp; Technology (FIST), </institution><institution>Multimedia University, Jalan Ayer Keroh Lama, </institution></institution-wrap>75450 Melaka, Malaysia </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00gvj4587</institution-id><institution-id institution-id-type="GRID">grid.443019.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 0479 1356</institution-id><institution>Department of Information and Communication Technology, </institution><institution>Mawlana Bhashani Science and Technology University, </institution></institution-wrap>Santosh, Tangail, 1902 Bangladesh </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01znkr924</institution-id><institution-id institution-id-type="GRID">grid.10223.32</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0490</institution-id><institution>Center for Research Innovation and Biomedical Informatics, Faculty of Medical Technology, </institution><institution>Mahidol University, </institution></institution-wrap>Bangkok, 10700 Thailand </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>2961</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>1</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">DNA-binding proteins (DBPs) play a significant role in all phases of genetic processes, including DNA recombination, repair, and modification. They are often utilized in drug discovery as fundamental elements of steroids, antibiotics, and anticancer drugs. Predicting them poses the most challenging task in proteomics research. Conventional experimental methods for DBP identification are costly and sometimes biased toward prediction. Therefore, developing powerful computational methods that can accurately and rapidly identify DBPs from sequence information is an urgent need. In this study, we propose a novel deep learning-based method called Deep-WET to accurately identify DBPs from primary sequence information. In Deep-WET, we employed three powerful feature encoding schemes containing Global Vectors, Word2Vec, and fastText to encode the protein sequence. Subsequently, these three features were sequentially combined and weighted using the weights obtained from the elements learned through the differential evolution (DE) algorithm. To enhance the predictive performance of Deep-WET, we applied the SHapley Additive exPlanations approach to remove irrelevant features. Finally, the optimal feature subset was input into convolutional neural networks to construct the Deep-WET predictor. Both cross-validation and independent tests indicated that Deep-WET achieved superior predictive performance compared to conventional machine learning classifiers. In addition, in extensive independent test, Deep-WET was effective and outperformed than several state-of-the-art methods for DBP prediction, with accuracy of 78.08%, MCC of 0.559, and AUC of 0.805. This superior performance shows that Deep-WET has a tremendous predictive capacity to predict DBPs. The web server of Deep-WET and curated datasets in this study are available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/">https://deepwet-dna.monarcatechnical.com/</ext-link>. The proposed Deep-WET is anticipated to serve the community-wide effort for large-scale identification of potential DBPs.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Computational models</kwd>
      <kwd>Biological techniques</kwd>
      <kwd>Computational biology and bioinformatics</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100012024</institution-id>
            <institution>Multimedia University</institution>
          </institution-wrap>
        </funding-source>
        <award-id>IR Fund (Project ID MMUI/220041)</award-id>
        <principal-award-recipient>
          <name>
            <surname>Mahmud</surname>
            <given-names>S. M. Hasan</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Nature Limited 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">DNA-binding Proteins (DBPs) participate in many essential biological processes, including DNA replication, gene regulation, repair, and modification<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Identification of DBPs is fundamentally important for understanding characterizations of protein function and drug design. A number of large-scale proteomics experiments have been performed to identify DBPs based on biochemical methods, such as X-ray crystallography<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> and fast ChIP<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. Despite the increasing number of experimentally determined DBPs, the underlying mechanism of DBP specificity remains mostly unidentified, and these approaches are laborious, time-consuming, and sometimes biased toward prediction in the post-genome era, when large numbers of unannotated DBPs are rapidly being sequenced and deposited. As an alternative, computational methods are accurate and cost-effective and can be used to complement the experimental efforts.</p>
    <p id="Par3">To date, several computational algorithms, including machine-learning (ML)-based and template-based methods, have been developed for in silico prediction of DBPs<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>. DBPs can be predicted based on two types of protein data input: i sequence-driven (e.g., iDNA-Prot<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, PseDNA-Pro<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, iDNAPro-PseAAC<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, iDNA-Prot|dis<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, Local-DPP<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, PSFM-DBT<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, IKP-DBPPred<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, iDNAProt-ES<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, DPPPseAAC<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>) and 3D-structure-driven (e.g., DBD-Hunter<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, iDBPs<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, and SPOT-Seq (DNA)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>) methods. Only protein sequence data is required for sequence-driven techniques. The 3D-structure-driven techniques require native or projected 3D structure data from the query protein. In this case, 3D-structure-driven techniques cannot function correctly without 3D structure information. This method performs better when the protein’s native structure is known. On the other hand, sequence-driven techniques do not have this problem. Furthermore, due to the inherent challenges of measuring protein 3D structures in experimental studies, there is a significant gap between the quantities of sequences and 3D structures<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, which is currently expanding quickly in the postgenomic era. Recently, PSI-BLAST was utilized by Chowdhury et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> to derive polysaccharide storage myopathy, which revealed evolutionary information to predict DBP. The secondary structure information of the protein sequences was extracted using SPIDER2. To retrieve protein sequence information, Nanni et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> utilized AAC and quasi residue couple (QRC). Meanwhile, the autocovariance method was used to derive physicochemical characteristics. In addition, evolutionary data was retrieved using the pseudo-position specific scoring matrix (PsePSSM), N-gram features (NGR), and texture descriptors (TD). Sang et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> calculated the HMM matrix for each sequence using the hidden Markov model (HMM). The HMM matrix was converted into feature vectors of the same length using AAC, autocovariance transformation (ACT), and cross-covariance transformation (CCT). Thus, designing sequence-driven computational strategies is essential to accurate prediction of DBPs.</p>
    <p id="Par4">Choosing appropriate feature extraction methods and classification algorithms in order to select the best subset of features is a key factor for the successful discovery of DBPs. In TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, four single-view features (AAC, PsePSSM, PsePRSA, and PsePPDBS) are used to extract the DNA-binding features and apply a learning-based technique to the weights of features to combine them for training an SVM classifier. In addition, an excellent feature subset was selected using SVM-REF + CBR from the non-redundant benchmark and new gold-standard dataset. Rahman et al.<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> utilized the same feature selection (REF) and classifier (SVM) to develop a model DPP-PseAAC for which the authors focused on Chou’s general PseAAC for generating features. Another proposed DBP predictor method is DNAPred<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, where authors use the E-HDSVM algorithm, which includes HD-US and EAdaBoost, to predict protein DNA binding sites. A similar ensemble-based method performed by Zhang et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, XGB-RFE, is used to attain effective features, after which the best features are fed to the stacked ensemble classifier (the combined form of LightGBM, XGBoost, and SVM) to build the proposed StackPDB model.</p>
    <p id="Par5">The above-mentioned algorithms have proven to be exemplary, but we opted to use convolutional neural networks (CNNs) to improve prediction performance. In the meanwhile, it is important to devise an appropriate encoding scheme to represent the sequence fragments surrounding DBPs/non-DBPs to develop a ML-based predictor. In this study, we present a new convolutional neural network (CNN)-based predictor called Deep-WET for accurately identifying DBPs from primary sequence information. Firstly, we applied three consecutive sequence encoding approaches, namely Global Vectors (GloVe), Word2Vec, and fastText, to extract the protein sequence patterns. Secondly, the DE is utilized to acquire the weights for three base features. With these obtained weights, we combined three base features in a weighted manner to create the super feature. In order to improve the predictive performance of Deep-WET, we employed SHapley Additive exPlanations (SHAP) approach to remove irrelevant features from super features and then inputted the optimal one into CNN algorithm for the final model construction. Experimental results demonstrated that Deep-WET achieved a accurate and robust performance as compared with conventional ML classifiers on both the training and independent test datasets. Moreover, comparative analysis on the independent test dataset showed that Deep-WET achieved improved performance compared with the existing approaches, highlight the effectiveness and robustness of the proposed Deep-WET. We also conducted a series of computational analyses to provide in-depth understanding of the DBPs. Finally, the proposed method, Deep-WET, was implemented as a user-friendly web server: at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/">https://deepwet-dna.monarcatechnical.com/</ext-link>.</p>
  </sec>
  <sec id="Sec2">
    <title>Materials and methods</title>
    <sec id="Sec3">
      <title>The overall framework of Deep-WET</title>
      <p id="Par6">The construction process of Deep-WET is depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Deep-WET consists of multiple steps, including data preparation, natural language processing (NLP)-based feature encoding, weighted features, optimal feature subset selection, best classifier selection, and final prediction. In the first stage, three NLP-based Word embedding feature encoding techniques were employed (GloVe, Word2Vec, and fastText), and then the optimal subset of features was selected using the SHAP technique from the weighted features. The selected feature subsets from each feature encoding were fed to four ML and one DL algorithms to build the final prediction models using the training and independent test datasets. Finally, the classifier having the highest cross-validation AUC was considered to construct the final predictor herein.<fig id="Fig1"><label>Figure 1</label><caption><p>The flowchart illustrates our proposed methodology. The upper part represents data pre-processing, the middle part depicts feature extraction with various classifiers, and the lower part showcases classification using the CNN model.</p></caption><graphic xlink:href="41598_2024_52653_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Data preparation</title>
      <p id="Par7">Developing a reliable, comprehensive, and stringent dataset is the first important step of statistical predictor development. Here, the curated dataset denoted with S was presented as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} S = S_{posi} \cup S_{nega} \end{aligned}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where, <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq1.gif"/></alternatives></inline-formula> denotes the positive subset containing DBPs or positive samples, while, <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq2.gif"/></alternatives></inline-formula> denotes the negative subset containing non-DBPs or negative samples, and <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cup$$\end{document}</tex-math><mml:math id="M8"><mml:mo>∪</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq3.gif"/></alternatives></inline-formula> symbol resembles the union of the following sets. The <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq5.gif"/></alternatives></inline-formula> datasets were collected and primarily used by Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, who collected both DBP and non-DBP chains from PDB Data Bank. There are two main reasons why we used the dataset established by Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> as follows. Firstly, this dataset applied a lower CD-HIT<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> threshold of 0.25 to exclude the redundant protein chains. Secondly, this dataset exclude the protein chain sequences having below 50 residues and unknown residues. For the the <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq7.gif"/></alternatives></inline-formula> datasets, they were randomly selected to create the training and independent test datasets. The training dataset consists of 1052 DBPs and 1052 non-DBPs, while The independent test dataset consists of 148 DBPs and 148 non-DBPs. More details on the training and independent test datasets are provided in an article of Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.</p>
    </sec>
    <sec id="Sec5">
      <title>Feature encodings</title>
      <p id="Par8">Word embedding (WE), in which the vocabulary of words can be represented as vectors using large text as an input, is the most popular technique in the area of natural language processing (NLP). WE techniques are able to convert amino acids in a fixed-length vector, where a user needs to define the fixed feature dimensions that can provide adequate prediction results. In this study, we implemented three unsupervised embedding techniques to encode protein sequences: GloVe<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, Word2Vec<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, and fastText<sup><xref ref-type="bibr" rid="CR37">37</xref>–<xref ref-type="bibr" rid="CR39">39</xref></sup>.</p>
      <sec id="Sec6">
        <title>Word2Vec</title>
        <p id="Par9">Word2Vec, a model developed by Tomas Mikolav at Google, computes and generates high-quality, distributed, and continuous dense representations of words<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. These are unsupervised models that can take in massive textual corpora, create a vocabulary of possible word combinations, and generate dense word embeddings on the vector space. The size of the vocabulary determines the size of the word embedding vectors. This decreases the dimensionality of the following dense vector, compared to high-dimensional sparse vector generation using the traditional bag of words (BOW). To construct word embedding, Word2Vec employs two different methods: (1) common bag of words (CBOW) and (2) the Skip-gram model. Notably, the CBOW is faster than the Skip-gram model and generates a better representation of more frequent words<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. On the other hand, the Skip-gram model performs well with a relatively small amount of data and generates a better representation of rare words<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p>
        <p id="Par10">Finding the target word <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_t$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq8.gif"/></alternatives></inline-formula> through n predictions using the CBOW model can be accomplished by the following equation:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} J_{\phi } = \frac{1}{T}\sum _{t=1}^{T}logP(w_t \big | w_{(t-n)}, \ldots ,w_{(t-1)},w_{(t+1)}, \ldots ,w_{(t+n)}) \end{aligned}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par11">Here, <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{(t-1)}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq9.gif"/></alternatives></inline-formula> to <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{(t+n)}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq10.gif"/></alternatives></inline-formula> sequence of words represents the context words. The following equation can further simplify the above equation since the hidden layer can be equivalent to a softmax layer:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} P(w_t \big | w_{(t-n)}, \ldots ,w_{(t-1)},w_{(t+1)}, \ldots ,w_{(t+n)}) = \frac{exp(W_k^Th_t)}{\sum _{k=1}^{v} exp(W_k^Th_t)} \end{aligned}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par12">Here, the output weight matrix between hidden layers is denoted as <italic>W</italic>, and after matrix operation, the average value of input vectors is represented as <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_t$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq11.gif"/></alternatives></inline-formula>.</p>
      </sec>
      <sec id="Sec7">
        <title>GloVe</title>
        <p id="Par13">GloVe is an unsupervised learning vectorization technique. It is a log-bilinear regression model that incorporates both local statistics and global statistics<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. The training of this model is performed on non-zero entries of global word-to-word co-occurrence statistics that tabulates how frequently words are co-occurring within a given corpus. For collecting statistics, the following matrix needs a single pass through the entire corpus. These passes can be expensive for large corpora. Moreover, its resulting representations show the interesting linear substructures of those word vector spaces.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \sum _{i,j}^{N}f(X_{ij})(v_i^Tv_j + b_i + b_j - log(X_{ij}))^2 \end{aligned}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par14">Here, <inline-formula id="IEq12"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_i, v_j$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq12.gif"/></alternatives></inline-formula> correspond to the word embedding of <italic>i</italic>, <italic>j</italic>; <italic>X</italic> represents the word-to-word co-occurrence matrix; and <inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i^{th}$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq13.gif"/></alternatives></inline-formula> number of co-occurrences of word <italic>j</italic> is denoted by <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{ij}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq14.gif"/></alternatives></inline-formula>. Furthermore, the probability of word <italic>j</italic> occurring in the context <italic>i</italic> is the following:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} X_{ij} = P(i) = \frac{X_{ij}}{X_i} \end{aligned}$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec8">
        <title>fastText</title>
        <p id="Par15">fastText, proposed by Facebook<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, is an extension of Word2Vec. It provides tools to learn word representation and sentence classifications of ML. Word vectors are a more organized, numerical, and efficient representation of words and sentences. fastText provides a supervised module to build a model for text classifications. It technique breaks an individual word into a bag of n-grams or sub-words and feeds them into the network, which also generates vector representation for rare or unseen words<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Since the technique uses the same architecture as Word2Vec, the following equation minimizes the loss of softmax layer, <italic>l</italic> over <italic>N</italic> sequences using CBOW model:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \sum _{n=1}^{n} l(y_n, BAx_n) \end{aligned}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par16">Here, <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_n$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq15.gif"/></alternatives></inline-formula> represents the bag of one-hot encoded vectors and <inline-formula id="IEq16"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_n$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq16.gif"/></alternatives></inline-formula> represents the label of the nth sequence of words. The purpose of using FastText in the present study is to find the partial information single DNA sequence order.</p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Weight learning for weighted features</title>
      <p id="Par17">Single-view features represent the discriminative information for each sequence, but combing single-view features to make a weighted feature is critical in ML-based DBP prediction. The most common technique involves serially adding (’+’) single features. However, this straightforward combination technique lacks a guarantee to represent discriminative capability and may overlook the relative importance of the base sequence. To address this issue, we employ a differential evolution (DE) method to determine the optimal weights for each feature. DE algorithm variants of evolutionary algorithms and applied in various works<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup> to show the positive effect. The process we followed for DE algorithm to learn feature weights from a single feature is illustrated as follows: <def-list><def-item><term><bold>Step 1:</bold></term><def><p id="Par18"><bold>Initialization</bold> Randomly create an initial population <inline-formula id="IEq17"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_o = \{FW_1^g, FW_2^g, FW_3^g \ldots \, \ldots \,FW_n^g\}, where (FW_{i,1}^g, FW_{i,2}^g, FW_{i,3}^g, FW_{i,4}^g)^T$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>3</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq17.gif"/></alternatives></inline-formula> represents <italic>i</italic>th number solution in the population <italic>g</italic>th. <italic>N</italic> means size of the generation population where to set the maximum generation <inline-formula id="IEq20"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_{max}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq20.gif"/></alternatives></inline-formula>, crossover rate (<italic>CR</italic>), scaling factor (<italic>F</italic>) to 1000, 0.5, and 0.5, respectively.</p></def></def-item><def-item><term><bold>Step 2:</bold></term><def><p id="Par19"><bold>Mutation</bold> A mutation vector <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MV_i^g$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi>M</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq21.gif"/></alternatives></inline-formula> was initialized for each salutation according. <disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MV_i^g = FW_{r1}^g + F.(FW_{r2}^g - FW_{r3}^g) \end{aligned}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p></def></def-item><def-item><term><bold>Step 3:</bold></term><def><p id="Par20"><bold>Crossover</bold> For the diversity of each solution, a trial vector <inline-formula id="IEq22"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TV_i^g = (TV_{i,1}^g, TV_{i,2}^g, TV_{i,3}^g \ldots \, \ldots \,TV_{i,D}^g)$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq22.gif"/></alternatives></inline-formula> of crossover is established in the DE technique as follows: <disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} TV_{i, j}^g = {\left\{ \begin{array}{ll} &amp;{} \text {TV}_{i,j}^g \quad if \; R_j \le CR \;\;ot \;j = j_r\\ &amp;{} \,\,\text {FW}_{i,j}^g \qquad \,\,\text {otherwise}\\ \end{array}\right. } \end{aligned}$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msubsup><mml:mtext>TV</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.277778em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.277778em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msubsup><mml:mtext>FW</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mspace width="2em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula> where <inline-formula id="IEq23"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j = 1,2,3 \ldots ,D, j_r$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq23.gif"/></alternatives></inline-formula> represent randomly produced integer with [1, <italic>D</italic>]; <inline-formula id="IEq24"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_j$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq24.gif"/></alternatives></inline-formula> means uniformly distributed range [0,1] and <inline-formula id="IEq25"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CR \in (0,1)$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq25.gif"/></alternatives></inline-formula> indicate crossover rate.</p></def></def-item><def-item><term><bold>Step 4:</bold></term><def><p id="Par21"><bold>Selection</bold> Find the better vector from trial <inline-formula id="IEq26"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TV_i^g$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq26.gif"/></alternatives></inline-formula> and target <inline-formula id="IEq27"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FW_i^g$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq27.gif"/></alternatives></inline-formula> using the following way: <disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} FW_i^{g+1} = {\left\{ \begin{array}{ll} &amp;{} TV_{i}^g, \quad \text {if} \; f(TV_i^{g}) \le f(FW_i^g)\\ &amp;{} \,\,\text {FW}_{i}^g, \qquad \,\,\text {otherwise}\\ \end{array}\right. } \end{aligned}$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.277778em"/><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msubsup><mml:mtext>FW</mml:mtext><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p></def></def-item><def-item><term><bold>Step 5:</bold></term><def><p id="Par22"><bold>Termination</bold><inline-formula id="IEq28"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g=g+1$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq28.gif"/></alternatives></inline-formula> and repeat steps 2 to 4 until <italic>g</italic> is greater the <inline-formula id="IEq29"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_{max}.$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq29.gif"/></alternatives></inline-formula></p></def></def-item></def-list></p>
      <p id="Par23">After concluding the DE procedure, we can get the final results. In this study, We have generated a novel super feature, represented as GloVe + fastText + Word2Vec, by the weighted and sequential fusion of GloVe, fastText, and Word2Vec features. DE is a powerful optimization algorithm; however, using it for feature weighting in ML presents certain limitations and challenges. DE may struggle with slow convergence, susceptibility to local optima, and sensitivity to parameter choices. Additionally, the algorithm may violate constraints, lack robustness across diverse datasets, and exhibit computational intensity. To avoid these challenges, we have performed parameter tuning <inline-formula id="IEq30"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(population-size, \, mutation \, rate, \, crossover \, probabilities)$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.166667em"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq30.gif"/></alternatives></inline-formula> in experiments, considering adaptive strategies for mutation and crossover rates. Furthermore, exploring parallelization methods helps alleviate computational burdens, while strategies like diversity maintenance mechanisms aim to address convergence issues.</p>
    </sec>
    <sec id="Sec10">
      <title>SHAP-based feature selection scheme</title>
      <p id="Par24">SHAP is an additive feature attribution method introduced by Lunberg and Lee<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> in which each individual prediction is interpreted by the contribution of the features and then ordered according to their importance<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. SHAP allocates each feature an importance value for a particular prediction. This SHAP feature selection approach is based on game theory<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>; SHAP values break down a prediction to show the impact of each individual feature. Suppose each feature is <inline-formula id="IEq31"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M76"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq31.gif"/></alternatives></inline-formula>, is replaced by <inline-formula id="IEq32"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_i$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq32.gif"/></alternatives></inline-formula> for determining whether the feature value <inline-formula id="IEq33"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq33.gif"/></alternatives></inline-formula> exists or not. SHAP represents the explanation as:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} g(z) = \phi _o + \sum _{i=1}^{M} \phi _iz_i \end{aligned}$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par25">In the above equation, <italic>g</italic> represents the explanation model; <inline-formula id="IEq34"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z \in {0,1} ^M$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq34.gif"/></alternatives></inline-formula> represents the coalition vector; 0 and 1 indicate that the corresponding feature is absent or present, respectively; the number of input features included in the model is denoted as <italic>M</italic>; and <inline-formula id="IEq35"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi _i \in R, \phi _i$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq35.gif"/></alternatives></inline-formula> represents the feature attribution values for a feature <italic>i</italic>. Considering the game theory concept, Shapley values can be calculated using the following equation:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \phi _i = \sum _{S \subset M \setminus \{i\}} \frac{|S|!(M - |S| -1)!}{M!} [f_x(S \cup \{i\} - f_x(S))] \end{aligned}$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊂</mml:mo><mml:mi>M</mml:mi><mml:mo lspace="0.15em" rspace="0.15em" stretchy="false">\</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par26">In the above equation, <italic>M</italic> represents the set of features in the model; all feature subsets achieved from <italic>M</italic> are represented as <italic>S</italic>; the function computes the total contribution of a given features set <italic>S</italic>; <inline-formula id="IEq36"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S \subset M \setminus \{i\}$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊂</mml:mo><mml:mi>M</mml:mi><mml:mo lspace="0.15em" rspace="0.15em" stretchy="false">\</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq36.gif"/></alternatives></inline-formula> represents the value of the corresponding feature when <italic>i</italic> is known, versus when the corresponding feature value <italic>i</italic> is unknown for all subsets.</p>
      <p id="Par27">One of the important features of the SHAP is the barplot in the form of rectangular horizontal bars, where the length of the bars represents the importance of a given feature. As we need the global significance, we sum the contribution of each feature, or absolute Shapley values.<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I_j = \sum _{i=1}^{n} |\phi _j^{(i)}| \end{aligned}$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par28">Then, we plot each of the features by sorting them in decreasing order. Figure 2A shows the important features based on SHAP contributions for the XGBoost trained before predicting DBPs. The SHAP summary plot gives a high-level composite view that displays the importance of features with feature effects. Each point in the plot represents a SHAP value for a specific feature of an instance. The values that pull the prediction power of the model downwards are on the left, and the values that push the prediction further up are on the right. On the y-axis, the features are placed in descending order, and on the x-axis, there is a scale representing the Shapley value with a vertical line at point zero. The positive and negative values are to the right and left part of that vertical line, respectively. Here the colors separate the relative size of the features between instances. Specifically, low values are colored blue and high values are colored red. Overlapping more data points in the y-axis direction shows the distribution of SHAP values for each individual feature. Moreover, in the summary plot, we clearly observe the relationships between the value of a feature and the effect on the prediction. Figure 2B shows the SHAP summary plot, which orders important features for identifying DBPs.</p>
    </sec>
    <sec id="Sec11">
      <title>Implementation of convolutional neural network</title>
      <p id="Par29">CNNs are a type of deep learning model commonly used in applications including recommender systems, image and video recognition, and natural language processing<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. In CNN architecture, the deeper convolutional layers (CLs) lead to learning high dimension features using sliding convolution kernels on the upper part of previous layers with different hyper-parameter settings such as filters, control layer outputs, stride, and zero-paddings. Pooling layers (PLs) are able to reduce the input feature size and offer translation invariance by local non-linear operations<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Fully connected layers (FCLs) utilized to classify the tasks, consisting of an equal number of output neurons as artificial neural networks.</p>
      <p id="Par30">Each neuron is completely linked to all of the nodes in the preceding and subsequent levels<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. After adding one additional CL and max-PL to the process, the technique demonstrated a significant improvement in terms of computational complexity and program runtime. The following equation may be used to compute the outputs of each convolutional layer:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} y_k^l = f\big ( \sum _{m} W_{m,k}^l y_m^{l-1} + b_k^l \big ) \end{aligned}$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">(</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par31">The layer index is <italic>l</italic>, while the input and output feature maps are <italic>m</italic> and <italic>k</italic>, respectively. Specifically, <inline-formula id="IEq37"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_k^l$$\end{document}</tex-math><mml:math id="M96"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq37.gif"/></alternatives></inline-formula> denotes the <italic>k</italic>th feature map of the <italic>l</italic> layer’s input, while <inline-formula id="IEq39"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_m^{(l-1)}$$\end{document}</tex-math><mml:math id="M98"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq39.gif"/></alternatives></inline-formula> denotes the <inline-formula id="IEq40"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m-$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq40.gif"/></alternatives></inline-formula>th feature map of layer <inline-formula id="IEq41"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l-1$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq41.gif"/></alternatives></inline-formula> output. The weight tensor and bias term, respectively, are <italic>W</italic> and <italic>b</italic>. Back-propagation and adaptive estimating approaches were used to reduce cross-entropy loss<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. Our model’s output layer is essentially a logistic regression classifier. It takes <inline-formula id="IEq42"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_k^l$$\end{document}</tex-math><mml:math id="M104"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq42.gif"/></alternatives></inline-formula> as an input and computes the following:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{y} = f(W^ly^l + b^l) \end{aligned}$$\end{document}</tex-math><mml:math id="M106" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par32">The output <inline-formula id="IEq43"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><mml:math id="M108"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq43.gif"/></alternatives></inline-formula>is the final predicted score; <italic>W</italic> is the weight matrix; <italic>b</italic> is the bias vector. Each output size is 2, denoting positive or negative classes for the binary classification task of DNA binding predictions. In order to discover suitable parameters, we want to minimize cross-entropy loss by adaptive moment estimation and back-propagation techniques:<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} loss = - \frac{1}{N} \sum _{i=1}^{N} y_ilog\hat{y_l} + (1-y_i)log(1-\hat{y_l}) \end{aligned}$$\end{document}</tex-math><mml:math id="M110" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par33">To improve the model’s efficiency, batch normalization and dropout techniques<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> were employed. The dropout in FCLs decreases by a few units during the training phase, whereas batch normalization helps to standardize the inputs into unit standard deviation and zero means. Furthermore, dropout was able to overcome the problem of overfitting, and batch normalization supported the model with sufficient learning ratios.</p>
      <p id="Par34">To achieve a better performance, hyperparameter optimization plays a vital part in the implementation of the proposed methodology. The following hyperparameters are optimized before training the model: learning rate, number of filters, kernel size, batch size, number of hidden layers, optimizers, dropout layers, and activation function. Here, three convolutional layers are used as hidden layers in the CNN model architecture. In addition, 32, 48, 64 filters and kernel sizes of 3, 4, 5 are used. Using ReLu as an activation in the hidden layers and Sigmoid in the fully connected layer results in the desired outcome. Dropout layers with dropout rates of 0.2, 0.3, and 0.5 are used to prevent overfitting. With extensive experimentation, employment of the Adam optimizer with a learning rate of 0.00001 and binary cross-entropy loss function shows the optimal result. Table <xref rid="Tab1" ref-type="table">1</xref> comprehensively illustrates the hyperparameters used in our method. Detailed parameter settings of the other three classifiers for different feature encoding are also listed in Table 6.</p>
    </sec>
    <sec id="Sec12">
      <title>Performance evaluation</title>
      <p id="Par35">The performance of Deep-WET was evaluated in terms of six standard performance metrics for the binary classification problem including accuracy (ACC), sensitivity (Sen),specificity (Spe), Matthew’s coefficient correlation (MCC), and precision (Pre).<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} ACC= &amp; {} \frac{TP + TN}{TN+TP+FN+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M112" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Sen= &amp; {} \frac{TP}{TP + FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M114" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Spe= &amp; {} \frac{TN}{FP+TN} \end{aligned}$$\end{document}</tex-math><mml:math id="M116" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MCC= &amp; {} \frac{(TP\times TN) - (FP\times FN)}{\sqrt{(TP+FP)\times (TP+FN)\times (TN+FP)\times (TN+FN)}} \end{aligned}$$\end{document}</tex-math><mml:math id="M118" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Pre= &amp; {} \frac{TP}{TP+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M120" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1\, score= &amp; {} \frac{2\times (Precision\times Recall)}{Precision + Recall} \end{aligned}$$\end{document}</tex-math><mml:math id="M122" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mspace width="0.166667em"/><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula>where <italic>TP</italic>, <italic>FP</italic>, <italic>TN</italic>,  and <italic>FN</italic> respectively represent the number of true positives (correctly classified positive), false positives (incorrectly classified as positive), true negatives (correctly classified negative), and false negatives (incorrectly classified as negative), respectively. Furthermore, the AUC metric was also used to evaluate the performances of the proposed DeepWET model, where the curve is plotted by TPR (sensitivity) and FPR (1 – specificity) with different threshold settings.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Hyperparameters setting of CNN classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Hyperparameters</th><th align="left">Range</th></tr></thead><tbody><tr><td align="left">Learning rate</td><td align="left">[0.00001, 0.01, 0.001, 0.0001]</td></tr><tr><td align="left">Number of filters</td><td align="left">[32, 48, 64]</td></tr><tr><td align="left">Kernel size</td><td align="left">[3, 4, 5]</td></tr><tr><td align="left">Batch size</td><td align="left">[16, 32, 64, 128]</td></tr><tr><td align="left">Number of hidden layers</td><td align="left">[2, 3]</td></tr><tr><td align="left">Optimizer</td><td align="left">[‘Adam’]</td></tr><tr><td align="left">Dropout rate</td><td align="left">[0.2, 0.3, 0.5]</td></tr><tr><td align="left">Activation function</td><td align="left">[’relu’, ’sigmoid’]</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec13">
      <title>Experimental setup and packages</title>
      <p id="Par36">All tests in this study were carried out on three independent computers with the following settings, using Python version 3.7.7 or above:<list list-type="bullet"><list-item><p id="Par37">A desktop computer with Intel Core i5 CPU @ 2.71GHz x 4, Windows 10, 64-bit OS and 8 GB RAM.</p></list-item><list-item><p id="Par38">A desktop computer with Intel Core i5 CPU @ 2.11GHz x 4, Windows 10, 64-bit OS and 8 GB RAM.</p></list-item><list-item><p id="Par39">A server machine with Intel Core i5-3320M CPU @ 2.60GHz x 4, Ubuntu 18.04.2 LTS, 64-bit OS, 13 MB L3 cache and 64 GB RAM.</p></list-item></list></p>
      <p id="Par40">CNN classifier and SHAP technique were employed for model learning and feature selection on TensorFlow 2.0 and SHAP 0.39.0 Python libraries to implement them. We utilized improved parameter settings of the CNN algorithm such as batch size 16, kernel size 4, 2 hidden layers, and dropout rate 0.5. Several graphs were plotted in this experiment using Matplotlib<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, Seaborn<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, and Plotly<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, in addition to pre-installed Python tools.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Results and discussion</title>
    <sec id="Sec15">
      <title>Performance comparison of different feature encodings</title>
      <p id="Par41">In this section, we systematically evaluated the effect of various feature encodings, including single-feature (GloVe, fastText, and Word2Vec) and weighted-feature (GloVe + fastText, GloVe + Word2Vec, fastText + Word2Vec, and GloVe + fastText + Word2Vec) encodings in DBP identification. These features were inputted to a CNN classifier to evaluate their corresponding models using the 5-fold cross-validation test. The cross-validation performance of variant CNN classifiers trained with different features are provided in Table <xref rid="Tab2" ref-type="table">2</xref> and Fig. <xref rid="Fig3" ref-type="fig">3</xref>A. It is worth noting that the parameters of CNN classifiers were carefully determined to improve their performance under the 5-fold cross-validation process.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance comparison of CNN classifiers trained with different feature encodings on the training dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">GloVe</td><td align="left">0.810</td><td align="left">75.00</td><td align="left">71.15</td><td align="left">77.63</td><td align="left">0.485</td><td align="left">68.52</td><td align="left">0.698</td></tr><tr><td align="left">fastText</td><td align="left">0.785</td><td align="left">73.44</td><td align="left">67.24</td><td align="left">78.57</td><td align="left">0.462</td><td align="left">72.22</td><td align="left">0.696</td></tr><tr><td align="left">Word2Vec</td><td align="left">0.793</td><td align="left">71.09</td><td align="left">73.13</td><td align="left">68.85</td><td align="left">0.420</td><td align="left">72.06</td><td align="left">0.726</td></tr><tr><td align="left">fastText + Word2Vec</td><td align="left">0.826</td><td align="left">75.78</td><td align="left">70.18</td><td align="left">80.28</td><td align="left">0.508</td><td align="left">74.07</td><td align="left">0.721</td></tr><tr><td align="left">GloVe + Word2Vec</td><td align="left">0.820</td><td align="left">76.64</td><td align="left">71.96</td><td align="left">85.00</td><td align="left">0.523</td><td align="left">77.50</td><td align="left">0.713</td></tr><tr><td align="left">GloVe + fastText</td><td align="left">0.839</td><td align="left">78.12</td><td align="left">72.96</td><td align="left">89.19</td><td align="left">0.549</td><td align="left">80.95</td><td align="left">0.738</td></tr><tr><td align="left">GloVe + fastText +Word2Vec</td><td align="left">0.864</td><td align="left">79.07</td><td align="left">75.10</td><td align="left">91.49</td><td align="left">0.585</td><td align="left">86.21</td><td align="left">0.740</td></tr></tbody></table></table-wrap></p>
      <p id="Par42">Among single-based features, GloVe outperformed fastText and Word2Vec in terms of all performance metrics. The AUC, ACC, Sen, Spe, and MCC of GloVe were 0.810, 75.00%, 71.15%, 77.63% and 0.485, respectively. Interestingly, AUC, ACC, and MCC of GloVe were 2.5–1.7%, 1.56–3.91%, and 2.3–6.5% higher than fastText and Word2Vec, respectively. A weighted feature was created by adding different combinations of the single feature extraction methods in order to improve the predictive performance. As can be seen from Table <xref rid="Tab2" ref-type="table">2</xref>, we observe that the performance the combination of GloVe, fastText and Word2Vec is better than those of other three weighted features in terms of all performance metrics. The ACC, Sen, Spe, and MCC of the combination of GloVe, fastText and Word2Vec are 79.07% 64.10%, 91.49% and 0.585, respectively, which are 0.95–3.29%, 2.14–4.92%, 2.30–11.91%, 0.036–0.077%, 5.26–12.14% and 0.002–0.027% higher than other combination features, respectively. Figure <xref rid="Fig3" ref-type="fig">3</xref>A shows that the AUC value of GloVe+fastText+Word2Vec 0.864, which is larger than the other three weighted features. Overall, we observed that the Sen value of individual features was slightly higher than that of the corresponding weighted features in some cases. Moreover, the performance of the top weighted features (GloVe + fastText + Word2Vec) is significantly higher than the single-view feature in terms of all evaluation metrics. Weighteds features archive higher prediction performances to the single-view feature in terms of all evaluation metrics. Therefore, in this study, the GloVe + fastText + Word2Vec feature outperformed other single and weighted features and is considered as the optimal one in termes of computational cost and predictive performance.</p>
    </sec>
    <sec id="Sec16">
      <title>Feature section approaches improve the predictive performance</title>
      <p id="Par43">The original feature subsets extracted from feature encoding techniques might contain noisy and redundant information that can affect the classifiers’ performance. Therefore, we utilized feature selection methods to determine important features from the original feature subsets. Here, three feature selection techniques, including RFE<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, LASSO<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, and SHAP<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, were utilized for determining the important features from GloVe + fastText + Word2Vec feature encoding. In our experiment, we ranked all features using its importance obtained from RFE, LASSO, and SHAP and then established the six feature subsets that consisted of the top-ranked features ranging from top 200 to the top 450 features with an interval of 50. Then, for each feature selection technique, the six feature subsets were fed to develop individual CNN classifiers whose corresponding prediction results based on a 5-fold cross-validation were provided in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance comparison of various feature sets derived from different feature selection techniques.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature selection</th><th align="left">No. of features</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="6">RFE</td><td align="left">200</td><td align="left">0.859</td><td align="left">76.74</td><td align="left">58.82</td><td align="left">88.46</td><td align="left">0.503</td><td align="left">76.92</td><td align="left">0.667</td></tr><tr><td align="left">250</td><td align="left">0.853</td><td align="left">79.69</td><td align="left">65.38</td><td align="left">89.47</td><td align="left">0.574</td><td align="left">80.95</td><td align="left">0.723</td></tr><tr><td align="left">300</td><td align="left">0.866</td><td align="left">82.55</td><td align="left">71.87</td><td align="left">88.89</td><td align="left">0.621</td><td align="left">79.31</td><td align="left">0.754</td></tr><tr><td align="left">350</td><td align="left">0.853</td><td align="left">80.23</td><td align="left">85.11</td><td align="left">74.36</td><td align="left">0.600</td><td align="left">80.00</td><td align="left">0.825</td></tr><tr><td align="left">400</td><td align="left">0.876</td><td align="left">81.40</td><td align="left">67.57</td><td align="left">91.83</td><td align="left">0.622</td><td align="left">86.20</td><td align="left">0.758</td></tr><tr><td align="left">450</td><td align="left">0.866</td><td align="left">79.10</td><td align="left">62.50</td><td align="left">88.89</td><td align="left">0.541</td><td align="left">76.92</td><td align="left">0.690</td></tr><tr><td align="left" rowspan="6">Lasso</td><td align="left">200</td><td align="left">0.837</td><td align="left">77.57</td><td align="left">69.05</td><td align="left">83.07</td><td align="left">0.526</td><td align="left">72.50</td><td align="left">0.707</td></tr><tr><td align="left">250</td><td align="left">0.869</td><td align="left">81.25</td><td align="left">82.86</td><td align="left">79.31</td><td align="left">0.622</td><td align="left">82.86</td><td align="left">0.829</td></tr><tr><td align="left">300</td><td align="left">0.867</td><td align="left">81.35</td><td align="left">87.50</td><td align="left">76.09</td><td align="left">0.581</td><td align="left">76.09</td><td align="left">0.814</td></tr><tr><td align="left">350</td><td align="left">0.856</td><td align="left">79.10</td><td align="left">81.82</td><td align="left">76.19</td><td align="left">0.581</td><td align="left">78.26</td><td align="left">0.800</td></tr><tr><td align="left">400</td><td align="left">0.882</td><td align="left">81.40</td><td align="left">64.71</td><td align="left">92.30</td><td align="left">0.607</td><td align="left">84.61</td><td align="left">0.733</td></tr><tr><td align="left">450</td><td align="left">0.877</td><td align="left">79.68</td><td align="left">65.38</td><td align="left">89.47</td><td align="left">0.574</td><td align="left">80.95</td><td align="left">0.723</td></tr><tr><td align="left" rowspan="6">SHAP</td><td align="left">200</td><td align="left">0.873</td><td align="left">76.56</td><td align="left">60.00</td><td align="left">91.18</td><td align="left">0.544</td><td align="left">85.71</td><td align="left">0.706</td></tr><tr><td align="left">250</td><td align="left">0.866</td><td align="left">80.25</td><td align="left">65.79</td><td align="left">91.67</td><td align="left">0.604</td><td align="left">86.21</td><td align="left">0.746</td></tr><tr><td align="left">300</td><td align="left">0.883</td><td align="left">81.31</td><td align="left">66.66</td><td align="left">91.89</td><td align="left">0.616</td><td align="left">85.70</td><td align="left">0.750</td></tr><tr><td align="left">350</td><td align="left">0.880</td><td align="left">81.40</td><td align="left">68.57</td><td align="left">90.19</td><td align="left">0.611</td><td align="left">82.76</td><td align="left">0.750</td></tr><tr><td align="left">400</td><td align="left"><bold>0.883</bold></td><td align="left"><bold>82.56</bold></td><td align="left"><bold>69.44</bold></td><td align="left"><bold>92.00</bold></td><td align="left"><bold>0.641</bold></td><td align="left"><bold>86.21</bold></td><td align="left"><bold>0.769</bold></td></tr><tr><td align="left">450</td><td align="left">0.863</td><td align="left">80.23</td><td align="left">66.67</td><td align="left">90.00</td><td align="left">0.591</td><td align="left">82.76</td><td align="left">0.739</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par44">As seen in Table <xref rid="Tab3" ref-type="table">3</xref>, the optimal subsets containing top 300, 400, and 400 optimal features derived from the RFE, LASSO and SHAP techniques, respectively, outperformed other feature sets in terms of both ACC and AUC. In the meanwhile, the performance of the optimal subsets from the SHAP technique outperformed than the RFE and LASSO techniques. To be specific, the AUC, ACC, Sen, Spe, MCC, Pre, and F1 of the optimal subset from the SHAP technique were 0.883, 82.56%, 69.44%, 92.00%, 0.641, 86.21, and 0.769, respectively. Thus, the optimal subset derived from the SHAP technique was considered to develop our proposed model. To check the effectiveness of the optimal subset, we compared its performance with the original feature set. As shown in Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab4" ref-type="table">4</xref>, the ACC, Sen, MCC and F1 of the optimal subset were 3.49%, 5.34%, 5.60%, and 3.40% higher than the original feature set. For convenience of discussion, the CNN classifier combined with the optimal subset from the SHAP technique is referred herein as Deep-WET.
</p>
      <p id="Par45">Altogether, the SHAP technique was a powerful approach for implementing DNA binding protein datasets. To make a clear comparison of prediction effects, the results of the SHAP importance bar graph on the GloVe + fastText + Word2Vec dataset for 400 feature dimensions are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>A. In Fig. <xref rid="Fig2" ref-type="fig">2</xref>A, the bar plot generated by SHAP shows the important features in the form of horizontal bars, with length representing the importance of features. We summarized the most significant features by sorting them in decreasing order based on absolute Shapley values. In addition, the SHAP summary plot for 400 feature dimensions is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>B. It represents a high-level composite look that indicates the important features and effects. Each point depicts a SHAP score in the plot for a particular feature instance. Notably, we can observe the relationship between the feature value and the effect on prediction in the SHAP summary plot.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance comparison of CNN classifiers trained with different optimal feature sets on the training dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">GloVe</td><td align="left">0.852</td><td align="left">75.70</td><td align="left">63.46</td><td align="left">87.27</td><td align="left">0.524</td><td align="left">82.50</td><td align="left">0.717</td></tr><tr><td align="left">fastText</td><td align="left">0.826</td><td align="left">77.91</td><td align="left">63.89</td><td align="left">88.00</td><td align="left">0.542</td><td align="left">79.31</td><td align="left">0.708</td></tr><tr><td align="left">Word2Vec</td><td align="left">0.806</td><td align="left">74.22</td><td align="left">66.67</td><td align="left">81.54</td><td align="left">0.488</td><td align="left">77.78</td><td align="left">0.718</td></tr><tr><td align="left">fastText + Word2Vec</td><td align="left">0.857</td><td align="left">77.57</td><td align="left">68.18</td><td align="left">84.13</td><td align="left">0.532</td><td align="left">75.00</td><td align="left">0.714</td></tr><tr><td align="left">GloVe + Word2Vec</td><td align="left">0.856</td><td align="left">79.44</td><td align="left">70.45</td><td align="left">85.71</td><td align="left">0.571</td><td align="left">77.50</td><td align="left">0.738</td></tr><tr><td align="left">GloVe + fastText</td><td align="left">0.850</td><td align="left">80.37</td><td align="left">69.39</td><td align="left">89.66</td><td align="left">0.608</td><td align="left">85.00</td><td align="left">0.764</td></tr><tr><td align="left">GloVe + fastText +Word2Vec</td><td align="left">0.883</td><td align="left">82.56</td><td align="left">69.44</td><td align="left">92.00</td><td align="left">0.641</td><td align="left">86.21</td><td align="left">0.769</td></tr></tbody></table></table-wrap></p>
      <p id="Par46">From the above-mentioned observations and discussion, we concluded that the SHAP technique was a more powerful and effective feature selection one; therefore, this technique was chose for selecting a subset of features for predicting DBPs herein. In addition, we also applied the SHAP technique in other types of features whose corresponding prediction results were summarized in Table <xref rid="Tab4" ref-type="table">4</xref> and Fig. <xref rid="Fig3" ref-type="fig">3</xref>. By comparing the performance of the models without feature selection (Table <xref rid="Tab2" ref-type="table">2</xref> along with Fig. <xref rid="Fig3" ref-type="fig">3</xref>A and C) and with the SHAP-based feature selection (Table <xref rid="Tab4" ref-type="table">4</xref> along with Fig. <xref rid="Fig3" ref-type="fig">3</xref>B and D), the models with the SHAP-based feature selection achieve better performance than those of the models without feature selection.</p>
    </sec>
    <sec id="Sec17">
      <title>Hyperparameter of CNN</title>
      <p id="Par47">The hyperparameter learning rate controls how the model changes according to estimated error each time the weights are updated. Finding the optimal learning rate can be challenging, because a higher learning rate makes the gradient drop faster, and a lower learning rate leads to the gradient hardly converging. Here, the faster rate of gradient drop results in informative and meaningful features failing to get extracted over each iteration, and the lower rate of gradient convergence results in a longer training time. Therefore, five learning rates in a range of 1e–6 to 1e–2 are implemented for the proposed model to find the optimal performance. From Table <xref rid="Tab5" ref-type="table">5</xref>, the learning rate of 0.00001 gives the highest performance compared to implementing the remaining four learning rates. However, the sensitivity value of using the 0.00001 learning rate is suboptimal compared to the value of the 0.0001 and 0.001 learning rates.<fig id="Fig2"><label>Figure 2</label><caption><p>The SHAP importance bar graph results for the GloVe + fastText + Word2Vec dataset with 400 feature dimensions are presented. (<bold>A</bold>) bar plot generated by SHAP shows the important features in the form of horizontal bars (<bold>B</bold>) SHAP summary plot for the 400 feature dimensions.</p></caption><graphic xlink:href="41598_2024_52653_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Figure 3</label><caption><p>ROC curves and AUPR curves of CNN classifiers are depicted for both single and hybrid feature spaces without feature selection (<bold>A</bold>, <bold>C</bold>) and with SHAP-based feature selection (<bold>B</bold>, <bold>D</bold>).</p></caption><graphic xlink:href="41598_2024_52653_Fig3_HTML" id="MO3"/></fig><table-wrap id="Tab5"><label>Table 5</label><caption><p>Cross-validation results of CNN classifiers trained with different learning rates.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Learning rate</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">1e–2</td><td align="left">0.835</td><td align="left">79.06</td><td align="left">64.29</td><td align="left">86.21</td><td align="left">0.515</td><td align="left">69.23</td><td align="left">0.667</td></tr><tr><td align="left">1e–3</td><td align="left">0.803</td><td align="left">81.39</td><td align="left"><bold>72.73</bold></td><td align="left">84.38</td><td align="left">0.543</td><td align="left">61.54</td><td align="left">0.667</td></tr><tr><td align="left">1e–4</td><td align="left">0.838</td><td align="left">81.25</td><td align="left">69.57</td><td align="left">87.80</td><td align="left">0.586</td><td align="left">76.19</td><td align="left">0.727</td></tr><tr><td align="left">1e–5</td><td align="left">0.883</td><td align="left">82.56</td><td align="left">69.44</td><td align="left">92.00</td><td align="left">0.641</td><td align="left">86.21</td><td align="left">0.769</td></tr><tr><td align="left">1e–6</td><td align="left">0.874</td><td align="left">79.07</td><td align="left">61.11</td><td align="left">92.00</td><td align="left">0.571</td><td align="left">84.62</td><td align="left">0.710</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec18">
      <title>Comparison of Deep-WET with conventional ML classifiers</title>
      <p id="Par48">To evaluate the performance of the proposed Deep-WET, we compared its predictive performance with conventional ML classifiers. Herein, the conventional ML classifiers were built using four well-known ML classifiers (i.e., SVM<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, XGBoost<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, LightGBM<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, and CNN<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>) and the three NLP-based word embedding techniques (i.e., GloVe, fastText, and Word2Vec). In total, 11 conventional ML classifiers were created in this study. It is noteworthy that the parameters of all ML classifiers were carefully optimized to improve their prediction capability under a 5-fold cross-validation procedure. In these experiments, classifiers have been trained a total of 24 times. The prediction performance based on both 5-fold cross-validation and independent tests are listed in Tables 7-8. In addition, their respective graphs are shown in Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>Performance comparison of three feature groups for SVM, XGBoost, LightGBM, and CNN classifiers under 5-fold cross-validation test on various evaluation metrics: (<bold>A</bold>) GloVE, (<bold>B</bold>) Word2Vec, and (<bold>C</bold>) fastText.</p></caption><graphic xlink:href="41598_2024_52653_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Figure 5</label><caption><p>Performance comparison of three feature groups for SVM, XGBoost, LightGBM, and CNN classifiers under independent test on various evaluation metrics: (<bold>A</bold>) GloVE, (<bold>B</bold>) Word2Vec, and (<bold>C</bold>) fastText.</p></caption><graphic xlink:href="41598_2024_52653_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Figure 6</label><caption><p>Performance comparison of various machine learning classifiers trained with three feature groups, utilizing different classifier and feature representations, is presented in terms of AUC and MCC evaluation metrics under 5-fold cross-validation (<bold>A</bold>, <bold>B</bold>) and independent testing (<bold>C</bold>, <bold>D</bold>).</p></caption><graphic xlink:href="41598_2024_52653_Fig6_HTML" id="MO6"/></fig><table-wrap id="Tab6"><label>Table 6</label><caption><p>Cross-validation results of different ML classifiers and feature encoding schemes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">Classifier</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="4">GloVe</td><td align="left">SVM</td><td align="left">0.822</td><td align="left">74.42</td><td align="left">58.14</td><td align="left">90.69</td><td align="left">0.517</td><td align="left">86.21</td><td align="left">0.694</td></tr><tr><td align="left">XGBoost</td><td align="left">0.805</td><td align="left">76.63</td><td align="left">66.67</td><td align="left">83.87</td><td align="left">0.516</td><td align="left">75.00</td><td align="left">0.706</td></tr><tr><td align="left">LightGBM</td><td align="left">0.817</td><td align="left">77.90</td><td align="left">63.16</td><td align="left">89.58</td><td align="left">0.554</td><td align="left">82.76</td><td align="left">0.716</td></tr><tr><td align="left">CNN (Deep-WET)</td><td align="left"><bold>0.883</bold></td><td align="left"><bold>82.56</bold></td><td align="left"><bold>69.44</bold></td><td align="left"><bold>92.00</bold></td><td align="left"><bold>0.641</bold></td><td align="left"><bold>86.21</bold></td><td align="left"><bold>0.769</bold></td></tr><tr><td align="left" rowspan="4">fastText</td><td align="left">SVM</td><td align="left">0.823</td><td align="left">73.26</td><td align="left">57.50</td><td align="left">86.96</td><td align="left">0.469</td><td align="left">79.31</td><td align="left">0.667</td></tr><tr><td align="left">XGBoost</td><td align="left">0.800</td><td align="left">76.74</td><td align="left">81.58</td><td align="left">72.92</td><td align="left">0.541</td><td align="left">70.45</td><td align="left">0.756</td></tr><tr><td align="left">LightGBM</td><td align="left">0.825</td><td align="left">75.58</td><td align="left">61.11</td><td align="left">86.00</td><td align="left">0.492</td><td align="left">75.86</td><td align="left">0.677</td></tr><tr><td align="left">CNN</td><td align="left">0.849</td><td align="left">80.37</td><td align="left">69.39</td><td align="left">89.66</td><td align="left">0.608</td><td align="left">85.00</td><td align="left">0.764</td></tr><tr><td align="left" rowspan="4">Word2Vec</td><td align="left">SVM</td><td align="left">0.805</td><td align="left">74.21</td><td align="left">69.81</td><td align="left">77.33</td><td align="left">0.470</td><td align="left">68.52</td><td align="left">0.692</td></tr><tr><td align="left">XGBoost</td><td align="left">0.806</td><td align="left">75.69</td><td align="left">65.91</td><td align="left">82.54</td><td align="left">0.493</td><td align="left">72.50</td><td align="left">0.691</td></tr><tr><td align="left">LightGBM</td><td align="left">0.810</td><td align="left">74.77</td><td align="left">64.44</td><td align="left">82.26</td><td align="left">0.477</td><td align="left">72.50</td><td align="left">0.683</td></tr><tr><td align="left">CNN</td><td align="left">0.826</td><td align="left">77.91</td><td align="left">63.89</td><td align="left">88.00</td><td align="left">0.542</td><td align="left">79.31</td><td align="left">0.708</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par49">As can be seen from Table <xref rid="Tab6" ref-type="table">6</xref>, Deep-WET achieved the overall best performance compared with the compared ML classifiers in terms of almost performance metrics, with the only exception of the Sen. Meanwhile, CNN-fastText and CNN-Word2Vec were the second-best and third-best classifiers in terms of ACC. To be specific, the ACC values of Deep-WET, CNN-fastText, and CNN-Word2Vec were 82.56%, 80.37%, and 77.91%, respectively. In addition, Deep-WET’s AUC, ACC, Spe, and MCC were 3.40%, 2.19%, 2.34% and 3.30%, respectively, higher than the second-best method CNN-fastText. In case of the independent test results, Deep-WET still outperformed the compared ML classifiers in terms of ACC, Spe, MCC, Pre and F1. Deep-WET’s ACC, Spe, MCC, Pre, and F1 were 1.37%, 3.84%, 2.60%, 5.13% and 2.10%, respectively, higher than the second-best method CNN-fastText.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Independent test results of different ML classifiers and feature encoding schemes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">Classifier</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="4">GloVe</td><td align="left">SVM</td><td align="left">0.803</td><td align="left">73.52</td><td align="left">76.25</td><td align="left">71.11</td><td align="left">0.473</td><td align="left">70.11</td><td align="left">0.731</td></tr><tr><td align="left">XGBoost</td><td align="left">0.787</td><td align="left">73.97</td><td align="left">79.41</td><td align="left">69.23</td><td align="left">0.486</td><td align="left">69.23</td><td align="left">0.740</td></tr><tr><td align="left">LightGBM</td><td align="left">0.802</td><td align="left">75.34</td><td align="left">78.37</td><td align="left">72.22</td><td align="left">0.507</td><td align="left">74.36</td><td align="left">0.763</td></tr><tr><td align="left">CNN (Deep-WET)</td><td align="left">0.805</td><td align="left">78.08</td><td align="left">78.05</td><td align="left">78.13</td><td align="left">0.559</td><td align="left">82.05</td><td align="left">0.800</td></tr><tr><td align="left" rowspan="4">fastText</td><td align="left">SVM</td><td align="left">0.812</td><td align="left">73.20</td><td align="left">78.26</td><td align="left">68.63</td><td align="left">0.470</td><td align="left">69.23</td><td align="left">0.735</td></tr><tr><td align="left">XGBoost</td><td align="left">0.804</td><td align="left">73.77</td><td align="left">75.41</td><td align="left">72.13</td><td align="left">0.476</td><td align="left">73.02</td><td align="left">0.742</td></tr><tr><td align="left">LightGBM</td><td align="left">0.783</td><td align="left">74.59</td><td align="left">72.22</td><td align="left">78.00</td><td align="left">0.494</td><td align="left">82.54</td><td align="left">0.770</td></tr><tr><td align="left">CNN</td><td align="left"><bold>0.816</bold></td><td align="left">76.71</td><td align="left">78.95</td><td align="left">74.29</td><td align="left">0.533</td><td align="left">76.92</td><td align="left">0.779</td></tr><tr><td align="left" rowspan="4">Word2Vec</td><td align="left">SVM</td><td align="left">0.778</td><td align="left">73.19</td><td align="left">66.67</td><td align="left">77.05</td><td align="left">0.433</td><td align="left">63.16</td><td align="left">0.649</td></tr><tr><td align="left">XGBoost</td><td align="left">0.773</td><td align="left">75.25</td><td align="left">69.44</td><td align="left">78.69</td><td align="left">0.476</td><td align="left">65.79</td><td align="left">0.676</td></tr><tr><td align="left">LightGBM</td><td align="left">0.797</td><td align="left">73.98</td><td align="left">81.08</td><td align="left">66.67</td><td align="left">0.483</td><td align="left">71.43</td><td align="left">0.760</td></tr><tr><td align="left">CNN</td><td align="left">0.807</td><td align="left">75.35</td><td align="left">74.42</td><td align="left">76.67</td><td align="left">0.504</td><td align="left">82.05</td><td align="left">0.781</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par50">To further the comparison, Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref> illustrate the cross-validation and independent test performance for our proposed Deep-WET approach along with four robust classifiers with all the evaluation metrics. From Tables <xref rid="Tab7" ref-type="table">7</xref>, <xref rid="Tab8" ref-type="table">8</xref> and Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref>, we can summarize several observations as follows: (i) GloVe features obtained the highest predictive results as compared to fastText and Word2Vec; however, these three feature-encoding techniques all achieved promising performance for the CNN classifiers, followed by LightGBM, XGBoost, and SVM classifier. Word2Vec achieved relatively lower performance, whereas fastText was slightly better than Word2Vec, (ii) CNN classifier consistently achieved the highest results compared to the other three classifiers for all three feature-encoding techniques, and (iii) Finally, our proposed Deep-WET achieved better performance than other conventional ML classifiers, highlighting its superior discriminative power.</p>
    </sec>
    <sec id="Sec19">
      <title>Comparison of Deep-WET with the state-of-the-art methods</title>
      <p id="Par51">To further validate the discriminative power of Deep-WET method, we compared its prediction performance against other existing DBP methods, including DPP-PseAAC<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, PseDNA-Pro<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, iDNA-Prot<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, iDNA-Prot|dis<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, PSFM-DBT<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, Local-DPP<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, iDNAProt-ES<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, IKP-DBPPred<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> Xiuquan et al.<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, iDRBP-MMC<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> and TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, on the independent test data. The prediction performance of the existing methods were obtained by submitting protein sequences in the independent test dataset (148 DBPs and 148 non-DBPs) to their own webservers. Since the web sever of iDNAProt-ES was not functional, the prediction results of iDNAProt-ES were obtained from the reimplementation of iDNAProt-ES and the standalone version of HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, respectively. Table 8 shows the prediction results of Deep-WET and other existing methods.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Performance comparisons of DeepWET with the state-of-the-art methods on the independent test dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Predictor<sup>a</sup></th><th align="left">AUC</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">DPP-PseAAC</td><td align="left">61.15</td><td align="left">55.41</td><td align="left">66.89</td><td align="left">0.225</td><td align="left">62.60</td><td align="left">0.588</td></tr><tr><td align="left">iDNA-Prot</td><td align="left">62.16</td><td align="left">63.51</td><td align="left">60.81</td><td align="left">0.243</td><td align="left">61.84</td><td align="left">0.627</td></tr><tr><td align="left">iDNA-Prot|dis</td><td align="left">68.24</td><td align="left">72.30</td><td align="left">64.19</td><td align="left">0.366</td><td align="left">66.88</td><td align="left">0.695</td></tr><tr><td align="left">PseDNA-Pro</td><td align="left">67.23</td><td align="left">78.38</td><td align="left">56.08</td><td align="left">0.354</td><td align="left">64.09</td><td align="left">0.705</td></tr><tr><td align="left">PSFM-DBT</td><td align="left">68.58</td><td align="left">71 .62</td><td align="left">65.54</td><td align="left">0.372</td><td align="left">67.52</td><td align="left">0.695</td></tr><tr><td align="left">IKP-DBPPred</td><td align="left">58.11</td><td align="left">52.70</td><td align="left">63.51</td><td align="left">0.163</td><td align="left">59.09</td><td align="left">0.557</td></tr><tr><td align="left">Local-DPP</td><td align="left">48.65</td><td align="left">3.38</td><td align="left"><bold>93.92</bold></td><td align="left">– 0.06</td><td align="left">35.71</td><td align="left">0.062</td></tr><tr><td align="left">iDNAProt-ES(on PDB1075)</td><td align="left">71.62</td><td align="left">91 .89</td><td align="left">51.35</td><td align="left">0.473</td><td align="left">65.38</td><td align="left">0.764</td></tr><tr><td align="left">TargetDBP</td><td align="left">76.69</td><td align="left">76.35</td><td align="left">77.03</td><td align="left">0.534</td><td align="left">76.87</td><td align="left">0.766</td></tr><tr><td align="left">Xiuquan et al.</td><td align="left">77</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">iDRBP-MMC</td><td align="left">70</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">Deep-WET</td><td align="left"><bold>78.08</bold></td><td align="left"><bold>78.05</bold></td><td align="left"><bold>78.13</bold></td><td align="left"><bold>0.559</bold></td><td align="left"><bold>82.05</bold></td><td align="left"><bold>0.800</bold></td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p><p><sup>a</sup>The prediction performance of the existing methods were obtained by submitting protein sequences in the independent test dataset (148 DBPs and 148 non-DBPs) to their own webservers.</p></table-wrap-foot></table-wrap></p>
      <p id="Par52">According to the F1 and MCC values, these two evaluation metrics of binary predictions, recorded in Table <xref rid="Tab8" ref-type="table">8</xref>, we can see that Deep-WET has superior performance over other exiting methods in terms of ACC, MCC, Pre, and F1. Notably, by comparing the proposed Deep-WET approach with the second-best predictor TargetDBP in terms of ACC, we observe that Deep-WET achieved improvements of 1.39%, 1.70%, 2.50%, 5.18%, and 3.40% on ACC, Sen, MCC, Pre, and F1, respectively. Although, iDNAProt-ES obtained the highest Sen value of 91.89%, this method provided the lowest Spe value of 51.35%. The main reason behind this high Sen is that iDNAProt-ES has lower false negative (FN) prediction. In contrast, Local-DPP obtained the highest prediction performance in terms of Spe (93.92%) and shows much lower Pre scores (35.71%) producing many false negatives (FN) during prediction, but Acc values of iDNAProt-ES and Local-DPP are lower than those of Deep-WET. Taken together, these results demonstrated that Deep-WET has a great potential for DBP prediction.</p>
    </sec>
    <sec id="Sec20">
      <title>Ablation study</title>
      <p id="Par53">Our CNN model has key components such as convolutional filters, pooling strategies, kernel sizes and fully connected layers, etc. Here, we have conducted ablation studies using the GloVe + fastText + Word2Vec dataset under 5-fold CV, assessing how each individual component influences the predictive performance of Deep-WET:<list list-type="bullet"><list-item><p id="Par54">Remove Specific Convolutional Filters (RSCF): we removed specific filters in the convolutional layers responsible for capturing sequence motifs or patterns linked to DNA binding.</p></list-item><list-item><p id="Par55">Variation in Pooling Strategies (VPS): adjust the baseline model by altering pooling strategies to evaluate how these changes affect the recognition of relevant sequence features.</p></list-item><list-item><p id="Par56">Variation in Kernel Sizes (VKS): Explore diverse kernel sizes within the convolutional layers to capture sequence motifs associated with DNA binding of different lengths.</p></list-item><list-item><p id="Par57">Removal of Fully Connected Layers (RFCL): create a modified version of the baseline model by removing one or more fully connected layers to examine the significance of global features in the classification task.</p></list-item></list></p>
      <p id="Par58">Figure <xref rid="Fig7" ref-type="fig">7</xref> show the performance comparison of Deep-WET and its four variants in terms of AUC on GloVe + fastText + Word2Vec dataset. We can observe that our CNN has better performance than CNN-RSCF, CNN-VPS, CNN-VKS and CNN-RFCL on experiment datasets Here, our-CNN obtains the best AUC score of 0.883, and it is 0.085%, 0.135%, 0.105% and 0.165% higher than that of CNN-RSCF, CNN-VPS, CNN- VKS and CNN-RFCL, respectively, which can illustrate that these parts in our design can improve the predictive performance. Among them, CNN-VPS and CNN-RFCL have the lowest performance. This shows that it is very important to perform the hyperparameters setting of CNN classifiers (see Table <xref rid="Tab1" ref-type="table">1</xref>), that can effectively improve the performance of CNN.<fig id="Fig7"><label>Figure 7</label><caption><p>Comparative analysis between Our CNN and its ablation experiments on the GloVe + fastText + Word2Vec dataset.</p></caption><graphic xlink:href="41598_2024_52653_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec21">
      <title>Web server, data and software availability</title>
      <p id="Par59">We used Apache (2.4.48), Python (3.8.0), and Laravel (8.16.1) to develop a web server for Deep-WET at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.000webhostapp.com/">https://deepwet-dna.000webhostapp.com/</ext-link>. Users can upload or input DNA binding protein sequences of viruses and humans in FASTA format to predict DBPs with probability scores. After clicking the submit button, the server will evaluate the protein sequence and check the format for processing. Prediction results will be generated in a tabular format with detailed information on the word serial number and predicted probability of DBPs and predicted class (DBPs/non-DBPs). Detailed instructions for the webserver can be found on the README option. After the final job, users will get a job ID to be used for further queries. The Deep-WET web server application stores this job ID for fifteen days. Deep-WET may have a long computational time when users input large protein sequences files, since Deep-WET needs to perform NLP-based word embedding packages to generate discriminative features and fix the suitable parameters for the CNN classifier to predict. We strongly suggest inputting a low number of DBP sequences at a time. The experimental datasets for this study are available at: <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.000webhostapp.com/data">https://deepwet-dna.000webhostapp.com/data</ext-link>.</p>
    </sec>
  </sec>
  <sec id="Sec22">
    <title>Conclusions</title>
    <p id="Par60">Identifying DBPs is vital to discovering fundamental protein-DNA mechanisms and understanding their biological interactions. Here, we develop a new deep learning-based approach termed Deep-WET,to achieve more accurate and improved prediction of DBPs. In Deep-WET, we extracted three NLP-based word embedding features to generate single features then combined them sequentially, and assigned weights learned through the use of the DE algorithm. The SHAP technique was utilized to gain the effective feature subsets, and a deep learning-based CNN algorithm was used as a model classifier for predicting DBPs. Comparative analysis on the independent test dataset showed that Deep-WET achieved improved performance compared with conventional ML classifiers and the existing methods, highlight the effectiveness and robustnessof the proposed Deep-WET. The improved performance of Deep-WET is mainly due to the utilization of NLP-based word embedding features that can effectively capture the characteristics of DBPs. A user-friendly web server for Deep-WET is available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.royalit.agency/">https://deepwet-dna.royalit.agency/</ext-link>. Deep-WET is anticipated to be a powerful tool to serve the community-wide effort for the accurate and large-scale identification of potential DBPs from sequences information. Deep learning shows advanced prediction abilities in various fields of computational biology such as hERG blockers<sup><xref ref-type="bibr" rid="CR57">57</xref></sup> , disease-related metabolites<sup><xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR59">59</xref></sup>, single-cell<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> and human lncRNA-miRNA interactions<sup><xref ref-type="bibr" rid="CR61">61</xref>,<xref ref-type="bibr" rid="CR62">62</xref></sup>. Most studies propose deep learning based models<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> for prediction tasks. To enhance the predictive capabilities of our Deep-WET model, our future efforts will focus on three key areas: (1) although our NLP-based feature extraction is now commonly used for extracting distinct features, there may be some limitations, such as ambiguities, lexical gaps, and structural gaps. It would be interesting to use deep learning-based autoencoders<sup><xref ref-type="bibr" rid="CR59">59</xref>,<xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup> to effectively convey the hidden information within DBPs sequences; (2) implementing a small-loss approach and integrating probabilistic local outlier factor (pLOF) with the extracted features to tackle the challenge of label noise in the dataset, ensuring a trustworthy application; (3) developing a graph-based deep learning model for predicting DBPs with unknown structures.</p>
    <p id="Par61">Cellular death is a fundamental and complex biological process that is an underlying driver for many diseases. Authors in<sup><xref ref-type="bibr" rid="CR65">65</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup>, worked for cell death. Our CNN model can be used to classify cells undergoing cell death. This deep learning network has the ability to highly predict cell death. Finally, it is possible to provide a simple Python tool that can be broadly used to detect cell death. Furthermore, our CNN model can recommend specific drugs for the disease.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported by the TM R&amp;D Fund (Project no. RDTC/221054 and SAP ID: MMUE/220023) and the Multimedia University (MMU) IR Fund (Project ID MMUI/220041).</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>Project administration and supervision: K.O.M.G., W.S.; conceptualization, investigation, methodology and visualization: S.M. H.M., M.F.H. and D.N.; analysis, validation and software: S.M.H.M. and M.F.H.; web server development: M.F.H.; writing—original draft: S.M.H.M.; writing—review and editing: S.M.H.M., W.S., K.O.M.G., and D.N. All authors reviewed and approved the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All the data used in this study are available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/data">https://deepwet-dna.monarcatechnical.com/data</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par62">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>J-M</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>X-P</given-names>
          </name>
        </person-group>
        <article-title>Competitive aptamer bioassay for selective detection of adenosine triphosphate based on metal-paired molecular conformational switch and fluorescent gold nanoclusters</article-title>
        <source>Biosens. Bioelectron.</source>
        <year>2012</year>
        <volume>36</volume>
        <fpage>135</fpage>
        <lpage>141</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bios.2012.04.015</pub-id>
        <?supplied-pmid 22560440?>
        <pub-id pub-id-type="pmid">22560440</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ren</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Genome-wide location and function of dna binding proteins</article-title>
        <source>Science</source>
        <year>2000</year>
        <volume>290</volume>
        <fpage>2306</fpage>
        <lpage>2309</lpage>
        <pub-id pub-id-type="doi">10.1126/science.290.5500.2306</pub-id>
        <?supplied-pmid 11125145?>
        <pub-id pub-id-type="pmid">11125145</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gurova</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>New hopes from old drugs: Revisiting dna-binding small molecules as anticancer agents</article-title>
        <source>Future Oncol.</source>
        <year>2009</year>
        <volume>5</volume>
        <fpage>1685</fpage>
        <lpage>1704</lpage>
        <pub-id pub-id-type="doi">10.2217/fon.09.127</pub-id>
        <?supplied-pmid 20001804?>
        <pub-id pub-id-type="pmid">20001804</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leung</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>DS-H</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>VP-Y</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>D-L</given-names>
          </name>
        </person-group>
        <article-title>Dna-binding small molecules as inhibitors of transcription factors</article-title>
        <source>Med. Res. Rev.</source>
        <year>2013</year>
        <volume>33</volume>
        <fpage>823</fpage>
        <lpage>846</lpage>
        <pub-id pub-id-type="doi">10.1002/med.21266</pub-id>
        <?supplied-pmid 22549740?>
        <pub-id pub-id-type="pmid">22549740</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eisenberg</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Marcotte</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Xenarios</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Yeates</surname>
            <given-names>TO</given-names>
          </name>
        </person-group>
        <article-title>Protein function in the post-genomic era</article-title>
        <source>Nature</source>
        <year>2000</year>
        <volume>405</volume>
        <fpage>823</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="doi">10.1038/35015694</pub-id>
        <?supplied-pmid 10866208?>
        <pub-id pub-id-type="pmid">10866208</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Predicting dna-binding proteins: Approached from chou’s pseudo amino acid composition and other specific sequence features</article-title>
        <source>Amino Acids</source>
        <year>2008</year>
        <volume>34</volume>
        <fpage>103</fpage>
        <lpage>109</lpage>
        <pub-id pub-id-type="doi">10.1007/s00726-007-0568-2</pub-id>
        <?supplied-pmid 17624492?>
        <pub-id pub-id-type="pmid">17624492</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chowdhury</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Shatabda</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>idnaprot-es: Identification of dna-binding proteins using evolutionary and structural features</article-title>
        <source>Sci. Rep.</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>14938</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-14945-1</pub-id>
        <?supplied-pmid 29097781?>
        <pub-id pub-id-type="pmid">29097781</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>endna-prot: Identification of dna-binding proteins by applying ensemble learning</article-title>
        <source>BioMed Res. Int.</source>
        <year>2014</year>
        <pub-id pub-id-type="doi">10.1155/2014/294279</pub-id>
        <?supplied-pmid 25580429?>
        <pub-id pub-id-type="pmid">25580429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identifying dna-binding proteins by combining support vector machine and pssm distance transformation</article-title>
        <source>BMC Syst. Biol.</source>
        <year>2015</year>
        <volume>9</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1186/1752-0509-9-S1-S10</pub-id>
        <?supplied-pmid 25582171?>
        <pub-id pub-id-type="pmid">25582171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rahman</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Shatabda</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Saha</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kaykobad</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>Dpp-pseaac: A dna-binding protein prediction model using Chou’s general pseaac</article-title>
        <source>J. Theor. Biol.</source>
        <year>2018</year>
        <volume>452</volume>
        <fpage>22</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jtbi.2018.05.006</pub-id>
        <?supplied-pmid 29753757?>
        <pub-id pub-id-type="pmid">29753757</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hwang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Kuznetsov</surname>
            <given-names>IB</given-names>
          </name>
        </person-group>
        <article-title>Dp-bind: A web server for sequence-based prediction of dna-binding residues in dna-binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>634</fpage>
        <lpage>636</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl672</pub-id>
        <?supplied-pmid 17237068?>
        <pub-id pub-id-type="pmid">17237068</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lou</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sequence based prediction of dna-binding proteins based on hybrid feature selection using random forest and gaussian naive bayes</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e86703</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0086703</pub-id>
        <?supplied-pmid 24475169?>
        <pub-id pub-id-type="pmid">24475169</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Improved detection of dna-binding proteins via compression technology on pssm information</article-title>
        <source>PLoS ONE</source>
        <year>2017</year>
        <volume>12</volume>
        <fpage>e0185587</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0185587</pub-id>
        <?supplied-pmid 28961273?>
        <pub-id pub-id-type="pmid">28961273</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>idna-prot| dis: Identifying dna-binding proteins by incorporating amino acid distance-pairs and reduced alphabet profile into the general pseudo amino acid composition</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e106691</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0106691</pub-id>
        <?supplied-pmid 25184541?>
        <pub-id pub-id-type="pmid">25184541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>X-W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X-T</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Z-Q</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>M-H</given-names>
          </name>
        </person-group>
        <article-title>Identify dna-binding proteins with optimal chou’s amino acid composition</article-title>
        <source>Protein Peptid. Lett.</source>
        <year>2012</year>
        <volume>19</volume>
        <fpage>398</fpage>
        <lpage>405</lpage>
        <pub-id pub-id-type="doi">10.2174/092986612799789404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ahmad</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gromiha</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Sarai</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Analysis and prediction of dna-binding proteins and their binding residues based on composition, sequence and structural information</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <fpage>477</fpage>
        <lpage>486</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg432</pub-id>
        <?supplied-pmid 14990443?>
        <pub-id pub-id-type="pmid">14990443</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of dna-binding proteins by incorporating evolutionary information into pseudo amino acid composition via the top-n-gram approach</article-title>
        <source>J. Biomol. Struct. Dyn.</source>
        <year>2015</year>
        <volume>33</volume>
        <fpage>1720</fpage>
        <lpage>1730</lpage>
        <pub-id pub-id-type="doi">10.1080/07391102.2014.968624</pub-id>
        <?supplied-pmid 25252709?>
        <pub-id pub-id-type="pmid">25252709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>W-Z</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>J-A</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>idna-prot: Identification of dna binding proteins using random forest with grey model</article-title>
        <source>PLoS ONE</source>
        <year>2011</year>
        <volume>6</volume>
        <fpage>e24756</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0024756</pub-id>
        <?supplied-pmid 21935457?>
        <pub-id pub-id-type="pmid">21935457</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Psedna-pro: Dna-binding protein identification by combining chou’s pseaac and physicochemical distance transformation</article-title>
        <source>Mol. Inf.</source>
        <year>2015</year>
        <volume>34</volume>
        <fpage>8</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1002/minf.201400025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Dna binding protein identification by combining pseudo amino acid composition and profile-based protein representation</article-title>
        <source>Sci. Rep.</source>
        <year>2015</year>
        <volume>5</volume>
        <fpage>15479</fpage>
        <pub-id pub-id-type="doi">10.1038/srep15479</pub-id>
        <?supplied-pmid 26482832?>
        <pub-id pub-id-type="pmid">26482832</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Local-dpp: An improved dna-binding protein prediction method by exploring local evolutionary information</article-title>
        <source>Inf. Sci.</source>
        <year>2017</year>
        <volume>384</volume>
        <fpage>135</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2016.06.026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Psfm-dbt: Identifying dna-binding proteins by combing position specific frequency matrix and distance-bigram transformation</article-title>
        <source>Int. J. Mol. Sci.</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>1856</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms18091856</pub-id>
        <?supplied-pmid 28841194?>
        <pub-id pub-id-type="pmid">28841194</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zaman</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hmmbinder: Dna-binding protein prediction using hmm profile based features</article-title>
        <source>BioMed Res. Int.</source>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.1155/2017/4590609</pub-id>
        <?supplied-pmid 29270430?>
        <pub-id pub-id-type="pmid">29270430</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Identification of dna-binding proteins using mixed feature representation methods</article-title>
        <source>Molecules</source>
        <year>2017</year>
        <volume>22</volume>
        <fpage>1602</fpage>
        <pub-id pub-id-type="doi">10.3390/molecules22101602</pub-id>
        <?supplied-pmid 28937647?>
        <pub-id pub-id-type="pmid">28937647</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X-G</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y-H</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>D-J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G-J</given-names>
          </name>
        </person-group>
        <article-title>Targetdbp: Accurate dna-binding protein prediction via sequence-based multi-view feature learning</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2019</year>
        <volume>17</volume>
        <fpage>1419</fpage>
        <lpage>1429</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2019.2893634</pub-id>
        <?supplied-pmid 30668479?>
        <pub-id pub-id-type="pmid">30668479</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Skolnick</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Dbd-hunter: A knowledge-based method for the prediction of dna–protein interactions</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2008</year>
        <volume>36</volume>
        <fpage>3978</fpage>
        <lpage>3992</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkn332</pub-id>
        <?supplied-pmid 18515839?>
        <pub-id pub-id-type="pmid">18515839</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nimrod</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Schushan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Szilágyi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Leslie</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ben-Tal</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>idbps: A web server for the identification of dna binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <fpage>692</fpage>
        <lpage>693</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq019</pub-id>
        <?supplied-pmid 20089514?>
        <pub-id pub-id-type="pmid">20089514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Predicting dna-binding proteins and binding residues by complex structure prediction and application to human proteome</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e96694</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0096694</pub-id>
        <?supplied-pmid 24792350?>
        <pub-id pub-id-type="pmid">24792350</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The i-tasser suite: Protein structure and function prediction</article-title>
        <source>Nat. Methods</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>7</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3213</pub-id>
        <?supplied-pmid 25549265?>
        <pub-id pub-id-type="pmid">25549265</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nanni</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Brahnam</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Set of approaches based on 3d structure and position specific-scoring matrix for predicting dna-binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <fpage>1844</fpage>
        <lpage>1851</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty912</pub-id>
        <?supplied-pmid 30395157?>
        <pub-id pub-id-type="pmid">30395157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sang</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hmmpred: Accurate prediction of dna-binding proteins based on hmm profiles and xgboost feature selection</article-title>
        <source>Comput. Math. Methods Med.</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1155/2020/1384749</pub-id>
        <?supplied-pmid 33133226?>
        <pub-id pub-id-type="pmid">33133226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>Y-H</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>X-N</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>D-J</given-names>
          </name>
        </person-group>
        <article-title>Dnapred: Accurate identification of dna-binding sites from protein sequence by ensembled hyperplane-distance-based support vector machines</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2019</year>
        <volume>59</volume>
        <fpage>3057</fpage>
        <lpage>3071</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00749</pub-id>
        <?supplied-pmid 30943723?>
        <pub-id pub-id-type="pmid">30943723</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Q</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Stackpdb: Predicting dna-binding proteins based on xgb-rfe feature optimization and stacked ensemble classifier</article-title>
        <source>Appl. Soft Comput.</source>
        <year>2021</year>
        <volume>99</volume>
        <fpage>106921</fpage>
        <pub-id pub-id-type="doi">10.1016/j.asoc.2020.106921</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rose</surname>
            <given-names>PW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The rcsb protein data bank: Views of structural biology for basic and applied research and education</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2015</year>
        <volume>43</volume>
        <fpage>D345</fpage>
        <lpage>D356</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku1214</pub-id>
        <?supplied-pmid 25428375?>
        <pub-id pub-id-type="pmid">25428375</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Pennington, J., Socher, R. &amp; Manning, C. D. Glove: Global vectors for word representation. in <italic>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, 1532–1543 (2014).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Distributed representations of words and phrases and their compositionality</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2013</year>
        <volume>26</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bojanowski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Grave</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Joulin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Enriching word vectors with subword information</article-title>
        <source>Trans. Assoc. Comput. Ling.</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>135</fpage>
        <lpage>146</lpage>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Joulin, A. <italic>et al.</italic> Fasttext.zip: Compressing text classification models. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1612.03651">http://arxiv.org/abs/1612.03651</ext-link>10.48550/arXiv.1612.03651 (2016).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Joulin, A., Grave, E., Bojanowski, P. &amp; Mikolov, T. Bag of tricks for efficient text classification. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.01759">http://arxiv.org/abs/1607.01759</ext-link>10.48550/arXiv.1607.01759 (2016).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support-vector networks</article-title>
        <source>Mach. Learn.</source>
        <year>1995</year>
        <volume>20</volume>
        <fpage>273</fpage>
        <lpage>297</lpage>
        <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. in <italic>Proceedings of the 22nd ACM sigkdd International Conference on Knowledge Discovery and Data Mining</italic>, 785–794. 10.1145/2939672.2939785 (2016).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lundberg</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S-I</given-names>
          </name>
        </person-group>
        <article-title>A unified approach to interpreting model predictions</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1705.07874</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parsa</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Movahedi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taghipour</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Derrible</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mohammadian</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Toward safer highways, application of xgboost and shap for real-time accident detection and feature analysis</article-title>
        <source>Accid. Anal. Prev.</source>
        <year>2020</year>
        <volume>136</volume>
        <fpage>105405</fpage>
        <pub-id pub-id-type="doi">10.1016/j.aap.2019.105405</pub-id>
        <?supplied-pmid 31864931?>
        <pub-id pub-id-type="pmid">31864931</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ke</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Lightgbm: A highly efficient gradient boosting decision tree</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grinblat</surname>
            <given-names>GL</given-names>
          </name>
          <name>
            <surname>Uzal</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Larese</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Granitto</surname>
            <given-names>PM</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for plant identification using vein morphological patterns</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2016</year>
        <volume>127</volume>
        <fpage>418</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2016.07.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yen</surname>
            <given-names>S-J</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Y-S</given-names>
          </name>
        </person-group>
        <article-title>Cluster-based under-sampling approaches for imbalanced data distributions</article-title>
        <source>Expert Syst. Appl.</source>
        <year>2009</year>
        <volume>36</volume>
        <fpage>5718</fpage>
        <lpage>5727</lpage>
        <pub-id pub-id-type="doi">10.1016/j.eswa.2008.06.108</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wiatowski</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bölcskei</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>A mathematical theory of deep convolutional neural networks for feature extraction</article-title>
        <source>IEEE Trans. Inf. Theor.</source>
        <year>2017</year>
        <volume>64</volume>
        <fpage>1845</fpage>
        <lpage>1866</lpage>
        <pub-id pub-id-type="doi">10.1109/TIT.2017.2776228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hunter</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>Matplotlib: A 2d graphics environment</article-title>
        <source>Comput. Sci. Eng.</source>
        <year>2007</year>
        <volume>9</volume>
        <fpage>90</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waskom</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Seaborn: Statistical data visualization</article-title>
        <source>J. Open Source Softw.</source>
        <year>2021</year>
        <volume>6</volume>
        <fpage>3021</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lumley</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Interactive Visualization of Climate Change: Characteristics, Intentions, and Metrics for Success</source>
        <year>2021</year>
        <publisher-name>McGill University</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guyon</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Barnhill</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Gene selection for cancer classification using support vector machines</article-title>
        <source>Mach. Learn.</source>
        <year>2002</year>
        <volume>46</volume>
        <fpage>389</fpage>
        <lpage>422</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regression shrinkage and selection via the lasso</article-title>
        <source>J. R. Stat. Soc. B</source>
        <year>1996</year>
        <volume>58</volume>
        <fpage>267</fpage>
        <lpage>288</lpage>
        <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A convolutional neural network system to discriminate drug-target interactions</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2019</year>
        <volume>18</volume>
        <fpage>1315</fpage>
        <lpage>1324</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2019.2940187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Du</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Deep multi-label joint learning for rna and dna-binding proteins prediction</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2022</year>
        <volume>20</volume>
        <fpage>307</fpage>
        <lpage>320</lpage>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>idrbp\_mmc: Identifying dna-binding proteins and rna-binding proteins based on multi-label learning model and motif-based convolutional neural network</article-title>
        <source>J. Mol. Biol.</source>
        <year>2020</year>
        <volume>432</volume>
        <fpage>5860</fpage>
        <lpage>5875</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2020.09.008</pub-id>
        <?supplied-pmid 32920048?>
        <pub-id pub-id-type="pmid">32920048</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Investigating cardiotoxicity related with herg channel blockers using molecular fingerprints and graph attention mechanism</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>153</volume>
        <fpage>106464</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.106464</pub-id>
        <?supplied-pmid 36584603?>
        <pub-id pub-id-type="pmid">36584603</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>A deep learning method for predicting metabolite-disease associations via graph neural network</article-title>
        <source>Brief. Bioinform.</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>266</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbac266</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting metabolite-disease associations based on auto-encoder and non-negative matrix factorization</article-title>
        <source>Brief. Bioinform.</source>
        <year>2023</year>
        <volume>24</volume>
        <fpage>259</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbad259</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene function and cell surface protein association analysis based on single-cell multiomics data</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>157</volume>
        <fpage>106733</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.106733</pub-id>
        <?supplied-pmid 36924730?>
        <pub-id pub-id-type="pmid">36924730</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Shuai</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Predicting the potential human lncrna–mirna interactions based on graph convolution network with conditional random field</article-title>
        <source>Brief. Bioinform.</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>463</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbac463</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Using network distance analysis to predict lncrna–mirna interactions</article-title>
        <source>Interdiscipl. Sci. Comput. Life Sci.</source>
        <year>2021</year>
        <volume>13</volume>
        <fpage>535</fpage>
        <lpage>545</lpage>
        <pub-id pub-id-type="doi">10.1007/s12539-021-00458-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dcamcp: A deep learning model based on capsule network and attention mechanism for molecular carcinogenicity prediction</article-title>
        <source>J. Cell. Mol. Med.</source>
        <year>2023</year>
        <volume>27</volume>
        <fpage>3117</fpage>
        <lpage>3126</lpage>
        <pub-id pub-id-type="doi">10.1111/jcmm.17889</pub-id>
        <?supplied-pmid 37525507?>
        <pub-id pub-id-type="pmid">37525507</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meng</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>scaaga: Single cell data analysis framework using asymmetric autoencoder with gene attention</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>165</volume>
        <fpage>107414</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.107414</pub-id>
        <?supplied-pmid 37660567?>
        <pub-id pub-id-type="pmid">37660567</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Rip1-dependent linear and nonlinear recruitments of caspase-8 and rip3 respectively to necrosome specify distinct cell death outcomes</article-title>
        <source>Protein Cell</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>858</fpage>
        <lpage>876</lpage>
        <pub-id pub-id-type="doi">10.1007/s13238-020-00810-x</pub-id>
        <?supplied-pmid 33389663?>
        <pub-id pub-id-type="pmid">33389663</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shuai</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Quantifying the underlying landscape, entropy production and biological path of the cell fate decision between apoptosis and pyroptosis</article-title>
        <source>Chaos Solitons Fract.</source>
        <year>2024</year>
        <volume>178</volume>
        <fpage>114328</fpage>
        <pub-id pub-id-type="doi">10.1016/j.chaos.2023.114328</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10844231</article-id>
    <article-id pub-id-type="pmid">38316843</article-id>
    <article-id pub-id-type="publisher-id">52653</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-024-52653-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep-WET: a deep learning-based approach for predicting DNA-binding proteins using word embedding techniques with weighted features</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Mahmud</surname>
          <given-names>S. M. Hasan</given-names>
        </name>
        <address>
          <email>hasan.swe@aiub.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Goh</surname>
          <given-names>Kah Ong Michael</given-names>
        </name>
        <address>
          <email>michael.goh@mmu.edu.my</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hosen</surname>
          <given-names>Md. Faruk</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nandi</surname>
          <given-names>Dip</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shoombuatong</surname>
          <given-names>Watshara</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02j8ga255</institution-id><institution-id institution-id-type="GRID">grid.442972.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2218 5390</institution-id><institution>Department of Computer Science, </institution><institution>American International University-Bangladesh (AIUB), </institution></institution-wrap>Kuratoli, Dhaka, 1229 Bangladesh </aff>
      <aff id="Aff2"><label>2</label>Centre for Advanced Machine Learning and Applications (CAMLAs), Dhaka, 1229 Bangladesh </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04zrbnc33</institution-id><institution-id institution-id-type="GRID">grid.411865.f</institution-id><institution-id institution-id-type="ISNI">0000 0000 8610 6308</institution-id><institution>Faculty of Information Science &amp; Technology (FIST), </institution><institution>Multimedia University, Jalan Ayer Keroh Lama, </institution></institution-wrap>75450 Melaka, Malaysia </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00gvj4587</institution-id><institution-id institution-id-type="GRID">grid.443019.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 0479 1356</institution-id><institution>Department of Information and Communication Technology, </institution><institution>Mawlana Bhashani Science and Technology University, </institution></institution-wrap>Santosh, Tangail, 1902 Bangladesh </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01znkr924</institution-id><institution-id institution-id-type="GRID">grid.10223.32</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0490</institution-id><institution>Center for Research Innovation and Biomedical Informatics, Faculty of Medical Technology, </institution><institution>Mahidol University, </institution></institution-wrap>Bangkok, 10700 Thailand </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>2961</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>1</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">DNA-binding proteins (DBPs) play a significant role in all phases of genetic processes, including DNA recombination, repair, and modification. They are often utilized in drug discovery as fundamental elements of steroids, antibiotics, and anticancer drugs. Predicting them poses the most challenging task in proteomics research. Conventional experimental methods for DBP identification are costly and sometimes biased toward prediction. Therefore, developing powerful computational methods that can accurately and rapidly identify DBPs from sequence information is an urgent need. In this study, we propose a novel deep learning-based method called Deep-WET to accurately identify DBPs from primary sequence information. In Deep-WET, we employed three powerful feature encoding schemes containing Global Vectors, Word2Vec, and fastText to encode the protein sequence. Subsequently, these three features were sequentially combined and weighted using the weights obtained from the elements learned through the differential evolution (DE) algorithm. To enhance the predictive performance of Deep-WET, we applied the SHapley Additive exPlanations approach to remove irrelevant features. Finally, the optimal feature subset was input into convolutional neural networks to construct the Deep-WET predictor. Both cross-validation and independent tests indicated that Deep-WET achieved superior predictive performance compared to conventional machine learning classifiers. In addition, in extensive independent test, Deep-WET was effective and outperformed than several state-of-the-art methods for DBP prediction, with accuracy of 78.08%, MCC of 0.559, and AUC of 0.805. This superior performance shows that Deep-WET has a tremendous predictive capacity to predict DBPs. The web server of Deep-WET and curated datasets in this study are available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/">https://deepwet-dna.monarcatechnical.com/</ext-link>. The proposed Deep-WET is anticipated to serve the community-wide effort for large-scale identification of potential DBPs.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Computational models</kwd>
      <kwd>Biological techniques</kwd>
      <kwd>Computational biology and bioinformatics</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100012024</institution-id>
            <institution>Multimedia University</institution>
          </institution-wrap>
        </funding-source>
        <award-id>IR Fund (Project ID MMUI/220041)</award-id>
        <principal-award-recipient>
          <name>
            <surname>Mahmud</surname>
            <given-names>S. M. Hasan</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Nature Limited 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">DNA-binding Proteins (DBPs) participate in many essential biological processes, including DNA replication, gene regulation, repair, and modification<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Identification of DBPs is fundamentally important for understanding characterizations of protein function and drug design. A number of large-scale proteomics experiments have been performed to identify DBPs based on biochemical methods, such as X-ray crystallography<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> and fast ChIP<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. Despite the increasing number of experimentally determined DBPs, the underlying mechanism of DBP specificity remains mostly unidentified, and these approaches are laborious, time-consuming, and sometimes biased toward prediction in the post-genome era, when large numbers of unannotated DBPs are rapidly being sequenced and deposited. As an alternative, computational methods are accurate and cost-effective and can be used to complement the experimental efforts.</p>
    <p id="Par3">To date, several computational algorithms, including machine-learning (ML)-based and template-based methods, have been developed for in silico prediction of DBPs<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>. DBPs can be predicted based on two types of protein data input: i sequence-driven (e.g., iDNA-Prot<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, PseDNA-Pro<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, iDNAPro-PseAAC<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, iDNA-Prot|dis<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, Local-DPP<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, PSFM-DBT<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, IKP-DBPPred<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, iDNAProt-ES<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, DPPPseAAC<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>) and 3D-structure-driven (e.g., DBD-Hunter<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, iDBPs<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, and SPOT-Seq (DNA)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>) methods. Only protein sequence data is required for sequence-driven techniques. The 3D-structure-driven techniques require native or projected 3D structure data from the query protein. In this case, 3D-structure-driven techniques cannot function correctly without 3D structure information. This method performs better when the protein’s native structure is known. On the other hand, sequence-driven techniques do not have this problem. Furthermore, due to the inherent challenges of measuring protein 3D structures in experimental studies, there is a significant gap between the quantities of sequences and 3D structures<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, which is currently expanding quickly in the postgenomic era. Recently, PSI-BLAST was utilized by Chowdhury et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> to derive polysaccharide storage myopathy, which revealed evolutionary information to predict DBP. The secondary structure information of the protein sequences was extracted using SPIDER2. To retrieve protein sequence information, Nanni et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> utilized AAC and quasi residue couple (QRC). Meanwhile, the autocovariance method was used to derive physicochemical characteristics. In addition, evolutionary data was retrieved using the pseudo-position specific scoring matrix (PsePSSM), N-gram features (NGR), and texture descriptors (TD). Sang et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> calculated the HMM matrix for each sequence using the hidden Markov model (HMM). The HMM matrix was converted into feature vectors of the same length using AAC, autocovariance transformation (ACT), and cross-covariance transformation (CCT). Thus, designing sequence-driven computational strategies is essential to accurate prediction of DBPs.</p>
    <p id="Par4">Choosing appropriate feature extraction methods and classification algorithms in order to select the best subset of features is a key factor for the successful discovery of DBPs. In TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, four single-view features (AAC, PsePSSM, PsePRSA, and PsePPDBS) are used to extract the DNA-binding features and apply a learning-based technique to the weights of features to combine them for training an SVM classifier. In addition, an excellent feature subset was selected using SVM-REF + CBR from the non-redundant benchmark and new gold-standard dataset. Rahman et al.<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> utilized the same feature selection (REF) and classifier (SVM) to develop a model DPP-PseAAC for which the authors focused on Chou’s general PseAAC for generating features. Another proposed DBP predictor method is DNAPred<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, where authors use the E-HDSVM algorithm, which includes HD-US and EAdaBoost, to predict protein DNA binding sites. A similar ensemble-based method performed by Zhang et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, XGB-RFE, is used to attain effective features, after which the best features are fed to the stacked ensemble classifier (the combined form of LightGBM, XGBoost, and SVM) to build the proposed StackPDB model.</p>
    <p id="Par5">The above-mentioned algorithms have proven to be exemplary, but we opted to use convolutional neural networks (CNNs) to improve prediction performance. In the meanwhile, it is important to devise an appropriate encoding scheme to represent the sequence fragments surrounding DBPs/non-DBPs to develop a ML-based predictor. In this study, we present a new convolutional neural network (CNN)-based predictor called Deep-WET for accurately identifying DBPs from primary sequence information. Firstly, we applied three consecutive sequence encoding approaches, namely Global Vectors (GloVe), Word2Vec, and fastText, to extract the protein sequence patterns. Secondly, the DE is utilized to acquire the weights for three base features. With these obtained weights, we combined three base features in a weighted manner to create the super feature. In order to improve the predictive performance of Deep-WET, we employed SHapley Additive exPlanations (SHAP) approach to remove irrelevant features from super features and then inputted the optimal one into CNN algorithm for the final model construction. Experimental results demonstrated that Deep-WET achieved a accurate and robust performance as compared with conventional ML classifiers on both the training and independent test datasets. Moreover, comparative analysis on the independent test dataset showed that Deep-WET achieved improved performance compared with the existing approaches, highlight the effectiveness and robustness of the proposed Deep-WET. We also conducted a series of computational analyses to provide in-depth understanding of the DBPs. Finally, the proposed method, Deep-WET, was implemented as a user-friendly web server: at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/">https://deepwet-dna.monarcatechnical.com/</ext-link>.</p>
  </sec>
  <sec id="Sec2">
    <title>Materials and methods</title>
    <sec id="Sec3">
      <title>The overall framework of Deep-WET</title>
      <p id="Par6">The construction process of Deep-WET is depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Deep-WET consists of multiple steps, including data preparation, natural language processing (NLP)-based feature encoding, weighted features, optimal feature subset selection, best classifier selection, and final prediction. In the first stage, three NLP-based Word embedding feature encoding techniques were employed (GloVe, Word2Vec, and fastText), and then the optimal subset of features was selected using the SHAP technique from the weighted features. The selected feature subsets from each feature encoding were fed to four ML and one DL algorithms to build the final prediction models using the training and independent test datasets. Finally, the classifier having the highest cross-validation AUC was considered to construct the final predictor herein.<fig id="Fig1"><label>Figure 1</label><caption><p>The flowchart illustrates our proposed methodology. The upper part represents data pre-processing, the middle part depicts feature extraction with various classifiers, and the lower part showcases classification using the CNN model.</p></caption><graphic xlink:href="41598_2024_52653_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Data preparation</title>
      <p id="Par7">Developing a reliable, comprehensive, and stringent dataset is the first important step of statistical predictor development. Here, the curated dataset denoted with S was presented as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} S = S_{posi} \cup S_{nega} \end{aligned}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where, <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq1.gif"/></alternatives></inline-formula> denotes the positive subset containing DBPs or positive samples, while, <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq2.gif"/></alternatives></inline-formula> denotes the negative subset containing non-DBPs or negative samples, and <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cup$$\end{document}</tex-math><mml:math id="M8"><mml:mo>∪</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq3.gif"/></alternatives></inline-formula> symbol resembles the union of the following sets. The <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq5.gif"/></alternatives></inline-formula> datasets were collected and primarily used by Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, who collected both DBP and non-DBP chains from PDB Data Bank. There are two main reasons why we used the dataset established by Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> as follows. Firstly, this dataset applied a lower CD-HIT<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> threshold of 0.25 to exclude the redundant protein chains. Secondly, this dataset exclude the protein chain sequences having below 50 residues and unknown residues. For the the <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq7.gif"/></alternatives></inline-formula> datasets, they were randomly selected to create the training and independent test datasets. The training dataset consists of 1052 DBPs and 1052 non-DBPs, while The independent test dataset consists of 148 DBPs and 148 non-DBPs. More details on the training and independent test datasets are provided in an article of Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.</p>
    </sec>
    <sec id="Sec5">
      <title>Feature encodings</title>
      <p id="Par8">Word embedding (WE), in which the vocabulary of words can be represented as vectors using large text as an input, is the most popular technique in the area of natural language processing (NLP). WE techniques are able to convert amino acids in a fixed-length vector, where a user needs to define the fixed feature dimensions that can provide adequate prediction results. In this study, we implemented three unsupervised embedding techniques to encode protein sequences: GloVe<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, Word2Vec<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, and fastText<sup><xref ref-type="bibr" rid="CR37">37</xref>–<xref ref-type="bibr" rid="CR39">39</xref></sup>.</p>
      <sec id="Sec6">
        <title>Word2Vec</title>
        <p id="Par9">Word2Vec, a model developed by Tomas Mikolav at Google, computes and generates high-quality, distributed, and continuous dense representations of words<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. These are unsupervised models that can take in massive textual corpora, create a vocabulary of possible word combinations, and generate dense word embeddings on the vector space. The size of the vocabulary determines the size of the word embedding vectors. This decreases the dimensionality of the following dense vector, compared to high-dimensional sparse vector generation using the traditional bag of words (BOW). To construct word embedding, Word2Vec employs two different methods: (1) common bag of words (CBOW) and (2) the Skip-gram model. Notably, the CBOW is faster than the Skip-gram model and generates a better representation of more frequent words<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. On the other hand, the Skip-gram model performs well with a relatively small amount of data and generates a better representation of rare words<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p>
        <p id="Par10">Finding the target word <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_t$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq8.gif"/></alternatives></inline-formula> through n predictions using the CBOW model can be accomplished by the following equation:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} J_{\phi } = \frac{1}{T}\sum _{t=1}^{T}logP(w_t \big | w_{(t-n)}, \ldots ,w_{(t-1)},w_{(t+1)}, \ldots ,w_{(t+n)}) \end{aligned}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par11">Here, <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{(t-1)}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq9.gif"/></alternatives></inline-formula> to <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{(t+n)}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq10.gif"/></alternatives></inline-formula> sequence of words represents the context words. The following equation can further simplify the above equation since the hidden layer can be equivalent to a softmax layer:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} P(w_t \big | w_{(t-n)}, \ldots ,w_{(t-1)},w_{(t+1)}, \ldots ,w_{(t+n)}) = \frac{exp(W_k^Th_t)}{\sum _{k=1}^{v} exp(W_k^Th_t)} \end{aligned}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par12">Here, the output weight matrix between hidden layers is denoted as <italic>W</italic>, and after matrix operation, the average value of input vectors is represented as <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_t$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq11.gif"/></alternatives></inline-formula>.</p>
      </sec>
      <sec id="Sec7">
        <title>GloVe</title>
        <p id="Par13">GloVe is an unsupervised learning vectorization technique. It is a log-bilinear regression model that incorporates both local statistics and global statistics<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. The training of this model is performed on non-zero entries of global word-to-word co-occurrence statistics that tabulates how frequently words are co-occurring within a given corpus. For collecting statistics, the following matrix needs a single pass through the entire corpus. These passes can be expensive for large corpora. Moreover, its resulting representations show the interesting linear substructures of those word vector spaces.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \sum _{i,j}^{N}f(X_{ij})(v_i^Tv_j + b_i + b_j - log(X_{ij}))^2 \end{aligned}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par14">Here, <inline-formula id="IEq12"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_i, v_j$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq12.gif"/></alternatives></inline-formula> correspond to the word embedding of <italic>i</italic>, <italic>j</italic>; <italic>X</italic> represents the word-to-word co-occurrence matrix; and <inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i^{th}$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq13.gif"/></alternatives></inline-formula> number of co-occurrences of word <italic>j</italic> is denoted by <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{ij}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq14.gif"/></alternatives></inline-formula>. Furthermore, the probability of word <italic>j</italic> occurring in the context <italic>i</italic> is the following:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} X_{ij} = P(i) = \frac{X_{ij}}{X_i} \end{aligned}$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec8">
        <title>fastText</title>
        <p id="Par15">fastText, proposed by Facebook<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, is an extension of Word2Vec. It provides tools to learn word representation and sentence classifications of ML. Word vectors are a more organized, numerical, and efficient representation of words and sentences. fastText provides a supervised module to build a model for text classifications. It technique breaks an individual word into a bag of n-grams or sub-words and feeds them into the network, which also generates vector representation for rare or unseen words<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Since the technique uses the same architecture as Word2Vec, the following equation minimizes the loss of softmax layer, <italic>l</italic> over <italic>N</italic> sequences using CBOW model:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \sum _{n=1}^{n} l(y_n, BAx_n) \end{aligned}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par16">Here, <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_n$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq15.gif"/></alternatives></inline-formula> represents the bag of one-hot encoded vectors and <inline-formula id="IEq16"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_n$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq16.gif"/></alternatives></inline-formula> represents the label of the nth sequence of words. The purpose of using FastText in the present study is to find the partial information single DNA sequence order.</p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Weight learning for weighted features</title>
      <p id="Par17">Single-view features represent the discriminative information for each sequence, but combing single-view features to make a weighted feature is critical in ML-based DBP prediction. The most common technique involves serially adding (’+’) single features. However, this straightforward combination technique lacks a guarantee to represent discriminative capability and may overlook the relative importance of the base sequence. To address this issue, we employ a differential evolution (DE) method to determine the optimal weights for each feature. DE algorithm variants of evolutionary algorithms and applied in various works<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup> to show the positive effect. The process we followed for DE algorithm to learn feature weights from a single feature is illustrated as follows: <def-list><def-item><term><bold>Step 1:</bold></term><def><p id="Par18"><bold>Initialization</bold> Randomly create an initial population <inline-formula id="IEq17"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_o = \{FW_1^g, FW_2^g, FW_3^g \ldots \, \ldots \,FW_n^g\}, where (FW_{i,1}^g, FW_{i,2}^g, FW_{i,3}^g, FW_{i,4}^g)^T$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>3</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq17.gif"/></alternatives></inline-formula> represents <italic>i</italic>th number solution in the population <italic>g</italic>th. <italic>N</italic> means size of the generation population where to set the maximum generation <inline-formula id="IEq20"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_{max}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq20.gif"/></alternatives></inline-formula>, crossover rate (<italic>CR</italic>), scaling factor (<italic>F</italic>) to 1000, 0.5, and 0.5, respectively.</p></def></def-item><def-item><term><bold>Step 2:</bold></term><def><p id="Par19"><bold>Mutation</bold> A mutation vector <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MV_i^g$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi>M</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq21.gif"/></alternatives></inline-formula> was initialized for each salutation according. <disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MV_i^g = FW_{r1}^g + F.(FW_{r2}^g - FW_{r3}^g) \end{aligned}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p></def></def-item><def-item><term><bold>Step 3:</bold></term><def><p id="Par20"><bold>Crossover</bold> For the diversity of each solution, a trial vector <inline-formula id="IEq22"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TV_i^g = (TV_{i,1}^g, TV_{i,2}^g, TV_{i,3}^g \ldots \, \ldots \,TV_{i,D}^g)$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq22.gif"/></alternatives></inline-formula> of crossover is established in the DE technique as follows: <disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} TV_{i, j}^g = {\left\{ \begin{array}{ll} &amp;{} \text {TV}_{i,j}^g \quad if \; R_j \le CR \;\;ot \;j = j_r\\ &amp;{} \,\,\text {FW}_{i,j}^g \qquad \,\,\text {otherwise}\\ \end{array}\right. } \end{aligned}$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msubsup><mml:mtext>TV</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.277778em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.277778em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msubsup><mml:mtext>FW</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mspace width="2em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula> where <inline-formula id="IEq23"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j = 1,2,3 \ldots ,D, j_r$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq23.gif"/></alternatives></inline-formula> represent randomly produced integer with [1, <italic>D</italic>]; <inline-formula id="IEq24"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_j$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq24.gif"/></alternatives></inline-formula> means uniformly distributed range [0,1] and <inline-formula id="IEq25"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CR \in (0,1)$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq25.gif"/></alternatives></inline-formula> indicate crossover rate.</p></def></def-item><def-item><term><bold>Step 4:</bold></term><def><p id="Par21"><bold>Selection</bold> Find the better vector from trial <inline-formula id="IEq26"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TV_i^g$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq26.gif"/></alternatives></inline-formula> and target <inline-formula id="IEq27"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FW_i^g$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq27.gif"/></alternatives></inline-formula> using the following way: <disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} FW_i^{g+1} = {\left\{ \begin{array}{ll} &amp;{} TV_{i}^g, \quad \text {if} \; f(TV_i^{g}) \le f(FW_i^g)\\ &amp;{} \,\,\text {FW}_{i}^g, \qquad \,\,\text {otherwise}\\ \end{array}\right. } \end{aligned}$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.277778em"/><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msubsup><mml:mtext>FW</mml:mtext><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p></def></def-item><def-item><term><bold>Step 5:</bold></term><def><p id="Par22"><bold>Termination</bold><inline-formula id="IEq28"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g=g+1$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq28.gif"/></alternatives></inline-formula> and repeat steps 2 to 4 until <italic>g</italic> is greater the <inline-formula id="IEq29"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_{max}.$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq29.gif"/></alternatives></inline-formula></p></def></def-item></def-list></p>
      <p id="Par23">After concluding the DE procedure, we can get the final results. In this study, We have generated a novel super feature, represented as GloVe + fastText + Word2Vec, by the weighted and sequential fusion of GloVe, fastText, and Word2Vec features. DE is a powerful optimization algorithm; however, using it for feature weighting in ML presents certain limitations and challenges. DE may struggle with slow convergence, susceptibility to local optima, and sensitivity to parameter choices. Additionally, the algorithm may violate constraints, lack robustness across diverse datasets, and exhibit computational intensity. To avoid these challenges, we have performed parameter tuning <inline-formula id="IEq30"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(population-size, \, mutation \, rate, \, crossover \, probabilities)$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.166667em"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq30.gif"/></alternatives></inline-formula> in experiments, considering adaptive strategies for mutation and crossover rates. Furthermore, exploring parallelization methods helps alleviate computational burdens, while strategies like diversity maintenance mechanisms aim to address convergence issues.</p>
    </sec>
    <sec id="Sec10">
      <title>SHAP-based feature selection scheme</title>
      <p id="Par24">SHAP is an additive feature attribution method introduced by Lunberg and Lee<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> in which each individual prediction is interpreted by the contribution of the features and then ordered according to their importance<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. SHAP allocates each feature an importance value for a particular prediction. This SHAP feature selection approach is based on game theory<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>; SHAP values break down a prediction to show the impact of each individual feature. Suppose each feature is <inline-formula id="IEq31"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M76"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq31.gif"/></alternatives></inline-formula>, is replaced by <inline-formula id="IEq32"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_i$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq32.gif"/></alternatives></inline-formula> for determining whether the feature value <inline-formula id="IEq33"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq33.gif"/></alternatives></inline-formula> exists or not. SHAP represents the explanation as:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} g(z) = \phi _o + \sum _{i=1}^{M} \phi _iz_i \end{aligned}$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par25">In the above equation, <italic>g</italic> represents the explanation model; <inline-formula id="IEq34"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z \in {0,1} ^M$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq34.gif"/></alternatives></inline-formula> represents the coalition vector; 0 and 1 indicate that the corresponding feature is absent or present, respectively; the number of input features included in the model is denoted as <italic>M</italic>; and <inline-formula id="IEq35"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi _i \in R, \phi _i$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq35.gif"/></alternatives></inline-formula> represents the feature attribution values for a feature <italic>i</italic>. Considering the game theory concept, Shapley values can be calculated using the following equation:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \phi _i = \sum _{S \subset M \setminus \{i\}} \frac{|S|!(M - |S| -1)!}{M!} [f_x(S \cup \{i\} - f_x(S))] \end{aligned}$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊂</mml:mo><mml:mi>M</mml:mi><mml:mo lspace="0.15em" rspace="0.15em" stretchy="false">\</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par26">In the above equation, <italic>M</italic> represents the set of features in the model; all feature subsets achieved from <italic>M</italic> are represented as <italic>S</italic>; the function computes the total contribution of a given features set <italic>S</italic>; <inline-formula id="IEq36"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S \subset M \setminus \{i\}$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊂</mml:mo><mml:mi>M</mml:mi><mml:mo lspace="0.15em" rspace="0.15em" stretchy="false">\</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq36.gif"/></alternatives></inline-formula> represents the value of the corresponding feature when <italic>i</italic> is known, versus when the corresponding feature value <italic>i</italic> is unknown for all subsets.</p>
      <p id="Par27">One of the important features of the SHAP is the barplot in the form of rectangular horizontal bars, where the length of the bars represents the importance of a given feature. As we need the global significance, we sum the contribution of each feature, or absolute Shapley values.<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I_j = \sum _{i=1}^{n} |\phi _j^{(i)}| \end{aligned}$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par28">Then, we plot each of the features by sorting them in decreasing order. Figure 2A shows the important features based on SHAP contributions for the XGBoost trained before predicting DBPs. The SHAP summary plot gives a high-level composite view that displays the importance of features with feature effects. Each point in the plot represents a SHAP value for a specific feature of an instance. The values that pull the prediction power of the model downwards are on the left, and the values that push the prediction further up are on the right. On the y-axis, the features are placed in descending order, and on the x-axis, there is a scale representing the Shapley value with a vertical line at point zero. The positive and negative values are to the right and left part of that vertical line, respectively. Here the colors separate the relative size of the features between instances. Specifically, low values are colored blue and high values are colored red. Overlapping more data points in the y-axis direction shows the distribution of SHAP values for each individual feature. Moreover, in the summary plot, we clearly observe the relationships between the value of a feature and the effect on the prediction. Figure 2B shows the SHAP summary plot, which orders important features for identifying DBPs.</p>
    </sec>
    <sec id="Sec11">
      <title>Implementation of convolutional neural network</title>
      <p id="Par29">CNNs are a type of deep learning model commonly used in applications including recommender systems, image and video recognition, and natural language processing<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. In CNN architecture, the deeper convolutional layers (CLs) lead to learning high dimension features using sliding convolution kernels on the upper part of previous layers with different hyper-parameter settings such as filters, control layer outputs, stride, and zero-paddings. Pooling layers (PLs) are able to reduce the input feature size and offer translation invariance by local non-linear operations<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Fully connected layers (FCLs) utilized to classify the tasks, consisting of an equal number of output neurons as artificial neural networks.</p>
      <p id="Par30">Each neuron is completely linked to all of the nodes in the preceding and subsequent levels<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. After adding one additional CL and max-PL to the process, the technique demonstrated a significant improvement in terms of computational complexity and program runtime. The following equation may be used to compute the outputs of each convolutional layer:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} y_k^l = f\big ( \sum _{m} W_{m,k}^l y_m^{l-1} + b_k^l \big ) \end{aligned}$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">(</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par31">The layer index is <italic>l</italic>, while the input and output feature maps are <italic>m</italic> and <italic>k</italic>, respectively. Specifically, <inline-formula id="IEq37"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_k^l$$\end{document}</tex-math><mml:math id="M96"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq37.gif"/></alternatives></inline-formula> denotes the <italic>k</italic>th feature map of the <italic>l</italic> layer’s input, while <inline-formula id="IEq39"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_m^{(l-1)}$$\end{document}</tex-math><mml:math id="M98"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq39.gif"/></alternatives></inline-formula> denotes the <inline-formula id="IEq40"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m-$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq40.gif"/></alternatives></inline-formula>th feature map of layer <inline-formula id="IEq41"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l-1$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq41.gif"/></alternatives></inline-formula> output. The weight tensor and bias term, respectively, are <italic>W</italic> and <italic>b</italic>. Back-propagation and adaptive estimating approaches were used to reduce cross-entropy loss<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. Our model’s output layer is essentially a logistic regression classifier. It takes <inline-formula id="IEq42"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_k^l$$\end{document}</tex-math><mml:math id="M104"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq42.gif"/></alternatives></inline-formula> as an input and computes the following:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{y} = f(W^ly^l + b^l) \end{aligned}$$\end{document}</tex-math><mml:math id="M106" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par32">The output <inline-formula id="IEq43"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><mml:math id="M108"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq43.gif"/></alternatives></inline-formula>is the final predicted score; <italic>W</italic> is the weight matrix; <italic>b</italic> is the bias vector. Each output size is 2, denoting positive or negative classes for the binary classification task of DNA binding predictions. In order to discover suitable parameters, we want to minimize cross-entropy loss by adaptive moment estimation and back-propagation techniques:<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} loss = - \frac{1}{N} \sum _{i=1}^{N} y_ilog\hat{y_l} + (1-y_i)log(1-\hat{y_l}) \end{aligned}$$\end{document}</tex-math><mml:math id="M110" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par33">To improve the model’s efficiency, batch normalization and dropout techniques<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> were employed. The dropout in FCLs decreases by a few units during the training phase, whereas batch normalization helps to standardize the inputs into unit standard deviation and zero means. Furthermore, dropout was able to overcome the problem of overfitting, and batch normalization supported the model with sufficient learning ratios.</p>
      <p id="Par34">To achieve a better performance, hyperparameter optimization plays a vital part in the implementation of the proposed methodology. The following hyperparameters are optimized before training the model: learning rate, number of filters, kernel size, batch size, number of hidden layers, optimizers, dropout layers, and activation function. Here, three convolutional layers are used as hidden layers in the CNN model architecture. In addition, 32, 48, 64 filters and kernel sizes of 3, 4, 5 are used. Using ReLu as an activation in the hidden layers and Sigmoid in the fully connected layer results in the desired outcome. Dropout layers with dropout rates of 0.2, 0.3, and 0.5 are used to prevent overfitting. With extensive experimentation, employment of the Adam optimizer with a learning rate of 0.00001 and binary cross-entropy loss function shows the optimal result. Table <xref rid="Tab1" ref-type="table">1</xref> comprehensively illustrates the hyperparameters used in our method. Detailed parameter settings of the other three classifiers for different feature encoding are also listed in Table 6.</p>
    </sec>
    <sec id="Sec12">
      <title>Performance evaluation</title>
      <p id="Par35">The performance of Deep-WET was evaluated in terms of six standard performance metrics for the binary classification problem including accuracy (ACC), sensitivity (Sen),specificity (Spe), Matthew’s coefficient correlation (MCC), and precision (Pre).<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} ACC= &amp; {} \frac{TP + TN}{TN+TP+FN+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M112" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Sen= &amp; {} \frac{TP}{TP + FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M114" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Spe= &amp; {} \frac{TN}{FP+TN} \end{aligned}$$\end{document}</tex-math><mml:math id="M116" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MCC= &amp; {} \frac{(TP\times TN) - (FP\times FN)}{\sqrt{(TP+FP)\times (TP+FN)\times (TN+FP)\times (TN+FN)}} \end{aligned}$$\end{document}</tex-math><mml:math id="M118" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Pre= &amp; {} \frac{TP}{TP+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M120" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1\, score= &amp; {} \frac{2\times (Precision\times Recall)}{Precision + Recall} \end{aligned}$$\end{document}</tex-math><mml:math id="M122" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mspace width="0.166667em"/><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula>where <italic>TP</italic>, <italic>FP</italic>, <italic>TN</italic>,  and <italic>FN</italic> respectively represent the number of true positives (correctly classified positive), false positives (incorrectly classified as positive), true negatives (correctly classified negative), and false negatives (incorrectly classified as negative), respectively. Furthermore, the AUC metric was also used to evaluate the performances of the proposed DeepWET model, where the curve is plotted by TPR (sensitivity) and FPR (1 – specificity) with different threshold settings.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Hyperparameters setting of CNN classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Hyperparameters</th><th align="left">Range</th></tr></thead><tbody><tr><td align="left">Learning rate</td><td align="left">[0.00001, 0.01, 0.001, 0.0001]</td></tr><tr><td align="left">Number of filters</td><td align="left">[32, 48, 64]</td></tr><tr><td align="left">Kernel size</td><td align="left">[3, 4, 5]</td></tr><tr><td align="left">Batch size</td><td align="left">[16, 32, 64, 128]</td></tr><tr><td align="left">Number of hidden layers</td><td align="left">[2, 3]</td></tr><tr><td align="left">Optimizer</td><td align="left">[‘Adam’]</td></tr><tr><td align="left">Dropout rate</td><td align="left">[0.2, 0.3, 0.5]</td></tr><tr><td align="left">Activation function</td><td align="left">[’relu’, ’sigmoid’]</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec13">
      <title>Experimental setup and packages</title>
      <p id="Par36">All tests in this study were carried out on three independent computers with the following settings, using Python version 3.7.7 or above:<list list-type="bullet"><list-item><p id="Par37">A desktop computer with Intel Core i5 CPU @ 2.71GHz x 4, Windows 10, 64-bit OS and 8 GB RAM.</p></list-item><list-item><p id="Par38">A desktop computer with Intel Core i5 CPU @ 2.11GHz x 4, Windows 10, 64-bit OS and 8 GB RAM.</p></list-item><list-item><p id="Par39">A server machine with Intel Core i5-3320M CPU @ 2.60GHz x 4, Ubuntu 18.04.2 LTS, 64-bit OS, 13 MB L3 cache and 64 GB RAM.</p></list-item></list></p>
      <p id="Par40">CNN classifier and SHAP technique were employed for model learning and feature selection on TensorFlow 2.0 and SHAP 0.39.0 Python libraries to implement them. We utilized improved parameter settings of the CNN algorithm such as batch size 16, kernel size 4, 2 hidden layers, and dropout rate 0.5. Several graphs were plotted in this experiment using Matplotlib<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, Seaborn<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, and Plotly<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, in addition to pre-installed Python tools.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Results and discussion</title>
    <sec id="Sec15">
      <title>Performance comparison of different feature encodings</title>
      <p id="Par41">In this section, we systematically evaluated the effect of various feature encodings, including single-feature (GloVe, fastText, and Word2Vec) and weighted-feature (GloVe + fastText, GloVe + Word2Vec, fastText + Word2Vec, and GloVe + fastText + Word2Vec) encodings in DBP identification. These features were inputted to a CNN classifier to evaluate their corresponding models using the 5-fold cross-validation test. The cross-validation performance of variant CNN classifiers trained with different features are provided in Table <xref rid="Tab2" ref-type="table">2</xref> and Fig. <xref rid="Fig3" ref-type="fig">3</xref>A. It is worth noting that the parameters of CNN classifiers were carefully determined to improve their performance under the 5-fold cross-validation process.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance comparison of CNN classifiers trained with different feature encodings on the training dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">GloVe</td><td align="left">0.810</td><td align="left">75.00</td><td align="left">71.15</td><td align="left">77.63</td><td align="left">0.485</td><td align="left">68.52</td><td align="left">0.698</td></tr><tr><td align="left">fastText</td><td align="left">0.785</td><td align="left">73.44</td><td align="left">67.24</td><td align="left">78.57</td><td align="left">0.462</td><td align="left">72.22</td><td align="left">0.696</td></tr><tr><td align="left">Word2Vec</td><td align="left">0.793</td><td align="left">71.09</td><td align="left">73.13</td><td align="left">68.85</td><td align="left">0.420</td><td align="left">72.06</td><td align="left">0.726</td></tr><tr><td align="left">fastText + Word2Vec</td><td align="left">0.826</td><td align="left">75.78</td><td align="left">70.18</td><td align="left">80.28</td><td align="left">0.508</td><td align="left">74.07</td><td align="left">0.721</td></tr><tr><td align="left">GloVe + Word2Vec</td><td align="left">0.820</td><td align="left">76.64</td><td align="left">71.96</td><td align="left">85.00</td><td align="left">0.523</td><td align="left">77.50</td><td align="left">0.713</td></tr><tr><td align="left">GloVe + fastText</td><td align="left">0.839</td><td align="left">78.12</td><td align="left">72.96</td><td align="left">89.19</td><td align="left">0.549</td><td align="left">80.95</td><td align="left">0.738</td></tr><tr><td align="left">GloVe + fastText +Word2Vec</td><td align="left">0.864</td><td align="left">79.07</td><td align="left">75.10</td><td align="left">91.49</td><td align="left">0.585</td><td align="left">86.21</td><td align="left">0.740</td></tr></tbody></table></table-wrap></p>
      <p id="Par42">Among single-based features, GloVe outperformed fastText and Word2Vec in terms of all performance metrics. The AUC, ACC, Sen, Spe, and MCC of GloVe were 0.810, 75.00%, 71.15%, 77.63% and 0.485, respectively. Interestingly, AUC, ACC, and MCC of GloVe were 2.5–1.7%, 1.56–3.91%, and 2.3–6.5% higher than fastText and Word2Vec, respectively. A weighted feature was created by adding different combinations of the single feature extraction methods in order to improve the predictive performance. As can be seen from Table <xref rid="Tab2" ref-type="table">2</xref>, we observe that the performance the combination of GloVe, fastText and Word2Vec is better than those of other three weighted features in terms of all performance metrics. The ACC, Sen, Spe, and MCC of the combination of GloVe, fastText and Word2Vec are 79.07% 64.10%, 91.49% and 0.585, respectively, which are 0.95–3.29%, 2.14–4.92%, 2.30–11.91%, 0.036–0.077%, 5.26–12.14% and 0.002–0.027% higher than other combination features, respectively. Figure <xref rid="Fig3" ref-type="fig">3</xref>A shows that the AUC value of GloVe+fastText+Word2Vec 0.864, which is larger than the other three weighted features. Overall, we observed that the Sen value of individual features was slightly higher than that of the corresponding weighted features in some cases. Moreover, the performance of the top weighted features (GloVe + fastText + Word2Vec) is significantly higher than the single-view feature in terms of all evaluation metrics. Weighteds features archive higher prediction performances to the single-view feature in terms of all evaluation metrics. Therefore, in this study, the GloVe + fastText + Word2Vec feature outperformed other single and weighted features and is considered as the optimal one in termes of computational cost and predictive performance.</p>
    </sec>
    <sec id="Sec16">
      <title>Feature section approaches improve the predictive performance</title>
      <p id="Par43">The original feature subsets extracted from feature encoding techniques might contain noisy and redundant information that can affect the classifiers’ performance. Therefore, we utilized feature selection methods to determine important features from the original feature subsets. Here, three feature selection techniques, including RFE<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, LASSO<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, and SHAP<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, were utilized for determining the important features from GloVe + fastText + Word2Vec feature encoding. In our experiment, we ranked all features using its importance obtained from RFE, LASSO, and SHAP and then established the six feature subsets that consisted of the top-ranked features ranging from top 200 to the top 450 features with an interval of 50. Then, for each feature selection technique, the six feature subsets were fed to develop individual CNN classifiers whose corresponding prediction results based on a 5-fold cross-validation were provided in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance comparison of various feature sets derived from different feature selection techniques.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature selection</th><th align="left">No. of features</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="6">RFE</td><td align="left">200</td><td align="left">0.859</td><td align="left">76.74</td><td align="left">58.82</td><td align="left">88.46</td><td align="left">0.503</td><td align="left">76.92</td><td align="left">0.667</td></tr><tr><td align="left">250</td><td align="left">0.853</td><td align="left">79.69</td><td align="left">65.38</td><td align="left">89.47</td><td align="left">0.574</td><td align="left">80.95</td><td align="left">0.723</td></tr><tr><td align="left">300</td><td align="left">0.866</td><td align="left">82.55</td><td align="left">71.87</td><td align="left">88.89</td><td align="left">0.621</td><td align="left">79.31</td><td align="left">0.754</td></tr><tr><td align="left">350</td><td align="left">0.853</td><td align="left">80.23</td><td align="left">85.11</td><td align="left">74.36</td><td align="left">0.600</td><td align="left">80.00</td><td align="left">0.825</td></tr><tr><td align="left">400</td><td align="left">0.876</td><td align="left">81.40</td><td align="left">67.57</td><td align="left">91.83</td><td align="left">0.622</td><td align="left">86.20</td><td align="left">0.758</td></tr><tr><td align="left">450</td><td align="left">0.866</td><td align="left">79.10</td><td align="left">62.50</td><td align="left">88.89</td><td align="left">0.541</td><td align="left">76.92</td><td align="left">0.690</td></tr><tr><td align="left" rowspan="6">Lasso</td><td align="left">200</td><td align="left">0.837</td><td align="left">77.57</td><td align="left">69.05</td><td align="left">83.07</td><td align="left">0.526</td><td align="left">72.50</td><td align="left">0.707</td></tr><tr><td align="left">250</td><td align="left">0.869</td><td align="left">81.25</td><td align="left">82.86</td><td align="left">79.31</td><td align="left">0.622</td><td align="left">82.86</td><td align="left">0.829</td></tr><tr><td align="left">300</td><td align="left">0.867</td><td align="left">81.35</td><td align="left">87.50</td><td align="left">76.09</td><td align="left">0.581</td><td align="left">76.09</td><td align="left">0.814</td></tr><tr><td align="left">350</td><td align="left">0.856</td><td align="left">79.10</td><td align="left">81.82</td><td align="left">76.19</td><td align="left">0.581</td><td align="left">78.26</td><td align="left">0.800</td></tr><tr><td align="left">400</td><td align="left">0.882</td><td align="left">81.40</td><td align="left">64.71</td><td align="left">92.30</td><td align="left">0.607</td><td align="left">84.61</td><td align="left">0.733</td></tr><tr><td align="left">450</td><td align="left">0.877</td><td align="left">79.68</td><td align="left">65.38</td><td align="left">89.47</td><td align="left">0.574</td><td align="left">80.95</td><td align="left">0.723</td></tr><tr><td align="left" rowspan="6">SHAP</td><td align="left">200</td><td align="left">0.873</td><td align="left">76.56</td><td align="left">60.00</td><td align="left">91.18</td><td align="left">0.544</td><td align="left">85.71</td><td align="left">0.706</td></tr><tr><td align="left">250</td><td align="left">0.866</td><td align="left">80.25</td><td align="left">65.79</td><td align="left">91.67</td><td align="left">0.604</td><td align="left">86.21</td><td align="left">0.746</td></tr><tr><td align="left">300</td><td align="left">0.883</td><td align="left">81.31</td><td align="left">66.66</td><td align="left">91.89</td><td align="left">0.616</td><td align="left">85.70</td><td align="left">0.750</td></tr><tr><td align="left">350</td><td align="left">0.880</td><td align="left">81.40</td><td align="left">68.57</td><td align="left">90.19</td><td align="left">0.611</td><td align="left">82.76</td><td align="left">0.750</td></tr><tr><td align="left">400</td><td align="left"><bold>0.883</bold></td><td align="left"><bold>82.56</bold></td><td align="left"><bold>69.44</bold></td><td align="left"><bold>92.00</bold></td><td align="left"><bold>0.641</bold></td><td align="left"><bold>86.21</bold></td><td align="left"><bold>0.769</bold></td></tr><tr><td align="left">450</td><td align="left">0.863</td><td align="left">80.23</td><td align="left">66.67</td><td align="left">90.00</td><td align="left">0.591</td><td align="left">82.76</td><td align="left">0.739</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par44">As seen in Table <xref rid="Tab3" ref-type="table">3</xref>, the optimal subsets containing top 300, 400, and 400 optimal features derived from the RFE, LASSO and SHAP techniques, respectively, outperformed other feature sets in terms of both ACC and AUC. In the meanwhile, the performance of the optimal subsets from the SHAP technique outperformed than the RFE and LASSO techniques. To be specific, the AUC, ACC, Sen, Spe, MCC, Pre, and F1 of the optimal subset from the SHAP technique were 0.883, 82.56%, 69.44%, 92.00%, 0.641, 86.21, and 0.769, respectively. Thus, the optimal subset derived from the SHAP technique was considered to develop our proposed model. To check the effectiveness of the optimal subset, we compared its performance with the original feature set. As shown in Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab4" ref-type="table">4</xref>, the ACC, Sen, MCC and F1 of the optimal subset were 3.49%, 5.34%, 5.60%, and 3.40% higher than the original feature set. For convenience of discussion, the CNN classifier combined with the optimal subset from the SHAP technique is referred herein as Deep-WET.
</p>
      <p id="Par45">Altogether, the SHAP technique was a powerful approach for implementing DNA binding protein datasets. To make a clear comparison of prediction effects, the results of the SHAP importance bar graph on the GloVe + fastText + Word2Vec dataset for 400 feature dimensions are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>A. In Fig. <xref rid="Fig2" ref-type="fig">2</xref>A, the bar plot generated by SHAP shows the important features in the form of horizontal bars, with length representing the importance of features. We summarized the most significant features by sorting them in decreasing order based on absolute Shapley values. In addition, the SHAP summary plot for 400 feature dimensions is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>B. It represents a high-level composite look that indicates the important features and effects. Each point depicts a SHAP score in the plot for a particular feature instance. Notably, we can observe the relationship between the feature value and the effect on prediction in the SHAP summary plot.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance comparison of CNN classifiers trained with different optimal feature sets on the training dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">GloVe</td><td align="left">0.852</td><td align="left">75.70</td><td align="left">63.46</td><td align="left">87.27</td><td align="left">0.524</td><td align="left">82.50</td><td align="left">0.717</td></tr><tr><td align="left">fastText</td><td align="left">0.826</td><td align="left">77.91</td><td align="left">63.89</td><td align="left">88.00</td><td align="left">0.542</td><td align="left">79.31</td><td align="left">0.708</td></tr><tr><td align="left">Word2Vec</td><td align="left">0.806</td><td align="left">74.22</td><td align="left">66.67</td><td align="left">81.54</td><td align="left">0.488</td><td align="left">77.78</td><td align="left">0.718</td></tr><tr><td align="left">fastText + Word2Vec</td><td align="left">0.857</td><td align="left">77.57</td><td align="left">68.18</td><td align="left">84.13</td><td align="left">0.532</td><td align="left">75.00</td><td align="left">0.714</td></tr><tr><td align="left">GloVe + Word2Vec</td><td align="left">0.856</td><td align="left">79.44</td><td align="left">70.45</td><td align="left">85.71</td><td align="left">0.571</td><td align="left">77.50</td><td align="left">0.738</td></tr><tr><td align="left">GloVe + fastText</td><td align="left">0.850</td><td align="left">80.37</td><td align="left">69.39</td><td align="left">89.66</td><td align="left">0.608</td><td align="left">85.00</td><td align="left">0.764</td></tr><tr><td align="left">GloVe + fastText +Word2Vec</td><td align="left">0.883</td><td align="left">82.56</td><td align="left">69.44</td><td align="left">92.00</td><td align="left">0.641</td><td align="left">86.21</td><td align="left">0.769</td></tr></tbody></table></table-wrap></p>
      <p id="Par46">From the above-mentioned observations and discussion, we concluded that the SHAP technique was a more powerful and effective feature selection one; therefore, this technique was chose for selecting a subset of features for predicting DBPs herein. In addition, we also applied the SHAP technique in other types of features whose corresponding prediction results were summarized in Table <xref rid="Tab4" ref-type="table">4</xref> and Fig. <xref rid="Fig3" ref-type="fig">3</xref>. By comparing the performance of the models without feature selection (Table <xref rid="Tab2" ref-type="table">2</xref> along with Fig. <xref rid="Fig3" ref-type="fig">3</xref>A and C) and with the SHAP-based feature selection (Table <xref rid="Tab4" ref-type="table">4</xref> along with Fig. <xref rid="Fig3" ref-type="fig">3</xref>B and D), the models with the SHAP-based feature selection achieve better performance than those of the models without feature selection.</p>
    </sec>
    <sec id="Sec17">
      <title>Hyperparameter of CNN</title>
      <p id="Par47">The hyperparameter learning rate controls how the model changes according to estimated error each time the weights are updated. Finding the optimal learning rate can be challenging, because a higher learning rate makes the gradient drop faster, and a lower learning rate leads to the gradient hardly converging. Here, the faster rate of gradient drop results in informative and meaningful features failing to get extracted over each iteration, and the lower rate of gradient convergence results in a longer training time. Therefore, five learning rates in a range of 1e–6 to 1e–2 are implemented for the proposed model to find the optimal performance. From Table <xref rid="Tab5" ref-type="table">5</xref>, the learning rate of 0.00001 gives the highest performance compared to implementing the remaining four learning rates. However, the sensitivity value of using the 0.00001 learning rate is suboptimal compared to the value of the 0.0001 and 0.001 learning rates.<fig id="Fig2"><label>Figure 2</label><caption><p>The SHAP importance bar graph results for the GloVe + fastText + Word2Vec dataset with 400 feature dimensions are presented. (<bold>A</bold>) bar plot generated by SHAP shows the important features in the form of horizontal bars (<bold>B</bold>) SHAP summary plot for the 400 feature dimensions.</p></caption><graphic xlink:href="41598_2024_52653_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Figure 3</label><caption><p>ROC curves and AUPR curves of CNN classifiers are depicted for both single and hybrid feature spaces without feature selection (<bold>A</bold>, <bold>C</bold>) and with SHAP-based feature selection (<bold>B</bold>, <bold>D</bold>).</p></caption><graphic xlink:href="41598_2024_52653_Fig3_HTML" id="MO3"/></fig><table-wrap id="Tab5"><label>Table 5</label><caption><p>Cross-validation results of CNN classifiers trained with different learning rates.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Learning rate</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">1e–2</td><td align="left">0.835</td><td align="left">79.06</td><td align="left">64.29</td><td align="left">86.21</td><td align="left">0.515</td><td align="left">69.23</td><td align="left">0.667</td></tr><tr><td align="left">1e–3</td><td align="left">0.803</td><td align="left">81.39</td><td align="left"><bold>72.73</bold></td><td align="left">84.38</td><td align="left">0.543</td><td align="left">61.54</td><td align="left">0.667</td></tr><tr><td align="left">1e–4</td><td align="left">0.838</td><td align="left">81.25</td><td align="left">69.57</td><td align="left">87.80</td><td align="left">0.586</td><td align="left">76.19</td><td align="left">0.727</td></tr><tr><td align="left">1e–5</td><td align="left">0.883</td><td align="left">82.56</td><td align="left">69.44</td><td align="left">92.00</td><td align="left">0.641</td><td align="left">86.21</td><td align="left">0.769</td></tr><tr><td align="left">1e–6</td><td align="left">0.874</td><td align="left">79.07</td><td align="left">61.11</td><td align="left">92.00</td><td align="left">0.571</td><td align="left">84.62</td><td align="left">0.710</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec18">
      <title>Comparison of Deep-WET with conventional ML classifiers</title>
      <p id="Par48">To evaluate the performance of the proposed Deep-WET, we compared its predictive performance with conventional ML classifiers. Herein, the conventional ML classifiers were built using four well-known ML classifiers (i.e., SVM<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, XGBoost<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, LightGBM<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, and CNN<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>) and the three NLP-based word embedding techniques (i.e., GloVe, fastText, and Word2Vec). In total, 11 conventional ML classifiers were created in this study. It is noteworthy that the parameters of all ML classifiers were carefully optimized to improve their prediction capability under a 5-fold cross-validation procedure. In these experiments, classifiers have been trained a total of 24 times. The prediction performance based on both 5-fold cross-validation and independent tests are listed in Tables 7-8. In addition, their respective graphs are shown in Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>Performance comparison of three feature groups for SVM, XGBoost, LightGBM, and CNN classifiers under 5-fold cross-validation test on various evaluation metrics: (<bold>A</bold>) GloVE, (<bold>B</bold>) Word2Vec, and (<bold>C</bold>) fastText.</p></caption><graphic xlink:href="41598_2024_52653_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Figure 5</label><caption><p>Performance comparison of three feature groups for SVM, XGBoost, LightGBM, and CNN classifiers under independent test on various evaluation metrics: (<bold>A</bold>) GloVE, (<bold>B</bold>) Word2Vec, and (<bold>C</bold>) fastText.</p></caption><graphic xlink:href="41598_2024_52653_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Figure 6</label><caption><p>Performance comparison of various machine learning classifiers trained with three feature groups, utilizing different classifier and feature representations, is presented in terms of AUC and MCC evaluation metrics under 5-fold cross-validation (<bold>A</bold>, <bold>B</bold>) and independent testing (<bold>C</bold>, <bold>D</bold>).</p></caption><graphic xlink:href="41598_2024_52653_Fig6_HTML" id="MO6"/></fig><table-wrap id="Tab6"><label>Table 6</label><caption><p>Cross-validation results of different ML classifiers and feature encoding schemes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">Classifier</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="4">GloVe</td><td align="left">SVM</td><td align="left">0.822</td><td align="left">74.42</td><td align="left">58.14</td><td align="left">90.69</td><td align="left">0.517</td><td align="left">86.21</td><td align="left">0.694</td></tr><tr><td align="left">XGBoost</td><td align="left">0.805</td><td align="left">76.63</td><td align="left">66.67</td><td align="left">83.87</td><td align="left">0.516</td><td align="left">75.00</td><td align="left">0.706</td></tr><tr><td align="left">LightGBM</td><td align="left">0.817</td><td align="left">77.90</td><td align="left">63.16</td><td align="left">89.58</td><td align="left">0.554</td><td align="left">82.76</td><td align="left">0.716</td></tr><tr><td align="left">CNN (Deep-WET)</td><td align="left"><bold>0.883</bold></td><td align="left"><bold>82.56</bold></td><td align="left"><bold>69.44</bold></td><td align="left"><bold>92.00</bold></td><td align="left"><bold>0.641</bold></td><td align="left"><bold>86.21</bold></td><td align="left"><bold>0.769</bold></td></tr><tr><td align="left" rowspan="4">fastText</td><td align="left">SVM</td><td align="left">0.823</td><td align="left">73.26</td><td align="left">57.50</td><td align="left">86.96</td><td align="left">0.469</td><td align="left">79.31</td><td align="left">0.667</td></tr><tr><td align="left">XGBoost</td><td align="left">0.800</td><td align="left">76.74</td><td align="left">81.58</td><td align="left">72.92</td><td align="left">0.541</td><td align="left">70.45</td><td align="left">0.756</td></tr><tr><td align="left">LightGBM</td><td align="left">0.825</td><td align="left">75.58</td><td align="left">61.11</td><td align="left">86.00</td><td align="left">0.492</td><td align="left">75.86</td><td align="left">0.677</td></tr><tr><td align="left">CNN</td><td align="left">0.849</td><td align="left">80.37</td><td align="left">69.39</td><td align="left">89.66</td><td align="left">0.608</td><td align="left">85.00</td><td align="left">0.764</td></tr><tr><td align="left" rowspan="4">Word2Vec</td><td align="left">SVM</td><td align="left">0.805</td><td align="left">74.21</td><td align="left">69.81</td><td align="left">77.33</td><td align="left">0.470</td><td align="left">68.52</td><td align="left">0.692</td></tr><tr><td align="left">XGBoost</td><td align="left">0.806</td><td align="left">75.69</td><td align="left">65.91</td><td align="left">82.54</td><td align="left">0.493</td><td align="left">72.50</td><td align="left">0.691</td></tr><tr><td align="left">LightGBM</td><td align="left">0.810</td><td align="left">74.77</td><td align="left">64.44</td><td align="left">82.26</td><td align="left">0.477</td><td align="left">72.50</td><td align="left">0.683</td></tr><tr><td align="left">CNN</td><td align="left">0.826</td><td align="left">77.91</td><td align="left">63.89</td><td align="left">88.00</td><td align="left">0.542</td><td align="left">79.31</td><td align="left">0.708</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par49">As can be seen from Table <xref rid="Tab6" ref-type="table">6</xref>, Deep-WET achieved the overall best performance compared with the compared ML classifiers in terms of almost performance metrics, with the only exception of the Sen. Meanwhile, CNN-fastText and CNN-Word2Vec were the second-best and third-best classifiers in terms of ACC. To be specific, the ACC values of Deep-WET, CNN-fastText, and CNN-Word2Vec were 82.56%, 80.37%, and 77.91%, respectively. In addition, Deep-WET’s AUC, ACC, Spe, and MCC were 3.40%, 2.19%, 2.34% and 3.30%, respectively, higher than the second-best method CNN-fastText. In case of the independent test results, Deep-WET still outperformed the compared ML classifiers in terms of ACC, Spe, MCC, Pre and F1. Deep-WET’s ACC, Spe, MCC, Pre, and F1 were 1.37%, 3.84%, 2.60%, 5.13% and 2.10%, respectively, higher than the second-best method CNN-fastText.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Independent test results of different ML classifiers and feature encoding schemes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">Classifier</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="4">GloVe</td><td align="left">SVM</td><td align="left">0.803</td><td align="left">73.52</td><td align="left">76.25</td><td align="left">71.11</td><td align="left">0.473</td><td align="left">70.11</td><td align="left">0.731</td></tr><tr><td align="left">XGBoost</td><td align="left">0.787</td><td align="left">73.97</td><td align="left">79.41</td><td align="left">69.23</td><td align="left">0.486</td><td align="left">69.23</td><td align="left">0.740</td></tr><tr><td align="left">LightGBM</td><td align="left">0.802</td><td align="left">75.34</td><td align="left">78.37</td><td align="left">72.22</td><td align="left">0.507</td><td align="left">74.36</td><td align="left">0.763</td></tr><tr><td align="left">CNN (Deep-WET)</td><td align="left">0.805</td><td align="left">78.08</td><td align="left">78.05</td><td align="left">78.13</td><td align="left">0.559</td><td align="left">82.05</td><td align="left">0.800</td></tr><tr><td align="left" rowspan="4">fastText</td><td align="left">SVM</td><td align="left">0.812</td><td align="left">73.20</td><td align="left">78.26</td><td align="left">68.63</td><td align="left">0.470</td><td align="left">69.23</td><td align="left">0.735</td></tr><tr><td align="left">XGBoost</td><td align="left">0.804</td><td align="left">73.77</td><td align="left">75.41</td><td align="left">72.13</td><td align="left">0.476</td><td align="left">73.02</td><td align="left">0.742</td></tr><tr><td align="left">LightGBM</td><td align="left">0.783</td><td align="left">74.59</td><td align="left">72.22</td><td align="left">78.00</td><td align="left">0.494</td><td align="left">82.54</td><td align="left">0.770</td></tr><tr><td align="left">CNN</td><td align="left"><bold>0.816</bold></td><td align="left">76.71</td><td align="left">78.95</td><td align="left">74.29</td><td align="left">0.533</td><td align="left">76.92</td><td align="left">0.779</td></tr><tr><td align="left" rowspan="4">Word2Vec</td><td align="left">SVM</td><td align="left">0.778</td><td align="left">73.19</td><td align="left">66.67</td><td align="left">77.05</td><td align="left">0.433</td><td align="left">63.16</td><td align="left">0.649</td></tr><tr><td align="left">XGBoost</td><td align="left">0.773</td><td align="left">75.25</td><td align="left">69.44</td><td align="left">78.69</td><td align="left">0.476</td><td align="left">65.79</td><td align="left">0.676</td></tr><tr><td align="left">LightGBM</td><td align="left">0.797</td><td align="left">73.98</td><td align="left">81.08</td><td align="left">66.67</td><td align="left">0.483</td><td align="left">71.43</td><td align="left">0.760</td></tr><tr><td align="left">CNN</td><td align="left">0.807</td><td align="left">75.35</td><td align="left">74.42</td><td align="left">76.67</td><td align="left">0.504</td><td align="left">82.05</td><td align="left">0.781</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par50">To further the comparison, Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref> illustrate the cross-validation and independent test performance for our proposed Deep-WET approach along with four robust classifiers with all the evaluation metrics. From Tables <xref rid="Tab7" ref-type="table">7</xref>, <xref rid="Tab8" ref-type="table">8</xref> and Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref>, we can summarize several observations as follows: (i) GloVe features obtained the highest predictive results as compared to fastText and Word2Vec; however, these three feature-encoding techniques all achieved promising performance for the CNN classifiers, followed by LightGBM, XGBoost, and SVM classifier. Word2Vec achieved relatively lower performance, whereas fastText was slightly better than Word2Vec, (ii) CNN classifier consistently achieved the highest results compared to the other three classifiers for all three feature-encoding techniques, and (iii) Finally, our proposed Deep-WET achieved better performance than other conventional ML classifiers, highlighting its superior discriminative power.</p>
    </sec>
    <sec id="Sec19">
      <title>Comparison of Deep-WET with the state-of-the-art methods</title>
      <p id="Par51">To further validate the discriminative power of Deep-WET method, we compared its prediction performance against other existing DBP methods, including DPP-PseAAC<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, PseDNA-Pro<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, iDNA-Prot<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, iDNA-Prot|dis<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, PSFM-DBT<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, Local-DPP<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, iDNAProt-ES<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, IKP-DBPPred<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> Xiuquan et al.<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, iDRBP-MMC<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> and TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, on the independent test data. The prediction performance of the existing methods were obtained by submitting protein sequences in the independent test dataset (148 DBPs and 148 non-DBPs) to their own webservers. Since the web sever of iDNAProt-ES was not functional, the prediction results of iDNAProt-ES were obtained from the reimplementation of iDNAProt-ES and the standalone version of HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, respectively. Table 8 shows the prediction results of Deep-WET and other existing methods.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Performance comparisons of DeepWET with the state-of-the-art methods on the independent test dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Predictor<sup>a</sup></th><th align="left">AUC</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">DPP-PseAAC</td><td align="left">61.15</td><td align="left">55.41</td><td align="left">66.89</td><td align="left">0.225</td><td align="left">62.60</td><td align="left">0.588</td></tr><tr><td align="left">iDNA-Prot</td><td align="left">62.16</td><td align="left">63.51</td><td align="left">60.81</td><td align="left">0.243</td><td align="left">61.84</td><td align="left">0.627</td></tr><tr><td align="left">iDNA-Prot|dis</td><td align="left">68.24</td><td align="left">72.30</td><td align="left">64.19</td><td align="left">0.366</td><td align="left">66.88</td><td align="left">0.695</td></tr><tr><td align="left">PseDNA-Pro</td><td align="left">67.23</td><td align="left">78.38</td><td align="left">56.08</td><td align="left">0.354</td><td align="left">64.09</td><td align="left">0.705</td></tr><tr><td align="left">PSFM-DBT</td><td align="left">68.58</td><td align="left">71 .62</td><td align="left">65.54</td><td align="left">0.372</td><td align="left">67.52</td><td align="left">0.695</td></tr><tr><td align="left">IKP-DBPPred</td><td align="left">58.11</td><td align="left">52.70</td><td align="left">63.51</td><td align="left">0.163</td><td align="left">59.09</td><td align="left">0.557</td></tr><tr><td align="left">Local-DPP</td><td align="left">48.65</td><td align="left">3.38</td><td align="left"><bold>93.92</bold></td><td align="left">– 0.06</td><td align="left">35.71</td><td align="left">0.062</td></tr><tr><td align="left">iDNAProt-ES(on PDB1075)</td><td align="left">71.62</td><td align="left">91 .89</td><td align="left">51.35</td><td align="left">0.473</td><td align="left">65.38</td><td align="left">0.764</td></tr><tr><td align="left">TargetDBP</td><td align="left">76.69</td><td align="left">76.35</td><td align="left">77.03</td><td align="left">0.534</td><td align="left">76.87</td><td align="left">0.766</td></tr><tr><td align="left">Xiuquan et al.</td><td align="left">77</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">iDRBP-MMC</td><td align="left">70</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">Deep-WET</td><td align="left"><bold>78.08</bold></td><td align="left"><bold>78.05</bold></td><td align="left"><bold>78.13</bold></td><td align="left"><bold>0.559</bold></td><td align="left"><bold>82.05</bold></td><td align="left"><bold>0.800</bold></td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p><p><sup>a</sup>The prediction performance of the existing methods were obtained by submitting protein sequences in the independent test dataset (148 DBPs and 148 non-DBPs) to their own webservers.</p></table-wrap-foot></table-wrap></p>
      <p id="Par52">According to the F1 and MCC values, these two evaluation metrics of binary predictions, recorded in Table <xref rid="Tab8" ref-type="table">8</xref>, we can see that Deep-WET has superior performance over other exiting methods in terms of ACC, MCC, Pre, and F1. Notably, by comparing the proposed Deep-WET approach with the second-best predictor TargetDBP in terms of ACC, we observe that Deep-WET achieved improvements of 1.39%, 1.70%, 2.50%, 5.18%, and 3.40% on ACC, Sen, MCC, Pre, and F1, respectively. Although, iDNAProt-ES obtained the highest Sen value of 91.89%, this method provided the lowest Spe value of 51.35%. The main reason behind this high Sen is that iDNAProt-ES has lower false negative (FN) prediction. In contrast, Local-DPP obtained the highest prediction performance in terms of Spe (93.92%) and shows much lower Pre scores (35.71%) producing many false negatives (FN) during prediction, but Acc values of iDNAProt-ES and Local-DPP are lower than those of Deep-WET. Taken together, these results demonstrated that Deep-WET has a great potential for DBP prediction.</p>
    </sec>
    <sec id="Sec20">
      <title>Ablation study</title>
      <p id="Par53">Our CNN model has key components such as convolutional filters, pooling strategies, kernel sizes and fully connected layers, etc. Here, we have conducted ablation studies using the GloVe + fastText + Word2Vec dataset under 5-fold CV, assessing how each individual component influences the predictive performance of Deep-WET:<list list-type="bullet"><list-item><p id="Par54">Remove Specific Convolutional Filters (RSCF): we removed specific filters in the convolutional layers responsible for capturing sequence motifs or patterns linked to DNA binding.</p></list-item><list-item><p id="Par55">Variation in Pooling Strategies (VPS): adjust the baseline model by altering pooling strategies to evaluate how these changes affect the recognition of relevant sequence features.</p></list-item><list-item><p id="Par56">Variation in Kernel Sizes (VKS): Explore diverse kernel sizes within the convolutional layers to capture sequence motifs associated with DNA binding of different lengths.</p></list-item><list-item><p id="Par57">Removal of Fully Connected Layers (RFCL): create a modified version of the baseline model by removing one or more fully connected layers to examine the significance of global features in the classification task.</p></list-item></list></p>
      <p id="Par58">Figure <xref rid="Fig7" ref-type="fig">7</xref> show the performance comparison of Deep-WET and its four variants in terms of AUC on GloVe + fastText + Word2Vec dataset. We can observe that our CNN has better performance than CNN-RSCF, CNN-VPS, CNN-VKS and CNN-RFCL on experiment datasets Here, our-CNN obtains the best AUC score of 0.883, and it is 0.085%, 0.135%, 0.105% and 0.165% higher than that of CNN-RSCF, CNN-VPS, CNN- VKS and CNN-RFCL, respectively, which can illustrate that these parts in our design can improve the predictive performance. Among them, CNN-VPS and CNN-RFCL have the lowest performance. This shows that it is very important to perform the hyperparameters setting of CNN classifiers (see Table <xref rid="Tab1" ref-type="table">1</xref>), that can effectively improve the performance of CNN.<fig id="Fig7"><label>Figure 7</label><caption><p>Comparative analysis between Our CNN and its ablation experiments on the GloVe + fastText + Word2Vec dataset.</p></caption><graphic xlink:href="41598_2024_52653_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec21">
      <title>Web server, data and software availability</title>
      <p id="Par59">We used Apache (2.4.48), Python (3.8.0), and Laravel (8.16.1) to develop a web server for Deep-WET at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.000webhostapp.com/">https://deepwet-dna.000webhostapp.com/</ext-link>. Users can upload or input DNA binding protein sequences of viruses and humans in FASTA format to predict DBPs with probability scores. After clicking the submit button, the server will evaluate the protein sequence and check the format for processing. Prediction results will be generated in a tabular format with detailed information on the word serial number and predicted probability of DBPs and predicted class (DBPs/non-DBPs). Detailed instructions for the webserver can be found on the README option. After the final job, users will get a job ID to be used for further queries. The Deep-WET web server application stores this job ID for fifteen days. Deep-WET may have a long computational time when users input large protein sequences files, since Deep-WET needs to perform NLP-based word embedding packages to generate discriminative features and fix the suitable parameters for the CNN classifier to predict. We strongly suggest inputting a low number of DBP sequences at a time. The experimental datasets for this study are available at: <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.000webhostapp.com/data">https://deepwet-dna.000webhostapp.com/data</ext-link>.</p>
    </sec>
  </sec>
  <sec id="Sec22">
    <title>Conclusions</title>
    <p id="Par60">Identifying DBPs is vital to discovering fundamental protein-DNA mechanisms and understanding their biological interactions. Here, we develop a new deep learning-based approach termed Deep-WET,to achieve more accurate and improved prediction of DBPs. In Deep-WET, we extracted three NLP-based word embedding features to generate single features then combined them sequentially, and assigned weights learned through the use of the DE algorithm. The SHAP technique was utilized to gain the effective feature subsets, and a deep learning-based CNN algorithm was used as a model classifier for predicting DBPs. Comparative analysis on the independent test dataset showed that Deep-WET achieved improved performance compared with conventional ML classifiers and the existing methods, highlight the effectiveness and robustnessof the proposed Deep-WET. The improved performance of Deep-WET is mainly due to the utilization of NLP-based word embedding features that can effectively capture the characteristics of DBPs. A user-friendly web server for Deep-WET is available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.royalit.agency/">https://deepwet-dna.royalit.agency/</ext-link>. Deep-WET is anticipated to be a powerful tool to serve the community-wide effort for the accurate and large-scale identification of potential DBPs from sequences information. Deep learning shows advanced prediction abilities in various fields of computational biology such as hERG blockers<sup><xref ref-type="bibr" rid="CR57">57</xref></sup> , disease-related metabolites<sup><xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR59">59</xref></sup>, single-cell<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> and human lncRNA-miRNA interactions<sup><xref ref-type="bibr" rid="CR61">61</xref>,<xref ref-type="bibr" rid="CR62">62</xref></sup>. Most studies propose deep learning based models<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> for prediction tasks. To enhance the predictive capabilities of our Deep-WET model, our future efforts will focus on three key areas: (1) although our NLP-based feature extraction is now commonly used for extracting distinct features, there may be some limitations, such as ambiguities, lexical gaps, and structural gaps. It would be interesting to use deep learning-based autoencoders<sup><xref ref-type="bibr" rid="CR59">59</xref>,<xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup> to effectively convey the hidden information within DBPs sequences; (2) implementing a small-loss approach and integrating probabilistic local outlier factor (pLOF) with the extracted features to tackle the challenge of label noise in the dataset, ensuring a trustworthy application; (3) developing a graph-based deep learning model for predicting DBPs with unknown structures.</p>
    <p id="Par61">Cellular death is a fundamental and complex biological process that is an underlying driver for many diseases. Authors in<sup><xref ref-type="bibr" rid="CR65">65</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup>, worked for cell death. Our CNN model can be used to classify cells undergoing cell death. This deep learning network has the ability to highly predict cell death. Finally, it is possible to provide a simple Python tool that can be broadly used to detect cell death. Furthermore, our CNN model can recommend specific drugs for the disease.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported by the TM R&amp;D Fund (Project no. RDTC/221054 and SAP ID: MMUE/220023) and the Multimedia University (MMU) IR Fund (Project ID MMUI/220041).</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>Project administration and supervision: K.O.M.G., W.S.; conceptualization, investigation, methodology and visualization: S.M. H.M., M.F.H. and D.N.; analysis, validation and software: S.M.H.M. and M.F.H.; web server development: M.F.H.; writing—original draft: S.M.H.M.; writing—review and editing: S.M.H.M., W.S., K.O.M.G., and D.N. All authors reviewed and approved the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All the data used in this study are available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/data">https://deepwet-dna.monarcatechnical.com/data</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par62">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>J-M</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>X-P</given-names>
          </name>
        </person-group>
        <article-title>Competitive aptamer bioassay for selective detection of adenosine triphosphate based on metal-paired molecular conformational switch and fluorescent gold nanoclusters</article-title>
        <source>Biosens. Bioelectron.</source>
        <year>2012</year>
        <volume>36</volume>
        <fpage>135</fpage>
        <lpage>141</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bios.2012.04.015</pub-id>
        <?supplied-pmid 22560440?>
        <pub-id pub-id-type="pmid">22560440</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ren</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Genome-wide location and function of dna binding proteins</article-title>
        <source>Science</source>
        <year>2000</year>
        <volume>290</volume>
        <fpage>2306</fpage>
        <lpage>2309</lpage>
        <pub-id pub-id-type="doi">10.1126/science.290.5500.2306</pub-id>
        <?supplied-pmid 11125145?>
        <pub-id pub-id-type="pmid">11125145</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gurova</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>New hopes from old drugs: Revisiting dna-binding small molecules as anticancer agents</article-title>
        <source>Future Oncol.</source>
        <year>2009</year>
        <volume>5</volume>
        <fpage>1685</fpage>
        <lpage>1704</lpage>
        <pub-id pub-id-type="doi">10.2217/fon.09.127</pub-id>
        <?supplied-pmid 20001804?>
        <pub-id pub-id-type="pmid">20001804</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leung</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>DS-H</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>VP-Y</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>D-L</given-names>
          </name>
        </person-group>
        <article-title>Dna-binding small molecules as inhibitors of transcription factors</article-title>
        <source>Med. Res. Rev.</source>
        <year>2013</year>
        <volume>33</volume>
        <fpage>823</fpage>
        <lpage>846</lpage>
        <pub-id pub-id-type="doi">10.1002/med.21266</pub-id>
        <?supplied-pmid 22549740?>
        <pub-id pub-id-type="pmid">22549740</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eisenberg</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Marcotte</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Xenarios</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Yeates</surname>
            <given-names>TO</given-names>
          </name>
        </person-group>
        <article-title>Protein function in the post-genomic era</article-title>
        <source>Nature</source>
        <year>2000</year>
        <volume>405</volume>
        <fpage>823</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="doi">10.1038/35015694</pub-id>
        <?supplied-pmid 10866208?>
        <pub-id pub-id-type="pmid">10866208</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Predicting dna-binding proteins: Approached from chou’s pseudo amino acid composition and other specific sequence features</article-title>
        <source>Amino Acids</source>
        <year>2008</year>
        <volume>34</volume>
        <fpage>103</fpage>
        <lpage>109</lpage>
        <pub-id pub-id-type="doi">10.1007/s00726-007-0568-2</pub-id>
        <?supplied-pmid 17624492?>
        <pub-id pub-id-type="pmid">17624492</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chowdhury</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Shatabda</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>idnaprot-es: Identification of dna-binding proteins using evolutionary and structural features</article-title>
        <source>Sci. Rep.</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>14938</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-14945-1</pub-id>
        <?supplied-pmid 29097781?>
        <pub-id pub-id-type="pmid">29097781</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>endna-prot: Identification of dna-binding proteins by applying ensemble learning</article-title>
        <source>BioMed Res. Int.</source>
        <year>2014</year>
        <pub-id pub-id-type="doi">10.1155/2014/294279</pub-id>
        <?supplied-pmid 25580429?>
        <pub-id pub-id-type="pmid">25580429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identifying dna-binding proteins by combining support vector machine and pssm distance transformation</article-title>
        <source>BMC Syst. Biol.</source>
        <year>2015</year>
        <volume>9</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1186/1752-0509-9-S1-S10</pub-id>
        <?supplied-pmid 25582171?>
        <pub-id pub-id-type="pmid">25582171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rahman</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Shatabda</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Saha</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kaykobad</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>Dpp-pseaac: A dna-binding protein prediction model using Chou’s general pseaac</article-title>
        <source>J. Theor. Biol.</source>
        <year>2018</year>
        <volume>452</volume>
        <fpage>22</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jtbi.2018.05.006</pub-id>
        <?supplied-pmid 29753757?>
        <pub-id pub-id-type="pmid">29753757</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hwang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Kuznetsov</surname>
            <given-names>IB</given-names>
          </name>
        </person-group>
        <article-title>Dp-bind: A web server for sequence-based prediction of dna-binding residues in dna-binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>634</fpage>
        <lpage>636</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl672</pub-id>
        <?supplied-pmid 17237068?>
        <pub-id pub-id-type="pmid">17237068</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lou</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sequence based prediction of dna-binding proteins based on hybrid feature selection using random forest and gaussian naive bayes</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e86703</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0086703</pub-id>
        <?supplied-pmid 24475169?>
        <pub-id pub-id-type="pmid">24475169</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Improved detection of dna-binding proteins via compression technology on pssm information</article-title>
        <source>PLoS ONE</source>
        <year>2017</year>
        <volume>12</volume>
        <fpage>e0185587</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0185587</pub-id>
        <?supplied-pmid 28961273?>
        <pub-id pub-id-type="pmid">28961273</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>idna-prot| dis: Identifying dna-binding proteins by incorporating amino acid distance-pairs and reduced alphabet profile into the general pseudo amino acid composition</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e106691</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0106691</pub-id>
        <?supplied-pmid 25184541?>
        <pub-id pub-id-type="pmid">25184541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>X-W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X-T</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Z-Q</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>M-H</given-names>
          </name>
        </person-group>
        <article-title>Identify dna-binding proteins with optimal chou’s amino acid composition</article-title>
        <source>Protein Peptid. Lett.</source>
        <year>2012</year>
        <volume>19</volume>
        <fpage>398</fpage>
        <lpage>405</lpage>
        <pub-id pub-id-type="doi">10.2174/092986612799789404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ahmad</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gromiha</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Sarai</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Analysis and prediction of dna-binding proteins and their binding residues based on composition, sequence and structural information</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <fpage>477</fpage>
        <lpage>486</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg432</pub-id>
        <?supplied-pmid 14990443?>
        <pub-id pub-id-type="pmid">14990443</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of dna-binding proteins by incorporating evolutionary information into pseudo amino acid composition via the top-n-gram approach</article-title>
        <source>J. Biomol. Struct. Dyn.</source>
        <year>2015</year>
        <volume>33</volume>
        <fpage>1720</fpage>
        <lpage>1730</lpage>
        <pub-id pub-id-type="doi">10.1080/07391102.2014.968624</pub-id>
        <?supplied-pmid 25252709?>
        <pub-id pub-id-type="pmid">25252709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>W-Z</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>J-A</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>idna-prot: Identification of dna binding proteins using random forest with grey model</article-title>
        <source>PLoS ONE</source>
        <year>2011</year>
        <volume>6</volume>
        <fpage>e24756</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0024756</pub-id>
        <?supplied-pmid 21935457?>
        <pub-id pub-id-type="pmid">21935457</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Psedna-pro: Dna-binding protein identification by combining chou’s pseaac and physicochemical distance transformation</article-title>
        <source>Mol. Inf.</source>
        <year>2015</year>
        <volume>34</volume>
        <fpage>8</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1002/minf.201400025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Dna binding protein identification by combining pseudo amino acid composition and profile-based protein representation</article-title>
        <source>Sci. Rep.</source>
        <year>2015</year>
        <volume>5</volume>
        <fpage>15479</fpage>
        <pub-id pub-id-type="doi">10.1038/srep15479</pub-id>
        <?supplied-pmid 26482832?>
        <pub-id pub-id-type="pmid">26482832</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Local-dpp: An improved dna-binding protein prediction method by exploring local evolutionary information</article-title>
        <source>Inf. Sci.</source>
        <year>2017</year>
        <volume>384</volume>
        <fpage>135</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2016.06.026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Psfm-dbt: Identifying dna-binding proteins by combing position specific frequency matrix and distance-bigram transformation</article-title>
        <source>Int. J. Mol. Sci.</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>1856</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms18091856</pub-id>
        <?supplied-pmid 28841194?>
        <pub-id pub-id-type="pmid">28841194</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zaman</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hmmbinder: Dna-binding protein prediction using hmm profile based features</article-title>
        <source>BioMed Res. Int.</source>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.1155/2017/4590609</pub-id>
        <?supplied-pmid 29270430?>
        <pub-id pub-id-type="pmid">29270430</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Identification of dna-binding proteins using mixed feature representation methods</article-title>
        <source>Molecules</source>
        <year>2017</year>
        <volume>22</volume>
        <fpage>1602</fpage>
        <pub-id pub-id-type="doi">10.3390/molecules22101602</pub-id>
        <?supplied-pmid 28937647?>
        <pub-id pub-id-type="pmid">28937647</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X-G</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y-H</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>D-J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G-J</given-names>
          </name>
        </person-group>
        <article-title>Targetdbp: Accurate dna-binding protein prediction via sequence-based multi-view feature learning</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2019</year>
        <volume>17</volume>
        <fpage>1419</fpage>
        <lpage>1429</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2019.2893634</pub-id>
        <?supplied-pmid 30668479?>
        <pub-id pub-id-type="pmid">30668479</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Skolnick</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Dbd-hunter: A knowledge-based method for the prediction of dna–protein interactions</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2008</year>
        <volume>36</volume>
        <fpage>3978</fpage>
        <lpage>3992</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkn332</pub-id>
        <?supplied-pmid 18515839?>
        <pub-id pub-id-type="pmid">18515839</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nimrod</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Schushan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Szilágyi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Leslie</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ben-Tal</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>idbps: A web server for the identification of dna binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <fpage>692</fpage>
        <lpage>693</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq019</pub-id>
        <?supplied-pmid 20089514?>
        <pub-id pub-id-type="pmid">20089514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Predicting dna-binding proteins and binding residues by complex structure prediction and application to human proteome</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e96694</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0096694</pub-id>
        <?supplied-pmid 24792350?>
        <pub-id pub-id-type="pmid">24792350</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The i-tasser suite: Protein structure and function prediction</article-title>
        <source>Nat. Methods</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>7</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3213</pub-id>
        <?supplied-pmid 25549265?>
        <pub-id pub-id-type="pmid">25549265</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nanni</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Brahnam</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Set of approaches based on 3d structure and position specific-scoring matrix for predicting dna-binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <fpage>1844</fpage>
        <lpage>1851</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty912</pub-id>
        <?supplied-pmid 30395157?>
        <pub-id pub-id-type="pmid">30395157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sang</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hmmpred: Accurate prediction of dna-binding proteins based on hmm profiles and xgboost feature selection</article-title>
        <source>Comput. Math. Methods Med.</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1155/2020/1384749</pub-id>
        <?supplied-pmid 33133226?>
        <pub-id pub-id-type="pmid">33133226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>Y-H</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>X-N</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>D-J</given-names>
          </name>
        </person-group>
        <article-title>Dnapred: Accurate identification of dna-binding sites from protein sequence by ensembled hyperplane-distance-based support vector machines</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2019</year>
        <volume>59</volume>
        <fpage>3057</fpage>
        <lpage>3071</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00749</pub-id>
        <?supplied-pmid 30943723?>
        <pub-id pub-id-type="pmid">30943723</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Q</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Stackpdb: Predicting dna-binding proteins based on xgb-rfe feature optimization and stacked ensemble classifier</article-title>
        <source>Appl. Soft Comput.</source>
        <year>2021</year>
        <volume>99</volume>
        <fpage>106921</fpage>
        <pub-id pub-id-type="doi">10.1016/j.asoc.2020.106921</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rose</surname>
            <given-names>PW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The rcsb protein data bank: Views of structural biology for basic and applied research and education</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2015</year>
        <volume>43</volume>
        <fpage>D345</fpage>
        <lpage>D356</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku1214</pub-id>
        <?supplied-pmid 25428375?>
        <pub-id pub-id-type="pmid">25428375</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Pennington, J., Socher, R. &amp; Manning, C. D. Glove: Global vectors for word representation. in <italic>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, 1532–1543 (2014).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Distributed representations of words and phrases and their compositionality</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2013</year>
        <volume>26</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bojanowski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Grave</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Joulin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Enriching word vectors with subword information</article-title>
        <source>Trans. Assoc. Comput. Ling.</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>135</fpage>
        <lpage>146</lpage>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Joulin, A. <italic>et al.</italic> Fasttext.zip: Compressing text classification models. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1612.03651">http://arxiv.org/abs/1612.03651</ext-link>10.48550/arXiv.1612.03651 (2016).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Joulin, A., Grave, E., Bojanowski, P. &amp; Mikolov, T. Bag of tricks for efficient text classification. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.01759">http://arxiv.org/abs/1607.01759</ext-link>10.48550/arXiv.1607.01759 (2016).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support-vector networks</article-title>
        <source>Mach. Learn.</source>
        <year>1995</year>
        <volume>20</volume>
        <fpage>273</fpage>
        <lpage>297</lpage>
        <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. in <italic>Proceedings of the 22nd ACM sigkdd International Conference on Knowledge Discovery and Data Mining</italic>, 785–794. 10.1145/2939672.2939785 (2016).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lundberg</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S-I</given-names>
          </name>
        </person-group>
        <article-title>A unified approach to interpreting model predictions</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1705.07874</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parsa</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Movahedi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taghipour</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Derrible</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mohammadian</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Toward safer highways, application of xgboost and shap for real-time accident detection and feature analysis</article-title>
        <source>Accid. Anal. Prev.</source>
        <year>2020</year>
        <volume>136</volume>
        <fpage>105405</fpage>
        <pub-id pub-id-type="doi">10.1016/j.aap.2019.105405</pub-id>
        <?supplied-pmid 31864931?>
        <pub-id pub-id-type="pmid">31864931</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ke</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Lightgbm: A highly efficient gradient boosting decision tree</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grinblat</surname>
            <given-names>GL</given-names>
          </name>
          <name>
            <surname>Uzal</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Larese</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Granitto</surname>
            <given-names>PM</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for plant identification using vein morphological patterns</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2016</year>
        <volume>127</volume>
        <fpage>418</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2016.07.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yen</surname>
            <given-names>S-J</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Y-S</given-names>
          </name>
        </person-group>
        <article-title>Cluster-based under-sampling approaches for imbalanced data distributions</article-title>
        <source>Expert Syst. Appl.</source>
        <year>2009</year>
        <volume>36</volume>
        <fpage>5718</fpage>
        <lpage>5727</lpage>
        <pub-id pub-id-type="doi">10.1016/j.eswa.2008.06.108</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wiatowski</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bölcskei</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>A mathematical theory of deep convolutional neural networks for feature extraction</article-title>
        <source>IEEE Trans. Inf. Theor.</source>
        <year>2017</year>
        <volume>64</volume>
        <fpage>1845</fpage>
        <lpage>1866</lpage>
        <pub-id pub-id-type="doi">10.1109/TIT.2017.2776228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hunter</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>Matplotlib: A 2d graphics environment</article-title>
        <source>Comput. Sci. Eng.</source>
        <year>2007</year>
        <volume>9</volume>
        <fpage>90</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waskom</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Seaborn: Statistical data visualization</article-title>
        <source>J. Open Source Softw.</source>
        <year>2021</year>
        <volume>6</volume>
        <fpage>3021</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lumley</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Interactive Visualization of Climate Change: Characteristics, Intentions, and Metrics for Success</source>
        <year>2021</year>
        <publisher-name>McGill University</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guyon</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Barnhill</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Gene selection for cancer classification using support vector machines</article-title>
        <source>Mach. Learn.</source>
        <year>2002</year>
        <volume>46</volume>
        <fpage>389</fpage>
        <lpage>422</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regression shrinkage and selection via the lasso</article-title>
        <source>J. R. Stat. Soc. B</source>
        <year>1996</year>
        <volume>58</volume>
        <fpage>267</fpage>
        <lpage>288</lpage>
        <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A convolutional neural network system to discriminate drug-target interactions</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2019</year>
        <volume>18</volume>
        <fpage>1315</fpage>
        <lpage>1324</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2019.2940187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Du</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Deep multi-label joint learning for rna and dna-binding proteins prediction</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2022</year>
        <volume>20</volume>
        <fpage>307</fpage>
        <lpage>320</lpage>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>idrbp\_mmc: Identifying dna-binding proteins and rna-binding proteins based on multi-label learning model and motif-based convolutional neural network</article-title>
        <source>J. Mol. Biol.</source>
        <year>2020</year>
        <volume>432</volume>
        <fpage>5860</fpage>
        <lpage>5875</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2020.09.008</pub-id>
        <?supplied-pmid 32920048?>
        <pub-id pub-id-type="pmid">32920048</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Investigating cardiotoxicity related with herg channel blockers using molecular fingerprints and graph attention mechanism</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>153</volume>
        <fpage>106464</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.106464</pub-id>
        <?supplied-pmid 36584603?>
        <pub-id pub-id-type="pmid">36584603</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>A deep learning method for predicting metabolite-disease associations via graph neural network</article-title>
        <source>Brief. Bioinform.</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>266</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbac266</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting metabolite-disease associations based on auto-encoder and non-negative matrix factorization</article-title>
        <source>Brief. Bioinform.</source>
        <year>2023</year>
        <volume>24</volume>
        <fpage>259</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbad259</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene function and cell surface protein association analysis based on single-cell multiomics data</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>157</volume>
        <fpage>106733</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.106733</pub-id>
        <?supplied-pmid 36924730?>
        <pub-id pub-id-type="pmid">36924730</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Shuai</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Predicting the potential human lncrna–mirna interactions based on graph convolution network with conditional random field</article-title>
        <source>Brief. Bioinform.</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>463</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbac463</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Using network distance analysis to predict lncrna–mirna interactions</article-title>
        <source>Interdiscipl. Sci. Comput. Life Sci.</source>
        <year>2021</year>
        <volume>13</volume>
        <fpage>535</fpage>
        <lpage>545</lpage>
        <pub-id pub-id-type="doi">10.1007/s12539-021-00458-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dcamcp: A deep learning model based on capsule network and attention mechanism for molecular carcinogenicity prediction</article-title>
        <source>J. Cell. Mol. Med.</source>
        <year>2023</year>
        <volume>27</volume>
        <fpage>3117</fpage>
        <lpage>3126</lpage>
        <pub-id pub-id-type="doi">10.1111/jcmm.17889</pub-id>
        <?supplied-pmid 37525507?>
        <pub-id pub-id-type="pmid">37525507</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meng</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>scaaga: Single cell data analysis framework using asymmetric autoencoder with gene attention</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>165</volume>
        <fpage>107414</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.107414</pub-id>
        <?supplied-pmid 37660567?>
        <pub-id pub-id-type="pmid">37660567</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Rip1-dependent linear and nonlinear recruitments of caspase-8 and rip3 respectively to necrosome specify distinct cell death outcomes</article-title>
        <source>Protein Cell</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>858</fpage>
        <lpage>876</lpage>
        <pub-id pub-id-type="doi">10.1007/s13238-020-00810-x</pub-id>
        <?supplied-pmid 33389663?>
        <pub-id pub-id-type="pmid">33389663</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shuai</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Quantifying the underlying landscape, entropy production and biological path of the cell fate decision between apoptosis and pyroptosis</article-title>
        <source>Chaos Solitons Fract.</source>
        <year>2024</year>
        <volume>178</volume>
        <fpage>114328</fpage>
        <pub-id pub-id-type="doi">10.1016/j.chaos.2023.114328</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10844231</article-id>
    <article-id pub-id-type="pmid">38316843</article-id>
    <article-id pub-id-type="publisher-id">52653</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-024-52653-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep-WET: a deep learning-based approach for predicting DNA-binding proteins using word embedding techniques with weighted features</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Mahmud</surname>
          <given-names>S. M. Hasan</given-names>
        </name>
        <address>
          <email>hasan.swe@aiub.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Goh</surname>
          <given-names>Kah Ong Michael</given-names>
        </name>
        <address>
          <email>michael.goh@mmu.edu.my</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hosen</surname>
          <given-names>Md. Faruk</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nandi</surname>
          <given-names>Dip</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shoombuatong</surname>
          <given-names>Watshara</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02j8ga255</institution-id><institution-id institution-id-type="GRID">grid.442972.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2218 5390</institution-id><institution>Department of Computer Science, </institution><institution>American International University-Bangladesh (AIUB), </institution></institution-wrap>Kuratoli, Dhaka, 1229 Bangladesh </aff>
      <aff id="Aff2"><label>2</label>Centre for Advanced Machine Learning and Applications (CAMLAs), Dhaka, 1229 Bangladesh </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/04zrbnc33</institution-id><institution-id institution-id-type="GRID">grid.411865.f</institution-id><institution-id institution-id-type="ISNI">0000 0000 8610 6308</institution-id><institution>Faculty of Information Science &amp; Technology (FIST), </institution><institution>Multimedia University, Jalan Ayer Keroh Lama, </institution></institution-wrap>75450 Melaka, Malaysia </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00gvj4587</institution-id><institution-id institution-id-type="GRID">grid.443019.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 0479 1356</institution-id><institution>Department of Information and Communication Technology, </institution><institution>Mawlana Bhashani Science and Technology University, </institution></institution-wrap>Santosh, Tangail, 1902 Bangladesh </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01znkr924</institution-id><institution-id institution-id-type="GRID">grid.10223.32</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0490</institution-id><institution>Center for Research Innovation and Biomedical Informatics, Faculty of Medical Technology, </institution><institution>Mahidol University, </institution></institution-wrap>Bangkok, 10700 Thailand </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>2</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2024</year>
    </pub-date>
    <volume>14</volume>
    <elocation-id>2961</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>1</month>
        <year>2024</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2024</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">DNA-binding proteins (DBPs) play a significant role in all phases of genetic processes, including DNA recombination, repair, and modification. They are often utilized in drug discovery as fundamental elements of steroids, antibiotics, and anticancer drugs. Predicting them poses the most challenging task in proteomics research. Conventional experimental methods for DBP identification are costly and sometimes biased toward prediction. Therefore, developing powerful computational methods that can accurately and rapidly identify DBPs from sequence information is an urgent need. In this study, we propose a novel deep learning-based method called Deep-WET to accurately identify DBPs from primary sequence information. In Deep-WET, we employed three powerful feature encoding schemes containing Global Vectors, Word2Vec, and fastText to encode the protein sequence. Subsequently, these three features were sequentially combined and weighted using the weights obtained from the elements learned through the differential evolution (DE) algorithm. To enhance the predictive performance of Deep-WET, we applied the SHapley Additive exPlanations approach to remove irrelevant features. Finally, the optimal feature subset was input into convolutional neural networks to construct the Deep-WET predictor. Both cross-validation and independent tests indicated that Deep-WET achieved superior predictive performance compared to conventional machine learning classifiers. In addition, in extensive independent test, Deep-WET was effective and outperformed than several state-of-the-art methods for DBP prediction, with accuracy of 78.08%, MCC of 0.559, and AUC of 0.805. This superior performance shows that Deep-WET has a tremendous predictive capacity to predict DBPs. The web server of Deep-WET and curated datasets in this study are available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/">https://deepwet-dna.monarcatechnical.com/</ext-link>. The proposed Deep-WET is anticipated to serve the community-wide effort for large-scale identification of potential DBPs.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Computational models</kwd>
      <kwd>Biological techniques</kwd>
      <kwd>Computational biology and bioinformatics</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100012024</institution-id>
            <institution>Multimedia University</institution>
          </institution-wrap>
        </funding-source>
        <award-id>IR Fund (Project ID MMUI/220041)</award-id>
        <principal-award-recipient>
          <name>
            <surname>Mahmud</surname>
            <given-names>S. M. Hasan</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Nature Limited 2024</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">DNA-binding Proteins (DBPs) participate in many essential biological processes, including DNA replication, gene regulation, repair, and modification<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Identification of DBPs is fundamentally important for understanding characterizations of protein function and drug design. A number of large-scale proteomics experiments have been performed to identify DBPs based on biochemical methods, such as X-ray crystallography<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> and fast ChIP<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. Despite the increasing number of experimentally determined DBPs, the underlying mechanism of DBP specificity remains mostly unidentified, and these approaches are laborious, time-consuming, and sometimes biased toward prediction in the post-genome era, when large numbers of unannotated DBPs are rapidly being sequenced and deposited. As an alternative, computational methods are accurate and cost-effective and can be used to complement the experimental efforts.</p>
    <p id="Par3">To date, several computational algorithms, including machine-learning (ML)-based and template-based methods, have been developed for in silico prediction of DBPs<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>. DBPs can be predicted based on two types of protein data input: i sequence-driven (e.g., iDNA-Prot<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, PseDNA-Pro<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, iDNAPro-PseAAC<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, iDNA-Prot|dis<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, Local-DPP<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, PSFM-DBT<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, IKP-DBPPred<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, iDNAProt-ES<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, DPPPseAAC<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, and TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>) and 3D-structure-driven (e.g., DBD-Hunter<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, iDBPs<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, and SPOT-Seq (DNA)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>) methods. Only protein sequence data is required for sequence-driven techniques. The 3D-structure-driven techniques require native or projected 3D structure data from the query protein. In this case, 3D-structure-driven techniques cannot function correctly without 3D structure information. This method performs better when the protein’s native structure is known. On the other hand, sequence-driven techniques do not have this problem. Furthermore, due to the inherent challenges of measuring protein 3D structures in experimental studies, there is a significant gap between the quantities of sequences and 3D structures<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, which is currently expanding quickly in the postgenomic era. Recently, PSI-BLAST was utilized by Chowdhury et al.<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> to derive polysaccharide storage myopathy, which revealed evolutionary information to predict DBP. The secondary structure information of the protein sequences was extracted using SPIDER2. To retrieve protein sequence information, Nanni et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> utilized AAC and quasi residue couple (QRC). Meanwhile, the autocovariance method was used to derive physicochemical characteristics. In addition, evolutionary data was retrieved using the pseudo-position specific scoring matrix (PsePSSM), N-gram features (NGR), and texture descriptors (TD). Sang et al.<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> calculated the HMM matrix for each sequence using the hidden Markov model (HMM). The HMM matrix was converted into feature vectors of the same length using AAC, autocovariance transformation (ACT), and cross-covariance transformation (CCT). Thus, designing sequence-driven computational strategies is essential to accurate prediction of DBPs.</p>
    <p id="Par4">Choosing appropriate feature extraction methods and classification algorithms in order to select the best subset of features is a key factor for the successful discovery of DBPs. In TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, four single-view features (AAC, PsePSSM, PsePRSA, and PsePPDBS) are used to extract the DNA-binding features and apply a learning-based technique to the weights of features to combine them for training an SVM classifier. In addition, an excellent feature subset was selected using SVM-REF + CBR from the non-redundant benchmark and new gold-standard dataset. Rahman et al.<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> utilized the same feature selection (REF) and classifier (SVM) to develop a model DPP-PseAAC for which the authors focused on Chou’s general PseAAC for generating features. Another proposed DBP predictor method is DNAPred<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, where authors use the E-HDSVM algorithm, which includes HD-US and EAdaBoost, to predict protein DNA binding sites. A similar ensemble-based method performed by Zhang et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, XGB-RFE, is used to attain effective features, after which the best features are fed to the stacked ensemble classifier (the combined form of LightGBM, XGBoost, and SVM) to build the proposed StackPDB model.</p>
    <p id="Par5">The above-mentioned algorithms have proven to be exemplary, but we opted to use convolutional neural networks (CNNs) to improve prediction performance. In the meanwhile, it is important to devise an appropriate encoding scheme to represent the sequence fragments surrounding DBPs/non-DBPs to develop a ML-based predictor. In this study, we present a new convolutional neural network (CNN)-based predictor called Deep-WET for accurately identifying DBPs from primary sequence information. Firstly, we applied three consecutive sequence encoding approaches, namely Global Vectors (GloVe), Word2Vec, and fastText, to extract the protein sequence patterns. Secondly, the DE is utilized to acquire the weights for three base features. With these obtained weights, we combined three base features in a weighted manner to create the super feature. In order to improve the predictive performance of Deep-WET, we employed SHapley Additive exPlanations (SHAP) approach to remove irrelevant features from super features and then inputted the optimal one into CNN algorithm for the final model construction. Experimental results demonstrated that Deep-WET achieved a accurate and robust performance as compared with conventional ML classifiers on both the training and independent test datasets. Moreover, comparative analysis on the independent test dataset showed that Deep-WET achieved improved performance compared with the existing approaches, highlight the effectiveness and robustness of the proposed Deep-WET. We also conducted a series of computational analyses to provide in-depth understanding of the DBPs. Finally, the proposed method, Deep-WET, was implemented as a user-friendly web server: at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/">https://deepwet-dna.monarcatechnical.com/</ext-link>.</p>
  </sec>
  <sec id="Sec2">
    <title>Materials and methods</title>
    <sec id="Sec3">
      <title>The overall framework of Deep-WET</title>
      <p id="Par6">The construction process of Deep-WET is depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Deep-WET consists of multiple steps, including data preparation, natural language processing (NLP)-based feature encoding, weighted features, optimal feature subset selection, best classifier selection, and final prediction. In the first stage, three NLP-based Word embedding feature encoding techniques were employed (GloVe, Word2Vec, and fastText), and then the optimal subset of features was selected using the SHAP technique from the weighted features. The selected feature subsets from each feature encoding were fed to four ML and one DL algorithms to build the final prediction models using the training and independent test datasets. Finally, the classifier having the highest cross-validation AUC was considered to construct the final predictor herein.<fig id="Fig1"><label>Figure 1</label><caption><p>The flowchart illustrates our proposed methodology. The upper part represents data pre-processing, the middle part depicts feature extraction with various classifiers, and the lower part showcases classification using the CNN model.</p></caption><graphic xlink:href="41598_2024_52653_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Data preparation</title>
      <p id="Par7">Developing a reliable, comprehensive, and stringent dataset is the first important step of statistical predictor development. Here, the curated dataset denoted with S was presented as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} S = S_{posi} \cup S_{nega} \end{aligned}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where, <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq1.gif"/></alternatives></inline-formula> denotes the positive subset containing DBPs or positive samples, while, <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq2.gif"/></alternatives></inline-formula> denotes the negative subset containing non-DBPs or negative samples, and <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\cup$$\end{document}</tex-math><mml:math id="M8"><mml:mo>∪</mml:mo></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq3.gif"/></alternatives></inline-formula> symbol resembles the union of the following sets. The <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq5.gif"/></alternatives></inline-formula> datasets were collected and primarily used by Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, who collected both DBP and non-DBP chains from PDB Data Bank. There are two main reasons why we used the dataset established by Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> as follows. Firstly, this dataset applied a lower CD-HIT<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> threshold of 0.25 to exclude the redundant protein chains. Secondly, this dataset exclude the protein chain sequences having below 50 residues and unknown residues. For the the <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{posi}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">posi</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{nega}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="italic">nega</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq7.gif"/></alternatives></inline-formula> datasets, they were randomly selected to create the training and independent test datasets. The training dataset consists of 1052 DBPs and 1052 non-DBPs, while The independent test dataset consists of 148 DBPs and 148 non-DBPs. More details on the training and independent test datasets are provided in an article of Jun Hu et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.</p>
    </sec>
    <sec id="Sec5">
      <title>Feature encodings</title>
      <p id="Par8">Word embedding (WE), in which the vocabulary of words can be represented as vectors using large text as an input, is the most popular technique in the area of natural language processing (NLP). WE techniques are able to convert amino acids in a fixed-length vector, where a user needs to define the fixed feature dimensions that can provide adequate prediction results. In this study, we implemented three unsupervised embedding techniques to encode protein sequences: GloVe<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, Word2Vec<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, and fastText<sup><xref ref-type="bibr" rid="CR37">37</xref>–<xref ref-type="bibr" rid="CR39">39</xref></sup>.</p>
      <sec id="Sec6">
        <title>Word2Vec</title>
        <p id="Par9">Word2Vec, a model developed by Tomas Mikolav at Google, computes and generates high-quality, distributed, and continuous dense representations of words<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. These are unsupervised models that can take in massive textual corpora, create a vocabulary of possible word combinations, and generate dense word embeddings on the vector space. The size of the vocabulary determines the size of the word embedding vectors. This decreases the dimensionality of the following dense vector, compared to high-dimensional sparse vector generation using the traditional bag of words (BOW). To construct word embedding, Word2Vec employs two different methods: (1) common bag of words (CBOW) and (2) the Skip-gram model. Notably, the CBOW is faster than the Skip-gram model and generates a better representation of more frequent words<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. On the other hand, the Skip-gram model performs well with a relatively small amount of data and generates a better representation of rare words<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p>
        <p id="Par10">Finding the target word <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_t$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq8.gif"/></alternatives></inline-formula> through n predictions using the CBOW model can be accomplished by the following equation:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} J_{\phi } = \frac{1}{T}\sum _{t=1}^{T}logP(w_t \big | w_{(t-n)}, \ldots ,w_{(t-1)},w_{(t+1)}, \ldots ,w_{(t+n)}) \end{aligned}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par11">Here, <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{(t-1)}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq9.gif"/></alternatives></inline-formula> to <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{(t+n)}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq10.gif"/></alternatives></inline-formula> sequence of words represents the context words. The following equation can further simplify the above equation since the hidden layer can be equivalent to a softmax layer:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} P(w_t \big | w_{(t-n)}, \ldots ,w_{(t-1)},w_{(t+1)}, \ldots ,w_{(t+n)}) = \frac{exp(W_k^Th_t)}{\sum _{k=1}^{v} exp(W_k^Th_t)} \end{aligned}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">|</mml:mo></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>v</mml:mi></mml:msubsup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par12">Here, the output weight matrix between hidden layers is denoted as <italic>W</italic>, and after matrix operation, the average value of input vectors is represented as <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_t$$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq11.gif"/></alternatives></inline-formula>.</p>
      </sec>
      <sec id="Sec7">
        <title>GloVe</title>
        <p id="Par13">GloVe is an unsupervised learning vectorization technique. It is a log-bilinear regression model that incorporates both local statistics and global statistics<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. The training of this model is performed on non-zero entries of global word-to-word co-occurrence statistics that tabulates how frequently words are co-occurring within a given corpus. For collecting statistics, the following matrix needs a single pass through the entire corpus. These passes can be expensive for large corpora. Moreover, its resulting representations show the interesting linear substructures of those word vector spaces.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \sum _{i,j}^{N}f(X_{ij})(v_i^Tv_j + b_i + b_j - log(X_{ij}))^2 \end{aligned}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par14">Here, <inline-formula id="IEq12"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v_i, v_j$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq12.gif"/></alternatives></inline-formula> correspond to the word embedding of <italic>i</italic>, <italic>j</italic>; <italic>X</italic> represents the word-to-word co-occurrence matrix; and <inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i^{th}$$\end{document}</tex-math><mml:math id="M34"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">th</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq13.gif"/></alternatives></inline-formula> number of co-occurrences of word <italic>j</italic> is denoted by <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{ij}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq14.gif"/></alternatives></inline-formula>. Furthermore, the probability of word <italic>j</italic> occurring in the context <italic>i</italic> is the following:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} X_{ij} = P(i) = \frac{X_{ij}}{X_i} \end{aligned}$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec8">
        <title>fastText</title>
        <p id="Par15">fastText, proposed by Facebook<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, is an extension of Word2Vec. It provides tools to learn word representation and sentence classifications of ML. Word vectors are a more organized, numerical, and efficient representation of words and sentences. fastText provides a supervised module to build a model for text classifications. It technique breaks an individual word into a bag of n-grams or sub-words and feeds them into the network, which also generates vector representation for rare or unseen words<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Since the technique uses the same architecture as Word2Vec, the following equation minimizes the loss of softmax layer, <italic>l</italic> over <italic>N</italic> sequences using CBOW model:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \sum _{n=1}^{n} l(y_n, BAx_n) \end{aligned}$$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mi>A</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par16">Here, <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_n$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq15.gif"/></alternatives></inline-formula> represents the bag of one-hot encoded vectors and <inline-formula id="IEq16"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_n$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq16.gif"/></alternatives></inline-formula> represents the label of the nth sequence of words. The purpose of using FastText in the present study is to find the partial information single DNA sequence order.</p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Weight learning for weighted features</title>
      <p id="Par17">Single-view features represent the discriminative information for each sequence, but combing single-view features to make a weighted feature is critical in ML-based DBP prediction. The most common technique involves serially adding (’+’) single features. However, this straightforward combination technique lacks a guarantee to represent discriminative capability and may overlook the relative importance of the base sequence. To address this issue, we employ a differential evolution (DE) method to determine the optimal weights for each feature. DE algorithm variants of evolutionary algorithms and applied in various works<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup> to show the positive effect. The process we followed for DE algorithm to learn feature weights from a single feature is illustrated as follows: <def-list><def-item><term><bold>Step 1:</bold></term><def><p id="Par18"><bold>Initialization</bold> Randomly create an initial population <inline-formula id="IEq17"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_o = \{FW_1^g, FW_2^g, FW_3^g \ldots \, \ldots \,FW_n^g\}, where (FW_{i,1}^g, FW_{i,2}^g, FW_{i,3}^g, FW_{i,4}^g)^T$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mn>3</mml:mn><mml:mi>g</mml:mi></mml:msubsup><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq17.gif"/></alternatives></inline-formula> represents <italic>i</italic>th number solution in the population <italic>g</italic>th. <italic>N</italic> means size of the generation population where to set the maximum generation <inline-formula id="IEq20"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_{max}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq20.gif"/></alternatives></inline-formula>, crossover rate (<italic>CR</italic>), scaling factor (<italic>F</italic>) to 1000, 0.5, and 0.5, respectively.</p></def></def-item><def-item><term><bold>Step 2:</bold></term><def><p id="Par19"><bold>Mutation</bold> A mutation vector <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MV_i^g$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi>M</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq21.gif"/></alternatives></inline-formula> was initialized for each salutation according. <disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MV_i^g = FW_{r1}^g + F.(FW_{r2}^g - FW_{r3}^g) \end{aligned}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>.</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p></def></def-item><def-item><term><bold>Step 3:</bold></term><def><p id="Par20"><bold>Crossover</bold> For the diversity of each solution, a trial vector <inline-formula id="IEq22"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TV_i^g = (TV_{i,1}^g, TV_{i,2}^g, TV_{i,3}^g \ldots \, \ldots \,TV_{i,D}^g)$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mo>…</mml:mo><mml:mspace width="0.166667em"/><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq22.gif"/></alternatives></inline-formula> of crossover is established in the DE technique as follows: <disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} TV_{i, j}^g = {\left\{ \begin{array}{ll} &amp;{} \text {TV}_{i,j}^g \quad if \; R_j \le CR \;\;ot \;j = j_r\\ &amp;{} \,\,\text {FW}_{i,j}^g \qquad \,\,\text {otherwise}\\ \end{array}\right. } \end{aligned}$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:msubsup><mml:mtext>TV</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mspace width="1em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.277778em"/><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mspace width="0.277778em"/><mml:mspace width="0.277778em"/><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.277778em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msubsup><mml:mtext>FW</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mspace width="2em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula> where <inline-formula id="IEq23"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j = 1,2,3 \ldots ,D, j_r$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>j</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq23.gif"/></alternatives></inline-formula> represent randomly produced integer with [1, <italic>D</italic>]; <inline-formula id="IEq24"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_j$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq24.gif"/></alternatives></inline-formula> means uniformly distributed range [0,1] and <inline-formula id="IEq25"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CR \in (0,1)$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq25.gif"/></alternatives></inline-formula> indicate crossover rate.</p></def></def-item><def-item><term><bold>Step 4:</bold></term><def><p id="Par21"><bold>Selection</bold> Find the better vector from trial <inline-formula id="IEq26"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$TV_i^g$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq26.gif"/></alternatives></inline-formula> and target <inline-formula id="IEq27"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FW_i^g$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq27.gif"/></alternatives></inline-formula> using the following way: <disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} FW_i^{g+1} = {\left\{ \begin{array}{ll} &amp;{} TV_{i}^g, \quad \text {if} \; f(TV_i^{g}) \le f(FW_i^g)\\ &amp;{} \,\,\text {FW}_{i}^g, \qquad \,\,\text {otherwise}\\ \end{array}\right. } \end{aligned}$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.277778em"/><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msubsup><mml:mtext>FW</mml:mtext><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>g</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="2em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p></def></def-item><def-item><term><bold>Step 5:</bold></term><def><p id="Par22"><bold>Termination</bold><inline-formula id="IEq28"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g=g+1$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq28.gif"/></alternatives></inline-formula> and repeat steps 2 to 4 until <italic>g</italic> is greater the <inline-formula id="IEq29"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_{max}.$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq29.gif"/></alternatives></inline-formula></p></def></def-item></def-list></p>
      <p id="Par23">After concluding the DE procedure, we can get the final results. In this study, We have generated a novel super feature, represented as GloVe + fastText + Word2Vec, by the weighted and sequential fusion of GloVe, fastText, and Word2Vec features. DE is a powerful optimization algorithm; however, using it for feature weighting in ML presents certain limitations and challenges. DE may struggle with slow convergence, susceptibility to local optima, and sensitivity to parameter choices. Additionally, the algorithm may violate constraints, lack robustness across diverse datasets, and exhibit computational intensity. To avoid these challenges, we have performed parameter tuning <inline-formula id="IEq30"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(population-size, \, mutation \, rate, \, crossover \, probabilities)$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.166667em"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.166667em"/><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq30.gif"/></alternatives></inline-formula> in experiments, considering adaptive strategies for mutation and crossover rates. Furthermore, exploring parallelization methods helps alleviate computational burdens, while strategies like diversity maintenance mechanisms aim to address convergence issues.</p>
    </sec>
    <sec id="Sec10">
      <title>SHAP-based feature selection scheme</title>
      <p id="Par24">SHAP is an additive feature attribution method introduced by Lunberg and Lee<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> in which each individual prediction is interpreted by the contribution of the features and then ordered according to their importance<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. SHAP allocates each feature an importance value for a particular prediction. This SHAP feature selection approach is based on game theory<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>; SHAP values break down a prediction to show the impact of each individual feature. Suppose each feature is <inline-formula id="IEq31"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M76"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq31.gif"/></alternatives></inline-formula>, is replaced by <inline-formula id="IEq32"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_i$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq32.gif"/></alternatives></inline-formula> for determining whether the feature value <inline-formula id="IEq33"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq33.gif"/></alternatives></inline-formula> exists or not. SHAP represents the explanation as:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} g(z) = \phi _o + \sum _{i=1}^{M} \phi _iz_i \end{aligned}$$\end{document}</tex-math><mml:math id="M82" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par25">In the above equation, <italic>g</italic> represents the explanation model; <inline-formula id="IEq34"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z \in {0,1} ^M$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq34.gif"/></alternatives></inline-formula> represents the coalition vector; 0 and 1 indicate that the corresponding feature is absent or present, respectively; the number of input features included in the model is denoted as <italic>M</italic>; and <inline-formula id="IEq35"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi _i \in R, \phi _i$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq35.gif"/></alternatives></inline-formula> represents the feature attribution values for a feature <italic>i</italic>. Considering the game theory concept, Shapley values can be calculated using the following equation:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \phi _i = \sum _{S \subset M \setminus \{i\}} \frac{|S|!(M - |S| -1)!}{M!} [f_x(S \cup \{i\} - f_x(S))] \end{aligned}$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊂</mml:mo><mml:mi>M</mml:mi><mml:mo lspace="0.15em" rspace="0.15em" stretchy="false">\</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par26">In the above equation, <italic>M</italic> represents the set of features in the model; all feature subsets achieved from <italic>M</italic> are represented as <italic>S</italic>; the function computes the total contribution of a given features set <italic>S</italic>; <inline-formula id="IEq36"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S \subset M \setminus \{i\}$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mi>S</mml:mi><mml:mo>⊂</mml:mo><mml:mi>M</mml:mi><mml:mo lspace="0.15em" rspace="0.15em" stretchy="false">\</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq36.gif"/></alternatives></inline-formula> represents the value of the corresponding feature when <italic>i</italic> is known, versus when the corresponding feature value <italic>i</italic> is unknown for all subsets.</p>
      <p id="Par27">One of the important features of the SHAP is the barplot in the form of rectangular horizontal bars, where the length of the bars represents the importance of a given feature. As we need the global significance, we sum the contribution of each feature, or absolute Shapley values.<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I_j = \sum _{i=1}^{n} |\phi _j^{(i)}| \end{aligned}$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par28">Then, we plot each of the features by sorting them in decreasing order. Figure 2A shows the important features based on SHAP contributions for the XGBoost trained before predicting DBPs. The SHAP summary plot gives a high-level composite view that displays the importance of features with feature effects. Each point in the plot represents a SHAP value for a specific feature of an instance. The values that pull the prediction power of the model downwards are on the left, and the values that push the prediction further up are on the right. On the y-axis, the features are placed in descending order, and on the x-axis, there is a scale representing the Shapley value with a vertical line at point zero. The positive and negative values are to the right and left part of that vertical line, respectively. Here the colors separate the relative size of the features between instances. Specifically, low values are colored blue and high values are colored red. Overlapping more data points in the y-axis direction shows the distribution of SHAP values for each individual feature. Moreover, in the summary plot, we clearly observe the relationships between the value of a feature and the effect on the prediction. Figure 2B shows the SHAP summary plot, which orders important features for identifying DBPs.</p>
    </sec>
    <sec id="Sec11">
      <title>Implementation of convolutional neural network</title>
      <p id="Par29">CNNs are a type of deep learning model commonly used in applications including recommender systems, image and video recognition, and natural language processing<sup><xref ref-type="bibr" rid="CR45">45</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. In CNN architecture, the deeper convolutional layers (CLs) lead to learning high dimension features using sliding convolution kernels on the upper part of previous layers with different hyper-parameter settings such as filters, control layer outputs, stride, and zero-paddings. Pooling layers (PLs) are able to reduce the input feature size and offer translation invariance by local non-linear operations<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Fully connected layers (FCLs) utilized to classify the tasks, consisting of an equal number of output neurons as artificial neural networks.</p>
      <p id="Par30">Each neuron is completely linked to all of the nodes in the preceding and subsequent levels<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. After adding one additional CL and max-PL to the process, the technique demonstrated a significant improvement in terms of computational complexity and program runtime. The following equation may be used to compute the outputs of each convolutional layer:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} y_k^l = f\big ( \sum _{m} W_{m,k}^l y_m^{l-1} + b_k^l \big ) \end{aligned}$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">(</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par31">The layer index is <italic>l</italic>, while the input and output feature maps are <italic>m</italic> and <italic>k</italic>, respectively. Specifically, <inline-formula id="IEq37"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_k^l$$\end{document}</tex-math><mml:math id="M96"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq37.gif"/></alternatives></inline-formula> denotes the <italic>k</italic>th feature map of the <italic>l</italic> layer’s input, while <inline-formula id="IEq39"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_m^{(l-1)}$$\end{document}</tex-math><mml:math id="M98"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq39.gif"/></alternatives></inline-formula> denotes the <inline-formula id="IEq40"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m-$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:mi>m</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq40.gif"/></alternatives></inline-formula>th feature map of layer <inline-formula id="IEq41"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l-1$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq41.gif"/></alternatives></inline-formula> output. The weight tensor and bias term, respectively, are <italic>W</italic> and <italic>b</italic>. Back-propagation and adaptive estimating approaches were used to reduce cross-entropy loss<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. Our model’s output layer is essentially a logistic regression classifier. It takes <inline-formula id="IEq42"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_k^l$$\end{document}</tex-math><mml:math id="M104"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq42.gif"/></alternatives></inline-formula> as an input and computes the following:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{y} = f(W^ly^l + b^l) \end{aligned}$$\end{document}</tex-math><mml:math id="M106" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par32">The output <inline-formula id="IEq43"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><mml:math id="M108"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="41598_2024_52653_Article_IEq43.gif"/></alternatives></inline-formula>is the final predicted score; <italic>W</italic> is the weight matrix; <italic>b</italic> is the bias vector. Each output size is 2, denoting positive or negative classes for the binary classification task of DNA binding predictions. In order to discover suitable parameters, we want to minimize cross-entropy loss by adaptive moment estimation and back-propagation techniques:<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} loss = - \frac{1}{N} \sum _{i=1}^{N} y_ilog\hat{y_l} + (1-y_i)log(1-\hat{y_l}) \end{aligned}$$\end{document}</tex-math><mml:math id="M110" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>y</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par33">To improve the model’s efficiency, batch normalization and dropout techniques<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> were employed. The dropout in FCLs decreases by a few units during the training phase, whereas batch normalization helps to standardize the inputs into unit standard deviation and zero means. Furthermore, dropout was able to overcome the problem of overfitting, and batch normalization supported the model with sufficient learning ratios.</p>
      <p id="Par34">To achieve a better performance, hyperparameter optimization plays a vital part in the implementation of the proposed methodology. The following hyperparameters are optimized before training the model: learning rate, number of filters, kernel size, batch size, number of hidden layers, optimizers, dropout layers, and activation function. Here, three convolutional layers are used as hidden layers in the CNN model architecture. In addition, 32, 48, 64 filters and kernel sizes of 3, 4, 5 are used. Using ReLu as an activation in the hidden layers and Sigmoid in the fully connected layer results in the desired outcome. Dropout layers with dropout rates of 0.2, 0.3, and 0.5 are used to prevent overfitting. With extensive experimentation, employment of the Adam optimizer with a learning rate of 0.00001 and binary cross-entropy loss function shows the optimal result. Table <xref rid="Tab1" ref-type="table">1</xref> comprehensively illustrates the hyperparameters used in our method. Detailed parameter settings of the other three classifiers for different feature encoding are also listed in Table 6.</p>
    </sec>
    <sec id="Sec12">
      <title>Performance evaluation</title>
      <p id="Par35">The performance of Deep-WET was evaluated in terms of six standard performance metrics for the binary classification problem including accuracy (ACC), sensitivity (Sen),specificity (Spe), Matthew’s coefficient correlation (MCC), and precision (Pre).<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} ACC= &amp; {} \frac{TP + TN}{TN+TP+FN+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M112" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Sen= &amp; {} \frac{TP}{TP + FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M114" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Spe= &amp; {} \frac{TN}{FP+TN} \end{aligned}$$\end{document}</tex-math><mml:math id="M116" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MCC= &amp; {} \frac{(TP\times TN) - (FP\times FN)}{\sqrt{(TP+FP)\times (TP+FN)\times (TN+FP)\times (TN+FN)}} \end{aligned}$$\end{document}</tex-math><mml:math id="M118" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Pre= &amp; {} \frac{TP}{TP+FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M120" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F1\, score= &amp; {} \frac{2\times (Precision\times Recall)}{Precision + Recall} \end{aligned}$$\end{document}</tex-math><mml:math id="M122" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mspace width="0.166667em"/><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="41598_2024_52653_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula>where <italic>TP</italic>, <italic>FP</italic>, <italic>TN</italic>,  and <italic>FN</italic> respectively represent the number of true positives (correctly classified positive), false positives (incorrectly classified as positive), true negatives (correctly classified negative), and false negatives (incorrectly classified as negative), respectively. Furthermore, the AUC metric was also used to evaluate the performances of the proposed DeepWET model, where the curve is plotted by TPR (sensitivity) and FPR (1 – specificity) with different threshold settings.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Hyperparameters setting of CNN classifiers.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Hyperparameters</th><th align="left">Range</th></tr></thead><tbody><tr><td align="left">Learning rate</td><td align="left">[0.00001, 0.01, 0.001, 0.0001]</td></tr><tr><td align="left">Number of filters</td><td align="left">[32, 48, 64]</td></tr><tr><td align="left">Kernel size</td><td align="left">[3, 4, 5]</td></tr><tr><td align="left">Batch size</td><td align="left">[16, 32, 64, 128]</td></tr><tr><td align="left">Number of hidden layers</td><td align="left">[2, 3]</td></tr><tr><td align="left">Optimizer</td><td align="left">[‘Adam’]</td></tr><tr><td align="left">Dropout rate</td><td align="left">[0.2, 0.3, 0.5]</td></tr><tr><td align="left">Activation function</td><td align="left">[’relu’, ’sigmoid’]</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec13">
      <title>Experimental setup and packages</title>
      <p id="Par36">All tests in this study were carried out on three independent computers with the following settings, using Python version 3.7.7 or above:<list list-type="bullet"><list-item><p id="Par37">A desktop computer with Intel Core i5 CPU @ 2.71GHz x 4, Windows 10, 64-bit OS and 8 GB RAM.</p></list-item><list-item><p id="Par38">A desktop computer with Intel Core i5 CPU @ 2.11GHz x 4, Windows 10, 64-bit OS and 8 GB RAM.</p></list-item><list-item><p id="Par39">A server machine with Intel Core i5-3320M CPU @ 2.60GHz x 4, Ubuntu 18.04.2 LTS, 64-bit OS, 13 MB L3 cache and 64 GB RAM.</p></list-item></list></p>
      <p id="Par40">CNN classifier and SHAP technique were employed for model learning and feature selection on TensorFlow 2.0 and SHAP 0.39.0 Python libraries to implement them. We utilized improved parameter settings of the CNN algorithm such as batch size 16, kernel size 4, 2 hidden layers, and dropout rate 0.5. Several graphs were plotted in this experiment using Matplotlib<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, Seaborn<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, and Plotly<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, in addition to pre-installed Python tools.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Results and discussion</title>
    <sec id="Sec15">
      <title>Performance comparison of different feature encodings</title>
      <p id="Par41">In this section, we systematically evaluated the effect of various feature encodings, including single-feature (GloVe, fastText, and Word2Vec) and weighted-feature (GloVe + fastText, GloVe + Word2Vec, fastText + Word2Vec, and GloVe + fastText + Word2Vec) encodings in DBP identification. These features were inputted to a CNN classifier to evaluate their corresponding models using the 5-fold cross-validation test. The cross-validation performance of variant CNN classifiers trained with different features are provided in Table <xref rid="Tab2" ref-type="table">2</xref> and Fig. <xref rid="Fig3" ref-type="fig">3</xref>A. It is worth noting that the parameters of CNN classifiers were carefully determined to improve their performance under the 5-fold cross-validation process.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance comparison of CNN classifiers trained with different feature encodings on the training dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">GloVe</td><td align="left">0.810</td><td align="left">75.00</td><td align="left">71.15</td><td align="left">77.63</td><td align="left">0.485</td><td align="left">68.52</td><td align="left">0.698</td></tr><tr><td align="left">fastText</td><td align="left">0.785</td><td align="left">73.44</td><td align="left">67.24</td><td align="left">78.57</td><td align="left">0.462</td><td align="left">72.22</td><td align="left">0.696</td></tr><tr><td align="left">Word2Vec</td><td align="left">0.793</td><td align="left">71.09</td><td align="left">73.13</td><td align="left">68.85</td><td align="left">0.420</td><td align="left">72.06</td><td align="left">0.726</td></tr><tr><td align="left">fastText + Word2Vec</td><td align="left">0.826</td><td align="left">75.78</td><td align="left">70.18</td><td align="left">80.28</td><td align="left">0.508</td><td align="left">74.07</td><td align="left">0.721</td></tr><tr><td align="left">GloVe + Word2Vec</td><td align="left">0.820</td><td align="left">76.64</td><td align="left">71.96</td><td align="left">85.00</td><td align="left">0.523</td><td align="left">77.50</td><td align="left">0.713</td></tr><tr><td align="left">GloVe + fastText</td><td align="left">0.839</td><td align="left">78.12</td><td align="left">72.96</td><td align="left">89.19</td><td align="left">0.549</td><td align="left">80.95</td><td align="left">0.738</td></tr><tr><td align="left">GloVe + fastText +Word2Vec</td><td align="left">0.864</td><td align="left">79.07</td><td align="left">75.10</td><td align="left">91.49</td><td align="left">0.585</td><td align="left">86.21</td><td align="left">0.740</td></tr></tbody></table></table-wrap></p>
      <p id="Par42">Among single-based features, GloVe outperformed fastText and Word2Vec in terms of all performance metrics. The AUC, ACC, Sen, Spe, and MCC of GloVe were 0.810, 75.00%, 71.15%, 77.63% and 0.485, respectively. Interestingly, AUC, ACC, and MCC of GloVe were 2.5–1.7%, 1.56–3.91%, and 2.3–6.5% higher than fastText and Word2Vec, respectively. A weighted feature was created by adding different combinations of the single feature extraction methods in order to improve the predictive performance. As can be seen from Table <xref rid="Tab2" ref-type="table">2</xref>, we observe that the performance the combination of GloVe, fastText and Word2Vec is better than those of other three weighted features in terms of all performance metrics. The ACC, Sen, Spe, and MCC of the combination of GloVe, fastText and Word2Vec are 79.07% 64.10%, 91.49% and 0.585, respectively, which are 0.95–3.29%, 2.14–4.92%, 2.30–11.91%, 0.036–0.077%, 5.26–12.14% and 0.002–0.027% higher than other combination features, respectively. Figure <xref rid="Fig3" ref-type="fig">3</xref>A shows that the AUC value of GloVe+fastText+Word2Vec 0.864, which is larger than the other three weighted features. Overall, we observed that the Sen value of individual features was slightly higher than that of the corresponding weighted features in some cases. Moreover, the performance of the top weighted features (GloVe + fastText + Word2Vec) is significantly higher than the single-view feature in terms of all evaluation metrics. Weighteds features archive higher prediction performances to the single-view feature in terms of all evaluation metrics. Therefore, in this study, the GloVe + fastText + Word2Vec feature outperformed other single and weighted features and is considered as the optimal one in termes of computational cost and predictive performance.</p>
    </sec>
    <sec id="Sec16">
      <title>Feature section approaches improve the predictive performance</title>
      <p id="Par43">The original feature subsets extracted from feature encoding techniques might contain noisy and redundant information that can affect the classifiers’ performance. Therefore, we utilized feature selection methods to determine important features from the original feature subsets. Here, three feature selection techniques, including RFE<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, LASSO<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, and SHAP<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, were utilized for determining the important features from GloVe + fastText + Word2Vec feature encoding. In our experiment, we ranked all features using its importance obtained from RFE, LASSO, and SHAP and then established the six feature subsets that consisted of the top-ranked features ranging from top 200 to the top 450 features with an interval of 50. Then, for each feature selection technique, the six feature subsets were fed to develop individual CNN classifiers whose corresponding prediction results based on a 5-fold cross-validation were provided in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance comparison of various feature sets derived from different feature selection techniques.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature selection</th><th align="left">No. of features</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="6">RFE</td><td align="left">200</td><td align="left">0.859</td><td align="left">76.74</td><td align="left">58.82</td><td align="left">88.46</td><td align="left">0.503</td><td align="left">76.92</td><td align="left">0.667</td></tr><tr><td align="left">250</td><td align="left">0.853</td><td align="left">79.69</td><td align="left">65.38</td><td align="left">89.47</td><td align="left">0.574</td><td align="left">80.95</td><td align="left">0.723</td></tr><tr><td align="left">300</td><td align="left">0.866</td><td align="left">82.55</td><td align="left">71.87</td><td align="left">88.89</td><td align="left">0.621</td><td align="left">79.31</td><td align="left">0.754</td></tr><tr><td align="left">350</td><td align="left">0.853</td><td align="left">80.23</td><td align="left">85.11</td><td align="left">74.36</td><td align="left">0.600</td><td align="left">80.00</td><td align="left">0.825</td></tr><tr><td align="left">400</td><td align="left">0.876</td><td align="left">81.40</td><td align="left">67.57</td><td align="left">91.83</td><td align="left">0.622</td><td align="left">86.20</td><td align="left">0.758</td></tr><tr><td align="left">450</td><td align="left">0.866</td><td align="left">79.10</td><td align="left">62.50</td><td align="left">88.89</td><td align="left">0.541</td><td align="left">76.92</td><td align="left">0.690</td></tr><tr><td align="left" rowspan="6">Lasso</td><td align="left">200</td><td align="left">0.837</td><td align="left">77.57</td><td align="left">69.05</td><td align="left">83.07</td><td align="left">0.526</td><td align="left">72.50</td><td align="left">0.707</td></tr><tr><td align="left">250</td><td align="left">0.869</td><td align="left">81.25</td><td align="left">82.86</td><td align="left">79.31</td><td align="left">0.622</td><td align="left">82.86</td><td align="left">0.829</td></tr><tr><td align="left">300</td><td align="left">0.867</td><td align="left">81.35</td><td align="left">87.50</td><td align="left">76.09</td><td align="left">0.581</td><td align="left">76.09</td><td align="left">0.814</td></tr><tr><td align="left">350</td><td align="left">0.856</td><td align="left">79.10</td><td align="left">81.82</td><td align="left">76.19</td><td align="left">0.581</td><td align="left">78.26</td><td align="left">0.800</td></tr><tr><td align="left">400</td><td align="left">0.882</td><td align="left">81.40</td><td align="left">64.71</td><td align="left">92.30</td><td align="left">0.607</td><td align="left">84.61</td><td align="left">0.733</td></tr><tr><td align="left">450</td><td align="left">0.877</td><td align="left">79.68</td><td align="left">65.38</td><td align="left">89.47</td><td align="left">0.574</td><td align="left">80.95</td><td align="left">0.723</td></tr><tr><td align="left" rowspan="6">SHAP</td><td align="left">200</td><td align="left">0.873</td><td align="left">76.56</td><td align="left">60.00</td><td align="left">91.18</td><td align="left">0.544</td><td align="left">85.71</td><td align="left">0.706</td></tr><tr><td align="left">250</td><td align="left">0.866</td><td align="left">80.25</td><td align="left">65.79</td><td align="left">91.67</td><td align="left">0.604</td><td align="left">86.21</td><td align="left">0.746</td></tr><tr><td align="left">300</td><td align="left">0.883</td><td align="left">81.31</td><td align="left">66.66</td><td align="left">91.89</td><td align="left">0.616</td><td align="left">85.70</td><td align="left">0.750</td></tr><tr><td align="left">350</td><td align="left">0.880</td><td align="left">81.40</td><td align="left">68.57</td><td align="left">90.19</td><td align="left">0.611</td><td align="left">82.76</td><td align="left">0.750</td></tr><tr><td align="left">400</td><td align="left"><bold>0.883</bold></td><td align="left"><bold>82.56</bold></td><td align="left"><bold>69.44</bold></td><td align="left"><bold>92.00</bold></td><td align="left"><bold>0.641</bold></td><td align="left"><bold>86.21</bold></td><td align="left"><bold>0.769</bold></td></tr><tr><td align="left">450</td><td align="left">0.863</td><td align="left">80.23</td><td align="left">66.67</td><td align="left">90.00</td><td align="left">0.591</td><td align="left">82.76</td><td align="left">0.739</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par44">As seen in Table <xref rid="Tab3" ref-type="table">3</xref>, the optimal subsets containing top 300, 400, and 400 optimal features derived from the RFE, LASSO and SHAP techniques, respectively, outperformed other feature sets in terms of both ACC and AUC. In the meanwhile, the performance of the optimal subsets from the SHAP technique outperformed than the RFE and LASSO techniques. To be specific, the AUC, ACC, Sen, Spe, MCC, Pre, and F1 of the optimal subset from the SHAP technique were 0.883, 82.56%, 69.44%, 92.00%, 0.641, 86.21, and 0.769, respectively. Thus, the optimal subset derived from the SHAP technique was considered to develop our proposed model. To check the effectiveness of the optimal subset, we compared its performance with the original feature set. As shown in Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab4" ref-type="table">4</xref>, the ACC, Sen, MCC and F1 of the optimal subset were 3.49%, 5.34%, 5.60%, and 3.40% higher than the original feature set. For convenience of discussion, the CNN classifier combined with the optimal subset from the SHAP technique is referred herein as Deep-WET.
</p>
      <p id="Par45">Altogether, the SHAP technique was a powerful approach for implementing DNA binding protein datasets. To make a clear comparison of prediction effects, the results of the SHAP importance bar graph on the GloVe + fastText + Word2Vec dataset for 400 feature dimensions are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>A. In Fig. <xref rid="Fig2" ref-type="fig">2</xref>A, the bar plot generated by SHAP shows the important features in the form of horizontal bars, with length representing the importance of features. We summarized the most significant features by sorting them in decreasing order based on absolute Shapley values. In addition, the SHAP summary plot for 400 feature dimensions is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>B. It represents a high-level composite look that indicates the important features and effects. Each point depicts a SHAP score in the plot for a particular feature instance. Notably, we can observe the relationship between the feature value and the effect on prediction in the SHAP summary plot.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance comparison of CNN classifiers trained with different optimal feature sets on the training dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">GloVe</td><td align="left">0.852</td><td align="left">75.70</td><td align="left">63.46</td><td align="left">87.27</td><td align="left">0.524</td><td align="left">82.50</td><td align="left">0.717</td></tr><tr><td align="left">fastText</td><td align="left">0.826</td><td align="left">77.91</td><td align="left">63.89</td><td align="left">88.00</td><td align="left">0.542</td><td align="left">79.31</td><td align="left">0.708</td></tr><tr><td align="left">Word2Vec</td><td align="left">0.806</td><td align="left">74.22</td><td align="left">66.67</td><td align="left">81.54</td><td align="left">0.488</td><td align="left">77.78</td><td align="left">0.718</td></tr><tr><td align="left">fastText + Word2Vec</td><td align="left">0.857</td><td align="left">77.57</td><td align="left">68.18</td><td align="left">84.13</td><td align="left">0.532</td><td align="left">75.00</td><td align="left">0.714</td></tr><tr><td align="left">GloVe + Word2Vec</td><td align="left">0.856</td><td align="left">79.44</td><td align="left">70.45</td><td align="left">85.71</td><td align="left">0.571</td><td align="left">77.50</td><td align="left">0.738</td></tr><tr><td align="left">GloVe + fastText</td><td align="left">0.850</td><td align="left">80.37</td><td align="left">69.39</td><td align="left">89.66</td><td align="left">0.608</td><td align="left">85.00</td><td align="left">0.764</td></tr><tr><td align="left">GloVe + fastText +Word2Vec</td><td align="left">0.883</td><td align="left">82.56</td><td align="left">69.44</td><td align="left">92.00</td><td align="left">0.641</td><td align="left">86.21</td><td align="left">0.769</td></tr></tbody></table></table-wrap></p>
      <p id="Par46">From the above-mentioned observations and discussion, we concluded that the SHAP technique was a more powerful and effective feature selection one; therefore, this technique was chose for selecting a subset of features for predicting DBPs herein. In addition, we also applied the SHAP technique in other types of features whose corresponding prediction results were summarized in Table <xref rid="Tab4" ref-type="table">4</xref> and Fig. <xref rid="Fig3" ref-type="fig">3</xref>. By comparing the performance of the models without feature selection (Table <xref rid="Tab2" ref-type="table">2</xref> along with Fig. <xref rid="Fig3" ref-type="fig">3</xref>A and C) and with the SHAP-based feature selection (Table <xref rid="Tab4" ref-type="table">4</xref> along with Fig. <xref rid="Fig3" ref-type="fig">3</xref>B and D), the models with the SHAP-based feature selection achieve better performance than those of the models without feature selection.</p>
    </sec>
    <sec id="Sec17">
      <title>Hyperparameter of CNN</title>
      <p id="Par47">The hyperparameter learning rate controls how the model changes according to estimated error each time the weights are updated. Finding the optimal learning rate can be challenging, because a higher learning rate makes the gradient drop faster, and a lower learning rate leads to the gradient hardly converging. Here, the faster rate of gradient drop results in informative and meaningful features failing to get extracted over each iteration, and the lower rate of gradient convergence results in a longer training time. Therefore, five learning rates in a range of 1e–6 to 1e–2 are implemented for the proposed model to find the optimal performance. From Table <xref rid="Tab5" ref-type="table">5</xref>, the learning rate of 0.00001 gives the highest performance compared to implementing the remaining four learning rates. However, the sensitivity value of using the 0.00001 learning rate is suboptimal compared to the value of the 0.0001 and 0.001 learning rates.<fig id="Fig2"><label>Figure 2</label><caption><p>The SHAP importance bar graph results for the GloVe + fastText + Word2Vec dataset with 400 feature dimensions are presented. (<bold>A</bold>) bar plot generated by SHAP shows the important features in the form of horizontal bars (<bold>B</bold>) SHAP summary plot for the 400 feature dimensions.</p></caption><graphic xlink:href="41598_2024_52653_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Figure 3</label><caption><p>ROC curves and AUPR curves of CNN classifiers are depicted for both single and hybrid feature spaces without feature selection (<bold>A</bold>, <bold>C</bold>) and with SHAP-based feature selection (<bold>B</bold>, <bold>D</bold>).</p></caption><graphic xlink:href="41598_2024_52653_Fig3_HTML" id="MO3"/></fig><table-wrap id="Tab5"><label>Table 5</label><caption><p>Cross-validation results of CNN classifiers trained with different learning rates.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Learning rate</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">1e–2</td><td align="left">0.835</td><td align="left">79.06</td><td align="left">64.29</td><td align="left">86.21</td><td align="left">0.515</td><td align="left">69.23</td><td align="left">0.667</td></tr><tr><td align="left">1e–3</td><td align="left">0.803</td><td align="left">81.39</td><td align="left"><bold>72.73</bold></td><td align="left">84.38</td><td align="left">0.543</td><td align="left">61.54</td><td align="left">0.667</td></tr><tr><td align="left">1e–4</td><td align="left">0.838</td><td align="left">81.25</td><td align="left">69.57</td><td align="left">87.80</td><td align="left">0.586</td><td align="left">76.19</td><td align="left">0.727</td></tr><tr><td align="left">1e–5</td><td align="left">0.883</td><td align="left">82.56</td><td align="left">69.44</td><td align="left">92.00</td><td align="left">0.641</td><td align="left">86.21</td><td align="left">0.769</td></tr><tr><td align="left">1e–6</td><td align="left">0.874</td><td align="left">79.07</td><td align="left">61.11</td><td align="left">92.00</td><td align="left">0.571</td><td align="left">84.62</td><td align="left">0.710</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec18">
      <title>Comparison of Deep-WET with conventional ML classifiers</title>
      <p id="Par48">To evaluate the performance of the proposed Deep-WET, we compared its predictive performance with conventional ML classifiers. Herein, the conventional ML classifiers were built using four well-known ML classifiers (i.e., SVM<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, XGBoost<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, LightGBM<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, and CNN<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>) and the three NLP-based word embedding techniques (i.e., GloVe, fastText, and Word2Vec). In total, 11 conventional ML classifiers were created in this study. It is noteworthy that the parameters of all ML classifiers were carefully optimized to improve their prediction capability under a 5-fold cross-validation procedure. In these experiments, classifiers have been trained a total of 24 times. The prediction performance based on both 5-fold cross-validation and independent tests are listed in Tables 7-8. In addition, their respective graphs are shown in Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig4"><label>Figure 4</label><caption><p>Performance comparison of three feature groups for SVM, XGBoost, LightGBM, and CNN classifiers under 5-fold cross-validation test on various evaluation metrics: (<bold>A</bold>) GloVE, (<bold>B</bold>) Word2Vec, and (<bold>C</bold>) fastText.</p></caption><graphic xlink:href="41598_2024_52653_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Figure 5</label><caption><p>Performance comparison of three feature groups for SVM, XGBoost, LightGBM, and CNN classifiers under independent test on various evaluation metrics: (<bold>A</bold>) GloVE, (<bold>B</bold>) Word2Vec, and (<bold>C</bold>) fastText.</p></caption><graphic xlink:href="41598_2024_52653_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Figure 6</label><caption><p>Performance comparison of various machine learning classifiers trained with three feature groups, utilizing different classifier and feature representations, is presented in terms of AUC and MCC evaluation metrics under 5-fold cross-validation (<bold>A</bold>, <bold>B</bold>) and independent testing (<bold>C</bold>, <bold>D</bold>).</p></caption><graphic xlink:href="41598_2024_52653_Fig6_HTML" id="MO6"/></fig><table-wrap id="Tab6"><label>Table 6</label><caption><p>Cross-validation results of different ML classifiers and feature encoding schemes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">Classifier</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="4">GloVe</td><td align="left">SVM</td><td align="left">0.822</td><td align="left">74.42</td><td align="left">58.14</td><td align="left">90.69</td><td align="left">0.517</td><td align="left">86.21</td><td align="left">0.694</td></tr><tr><td align="left">XGBoost</td><td align="left">0.805</td><td align="left">76.63</td><td align="left">66.67</td><td align="left">83.87</td><td align="left">0.516</td><td align="left">75.00</td><td align="left">0.706</td></tr><tr><td align="left">LightGBM</td><td align="left">0.817</td><td align="left">77.90</td><td align="left">63.16</td><td align="left">89.58</td><td align="left">0.554</td><td align="left">82.76</td><td align="left">0.716</td></tr><tr><td align="left">CNN (Deep-WET)</td><td align="left"><bold>0.883</bold></td><td align="left"><bold>82.56</bold></td><td align="left"><bold>69.44</bold></td><td align="left"><bold>92.00</bold></td><td align="left"><bold>0.641</bold></td><td align="left"><bold>86.21</bold></td><td align="left"><bold>0.769</bold></td></tr><tr><td align="left" rowspan="4">fastText</td><td align="left">SVM</td><td align="left">0.823</td><td align="left">73.26</td><td align="left">57.50</td><td align="left">86.96</td><td align="left">0.469</td><td align="left">79.31</td><td align="left">0.667</td></tr><tr><td align="left">XGBoost</td><td align="left">0.800</td><td align="left">76.74</td><td align="left">81.58</td><td align="left">72.92</td><td align="left">0.541</td><td align="left">70.45</td><td align="left">0.756</td></tr><tr><td align="left">LightGBM</td><td align="left">0.825</td><td align="left">75.58</td><td align="left">61.11</td><td align="left">86.00</td><td align="left">0.492</td><td align="left">75.86</td><td align="left">0.677</td></tr><tr><td align="left">CNN</td><td align="left">0.849</td><td align="left">80.37</td><td align="left">69.39</td><td align="left">89.66</td><td align="left">0.608</td><td align="left">85.00</td><td align="left">0.764</td></tr><tr><td align="left" rowspan="4">Word2Vec</td><td align="left">SVM</td><td align="left">0.805</td><td align="left">74.21</td><td align="left">69.81</td><td align="left">77.33</td><td align="left">0.470</td><td align="left">68.52</td><td align="left">0.692</td></tr><tr><td align="left">XGBoost</td><td align="left">0.806</td><td align="left">75.69</td><td align="left">65.91</td><td align="left">82.54</td><td align="left">0.493</td><td align="left">72.50</td><td align="left">0.691</td></tr><tr><td align="left">LightGBM</td><td align="left">0.810</td><td align="left">74.77</td><td align="left">64.44</td><td align="left">82.26</td><td align="left">0.477</td><td align="left">72.50</td><td align="left">0.683</td></tr><tr><td align="left">CNN</td><td align="left">0.826</td><td align="left">77.91</td><td align="left">63.89</td><td align="left">88.00</td><td align="left">0.542</td><td align="left">79.31</td><td align="left">0.708</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par49">As can be seen from Table <xref rid="Tab6" ref-type="table">6</xref>, Deep-WET achieved the overall best performance compared with the compared ML classifiers in terms of almost performance metrics, with the only exception of the Sen. Meanwhile, CNN-fastText and CNN-Word2Vec were the second-best and third-best classifiers in terms of ACC. To be specific, the ACC values of Deep-WET, CNN-fastText, and CNN-Word2Vec were 82.56%, 80.37%, and 77.91%, respectively. In addition, Deep-WET’s AUC, ACC, Spe, and MCC were 3.40%, 2.19%, 2.34% and 3.30%, respectively, higher than the second-best method CNN-fastText. In case of the independent test results, Deep-WET still outperformed the compared ML classifiers in terms of ACC, Spe, MCC, Pre and F1. Deep-WET’s ACC, Spe, MCC, Pre, and F1 were 1.37%, 3.84%, 2.60%, 5.13% and 2.10%, respectively, higher than the second-best method CNN-fastText.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Independent test results of different ML classifiers and feature encoding schemes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">Classifier</th><th align="left">AUC</th><th align="left">ACC (%)</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left" rowspan="4">GloVe</td><td align="left">SVM</td><td align="left">0.803</td><td align="left">73.52</td><td align="left">76.25</td><td align="left">71.11</td><td align="left">0.473</td><td align="left">70.11</td><td align="left">0.731</td></tr><tr><td align="left">XGBoost</td><td align="left">0.787</td><td align="left">73.97</td><td align="left">79.41</td><td align="left">69.23</td><td align="left">0.486</td><td align="left">69.23</td><td align="left">0.740</td></tr><tr><td align="left">LightGBM</td><td align="left">0.802</td><td align="left">75.34</td><td align="left">78.37</td><td align="left">72.22</td><td align="left">0.507</td><td align="left">74.36</td><td align="left">0.763</td></tr><tr><td align="left">CNN (Deep-WET)</td><td align="left">0.805</td><td align="left">78.08</td><td align="left">78.05</td><td align="left">78.13</td><td align="left">0.559</td><td align="left">82.05</td><td align="left">0.800</td></tr><tr><td align="left" rowspan="4">fastText</td><td align="left">SVM</td><td align="left">0.812</td><td align="left">73.20</td><td align="left">78.26</td><td align="left">68.63</td><td align="left">0.470</td><td align="left">69.23</td><td align="left">0.735</td></tr><tr><td align="left">XGBoost</td><td align="left">0.804</td><td align="left">73.77</td><td align="left">75.41</td><td align="left">72.13</td><td align="left">0.476</td><td align="left">73.02</td><td align="left">0.742</td></tr><tr><td align="left">LightGBM</td><td align="left">0.783</td><td align="left">74.59</td><td align="left">72.22</td><td align="left">78.00</td><td align="left">0.494</td><td align="left">82.54</td><td align="left">0.770</td></tr><tr><td align="left">CNN</td><td align="left"><bold>0.816</bold></td><td align="left">76.71</td><td align="left">78.95</td><td align="left">74.29</td><td align="left">0.533</td><td align="left">76.92</td><td align="left">0.779</td></tr><tr><td align="left" rowspan="4">Word2Vec</td><td align="left">SVM</td><td align="left">0.778</td><td align="left">73.19</td><td align="left">66.67</td><td align="left">77.05</td><td align="left">0.433</td><td align="left">63.16</td><td align="left">0.649</td></tr><tr><td align="left">XGBoost</td><td align="left">0.773</td><td align="left">75.25</td><td align="left">69.44</td><td align="left">78.69</td><td align="left">0.476</td><td align="left">65.79</td><td align="left">0.676</td></tr><tr><td align="left">LightGBM</td><td align="left">0.797</td><td align="left">73.98</td><td align="left">81.08</td><td align="left">66.67</td><td align="left">0.483</td><td align="left">71.43</td><td align="left">0.760</td></tr><tr><td align="left">CNN</td><td align="left">0.807</td><td align="left">75.35</td><td align="left">74.42</td><td align="left">76.67</td><td align="left">0.504</td><td align="left">82.05</td><td align="left">0.781</td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p></table-wrap-foot></table-wrap></p>
      <p id="Par50">To further the comparison, Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref> illustrate the cross-validation and independent test performance for our proposed Deep-WET approach along with four robust classifiers with all the evaluation metrics. From Tables <xref rid="Tab7" ref-type="table">7</xref>, <xref rid="Tab8" ref-type="table">8</xref> and Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref>, we can summarize several observations as follows: (i) GloVe features obtained the highest predictive results as compared to fastText and Word2Vec; however, these three feature-encoding techniques all achieved promising performance for the CNN classifiers, followed by LightGBM, XGBoost, and SVM classifier. Word2Vec achieved relatively lower performance, whereas fastText was slightly better than Word2Vec, (ii) CNN classifier consistently achieved the highest results compared to the other three classifiers for all three feature-encoding techniques, and (iii) Finally, our proposed Deep-WET achieved better performance than other conventional ML classifiers, highlighting its superior discriminative power.</p>
    </sec>
    <sec id="Sec19">
      <title>Comparison of Deep-WET with the state-of-the-art methods</title>
      <p id="Par51">To further validate the discriminative power of Deep-WET method, we compared its prediction performance against other existing DBP methods, including DPP-PseAAC<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, PseDNA-Pro<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, iDNA-Prot<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, iDNA-Prot|dis<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, PSFM-DBT<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, Local-DPP<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, iDNAProt-ES<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, IKP-DBPPred<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> Xiuquan et al.<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, iDRBP-MMC<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> and TargetDBP<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, on the independent test data. The prediction performance of the existing methods were obtained by submitting protein sequences in the independent test dataset (148 DBPs and 148 non-DBPs) to their own webservers. Since the web sever of iDNAProt-ES was not functional, the prediction results of iDNAProt-ES were obtained from the reimplementation of iDNAProt-ES and the standalone version of HMMBinder<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, respectively. Table 8 shows the prediction results of Deep-WET and other existing methods.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Performance comparisons of DeepWET with the state-of-the-art methods on the independent test dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Predictor<sup>a</sup></th><th align="left">AUC</th><th align="left">Sen (%)</th><th align="left">Spe (%)</th><th align="left">MCC</th><th align="left">Pre (%)</th><th align="left">F1</th></tr></thead><tbody><tr><td align="left">DPP-PseAAC</td><td align="left">61.15</td><td align="left">55.41</td><td align="left">66.89</td><td align="left">0.225</td><td align="left">62.60</td><td align="left">0.588</td></tr><tr><td align="left">iDNA-Prot</td><td align="left">62.16</td><td align="left">63.51</td><td align="left">60.81</td><td align="left">0.243</td><td align="left">61.84</td><td align="left">0.627</td></tr><tr><td align="left">iDNA-Prot|dis</td><td align="left">68.24</td><td align="left">72.30</td><td align="left">64.19</td><td align="left">0.366</td><td align="left">66.88</td><td align="left">0.695</td></tr><tr><td align="left">PseDNA-Pro</td><td align="left">67.23</td><td align="left">78.38</td><td align="left">56.08</td><td align="left">0.354</td><td align="left">64.09</td><td align="left">0.705</td></tr><tr><td align="left">PSFM-DBT</td><td align="left">68.58</td><td align="left">71 .62</td><td align="left">65.54</td><td align="left">0.372</td><td align="left">67.52</td><td align="left">0.695</td></tr><tr><td align="left">IKP-DBPPred</td><td align="left">58.11</td><td align="left">52.70</td><td align="left">63.51</td><td align="left">0.163</td><td align="left">59.09</td><td align="left">0.557</td></tr><tr><td align="left">Local-DPP</td><td align="left">48.65</td><td align="left">3.38</td><td align="left"><bold>93.92</bold></td><td align="left">– 0.06</td><td align="left">35.71</td><td align="left">0.062</td></tr><tr><td align="left">iDNAProt-ES(on PDB1075)</td><td align="left">71.62</td><td align="left">91 .89</td><td align="left">51.35</td><td align="left">0.473</td><td align="left">65.38</td><td align="left">0.764</td></tr><tr><td align="left">TargetDBP</td><td align="left">76.69</td><td align="left">76.35</td><td align="left">77.03</td><td align="left">0.534</td><td align="left">76.87</td><td align="left">0.766</td></tr><tr><td align="left">Xiuquan et al.</td><td align="left">77</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">iDRBP-MMC</td><td align="left">70</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">Deep-WET</td><td align="left"><bold>78.08</bold></td><td align="left"><bold>78.05</bold></td><td align="left"><bold>78.13</bold></td><td align="left"><bold>0.559</bold></td><td align="left"><bold>82.05</bold></td><td align="left"><bold>0.800</bold></td></tr></tbody></table><table-wrap-foot><p>Significant values are in bold.</p><p><sup>a</sup>The prediction performance of the existing methods were obtained by submitting protein sequences in the independent test dataset (148 DBPs and 148 non-DBPs) to their own webservers.</p></table-wrap-foot></table-wrap></p>
      <p id="Par52">According to the F1 and MCC values, these two evaluation metrics of binary predictions, recorded in Table <xref rid="Tab8" ref-type="table">8</xref>, we can see that Deep-WET has superior performance over other exiting methods in terms of ACC, MCC, Pre, and F1. Notably, by comparing the proposed Deep-WET approach with the second-best predictor TargetDBP in terms of ACC, we observe that Deep-WET achieved improvements of 1.39%, 1.70%, 2.50%, 5.18%, and 3.40% on ACC, Sen, MCC, Pre, and F1, respectively. Although, iDNAProt-ES obtained the highest Sen value of 91.89%, this method provided the lowest Spe value of 51.35%. The main reason behind this high Sen is that iDNAProt-ES has lower false negative (FN) prediction. In contrast, Local-DPP obtained the highest prediction performance in terms of Spe (93.92%) and shows much lower Pre scores (35.71%) producing many false negatives (FN) during prediction, but Acc values of iDNAProt-ES and Local-DPP are lower than those of Deep-WET. Taken together, these results demonstrated that Deep-WET has a great potential for DBP prediction.</p>
    </sec>
    <sec id="Sec20">
      <title>Ablation study</title>
      <p id="Par53">Our CNN model has key components such as convolutional filters, pooling strategies, kernel sizes and fully connected layers, etc. Here, we have conducted ablation studies using the GloVe + fastText + Word2Vec dataset under 5-fold CV, assessing how each individual component influences the predictive performance of Deep-WET:<list list-type="bullet"><list-item><p id="Par54">Remove Specific Convolutional Filters (RSCF): we removed specific filters in the convolutional layers responsible for capturing sequence motifs or patterns linked to DNA binding.</p></list-item><list-item><p id="Par55">Variation in Pooling Strategies (VPS): adjust the baseline model by altering pooling strategies to evaluate how these changes affect the recognition of relevant sequence features.</p></list-item><list-item><p id="Par56">Variation in Kernel Sizes (VKS): Explore diverse kernel sizes within the convolutional layers to capture sequence motifs associated with DNA binding of different lengths.</p></list-item><list-item><p id="Par57">Removal of Fully Connected Layers (RFCL): create a modified version of the baseline model by removing one or more fully connected layers to examine the significance of global features in the classification task.</p></list-item></list></p>
      <p id="Par58">Figure <xref rid="Fig7" ref-type="fig">7</xref> show the performance comparison of Deep-WET and its four variants in terms of AUC on GloVe + fastText + Word2Vec dataset. We can observe that our CNN has better performance than CNN-RSCF, CNN-VPS, CNN-VKS and CNN-RFCL on experiment datasets Here, our-CNN obtains the best AUC score of 0.883, and it is 0.085%, 0.135%, 0.105% and 0.165% higher than that of CNN-RSCF, CNN-VPS, CNN- VKS and CNN-RFCL, respectively, which can illustrate that these parts in our design can improve the predictive performance. Among them, CNN-VPS and CNN-RFCL have the lowest performance. This shows that it is very important to perform the hyperparameters setting of CNN classifiers (see Table <xref rid="Tab1" ref-type="table">1</xref>), that can effectively improve the performance of CNN.<fig id="Fig7"><label>Figure 7</label><caption><p>Comparative analysis between Our CNN and its ablation experiments on the GloVe + fastText + Word2Vec dataset.</p></caption><graphic xlink:href="41598_2024_52653_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec21">
      <title>Web server, data and software availability</title>
      <p id="Par59">We used Apache (2.4.48), Python (3.8.0), and Laravel (8.16.1) to develop a web server for Deep-WET at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.000webhostapp.com/">https://deepwet-dna.000webhostapp.com/</ext-link>. Users can upload or input DNA binding protein sequences of viruses and humans in FASTA format to predict DBPs with probability scores. After clicking the submit button, the server will evaluate the protein sequence and check the format for processing. Prediction results will be generated in a tabular format with detailed information on the word serial number and predicted probability of DBPs and predicted class (DBPs/non-DBPs). Detailed instructions for the webserver can be found on the README option. After the final job, users will get a job ID to be used for further queries. The Deep-WET web server application stores this job ID for fifteen days. Deep-WET may have a long computational time when users input large protein sequences files, since Deep-WET needs to perform NLP-based word embedding packages to generate discriminative features and fix the suitable parameters for the CNN classifier to predict. We strongly suggest inputting a low number of DBP sequences at a time. The experimental datasets for this study are available at: <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.000webhostapp.com/data">https://deepwet-dna.000webhostapp.com/data</ext-link>.</p>
    </sec>
  </sec>
  <sec id="Sec22">
    <title>Conclusions</title>
    <p id="Par60">Identifying DBPs is vital to discovering fundamental protein-DNA mechanisms and understanding their biological interactions. Here, we develop a new deep learning-based approach termed Deep-WET,to achieve more accurate and improved prediction of DBPs. In Deep-WET, we extracted three NLP-based word embedding features to generate single features then combined them sequentially, and assigned weights learned through the use of the DE algorithm. The SHAP technique was utilized to gain the effective feature subsets, and a deep learning-based CNN algorithm was used as a model classifier for predicting DBPs. Comparative analysis on the independent test dataset showed that Deep-WET achieved improved performance compared with conventional ML classifiers and the existing methods, highlight the effectiveness and robustnessof the proposed Deep-WET. The improved performance of Deep-WET is mainly due to the utilization of NLP-based word embedding features that can effectively capture the characteristics of DBPs. A user-friendly web server for Deep-WET is available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.royalit.agency/">https://deepwet-dna.royalit.agency/</ext-link>. Deep-WET is anticipated to be a powerful tool to serve the community-wide effort for the accurate and large-scale identification of potential DBPs from sequences information. Deep learning shows advanced prediction abilities in various fields of computational biology such as hERG blockers<sup><xref ref-type="bibr" rid="CR57">57</xref></sup> , disease-related metabolites<sup><xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR59">59</xref></sup>, single-cell<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> and human lncRNA-miRNA interactions<sup><xref ref-type="bibr" rid="CR61">61</xref>,<xref ref-type="bibr" rid="CR62">62</xref></sup>. Most studies propose deep learning based models<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> for prediction tasks. To enhance the predictive capabilities of our Deep-WET model, our future efforts will focus on three key areas: (1) although our NLP-based feature extraction is now commonly used for extracting distinct features, there may be some limitations, such as ambiguities, lexical gaps, and structural gaps. It would be interesting to use deep learning-based autoencoders<sup><xref ref-type="bibr" rid="CR59">59</xref>,<xref ref-type="bibr" rid="CR60">60</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup> to effectively convey the hidden information within DBPs sequences; (2) implementing a small-loss approach and integrating probabilistic local outlier factor (pLOF) with the extracted features to tackle the challenge of label noise in the dataset, ensuring a trustworthy application; (3) developing a graph-based deep learning model for predicting DBPs with unknown structures.</p>
    <p id="Par61">Cellular death is a fundamental and complex biological process that is an underlying driver for many diseases. Authors in<sup><xref ref-type="bibr" rid="CR65">65</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup>, worked for cell death. Our CNN model can be used to classify cells undergoing cell death. This deep learning network has the ability to highly predict cell death. Finally, it is possible to provide a simple Python tool that can be broadly used to detect cell death. Furthermore, our CNN model can recommend specific drugs for the disease.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported by the TM R&amp;D Fund (Project no. RDTC/221054 and SAP ID: MMUE/220023) and the Multimedia University (MMU) IR Fund (Project ID MMUI/220041).</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>Project administration and supervision: K.O.M.G., W.S.; conceptualization, investigation, methodology and visualization: S.M. H.M., M.F.H. and D.N.; analysis, validation and software: S.M.H.M. and M.F.H.; web server development: M.F.H.; writing—original draft: S.M.H.M.; writing—review and editing: S.M.H.M., W.S., K.O.M.G., and D.N. All authors reviewed and approved the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>All the data used in this study are available at <ext-link ext-link-type="uri" xlink:href="https://deepwet-dna.monarcatechnical.com/data">https://deepwet-dna.monarcatechnical.com/data</ext-link>.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par62">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>J-M</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>X-P</given-names>
          </name>
        </person-group>
        <article-title>Competitive aptamer bioassay for selective detection of adenosine triphosphate based on metal-paired molecular conformational switch and fluorescent gold nanoclusters</article-title>
        <source>Biosens. Bioelectron.</source>
        <year>2012</year>
        <volume>36</volume>
        <fpage>135</fpage>
        <lpage>141</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bios.2012.04.015</pub-id>
        <?supplied-pmid 22560440?>
        <pub-id pub-id-type="pmid">22560440</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ren</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Genome-wide location and function of dna binding proteins</article-title>
        <source>Science</source>
        <year>2000</year>
        <volume>290</volume>
        <fpage>2306</fpage>
        <lpage>2309</lpage>
        <pub-id pub-id-type="doi">10.1126/science.290.5500.2306</pub-id>
        <?supplied-pmid 11125145?>
        <pub-id pub-id-type="pmid">11125145</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gurova</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>New hopes from old drugs: Revisiting dna-binding small molecules as anticancer agents</article-title>
        <source>Future Oncol.</source>
        <year>2009</year>
        <volume>5</volume>
        <fpage>1685</fpage>
        <lpage>1704</lpage>
        <pub-id pub-id-type="doi">10.2217/fon.09.127</pub-id>
        <?supplied-pmid 20001804?>
        <pub-id pub-id-type="pmid">20001804</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leung</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>DS-H</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>VP-Y</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>D-L</given-names>
          </name>
        </person-group>
        <article-title>Dna-binding small molecules as inhibitors of transcription factors</article-title>
        <source>Med. Res. Rev.</source>
        <year>2013</year>
        <volume>33</volume>
        <fpage>823</fpage>
        <lpage>846</lpage>
        <pub-id pub-id-type="doi">10.1002/med.21266</pub-id>
        <?supplied-pmid 22549740?>
        <pub-id pub-id-type="pmid">22549740</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eisenberg</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Marcotte</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Xenarios</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Yeates</surname>
            <given-names>TO</given-names>
          </name>
        </person-group>
        <article-title>Protein function in the post-genomic era</article-title>
        <source>Nature</source>
        <year>2000</year>
        <volume>405</volume>
        <fpage>823</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="doi">10.1038/35015694</pub-id>
        <?supplied-pmid 10866208?>
        <pub-id pub-id-type="pmid">10866208</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Predicting dna-binding proteins: Approached from chou’s pseudo amino acid composition and other specific sequence features</article-title>
        <source>Amino Acids</source>
        <year>2008</year>
        <volume>34</volume>
        <fpage>103</fpage>
        <lpage>109</lpage>
        <pub-id pub-id-type="doi">10.1007/s00726-007-0568-2</pub-id>
        <?supplied-pmid 17624492?>
        <pub-id pub-id-type="pmid">17624492</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chowdhury</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Shatabda</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>idnaprot-es: Identification of dna-binding proteins using evolutionary and structural features</article-title>
        <source>Sci. Rep.</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>14938</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-14945-1</pub-id>
        <?supplied-pmid 29097781?>
        <pub-id pub-id-type="pmid">29097781</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>endna-prot: Identification of dna-binding proteins by applying ensemble learning</article-title>
        <source>BioMed Res. Int.</source>
        <year>2014</year>
        <pub-id pub-id-type="doi">10.1155/2014/294279</pub-id>
        <?supplied-pmid 25580429?>
        <pub-id pub-id-type="pmid">25580429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identifying dna-binding proteins by combining support vector machine and pssm distance transformation</article-title>
        <source>BMC Syst. Biol.</source>
        <year>2015</year>
        <volume>9</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1186/1752-0509-9-S1-S10</pub-id>
        <?supplied-pmid 25582171?>
        <pub-id pub-id-type="pmid">25582171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rahman</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Shatabda</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Saha</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kaykobad</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>MS</given-names>
          </name>
        </person-group>
        <article-title>Dpp-pseaac: A dna-binding protein prediction model using Chou’s general pseaac</article-title>
        <source>J. Theor. Biol.</source>
        <year>2018</year>
        <volume>452</volume>
        <fpage>22</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jtbi.2018.05.006</pub-id>
        <?supplied-pmid 29753757?>
        <pub-id pub-id-type="pmid">29753757</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hwang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Kuznetsov</surname>
            <given-names>IB</given-names>
          </name>
        </person-group>
        <article-title>Dp-bind: A web server for sequence-based prediction of dna-binding residues in dna-binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <fpage>634</fpage>
        <lpage>636</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl672</pub-id>
        <?supplied-pmid 17237068?>
        <pub-id pub-id-type="pmid">17237068</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lou</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sequence based prediction of dna-binding proteins based on hybrid feature selection using random forest and gaussian naive bayes</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e86703</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0086703</pub-id>
        <?supplied-pmid 24475169?>
        <pub-id pub-id-type="pmid">24475169</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Improved detection of dna-binding proteins via compression technology on pssm information</article-title>
        <source>PLoS ONE</source>
        <year>2017</year>
        <volume>12</volume>
        <fpage>e0185587</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0185587</pub-id>
        <?supplied-pmid 28961273?>
        <pub-id pub-id-type="pmid">28961273</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>idna-prot| dis: Identifying dna-binding proteins by incorporating amino acid distance-pairs and reduced alphabet profile into the general pseudo amino acid composition</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e106691</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0106691</pub-id>
        <?supplied-pmid 25184541?>
        <pub-id pub-id-type="pmid">25184541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>X-W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X-T</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Z-Q</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>M-H</given-names>
          </name>
        </person-group>
        <article-title>Identify dna-binding proteins with optimal chou’s amino acid composition</article-title>
        <source>Protein Peptid. Lett.</source>
        <year>2012</year>
        <volume>19</volume>
        <fpage>398</fpage>
        <lpage>405</lpage>
        <pub-id pub-id-type="doi">10.2174/092986612799789404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ahmad</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gromiha</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Sarai</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Analysis and prediction of dna-binding proteins and their binding residues based on composition, sequence and structural information</article-title>
        <source>Bioinformatics</source>
        <year>2004</year>
        <volume>20</volume>
        <fpage>477</fpage>
        <lpage>486</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg432</pub-id>
        <?supplied-pmid 14990443?>
        <pub-id pub-id-type="pmid">14990443</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of dna-binding proteins by incorporating evolutionary information into pseudo amino acid composition via the top-n-gram approach</article-title>
        <source>J. Biomol. Struct. Dyn.</source>
        <year>2015</year>
        <volume>33</volume>
        <fpage>1720</fpage>
        <lpage>1730</lpage>
        <pub-id pub-id-type="doi">10.1080/07391102.2014.968624</pub-id>
        <?supplied-pmid 25252709?>
        <pub-id pub-id-type="pmid">25252709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>W-Z</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>J-A</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>idna-prot: Identification of dna binding proteins using random forest with grey model</article-title>
        <source>PLoS ONE</source>
        <year>2011</year>
        <volume>6</volume>
        <fpage>e24756</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0024756</pub-id>
        <?supplied-pmid 21935457?>
        <pub-id pub-id-type="pmid">21935457</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Psedna-pro: Dna-binding protein identification by combining chou’s pseaac and physicochemical distance transformation</article-title>
        <source>Mol. Inf.</source>
        <year>2015</year>
        <volume>34</volume>
        <fpage>8</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1002/minf.201400025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Dna binding protein identification by combining pseudo amino acid composition and profile-based protein representation</article-title>
        <source>Sci. Rep.</source>
        <year>2015</year>
        <volume>5</volume>
        <fpage>15479</fpage>
        <pub-id pub-id-type="doi">10.1038/srep15479</pub-id>
        <?supplied-pmid 26482832?>
        <pub-id pub-id-type="pmid">26482832</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Local-dpp: An improved dna-binding protein prediction method by exploring local evolutionary information</article-title>
        <source>Inf. Sci.</source>
        <year>2017</year>
        <volume>384</volume>
        <fpage>135</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2016.06.026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Psfm-dbt: Identifying dna-binding proteins by combing position specific frequency matrix and distance-bigram transformation</article-title>
        <source>Int. J. Mol. Sci.</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>1856</fpage>
        <pub-id pub-id-type="doi">10.3390/ijms18091856</pub-id>
        <?supplied-pmid 28841194?>
        <pub-id pub-id-type="pmid">28841194</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zaman</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hmmbinder: Dna-binding protein prediction using hmm profile based features</article-title>
        <source>BioMed Res. Int.</source>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.1155/2017/4590609</pub-id>
        <?supplied-pmid 29270430?>
        <pub-id pub-id-type="pmid">29270430</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Identification of dna-binding proteins using mixed feature representation methods</article-title>
        <source>Molecules</source>
        <year>2017</year>
        <volume>22</volume>
        <fpage>1602</fpage>
        <pub-id pub-id-type="doi">10.3390/molecules22101602</pub-id>
        <?supplied-pmid 28937647?>
        <pub-id pub-id-type="pmid">28937647</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X-G</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y-H</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>D-J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>G-J</given-names>
          </name>
        </person-group>
        <article-title>Targetdbp: Accurate dna-binding protein prediction via sequence-based multi-view feature learning</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2019</year>
        <volume>17</volume>
        <fpage>1419</fpage>
        <lpage>1429</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2019.2893634</pub-id>
        <?supplied-pmid 30668479?>
        <pub-id pub-id-type="pmid">30668479</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Skolnick</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Dbd-hunter: A knowledge-based method for the prediction of dna–protein interactions</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2008</year>
        <volume>36</volume>
        <fpage>3978</fpage>
        <lpage>3992</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkn332</pub-id>
        <?supplied-pmid 18515839?>
        <pub-id pub-id-type="pmid">18515839</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nimrod</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Schushan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Szilágyi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Leslie</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ben-Tal</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>idbps: A web server for the identification of dna binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <fpage>692</fpage>
        <lpage>693</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq019</pub-id>
        <?supplied-pmid 20089514?>
        <pub-id pub-id-type="pmid">20089514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Predicting dna-binding proteins and binding residues by complex structure prediction and application to human proteome</article-title>
        <source>PLoS ONE</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e96694</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0096694</pub-id>
        <?supplied-pmid 24792350?>
        <pub-id pub-id-type="pmid">24792350</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The i-tasser suite: Protein structure and function prediction</article-title>
        <source>Nat. Methods</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>7</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3213</pub-id>
        <?supplied-pmid 25549265?>
        <pub-id pub-id-type="pmid">25549265</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nanni</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Brahnam</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Set of approaches based on 3d structure and position specific-scoring matrix for predicting dna-binding proteins</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <fpage>1844</fpage>
        <lpage>1851</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty912</pub-id>
        <?supplied-pmid 30395157?>
        <pub-id pub-id-type="pmid">30395157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sang</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hmmpred: Accurate prediction of dna-binding proteins based on hmm profiles and xgboost feature selection</article-title>
        <source>Comput. Math. Methods Med.</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1155/2020/1384749</pub-id>
        <?supplied-pmid 33133226?>
        <pub-id pub-id-type="pmid">33133226</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>Y-H</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>X-N</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>D-J</given-names>
          </name>
        </person-group>
        <article-title>Dnapred: Accurate identification of dna-binding sites from protein sequence by ensembled hyperplane-distance-based support vector machines</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2019</year>
        <volume>59</volume>
        <fpage>3057</fpage>
        <lpage>3071</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00749</pub-id>
        <?supplied-pmid 30943723?>
        <pub-id pub-id-type="pmid">30943723</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Q</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Stackpdb: Predicting dna-binding proteins based on xgb-rfe feature optimization and stacked ensemble classifier</article-title>
        <source>Appl. Soft Comput.</source>
        <year>2021</year>
        <volume>99</volume>
        <fpage>106921</fpage>
        <pub-id pub-id-type="doi">10.1016/j.asoc.2020.106921</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rose</surname>
            <given-names>PW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The rcsb protein data bank: Views of structural biology for basic and applied research and education</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2015</year>
        <volume>43</volume>
        <fpage>D345</fpage>
        <lpage>D356</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku1214</pub-id>
        <?supplied-pmid 25428375?>
        <pub-id pub-id-type="pmid">25428375</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Pennington, J., Socher, R. &amp; Manning, C. D. Glove: Global vectors for word representation. in <italic>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, 1532–1543 (2014).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Distributed representations of words and phrases and their compositionality</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2013</year>
        <volume>26</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bojanowski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Grave</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Joulin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Enriching word vectors with subword information</article-title>
        <source>Trans. Assoc. Comput. Ling.</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>135</fpage>
        <lpage>146</lpage>
        <pub-id pub-id-type="doi">10.1162/tacl_a_00051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Joulin, A. <italic>et al.</italic> Fasttext.zip: Compressing text classification models. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1612.03651">http://arxiv.org/abs/1612.03651</ext-link>10.48550/arXiv.1612.03651 (2016).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Joulin, A., Grave, E., Bojanowski, P. &amp; Mikolov, T. Bag of tricks for efficient text classification. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1607.01759">http://arxiv.org/abs/1607.01759</ext-link>10.48550/arXiv.1607.01759 (2016).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support-vector networks</article-title>
        <source>Mach. Learn.</source>
        <year>1995</year>
        <volume>20</volume>
        <fpage>273</fpage>
        <lpage>297</lpage>
        <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Chen, T. &amp; Guestrin, C. Xgboost: A scalable tree boosting system. in <italic>Proceedings of the 22nd ACM sigkdd International Conference on Knowledge Discovery and Data Mining</italic>, 785–794. 10.1145/2939672.2939785 (2016).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lundberg</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S-I</given-names>
          </name>
        </person-group>
        <article-title>A unified approach to interpreting model predictions</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2017</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1705.07874</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parsa</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Movahedi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taghipour</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Derrible</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mohammadian</surname>
            <given-names>AK</given-names>
          </name>
        </person-group>
        <article-title>Toward safer highways, application of xgboost and shap for real-time accident detection and feature analysis</article-title>
        <source>Accid. Anal. Prev.</source>
        <year>2020</year>
        <volume>136</volume>
        <fpage>105405</fpage>
        <pub-id pub-id-type="doi">10.1016/j.aap.2019.105405</pub-id>
        <?supplied-pmid 31864931?>
        <pub-id pub-id-type="pmid">31864931</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ke</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Lightgbm: A highly efficient gradient boosting decision tree</article-title>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grinblat</surname>
            <given-names>GL</given-names>
          </name>
          <name>
            <surname>Uzal</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Larese</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Granitto</surname>
            <given-names>PM</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for plant identification using vein morphological patterns</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2016</year>
        <volume>127</volume>
        <fpage>418</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2016.07.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yen</surname>
            <given-names>S-J</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Y-S</given-names>
          </name>
        </person-group>
        <article-title>Cluster-based under-sampling approaches for imbalanced data distributions</article-title>
        <source>Expert Syst. Appl.</source>
        <year>2009</year>
        <volume>36</volume>
        <fpage>5718</fpage>
        <lpage>5727</lpage>
        <pub-id pub-id-type="doi">10.1016/j.eswa.2008.06.108</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wiatowski</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bölcskei</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>A mathematical theory of deep convolutional neural networks for feature extraction</article-title>
        <source>IEEE Trans. Inf. Theor.</source>
        <year>2017</year>
        <volume>64</volume>
        <fpage>1845</fpage>
        <lpage>1866</lpage>
        <pub-id pub-id-type="doi">10.1109/TIT.2017.2776228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hunter</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>Matplotlib: A 2d graphics environment</article-title>
        <source>Comput. Sci. Eng.</source>
        <year>2007</year>
        <volume>9</volume>
        <fpage>90</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waskom</surname>
            <given-names>ML</given-names>
          </name>
        </person-group>
        <article-title>Seaborn: Statistical data visualization</article-title>
        <source>J. Open Source Softw.</source>
        <year>2021</year>
        <volume>6</volume>
        <fpage>3021</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lumley</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Interactive Visualization of Climate Change: Characteristics, Intentions, and Metrics for Success</source>
        <year>2021</year>
        <publisher-name>McGill University</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guyon</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Barnhill</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Gene selection for cancer classification using support vector machines</article-title>
        <source>Mach. Learn.</source>
        <year>2002</year>
        <volume>46</volume>
        <fpage>389</fpage>
        <lpage>422</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regression shrinkage and selection via the lasso</article-title>
        <source>J. R. Stat. Soc. B</source>
        <year>1996</year>
        <volume>58</volume>
        <fpage>267</fpage>
        <lpage>288</lpage>
        <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A convolutional neural network system to discriminate drug-target interactions</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2019</year>
        <volume>18</volume>
        <fpage>1315</fpage>
        <lpage>1324</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2019.2940187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Du</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Deep multi-label joint learning for rna and dna-binding proteins prediction</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2022</year>
        <volume>20</volume>
        <fpage>307</fpage>
        <lpage>320</lpage>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>idrbp\_mmc: Identifying dna-binding proteins and rna-binding proteins based on multi-label learning model and motif-based convolutional neural network</article-title>
        <source>J. Mol. Biol.</source>
        <year>2020</year>
        <volume>432</volume>
        <fpage>5860</fpage>
        <lpage>5875</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2020.09.008</pub-id>
        <?supplied-pmid 32920048?>
        <pub-id pub-id-type="pmid">32920048</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Investigating cardiotoxicity related with herg channel blockers using molecular fingerprints and graph attention mechanism</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>153</volume>
        <fpage>106464</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.106464</pub-id>
        <?supplied-pmid 36584603?>
        <pub-id pub-id-type="pmid">36584603</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>A deep learning method for predicting metabolite-disease associations via graph neural network</article-title>
        <source>Brief. Bioinform.</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>266</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbac266</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gao</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting metabolite-disease associations based on auto-encoder and non-negative matrix factorization</article-title>
        <source>Brief. Bioinform.</source>
        <year>2023</year>
        <volume>24</volume>
        <fpage>259</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbad259</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene function and cell surface protein association analysis based on single-cell multiomics data</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>157</volume>
        <fpage>106733</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.106733</pub-id>
        <?supplied-pmid 36924730?>
        <pub-id pub-id-type="pmid">36924730</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Shuai</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Predicting the potential human lncrna–mirna interactions based on graph convolution network with conditional random field</article-title>
        <source>Brief. Bioinform.</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>463</fpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbac463</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Using network distance analysis to predict lncrna–mirna interactions</article-title>
        <source>Interdiscipl. Sci. Comput. Life Sci.</source>
        <year>2021</year>
        <volume>13</volume>
        <fpage>535</fpage>
        <lpage>545</lpage>
        <pub-id pub-id-type="doi">10.1007/s12539-021-00458-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dcamcp: A deep learning model based on capsule network and attention mechanism for molecular carcinogenicity prediction</article-title>
        <source>J. Cell. Mol. Med.</source>
        <year>2023</year>
        <volume>27</volume>
        <fpage>3117</fpage>
        <lpage>3126</lpage>
        <pub-id pub-id-type="doi">10.1111/jcmm.17889</pub-id>
        <?supplied-pmid 37525507?>
        <pub-id pub-id-type="pmid">37525507</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meng</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>scaaga: Single cell data analysis framework using asymmetric autoencoder with gene attention</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2023</year>
        <volume>165</volume>
        <fpage>107414</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.107414</pub-id>
        <?supplied-pmid 37660567?>
        <pub-id pub-id-type="pmid">37660567</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Rip1-dependent linear and nonlinear recruitments of caspase-8 and rip3 respectively to necrosome specify distinct cell death outcomes</article-title>
        <source>Protein Cell</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>858</fpage>
        <lpage>876</lpage>
        <pub-id pub-id-type="doi">10.1007/s13238-020-00810-x</pub-id>
        <?supplied-pmid 33389663?>
        <pub-id pub-id-type="pmid">33389663</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shuai</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Quantifying the underlying landscape, entropy production and biological path of the cell fate decision between apoptosis and pyroptosis</article-title>
        <source>Chaos Solitons Fract.</source>
        <year>2024</year>
        <volume>178</volume>
        <fpage>114328</fpage>
        <pub-id pub-id-type="doi">10.1016/j.chaos.2023.114328</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
