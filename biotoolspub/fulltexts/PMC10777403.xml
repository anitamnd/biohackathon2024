<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="publisher-id">ci</journal-id>
    <journal-id journal-id-type="coden">jcisd8</journal-id>
    <journal-title-group>
      <journal-title>Journal of Chemical Information and Modeling</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1549-9596</issn>
    <issn pub-type="epub">1549-960X</issn>
    <publisher>
      <publisher-name>American Chemical Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10777403</article-id>
    <article-id pub-id-type="pmid">38147829</article-id>
    <article-id pub-id-type="doi">10.1021/acs.jcim.3c01250</article-id>
    <article-categories>
      <subj-group>
        <subject>Application Note</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Chemprop: A Machine Learning Package for Chemical
Property Prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="ath1">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8404-6596</contrib-id>
        <name>
          <surname>Heid</surname>
          <given-names>Esther</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath2">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6466-1401</contrib-id>
        <name>
          <surname>Greenman</surname>
          <given-names>Kevin P.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath3">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3097-010X</contrib-id>
        <name>
          <surname>Chung</surname>
          <given-names>Yunsie</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath4">
        <name>
          <surname>Li</surname>
          <given-names>Shih-Cheng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff3" ref-type="aff">§</xref>
      </contrib>
      <contrib contrib-type="author" id="ath5">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1250-3329</contrib-id>
        <name>
          <surname>Graff</surname>
          <given-names>David E.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff4" ref-type="aff">∥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath6">
        <name>
          <surname>Vermeire</surname>
          <given-names>Florence H.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff5" ref-type="aff">⊥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath7">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0644-7554</contrib-id>
        <name>
          <surname>Wu</surname>
          <given-names>Haoyang</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath8">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2603-9694</contrib-id>
        <name>
          <surname>Green</surname>
          <given-names>William H.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath9">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2704-7717</contrib-id>
        <name>
          <surname>McGill</surname>
          <given-names>Charles J.</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff6" ref-type="aff">#</xref>
      </contrib>
      <aff id="aff1"><label>†</label>Department
of Chemical Engineering, <institution>Massachusetts Institute
of Technology</institution>, Cambridge, Massachusetts 02139, <country>United States</country></aff>
      <aff id="aff2"><label>‡</label><institution>Institute
of Materials Chemistry</institution>, TU Wien, 1060 Vienna, <country>Austria</country></aff>
      <aff id="aff3"><label>§</label>Department
of Chemical Engineering, <institution>National Taiwan
University</institution>, Taipei 10617, <country>Taiwan</country></aff>
      <aff id="aff4"><label>∥</label>Department
of Chemistry and Chemical Biology, <institution>Harvard
University</institution>, Cambridge, Massachusetts 02138, <country>United States</country></aff>
      <aff id="aff5"><label>⊥</label>Department
of Chemical Engineering, <institution>KU Leuven</institution>, Celestijnenlaan 200F, B-3001 Leuven, <country>Belgium</country></aff>
      <aff id="aff6"><label>#</label>Department
of Chemical and Life Science Engineering, <institution>Virginia Commonwealth University</institution>, Richmond, Virginia 23284, <country>United States</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>E-mail: <email>mcgillc2@vcu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>26</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <day>08</day>
      <month>01</month>
      <year>2024</year>
    </pub-date>
    <volume>64</volume>
    <issue>1</issue>
    <fpage>9</fpage>
    <lpage>17</lpage>
    <history>
      <date date-type="received">
        <day>08</day>
        <month>08</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>04</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by American Chemical Society</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>The Authors</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Permits the broadest form of re-use including for commercial purposes, provided that author attribution and integrity are maintained (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0004" id="ab-tgr1"/>
      </p>
      <p>Deep learning has
become a powerful and frequently employed tool
for the prediction of molecular properties, thus creating a need for
open-source and versatile software solutions that can be operated
by nonexperts. Among the current approaches, directed message-passing
neural networks (D-MPNNs) have proven to perform well on a variety
of property prediction tasks. The software package Chemprop implements
the D-MPNN architecture and offers simple, easy, and fast access to
machine-learned molecular properties. Compared to its initial version,
we present a multitude of new Chemprop functionalities such as the
support of multimolecule properties, reactions, atom/bond-level properties,
and spectra. Further, we incorporate various uncertainty quantification
and calibration methods along with related metrics as well as pretraining
and transfer learning workflows, improved hyperparameter optimization,
and other customization options concerning loss functions or atom/bond
features. We benchmark D-MPNN models trained using Chemprop with the
new reaction, atom-level, and spectra functionality on a variety of
property prediction data sets, including MoleculeNet and SAMPL, and
observe state-of-the-art performance on the prediction of water-octanol
partition coefficients, reaction barrier heights, atomic partial charges,
and absorption spectra. Chemprop enables out-of-the-box training of
D-MPNN models for a variety of problem settings in fast, user-friendly,
and open-source software.</p>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Division of Graduate Education</institution>
            <institution-id institution-id-type="doi">10.13039/100000082</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>1745302</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>MIT-IBM Watson Lab</institution>
            <institution-id institution-id-type="doi">NA</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>KU Leuven</institution>
            <institution-id institution-id-type="doi">10.13039/501100004040</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>STG/22/032</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Austrian Science Fund</institution>
            <institution-id institution-id-type="doi">10.13039/501100002428</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>J-4415</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Virginia Commonwealth University</institution>
            <institution-id institution-id-type="doi">10.13039/100009238</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Massachusetts Institute of Technology</institution>
            <institution-id institution-id-type="doi">10.13039/100006919</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Defense Sciences Office, DARPA</institution>
            <institution-id institution-id-type="doi">10.13039/100006502</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>HR00111920025</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ci3c01250</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ci3c01250</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p>Machine learning in general
and especially deep learning has become
a powerful tool in various fields of chemistry. Applications range
from the prediction of physicochemical<sup><xref ref-type="bibr" rid="ref1">1</xref>−<xref ref-type="bibr" rid="ref9">9</xref></sup> and pharmacological<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> properties of molecules
to the design of molecules or materials with certain properties,<sup><xref ref-type="bibr" rid="ref11">11</xref>−<xref ref-type="bibr" rid="ref13">13</xref></sup> the exploration of chemical synthesis pathways,<sup><xref ref-type="bibr" rid="ref14">14</xref>−<xref ref-type="bibr" rid="ref27">27</xref></sup> or the prediction of properties important for chemical analysis
like IR,<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> UV/vis,<sup><xref ref-type="bibr" rid="ref29">29</xref></sup> or mass spectra.<sup><xref ref-type="bibr" rid="ref30">30</xref>−<xref ref-type="bibr" rid="ref33">33</xref></sup></p>
    <p>Many combinations of molecular representations and model architectures
have been developed to extract features from molecules and predict
molecular properties. Molecules can be represented as graphs, strings,
precomputed feature vectors, or sets of atomic coordinates and processed
using graph-convolutional neural networks, transformers, or feed-forward
neural networks to train predictive models. While early works focused
on handmade features or simple fingerprinting methods combined with
kernel regression or neural networks,<sup><xref ref-type="bibr" rid="ref34">34</xref></sup> the current state-of-the-art has shifted to end-to-end trainable
models which directly learn to extract their own features.<sup><xref ref-type="bibr" rid="ref35">35</xref></sup> Here, the models can achieve extreme complexity
based on the mechanisms of information exchange between parts of the
molecule. For example, graph convolutional neural networks (GCNNs)
extract local information from the molecular graph for single or small
groups of atoms and use that information to update the immediate neighborhood.<sup><xref ref-type="bibr" rid="ref1">1</xref>−<xref ref-type="bibr" rid="ref3">3</xref>,<xref ref-type="bibr" rid="ref36">36</xref></sup> They offer robust performance
for properties dependent on the local structure and if the three-dimensional
conformation of a molecule is not known or not relevant for a prediction
task. Graph attention transformers allow for a less local information
exchange via attention layers, which learn to accumulate the features
of atoms both close and far away in the graph.<sup><xref ref-type="bibr" rid="ref37">37</xref>,<xref ref-type="bibr" rid="ref38">38</xref></sup> Another important line of research comprises the prediction of properties
dependent on the three-dimensional conformation of a molecule, such
as the prediction of properties obtained from quantum mechanics.<sup><xref ref-type="bibr" rid="ref2">2</xref>,<xref ref-type="bibr" rid="ref39">39</xref>−<xref ref-type="bibr" rid="ref41">41</xref></sup> Finally, transformer models from natural language
processing can be trained on string representations such as SMILES
or SELFIES, also leading to promising results.<sup><xref ref-type="bibr" rid="ref42">42</xref>−<xref ref-type="bibr" rid="ref45">45</xref></sup> In this work, we discuss our
application of GCNNs, namely, Chemprop,<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> a directed-message passing algorithm derived from the seminal work
of Gilmer et al.<sup><xref ref-type="bibr" rid="ref1">1</xref></sup></p>
    <p>An early version
of Chemprop was published in ref (<xref ref-type="bibr" rid="ref36">36</xref>). Since then, the software
has substantially evolved and now includes a vast collection of new
features. For example, Chemprop is now able to predict properties
for systems containing multiple molecules, such as solute/solvent
combinations or reactions with and without solvent. It can train on
molecular targets, spectra, or atom/bond-level targets and output
the latent representation for analysis of the learned feature embedding.
Available uncertainty metrics include popular approaches, such as
ensembling, mean-variance estimation, and evidential learning. Chemprop
is thus a general and versatile deep learning toolbox and enjoys a
wide user base.</p>
    <p>The remainder of the article is structured as
follows: First, we
summarize the architecture of Chemprop. We discuss a selection of
Chemprop features with a focus on features introduced after the initial
release of Chemprop. We then conclude the main body of the article
and provide details on the data and software, which we have open-sourced
including all scripts to allow for full reproducibility. Alongside
the main body of this article, we provide <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Supporting Information</ext-link> that contains further model design details; descriptions
of the data acquisition, preprocessing, and splitting of all data
sets used in benchmarking; and the results of Chemprop benchmarks
on a variety of data sets showcasing its performance on both simple
and advanced prediction tasks.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Model Structure</title>
    <p>Chemprop consists of four modules: (1) a local features encoding
function, (2) a directed message passing neural network (D-MPNN) to
learn atomic embeddings from the local features, (3) an aggregation
function to join atomic embeddings into molecular embeddings, and
(4) a standard feed-forward neural network (FFN) for the transformation
of molecular embeddings to target properties, summarized in <xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>. The D-MPNN is a
class of graph-convolutional neural networks (GCNN), which updates
hidden representations of the vertices <italic>V</italic> and edges <italic>E</italic> of a graph <italic>G</italic> based on the local environment.
In the following, we use bold lower case to denote vectors, bold upper
case to denote matrices, and italic light font for scalars and objects.</p>
    <fig id="fig1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Overview
of the architecture of Chemprop. The message passing update
of the hidden vector for directed edge 2 → 1 is expanded for
demonstration.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0001" id="gr1" position="float"/>
    </fig>
    <p>For a molecule, the molecule SMILES
string is used as input, which
is then transformed to a molecular graph using RDKit,<sup><xref ref-type="bibr" rid="ref46">46</xref></sup> where atoms correspond to vertices and bonds to edges.
Initial features are constructed based on the identity and topology
of each atom and bond. For each vertex <italic>v</italic>, initial
feature vectors {<bold>x</bold><sub><italic>v</italic></sub> |<italic>v</italic> ∈ <italic>V</italic>} are obtained from a one-hot
encoding of the atomic number, number of bonds linked to each atom,
formal charge, chirality (if encoded in the SMILES), number of hydrogens,
hybridization, and aromaticity of the atom, as well as the atomic
mass (divided by 100 for scaling). For each edge <italic>e</italic>, initial feature vectors {<bold>e</bold><sub><italic>vw</italic></sub>|{<italic>v</italic>, <italic>w</italic>} ∈ <italic>E</italic>} arise from the bond type, whether the bond is conjugated
or in a ring, and whether it contains stereochemical information,
such as a cis/trans double bond. The D-MPNN uses directed edges in
a graph to pass information, where each undirected edge (bond) has
two corresponding directed edges, one in each direction. Initial directed
edge features <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> are obtained via simple concatenation
of the atom features of the first atom of a bond <bold>x</bold><sub><italic>v</italic></sub> to the respective undirected bond features <bold>e</bold><sub><italic>vw</italic></sub><disp-formula id="eq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m001" position="anchor"/><label>1</label></disp-formula>where cat() denotes simple concatenation.
The directed edges <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> and <bold>e</bold><sub arrange="stack"><italic>wv</italic></sub><sup arrange="stack">d</sup> are distinguished only by the choice of which atom to use in <xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>. Chemprop also offers
the option to read in custom atom and bond features in addition to
or as a replacement for the default features, as described in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section S1.2, and thus offers full control
of the initial features. In summary, Module 1 of Chemprop constructs
atom and directed bond feature vectors <bold>x</bold><sub><italic>v</italic></sub> and <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> from the input molecules.</p>
    <p>The initial atom and bond features are then passed to a D-MPNN.
In a D-MPNN structure, messages are passed between directed edges
rather than between nodes as would be done in a traditional MPNN.
To construct the hidden directed edge features <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">0</sup> of hidden size <italic>h</italic>, the initial directed edge features <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> are passed through a single neural network
layer with learnable weights <inline-formula id="d33e432"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m002.gif"/></inline-formula><disp-formula id="eq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m003" position="anchor"/><label>2</label></disp-formula>and a nonlinear
activation function τ
which can be chosen by the user (default ReLU). The size <italic>h</italic> of <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">0</sup> can be chosen by the user (default 300). The
size of <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup>, which we term <italic>h</italic><sub><italic>i</italic></sub>, is set by the lengths of initial feature
encodings, per <xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>.
The directed edge features are then iteratively updated based on the
local environment via <italic>T</italic> (default 3) message passing
steps<disp-formula id="eq3"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m004" position="anchor"/><label>3</label></disp-formula>until <italic>t</italic> + 1 = <italic>T</italic>, where <inline-formula id="d33e474"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m005.gif"/></inline-formula> and <italic>N</italic>(<italic>v</italic>)\<italic>w</italic> denotes
the neighbors of node <italic>v</italic> excluding <italic>w</italic>. The opposite facing directed edge
is excluded from the message passing update for increased numerical
stability (see Mahé et al.<sup><xref ref-type="bibr" rid="ref47">47</xref></sup>). Finally,
the updated hidden states <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack"><italic>T</italic></sup> are
aggregated into atomic embeddings via<disp-formula id="eq4"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m006" position="anchor"/><label>4</label></disp-formula>where <bold>q</bold> is a concatenation of
the initial atom features <bold>x</bold><sub><italic>v</italic></sub> and the sum of all incoming directed edge hidden states<disp-formula id="eq5"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m007" position="anchor"/><label>5</label></disp-formula>with <inline-formula id="d33e513"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m008.gif"/></inline-formula>. Here, <italic>h</italic><sub><italic>o</italic></sub> is the size
of <bold>q</bold>, i.e., the sum of the hidden
size <italic>h</italic> and the size of <bold>x</bold><sub><italic>v</italic></sub>. In summary, in Module 2, the D-MPNN weights <bold>W</bold><sub><italic>i</italic></sub>, <bold>W</bold><sub><italic>h</italic></sub>, and <bold>W</bold><sub><italic>o</italic></sub> are learned from the training data, outputting learnable atomic
embeddings <bold>h</bold><sub><italic>v</italic></sub>. Customizations
and hyperparameter tuning include the choice of the activation function
τ, the hidden size <italic>h</italic>, and the number of message
passing steps <italic>T</italic>. Chemprop offers the option to add
bias terms to all neural network layers (defaults to False).</p>
    <p>The atomic embeddings <bold>h</bold><sub><italic>v</italic></sub> of all atoms in a molecule are then aggregated into a single molecular
embedding <bold>h</bold><sub><italic>m</italic></sub> via<disp-formula id="eq6"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m009" position="anchor"/><label>6</label></disp-formula>with<disp-formula id="eq7"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m010" position="anchor"/><label>7</label></disp-formula>where <bold>x</bold><sub><italic>m</italic></sub> is an optional
vector of additional molecular features.
Chemprop offers three aggregation options: summation (as shown in <xref rid="eq7" ref-type="disp-formula">eq <xref rid="eq7" ref-type="disp-formula">7</xref></xref>), a scaled sum (divided
by a user specified scaler, called a norm within Chemprop), or an
average (the default aggregation). Schweidtmann et al.<sup><xref ref-type="bibr" rid="ref48">48</xref></sup> compared the performance of such aggregation
functions for different data sets. The optional additional molecular
features, <bold>x</bold><sub><italic>m</italic></sub>, may be provided
features from outside sources (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section
S2) or generated engineered fingerprints (Morgan circular fingerprints<sup><xref ref-type="bibr" rid="ref49">49</xref></sup> and RDKIT 2D fingerprints<sup><xref ref-type="bibr" rid="ref46">46</xref></sup> are implemented in Chemprop). By default, <bold>x</bold><sub><italic>m</italic></sub> is empty such that the molecular embedding
is simply an aggregation over atomic embeddings, i.e., <bold>h</bold><sub><italic>m</italic></sub> = <bold>h</bold><sub arrange="stack"><italic>m</italic></sub><sup arrange="stack">′</sup> . In
summary, Module 3 produces molecular embeddings <bold>h</bold><sub><italic>m</italic></sub> of length <italic>h</italic> plus the size
of <bold>x</bold><sub><italic>m</italic></sub>. Chemprop offers the
option to circumvent Modules 1–3 and only using <bold>x</bold><sub><italic>m</italic></sub> as fixed molecular embedding, so that <bold>h</bold><sub><italic>m</italic></sub> = <bold>x</bold><sub><italic>m</italic></sub>.</p>
    <p>Finally, in the last module, molecular target
properties are learned
from the molecular embeddings <bold>h</bold><sub><italic>m</italic></sub> via a feed-forward neural network, where the number of layers
(default 2) and the number of hidden neurons (default 300) can be
chosen by the user. The number of input neurons is set by the length
of <bold>h</bold><sub><italic>m</italic></sub>, and the number of
output neurons is set by the number of targets. The activation function
between linear layers is set to be the same as in the D-MPNN, and
bias is turned on per default. For binary classification tasks, the
final model output is passed through a sigmoid function to constrain
values to the range (0,1). For multiclass classification, the final
model output is transformed with a softmax function, such that the
classification scores sum to 1 across classes.</p>
    <p>Chemprop is fully
end-to-end trainable, so that the weights for
D-MPNN and FFN are updated simultaneously. Users have the option to
train models using cross-validation and ensembles of submodels. By
default, a single model is trained on a random data split for 30 epochs.
We note that small data sets need a much larger number of epochs to
train and advise to check for convergence of the learning curve. Chemprop
uses the Adam optimizer.<sup><xref ref-type="bibr" rid="ref50">50</xref></sup> The default
learning rate schedule increases the learning rate linearly from 10<sup>–4</sup> to 10<sup>–3</sup> for the first two warmup
epochs and then decreases the learning rate exponentially from 10<sup>–3</sup> to 10<sup>–4</sup> for the remaining epochs.
By default, a batch size of 50 data points is used for each optimizer
step. Early stopping and dropout are available as means of regularization.
The PyTorch backend of Chemprop enables seamless GPU acceleration
of both model training and inference. The acceleration of training
and inference processes when used with a GPU can be significant, as
shown in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section S3.3.</p>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Discussion of Features</title>
    <p><xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref> lists a
nonexhaustive selection of studies based on Chemprop, showcasing its
versatility and applicability for the prediction of a large variety
of chemical properties, but also its ease of use. Models can be trained
and tested with a single line on the command line (or a few lines
of python code) and a user-supplied CSV file (see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for some examples). In the following, we discuss specialty
options introduced since its first release.</p>
    <table-wrap id="tbl1" position="float">
      <label>Table 1</label>
      <caption>
        <title>Selected
Published Studies Based on
Chemprop</title>
      </caption>
      <table frame="hsides" rules="groups" border="0">
        <colgroup>
          <col align="center"/>
          <col align="center"/>
          <col align="left"/>
          <col align="center"/>
          <col align="center"/>
          <col align="left"/>
        </colgroup>
        <thead>
          <tr>
            <th style="border:none;" align="center">ref</th>
            <th style="border:none;" align="center">Year</th>
            <th style="border:none;" align="center">Prediction</th>
            <th style="border:none;" align="center">ref</th>
            <th style="border:none;" align="center">Year</th>
            <th style="border:none;" align="center">Prediction</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref10">10</xref>)</td>
            <td style="border:none;" align="center">2020</td>
            <td style="border:none;" align="left">Growth inhibitory activity against <italic>E. coli</italic>; led to an identification of a potential new drug</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref51">51</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Absorption, distribution, metabolism, excretion
(ADME) properties
for drug discovery</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref52">52</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Chemical synergy against SARS-CoV-2; identified two drug combinations
with antiviral synergy in vitro</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref53">53</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Growth inhibitory activity against <italic>A. baumannii</italic>; led to an identification of a potential new drug</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref28">28</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">IR spectra of molecules</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref54">54</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Fuel properties</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref55">55</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Atomic charges, Fukui indices, NMR constants, bond lengths,
and bond orders</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref56">56</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Critical properties, acentric
factor, and phase change properties</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref57">57</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Lipophilicity</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref58">58</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Molecular optical peaks</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref59">59</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Reaction rates and barrier heights</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref60">60</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Lipophilicity</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref61">61</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Barrier heights of reactions</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref62">62</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Solvent effects on reaction rate constants</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref29">29</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Molecular optical peaks</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref63">63</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Vapor pressure in the low volatility regime</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref64">64</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Solvation free energy, solvation
enthalpy, and Abraham solute
descriptors</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref65">65</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Molecular optical peaks
and partition coefficients for closed-loop
active learning</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref66">66</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Solid solubility
of organic solutes in water and organic solvents</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref67">67</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Senolytic activity of compounds to selectively target senescent
cells</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref68">68</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Activity coefficients</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref69">69</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Toxicity measurements using 12 nuclear receptor
signaling and
stress response pathways</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Additional Features</title>
      <p>Chemprop can
take additional features at the molecule-, atom-, or bond-level as
input. While Chemprop often generates accurate models without requiring
any input beyond the SMILES, it has been shown that outside information
added as additional features can further improve performance.<sup><xref ref-type="bibr" rid="ref36">36</xref>,<xref ref-type="bibr" rid="ref64">64</xref></sup> Users can provide their custom additional features by adding keywords
and paths to the data files containing the features. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for command-line arguments and details.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Multimolecule Models</title>
      <p>Chemprop can
also train on a data set containing more than one molecule as input.
For example, when properties related to solvation need to be predicted,
both a solute and a solvent are required as input to the model. Users
can provide multiple molecules as inputs to Chemprop. When multiple
molecules are used, by default Chemprop trains a separate D-MPNN for
each molecule (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Figure S1a</ext-link>). If the option <monospace>--mpn_shared</monospace> is specified, then the same D-MPNN is used
for all molecules (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Figure S1b</ext-link>). The embeddings
of the different molecules are then concatenated prior to the FFN.
Note that the current implementation of multiple molecules in Chemprop
does not ensure permutational invariance toward the input molecules.
This is suited to situations where the input molecules have different
roles, e.g., molecule 1 = solute, molecule 2 = solvent. For additional
input information and a figure depicting the multimolecule model structure,
see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link>.</p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Reaction
Support</title>
      <p>Chemprop supports
the input of atom-mapped reactions, i.e., pairs of reactants and products
SMILES connected via the “≫” symbol,by using
the keyword <monospace>--reaction</monospace>. The pair of reactants
and products is transformed into a single pseudomolecule, namely,
the condensed graph of reaction (CGR), and then passed to a regular
D-MPNN block. The construction of a CGR within Chemprop is described
in detail in ref (<xref ref-type="bibr" rid="ref59">59</xref>) and summarized in the following. In general, the input of a reaction
vs a molecule only affects the setup of the graph object and its initial
features, but not any other part of the architecture. The graph of
a reaction has a different set of edges <italic>E</italic> as the
graph of a molecule, as shown in <xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref> for an example Diels–Alder reaction.
To build the CGR pseudomolecule, the set of atoms is obtained as the
union of the sets of atoms in the reactants and products. Similarly,
the set of bonds is obtained as the union of the sets of bonds in
the reactants and products. Once constructed, the CGR is passed through
the D-MPNN and other model architecture components in the same way
a molecular graph would. Optionally, Chemprop accepts an additional
molecule object as input, such as a solvent, a reagent, etc. which
is passed to its own D-MPNN similar to the multimolecule model. The
output of the reaction D-MPNN and molecule D-MPNN is concatenated
after atomic aggregation, before the FFN. This option is available
via the <monospace>reaction_solvent</monospace> keyword. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for further commandline options and details.</p>
      <fig id="fig2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Construction
of the condensed graph of reaction (CGR) of an example
reaction. The vertices and edges are obtained as the union of the
respective reactant and product vertices and edges. The features are
obtained as a combination of the reactant (white background) and product
(gray background) features for atoms and bonds.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0002" id="gr2" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Spectra Data Support</title>
      <p>Chemprop supports
the prediction of whole-spectrum properties for molecules. An initial
version of this capability was discussed in ref (<xref ref-type="bibr" rid="ref28">28</xref>) for use with IR absorbance
spectra. Targets for the spectra data set type are composed of an
array of intensity values, set at fixed bin locations typically specified
in terms of wavelengths, frequencies, or wavenumbers. Spectral Information
Divergence was originally developed as a method of comparing spectra
to reference databases<sup><xref ref-type="bibr" rid="ref70">70</xref></sup> and is adapted
in Chemprop to be used as a loss function, considering the deviation
of the spectrum as a whole rather than independently at each bin location.
The treatment of spectra can handle targets with gaps or missing values
within a data set. With the expectation that spectra will often be
collected in systems where a portion of the range will be obscured
or invalid (e.g., from solvent absorbance), Chemprop can create exclusion
regions in specified spectra where no predictions are provided and
targets are ignored for training purposes.</p>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Latent
Representations</title>
      <p>Graph neural
networks enable learning both molecular representation and property
end-to-end directly from the molecular graph. As detailed above in <xref rid="sec2" ref-type="other">Section <xref rid="sec2" ref-type="other">2</xref></xref>, the learned node
representations are aggregated into a molecule-level representation
after the message-passing phase, which we refer to as the “learned
fingerprint.” This embedding is then further fed into the FFN
network. Within the FFN, we consider the final hidden representation,
which we refer to as the “ffn embedding”. Both of these
vectors are latent representations of a molecule as it relates to
a particular trained model. Molecule latent representations can be
useful for data clustering or used as additional features in other
models. Chemprop supports the calculation of either from a trained
model for a given set of molecules.</p>
    </sec>
    <sec id="sec3.6">
      <label>3.6</label>
      <title>Loss
Function Options</title>
      <p>Chemprop can
train models according to many common loss functions. The loss functions
available for a given task are determined by the data set type (regression,
classification, multiclass, or spectra). Regression models can be
trained with mean squared error (MSE), bounded MSE (which allows inequalities
as targets), or negative log-likelihood (NLL) based on prediction
uncertainty distributions consistent with mean-variance estimation
(MVE)<sup><xref ref-type="bibr" rid="ref71">71</xref></sup> or evidential uncertainty.<sup><xref ref-type="bibr" rid="ref72">72</xref></sup> Classification tasks default to the binary cross
entropy loss and have additional options of Matthews correlation coefficient
(MCC) and Dirichlet (evidential classification).<sup><xref ref-type="bibr" rid="ref73">73</xref></sup> Cross entropy and MCC are also available for multiclass
problems. There are two options available for training on spectra:
spectral information divergence (SID)<sup><xref ref-type="bibr" rid="ref70">70</xref></sup> and first-order Wasserstein distance (a.k.a. earthmover’s
distance).<sup><xref ref-type="bibr" rid="ref74">74</xref></sup> Loss functions must be differentiable
since they are used to calculate gradients that update the model parameters,
but Chemprop also provides the option to use several nondifferentiable
metrics for model evaluation.</p>
    </sec>
    <sec id="sec3.7">
      <label>3.7</label>
      <title>Transfer
Learning</title>
      <p>Transfer learning
is a general strategy of using information gained through the training
of one model to inform and improve the training of a related model.
Often, this strategy is used to transfer information from a previously
trained model of a large data set to a model of a small data set in
order to improve the performance of the model of the small data set.
The simplest method of transfer learning would be taking predictions
or latent representations from one model and supplying them as additional
features to another model (<xref rid="sec3.1" ref-type="other">Sections <xref rid="sec3.1" ref-type="other">3.1</xref></xref> and <xref rid="sec3.5" ref-type="other">3.5</xref>).</p>
      <p>In Chemprop, different strategies are available to transfer learned
model parameters from a previously trained model to a new model as
a form of transfer learning, as shown in <xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>. A pretrained model may be used to initialize
a new model with normal updating of the transferred weights in training.
Alternatively, parameters from the transferred model can be frozen,
holding them constant during training. Freezing parameters always
include the D-MPNN weights but can be specified to include some FFN
layers as well. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for the corresponding
arguments.</p>
      <fig id="fig3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Options to transfer model parameters from a pretrained model (squared)
to a new model, by (a) initializing the new model parameters or by
freezing the (b) D-MPNN layers and (c) <italic>n</italic> FFN layers.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0003" id="gr3" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.8">
      <label>3.8</label>
      <title>Hyperparameter Optimization</title>
      <p>Chemprop
provides a command-line utility, allowing for the simple initiation
of hyperparameter optimization jobs. Options for the hyperparameter
job such as how many trials to carry out and which hyperparameters
to include in the search (options in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Table S3</ext-link>) can be specified with simple command-line arguments. The optimization
initially uses randomly sampled trials, followed by targeted sampling
using the Tree-structured Parzen Estimator algorithm.<sup><xref ref-type="bibr" rid="ref75">75</xref>,<xref ref-type="bibr" rid="ref76">76</xref></sup></p>
      <p>Hyperparameter optimization is often the most resource-intensive
step in model training. In order to search a large parameter space
adequately, a large number of trials is needed. Chemprop allows for
parallel operation of multiple hyperparameter optimization instances,
removing the need to carry out all trials in series and reducing the
wall time needed to perform the optimization significantly. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for the available hyperparameter options and
other details.</p>
    </sec>
    <sec id="sec3.9">
      <label>3.9</label>
      <title>Uncertainty Tools</title>
      <p>Chemprop includes
a variety of popular uncertainty estimation, calibration, and evaluation
tools. The estimation methods include deep ensembles,<sup><xref ref-type="bibr" rid="ref77">77</xref></sup> dropout,<sup><xref ref-type="bibr" rid="ref78">78</xref></sup> mean-variance estimation
(MVE),<sup><xref ref-type="bibr" rid="ref71">71</xref></sup> and evidential,<sup><xref ref-type="bibr" rid="ref72">72</xref></sup> as well as a special version of ensemble variance for spectral
predictions,<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> and the inherently probabilistic
outputs of classification models. After estimating the uncertainty
in a model’s predictions, it is often helpful to calibrate
these uncertainties to improve their performance on new predictions.
We provide four such methods for regression tasks (<italic>z</italic>-scaling,<sup><xref ref-type="bibr" rid="ref79">79</xref></sup> t-scaling, Zelikman’s
CRUDE,<sup><xref ref-type="bibr" rid="ref80">80</xref></sup> and MVE weighting<sup><xref ref-type="bibr" rid="ref81">81</xref></sup>) and two for classification (single-parameter Platt scaling<sup><xref ref-type="bibr" rid="ref82">82</xref></sup> and isotonic regression<sup><xref ref-type="bibr" rid="ref83">83</xref></sup>). In addition to standard metrics such as RMSE, MAE, etc. for evaluating
predictions, Chemprop also includes several metrics specifically for
evaluating the quality of uncertainty estimates. These include negative
log likelihood, Spearman rank correlation, expected normalized calibration
error (ENCE),<sup><xref ref-type="bibr" rid="ref84">84</xref></sup> and miscalibration area.<sup><xref ref-type="bibr" rid="ref84">84</xref></sup> Any valid classification or multiclass metric
used to assess predictions can also be used to assess uncertainties.</p>
    </sec>
    <sec id="sec3.10">
      <label>3.10</label>
      <title>Atom/Bond-Level Targets</title>
      <p>Chemprop
supports a multitask constrained D-MPNN architecture for predicting
atom- and bond-level properties, such as charge density or bond length.
This model enables a D-MPNN to be trained on multiple atomic and bond
properties simultaneously, though unlike molecular property targets,
they do not share a single FFN. Optionally, an attention-based constraining
method may be used to enforce that predicted atomic or bond properties
sum to a specified molecular net value, such as the overall charge
of a molecule. An initial, more limited version of this capability
was developed in ref (<xref ref-type="bibr" rid="ref55">55</xref>). For details on the input formats to be used for atom/bond targets,
both for training and for inference, see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link>.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Benchmarking</title>
    <p>See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for general performance benchmarks,
benchmarks using specific Chemprop features (atom/bond-level targets,
reaction support, multimolecule models, spectra prediction, and uncertainty
estimation), and system timing benchmarks.</p>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Conclusion</title>
    <p>We have presented the software package Chemprop, a powerful toolbox
for machine learning of the chemical properties of molecules and reactions.
Significant improvements have been made to the software since its
initial release and study,<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> including
the support of multimolecule properties, reactions, atom/bond-level
properties, and spectra. Additionally, several state-of-the-art approaches
to estimate the uncertainty of predictions have been incorporated
as well as pretraining and transfer learning procedures. Furthermore,
the code now offers a variety of customization options, such as custom
atom and bond features, a large variety of loss functions, and the
ability to save the learned feature embeddings for subsequent use
with different algorithms. We have showcased and benchmarked Chemprop
on a variety of example tasks and data sets and have found competitive
performances for molecular property prediction compared to other approaches
available on public leaderboards. In summary, Chemprop is a powerful,
fast, and convenient tool to learn conformation-independent properties
of molecules, sets of molecules, or reactions.</p>
  </sec>
</body>
<back>
  <notes notes-type="data-availability" id="notes-1">
    <title>Data Availability Statement</title>
    <p>Chemprop, including
all features described in this paper, is available under the open-source
MIT License on GitHub, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/chemprop/chemprop">github.com/chemprop/chemprop</uri>. An extensive documentation including
tutorials is available online,<sup><xref ref-type="bibr" rid="ref85">85</xref></sup> including
a workshop on YouTube.<sup><xref ref-type="bibr" rid="ref86">86</xref></sup> Scripts and data
splits to fully reproduce this study are available on GitHub, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/chemprop/chemprop_benchmark">github.com/chemprop/chemprop_benchmark</uri>, and on Zenodo, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://doi.org/10.5281/zenodo.8174267">doi.org/10.5281/zenodo.8174267</uri>, respectively.</p>
  </notes>
  <notes id="notes-2" notes-type="si">
    <title>Supporting Information Available</title>
    <p>The Supporting Information
is available free of charge at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acs.jcim.3c01250?goto=supporting-info">https://pubs.acs.org/doi/10.1021/acs.jcim.3c01250</ext-link>.<list id="silist" list-type="simple"><list-item><p>Additional software details
and usage examples, data
set and data handling details for benchmarking, and results of software
benchmarks (general performance, feature demonstrations, timing) (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">PDF</ext-link>)</p></list-item></list></p>
  </notes>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sifile1">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_si_001.pdf">
        <caption>
          <p>ci3c01250_si_001.pdf</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <notes notes-type="COI-statement" id="NOTES-d14e1343-autogenerated">
    <p>The authors declare no
competing financial interest.</p>
  </notes>
  <ack>
    <title>Acknowledgments</title>
    <p>E.H., Y.C., F.H.V., and W.H.G. acknowledge support from the
Machine Learning for Pharmaceutical Discovery and Synthesis Consortium
(MLPDS). K.P.G., S.-C.L., F.H.V., H.W., W.H.G., and C.J.M. acknowledge
support from the DARPA Accelerated Molecular Discovery (AMD) program
(DARPA HR00111920025). E.H. acknowledges support from the Austrian
Science Fund (FWF), project J-4415. K.P.G. was supported by the National
Science Foundation Graduate Research Fellowship Program under Grant
No. 1745302. D.E.G. acknowledges support from the MIT IBM Watson AI
Lab. F.H.V. would like to acknowledge the KU Leuven Internal Starting
Grant (STG/22/032). C.J.M. would like to acknowledge support from
VCU Startup Funding. Parts of the data reported within this paper
were generated with resources from the MIT SuperCloud Lincoln Laboratory
Supercomputing Center.<sup><xref ref-type="bibr" rid="ref87">87</xref></sup></p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="journal" id="cit1"><name><surname>Gilmer</surname><given-names>J.</given-names></name>; <name><surname>Schoenholz</surname><given-names>S. S.</given-names></name>; <name><surname>Riley</surname><given-names>P. F.</given-names></name>; <name><surname>Vinyals</surname><given-names>O.</given-names></name>; <name><surname>Dahl</surname><given-names>G. E.</given-names></name><article-title>Neural
Message Passing for Quantum Chemistry</article-title>. <source>Proceedings
of the International Conference on Machine Learning</source><year>2017</year>, <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="report" id="cit2"><person-group person-group-type="allauthors"><name><surname>Gasteiger</surname><given-names>J.</given-names></name>; <name><surname>Groß</surname><given-names>J.</given-names></name>; <name><surname>Günnemann</surname><given-names>S.</given-names></name></person-group><article-title>Directional Message
Passing for Molecular Graphs</article-title>. <source>Proceedings
of the International Conference on Learning Representations</source>, <bold>2003</bold>, arXiv:2003.03123.</mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="report" id="cit3"><person-group person-group-type="allauthors"><name><surname>Zhang</surname><given-names>S.</given-names></name>; <name><surname>Liu</surname><given-names>Y.</given-names></name>; <name><surname>Xie</surname><given-names>L.</given-names></name></person-group><article-title>Molecular Mechanics-Driven Graph Neural
Network with Multiplex Graph for Molecular Structures</article-title>. <source>Machine Learning for Molecules Workshop at NeurIPS</source>, <bold>2020</bold>, arXiv:2011.07457.</mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting Solubility Limits of Organic
Solutes for a Wide Range of Solvents and Temperatures</article-title>. <source>J. Am. Chem. Soc.</source><year>2022</year>, <volume>144</volume>, <fpage>10785</fpage>–<lpage>10797</lpage>. <pub-id pub-id-type="doi">10.1021/jacs.2c01768</pub-id>.<pub-id pub-id-type="pmid">35687887</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="journal" id="cit5"><name><surname>Dobbelaere</surname><given-names>M. R.</given-names></name>; <name><surname>Ureel</surname><given-names>Y.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Tomme</surname><given-names>L.</given-names></name>; <name><surname>Stevens</surname><given-names>C. V.</given-names></name>; <name><surname>Van Geem</surname><given-names>K. M.</given-names></name><article-title>Machine Learning for Physicochemical Property Prediction
of Complex Hydrocarbon Mixtures</article-title>. <source>Ind. Eng. Chem.
Res.</source><year>2022</year>, <volume>61</volume>, <fpage>8581</fpage>–<lpage>8594</lpage>. <pub-id pub-id-type="doi">10.1021/acs.iecr.2c00442</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal" id="cit6"><name><surname>Dobbelaere</surname><given-names>M. R.</given-names></name>; <name><surname>Plehiers</surname><given-names>P. P.</given-names></name>; <name><surname>Van de Vijver</surname><given-names>R.</given-names></name>; <name><surname>Stevens</surname><given-names>C. V.</given-names></name>; <name><surname>Van Geem</surname><given-names>K. M.</given-names></name><article-title>Learning
Molecular Representations for Thermochemistry Prediction of Cyclic
Hydrocarbons and Oxygenates</article-title>. <source>J. Phys. Chem.
A</source><year>2021</year>, <volume>125</volume>, <fpage>5166</fpage>–<lpage>5179</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpca.1c01956</pub-id>.<pub-id pub-id-type="pmid">34081474</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal" id="cit7"><name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Ben Hicham</surname><given-names>K.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name><article-title>Graph Neural
Networks for Temperature-Dependent Activity Coefficient Prediction
of Solutes in Ionic Liquids</article-title>. <source>Comput. Chem. Eng.</source><year>2023</year>, <volume>171</volume>, <fpage>108153</fpage><pub-id pub-id-type="doi">10.1016/j.compchemeng.2023.108153</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal" id="cit8"><name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Ritzert</surname><given-names>M.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Winkler</surname><given-names>S.</given-names></name>; <name><surname>Weber</surname><given-names>J. M.</given-names></name>; <name><surname>Morsch</surname><given-names>P.</given-names></name>; <name><surname>Heufer</surname><given-names>K. A.</given-names></name>; <name><surname>Grohe</surname><given-names>M.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name><article-title>Graph Machine Learning
for Design of High-Octane Fuels</article-title>. <source>AIChE J.</source><year>2023</year>, <volume>69</volume>, <fpage>e17971</fpage><pub-id pub-id-type="doi">10.1002/aic.17971</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="journal" id="cit9"><name><surname>Fleitmann</surname><given-names>L.</given-names></name>; <name><surname>Ackermann</surname><given-names>P.</given-names></name>; <name><surname>Schilling</surname><given-names>J.</given-names></name>; <name><surname>Kleinekorte</surname><given-names>J.</given-names></name>; <name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>vom Lehn</surname><given-names>F.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Pitsch</surname><given-names>H.</given-names></name>; <name><surname>Leonhard</surname><given-names>K.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name>; <name><surname>Bardow</surname><given-names>A.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name><article-title>Molecular Design of Fuels for Maximum
Spark-Ignition
Engine Efficiency by Combining Predictive Thermodynamics and Machine
Learning</article-title>. <source>Energ. Fuel.</source><year>2023</year>, <volume>37</volume>, <fpage>2213</fpage>–<lpage>2229</lpage>. <pub-id pub-id-type="doi">10.1021/acs.energyfuels.2c03296</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="journal" id="cit10"><name><surname>Stokes</surname><given-names>J. M.</given-names></name>; <name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Cubillos-Ruiz</surname><given-names>A.</given-names></name>; <name><surname>Donghia</surname><given-names>N. M.</given-names></name>; <name><surname>MacNair</surname><given-names>C. R.</given-names></name>; <name><surname>French</surname><given-names>S.</given-names></name>; <name><surname>Carfrae</surname><given-names>L. A.</given-names></name>; <name><surname>Bloom-Ackermann</surname><given-names>Z.</given-names></name>; et al. <article-title>A Deep Learning Approach to Antibiotic
Discovery</article-title>. <source>Cell</source><year>2020</year>, <volume>180</volume>, <fpage>688</fpage>–<lpage>702</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2020.01.021</pub-id>.<pub-id pub-id-type="pmid">32084340</pub-id></mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="report" id="cit11"><person-group person-group-type="allauthors"><name><surname>De Cao</surname><given-names>N.</given-names></name>; <name><surname>Kipf</surname><given-names>T.</given-names></name></person-group><article-title>MolGAN: An Implicit Generative
Model for Small Molecular Graphs</article-title>. <source>arXiv Preprint</source>, <bold>2022</bold>, arXiv:1805.11973.</mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="journal" id="cit12"><name><surname>Dan</surname><given-names>Y.</given-names></name>; <name><surname>Zhao</surname><given-names>Y.</given-names></name>; <name><surname>Li</surname><given-names>X.</given-names></name>; <name><surname>Li</surname><given-names>S.</given-names></name>; <name><surname>Hu</surname><given-names>M.</given-names></name>; <name><surname>Hu</surname><given-names>J.</given-names></name><article-title>Generative Adversarial
Networks (GAN) Based Efficient Sampling of
Chemical Composition Space for Inverse Design of Inorganic Materials</article-title>. <source>Npj Comput. Mater.</source><year>2020</year>, <volume>6</volume>, <fpage>1</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1038/s41524-020-00352-0</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="journal" id="cit13"><name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name>; <name><surname>Wei</surname><given-names>J. N.</given-names></name>; <name><surname>Duvenaud</surname><given-names>D.</given-names></name>; <name><surname>Hernández-Lobato</surname><given-names>J. M.</given-names></name>; <name><surname>Sánchez-Lengeling</surname><given-names>B.</given-names></name>; <name><surname>Sheberla</surname><given-names>D.</given-names></name>; <name><surname>Aguilera-Iparraguirre</surname><given-names>J.</given-names></name>; <name><surname>Hirzel</surname><given-names>T. D.</given-names></name>; <name><surname>Adams</surname><given-names>R. P.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name><article-title>Automatic
Chemical Design using a Data-Driven Continuous Representation of Molecules</article-title>. <source>ACS Cent. Sci.</source><year>2018</year>, <volume>4</volume>, <fpage>268</fpage>–<lpage>276</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.7b00572</pub-id>.<pub-id pub-id-type="pmid">29532027</pub-id></mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="journal" id="cit14"><name><surname>Wei</surname><given-names>J. N.</given-names></name>; <name><surname>Duvenaud</surname><given-names>D.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name><article-title>Neural Networks
for the Prediction
of Organic Chemistry Reactions</article-title>. <source>ACS Cent. Sci.</source><year>2016</year>, <volume>2</volume>, <fpage>725</fpage>–<lpage>732</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.6b00219</pub-id>.<pub-id pub-id-type="pmid">27800555</pub-id></mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="journal" id="cit15"><name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Waller</surname><given-names>M. P.</given-names></name><article-title>Neural-Symbolic
Machine Learning for Retrosynthesis
and Reaction Prediction</article-title>. <source>Chem. Eur. J.</source><year>2017</year>, <volume>23</volume>, <fpage>5966</fpage>–<lpage>5971</lpage>. <pub-id pub-id-type="doi">10.1002/chem.201605499</pub-id>.<pub-id pub-id-type="pmid">28134452</pub-id></mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal" id="cit16"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Prediction
of Organic Reaction Outcomes using Machine Learning</article-title>. <source>ACS Cent. Sci.</source><year>2017</year>, <volume>3</volume>, <fpage>434</fpage>–<lpage>443</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.7b00064</pub-id>.<pub-id pub-id-type="pmid">28573205</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="journal" id="cit17"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Machine Learning
in Computer-Aided
Synthesis Planning</article-title>. <source>Acc. Chem. Res.</source><year>2018</year>, <volume>51</volume>, <fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1021/acs.accounts.8b00087</pub-id>.<pub-id pub-id-type="pmid">29715002</pub-id></mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="journal" id="cit18"><name><surname>Szymkuć</surname><given-names>S.</given-names></name>; <name><surname>Gajewska</surname><given-names>E. P.</given-names></name>; <name><surname>Klucznik</surname><given-names>T.</given-names></name>; <name><surname>Molga</surname><given-names>K.</given-names></name>; <name><surname>Dittwald</surname><given-names>P.</given-names></name>; <name><surname>Startek</surname><given-names>M.</given-names></name>; <name><surname>Bajczyk</surname><given-names>M.</given-names></name>; <name><surname>Grzybowski</surname><given-names>B. A.</given-names></name><article-title>Computer-Assisted
Synthetic Planning: The End of the Beginning</article-title>. <source>Angew. Chem., Int. Ed.</source><year>2016</year>, <volume>55</volume>, <fpage>5904</fpage>–<lpage>5937</lpage>. <pub-id pub-id-type="doi">10.1002/anie.201506101</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="journal" id="cit19"><name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Preuss</surname><given-names>M.</given-names></name>; <name><surname>Waller</surname><given-names>M. P.</given-names></name><article-title>Planning Chemical Syntheses with
Deep Neural Networks and Symbolic AI</article-title>. <source>Nature</source><year>2018</year>, <volume>555</volume>, <fpage>604</fpage>–<lpage>610</lpage>. <pub-id pub-id-type="doi">10.1038/nature25978</pub-id>.<pub-id pub-id-type="pmid">29595767</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="journal" id="cit20"><name><surname>Kayala</surname><given-names>M. A.</given-names></name>; <name><surname>Azencott</surname><given-names>C.-A.</given-names></name>; <name><surname>Chen</surname><given-names>J. H.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>Learning to Predict
Chemical Reactions</article-title>. <source>J. Chem. Inf. Model.</source><year>2011</year>, <volume>51</volume>, <fpage>2209</fpage>–<lpage>2222</lpage>. <pub-id pub-id-type="doi">10.1021/ci200207y</pub-id>.<pub-id pub-id-type="pmid">21819139</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="journal" id="cit21"><name><surname>Kayala</surname><given-names>M. A.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>ReactionPredictor:
Prediction of Complex Chemical Reactions at the
Mechanistic Level using Machine Learning</article-title>. <source>J.
Chem. Inf. Model.</source><year>2012</year>, <volume>52</volume>, <fpage>2526</fpage>–<lpage>2540</lpage>. <pub-id pub-id-type="doi">10.1021/ci3003039</pub-id>.<pub-id pub-id-type="pmid">22978639</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="journal" id="cit22"><name><surname>Fooshee</surname><given-names>D.</given-names></name>; <name><surname>Mood</surname><given-names>A.</given-names></name>; <name><surname>Gutman</surname><given-names>E.</given-names></name>; <name><surname>Tavakoli</surname><given-names>M.</given-names></name>; <name><surname>Urban</surname><given-names>G.</given-names></name>; <name><surname>Liu</surname><given-names>F.</given-names></name>; <name><surname>Huynh</surname><given-names>N.</given-names></name>; <name><surname>Van Vranken</surname><given-names>D.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>Deep Learning for Chemical
Reaction Prediction</article-title>. <source>Mol. Syst. Des. Eng.</source><year>2018</year>, <volume>3</volume>, <fpage>442</fpage>–<lpage>452</lpage>. <pub-id pub-id-type="doi">10.1039/C7ME00107J</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="report" id="cit23"><person-group person-group-type="allauthors"><name><surname>Bradshaw</surname><given-names>J.</given-names></name>; <name><surname>Kusner</surname><given-names>M. J.</given-names></name>; <name><surname>Paige</surname><given-names>B.</given-names></name>; <name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Hernández-Lobato</surname><given-names>J. M.</given-names></name></person-group><article-title>A Generative Model for Electron Paths</article-title>. <source>Proceedings
of the International Conference on Learning Representations</source>, <bold>2019</bold>, arXiv:1805.10970.</mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="journal" id="cit24"><name><surname>Bi</surname><given-names>H.</given-names></name>; <name><surname>Wang</surname><given-names>H.</given-names></name>; <name><surname>Shi</surname><given-names>C.</given-names></name>; <name><surname>Coley</surname><given-names>C.</given-names></name>; <name><surname>Tang</surname><given-names>J.</given-names></name>; <name><surname>Guo</surname><given-names>H.</given-names></name><article-title>Non-Autoregressive
Electron Redistribution Modeling for Reaction
Prediction</article-title>. <source>Proceedings of the International
Conference on Machine Learning</source><year>2021</year>, <volume>139</volume>, <fpage>904</fpage>–<lpage>913</lpage>.</mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="journal" id="cit25"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Rogers</surname><given-names>L.</given-names></name>; <name><surname>Jamison</surname><given-names>T. F.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>A Graph-Convolutional Neural Network
Model for the Prediction of Chemical Reactivity</article-title>. <source>Chem. Sci.</source><year>2019</year>, <volume>10</volume>, <fpage>370</fpage>–<lpage>377</lpage>. <pub-id pub-id-type="doi">10.1039/C8SC04228D</pub-id>.<pub-id pub-id-type="pmid">30746086</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <mixed-citation publication-type="journal" id="cit26"><name><surname>Sacha</surname><given-names>M.</given-names></name>; <name><surname>Błaz</surname><given-names>M.</given-names></name>; <name><surname>Byrski</surname><given-names>P.</given-names></name>; <name><surname>Dabrowski-Tumanski</surname><given-names>P.</given-names></name>; <name><surname>Chrominski</surname><given-names>M.</given-names></name>; <name><surname>Loska</surname><given-names>R.</given-names></name>; <name><surname>Włodarczyk-Pruszynski</surname><given-names>P.</given-names></name>; <name><surname>Jastrzebski</surname><given-names>S.</given-names></name><article-title>Molecule Edit Graph Attention Network: Modeling Chemical
Reactions as Sequences of Graph Edits</article-title>. <source>J. Chem.
Inf. Model.</source><year>2021</year>, <volume>61</volume>, <fpage>3273</fpage>–<lpage>3284</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00537</pub-id>.<pub-id pub-id-type="pmid">34251814</pub-id></mixed-citation>
    </ref>
    <ref id="ref27">
      <mixed-citation publication-type="journal" id="cit27"><name><surname>Schwaller</surname><given-names>P.</given-names></name>; <name><surname>Gaudin</surname><given-names>T.</given-names></name>; <name><surname>Lanyi</surname><given-names>D.</given-names></name>; <name><surname>Bekas</surname><given-names>C.</given-names></name>; <name><surname>Laino</surname><given-names>T.</given-names></name><article-title>“Found
in Translation”: Predicting Outcomes of Complex Organic Chemistry
Reactions using Neural Sequence-to-Sequence Models</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>6091</fpage>–<lpage>6098</lpage>. <pub-id pub-id-type="doi">10.1039/C8SC02339E</pub-id>.<pub-id pub-id-type="pmid">30090297</pub-id></mixed-citation>
    </ref>
    <ref id="ref28">
      <mixed-citation publication-type="journal" id="cit28"><name><surname>McGill</surname><given-names>C.</given-names></name>; <name><surname>Forsuelo</surname><given-names>M.</given-names></name>; <name><surname>Guan</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting Infrared
Spectra with Message Passing Neural Networks</article-title>. <source>J. Chem. Inf. Model.</source><year>2021</year>, <volume>61</volume>, <fpage>2594</fpage>–<lpage>2609</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00055</pub-id>.<pub-id pub-id-type="pmid">34048221</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <mixed-citation publication-type="journal" id="cit29"><name><surname>Greenman</surname><given-names>K. P.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name><article-title>Multi-Fidelity
Prediction
of Molecular Optical Peaks with Deep Learning</article-title>. <source>Chem. Sci.</source><year>2022</year>, <volume>13</volume>, <fpage>1152</fpage>–<lpage>1162</lpage>. <pub-id pub-id-type="doi">10.1039/D1SC05677H</pub-id>.<pub-id pub-id-type="pmid">35211282</pub-id></mixed-citation>
    </ref>
    <ref id="ref30">
      <mixed-citation publication-type="journal" id="cit30"><name><surname>Dührkop</surname><given-names>K.</given-names></name><article-title>Deep Kernel
Learning Improves Molecular Fingerprint Prediction from Tandem Mass
Spectra</article-title>. <source>Bioinf.</source><year>2022</year>, <volume>38</volume>, <fpage>i342</fpage>–<lpage>i349</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac260</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref31">
      <mixed-citation publication-type="journal" id="cit31"><name><surname>Nguyen</surname><given-names>D. H.</given-names></name>; <name><surname>Nguyen</surname><given-names>C. H.</given-names></name>; <name><surname>Mamitsuka</surname><given-names>H.</given-names></name><article-title>ADAPTIVE:
leArning DAta-dePendenT,
concIse molecular VEctors for Fast, Accurate Metabolite Identification
from Tandem Mass Spectra</article-title>. <source>Bioinf.</source><year>2019</year>, <volume>35</volume>, <fpage>i164</fpage>–<lpage>i172</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz319</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <mixed-citation publication-type="journal" id="cit32"><name><surname>Nguyen</surname><given-names>D. H.</given-names></name>; <name><surname>Nguyen</surname><given-names>C. H.</given-names></name>; <name><surname>Mamitsuka</surname><given-names>H.</given-names></name><article-title>Recent Advances
and Prospects of
Computational Methods for Metabolite Identification: A Review with
Emphasis on Machine Learning Approaches</article-title>. <source>Brief.
Bioinf.</source><year>2019</year>, <volume>20</volume>, <fpage>2028</fpage>–<lpage>2043</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bby066</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <mixed-citation publication-type="journal" id="cit33"><name><surname>Stravs</surname><given-names>M.
A.</given-names></name>; <name><surname>Dührkop</surname><given-names>K.</given-names></name>; <name><surname>Böcker</surname><given-names>S.</given-names></name>; <name><surname>Zamboni</surname><given-names>N.</given-names></name><article-title>MSNovelist: De Novo
Structure Generation from Mass Spectra</article-title>. <source>Nat.
Methods</source><year>2022</year>, <volume>19</volume>, <fpage>865</fpage>–<lpage>870</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-022-01486-3</pub-id>.<pub-id pub-id-type="pmid">35637304</pub-id></mixed-citation>
    </ref>
    <ref id="ref34">
      <mixed-citation publication-type="journal" id="cit34"><name><surname>Muratov</surname><given-names>E. N.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name>; <name><surname>Sheridan</surname><given-names>R. P.</given-names></name>; <name><surname>Tetko</surname><given-names>I. V.</given-names></name>; <name><surname>Filimonov</surname><given-names>D.</given-names></name>; <name><surname>Poroikov</surname><given-names>V.</given-names></name>; <name><surname>Oprea</surname><given-names>T. I.</given-names></name>; <name><surname>Baskin</surname><given-names>I. I.</given-names></name>; <name><surname>Varnek</surname><given-names>A.</given-names></name>; <name><surname>Roitberg</surname><given-names>A.</given-names></name>; et al. <article-title>QSAR without Borders</article-title>. <source>Chem. Soc. Rev.</source><year>2020</year>, <volume>49</volume>, <fpage>3525</fpage>–<lpage>3564</lpage>. <pub-id pub-id-type="doi">10.1039/D0CS00098A</pub-id>.<pub-id pub-id-type="pmid">32356548</pub-id></mixed-citation>
    </ref>
    <ref id="ref35">
      <mixed-citation publication-type="journal" id="cit35"><name><surname>Kearnes</surname><given-names>S.</given-names></name>; <name><surname>McCloskey</surname><given-names>K.</given-names></name>; <name><surname>Berndl</surname><given-names>M.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Riley</surname><given-names>P.</given-names></name><article-title>Molecular
Graph Convolutions: Moving Beyond Fingerprints</article-title>. <source>J. Comput. Aided Mol. Des.</source><year>2016</year>, <volume>30</volume>, <fpage>595</fpage>–<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>.<pub-id pub-id-type="pmid">27558503</pub-id></mixed-citation>
    </ref>
    <ref id="ref36">
      <mixed-citation publication-type="journal" id="cit36"><name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Coley</surname><given-names>C.</given-names></name>; <name><surname>Eiden</surname><given-names>P.</given-names></name>; <name><surname>Gao</surname><given-names>H.</given-names></name>; <name><surname>Guzman-Perez</surname><given-names>A.</given-names></name>; <name><surname>Hopper</surname><given-names>T.</given-names></name>; <name><surname>Kelley</surname><given-names>B.</given-names></name>; <name><surname>Mathea</surname><given-names>M.</given-names></name>; <name><surname>Palmer</surname><given-names>A.</given-names></name>; <name><surname>Settels</surname><given-names>V.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T.</given-names></name>; <name><surname>Jensen</surname><given-names>K.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name><article-title>Analyzing
Learned Molecular Representations for Property
Prediction</article-title>. <source>J. Chem. Inf. Model.</source><year>2019</year>, <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="ref37">
      <mixed-citation publication-type="report" id="cit37"><person-group person-group-type="allauthors"><name><surname>Maziarka</surname><given-names>Ł.</given-names></name>; <name><surname>Danel</surname><given-names>T.</given-names></name>; <name><surname>Mucha</surname><given-names>S.</given-names></name>; <name><surname>Rataj</surname><given-names>K.</given-names></name>; <name><surname>Tabor</surname><given-names>J.</given-names></name>; <name><surname>Jastrzebski</surname><given-names>S.</given-names></name></person-group><article-title>Molecule Attention Transformer</article-title>. <source>arXiv Preprint</source>, <bold>2020</bold>, arXiv:2002.08264.</mixed-citation>
    </ref>
    <ref id="ref38">
      <mixed-citation publication-type="journal" id="cit38"><name><surname>Kreuzer</surname><given-names>D.</given-names></name>; <name><surname>Beaini</surname><given-names>D.</given-names></name>; <name><surname>Hamilton</surname><given-names>W.</given-names></name>; <name><surname>Létourneau</surname><given-names>V.</given-names></name>; <name><surname>Tossou</surname><given-names>P.</given-names></name><article-title>Rethinking Graph Transformers with Spectral Attention</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2021</year>, <volume>34</volume>, <fpage>21618</fpage>–<lpage>21629</lpage>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <mixed-citation publication-type="journal" id="cit39"><name><surname>Schütt</surname><given-names>K. T.</given-names></name>; <name><surname>Sauceda</surname><given-names>H. E.</given-names></name>; <name><surname>Kindermans</surname><given-names>P.-J.</given-names></name>; <name><surname>Tkatchenko</surname><given-names>A.</given-names></name>; <name><surname>Müller</surname><given-names>K.-R.</given-names></name><article-title>Schnet–A Deep Learning Architecture
for Molecules
and Materials</article-title>. <source>J. Chem. Phys.</source><year>2018</year>, <volume>148</volume>, <fpage>241722</fpage><pub-id pub-id-type="doi">10.1063/1.5019779</pub-id>.<pub-id pub-id-type="pmid">29960322</pub-id></mixed-citation>
    </ref>
    <ref id="ref40">
      <mixed-citation publication-type="journal" id="cit40"><name><surname>Unke</surname><given-names>O. T.</given-names></name>; <name><surname>Meuwly</surname><given-names>M.</given-names></name><article-title>PhysNet: A Neural Network
for Predicting Energies,
Forces, Dipole Moments, and Partial Charges</article-title>. <source>J. Chem. Theory Comput.</source><year>2019</year>, <volume>15</volume>, <fpage>3678</fpage>–<lpage>3693</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.9b00181</pub-id>.<pub-id pub-id-type="pmid">31042390</pub-id></mixed-citation>
    </ref>
    <ref id="ref41">
      <mixed-citation publication-type="report" id="cit41"><person-group person-group-type="allauthors"><name><surname>Bigi</surname><given-names>F.</given-names></name>, <name><surname>Pozdnyakov</surname><given-names>S. N.</given-names></name>, <name><surname>Ceriotti</surname><given-names>M.</given-names></name></person-group><article-title>Wigner
Kernels:
Body-Ordered Equivariant Machine Learning without a Basis</article-title>. <source>arXiv Preprint</source>, <bold>2023</bold>, arXiv:2303.04124.</mixed-citation>
    </ref>
    <ref id="ref42">
      <mixed-citation publication-type="journal" id="cit42"><name><surname>Winter</surname><given-names>B.</given-names></name>; <name><surname>Winter</surname><given-names>C.</given-names></name>; <name><surname>Schilling</surname><given-names>J.</given-names></name>; <name><surname>Bardow</surname><given-names>A.</given-names></name><article-title>A Smile is All you
Need: Predicting Limiting Activity Coefficients from SMILES with Natural
Language Processing</article-title>. <source>Digital Discovery</source><year>2022</year>, <volume>1</volume>, <fpage>859</fpage>–<lpage>869</lpage>. <pub-id pub-id-type="doi">10.1039/D2DD00058J</pub-id>.<pub-id pub-id-type="pmid">36561987</pub-id></mixed-citation>
    </ref>
    <ref id="ref43">
      <mixed-citation publication-type="journal" id="cit43"><name><surname>Bagal</surname><given-names>V.</given-names></name>; <name><surname>Aggarwal</surname><given-names>R.</given-names></name>; <name><surname>Vinod</surname><given-names>P.</given-names></name>; <name><surname>Priyakumar</surname><given-names>U. D.</given-names></name><article-title>MolGPT:
Molecular Generation using a Transformer-Decoder Model</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>2064</fpage>–<lpage>2076</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00600</pub-id>.<pub-id pub-id-type="pmid">34694798</pub-id></mixed-citation>
    </ref>
    <ref id="ref44">
      <mixed-citation publication-type="report" id="cit44"><person-group person-group-type="allauthors"><name><surname>Honda</surname><given-names>S.</given-names></name>; <name><surname>Shi</surname><given-names>S.</given-names></name>; <name><surname>Ueda</surname><given-names>H.
R.</given-names></name></person-group><article-title>Smiles Transformer:
Pre-trained
Molecular Fingerprint for Low Data Drug Discovery</article-title>. <source>arXiv Preprint</source>, <bold>2019</bold>, arXiv:1911.04738.</mixed-citation>
    </ref>
    <ref id="ref45">
      <mixed-citation publication-type="conf-proc" id="cit45"><person-group person-group-type="allauthors"><name><surname>Chithrananda</surname><given-names>S.</given-names></name>; <name><surname>Grand</surname><given-names>G.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name></person-group><article-title>Chemberta: Large-Scale
Self-Supervised Pretraining for Molecular Property Prediction</article-title>. <source>Machine Learning for Molecules Workshop at NeurIPS</source>, <bold>2020</bold>, arXiv:2010.09885.</mixed-citation>
    </ref>
    <ref id="ref46">
      <mixed-citation publication-type="weblink" id="cit46"><person-group person-group-type="allauthors"><name><surname>Landrum</surname><given-names>G.</given-names></name></person-group><article-title>RDKit: Open-Source
Cheminformatics</article-title>. <bold>2006</bold>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.rdkit.org/">https://www.rdkit.org/</uri>.</mixed-citation>
    </ref>
    <ref id="ref47">
      <mixed-citation publication-type="conf-proc" id="cit47"><person-group person-group-type="allauthors"><name><surname>Mahé</surname><given-names>P.</given-names></name>; <name><surname>Ueda</surname><given-names>N.</given-names></name>; <name><surname>Akutsu</surname><given-names>T.</given-names></name>; <name><surname>Perret</surname><given-names>J.-L.</given-names></name>; <name><surname>Vert</surname><given-names>J.-P.</given-names></name></person-group><article-title>Extensions
of marginalized graph kernels</article-title>. <source>Proceedings
of the International Conference on Machine Learning</source>, <bold>2004</bold>, <volume>70</volume>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <mixed-citation publication-type="journal" id="cit48"><name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Weber</surname><given-names>J. M.</given-names></name>; <name><surname>Grohe</surname><given-names>M.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name>; <name><surname>Leonhard</surname><given-names>K.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name><article-title>Physical Pooling Functions in Graph
Neural Networks for Molecular Property Prediction</article-title>. <source>Comput. Chem. Eng.</source><year>2023</year>, <volume>172</volume>, <fpage>108202</fpage><pub-id pub-id-type="doi">10.1016/j.compchemeng.2023.108202</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref49">
      <mixed-citation publication-type="journal" id="cit49"><name><surname>Morgan</surname><given-names>H. L.</given-names></name><article-title>The Generation
of a Unique Machine Description for Chemical Structures – A
Technique Developed at Chemical Abstracts Service</article-title>. <source>J. Chem. Doc.</source><year>1965</year>, <volume>5</volume>, <fpage>107</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1021/c160017a018</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref50">
      <mixed-citation publication-type="report" id="cit50"><person-group person-group-type="allauthors"><name><surname>Kingma</surname><given-names>D. P.</given-names></name>; <name><surname>Ba</surname><given-names>J.</given-names></name></person-group><article-title>Adam: A Method for Stochastic
Optimization</article-title>. <source>Proceedings of the International
Conference on Learning Representations</source>, <bold>2017</bold>, arXiv:1412.6980.</mixed-citation>
    </ref>
    <ref id="ref51">
      <mixed-citation publication-type="journal" id="cit51"><name><surname>Lim</surname><given-names>M. A.</given-names></name>; <name><surname>Yang</surname><given-names>S.</given-names></name>; <name><surname>Mai</surname><given-names>H.</given-names></name>; <name><surname>Cheng</surname><given-names>A. C.</given-names></name><article-title>Exploring Deep Learning
of Quantum Chemical Properties for Absorption, Distribution, Metabolism,
and Excretion Predictions</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>6336</fpage>–<lpage>6341</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c00245</pub-id>.<pub-id pub-id-type="pmid">35758421</pub-id></mixed-citation>
    </ref>
    <ref id="ref52">
      <mixed-citation publication-type="journal" id="cit52"><name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Stokes</surname><given-names>J. M.</given-names></name>; <name><surname>Eastman</surname><given-names>R. T.</given-names></name>; <name><surname>Itkin</surname><given-names>Z.</given-names></name>; <name><surname>Zakharov</surname><given-names>A. V.</given-names></name>; <name><surname>Collins</surname><given-names>J. J.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name><article-title>Deep Learning Identifies
Synergistic Drug Combinations for Treating COVID-19</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><year>2021</year>, <volume>118</volume>, <fpage>e2105070118</fpage><pub-id pub-id-type="doi">10.1073/pnas.2105070118</pub-id>.<pub-id pub-id-type="pmid">34526388</pub-id></mixed-citation>
    </ref>
    <ref id="ref53">
      <mixed-citation publication-type="journal" id="cit53"><name><surname>Liu</surname><given-names>G.</given-names></name>; <name><surname>Catacutan</surname><given-names>D. B.</given-names></name>; <name><surname>Rathod</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Mohammed</surname><given-names>J. C.</given-names></name>; <name><surname>Chiappino-Pepe</surname><given-names>A.</given-names></name>; <name><surname>Syed</surname><given-names>S. A.</given-names></name>; <name><surname>Fragis</surname><given-names>M.</given-names></name>; <name><surname>Rachwalski</surname><given-names>K.</given-names></name>; et al. <article-title>Deep Learning-Guided Discovery of an Antibiotic
Targeting Acinetobacter Baumannii</article-title>. <source>Nat. Chem.
Biol.</source><year>2023</year>, <volume>19</volume>, <fpage>1342</fpage>–<lpage>1350</lpage>. <pub-id pub-id-type="doi">10.1038/s41589-023-01349-8</pub-id>.<pub-id pub-id-type="pmid">37231267</pub-id></mixed-citation>
    </ref>
    <ref id="ref54">
      <mixed-citation publication-type="journal" id="cit54"><name><surname>Larsson</surname><given-names>T.</given-names></name>; <name><surname>Vermeire</surname><given-names>F.</given-names></name>; <name><surname>Verhelst</surname><given-names>S.</given-names></name><article-title>Machine Learning for Fuel Property
Predictions: A Multi-Task and Transfer Learning Approach</article-title>. <source>SAE Technical Paper</source><year>2023</year>, <pub-id pub-id-type="doi">10.4271/2023-01-0337</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref55">
      <mixed-citation publication-type="journal" id="cit55"><name><surname>Guan</surname><given-names>Y.</given-names></name>; <name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Ranasinghe</surname><given-names>D.</given-names></name>; <name><surname>Heid</surname><given-names>E.</given-names></name>; <name><surname>Struble</surname><given-names>T. J.</given-names></name>; <name><surname>Pattanaik</surname><given-names>L.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Regio-Selectivity Prediction with
a Machine-Learned Reaction Representation and On-the-Fly Quantum Mechanical
Descriptors</article-title>. <source>Chem. Sci.</source><year>2021</year>, <volume>12</volume>, <fpage>2198</fpage>–<lpage>2208</lpage>. <pub-id pub-id-type="doi">10.1039/D0SC04823B</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref56">
      <mixed-citation publication-type="journal" id="cit56"><name><surname>Biswas</surname><given-names>S.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Ramirez</surname><given-names>J.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting
Critical Properties and Acentric Factor of Fluids using Multi-Task
Machine Learning</article-title>. <source>J. Chem. Inf. Model.</source><year>2023</year>, <volume>63</volume>, <fpage>4574</fpage>–<lpage>4588</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.3c00546</pub-id>.<pub-id pub-id-type="pmid">37487557</pub-id></mixed-citation>
    </ref>
    <ref id="ref57">
      <mixed-citation publication-type="journal" id="cit57"><name><surname>Lenselink</surname><given-names>E. B.</given-names></name>; <name><surname>Stouten</surname><given-names>P. F. W.</given-names></name><article-title>Multitask machine
learning models for predicting lipophilicity
(logP) in the SAMPL7 challenge</article-title>. <source>Journal of Computer-Aided
Molecular Design</source><year>2021</year>, <volume>35</volume>, <fpage>901</fpage>–<lpage>909</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-021-00405-6</pub-id>.<pub-id pub-id-type="pmid">34273053</pub-id></mixed-citation>
    </ref>
    <ref id="ref58">
      <mixed-citation publication-type="journal" id="cit58"><name><surname>McNaughton</surname><given-names>A. D.</given-names></name>; <name><surname>Joshi</surname><given-names>R. P.</given-names></name>; <name><surname>Knutson</surname><given-names>C. R.</given-names></name>; <name><surname>Fnu</surname><given-names>A.</given-names></name>; <name><surname>Luebke</surname><given-names>K. J.</given-names></name>; <name><surname>Malerich</surname><given-names>J. P.</given-names></name>; <name><surname>Madrid</surname><given-names>P. B.</given-names></name>; <name><surname>Kumar</surname><given-names>N.</given-names></name><article-title>Machine Learning
Models
for Predicting Molecular UV–Vis Spectra with Quantum Mechanical
Properties</article-title>. <source>J. Chem. Inf. Model.</source><year>2023</year>, <volume>63</volume>, <fpage>1462</fpage>–<lpage>1471</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c01662</pub-id>.<pub-id pub-id-type="pmid">36847578</pub-id></mixed-citation>
    </ref>
    <ref id="ref59">
      <mixed-citation publication-type="journal" id="cit59"><name><surname>Heid</surname><given-names>E.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Machine Learning
of Reaction Properties via Learned
Representations of the Condensed Graph of Reaction</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>2101</fpage>–<lpage>2110</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00975</pub-id>.<pub-id pub-id-type="pmid">34734699</pub-id></mixed-citation>
    </ref>
    <ref id="ref60">
      <mixed-citation publication-type="journal" id="cit60"><name><surname>Isert</surname><given-names>C.</given-names></name>; <name><surname>Kromann</surname><given-names>J. C.</given-names></name>; <name><surname>Stiefl</surname><given-names>N.</given-names></name>; <name><surname>Schneider</surname><given-names>G.</given-names></name>; <name><surname>Lewis</surname><given-names>R. A.</given-names></name><article-title>Machine Learning for Fast, Quantum Mechanics-Based
Approximation of Drug Lipophilicity</article-title>. <source>ACS Omega</source><year>2023</year>, <volume>8</volume>, <fpage>2046</fpage>–<lpage>2056</lpage>. <pub-id pub-id-type="doi">10.1021/acsomega.2c05607</pub-id>.<pub-id pub-id-type="pmid">36687099</pub-id></mixed-citation>
    </ref>
    <ref id="ref61">
      <mixed-citation publication-type="journal" id="cit61"><name><surname>Spiekermann</surname><given-names>K. A.</given-names></name>; <name><surname>Pattanaik</surname><given-names>L.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Fast Predictions of Reaction Barrier
Heights: Toward Coupled-Cluster Accuracy</article-title>. <source>J.
Phys. Chem. A</source><year>2022</year>, <volume>126</volume>, <fpage>3976</fpage>–<lpage>3986</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpca.2c02614</pub-id>.<pub-id pub-id-type="pmid">35727075</pub-id></mixed-citation>
    </ref>
    <ref id="ref62">
      <mixed-citation publication-type="report" id="cit62"><person-group person-group-type="allauthors"><name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W.
H.</given-names></name></person-group><article-title>Machine Learning
from Quantum Chemistry to Predict Experimental Solvent Effects on
Reaction Rates</article-title>. <source>ChemRxiv Preprint</source>, <bold>2023</bold>.</mixed-citation>
    </ref>
    <ref id="ref63">
      <mixed-citation publication-type="journal" id="cit63"><name><surname>Lansford</surname><given-names>J.
L.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name>; <name><surname>Barnes</surname><given-names>B. C.</given-names></name><article-title>Physics-informed Transfer Learning
for Out-of-sample Vapor Pressure Predictions</article-title>. <source>Propellants, Explosives, Pyrotechnics</source><year>2023</year>, <volume>48</volume>, <fpage>e202200265</fpage><pub-id pub-id-type="doi">10.1002/prep.202200265</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref64">
      <mixed-citation publication-type="journal" id="cit64"><name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Walker</surname><given-names>P. J.</given-names></name>; <name><surname>Abraham</surname><given-names>M. H.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Group Contribution and Machine Learning Approaches
to Predict Abraham Solute Parameters, Solvation Free Energy, and Solvation
Enthalpy</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>433</fpage>–<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c01103</pub-id>.<pub-id pub-id-type="pmid">35044781</pub-id></mixed-citation>
    </ref>
    <ref id="ref65">
      <mixed-citation publication-type="journal" id="cit65"><name><surname>Koscher</surname><given-names>B. A.</given-names></name>; <name><surname>Canty</surname><given-names>R. B.</given-names></name>; <name><surname>McDonald</surname><given-names>M. A.</given-names></name>; <name><surname>Greenman</surname><given-names>K. P.</given-names></name>; <name><surname>McGill</surname><given-names>C. J.</given-names></name>; <name><surname>Bilodeau</surname><given-names>C. L.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Jin</surname><given-names>B.</given-names></name>; <name><surname>Hart</surname><given-names>T.</given-names></name>; <name><surname>Kulesza</surname><given-names>T.</given-names></name>; <name><surname>Li</surname><given-names>S.-C.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Gomez-Bombarelli</surname><given-names>R.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Autonomous,
Multiproperty-Driven Molecular Discovery: From Predictions to Measurements
and Back</article-title>. <source>Science</source><year>2023</year>, <volume>382</volume> (<issue>6677</issue>), <fpage>eadi1407</fpage><pub-id pub-id-type="doi">10.1126/science.adi1407</pub-id>.<pub-id pub-id-type="pmid">38127734</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref66">
      <mixed-citation publication-type="journal" id="cit66"><name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting
Solubility Limits of Organic
Solutes for a Wide Range of Solvents and Temperatures</article-title>. <source>J. Am. Chem. Soc.</source><year>2022</year>, <volume>144</volume>, <fpage>10785</fpage>–<lpage>10797</lpage>. <pub-id pub-id-type="doi">10.1021/jacs.2c01768</pub-id>.<pub-id pub-id-type="pmid">35687887</pub-id></mixed-citation>
    </ref>
    <ref id="ref67">
      <mixed-citation publication-type="journal" id="cit67"><name><surname>Wong</surname><given-names>F.</given-names></name>; <name><surname>Omori</surname><given-names>S.</given-names></name>; <name><surname>Donghia</surname><given-names>N. M.</given-names></name>; <name><surname>Zheng</surname><given-names>E. J.</given-names></name>; <name><surname>Collins</surname><given-names>J. J.</given-names></name><article-title>Discovering
Small-Molecule Senolytics with Deep Neural Networks</article-title>. <source>Nature Aging</source><year>2023</year>, <volume>3</volume>, <fpage>734</fpage>–<lpage>750</lpage>. <pub-id pub-id-type="doi">10.1038/s43587-023-00415-z</pub-id>.<pub-id pub-id-type="pmid">37142829</pub-id></mixed-citation>
    </ref>
    <ref id="ref68">
      <mixed-citation publication-type="conf-proc" id="cit68"><person-group><name><surname>Felton</surname><given-names>K. C.</given-names></name>; <name><surname>Ben-Safar</surname><given-names>H.</given-names></name>; <name><surname>Alexei</surname><given-names>A.</given-names></name></person-group><article-title>DeepGamma: A Deep Learning
Model for
Activity Coefficient Prediction</article-title>. <source>1st Annual
AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)</source>, <bold>2022</bold>.</mixed-citation>
    </ref>
    <ref id="ref69">
      <mixed-citation publication-type="journal" id="cit69"><name><surname>Colomba</surname><given-names>M.</given-names></name>; <name><surname>Benedetti</surname><given-names>S.</given-names></name>; <name><surname>Fraternale</surname><given-names>D.</given-names></name>; <name><surname>Guidarelli</surname><given-names>A.</given-names></name>; <name><surname>Coppari</surname><given-names>S.</given-names></name>; <name><surname>Freschi</surname><given-names>V.</given-names></name>; <name><surname>Crinelli</surname><given-names>R.</given-names></name>; <name><surname>Kass</surname><given-names>G. E. N.</given-names></name>; <name><surname>Gorassini</surname><given-names>A.</given-names></name>; <name><surname>Verardo</surname><given-names>G.</given-names></name>; <name><surname>Roselli</surname><given-names>C.</given-names></name>; <name><surname>Meli</surname><given-names>M. A.</given-names></name>; <name><surname>Di Giacomo</surname><given-names>B.</given-names></name>; <name><surname>Albertini</surname><given-names>M. C.</given-names></name><article-title>Nrf2-Mediated Pathway Activated by
Prunus spinosa L. (Rosaceae) Fruit Extract: Bioinformatics Analyses
and Experimental Validation</article-title>. <source>Nutrients</source><year>2023</year>, <volume>15</volume>, <fpage>2132</fpage><pub-id pub-id-type="doi">10.3390/nu15092132</pub-id>.<pub-id pub-id-type="pmid">37432298</pub-id></mixed-citation>
    </ref>
    <ref id="ref70">
      <mixed-citation publication-type="journal" id="cit70"><name><surname>Chang</surname><given-names>C.-I.</given-names></name><article-title>An Information-Theoretic
Approach to Spectral Variability, Similarity, and Discrimination for
Hyperspectral Image Analysis</article-title>. <source>IEEE Transactions
on Information Theory</source><year>2000</year>, <volume>46</volume>, <fpage>1927</fpage>–<lpage>1932</lpage>. <pub-id pub-id-type="doi">10.1109/18.857802</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref71">
      <mixed-citation publication-type="journal" id="cit71"><name><surname>Nix</surname><given-names>D. A.</given-names></name>; <name><surname>Weigend</surname><given-names>A. S.</given-names></name><article-title>Estimating the Mean and Variance of the Target Probability
Distribution</article-title>. <source>Proceedings of IEEE International
Conference on Neural Networks</source><year>1994</year>, <volume>1</volume>, <fpage>55</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="ref72">
      <mixed-citation publication-type="journal" id="cit72"><name><surname>Amini</surname><given-names>A.</given-names></name>; <name><surname>Schwarting</surname><given-names>W.</given-names></name>; <name><surname>Soleimany</surname><given-names>A.</given-names></name>; <name><surname>Rus</surname><given-names>D.</given-names></name><article-title>Deep Evidential Regression</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2020</year>, <volume>33</volume>, <fpage>14927</fpage>–<lpage>14937</lpage>.</mixed-citation>
    </ref>
    <ref id="ref73">
      <mixed-citation publication-type="journal" id="cit73"><name><surname>Sensoy</surname><given-names>M.</given-names></name>; <name><surname>Kaplan</surname><given-names>L.</given-names></name>; <name><surname>Kandemir</surname><given-names>M.</given-names></name><article-title>Evidential Deep Learning
to Quantify
Classification Uncertainty</article-title>. <source>Adv. Neural Inf.
Process. Syst.</source><year>2018</year>, <volume>31</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref74">
      <mixed-citation publication-type="book" id="cit74"><person-group person-group-type="allauthors"><name><surname>Villani</surname><given-names>C.</given-names></name></person-group><source>Optimal Transport: Old
and New</source>; <publisher-name>Springer</publisher-name>, <bold>2009</bold>; Vol. <volume>338</volume>.</mixed-citation>
    </ref>
    <ref id="ref75">
      <mixed-citation publication-type="journal" id="cit75"><name><surname>Bergstra</surname><given-names>J.</given-names></name>; <name><surname>Bardenet</surname><given-names>R.</given-names></name>; <name><surname>Bengio</surname><given-names>Y.</given-names></name>; <name><surname>Kégl</surname><given-names>B.</given-names></name><article-title>Algorithms
for Hyper-Parameter Optimization</article-title>. <source>Adv. Neural
Inf. Process. Syst.</source><year>2011</year>, <volume>24</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref76">
      <mixed-citation publication-type="journal" id="cit76"><name><surname>Bergstra</surname><given-names>J.</given-names></name>; <name><surname>Yamins</surname><given-names>D.</given-names></name>; <name><surname>Cox</surname><given-names>D.</given-names></name><article-title>Making a Science
of Model Search:
Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures</article-title>. <source>Proceedings of the International Conference on Machine Learning</source><year>2013</year>, <fpage>115</fpage>–<lpage>123</lpage>.</mixed-citation>
    </ref>
    <ref id="ref77">
      <mixed-citation publication-type="journal" id="cit77"><name><surname>Lakshminarayanan</surname><given-names>B.</given-names></name>; <name><surname>Pritzel</surname><given-names>A.</given-names></name>; <name><surname>Blundell</surname><given-names>C.</given-names></name><article-title>Simple and Scalable Predictive Uncertainty
Estimation using Deep Ensembles</article-title>. <source>Adv. Neural
Inf. Process. Syst</source><year>2017</year>, <volume>30</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref78">
      <mixed-citation publication-type="journal" id="cit78"><name><surname>Gal</surname><given-names>Y.</given-names></name>; <name><surname>Ghahramani</surname><given-names>Z.</given-names></name><article-title>Dropout as
a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning</article-title>. <source>Proceedings of
the International Conference on Machine Learning</source><year>2016</year>, <volume>48</volume>, <fpage>1050</fpage>–<lpage>1059</lpage>.</mixed-citation>
    </ref>
    <ref id="ref79">
      <mixed-citation publication-type="journal" id="cit79"><name><surname>Levi</surname><given-names>D.</given-names></name>; <name><surname>Gispan</surname><given-names>L.</given-names></name>; <name><surname>Giladi</surname><given-names>N.</given-names></name>; <name><surname>Fetaya</surname><given-names>E.</given-names></name><article-title>Evaluating
and calibrating
uncertainty prediction in regression tasks</article-title>. <source>Sensors</source><year>2022</year>, <volume>22</volume>, <fpage>5540</fpage><pub-id pub-id-type="doi">10.3390/s22155540</pub-id>.<pub-id pub-id-type="pmid">35898047</pub-id></mixed-citation>
    </ref>
    <ref id="ref80">
      <mixed-citation publication-type="report" id="cit80"><person-group person-group-type="allauthors"><name><surname>Zelikman</surname><given-names>E.</given-names></name>; <name><surname>Healy</surname><given-names>C.</given-names></name>; <name><surname>Zhou</surname><given-names>S.</given-names></name>; <name><surname>Avati</surname><given-names>A.</given-names></name></person-group><article-title>CRUDE: Calibrating
Regression Uncertainty Distributions
Empirically</article-title>. <source>arXiv Preprint</source>, <bold>2020</bold>, arXiv:2005.12496.</mixed-citation>
    </ref>
    <ref id="ref81">
      <mixed-citation publication-type="journal" id="cit81"><name><surname>Wang</surname><given-names>D.</given-names></name>; <name><surname>Yu</surname><given-names>J.</given-names></name>; <name><surname>Chen</surname><given-names>L.</given-names></name>; <name><surname>Li</surname><given-names>X.</given-names></name>; <name><surname>Jiang</surname><given-names>H.</given-names></name>; <name><surname>Chen</surname><given-names>K.</given-names></name>; <name><surname>Zheng</surname><given-names>M.</given-names></name>; <name><surname>Luo</surname><given-names>X.</given-names></name><article-title>A Hybrid Framework for Improving
Uncertainty Quantification in Deep Learning-Based QSAR Regression
Modeling</article-title>. <source>J. Cheminf.</source><year>2021</year>, <volume>13</volume>, <fpage>1</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1186/s13321-021-00551-x</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref82">
      <mixed-citation publication-type="journal" id="cit82"><name><surname>Guo</surname><given-names>C.</given-names></name>; <name><surname>Pleiss</surname><given-names>G.</given-names></name>; <name><surname>Sun</surname><given-names>Y.</given-names></name>; <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name><article-title>On Calibration
of Modern Neural Networks</article-title>. <source>Proceedings of the
International Conference on Machine Learning</source><year>2017</year>, <fpage>1321</fpage>–<lpage>1330</lpage>.</mixed-citation>
    </ref>
    <ref id="ref83">
      <mixed-citation publication-type="journal" id="cit83"><name><surname>Zadrozny</surname><given-names>B.</given-names></name>; <name><surname>Elkan</surname><given-names>C.</given-names></name><article-title>Transforming Classifier Scores into Accurate Multiclass Probability
Estimates</article-title>. <source>Proceedings of the International
Conference on Knowledge Discovery and Data Mining</source><year>2002</year>, <fpage>694</fpage>–<lpage>699</lpage>.</mixed-citation>
    </ref>
    <ref id="ref84">
      <mixed-citation publication-type="journal" id="cit84"><name><surname>Scalia</surname><given-names>G.</given-names></name>; <name><surname>Grambow</surname><given-names>C. A.</given-names></name>; <name><surname>Pernici</surname><given-names>B.</given-names></name>; <name><surname>Li</surname><given-names>Y.-P.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Evaluating
Scalable Uncertainty Estimation Methods for Deep Learning-Based Molecular
Property Prediction</article-title>. <source>J. Chem. Inf. Model.</source><year>2020</year>, <volume>60</volume>, <fpage>2697</fpage>–<lpage>2717</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00975</pub-id>.<pub-id pub-id-type="pmid">32243154</pub-id></mixed-citation>
    </ref>
    <ref id="ref85">
      <mixed-citation publication-type="weblink" id="cit85">Chemprop. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://chemprop.readthedocs.io/en/latest/">https://chemprop.readthedocs.io/en/latest/</uri> (accessed April 6 2023).</mixed-citation>
    </ref>
    <ref id="ref86">
      <mixed-citation publication-type="weblink" id="cit86">Chemprop
Workshop. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.youtube.com/watch?v=TeOl5E8Wo2M">https://www.youtube.com/watch?v=TeOl5E8Wo2M</uri> (accessed April
6 2023).</mixed-citation>
    </ref>
    <ref id="ref87">
      <mixed-citation publication-type="journal" id="cit87"><name><surname>Reuther</surname><given-names>A.</given-names></name>; <name><surname>Kepner</surname><given-names>J.</given-names></name>; <name><surname>Byun</surname><given-names>C.</given-names></name>; <name><surname>Samsi</surname><given-names>S.</given-names></name>; <name><surname>Arcand</surname><given-names>W.</given-names></name>; <name><surname>Bestor</surname><given-names>D.</given-names></name>; <name><surname>Bergeron</surname><given-names>B.</given-names></name>; <name><surname>Gadepally</surname><given-names>V.</given-names></name>; <name><surname>Houle</surname><given-names>M.</given-names></name>; <name><surname>Hubbell</surname><given-names>M.</given-names></name>; et al. <article-title>Interactive
Supercomputing
on 40,000 Cores for Machine Learning and Data Analysis</article-title>. <source>Proceedings of the IEEE High Performance Extreme Computing
Conference</source><year>2018</year>, <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="publisher-id">ci</journal-id>
    <journal-id journal-id-type="coden">jcisd8</journal-id>
    <journal-title-group>
      <journal-title>Journal of Chemical Information and Modeling</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1549-9596</issn>
    <issn pub-type="epub">1549-960X</issn>
    <publisher>
      <publisher-name>American Chemical Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10777403</article-id>
    <article-id pub-id-type="pmid">38147829</article-id>
    <article-id pub-id-type="doi">10.1021/acs.jcim.3c01250</article-id>
    <article-categories>
      <subj-group>
        <subject>Application Note</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Chemprop: A Machine Learning Package for Chemical
Property Prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="ath1">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8404-6596</contrib-id>
        <name>
          <surname>Heid</surname>
          <given-names>Esther</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath2">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6466-1401</contrib-id>
        <name>
          <surname>Greenman</surname>
          <given-names>Kevin P.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath3">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3097-010X</contrib-id>
        <name>
          <surname>Chung</surname>
          <given-names>Yunsie</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath4">
        <name>
          <surname>Li</surname>
          <given-names>Shih-Cheng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff3" ref-type="aff">§</xref>
      </contrib>
      <contrib contrib-type="author" id="ath5">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1250-3329</contrib-id>
        <name>
          <surname>Graff</surname>
          <given-names>David E.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff4" ref-type="aff">∥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath6">
        <name>
          <surname>Vermeire</surname>
          <given-names>Florence H.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff5" ref-type="aff">⊥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath7">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0644-7554</contrib-id>
        <name>
          <surname>Wu</surname>
          <given-names>Haoyang</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath8">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2603-9694</contrib-id>
        <name>
          <surname>Green</surname>
          <given-names>William H.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath9">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2704-7717</contrib-id>
        <name>
          <surname>McGill</surname>
          <given-names>Charles J.</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff6" ref-type="aff">#</xref>
      </contrib>
      <aff id="aff1"><label>†</label>Department
of Chemical Engineering, <institution>Massachusetts Institute
of Technology</institution>, Cambridge, Massachusetts 02139, <country>United States</country></aff>
      <aff id="aff2"><label>‡</label><institution>Institute
of Materials Chemistry</institution>, TU Wien, 1060 Vienna, <country>Austria</country></aff>
      <aff id="aff3"><label>§</label>Department
of Chemical Engineering, <institution>National Taiwan
University</institution>, Taipei 10617, <country>Taiwan</country></aff>
      <aff id="aff4"><label>∥</label>Department
of Chemistry and Chemical Biology, <institution>Harvard
University</institution>, Cambridge, Massachusetts 02138, <country>United States</country></aff>
      <aff id="aff5"><label>⊥</label>Department
of Chemical Engineering, <institution>KU Leuven</institution>, Celestijnenlaan 200F, B-3001 Leuven, <country>Belgium</country></aff>
      <aff id="aff6"><label>#</label>Department
of Chemical and Life Science Engineering, <institution>Virginia Commonwealth University</institution>, Richmond, Virginia 23284, <country>United States</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>E-mail: <email>mcgillc2@vcu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>26</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <day>08</day>
      <month>01</month>
      <year>2024</year>
    </pub-date>
    <volume>64</volume>
    <issue>1</issue>
    <fpage>9</fpage>
    <lpage>17</lpage>
    <history>
      <date date-type="received">
        <day>08</day>
        <month>08</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>04</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by American Chemical Society</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>The Authors</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Permits the broadest form of re-use including for commercial purposes, provided that author attribution and integrity are maintained (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0004" id="ab-tgr1"/>
      </p>
      <p>Deep learning has
become a powerful and frequently employed tool
for the prediction of molecular properties, thus creating a need for
open-source and versatile software solutions that can be operated
by nonexperts. Among the current approaches, directed message-passing
neural networks (D-MPNNs) have proven to perform well on a variety
of property prediction tasks. The software package Chemprop implements
the D-MPNN architecture and offers simple, easy, and fast access to
machine-learned molecular properties. Compared to its initial version,
we present a multitude of new Chemprop functionalities such as the
support of multimolecule properties, reactions, atom/bond-level properties,
and spectra. Further, we incorporate various uncertainty quantification
and calibration methods along with related metrics as well as pretraining
and transfer learning workflows, improved hyperparameter optimization,
and other customization options concerning loss functions or atom/bond
features. We benchmark D-MPNN models trained using Chemprop with the
new reaction, atom-level, and spectra functionality on a variety of
property prediction data sets, including MoleculeNet and SAMPL, and
observe state-of-the-art performance on the prediction of water-octanol
partition coefficients, reaction barrier heights, atomic partial charges,
and absorption spectra. Chemprop enables out-of-the-box training of
D-MPNN models for a variety of problem settings in fast, user-friendly,
and open-source software.</p>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Division of Graduate Education</institution>
            <institution-id institution-id-type="doi">10.13039/100000082</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>1745302</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>MIT-IBM Watson Lab</institution>
            <institution-id institution-id-type="doi">NA</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>KU Leuven</institution>
            <institution-id institution-id-type="doi">10.13039/501100004040</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>STG/22/032</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Austrian Science Fund</institution>
            <institution-id institution-id-type="doi">10.13039/501100002428</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>J-4415</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Virginia Commonwealth University</institution>
            <institution-id institution-id-type="doi">10.13039/100009238</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Massachusetts Institute of Technology</institution>
            <institution-id institution-id-type="doi">10.13039/100006919</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Defense Sciences Office, DARPA</institution>
            <institution-id institution-id-type="doi">10.13039/100006502</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>HR00111920025</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ci3c01250</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ci3c01250</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p>Machine learning in general
and especially deep learning has become
a powerful tool in various fields of chemistry. Applications range
from the prediction of physicochemical<sup><xref ref-type="bibr" rid="ref1">1</xref>−<xref ref-type="bibr" rid="ref9">9</xref></sup> and pharmacological<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> properties of molecules
to the design of molecules or materials with certain properties,<sup><xref ref-type="bibr" rid="ref11">11</xref>−<xref ref-type="bibr" rid="ref13">13</xref></sup> the exploration of chemical synthesis pathways,<sup><xref ref-type="bibr" rid="ref14">14</xref>−<xref ref-type="bibr" rid="ref27">27</xref></sup> or the prediction of properties important for chemical analysis
like IR,<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> UV/vis,<sup><xref ref-type="bibr" rid="ref29">29</xref></sup> or mass spectra.<sup><xref ref-type="bibr" rid="ref30">30</xref>−<xref ref-type="bibr" rid="ref33">33</xref></sup></p>
    <p>Many combinations of molecular representations and model architectures
have been developed to extract features from molecules and predict
molecular properties. Molecules can be represented as graphs, strings,
precomputed feature vectors, or sets of atomic coordinates and processed
using graph-convolutional neural networks, transformers, or feed-forward
neural networks to train predictive models. While early works focused
on handmade features or simple fingerprinting methods combined with
kernel regression or neural networks,<sup><xref ref-type="bibr" rid="ref34">34</xref></sup> the current state-of-the-art has shifted to end-to-end trainable
models which directly learn to extract their own features.<sup><xref ref-type="bibr" rid="ref35">35</xref></sup> Here, the models can achieve extreme complexity
based on the mechanisms of information exchange between parts of the
molecule. For example, graph convolutional neural networks (GCNNs)
extract local information from the molecular graph for single or small
groups of atoms and use that information to update the immediate neighborhood.<sup><xref ref-type="bibr" rid="ref1">1</xref>−<xref ref-type="bibr" rid="ref3">3</xref>,<xref ref-type="bibr" rid="ref36">36</xref></sup> They offer robust performance
for properties dependent on the local structure and if the three-dimensional
conformation of a molecule is not known or not relevant for a prediction
task. Graph attention transformers allow for a less local information
exchange via attention layers, which learn to accumulate the features
of atoms both close and far away in the graph.<sup><xref ref-type="bibr" rid="ref37">37</xref>,<xref ref-type="bibr" rid="ref38">38</xref></sup> Another important line of research comprises the prediction of properties
dependent on the three-dimensional conformation of a molecule, such
as the prediction of properties obtained from quantum mechanics.<sup><xref ref-type="bibr" rid="ref2">2</xref>,<xref ref-type="bibr" rid="ref39">39</xref>−<xref ref-type="bibr" rid="ref41">41</xref></sup> Finally, transformer models from natural language
processing can be trained on string representations such as SMILES
or SELFIES, also leading to promising results.<sup><xref ref-type="bibr" rid="ref42">42</xref>−<xref ref-type="bibr" rid="ref45">45</xref></sup> In this work, we discuss our
application of GCNNs, namely, Chemprop,<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> a directed-message passing algorithm derived from the seminal work
of Gilmer et al.<sup><xref ref-type="bibr" rid="ref1">1</xref></sup></p>
    <p>An early version
of Chemprop was published in ref (<xref ref-type="bibr" rid="ref36">36</xref>). Since then, the software
has substantially evolved and now includes a vast collection of new
features. For example, Chemprop is now able to predict properties
for systems containing multiple molecules, such as solute/solvent
combinations or reactions with and without solvent. It can train on
molecular targets, spectra, or atom/bond-level targets and output
the latent representation for analysis of the learned feature embedding.
Available uncertainty metrics include popular approaches, such as
ensembling, mean-variance estimation, and evidential learning. Chemprop
is thus a general and versatile deep learning toolbox and enjoys a
wide user base.</p>
    <p>The remainder of the article is structured as
follows: First, we
summarize the architecture of Chemprop. We discuss a selection of
Chemprop features with a focus on features introduced after the initial
release of Chemprop. We then conclude the main body of the article
and provide details on the data and software, which we have open-sourced
including all scripts to allow for full reproducibility. Alongside
the main body of this article, we provide <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Supporting Information</ext-link> that contains further model design details; descriptions
of the data acquisition, preprocessing, and splitting of all data
sets used in benchmarking; and the results of Chemprop benchmarks
on a variety of data sets showcasing its performance on both simple
and advanced prediction tasks.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Model Structure</title>
    <p>Chemprop consists of four modules: (1) a local features encoding
function, (2) a directed message passing neural network (D-MPNN) to
learn atomic embeddings from the local features, (3) an aggregation
function to join atomic embeddings into molecular embeddings, and
(4) a standard feed-forward neural network (FFN) for the transformation
of molecular embeddings to target properties, summarized in <xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>. The D-MPNN is a
class of graph-convolutional neural networks (GCNN), which updates
hidden representations of the vertices <italic>V</italic> and edges <italic>E</italic> of a graph <italic>G</italic> based on the local environment.
In the following, we use bold lower case to denote vectors, bold upper
case to denote matrices, and italic light font for scalars and objects.</p>
    <fig id="fig1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Overview
of the architecture of Chemprop. The message passing update
of the hidden vector for directed edge 2 → 1 is expanded for
demonstration.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0001" id="gr1" position="float"/>
    </fig>
    <p>For a molecule, the molecule SMILES
string is used as input, which
is then transformed to a molecular graph using RDKit,<sup><xref ref-type="bibr" rid="ref46">46</xref></sup> where atoms correspond to vertices and bonds to edges.
Initial features are constructed based on the identity and topology
of each atom and bond. For each vertex <italic>v</italic>, initial
feature vectors {<bold>x</bold><sub><italic>v</italic></sub> |<italic>v</italic> ∈ <italic>V</italic>} are obtained from a one-hot
encoding of the atomic number, number of bonds linked to each atom,
formal charge, chirality (if encoded in the SMILES), number of hydrogens,
hybridization, and aromaticity of the atom, as well as the atomic
mass (divided by 100 for scaling). For each edge <italic>e</italic>, initial feature vectors {<bold>e</bold><sub><italic>vw</italic></sub>|{<italic>v</italic>, <italic>w</italic>} ∈ <italic>E</italic>} arise from the bond type, whether the bond is conjugated
or in a ring, and whether it contains stereochemical information,
such as a cis/trans double bond. The D-MPNN uses directed edges in
a graph to pass information, where each undirected edge (bond) has
two corresponding directed edges, one in each direction. Initial directed
edge features <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> are obtained via simple concatenation
of the atom features of the first atom of a bond <bold>x</bold><sub><italic>v</italic></sub> to the respective undirected bond features <bold>e</bold><sub><italic>vw</italic></sub><disp-formula id="eq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m001" position="anchor"/><label>1</label></disp-formula>where cat() denotes simple concatenation.
The directed edges <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> and <bold>e</bold><sub arrange="stack"><italic>wv</italic></sub><sup arrange="stack">d</sup> are distinguished only by the choice of which atom to use in <xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>. Chemprop also offers
the option to read in custom atom and bond features in addition to
or as a replacement for the default features, as described in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section S1.2, and thus offers full control
of the initial features. In summary, Module 1 of Chemprop constructs
atom and directed bond feature vectors <bold>x</bold><sub><italic>v</italic></sub> and <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> from the input molecules.</p>
    <p>The initial atom and bond features are then passed to a D-MPNN.
In a D-MPNN structure, messages are passed between directed edges
rather than between nodes as would be done in a traditional MPNN.
To construct the hidden directed edge features <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">0</sup> of hidden size <italic>h</italic>, the initial directed edge features <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> are passed through a single neural network
layer with learnable weights <inline-formula id="d33e432"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m002.gif"/></inline-formula><disp-formula id="eq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m003" position="anchor"/><label>2</label></disp-formula>and a nonlinear
activation function τ
which can be chosen by the user (default ReLU). The size <italic>h</italic> of <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">0</sup> can be chosen by the user (default 300). The
size of <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup>, which we term <italic>h</italic><sub><italic>i</italic></sub>, is set by the lengths of initial feature
encodings, per <xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>.
The directed edge features are then iteratively updated based on the
local environment via <italic>T</italic> (default 3) message passing
steps<disp-formula id="eq3"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m004" position="anchor"/><label>3</label></disp-formula>until <italic>t</italic> + 1 = <italic>T</italic>, where <inline-formula id="d33e474"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m005.gif"/></inline-formula> and <italic>N</italic>(<italic>v</italic>)\<italic>w</italic> denotes
the neighbors of node <italic>v</italic> excluding <italic>w</italic>. The opposite facing directed edge
is excluded from the message passing update for increased numerical
stability (see Mahé et al.<sup><xref ref-type="bibr" rid="ref47">47</xref></sup>). Finally,
the updated hidden states <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack"><italic>T</italic></sup> are
aggregated into atomic embeddings via<disp-formula id="eq4"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m006" position="anchor"/><label>4</label></disp-formula>where <bold>q</bold> is a concatenation of
the initial atom features <bold>x</bold><sub><italic>v</italic></sub> and the sum of all incoming directed edge hidden states<disp-formula id="eq5"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m007" position="anchor"/><label>5</label></disp-formula>with <inline-formula id="d33e513"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m008.gif"/></inline-formula>. Here, <italic>h</italic><sub><italic>o</italic></sub> is the size
of <bold>q</bold>, i.e., the sum of the hidden
size <italic>h</italic> and the size of <bold>x</bold><sub><italic>v</italic></sub>. In summary, in Module 2, the D-MPNN weights <bold>W</bold><sub><italic>i</italic></sub>, <bold>W</bold><sub><italic>h</italic></sub>, and <bold>W</bold><sub><italic>o</italic></sub> are learned from the training data, outputting learnable atomic
embeddings <bold>h</bold><sub><italic>v</italic></sub>. Customizations
and hyperparameter tuning include the choice of the activation function
τ, the hidden size <italic>h</italic>, and the number of message
passing steps <italic>T</italic>. Chemprop offers the option to add
bias terms to all neural network layers (defaults to False).</p>
    <p>The atomic embeddings <bold>h</bold><sub><italic>v</italic></sub> of all atoms in a molecule are then aggregated into a single molecular
embedding <bold>h</bold><sub><italic>m</italic></sub> via<disp-formula id="eq6"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m009" position="anchor"/><label>6</label></disp-formula>with<disp-formula id="eq7"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m010" position="anchor"/><label>7</label></disp-formula>where <bold>x</bold><sub><italic>m</italic></sub> is an optional
vector of additional molecular features.
Chemprop offers three aggregation options: summation (as shown in <xref rid="eq7" ref-type="disp-formula">eq <xref rid="eq7" ref-type="disp-formula">7</xref></xref>), a scaled sum (divided
by a user specified scaler, called a norm within Chemprop), or an
average (the default aggregation). Schweidtmann et al.<sup><xref ref-type="bibr" rid="ref48">48</xref></sup> compared the performance of such aggregation
functions for different data sets. The optional additional molecular
features, <bold>x</bold><sub><italic>m</italic></sub>, may be provided
features from outside sources (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section
S2) or generated engineered fingerprints (Morgan circular fingerprints<sup><xref ref-type="bibr" rid="ref49">49</xref></sup> and RDKIT 2D fingerprints<sup><xref ref-type="bibr" rid="ref46">46</xref></sup> are implemented in Chemprop). By default, <bold>x</bold><sub><italic>m</italic></sub> is empty such that the molecular embedding
is simply an aggregation over atomic embeddings, i.e., <bold>h</bold><sub><italic>m</italic></sub> = <bold>h</bold><sub arrange="stack"><italic>m</italic></sub><sup arrange="stack">′</sup> . In
summary, Module 3 produces molecular embeddings <bold>h</bold><sub><italic>m</italic></sub> of length <italic>h</italic> plus the size
of <bold>x</bold><sub><italic>m</italic></sub>. Chemprop offers the
option to circumvent Modules 1–3 and only using <bold>x</bold><sub><italic>m</italic></sub> as fixed molecular embedding, so that <bold>h</bold><sub><italic>m</italic></sub> = <bold>x</bold><sub><italic>m</italic></sub>.</p>
    <p>Finally, in the last module, molecular target
properties are learned
from the molecular embeddings <bold>h</bold><sub><italic>m</italic></sub> via a feed-forward neural network, where the number of layers
(default 2) and the number of hidden neurons (default 300) can be
chosen by the user. The number of input neurons is set by the length
of <bold>h</bold><sub><italic>m</italic></sub>, and the number of
output neurons is set by the number of targets. The activation function
between linear layers is set to be the same as in the D-MPNN, and
bias is turned on per default. For binary classification tasks, the
final model output is passed through a sigmoid function to constrain
values to the range (0,1). For multiclass classification, the final
model output is transformed with a softmax function, such that the
classification scores sum to 1 across classes.</p>
    <p>Chemprop is fully
end-to-end trainable, so that the weights for
D-MPNN and FFN are updated simultaneously. Users have the option to
train models using cross-validation and ensembles of submodels. By
default, a single model is trained on a random data split for 30 epochs.
We note that small data sets need a much larger number of epochs to
train and advise to check for convergence of the learning curve. Chemprop
uses the Adam optimizer.<sup><xref ref-type="bibr" rid="ref50">50</xref></sup> The default
learning rate schedule increases the learning rate linearly from 10<sup>–4</sup> to 10<sup>–3</sup> for the first two warmup
epochs and then decreases the learning rate exponentially from 10<sup>–3</sup> to 10<sup>–4</sup> for the remaining epochs.
By default, a batch size of 50 data points is used for each optimizer
step. Early stopping and dropout are available as means of regularization.
The PyTorch backend of Chemprop enables seamless GPU acceleration
of both model training and inference. The acceleration of training
and inference processes when used with a GPU can be significant, as
shown in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section S3.3.</p>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Discussion of Features</title>
    <p><xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref> lists a
nonexhaustive selection of studies based on Chemprop, showcasing its
versatility and applicability for the prediction of a large variety
of chemical properties, but also its ease of use. Models can be trained
and tested with a single line on the command line (or a few lines
of python code) and a user-supplied CSV file (see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for some examples). In the following, we discuss specialty
options introduced since its first release.</p>
    <table-wrap id="tbl1" position="float">
      <label>Table 1</label>
      <caption>
        <title>Selected
Published Studies Based on
Chemprop</title>
      </caption>
      <table frame="hsides" rules="groups" border="0">
        <colgroup>
          <col align="center"/>
          <col align="center"/>
          <col align="left"/>
          <col align="center"/>
          <col align="center"/>
          <col align="left"/>
        </colgroup>
        <thead>
          <tr>
            <th style="border:none;" align="center">ref</th>
            <th style="border:none;" align="center">Year</th>
            <th style="border:none;" align="center">Prediction</th>
            <th style="border:none;" align="center">ref</th>
            <th style="border:none;" align="center">Year</th>
            <th style="border:none;" align="center">Prediction</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref10">10</xref>)</td>
            <td style="border:none;" align="center">2020</td>
            <td style="border:none;" align="left">Growth inhibitory activity against <italic>E. coli</italic>; led to an identification of a potential new drug</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref51">51</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Absorption, distribution, metabolism, excretion
(ADME) properties
for drug discovery</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref52">52</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Chemical synergy against SARS-CoV-2; identified two drug combinations
with antiviral synergy in vitro</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref53">53</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Growth inhibitory activity against <italic>A. baumannii</italic>; led to an identification of a potential new drug</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref28">28</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">IR spectra of molecules</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref54">54</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Fuel properties</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref55">55</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Atomic charges, Fukui indices, NMR constants, bond lengths,
and bond orders</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref56">56</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Critical properties, acentric
factor, and phase change properties</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref57">57</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Lipophilicity</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref58">58</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Molecular optical peaks</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref59">59</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Reaction rates and barrier heights</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref60">60</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Lipophilicity</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref61">61</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Barrier heights of reactions</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref62">62</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Solvent effects on reaction rate constants</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref29">29</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Molecular optical peaks</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref63">63</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Vapor pressure in the low volatility regime</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref64">64</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Solvation free energy, solvation
enthalpy, and Abraham solute
descriptors</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref65">65</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Molecular optical peaks
and partition coefficients for closed-loop
active learning</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref66">66</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Solid solubility
of organic solutes in water and organic solvents</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref67">67</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Senolytic activity of compounds to selectively target senescent
cells</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref68">68</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Activity coefficients</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref69">69</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Toxicity measurements using 12 nuclear receptor
signaling and
stress response pathways</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Additional Features</title>
      <p>Chemprop can
take additional features at the molecule-, atom-, or bond-level as
input. While Chemprop often generates accurate models without requiring
any input beyond the SMILES, it has been shown that outside information
added as additional features can further improve performance.<sup><xref ref-type="bibr" rid="ref36">36</xref>,<xref ref-type="bibr" rid="ref64">64</xref></sup> Users can provide their custom additional features by adding keywords
and paths to the data files containing the features. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for command-line arguments and details.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Multimolecule Models</title>
      <p>Chemprop can
also train on a data set containing more than one molecule as input.
For example, when properties related to solvation need to be predicted,
both a solute and a solvent are required as input to the model. Users
can provide multiple molecules as inputs to Chemprop. When multiple
molecules are used, by default Chemprop trains a separate D-MPNN for
each molecule (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Figure S1a</ext-link>). If the option <monospace>--mpn_shared</monospace> is specified, then the same D-MPNN is used
for all molecules (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Figure S1b</ext-link>). The embeddings
of the different molecules are then concatenated prior to the FFN.
Note that the current implementation of multiple molecules in Chemprop
does not ensure permutational invariance toward the input molecules.
This is suited to situations where the input molecules have different
roles, e.g., molecule 1 = solute, molecule 2 = solvent. For additional
input information and a figure depicting the multimolecule model structure,
see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link>.</p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Reaction
Support</title>
      <p>Chemprop supports
the input of atom-mapped reactions, i.e., pairs of reactants and products
SMILES connected via the “≫” symbol,by using
the keyword <monospace>--reaction</monospace>. The pair of reactants
and products is transformed into a single pseudomolecule, namely,
the condensed graph of reaction (CGR), and then passed to a regular
D-MPNN block. The construction of a CGR within Chemprop is described
in detail in ref (<xref ref-type="bibr" rid="ref59">59</xref>) and summarized in the following. In general, the input of a reaction
vs a molecule only affects the setup of the graph object and its initial
features, but not any other part of the architecture. The graph of
a reaction has a different set of edges <italic>E</italic> as the
graph of a molecule, as shown in <xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref> for an example Diels–Alder reaction.
To build the CGR pseudomolecule, the set of atoms is obtained as the
union of the sets of atoms in the reactants and products. Similarly,
the set of bonds is obtained as the union of the sets of bonds in
the reactants and products. Once constructed, the CGR is passed through
the D-MPNN and other model architecture components in the same way
a molecular graph would. Optionally, Chemprop accepts an additional
molecule object as input, such as a solvent, a reagent, etc. which
is passed to its own D-MPNN similar to the multimolecule model. The
output of the reaction D-MPNN and molecule D-MPNN is concatenated
after atomic aggregation, before the FFN. This option is available
via the <monospace>reaction_solvent</monospace> keyword. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for further commandline options and details.</p>
      <fig id="fig2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Construction
of the condensed graph of reaction (CGR) of an example
reaction. The vertices and edges are obtained as the union of the
respective reactant and product vertices and edges. The features are
obtained as a combination of the reactant (white background) and product
(gray background) features for atoms and bonds.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0002" id="gr2" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Spectra Data Support</title>
      <p>Chemprop supports
the prediction of whole-spectrum properties for molecules. An initial
version of this capability was discussed in ref (<xref ref-type="bibr" rid="ref28">28</xref>) for use with IR absorbance
spectra. Targets for the spectra data set type are composed of an
array of intensity values, set at fixed bin locations typically specified
in terms of wavelengths, frequencies, or wavenumbers. Spectral Information
Divergence was originally developed as a method of comparing spectra
to reference databases<sup><xref ref-type="bibr" rid="ref70">70</xref></sup> and is adapted
in Chemprop to be used as a loss function, considering the deviation
of the spectrum as a whole rather than independently at each bin location.
The treatment of spectra can handle targets with gaps or missing values
within a data set. With the expectation that spectra will often be
collected in systems where a portion of the range will be obscured
or invalid (e.g., from solvent absorbance), Chemprop can create exclusion
regions in specified spectra where no predictions are provided and
targets are ignored for training purposes.</p>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Latent
Representations</title>
      <p>Graph neural
networks enable learning both molecular representation and property
end-to-end directly from the molecular graph. As detailed above in <xref rid="sec2" ref-type="other">Section <xref rid="sec2" ref-type="other">2</xref></xref>, the learned node
representations are aggregated into a molecule-level representation
after the message-passing phase, which we refer to as the “learned
fingerprint.” This embedding is then further fed into the FFN
network. Within the FFN, we consider the final hidden representation,
which we refer to as the “ffn embedding”. Both of these
vectors are latent representations of a molecule as it relates to
a particular trained model. Molecule latent representations can be
useful for data clustering or used as additional features in other
models. Chemprop supports the calculation of either from a trained
model for a given set of molecules.</p>
    </sec>
    <sec id="sec3.6">
      <label>3.6</label>
      <title>Loss
Function Options</title>
      <p>Chemprop can
train models according to many common loss functions. The loss functions
available for a given task are determined by the data set type (regression,
classification, multiclass, or spectra). Regression models can be
trained with mean squared error (MSE), bounded MSE (which allows inequalities
as targets), or negative log-likelihood (NLL) based on prediction
uncertainty distributions consistent with mean-variance estimation
(MVE)<sup><xref ref-type="bibr" rid="ref71">71</xref></sup> or evidential uncertainty.<sup><xref ref-type="bibr" rid="ref72">72</xref></sup> Classification tasks default to the binary cross
entropy loss and have additional options of Matthews correlation coefficient
(MCC) and Dirichlet (evidential classification).<sup><xref ref-type="bibr" rid="ref73">73</xref></sup> Cross entropy and MCC are also available for multiclass
problems. There are two options available for training on spectra:
spectral information divergence (SID)<sup><xref ref-type="bibr" rid="ref70">70</xref></sup> and first-order Wasserstein distance (a.k.a. earthmover’s
distance).<sup><xref ref-type="bibr" rid="ref74">74</xref></sup> Loss functions must be differentiable
since they are used to calculate gradients that update the model parameters,
but Chemprop also provides the option to use several nondifferentiable
metrics for model evaluation.</p>
    </sec>
    <sec id="sec3.7">
      <label>3.7</label>
      <title>Transfer
Learning</title>
      <p>Transfer learning
is a general strategy of using information gained through the training
of one model to inform and improve the training of a related model.
Often, this strategy is used to transfer information from a previously
trained model of a large data set to a model of a small data set in
order to improve the performance of the model of the small data set.
The simplest method of transfer learning would be taking predictions
or latent representations from one model and supplying them as additional
features to another model (<xref rid="sec3.1" ref-type="other">Sections <xref rid="sec3.1" ref-type="other">3.1</xref></xref> and <xref rid="sec3.5" ref-type="other">3.5</xref>).</p>
      <p>In Chemprop, different strategies are available to transfer learned
model parameters from a previously trained model to a new model as
a form of transfer learning, as shown in <xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>. A pretrained model may be used to initialize
a new model with normal updating of the transferred weights in training.
Alternatively, parameters from the transferred model can be frozen,
holding them constant during training. Freezing parameters always
include the D-MPNN weights but can be specified to include some FFN
layers as well. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for the corresponding
arguments.</p>
      <fig id="fig3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Options to transfer model parameters from a pretrained model (squared)
to a new model, by (a) initializing the new model parameters or by
freezing the (b) D-MPNN layers and (c) <italic>n</italic> FFN layers.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0003" id="gr3" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.8">
      <label>3.8</label>
      <title>Hyperparameter Optimization</title>
      <p>Chemprop
provides a command-line utility, allowing for the simple initiation
of hyperparameter optimization jobs. Options for the hyperparameter
job such as how many trials to carry out and which hyperparameters
to include in the search (options in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Table S3</ext-link>) can be specified with simple command-line arguments. The optimization
initially uses randomly sampled trials, followed by targeted sampling
using the Tree-structured Parzen Estimator algorithm.<sup><xref ref-type="bibr" rid="ref75">75</xref>,<xref ref-type="bibr" rid="ref76">76</xref></sup></p>
      <p>Hyperparameter optimization is often the most resource-intensive
step in model training. In order to search a large parameter space
adequately, a large number of trials is needed. Chemprop allows for
parallel operation of multiple hyperparameter optimization instances,
removing the need to carry out all trials in series and reducing the
wall time needed to perform the optimization significantly. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for the available hyperparameter options and
other details.</p>
    </sec>
    <sec id="sec3.9">
      <label>3.9</label>
      <title>Uncertainty Tools</title>
      <p>Chemprop includes
a variety of popular uncertainty estimation, calibration, and evaluation
tools. The estimation methods include deep ensembles,<sup><xref ref-type="bibr" rid="ref77">77</xref></sup> dropout,<sup><xref ref-type="bibr" rid="ref78">78</xref></sup> mean-variance estimation
(MVE),<sup><xref ref-type="bibr" rid="ref71">71</xref></sup> and evidential,<sup><xref ref-type="bibr" rid="ref72">72</xref></sup> as well as a special version of ensemble variance for spectral
predictions,<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> and the inherently probabilistic
outputs of classification models. After estimating the uncertainty
in a model’s predictions, it is often helpful to calibrate
these uncertainties to improve their performance on new predictions.
We provide four such methods for regression tasks (<italic>z</italic>-scaling,<sup><xref ref-type="bibr" rid="ref79">79</xref></sup> t-scaling, Zelikman’s
CRUDE,<sup><xref ref-type="bibr" rid="ref80">80</xref></sup> and MVE weighting<sup><xref ref-type="bibr" rid="ref81">81</xref></sup>) and two for classification (single-parameter Platt scaling<sup><xref ref-type="bibr" rid="ref82">82</xref></sup> and isotonic regression<sup><xref ref-type="bibr" rid="ref83">83</xref></sup>). In addition to standard metrics such as RMSE, MAE, etc. for evaluating
predictions, Chemprop also includes several metrics specifically for
evaluating the quality of uncertainty estimates. These include negative
log likelihood, Spearman rank correlation, expected normalized calibration
error (ENCE),<sup><xref ref-type="bibr" rid="ref84">84</xref></sup> and miscalibration area.<sup><xref ref-type="bibr" rid="ref84">84</xref></sup> Any valid classification or multiclass metric
used to assess predictions can also be used to assess uncertainties.</p>
    </sec>
    <sec id="sec3.10">
      <label>3.10</label>
      <title>Atom/Bond-Level Targets</title>
      <p>Chemprop
supports a multitask constrained D-MPNN architecture for predicting
atom- and bond-level properties, such as charge density or bond length.
This model enables a D-MPNN to be trained on multiple atomic and bond
properties simultaneously, though unlike molecular property targets,
they do not share a single FFN. Optionally, an attention-based constraining
method may be used to enforce that predicted atomic or bond properties
sum to a specified molecular net value, such as the overall charge
of a molecule. An initial, more limited version of this capability
was developed in ref (<xref ref-type="bibr" rid="ref55">55</xref>). For details on the input formats to be used for atom/bond targets,
both for training and for inference, see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link>.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Benchmarking</title>
    <p>See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for general performance benchmarks,
benchmarks using specific Chemprop features (atom/bond-level targets,
reaction support, multimolecule models, spectra prediction, and uncertainty
estimation), and system timing benchmarks.</p>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Conclusion</title>
    <p>We have presented the software package Chemprop, a powerful toolbox
for machine learning of the chemical properties of molecules and reactions.
Significant improvements have been made to the software since its
initial release and study,<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> including
the support of multimolecule properties, reactions, atom/bond-level
properties, and spectra. Additionally, several state-of-the-art approaches
to estimate the uncertainty of predictions have been incorporated
as well as pretraining and transfer learning procedures. Furthermore,
the code now offers a variety of customization options, such as custom
atom and bond features, a large variety of loss functions, and the
ability to save the learned feature embeddings for subsequent use
with different algorithms. We have showcased and benchmarked Chemprop
on a variety of example tasks and data sets and have found competitive
performances for molecular property prediction compared to other approaches
available on public leaderboards. In summary, Chemprop is a powerful,
fast, and convenient tool to learn conformation-independent properties
of molecules, sets of molecules, or reactions.</p>
  </sec>
</body>
<back>
  <notes notes-type="data-availability" id="notes-1">
    <title>Data Availability Statement</title>
    <p>Chemprop, including
all features described in this paper, is available under the open-source
MIT License on GitHub, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/chemprop/chemprop">github.com/chemprop/chemprop</uri>. An extensive documentation including
tutorials is available online,<sup><xref ref-type="bibr" rid="ref85">85</xref></sup> including
a workshop on YouTube.<sup><xref ref-type="bibr" rid="ref86">86</xref></sup> Scripts and data
splits to fully reproduce this study are available on GitHub, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/chemprop/chemprop_benchmark">github.com/chemprop/chemprop_benchmark</uri>, and on Zenodo, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://doi.org/10.5281/zenodo.8174267">doi.org/10.5281/zenodo.8174267</uri>, respectively.</p>
  </notes>
  <notes id="notes-2" notes-type="si">
    <title>Supporting Information Available</title>
    <p>The Supporting Information
is available free of charge at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acs.jcim.3c01250?goto=supporting-info">https://pubs.acs.org/doi/10.1021/acs.jcim.3c01250</ext-link>.<list id="silist" list-type="simple"><list-item><p>Additional software details
and usage examples, data
set and data handling details for benchmarking, and results of software
benchmarks (general performance, feature demonstrations, timing) (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">PDF</ext-link>)</p></list-item></list></p>
  </notes>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sifile1">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_si_001.pdf">
        <caption>
          <p>ci3c01250_si_001.pdf</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <notes notes-type="COI-statement" id="NOTES-d14e1343-autogenerated">
    <p>The authors declare no
competing financial interest.</p>
  </notes>
  <ack>
    <title>Acknowledgments</title>
    <p>E.H., Y.C., F.H.V., and W.H.G. acknowledge support from the
Machine Learning for Pharmaceutical Discovery and Synthesis Consortium
(MLPDS). K.P.G., S.-C.L., F.H.V., H.W., W.H.G., and C.J.M. acknowledge
support from the DARPA Accelerated Molecular Discovery (AMD) program
(DARPA HR00111920025). E.H. acknowledges support from the Austrian
Science Fund (FWF), project J-4415. K.P.G. was supported by the National
Science Foundation Graduate Research Fellowship Program under Grant
No. 1745302. D.E.G. acknowledges support from the MIT IBM Watson AI
Lab. F.H.V. would like to acknowledge the KU Leuven Internal Starting
Grant (STG/22/032). C.J.M. would like to acknowledge support from
VCU Startup Funding. Parts of the data reported within this paper
were generated with resources from the MIT SuperCloud Lincoln Laboratory
Supercomputing Center.<sup><xref ref-type="bibr" rid="ref87">87</xref></sup></p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="journal" id="cit1"><name><surname>Gilmer</surname><given-names>J.</given-names></name>; <name><surname>Schoenholz</surname><given-names>S. S.</given-names></name>; <name><surname>Riley</surname><given-names>P. F.</given-names></name>; <name><surname>Vinyals</surname><given-names>O.</given-names></name>; <name><surname>Dahl</surname><given-names>G. E.</given-names></name><article-title>Neural
Message Passing for Quantum Chemistry</article-title>. <source>Proceedings
of the International Conference on Machine Learning</source><year>2017</year>, <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="report" id="cit2"><person-group person-group-type="allauthors"><name><surname>Gasteiger</surname><given-names>J.</given-names></name>; <name><surname>Groß</surname><given-names>J.</given-names></name>; <name><surname>Günnemann</surname><given-names>S.</given-names></name></person-group><article-title>Directional Message
Passing for Molecular Graphs</article-title>. <source>Proceedings
of the International Conference on Learning Representations</source>, <bold>2003</bold>, arXiv:2003.03123.</mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="report" id="cit3"><person-group person-group-type="allauthors"><name><surname>Zhang</surname><given-names>S.</given-names></name>; <name><surname>Liu</surname><given-names>Y.</given-names></name>; <name><surname>Xie</surname><given-names>L.</given-names></name></person-group><article-title>Molecular Mechanics-Driven Graph Neural
Network with Multiplex Graph for Molecular Structures</article-title>. <source>Machine Learning for Molecules Workshop at NeurIPS</source>, <bold>2020</bold>, arXiv:2011.07457.</mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting Solubility Limits of Organic
Solutes for a Wide Range of Solvents and Temperatures</article-title>. <source>J. Am. Chem. Soc.</source><year>2022</year>, <volume>144</volume>, <fpage>10785</fpage>–<lpage>10797</lpage>. <pub-id pub-id-type="doi">10.1021/jacs.2c01768</pub-id>.<pub-id pub-id-type="pmid">35687887</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="journal" id="cit5"><name><surname>Dobbelaere</surname><given-names>M. R.</given-names></name>; <name><surname>Ureel</surname><given-names>Y.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Tomme</surname><given-names>L.</given-names></name>; <name><surname>Stevens</surname><given-names>C. V.</given-names></name>; <name><surname>Van Geem</surname><given-names>K. M.</given-names></name><article-title>Machine Learning for Physicochemical Property Prediction
of Complex Hydrocarbon Mixtures</article-title>. <source>Ind. Eng. Chem.
Res.</source><year>2022</year>, <volume>61</volume>, <fpage>8581</fpage>–<lpage>8594</lpage>. <pub-id pub-id-type="doi">10.1021/acs.iecr.2c00442</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal" id="cit6"><name><surname>Dobbelaere</surname><given-names>M. R.</given-names></name>; <name><surname>Plehiers</surname><given-names>P. P.</given-names></name>; <name><surname>Van de Vijver</surname><given-names>R.</given-names></name>; <name><surname>Stevens</surname><given-names>C. V.</given-names></name>; <name><surname>Van Geem</surname><given-names>K. M.</given-names></name><article-title>Learning
Molecular Representations for Thermochemistry Prediction of Cyclic
Hydrocarbons and Oxygenates</article-title>. <source>J. Phys. Chem.
A</source><year>2021</year>, <volume>125</volume>, <fpage>5166</fpage>–<lpage>5179</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpca.1c01956</pub-id>.<pub-id pub-id-type="pmid">34081474</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal" id="cit7"><name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Ben Hicham</surname><given-names>K.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name><article-title>Graph Neural
Networks for Temperature-Dependent Activity Coefficient Prediction
of Solutes in Ionic Liquids</article-title>. <source>Comput. Chem. Eng.</source><year>2023</year>, <volume>171</volume>, <fpage>108153</fpage><pub-id pub-id-type="doi">10.1016/j.compchemeng.2023.108153</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal" id="cit8"><name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Ritzert</surname><given-names>M.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Winkler</surname><given-names>S.</given-names></name>; <name><surname>Weber</surname><given-names>J. M.</given-names></name>; <name><surname>Morsch</surname><given-names>P.</given-names></name>; <name><surname>Heufer</surname><given-names>K. A.</given-names></name>; <name><surname>Grohe</surname><given-names>M.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name><article-title>Graph Machine Learning
for Design of High-Octane Fuels</article-title>. <source>AIChE J.</source><year>2023</year>, <volume>69</volume>, <fpage>e17971</fpage><pub-id pub-id-type="doi">10.1002/aic.17971</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="journal" id="cit9"><name><surname>Fleitmann</surname><given-names>L.</given-names></name>; <name><surname>Ackermann</surname><given-names>P.</given-names></name>; <name><surname>Schilling</surname><given-names>J.</given-names></name>; <name><surname>Kleinekorte</surname><given-names>J.</given-names></name>; <name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>vom Lehn</surname><given-names>F.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Pitsch</surname><given-names>H.</given-names></name>; <name><surname>Leonhard</surname><given-names>K.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name>; <name><surname>Bardow</surname><given-names>A.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name><article-title>Molecular Design of Fuels for Maximum
Spark-Ignition
Engine Efficiency by Combining Predictive Thermodynamics and Machine
Learning</article-title>. <source>Energ. Fuel.</source><year>2023</year>, <volume>37</volume>, <fpage>2213</fpage>–<lpage>2229</lpage>. <pub-id pub-id-type="doi">10.1021/acs.energyfuels.2c03296</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="journal" id="cit10"><name><surname>Stokes</surname><given-names>J. M.</given-names></name>; <name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Cubillos-Ruiz</surname><given-names>A.</given-names></name>; <name><surname>Donghia</surname><given-names>N. M.</given-names></name>; <name><surname>MacNair</surname><given-names>C. R.</given-names></name>; <name><surname>French</surname><given-names>S.</given-names></name>; <name><surname>Carfrae</surname><given-names>L. A.</given-names></name>; <name><surname>Bloom-Ackermann</surname><given-names>Z.</given-names></name>; et al. <article-title>A Deep Learning Approach to Antibiotic
Discovery</article-title>. <source>Cell</source><year>2020</year>, <volume>180</volume>, <fpage>688</fpage>–<lpage>702</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2020.01.021</pub-id>.<pub-id pub-id-type="pmid">32084340</pub-id></mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="report" id="cit11"><person-group person-group-type="allauthors"><name><surname>De Cao</surname><given-names>N.</given-names></name>; <name><surname>Kipf</surname><given-names>T.</given-names></name></person-group><article-title>MolGAN: An Implicit Generative
Model for Small Molecular Graphs</article-title>. <source>arXiv Preprint</source>, <bold>2022</bold>, arXiv:1805.11973.</mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="journal" id="cit12"><name><surname>Dan</surname><given-names>Y.</given-names></name>; <name><surname>Zhao</surname><given-names>Y.</given-names></name>; <name><surname>Li</surname><given-names>X.</given-names></name>; <name><surname>Li</surname><given-names>S.</given-names></name>; <name><surname>Hu</surname><given-names>M.</given-names></name>; <name><surname>Hu</surname><given-names>J.</given-names></name><article-title>Generative Adversarial
Networks (GAN) Based Efficient Sampling of
Chemical Composition Space for Inverse Design of Inorganic Materials</article-title>. <source>Npj Comput. Mater.</source><year>2020</year>, <volume>6</volume>, <fpage>1</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1038/s41524-020-00352-0</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="journal" id="cit13"><name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name>; <name><surname>Wei</surname><given-names>J. N.</given-names></name>; <name><surname>Duvenaud</surname><given-names>D.</given-names></name>; <name><surname>Hernández-Lobato</surname><given-names>J. M.</given-names></name>; <name><surname>Sánchez-Lengeling</surname><given-names>B.</given-names></name>; <name><surname>Sheberla</surname><given-names>D.</given-names></name>; <name><surname>Aguilera-Iparraguirre</surname><given-names>J.</given-names></name>; <name><surname>Hirzel</surname><given-names>T. D.</given-names></name>; <name><surname>Adams</surname><given-names>R. P.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name><article-title>Automatic
Chemical Design using a Data-Driven Continuous Representation of Molecules</article-title>. <source>ACS Cent. Sci.</source><year>2018</year>, <volume>4</volume>, <fpage>268</fpage>–<lpage>276</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.7b00572</pub-id>.<pub-id pub-id-type="pmid">29532027</pub-id></mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="journal" id="cit14"><name><surname>Wei</surname><given-names>J. N.</given-names></name>; <name><surname>Duvenaud</surname><given-names>D.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name><article-title>Neural Networks
for the Prediction
of Organic Chemistry Reactions</article-title>. <source>ACS Cent. Sci.</source><year>2016</year>, <volume>2</volume>, <fpage>725</fpage>–<lpage>732</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.6b00219</pub-id>.<pub-id pub-id-type="pmid">27800555</pub-id></mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="journal" id="cit15"><name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Waller</surname><given-names>M. P.</given-names></name><article-title>Neural-Symbolic
Machine Learning for Retrosynthesis
and Reaction Prediction</article-title>. <source>Chem. Eur. J.</source><year>2017</year>, <volume>23</volume>, <fpage>5966</fpage>–<lpage>5971</lpage>. <pub-id pub-id-type="doi">10.1002/chem.201605499</pub-id>.<pub-id pub-id-type="pmid">28134452</pub-id></mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal" id="cit16"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Prediction
of Organic Reaction Outcomes using Machine Learning</article-title>. <source>ACS Cent. Sci.</source><year>2017</year>, <volume>3</volume>, <fpage>434</fpage>–<lpage>443</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.7b00064</pub-id>.<pub-id pub-id-type="pmid">28573205</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="journal" id="cit17"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Machine Learning
in Computer-Aided
Synthesis Planning</article-title>. <source>Acc. Chem. Res.</source><year>2018</year>, <volume>51</volume>, <fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1021/acs.accounts.8b00087</pub-id>.<pub-id pub-id-type="pmid">29715002</pub-id></mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="journal" id="cit18"><name><surname>Szymkuć</surname><given-names>S.</given-names></name>; <name><surname>Gajewska</surname><given-names>E. P.</given-names></name>; <name><surname>Klucznik</surname><given-names>T.</given-names></name>; <name><surname>Molga</surname><given-names>K.</given-names></name>; <name><surname>Dittwald</surname><given-names>P.</given-names></name>; <name><surname>Startek</surname><given-names>M.</given-names></name>; <name><surname>Bajczyk</surname><given-names>M.</given-names></name>; <name><surname>Grzybowski</surname><given-names>B. A.</given-names></name><article-title>Computer-Assisted
Synthetic Planning: The End of the Beginning</article-title>. <source>Angew. Chem., Int. Ed.</source><year>2016</year>, <volume>55</volume>, <fpage>5904</fpage>–<lpage>5937</lpage>. <pub-id pub-id-type="doi">10.1002/anie.201506101</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="journal" id="cit19"><name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Preuss</surname><given-names>M.</given-names></name>; <name><surname>Waller</surname><given-names>M. P.</given-names></name><article-title>Planning Chemical Syntheses with
Deep Neural Networks and Symbolic AI</article-title>. <source>Nature</source><year>2018</year>, <volume>555</volume>, <fpage>604</fpage>–<lpage>610</lpage>. <pub-id pub-id-type="doi">10.1038/nature25978</pub-id>.<pub-id pub-id-type="pmid">29595767</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="journal" id="cit20"><name><surname>Kayala</surname><given-names>M. A.</given-names></name>; <name><surname>Azencott</surname><given-names>C.-A.</given-names></name>; <name><surname>Chen</surname><given-names>J. H.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>Learning to Predict
Chemical Reactions</article-title>. <source>J. Chem. Inf. Model.</source><year>2011</year>, <volume>51</volume>, <fpage>2209</fpage>–<lpage>2222</lpage>. <pub-id pub-id-type="doi">10.1021/ci200207y</pub-id>.<pub-id pub-id-type="pmid">21819139</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="journal" id="cit21"><name><surname>Kayala</surname><given-names>M. A.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>ReactionPredictor:
Prediction of Complex Chemical Reactions at the
Mechanistic Level using Machine Learning</article-title>. <source>J.
Chem. Inf. Model.</source><year>2012</year>, <volume>52</volume>, <fpage>2526</fpage>–<lpage>2540</lpage>. <pub-id pub-id-type="doi">10.1021/ci3003039</pub-id>.<pub-id pub-id-type="pmid">22978639</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="journal" id="cit22"><name><surname>Fooshee</surname><given-names>D.</given-names></name>; <name><surname>Mood</surname><given-names>A.</given-names></name>; <name><surname>Gutman</surname><given-names>E.</given-names></name>; <name><surname>Tavakoli</surname><given-names>M.</given-names></name>; <name><surname>Urban</surname><given-names>G.</given-names></name>; <name><surname>Liu</surname><given-names>F.</given-names></name>; <name><surname>Huynh</surname><given-names>N.</given-names></name>; <name><surname>Van Vranken</surname><given-names>D.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>Deep Learning for Chemical
Reaction Prediction</article-title>. <source>Mol. Syst. Des. Eng.</source><year>2018</year>, <volume>3</volume>, <fpage>442</fpage>–<lpage>452</lpage>. <pub-id pub-id-type="doi">10.1039/C7ME00107J</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="report" id="cit23"><person-group person-group-type="allauthors"><name><surname>Bradshaw</surname><given-names>J.</given-names></name>; <name><surname>Kusner</surname><given-names>M. J.</given-names></name>; <name><surname>Paige</surname><given-names>B.</given-names></name>; <name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Hernández-Lobato</surname><given-names>J. M.</given-names></name></person-group><article-title>A Generative Model for Electron Paths</article-title>. <source>Proceedings
of the International Conference on Learning Representations</source>, <bold>2019</bold>, arXiv:1805.10970.</mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="journal" id="cit24"><name><surname>Bi</surname><given-names>H.</given-names></name>; <name><surname>Wang</surname><given-names>H.</given-names></name>; <name><surname>Shi</surname><given-names>C.</given-names></name>; <name><surname>Coley</surname><given-names>C.</given-names></name>; <name><surname>Tang</surname><given-names>J.</given-names></name>; <name><surname>Guo</surname><given-names>H.</given-names></name><article-title>Non-Autoregressive
Electron Redistribution Modeling for Reaction
Prediction</article-title>. <source>Proceedings of the International
Conference on Machine Learning</source><year>2021</year>, <volume>139</volume>, <fpage>904</fpage>–<lpage>913</lpage>.</mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="journal" id="cit25"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Rogers</surname><given-names>L.</given-names></name>; <name><surname>Jamison</surname><given-names>T. F.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>A Graph-Convolutional Neural Network
Model for the Prediction of Chemical Reactivity</article-title>. <source>Chem. Sci.</source><year>2019</year>, <volume>10</volume>, <fpage>370</fpage>–<lpage>377</lpage>. <pub-id pub-id-type="doi">10.1039/C8SC04228D</pub-id>.<pub-id pub-id-type="pmid">30746086</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <mixed-citation publication-type="journal" id="cit26"><name><surname>Sacha</surname><given-names>M.</given-names></name>; <name><surname>Błaz</surname><given-names>M.</given-names></name>; <name><surname>Byrski</surname><given-names>P.</given-names></name>; <name><surname>Dabrowski-Tumanski</surname><given-names>P.</given-names></name>; <name><surname>Chrominski</surname><given-names>M.</given-names></name>; <name><surname>Loska</surname><given-names>R.</given-names></name>; <name><surname>Włodarczyk-Pruszynski</surname><given-names>P.</given-names></name>; <name><surname>Jastrzebski</surname><given-names>S.</given-names></name><article-title>Molecule Edit Graph Attention Network: Modeling Chemical
Reactions as Sequences of Graph Edits</article-title>. <source>J. Chem.
Inf. Model.</source><year>2021</year>, <volume>61</volume>, <fpage>3273</fpage>–<lpage>3284</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00537</pub-id>.<pub-id pub-id-type="pmid">34251814</pub-id></mixed-citation>
    </ref>
    <ref id="ref27">
      <mixed-citation publication-type="journal" id="cit27"><name><surname>Schwaller</surname><given-names>P.</given-names></name>; <name><surname>Gaudin</surname><given-names>T.</given-names></name>; <name><surname>Lanyi</surname><given-names>D.</given-names></name>; <name><surname>Bekas</surname><given-names>C.</given-names></name>; <name><surname>Laino</surname><given-names>T.</given-names></name><article-title>“Found
in Translation”: Predicting Outcomes of Complex Organic Chemistry
Reactions using Neural Sequence-to-Sequence Models</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>6091</fpage>–<lpage>6098</lpage>. <pub-id pub-id-type="doi">10.1039/C8SC02339E</pub-id>.<pub-id pub-id-type="pmid">30090297</pub-id></mixed-citation>
    </ref>
    <ref id="ref28">
      <mixed-citation publication-type="journal" id="cit28"><name><surname>McGill</surname><given-names>C.</given-names></name>; <name><surname>Forsuelo</surname><given-names>M.</given-names></name>; <name><surname>Guan</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting Infrared
Spectra with Message Passing Neural Networks</article-title>. <source>J. Chem. Inf. Model.</source><year>2021</year>, <volume>61</volume>, <fpage>2594</fpage>–<lpage>2609</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00055</pub-id>.<pub-id pub-id-type="pmid">34048221</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <mixed-citation publication-type="journal" id="cit29"><name><surname>Greenman</surname><given-names>K. P.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name><article-title>Multi-Fidelity
Prediction
of Molecular Optical Peaks with Deep Learning</article-title>. <source>Chem. Sci.</source><year>2022</year>, <volume>13</volume>, <fpage>1152</fpage>–<lpage>1162</lpage>. <pub-id pub-id-type="doi">10.1039/D1SC05677H</pub-id>.<pub-id pub-id-type="pmid">35211282</pub-id></mixed-citation>
    </ref>
    <ref id="ref30">
      <mixed-citation publication-type="journal" id="cit30"><name><surname>Dührkop</surname><given-names>K.</given-names></name><article-title>Deep Kernel
Learning Improves Molecular Fingerprint Prediction from Tandem Mass
Spectra</article-title>. <source>Bioinf.</source><year>2022</year>, <volume>38</volume>, <fpage>i342</fpage>–<lpage>i349</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac260</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref31">
      <mixed-citation publication-type="journal" id="cit31"><name><surname>Nguyen</surname><given-names>D. H.</given-names></name>; <name><surname>Nguyen</surname><given-names>C. H.</given-names></name>; <name><surname>Mamitsuka</surname><given-names>H.</given-names></name><article-title>ADAPTIVE:
leArning DAta-dePendenT,
concIse molecular VEctors for Fast, Accurate Metabolite Identification
from Tandem Mass Spectra</article-title>. <source>Bioinf.</source><year>2019</year>, <volume>35</volume>, <fpage>i164</fpage>–<lpage>i172</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz319</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <mixed-citation publication-type="journal" id="cit32"><name><surname>Nguyen</surname><given-names>D. H.</given-names></name>; <name><surname>Nguyen</surname><given-names>C. H.</given-names></name>; <name><surname>Mamitsuka</surname><given-names>H.</given-names></name><article-title>Recent Advances
and Prospects of
Computational Methods for Metabolite Identification: A Review with
Emphasis on Machine Learning Approaches</article-title>. <source>Brief.
Bioinf.</source><year>2019</year>, <volume>20</volume>, <fpage>2028</fpage>–<lpage>2043</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bby066</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <mixed-citation publication-type="journal" id="cit33"><name><surname>Stravs</surname><given-names>M.
A.</given-names></name>; <name><surname>Dührkop</surname><given-names>K.</given-names></name>; <name><surname>Böcker</surname><given-names>S.</given-names></name>; <name><surname>Zamboni</surname><given-names>N.</given-names></name><article-title>MSNovelist: De Novo
Structure Generation from Mass Spectra</article-title>. <source>Nat.
Methods</source><year>2022</year>, <volume>19</volume>, <fpage>865</fpage>–<lpage>870</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-022-01486-3</pub-id>.<pub-id pub-id-type="pmid">35637304</pub-id></mixed-citation>
    </ref>
    <ref id="ref34">
      <mixed-citation publication-type="journal" id="cit34"><name><surname>Muratov</surname><given-names>E. N.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name>; <name><surname>Sheridan</surname><given-names>R. P.</given-names></name>; <name><surname>Tetko</surname><given-names>I. V.</given-names></name>; <name><surname>Filimonov</surname><given-names>D.</given-names></name>; <name><surname>Poroikov</surname><given-names>V.</given-names></name>; <name><surname>Oprea</surname><given-names>T. I.</given-names></name>; <name><surname>Baskin</surname><given-names>I. I.</given-names></name>; <name><surname>Varnek</surname><given-names>A.</given-names></name>; <name><surname>Roitberg</surname><given-names>A.</given-names></name>; et al. <article-title>QSAR without Borders</article-title>. <source>Chem. Soc. Rev.</source><year>2020</year>, <volume>49</volume>, <fpage>3525</fpage>–<lpage>3564</lpage>. <pub-id pub-id-type="doi">10.1039/D0CS00098A</pub-id>.<pub-id pub-id-type="pmid">32356548</pub-id></mixed-citation>
    </ref>
    <ref id="ref35">
      <mixed-citation publication-type="journal" id="cit35"><name><surname>Kearnes</surname><given-names>S.</given-names></name>; <name><surname>McCloskey</surname><given-names>K.</given-names></name>; <name><surname>Berndl</surname><given-names>M.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Riley</surname><given-names>P.</given-names></name><article-title>Molecular
Graph Convolutions: Moving Beyond Fingerprints</article-title>. <source>J. Comput. Aided Mol. Des.</source><year>2016</year>, <volume>30</volume>, <fpage>595</fpage>–<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>.<pub-id pub-id-type="pmid">27558503</pub-id></mixed-citation>
    </ref>
    <ref id="ref36">
      <mixed-citation publication-type="journal" id="cit36"><name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Coley</surname><given-names>C.</given-names></name>; <name><surname>Eiden</surname><given-names>P.</given-names></name>; <name><surname>Gao</surname><given-names>H.</given-names></name>; <name><surname>Guzman-Perez</surname><given-names>A.</given-names></name>; <name><surname>Hopper</surname><given-names>T.</given-names></name>; <name><surname>Kelley</surname><given-names>B.</given-names></name>; <name><surname>Mathea</surname><given-names>M.</given-names></name>; <name><surname>Palmer</surname><given-names>A.</given-names></name>; <name><surname>Settels</surname><given-names>V.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T.</given-names></name>; <name><surname>Jensen</surname><given-names>K.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name><article-title>Analyzing
Learned Molecular Representations for Property
Prediction</article-title>. <source>J. Chem. Inf. Model.</source><year>2019</year>, <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="ref37">
      <mixed-citation publication-type="report" id="cit37"><person-group person-group-type="allauthors"><name><surname>Maziarka</surname><given-names>Ł.</given-names></name>; <name><surname>Danel</surname><given-names>T.</given-names></name>; <name><surname>Mucha</surname><given-names>S.</given-names></name>; <name><surname>Rataj</surname><given-names>K.</given-names></name>; <name><surname>Tabor</surname><given-names>J.</given-names></name>; <name><surname>Jastrzebski</surname><given-names>S.</given-names></name></person-group><article-title>Molecule Attention Transformer</article-title>. <source>arXiv Preprint</source>, <bold>2020</bold>, arXiv:2002.08264.</mixed-citation>
    </ref>
    <ref id="ref38">
      <mixed-citation publication-type="journal" id="cit38"><name><surname>Kreuzer</surname><given-names>D.</given-names></name>; <name><surname>Beaini</surname><given-names>D.</given-names></name>; <name><surname>Hamilton</surname><given-names>W.</given-names></name>; <name><surname>Létourneau</surname><given-names>V.</given-names></name>; <name><surname>Tossou</surname><given-names>P.</given-names></name><article-title>Rethinking Graph Transformers with Spectral Attention</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2021</year>, <volume>34</volume>, <fpage>21618</fpage>–<lpage>21629</lpage>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <mixed-citation publication-type="journal" id="cit39"><name><surname>Schütt</surname><given-names>K. T.</given-names></name>; <name><surname>Sauceda</surname><given-names>H. E.</given-names></name>; <name><surname>Kindermans</surname><given-names>P.-J.</given-names></name>; <name><surname>Tkatchenko</surname><given-names>A.</given-names></name>; <name><surname>Müller</surname><given-names>K.-R.</given-names></name><article-title>Schnet–A Deep Learning Architecture
for Molecules
and Materials</article-title>. <source>J. Chem. Phys.</source><year>2018</year>, <volume>148</volume>, <fpage>241722</fpage><pub-id pub-id-type="doi">10.1063/1.5019779</pub-id>.<pub-id pub-id-type="pmid">29960322</pub-id></mixed-citation>
    </ref>
    <ref id="ref40">
      <mixed-citation publication-type="journal" id="cit40"><name><surname>Unke</surname><given-names>O. T.</given-names></name>; <name><surname>Meuwly</surname><given-names>M.</given-names></name><article-title>PhysNet: A Neural Network
for Predicting Energies,
Forces, Dipole Moments, and Partial Charges</article-title>. <source>J. Chem. Theory Comput.</source><year>2019</year>, <volume>15</volume>, <fpage>3678</fpage>–<lpage>3693</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.9b00181</pub-id>.<pub-id pub-id-type="pmid">31042390</pub-id></mixed-citation>
    </ref>
    <ref id="ref41">
      <mixed-citation publication-type="report" id="cit41"><person-group person-group-type="allauthors"><name><surname>Bigi</surname><given-names>F.</given-names></name>, <name><surname>Pozdnyakov</surname><given-names>S. N.</given-names></name>, <name><surname>Ceriotti</surname><given-names>M.</given-names></name></person-group><article-title>Wigner
Kernels:
Body-Ordered Equivariant Machine Learning without a Basis</article-title>. <source>arXiv Preprint</source>, <bold>2023</bold>, arXiv:2303.04124.</mixed-citation>
    </ref>
    <ref id="ref42">
      <mixed-citation publication-type="journal" id="cit42"><name><surname>Winter</surname><given-names>B.</given-names></name>; <name><surname>Winter</surname><given-names>C.</given-names></name>; <name><surname>Schilling</surname><given-names>J.</given-names></name>; <name><surname>Bardow</surname><given-names>A.</given-names></name><article-title>A Smile is All you
Need: Predicting Limiting Activity Coefficients from SMILES with Natural
Language Processing</article-title>. <source>Digital Discovery</source><year>2022</year>, <volume>1</volume>, <fpage>859</fpage>–<lpage>869</lpage>. <pub-id pub-id-type="doi">10.1039/D2DD00058J</pub-id>.<pub-id pub-id-type="pmid">36561987</pub-id></mixed-citation>
    </ref>
    <ref id="ref43">
      <mixed-citation publication-type="journal" id="cit43"><name><surname>Bagal</surname><given-names>V.</given-names></name>; <name><surname>Aggarwal</surname><given-names>R.</given-names></name>; <name><surname>Vinod</surname><given-names>P.</given-names></name>; <name><surname>Priyakumar</surname><given-names>U. D.</given-names></name><article-title>MolGPT:
Molecular Generation using a Transformer-Decoder Model</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>2064</fpage>–<lpage>2076</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00600</pub-id>.<pub-id pub-id-type="pmid">34694798</pub-id></mixed-citation>
    </ref>
    <ref id="ref44">
      <mixed-citation publication-type="report" id="cit44"><person-group person-group-type="allauthors"><name><surname>Honda</surname><given-names>S.</given-names></name>; <name><surname>Shi</surname><given-names>S.</given-names></name>; <name><surname>Ueda</surname><given-names>H.
R.</given-names></name></person-group><article-title>Smiles Transformer:
Pre-trained
Molecular Fingerprint for Low Data Drug Discovery</article-title>. <source>arXiv Preprint</source>, <bold>2019</bold>, arXiv:1911.04738.</mixed-citation>
    </ref>
    <ref id="ref45">
      <mixed-citation publication-type="conf-proc" id="cit45"><person-group person-group-type="allauthors"><name><surname>Chithrananda</surname><given-names>S.</given-names></name>; <name><surname>Grand</surname><given-names>G.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name></person-group><article-title>Chemberta: Large-Scale
Self-Supervised Pretraining for Molecular Property Prediction</article-title>. <source>Machine Learning for Molecules Workshop at NeurIPS</source>, <bold>2020</bold>, arXiv:2010.09885.</mixed-citation>
    </ref>
    <ref id="ref46">
      <mixed-citation publication-type="weblink" id="cit46"><person-group person-group-type="allauthors"><name><surname>Landrum</surname><given-names>G.</given-names></name></person-group><article-title>RDKit: Open-Source
Cheminformatics</article-title>. <bold>2006</bold>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.rdkit.org/">https://www.rdkit.org/</uri>.</mixed-citation>
    </ref>
    <ref id="ref47">
      <mixed-citation publication-type="conf-proc" id="cit47"><person-group person-group-type="allauthors"><name><surname>Mahé</surname><given-names>P.</given-names></name>; <name><surname>Ueda</surname><given-names>N.</given-names></name>; <name><surname>Akutsu</surname><given-names>T.</given-names></name>; <name><surname>Perret</surname><given-names>J.-L.</given-names></name>; <name><surname>Vert</surname><given-names>J.-P.</given-names></name></person-group><article-title>Extensions
of marginalized graph kernels</article-title>. <source>Proceedings
of the International Conference on Machine Learning</source>, <bold>2004</bold>, <volume>70</volume>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <mixed-citation publication-type="journal" id="cit48"><name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Weber</surname><given-names>J. M.</given-names></name>; <name><surname>Grohe</surname><given-names>M.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name>; <name><surname>Leonhard</surname><given-names>K.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name><article-title>Physical Pooling Functions in Graph
Neural Networks for Molecular Property Prediction</article-title>. <source>Comput. Chem. Eng.</source><year>2023</year>, <volume>172</volume>, <fpage>108202</fpage><pub-id pub-id-type="doi">10.1016/j.compchemeng.2023.108202</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref49">
      <mixed-citation publication-type="journal" id="cit49"><name><surname>Morgan</surname><given-names>H. L.</given-names></name><article-title>The Generation
of a Unique Machine Description for Chemical Structures – A
Technique Developed at Chemical Abstracts Service</article-title>. <source>J. Chem. Doc.</source><year>1965</year>, <volume>5</volume>, <fpage>107</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1021/c160017a018</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref50">
      <mixed-citation publication-type="report" id="cit50"><person-group person-group-type="allauthors"><name><surname>Kingma</surname><given-names>D. P.</given-names></name>; <name><surname>Ba</surname><given-names>J.</given-names></name></person-group><article-title>Adam: A Method for Stochastic
Optimization</article-title>. <source>Proceedings of the International
Conference on Learning Representations</source>, <bold>2017</bold>, arXiv:1412.6980.</mixed-citation>
    </ref>
    <ref id="ref51">
      <mixed-citation publication-type="journal" id="cit51"><name><surname>Lim</surname><given-names>M. A.</given-names></name>; <name><surname>Yang</surname><given-names>S.</given-names></name>; <name><surname>Mai</surname><given-names>H.</given-names></name>; <name><surname>Cheng</surname><given-names>A. C.</given-names></name><article-title>Exploring Deep Learning
of Quantum Chemical Properties for Absorption, Distribution, Metabolism,
and Excretion Predictions</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>6336</fpage>–<lpage>6341</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c00245</pub-id>.<pub-id pub-id-type="pmid">35758421</pub-id></mixed-citation>
    </ref>
    <ref id="ref52">
      <mixed-citation publication-type="journal" id="cit52"><name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Stokes</surname><given-names>J. M.</given-names></name>; <name><surname>Eastman</surname><given-names>R. T.</given-names></name>; <name><surname>Itkin</surname><given-names>Z.</given-names></name>; <name><surname>Zakharov</surname><given-names>A. V.</given-names></name>; <name><surname>Collins</surname><given-names>J. J.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name><article-title>Deep Learning Identifies
Synergistic Drug Combinations for Treating COVID-19</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><year>2021</year>, <volume>118</volume>, <fpage>e2105070118</fpage><pub-id pub-id-type="doi">10.1073/pnas.2105070118</pub-id>.<pub-id pub-id-type="pmid">34526388</pub-id></mixed-citation>
    </ref>
    <ref id="ref53">
      <mixed-citation publication-type="journal" id="cit53"><name><surname>Liu</surname><given-names>G.</given-names></name>; <name><surname>Catacutan</surname><given-names>D. B.</given-names></name>; <name><surname>Rathod</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Mohammed</surname><given-names>J. C.</given-names></name>; <name><surname>Chiappino-Pepe</surname><given-names>A.</given-names></name>; <name><surname>Syed</surname><given-names>S. A.</given-names></name>; <name><surname>Fragis</surname><given-names>M.</given-names></name>; <name><surname>Rachwalski</surname><given-names>K.</given-names></name>; et al. <article-title>Deep Learning-Guided Discovery of an Antibiotic
Targeting Acinetobacter Baumannii</article-title>. <source>Nat. Chem.
Biol.</source><year>2023</year>, <volume>19</volume>, <fpage>1342</fpage>–<lpage>1350</lpage>. <pub-id pub-id-type="doi">10.1038/s41589-023-01349-8</pub-id>.<pub-id pub-id-type="pmid">37231267</pub-id></mixed-citation>
    </ref>
    <ref id="ref54">
      <mixed-citation publication-type="journal" id="cit54"><name><surname>Larsson</surname><given-names>T.</given-names></name>; <name><surname>Vermeire</surname><given-names>F.</given-names></name>; <name><surname>Verhelst</surname><given-names>S.</given-names></name><article-title>Machine Learning for Fuel Property
Predictions: A Multi-Task and Transfer Learning Approach</article-title>. <source>SAE Technical Paper</source><year>2023</year>, <pub-id pub-id-type="doi">10.4271/2023-01-0337</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref55">
      <mixed-citation publication-type="journal" id="cit55"><name><surname>Guan</surname><given-names>Y.</given-names></name>; <name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Ranasinghe</surname><given-names>D.</given-names></name>; <name><surname>Heid</surname><given-names>E.</given-names></name>; <name><surname>Struble</surname><given-names>T. J.</given-names></name>; <name><surname>Pattanaik</surname><given-names>L.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Regio-Selectivity Prediction with
a Machine-Learned Reaction Representation and On-the-Fly Quantum Mechanical
Descriptors</article-title>. <source>Chem. Sci.</source><year>2021</year>, <volume>12</volume>, <fpage>2198</fpage>–<lpage>2208</lpage>. <pub-id pub-id-type="doi">10.1039/D0SC04823B</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref56">
      <mixed-citation publication-type="journal" id="cit56"><name><surname>Biswas</surname><given-names>S.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Ramirez</surname><given-names>J.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting
Critical Properties and Acentric Factor of Fluids using Multi-Task
Machine Learning</article-title>. <source>J. Chem. Inf. Model.</source><year>2023</year>, <volume>63</volume>, <fpage>4574</fpage>–<lpage>4588</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.3c00546</pub-id>.<pub-id pub-id-type="pmid">37487557</pub-id></mixed-citation>
    </ref>
    <ref id="ref57">
      <mixed-citation publication-type="journal" id="cit57"><name><surname>Lenselink</surname><given-names>E. B.</given-names></name>; <name><surname>Stouten</surname><given-names>P. F. W.</given-names></name><article-title>Multitask machine
learning models for predicting lipophilicity
(logP) in the SAMPL7 challenge</article-title>. <source>Journal of Computer-Aided
Molecular Design</source><year>2021</year>, <volume>35</volume>, <fpage>901</fpage>–<lpage>909</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-021-00405-6</pub-id>.<pub-id pub-id-type="pmid">34273053</pub-id></mixed-citation>
    </ref>
    <ref id="ref58">
      <mixed-citation publication-type="journal" id="cit58"><name><surname>McNaughton</surname><given-names>A. D.</given-names></name>; <name><surname>Joshi</surname><given-names>R. P.</given-names></name>; <name><surname>Knutson</surname><given-names>C. R.</given-names></name>; <name><surname>Fnu</surname><given-names>A.</given-names></name>; <name><surname>Luebke</surname><given-names>K. J.</given-names></name>; <name><surname>Malerich</surname><given-names>J. P.</given-names></name>; <name><surname>Madrid</surname><given-names>P. B.</given-names></name>; <name><surname>Kumar</surname><given-names>N.</given-names></name><article-title>Machine Learning
Models
for Predicting Molecular UV–Vis Spectra with Quantum Mechanical
Properties</article-title>. <source>J. Chem. Inf. Model.</source><year>2023</year>, <volume>63</volume>, <fpage>1462</fpage>–<lpage>1471</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c01662</pub-id>.<pub-id pub-id-type="pmid">36847578</pub-id></mixed-citation>
    </ref>
    <ref id="ref59">
      <mixed-citation publication-type="journal" id="cit59"><name><surname>Heid</surname><given-names>E.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Machine Learning
of Reaction Properties via Learned
Representations of the Condensed Graph of Reaction</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>2101</fpage>–<lpage>2110</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00975</pub-id>.<pub-id pub-id-type="pmid">34734699</pub-id></mixed-citation>
    </ref>
    <ref id="ref60">
      <mixed-citation publication-type="journal" id="cit60"><name><surname>Isert</surname><given-names>C.</given-names></name>; <name><surname>Kromann</surname><given-names>J. C.</given-names></name>; <name><surname>Stiefl</surname><given-names>N.</given-names></name>; <name><surname>Schneider</surname><given-names>G.</given-names></name>; <name><surname>Lewis</surname><given-names>R. A.</given-names></name><article-title>Machine Learning for Fast, Quantum Mechanics-Based
Approximation of Drug Lipophilicity</article-title>. <source>ACS Omega</source><year>2023</year>, <volume>8</volume>, <fpage>2046</fpage>–<lpage>2056</lpage>. <pub-id pub-id-type="doi">10.1021/acsomega.2c05607</pub-id>.<pub-id pub-id-type="pmid">36687099</pub-id></mixed-citation>
    </ref>
    <ref id="ref61">
      <mixed-citation publication-type="journal" id="cit61"><name><surname>Spiekermann</surname><given-names>K. A.</given-names></name>; <name><surname>Pattanaik</surname><given-names>L.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Fast Predictions of Reaction Barrier
Heights: Toward Coupled-Cluster Accuracy</article-title>. <source>J.
Phys. Chem. A</source><year>2022</year>, <volume>126</volume>, <fpage>3976</fpage>–<lpage>3986</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpca.2c02614</pub-id>.<pub-id pub-id-type="pmid">35727075</pub-id></mixed-citation>
    </ref>
    <ref id="ref62">
      <mixed-citation publication-type="report" id="cit62"><person-group person-group-type="allauthors"><name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W.
H.</given-names></name></person-group><article-title>Machine Learning
from Quantum Chemistry to Predict Experimental Solvent Effects on
Reaction Rates</article-title>. <source>ChemRxiv Preprint</source>, <bold>2023</bold>.</mixed-citation>
    </ref>
    <ref id="ref63">
      <mixed-citation publication-type="journal" id="cit63"><name><surname>Lansford</surname><given-names>J.
L.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name>; <name><surname>Barnes</surname><given-names>B. C.</given-names></name><article-title>Physics-informed Transfer Learning
for Out-of-sample Vapor Pressure Predictions</article-title>. <source>Propellants, Explosives, Pyrotechnics</source><year>2023</year>, <volume>48</volume>, <fpage>e202200265</fpage><pub-id pub-id-type="doi">10.1002/prep.202200265</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref64">
      <mixed-citation publication-type="journal" id="cit64"><name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Walker</surname><given-names>P. J.</given-names></name>; <name><surname>Abraham</surname><given-names>M. H.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Group Contribution and Machine Learning Approaches
to Predict Abraham Solute Parameters, Solvation Free Energy, and Solvation
Enthalpy</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>433</fpage>–<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c01103</pub-id>.<pub-id pub-id-type="pmid">35044781</pub-id></mixed-citation>
    </ref>
    <ref id="ref65">
      <mixed-citation publication-type="journal" id="cit65"><name><surname>Koscher</surname><given-names>B. A.</given-names></name>; <name><surname>Canty</surname><given-names>R. B.</given-names></name>; <name><surname>McDonald</surname><given-names>M. A.</given-names></name>; <name><surname>Greenman</surname><given-names>K. P.</given-names></name>; <name><surname>McGill</surname><given-names>C. J.</given-names></name>; <name><surname>Bilodeau</surname><given-names>C. L.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Jin</surname><given-names>B.</given-names></name>; <name><surname>Hart</surname><given-names>T.</given-names></name>; <name><surname>Kulesza</surname><given-names>T.</given-names></name>; <name><surname>Li</surname><given-names>S.-C.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Gomez-Bombarelli</surname><given-names>R.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Autonomous,
Multiproperty-Driven Molecular Discovery: From Predictions to Measurements
and Back</article-title>. <source>Science</source><year>2023</year>, <volume>382</volume> (<issue>6677</issue>), <fpage>eadi1407</fpage><pub-id pub-id-type="doi">10.1126/science.adi1407</pub-id>.<pub-id pub-id-type="pmid">38127734</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref66">
      <mixed-citation publication-type="journal" id="cit66"><name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting
Solubility Limits of Organic
Solutes for a Wide Range of Solvents and Temperatures</article-title>. <source>J. Am. Chem. Soc.</source><year>2022</year>, <volume>144</volume>, <fpage>10785</fpage>–<lpage>10797</lpage>. <pub-id pub-id-type="doi">10.1021/jacs.2c01768</pub-id>.<pub-id pub-id-type="pmid">35687887</pub-id></mixed-citation>
    </ref>
    <ref id="ref67">
      <mixed-citation publication-type="journal" id="cit67"><name><surname>Wong</surname><given-names>F.</given-names></name>; <name><surname>Omori</surname><given-names>S.</given-names></name>; <name><surname>Donghia</surname><given-names>N. M.</given-names></name>; <name><surname>Zheng</surname><given-names>E. J.</given-names></name>; <name><surname>Collins</surname><given-names>J. J.</given-names></name><article-title>Discovering
Small-Molecule Senolytics with Deep Neural Networks</article-title>. <source>Nature Aging</source><year>2023</year>, <volume>3</volume>, <fpage>734</fpage>–<lpage>750</lpage>. <pub-id pub-id-type="doi">10.1038/s43587-023-00415-z</pub-id>.<pub-id pub-id-type="pmid">37142829</pub-id></mixed-citation>
    </ref>
    <ref id="ref68">
      <mixed-citation publication-type="conf-proc" id="cit68"><person-group><name><surname>Felton</surname><given-names>K. C.</given-names></name>; <name><surname>Ben-Safar</surname><given-names>H.</given-names></name>; <name><surname>Alexei</surname><given-names>A.</given-names></name></person-group><article-title>DeepGamma: A Deep Learning
Model for
Activity Coefficient Prediction</article-title>. <source>1st Annual
AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)</source>, <bold>2022</bold>.</mixed-citation>
    </ref>
    <ref id="ref69">
      <mixed-citation publication-type="journal" id="cit69"><name><surname>Colomba</surname><given-names>M.</given-names></name>; <name><surname>Benedetti</surname><given-names>S.</given-names></name>; <name><surname>Fraternale</surname><given-names>D.</given-names></name>; <name><surname>Guidarelli</surname><given-names>A.</given-names></name>; <name><surname>Coppari</surname><given-names>S.</given-names></name>; <name><surname>Freschi</surname><given-names>V.</given-names></name>; <name><surname>Crinelli</surname><given-names>R.</given-names></name>; <name><surname>Kass</surname><given-names>G. E. N.</given-names></name>; <name><surname>Gorassini</surname><given-names>A.</given-names></name>; <name><surname>Verardo</surname><given-names>G.</given-names></name>; <name><surname>Roselli</surname><given-names>C.</given-names></name>; <name><surname>Meli</surname><given-names>M. A.</given-names></name>; <name><surname>Di Giacomo</surname><given-names>B.</given-names></name>; <name><surname>Albertini</surname><given-names>M. C.</given-names></name><article-title>Nrf2-Mediated Pathway Activated by
Prunus spinosa L. (Rosaceae) Fruit Extract: Bioinformatics Analyses
and Experimental Validation</article-title>. <source>Nutrients</source><year>2023</year>, <volume>15</volume>, <fpage>2132</fpage><pub-id pub-id-type="doi">10.3390/nu15092132</pub-id>.<pub-id pub-id-type="pmid">37432298</pub-id></mixed-citation>
    </ref>
    <ref id="ref70">
      <mixed-citation publication-type="journal" id="cit70"><name><surname>Chang</surname><given-names>C.-I.</given-names></name><article-title>An Information-Theoretic
Approach to Spectral Variability, Similarity, and Discrimination for
Hyperspectral Image Analysis</article-title>. <source>IEEE Transactions
on Information Theory</source><year>2000</year>, <volume>46</volume>, <fpage>1927</fpage>–<lpage>1932</lpage>. <pub-id pub-id-type="doi">10.1109/18.857802</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref71">
      <mixed-citation publication-type="journal" id="cit71"><name><surname>Nix</surname><given-names>D. A.</given-names></name>; <name><surname>Weigend</surname><given-names>A. S.</given-names></name><article-title>Estimating the Mean and Variance of the Target Probability
Distribution</article-title>. <source>Proceedings of IEEE International
Conference on Neural Networks</source><year>1994</year>, <volume>1</volume>, <fpage>55</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="ref72">
      <mixed-citation publication-type="journal" id="cit72"><name><surname>Amini</surname><given-names>A.</given-names></name>; <name><surname>Schwarting</surname><given-names>W.</given-names></name>; <name><surname>Soleimany</surname><given-names>A.</given-names></name>; <name><surname>Rus</surname><given-names>D.</given-names></name><article-title>Deep Evidential Regression</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2020</year>, <volume>33</volume>, <fpage>14927</fpage>–<lpage>14937</lpage>.</mixed-citation>
    </ref>
    <ref id="ref73">
      <mixed-citation publication-type="journal" id="cit73"><name><surname>Sensoy</surname><given-names>M.</given-names></name>; <name><surname>Kaplan</surname><given-names>L.</given-names></name>; <name><surname>Kandemir</surname><given-names>M.</given-names></name><article-title>Evidential Deep Learning
to Quantify
Classification Uncertainty</article-title>. <source>Adv. Neural Inf.
Process. Syst.</source><year>2018</year>, <volume>31</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref74">
      <mixed-citation publication-type="book" id="cit74"><person-group person-group-type="allauthors"><name><surname>Villani</surname><given-names>C.</given-names></name></person-group><source>Optimal Transport: Old
and New</source>; <publisher-name>Springer</publisher-name>, <bold>2009</bold>; Vol. <volume>338</volume>.</mixed-citation>
    </ref>
    <ref id="ref75">
      <mixed-citation publication-type="journal" id="cit75"><name><surname>Bergstra</surname><given-names>J.</given-names></name>; <name><surname>Bardenet</surname><given-names>R.</given-names></name>; <name><surname>Bengio</surname><given-names>Y.</given-names></name>; <name><surname>Kégl</surname><given-names>B.</given-names></name><article-title>Algorithms
for Hyper-Parameter Optimization</article-title>. <source>Adv. Neural
Inf. Process. Syst.</source><year>2011</year>, <volume>24</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref76">
      <mixed-citation publication-type="journal" id="cit76"><name><surname>Bergstra</surname><given-names>J.</given-names></name>; <name><surname>Yamins</surname><given-names>D.</given-names></name>; <name><surname>Cox</surname><given-names>D.</given-names></name><article-title>Making a Science
of Model Search:
Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures</article-title>. <source>Proceedings of the International Conference on Machine Learning</source><year>2013</year>, <fpage>115</fpage>–<lpage>123</lpage>.</mixed-citation>
    </ref>
    <ref id="ref77">
      <mixed-citation publication-type="journal" id="cit77"><name><surname>Lakshminarayanan</surname><given-names>B.</given-names></name>; <name><surname>Pritzel</surname><given-names>A.</given-names></name>; <name><surname>Blundell</surname><given-names>C.</given-names></name><article-title>Simple and Scalable Predictive Uncertainty
Estimation using Deep Ensembles</article-title>. <source>Adv. Neural
Inf. Process. Syst</source><year>2017</year>, <volume>30</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref78">
      <mixed-citation publication-type="journal" id="cit78"><name><surname>Gal</surname><given-names>Y.</given-names></name>; <name><surname>Ghahramani</surname><given-names>Z.</given-names></name><article-title>Dropout as
a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning</article-title>. <source>Proceedings of
the International Conference on Machine Learning</source><year>2016</year>, <volume>48</volume>, <fpage>1050</fpage>–<lpage>1059</lpage>.</mixed-citation>
    </ref>
    <ref id="ref79">
      <mixed-citation publication-type="journal" id="cit79"><name><surname>Levi</surname><given-names>D.</given-names></name>; <name><surname>Gispan</surname><given-names>L.</given-names></name>; <name><surname>Giladi</surname><given-names>N.</given-names></name>; <name><surname>Fetaya</surname><given-names>E.</given-names></name><article-title>Evaluating
and calibrating
uncertainty prediction in regression tasks</article-title>. <source>Sensors</source><year>2022</year>, <volume>22</volume>, <fpage>5540</fpage><pub-id pub-id-type="doi">10.3390/s22155540</pub-id>.<pub-id pub-id-type="pmid">35898047</pub-id></mixed-citation>
    </ref>
    <ref id="ref80">
      <mixed-citation publication-type="report" id="cit80"><person-group person-group-type="allauthors"><name><surname>Zelikman</surname><given-names>E.</given-names></name>; <name><surname>Healy</surname><given-names>C.</given-names></name>; <name><surname>Zhou</surname><given-names>S.</given-names></name>; <name><surname>Avati</surname><given-names>A.</given-names></name></person-group><article-title>CRUDE: Calibrating
Regression Uncertainty Distributions
Empirically</article-title>. <source>arXiv Preprint</source>, <bold>2020</bold>, arXiv:2005.12496.</mixed-citation>
    </ref>
    <ref id="ref81">
      <mixed-citation publication-type="journal" id="cit81"><name><surname>Wang</surname><given-names>D.</given-names></name>; <name><surname>Yu</surname><given-names>J.</given-names></name>; <name><surname>Chen</surname><given-names>L.</given-names></name>; <name><surname>Li</surname><given-names>X.</given-names></name>; <name><surname>Jiang</surname><given-names>H.</given-names></name>; <name><surname>Chen</surname><given-names>K.</given-names></name>; <name><surname>Zheng</surname><given-names>M.</given-names></name>; <name><surname>Luo</surname><given-names>X.</given-names></name><article-title>A Hybrid Framework for Improving
Uncertainty Quantification in Deep Learning-Based QSAR Regression
Modeling</article-title>. <source>J. Cheminf.</source><year>2021</year>, <volume>13</volume>, <fpage>1</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1186/s13321-021-00551-x</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref82">
      <mixed-citation publication-type="journal" id="cit82"><name><surname>Guo</surname><given-names>C.</given-names></name>; <name><surname>Pleiss</surname><given-names>G.</given-names></name>; <name><surname>Sun</surname><given-names>Y.</given-names></name>; <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name><article-title>On Calibration
of Modern Neural Networks</article-title>. <source>Proceedings of the
International Conference on Machine Learning</source><year>2017</year>, <fpage>1321</fpage>–<lpage>1330</lpage>.</mixed-citation>
    </ref>
    <ref id="ref83">
      <mixed-citation publication-type="journal" id="cit83"><name><surname>Zadrozny</surname><given-names>B.</given-names></name>; <name><surname>Elkan</surname><given-names>C.</given-names></name><article-title>Transforming Classifier Scores into Accurate Multiclass Probability
Estimates</article-title>. <source>Proceedings of the International
Conference on Knowledge Discovery and Data Mining</source><year>2002</year>, <fpage>694</fpage>–<lpage>699</lpage>.</mixed-citation>
    </ref>
    <ref id="ref84">
      <mixed-citation publication-type="journal" id="cit84"><name><surname>Scalia</surname><given-names>G.</given-names></name>; <name><surname>Grambow</surname><given-names>C. A.</given-names></name>; <name><surname>Pernici</surname><given-names>B.</given-names></name>; <name><surname>Li</surname><given-names>Y.-P.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Evaluating
Scalable Uncertainty Estimation Methods for Deep Learning-Based Molecular
Property Prediction</article-title>. <source>J. Chem. Inf. Model.</source><year>2020</year>, <volume>60</volume>, <fpage>2697</fpage>–<lpage>2717</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00975</pub-id>.<pub-id pub-id-type="pmid">32243154</pub-id></mixed-citation>
    </ref>
    <ref id="ref85">
      <mixed-citation publication-type="weblink" id="cit85">Chemprop. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://chemprop.readthedocs.io/en/latest/">https://chemprop.readthedocs.io/en/latest/</uri> (accessed April 6 2023).</mixed-citation>
    </ref>
    <ref id="ref86">
      <mixed-citation publication-type="weblink" id="cit86">Chemprop
Workshop. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.youtube.com/watch?v=TeOl5E8Wo2M">https://www.youtube.com/watch?v=TeOl5E8Wo2M</uri> (accessed April
6 2023).</mixed-citation>
    </ref>
    <ref id="ref87">
      <mixed-citation publication-type="journal" id="cit87"><name><surname>Reuther</surname><given-names>A.</given-names></name>; <name><surname>Kepner</surname><given-names>J.</given-names></name>; <name><surname>Byun</surname><given-names>C.</given-names></name>; <name><surname>Samsi</surname><given-names>S.</given-names></name>; <name><surname>Arcand</surname><given-names>W.</given-names></name>; <name><surname>Bestor</surname><given-names>D.</given-names></name>; <name><surname>Bergeron</surname><given-names>B.</given-names></name>; <name><surname>Gadepally</surname><given-names>V.</given-names></name>; <name><surname>Houle</surname><given-names>M.</given-names></name>; <name><surname>Hubbell</surname><given-names>M.</given-names></name>; et al. <article-title>Interactive
Supercomputing
on 40,000 Cores for Machine Learning and Data Analysis</article-title>. <source>Proceedings of the IEEE High Performance Extreme Computing
Conference</source><year>2018</year>, <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Chem Inf Model</journal-id>
    <journal-id journal-id-type="publisher-id">ci</journal-id>
    <journal-id journal-id-type="coden">jcisd8</journal-id>
    <journal-title-group>
      <journal-title>Journal of Chemical Information and Modeling</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1549-9596</issn>
    <issn pub-type="epub">1549-960X</issn>
    <publisher>
      <publisher-name>American Chemical Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10777403</article-id>
    <article-id pub-id-type="pmid">38147829</article-id>
    <article-id pub-id-type="doi">10.1021/acs.jcim.3c01250</article-id>
    <article-categories>
      <subj-group>
        <subject>Application Note</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Chemprop: A Machine Learning Package for Chemical
Property Prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="ath1">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8404-6596</contrib-id>
        <name>
          <surname>Heid</surname>
          <given-names>Esther</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath2">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6466-1401</contrib-id>
        <name>
          <surname>Greenman</surname>
          <given-names>Kevin P.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath3">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3097-010X</contrib-id>
        <name>
          <surname>Chung</surname>
          <given-names>Yunsie</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath4">
        <name>
          <surname>Li</surname>
          <given-names>Shih-Cheng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff3" ref-type="aff">§</xref>
      </contrib>
      <contrib contrib-type="author" id="ath5">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1250-3329</contrib-id>
        <name>
          <surname>Graff</surname>
          <given-names>David E.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff4" ref-type="aff">∥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath6">
        <name>
          <surname>Vermeire</surname>
          <given-names>Florence H.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff5" ref-type="aff">⊥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath7">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0644-7554</contrib-id>
        <name>
          <surname>Wu</surname>
          <given-names>Haoyang</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath8">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2603-9694</contrib-id>
        <name>
          <surname>Green</surname>
          <given-names>William H.</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" id="ath9">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2704-7717</contrib-id>
        <name>
          <surname>McGill</surname>
          <given-names>Charles J.</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
        <xref rid="aff6" ref-type="aff">#</xref>
      </contrib>
      <aff id="aff1"><label>†</label>Department
of Chemical Engineering, <institution>Massachusetts Institute
of Technology</institution>, Cambridge, Massachusetts 02139, <country>United States</country></aff>
      <aff id="aff2"><label>‡</label><institution>Institute
of Materials Chemistry</institution>, TU Wien, 1060 Vienna, <country>Austria</country></aff>
      <aff id="aff3"><label>§</label>Department
of Chemical Engineering, <institution>National Taiwan
University</institution>, Taipei 10617, <country>Taiwan</country></aff>
      <aff id="aff4"><label>∥</label>Department
of Chemistry and Chemical Biology, <institution>Harvard
University</institution>, Cambridge, Massachusetts 02138, <country>United States</country></aff>
      <aff id="aff5"><label>⊥</label>Department
of Chemical Engineering, <institution>KU Leuven</institution>, Celestijnenlaan 200F, B-3001 Leuven, <country>Belgium</country></aff>
      <aff id="aff6"><label>#</label>Department
of Chemical and Life Science Engineering, <institution>Virginia Commonwealth University</institution>, Richmond, Virginia 23284, <country>United States</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>E-mail: <email>mcgillc2@vcu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>26</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <day>08</day>
      <month>01</month>
      <year>2024</year>
    </pub-date>
    <volume>64</volume>
    <issue>1</issue>
    <fpage>9</fpage>
    <lpage>17</lpage>
    <history>
      <date date-type="received">
        <day>08</day>
        <month>08</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>12</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>04</day>
        <month>12</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by American Chemical Society</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>The Authors</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Permits the broadest form of re-use including for commercial purposes, provided that author attribution and integrity are maintained (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0004" id="ab-tgr1"/>
      </p>
      <p>Deep learning has
become a powerful and frequently employed tool
for the prediction of molecular properties, thus creating a need for
open-source and versatile software solutions that can be operated
by nonexperts. Among the current approaches, directed message-passing
neural networks (D-MPNNs) have proven to perform well on a variety
of property prediction tasks. The software package Chemprop implements
the D-MPNN architecture and offers simple, easy, and fast access to
machine-learned molecular properties. Compared to its initial version,
we present a multitude of new Chemprop functionalities such as the
support of multimolecule properties, reactions, atom/bond-level properties,
and spectra. Further, we incorporate various uncertainty quantification
and calibration methods along with related metrics as well as pretraining
and transfer learning workflows, improved hyperparameter optimization,
and other customization options concerning loss functions or atom/bond
features. We benchmark D-MPNN models trained using Chemprop with the
new reaction, atom-level, and spectra functionality on a variety of
property prediction data sets, including MoleculeNet and SAMPL, and
observe state-of-the-art performance on the prediction of water-octanol
partition coefficients, reaction barrier heights, atomic partial charges,
and absorption spectra. Chemprop enables out-of-the-box training of
D-MPNN models for a variety of problem settings in fast, user-friendly,
and open-source software.</p>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Division of Graduate Education</institution>
            <institution-id institution-id-type="doi">10.13039/100000082</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>1745302</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>MIT-IBM Watson Lab</institution>
            <institution-id institution-id-type="doi">NA</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>KU Leuven</institution>
            <institution-id institution-id-type="doi">10.13039/501100004040</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>STG/22/032</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Austrian Science Fund</institution>
            <institution-id institution-id-type="doi">10.13039/501100002428</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>J-4415</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Virginia Commonwealth University</institution>
            <institution-id institution-id-type="doi">10.13039/100009238</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Massachusetts Institute of Technology</institution>
            <institution-id institution-id-type="doi">10.13039/100006919</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NA</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Defense Sciences Office, DARPA</institution>
            <institution-id institution-id-type="doi">10.13039/100006502</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>HR00111920025</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ci3c01250</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ci3c01250</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p>Machine learning in general
and especially deep learning has become
a powerful tool in various fields of chemistry. Applications range
from the prediction of physicochemical<sup><xref ref-type="bibr" rid="ref1">1</xref>−<xref ref-type="bibr" rid="ref9">9</xref></sup> and pharmacological<sup><xref ref-type="bibr" rid="ref10">10</xref></sup> properties of molecules
to the design of molecules or materials with certain properties,<sup><xref ref-type="bibr" rid="ref11">11</xref>−<xref ref-type="bibr" rid="ref13">13</xref></sup> the exploration of chemical synthesis pathways,<sup><xref ref-type="bibr" rid="ref14">14</xref>−<xref ref-type="bibr" rid="ref27">27</xref></sup> or the prediction of properties important for chemical analysis
like IR,<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> UV/vis,<sup><xref ref-type="bibr" rid="ref29">29</xref></sup> or mass spectra.<sup><xref ref-type="bibr" rid="ref30">30</xref>−<xref ref-type="bibr" rid="ref33">33</xref></sup></p>
    <p>Many combinations of molecular representations and model architectures
have been developed to extract features from molecules and predict
molecular properties. Molecules can be represented as graphs, strings,
precomputed feature vectors, or sets of atomic coordinates and processed
using graph-convolutional neural networks, transformers, or feed-forward
neural networks to train predictive models. While early works focused
on handmade features or simple fingerprinting methods combined with
kernel regression or neural networks,<sup><xref ref-type="bibr" rid="ref34">34</xref></sup> the current state-of-the-art has shifted to end-to-end trainable
models which directly learn to extract their own features.<sup><xref ref-type="bibr" rid="ref35">35</xref></sup> Here, the models can achieve extreme complexity
based on the mechanisms of information exchange between parts of the
molecule. For example, graph convolutional neural networks (GCNNs)
extract local information from the molecular graph for single or small
groups of atoms and use that information to update the immediate neighborhood.<sup><xref ref-type="bibr" rid="ref1">1</xref>−<xref ref-type="bibr" rid="ref3">3</xref>,<xref ref-type="bibr" rid="ref36">36</xref></sup> They offer robust performance
for properties dependent on the local structure and if the three-dimensional
conformation of a molecule is not known or not relevant for a prediction
task. Graph attention transformers allow for a less local information
exchange via attention layers, which learn to accumulate the features
of atoms both close and far away in the graph.<sup><xref ref-type="bibr" rid="ref37">37</xref>,<xref ref-type="bibr" rid="ref38">38</xref></sup> Another important line of research comprises the prediction of properties
dependent on the three-dimensional conformation of a molecule, such
as the prediction of properties obtained from quantum mechanics.<sup><xref ref-type="bibr" rid="ref2">2</xref>,<xref ref-type="bibr" rid="ref39">39</xref>−<xref ref-type="bibr" rid="ref41">41</xref></sup> Finally, transformer models from natural language
processing can be trained on string representations such as SMILES
or SELFIES, also leading to promising results.<sup><xref ref-type="bibr" rid="ref42">42</xref>−<xref ref-type="bibr" rid="ref45">45</xref></sup> In this work, we discuss our
application of GCNNs, namely, Chemprop,<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> a directed-message passing algorithm derived from the seminal work
of Gilmer et al.<sup><xref ref-type="bibr" rid="ref1">1</xref></sup></p>
    <p>An early version
of Chemprop was published in ref (<xref ref-type="bibr" rid="ref36">36</xref>). Since then, the software
has substantially evolved and now includes a vast collection of new
features. For example, Chemprop is now able to predict properties
for systems containing multiple molecules, such as solute/solvent
combinations or reactions with and without solvent. It can train on
molecular targets, spectra, or atom/bond-level targets and output
the latent representation for analysis of the learned feature embedding.
Available uncertainty metrics include popular approaches, such as
ensembling, mean-variance estimation, and evidential learning. Chemprop
is thus a general and versatile deep learning toolbox and enjoys a
wide user base.</p>
    <p>The remainder of the article is structured as
follows: First, we
summarize the architecture of Chemprop. We discuss a selection of
Chemprop features with a focus on features introduced after the initial
release of Chemprop. We then conclude the main body of the article
and provide details on the data and software, which we have open-sourced
including all scripts to allow for full reproducibility. Alongside
the main body of this article, we provide <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Supporting Information</ext-link> that contains further model design details; descriptions
of the data acquisition, preprocessing, and splitting of all data
sets used in benchmarking; and the results of Chemprop benchmarks
on a variety of data sets showcasing its performance on both simple
and advanced prediction tasks.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Model Structure</title>
    <p>Chemprop consists of four modules: (1) a local features encoding
function, (2) a directed message passing neural network (D-MPNN) to
learn atomic embeddings from the local features, (3) an aggregation
function to join atomic embeddings into molecular embeddings, and
(4) a standard feed-forward neural network (FFN) for the transformation
of molecular embeddings to target properties, summarized in <xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref>. The D-MPNN is a
class of graph-convolutional neural networks (GCNN), which updates
hidden representations of the vertices <italic>V</italic> and edges <italic>E</italic> of a graph <italic>G</italic> based on the local environment.
In the following, we use bold lower case to denote vectors, bold upper
case to denote matrices, and italic light font for scalars and objects.</p>
    <fig id="fig1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Overview
of the architecture of Chemprop. The message passing update
of the hidden vector for directed edge 2 → 1 is expanded for
demonstration.</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0001" id="gr1" position="float"/>
    </fig>
    <p>For a molecule, the molecule SMILES
string is used as input, which
is then transformed to a molecular graph using RDKit,<sup><xref ref-type="bibr" rid="ref46">46</xref></sup> where atoms correspond to vertices and bonds to edges.
Initial features are constructed based on the identity and topology
of each atom and bond. For each vertex <italic>v</italic>, initial
feature vectors {<bold>x</bold><sub><italic>v</italic></sub> |<italic>v</italic> ∈ <italic>V</italic>} are obtained from a one-hot
encoding of the atomic number, number of bonds linked to each atom,
formal charge, chirality (if encoded in the SMILES), number of hydrogens,
hybridization, and aromaticity of the atom, as well as the atomic
mass (divided by 100 for scaling). For each edge <italic>e</italic>, initial feature vectors {<bold>e</bold><sub><italic>vw</italic></sub>|{<italic>v</italic>, <italic>w</italic>} ∈ <italic>E</italic>} arise from the bond type, whether the bond is conjugated
or in a ring, and whether it contains stereochemical information,
such as a cis/trans double bond. The D-MPNN uses directed edges in
a graph to pass information, where each undirected edge (bond) has
two corresponding directed edges, one in each direction. Initial directed
edge features <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> are obtained via simple concatenation
of the atom features of the first atom of a bond <bold>x</bold><sub><italic>v</italic></sub> to the respective undirected bond features <bold>e</bold><sub><italic>vw</italic></sub><disp-formula id="eq1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m001" position="anchor"/><label>1</label></disp-formula>where cat() denotes simple concatenation.
The directed edges <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> and <bold>e</bold><sub arrange="stack"><italic>wv</italic></sub><sup arrange="stack">d</sup> are distinguished only by the choice of which atom to use in <xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>. Chemprop also offers
the option to read in custom atom and bond features in addition to
or as a replacement for the default features, as described in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section S1.2, and thus offers full control
of the initial features. In summary, Module 1 of Chemprop constructs
atom and directed bond feature vectors <bold>x</bold><sub><italic>v</italic></sub> and <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> from the input molecules.</p>
    <p>The initial atom and bond features are then passed to a D-MPNN.
In a D-MPNN structure, messages are passed between directed edges
rather than between nodes as would be done in a traditional MPNN.
To construct the hidden directed edge features <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">0</sup> of hidden size <italic>h</italic>, the initial directed edge features <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup> are passed through a single neural network
layer with learnable weights <inline-formula id="d33e432"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m002.gif"/></inline-formula><disp-formula id="eq2"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m003" position="anchor"/><label>2</label></disp-formula>and a nonlinear
activation function τ
which can be chosen by the user (default ReLU). The size <italic>h</italic> of <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">0</sup> can be chosen by the user (default 300). The
size of <bold>e</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack">d</sup>, which we term <italic>h</italic><sub><italic>i</italic></sub>, is set by the lengths of initial feature
encodings, per <xref rid="eq1" ref-type="disp-formula">eq <xref rid="eq1" ref-type="disp-formula">1</xref></xref>.
The directed edge features are then iteratively updated based on the
local environment via <italic>T</italic> (default 3) message passing
steps<disp-formula id="eq3"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m004" position="anchor"/><label>3</label></disp-formula>until <italic>t</italic> + 1 = <italic>T</italic>, where <inline-formula id="d33e474"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m005.gif"/></inline-formula> and <italic>N</italic>(<italic>v</italic>)\<italic>w</italic> denotes
the neighbors of node <italic>v</italic> excluding <italic>w</italic>. The opposite facing directed edge
is excluded from the message passing update for increased numerical
stability (see Mahé et al.<sup><xref ref-type="bibr" rid="ref47">47</xref></sup>). Finally,
the updated hidden states <bold>h</bold><sub arrange="stack"><italic>vw</italic></sub><sup arrange="stack"><italic>T</italic></sup> are
aggregated into atomic embeddings via<disp-formula id="eq4"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m006" position="anchor"/><label>4</label></disp-formula>where <bold>q</bold> is a concatenation of
the initial atom features <bold>x</bold><sub><italic>v</italic></sub> and the sum of all incoming directed edge hidden states<disp-formula id="eq5"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m007" position="anchor"/><label>5</label></disp-formula>with <inline-formula id="d33e513"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m008.gif"/></inline-formula>. Here, <italic>h</italic><sub><italic>o</italic></sub> is the size
of <bold>q</bold>, i.e., the sum of the hidden
size <italic>h</italic> and the size of <bold>x</bold><sub><italic>v</italic></sub>. In summary, in Module 2, the D-MPNN weights <bold>W</bold><sub><italic>i</italic></sub>, <bold>W</bold><sub><italic>h</italic></sub>, and <bold>W</bold><sub><italic>o</italic></sub> are learned from the training data, outputting learnable atomic
embeddings <bold>h</bold><sub><italic>v</italic></sub>. Customizations
and hyperparameter tuning include the choice of the activation function
τ, the hidden size <italic>h</italic>, and the number of message
passing steps <italic>T</italic>. Chemprop offers the option to add
bias terms to all neural network layers (defaults to False).</p>
    <p>The atomic embeddings <bold>h</bold><sub><italic>v</italic></sub> of all atoms in a molecule are then aggregated into a single molecular
embedding <bold>h</bold><sub><italic>m</italic></sub> via<disp-formula id="eq6"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m009" position="anchor"/><label>6</label></disp-formula>with<disp-formula id="eq7"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_m010" position="anchor"/><label>7</label></disp-formula>where <bold>x</bold><sub><italic>m</italic></sub> is an optional
vector of additional molecular features.
Chemprop offers three aggregation options: summation (as shown in <xref rid="eq7" ref-type="disp-formula">eq <xref rid="eq7" ref-type="disp-formula">7</xref></xref>), a scaled sum (divided
by a user specified scaler, called a norm within Chemprop), or an
average (the default aggregation). Schweidtmann et al.<sup><xref ref-type="bibr" rid="ref48">48</xref></sup> compared the performance of such aggregation
functions for different data sets. The optional additional molecular
features, <bold>x</bold><sub><italic>m</italic></sub>, may be provided
features from outside sources (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section
S2) or generated engineered fingerprints (Morgan circular fingerprints<sup><xref ref-type="bibr" rid="ref49">49</xref></sup> and RDKIT 2D fingerprints<sup><xref ref-type="bibr" rid="ref46">46</xref></sup> are implemented in Chemprop). By default, <bold>x</bold><sub><italic>m</italic></sub> is empty such that the molecular embedding
is simply an aggregation over atomic embeddings, i.e., <bold>h</bold><sub><italic>m</italic></sub> = <bold>h</bold><sub arrange="stack"><italic>m</italic></sub><sup arrange="stack">′</sup> . In
summary, Module 3 produces molecular embeddings <bold>h</bold><sub><italic>m</italic></sub> of length <italic>h</italic> plus the size
of <bold>x</bold><sub><italic>m</italic></sub>. Chemprop offers the
option to circumvent Modules 1–3 and only using <bold>x</bold><sub><italic>m</italic></sub> as fixed molecular embedding, so that <bold>h</bold><sub><italic>m</italic></sub> = <bold>x</bold><sub><italic>m</italic></sub>.</p>
    <p>Finally, in the last module, molecular target
properties are learned
from the molecular embeddings <bold>h</bold><sub><italic>m</italic></sub> via a feed-forward neural network, where the number of layers
(default 2) and the number of hidden neurons (default 300) can be
chosen by the user. The number of input neurons is set by the length
of <bold>h</bold><sub><italic>m</italic></sub>, and the number of
output neurons is set by the number of targets. The activation function
between linear layers is set to be the same as in the D-MPNN, and
bias is turned on per default. For binary classification tasks, the
final model output is passed through a sigmoid function to constrain
values to the range (0,1). For multiclass classification, the final
model output is transformed with a softmax function, such that the
classification scores sum to 1 across classes.</p>
    <p>Chemprop is fully
end-to-end trainable, so that the weights for
D-MPNN and FFN are updated simultaneously. Users have the option to
train models using cross-validation and ensembles of submodels. By
default, a single model is trained on a random data split for 30 epochs.
We note that small data sets need a much larger number of epochs to
train and advise to check for convergence of the learning curve. Chemprop
uses the Adam optimizer.<sup><xref ref-type="bibr" rid="ref50">50</xref></sup> The default
learning rate schedule increases the learning rate linearly from 10<sup>–4</sup> to 10<sup>–3</sup> for the first two warmup
epochs and then decreases the learning rate exponentially from 10<sup>–3</sup> to 10<sup>–4</sup> for the remaining epochs.
By default, a batch size of 50 data points is used for each optimizer
step. Early stopping and dropout are available as means of regularization.
The PyTorch backend of Chemprop enables seamless GPU acceleration
of both model training and inference. The acceleration of training
and inference processes when used with a GPU can be significant, as
shown in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> Section S3.3.</p>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Discussion of Features</title>
    <p><xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref> lists a
nonexhaustive selection of studies based on Chemprop, showcasing its
versatility and applicability for the prediction of a large variety
of chemical properties, but also its ease of use. Models can be trained
and tested with a single line on the command line (or a few lines
of python code) and a user-supplied CSV file (see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for some examples). In the following, we discuss specialty
options introduced since its first release.</p>
    <table-wrap id="tbl1" position="float">
      <label>Table 1</label>
      <caption>
        <title>Selected
Published Studies Based on
Chemprop</title>
      </caption>
      <table frame="hsides" rules="groups" border="0">
        <colgroup>
          <col align="center"/>
          <col align="center"/>
          <col align="left"/>
          <col align="center"/>
          <col align="center"/>
          <col align="left"/>
        </colgroup>
        <thead>
          <tr>
            <th style="border:none;" align="center">ref</th>
            <th style="border:none;" align="center">Year</th>
            <th style="border:none;" align="center">Prediction</th>
            <th style="border:none;" align="center">ref</th>
            <th style="border:none;" align="center">Year</th>
            <th style="border:none;" align="center">Prediction</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref10">10</xref>)</td>
            <td style="border:none;" align="center">2020</td>
            <td style="border:none;" align="left">Growth inhibitory activity against <italic>E. coli</italic>; led to an identification of a potential new drug</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref51">51</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Absorption, distribution, metabolism, excretion
(ADME) properties
for drug discovery</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref52">52</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Chemical synergy against SARS-CoV-2; identified two drug combinations
with antiviral synergy in vitro</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref53">53</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Growth inhibitory activity against <italic>A. baumannii</italic>; led to an identification of a potential new drug</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref28">28</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">IR spectra of molecules</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref54">54</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Fuel properties</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref55">55</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Atomic charges, Fukui indices, NMR constants, bond lengths,
and bond orders</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref56">56</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Critical properties, acentric
factor, and phase change properties</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref57">57</xref>)</td>
            <td style="border:none;" align="center">2021</td>
            <td style="border:none;" align="left">Lipophilicity</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref58">58</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Molecular optical peaks</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref59">59</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Reaction rates and barrier heights</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref60">60</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Lipophilicity</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref61">61</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Barrier heights of reactions</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref62">62</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Solvent effects on reaction rate constants</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref29">29</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Molecular optical peaks</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref63">63</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Vapor pressure in the low volatility regime</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref64">64</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Solvation free energy, solvation
enthalpy, and Abraham solute
descriptors</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref65">65</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Molecular optical peaks
and partition coefficients for closed-loop
active learning</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref66">66</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Solid solubility
of organic solutes in water and organic solvents</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref67">67</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Senolytic activity of compounds to selectively target senescent
cells</td>
          </tr>
          <tr>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref68">68</xref>)</td>
            <td style="border:none;" align="center">2022</td>
            <td style="border:none;" align="left">Activity coefficients</td>
            <td style="border:none;" align="center">(<xref ref-type="bibr" rid="ref69">69</xref>)</td>
            <td style="border:none;" align="center">2023</td>
            <td style="border:none;" align="left">Toxicity measurements using 12 nuclear receptor
signaling and
stress response pathways</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Additional Features</title>
      <p>Chemprop can
take additional features at the molecule-, atom-, or bond-level as
input. While Chemprop often generates accurate models without requiring
any input beyond the SMILES, it has been shown that outside information
added as additional features can further improve performance.<sup><xref ref-type="bibr" rid="ref36">36</xref>,<xref ref-type="bibr" rid="ref64">64</xref></sup> Users can provide their custom additional features by adding keywords
and paths to the data files containing the features. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for command-line arguments and details.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Multimolecule Models</title>
      <p>Chemprop can
also train on a data set containing more than one molecule as input.
For example, when properties related to solvation need to be predicted,
both a solute and a solvent are required as input to the model. Users
can provide multiple molecules as inputs to Chemprop. When multiple
molecules are used, by default Chemprop trains a separate D-MPNN for
each molecule (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Figure S1a</ext-link>). If the option <monospace>--mpn_shared</monospace> is specified, then the same D-MPNN is used
for all molecules (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Figure S1b</ext-link>). The embeddings
of the different molecules are then concatenated prior to the FFN.
Note that the current implementation of multiple molecules in Chemprop
does not ensure permutational invariance toward the input molecules.
This is suited to situations where the input molecules have different
roles, e.g., molecule 1 = solute, molecule 2 = solvent. For additional
input information and a figure depicting the multimolecule model structure,
see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link>.</p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Reaction
Support</title>
      <p>Chemprop supports
the input of atom-mapped reactions, i.e., pairs of reactants and products
SMILES connected via the “≫” symbol,by using
the keyword <monospace>--reaction</monospace>. The pair of reactants
and products is transformed into a single pseudomolecule, namely,
the condensed graph of reaction (CGR), and then passed to a regular
D-MPNN block. The construction of a CGR within Chemprop is described
in detail in ref (<xref ref-type="bibr" rid="ref59">59</xref>) and summarized in the following. In general, the input of a reaction
vs a molecule only affects the setup of the graph object and its initial
features, but not any other part of the architecture. The graph of
a reaction has a different set of edges <italic>E</italic> as the
graph of a molecule, as shown in <xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref> for an example Diels–Alder reaction.
To build the CGR pseudomolecule, the set of atoms is obtained as the
union of the sets of atoms in the reactants and products. Similarly,
the set of bonds is obtained as the union of the sets of bonds in
the reactants and products. Once constructed, the CGR is passed through
the D-MPNN and other model architecture components in the same way
a molecular graph would. Optionally, Chemprop accepts an additional
molecule object as input, such as a solvent, a reagent, etc. which
is passed to its own D-MPNN similar to the multimolecule model. The
output of the reaction D-MPNN and molecule D-MPNN is concatenated
after atomic aggregation, before the FFN. This option is available
via the <monospace>reaction_solvent</monospace> keyword. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for further commandline options and details.</p>
      <fig id="fig2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Construction
of the condensed graph of reaction (CGR) of an example
reaction. The vertices and edges are obtained as the union of the
respective reactant and product vertices and edges. The features are
obtained as a combination of the reactant (white background) and product
(gray background) features for atoms and bonds.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0002" id="gr2" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Spectra Data Support</title>
      <p>Chemprop supports
the prediction of whole-spectrum properties for molecules. An initial
version of this capability was discussed in ref (<xref ref-type="bibr" rid="ref28">28</xref>) for use with IR absorbance
spectra. Targets for the spectra data set type are composed of an
array of intensity values, set at fixed bin locations typically specified
in terms of wavelengths, frequencies, or wavenumbers. Spectral Information
Divergence was originally developed as a method of comparing spectra
to reference databases<sup><xref ref-type="bibr" rid="ref70">70</xref></sup> and is adapted
in Chemprop to be used as a loss function, considering the deviation
of the spectrum as a whole rather than independently at each bin location.
The treatment of spectra can handle targets with gaps or missing values
within a data set. With the expectation that spectra will often be
collected in systems where a portion of the range will be obscured
or invalid (e.g., from solvent absorbance), Chemprop can create exclusion
regions in specified spectra where no predictions are provided and
targets are ignored for training purposes.</p>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Latent
Representations</title>
      <p>Graph neural
networks enable learning both molecular representation and property
end-to-end directly from the molecular graph. As detailed above in <xref rid="sec2" ref-type="other">Section <xref rid="sec2" ref-type="other">2</xref></xref>, the learned node
representations are aggregated into a molecule-level representation
after the message-passing phase, which we refer to as the “learned
fingerprint.” This embedding is then further fed into the FFN
network. Within the FFN, we consider the final hidden representation,
which we refer to as the “ffn embedding”. Both of these
vectors are latent representations of a molecule as it relates to
a particular trained model. Molecule latent representations can be
useful for data clustering or used as additional features in other
models. Chemprop supports the calculation of either from a trained
model for a given set of molecules.</p>
    </sec>
    <sec id="sec3.6">
      <label>3.6</label>
      <title>Loss
Function Options</title>
      <p>Chemprop can
train models according to many common loss functions. The loss functions
available for a given task are determined by the data set type (regression,
classification, multiclass, or spectra). Regression models can be
trained with mean squared error (MSE), bounded MSE (which allows inequalities
as targets), or negative log-likelihood (NLL) based on prediction
uncertainty distributions consistent with mean-variance estimation
(MVE)<sup><xref ref-type="bibr" rid="ref71">71</xref></sup> or evidential uncertainty.<sup><xref ref-type="bibr" rid="ref72">72</xref></sup> Classification tasks default to the binary cross
entropy loss and have additional options of Matthews correlation coefficient
(MCC) and Dirichlet (evidential classification).<sup><xref ref-type="bibr" rid="ref73">73</xref></sup> Cross entropy and MCC are also available for multiclass
problems. There are two options available for training on spectra:
spectral information divergence (SID)<sup><xref ref-type="bibr" rid="ref70">70</xref></sup> and first-order Wasserstein distance (a.k.a. earthmover’s
distance).<sup><xref ref-type="bibr" rid="ref74">74</xref></sup> Loss functions must be differentiable
since they are used to calculate gradients that update the model parameters,
but Chemprop also provides the option to use several nondifferentiable
metrics for model evaluation.</p>
    </sec>
    <sec id="sec3.7">
      <label>3.7</label>
      <title>Transfer
Learning</title>
      <p>Transfer learning
is a general strategy of using information gained through the training
of one model to inform and improve the training of a related model.
Often, this strategy is used to transfer information from a previously
trained model of a large data set to a model of a small data set in
order to improve the performance of the model of the small data set.
The simplest method of transfer learning would be taking predictions
or latent representations from one model and supplying them as additional
features to another model (<xref rid="sec3.1" ref-type="other">Sections <xref rid="sec3.1" ref-type="other">3.1</xref></xref> and <xref rid="sec3.5" ref-type="other">3.5</xref>).</p>
      <p>In Chemprop, different strategies are available to transfer learned
model parameters from a previously trained model to a new model as
a form of transfer learning, as shown in <xref rid="fig3" ref-type="fig">Figure <xref rid="fig3" ref-type="fig">3</xref></xref>. A pretrained model may be used to initialize
a new model with normal updating of the transferred weights in training.
Alternatively, parameters from the transferred model can be frozen,
holding them constant during training. Freezing parameters always
include the D-MPNN weights but can be specified to include some FFN
layers as well. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for the corresponding
arguments.</p>
      <fig id="fig3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Options to transfer model parameters from a pretrained model (squared)
to a new model, by (a) initializing the new model parameters or by
freezing the (b) D-MPNN layers and (c) <italic>n</italic> FFN layers.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_0003" id="gr3" position="float"/>
      </fig>
    </sec>
    <sec id="sec3.8">
      <label>3.8</label>
      <title>Hyperparameter Optimization</title>
      <p>Chemprop
provides a command-line utility, allowing for the simple initiation
of hyperparameter optimization jobs. Options for the hyperparameter
job such as how many trials to carry out and which hyperparameters
to include in the search (options in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">Table S3</ext-link>) can be specified with simple command-line arguments. The optimization
initially uses randomly sampled trials, followed by targeted sampling
using the Tree-structured Parzen Estimator algorithm.<sup><xref ref-type="bibr" rid="ref75">75</xref>,<xref ref-type="bibr" rid="ref76">76</xref></sup></p>
      <p>Hyperparameter optimization is often the most resource-intensive
step in model training. In order to search a large parameter space
adequately, a large number of trials is needed. Chemprop allows for
parallel operation of multiple hyperparameter optimization instances,
removing the need to carry out all trials in series and reducing the
wall time needed to perform the optimization significantly. See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for the available hyperparameter options and
other details.</p>
    </sec>
    <sec id="sec3.9">
      <label>3.9</label>
      <title>Uncertainty Tools</title>
      <p>Chemprop includes
a variety of popular uncertainty estimation, calibration, and evaluation
tools. The estimation methods include deep ensembles,<sup><xref ref-type="bibr" rid="ref77">77</xref></sup> dropout,<sup><xref ref-type="bibr" rid="ref78">78</xref></sup> mean-variance estimation
(MVE),<sup><xref ref-type="bibr" rid="ref71">71</xref></sup> and evidential,<sup><xref ref-type="bibr" rid="ref72">72</xref></sup> as well as a special version of ensemble variance for spectral
predictions,<sup><xref ref-type="bibr" rid="ref28">28</xref></sup> and the inherently probabilistic
outputs of classification models. After estimating the uncertainty
in a model’s predictions, it is often helpful to calibrate
these uncertainties to improve their performance on new predictions.
We provide four such methods for regression tasks (<italic>z</italic>-scaling,<sup><xref ref-type="bibr" rid="ref79">79</xref></sup> t-scaling, Zelikman’s
CRUDE,<sup><xref ref-type="bibr" rid="ref80">80</xref></sup> and MVE weighting<sup><xref ref-type="bibr" rid="ref81">81</xref></sup>) and two for classification (single-parameter Platt scaling<sup><xref ref-type="bibr" rid="ref82">82</xref></sup> and isotonic regression<sup><xref ref-type="bibr" rid="ref83">83</xref></sup>). In addition to standard metrics such as RMSE, MAE, etc. for evaluating
predictions, Chemprop also includes several metrics specifically for
evaluating the quality of uncertainty estimates. These include negative
log likelihood, Spearman rank correlation, expected normalized calibration
error (ENCE),<sup><xref ref-type="bibr" rid="ref84">84</xref></sup> and miscalibration area.<sup><xref ref-type="bibr" rid="ref84">84</xref></sup> Any valid classification or multiclass metric
used to assess predictions can also be used to assess uncertainties.</p>
    </sec>
    <sec id="sec3.10">
      <label>3.10</label>
      <title>Atom/Bond-Level Targets</title>
      <p>Chemprop
supports a multitask constrained D-MPNN architecture for predicting
atom- and bond-level properties, such as charge density or bond length.
This model enables a D-MPNN to be trained on multiple atomic and bond
properties simultaneously, though unlike molecular property targets,
they do not share a single FFN. Optionally, an attention-based constraining
method may be used to enforce that predicted atomic or bond properties
sum to a specified molecular net value, such as the overall charge
of a molecule. An initial, more limited version of this capability
was developed in ref (<xref ref-type="bibr" rid="ref55">55</xref>). For details on the input formats to be used for atom/bond targets,
both for training and for inference, see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link>.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Benchmarking</title>
    <p>See <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">SI</ext-link> for general performance benchmarks,
benchmarks using specific Chemprop features (atom/bond-level targets,
reaction support, multimolecule models, spectra prediction, and uncertainty
estimation), and system timing benchmarks.</p>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Conclusion</title>
    <p>We have presented the software package Chemprop, a powerful toolbox
for machine learning of the chemical properties of molecules and reactions.
Significant improvements have been made to the software since its
initial release and study,<sup><xref ref-type="bibr" rid="ref36">36</xref></sup> including
the support of multimolecule properties, reactions, atom/bond-level
properties, and spectra. Additionally, several state-of-the-art approaches
to estimate the uncertainty of predictions have been incorporated
as well as pretraining and transfer learning procedures. Furthermore,
the code now offers a variety of customization options, such as custom
atom and bond features, a large variety of loss functions, and the
ability to save the learned feature embeddings for subsequent use
with different algorithms. We have showcased and benchmarked Chemprop
on a variety of example tasks and data sets and have found competitive
performances for molecular property prediction compared to other approaches
available on public leaderboards. In summary, Chemprop is a powerful,
fast, and convenient tool to learn conformation-independent properties
of molecules, sets of molecules, or reactions.</p>
  </sec>
</body>
<back>
  <notes notes-type="data-availability" id="notes-1">
    <title>Data Availability Statement</title>
    <p>Chemprop, including
all features described in this paper, is available under the open-source
MIT License on GitHub, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/chemprop/chemprop">github.com/chemprop/chemprop</uri>. An extensive documentation including
tutorials is available online,<sup><xref ref-type="bibr" rid="ref85">85</xref></sup> including
a workshop on YouTube.<sup><xref ref-type="bibr" rid="ref86">86</xref></sup> Scripts and data
splits to fully reproduce this study are available on GitHub, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://github.com/chemprop/chemprop_benchmark">github.com/chemprop/chemprop_benchmark</uri>, and on Zenodo, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://doi.org/10.5281/zenodo.8174267">doi.org/10.5281/zenodo.8174267</uri>, respectively.</p>
  </notes>
  <notes id="notes-2" notes-type="si">
    <title>Supporting Information Available</title>
    <p>The Supporting Information
is available free of charge at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acs.jcim.3c01250?goto=supporting-info">https://pubs.acs.org/doi/10.1021/acs.jcim.3c01250</ext-link>.<list id="silist" list-type="simple"><list-item><p>Additional software details
and usage examples, data
set and data handling details for benchmarking, and results of software
benchmarks (general performance, feature demonstrations, timing) (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c01250/suppl_file/ci3c01250_si_001.pdf">PDF</ext-link>)</p></list-item></list></p>
  </notes>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sifile1">
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ci3c01250_si_001.pdf">
        <caption>
          <p>ci3c01250_si_001.pdf</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <notes notes-type="COI-statement" id="NOTES-d14e1343-autogenerated">
    <p>The authors declare no
competing financial interest.</p>
  </notes>
  <ack>
    <title>Acknowledgments</title>
    <p>E.H., Y.C., F.H.V., and W.H.G. acknowledge support from the
Machine Learning for Pharmaceutical Discovery and Synthesis Consortium
(MLPDS). K.P.G., S.-C.L., F.H.V., H.W., W.H.G., and C.J.M. acknowledge
support from the DARPA Accelerated Molecular Discovery (AMD) program
(DARPA HR00111920025). E.H. acknowledges support from the Austrian
Science Fund (FWF), project J-4415. K.P.G. was supported by the National
Science Foundation Graduate Research Fellowship Program under Grant
No. 1745302. D.E.G. acknowledges support from the MIT IBM Watson AI
Lab. F.H.V. would like to acknowledge the KU Leuven Internal Starting
Grant (STG/22/032). C.J.M. would like to acknowledge support from
VCU Startup Funding. Parts of the data reported within this paper
were generated with resources from the MIT SuperCloud Lincoln Laboratory
Supercomputing Center.<sup><xref ref-type="bibr" rid="ref87">87</xref></sup></p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="journal" id="cit1"><name><surname>Gilmer</surname><given-names>J.</given-names></name>; <name><surname>Schoenholz</surname><given-names>S. S.</given-names></name>; <name><surname>Riley</surname><given-names>P. F.</given-names></name>; <name><surname>Vinyals</surname><given-names>O.</given-names></name>; <name><surname>Dahl</surname><given-names>G. E.</given-names></name><article-title>Neural
Message Passing for Quantum Chemistry</article-title>. <source>Proceedings
of the International Conference on Machine Learning</source><year>2017</year>, <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="report" id="cit2"><person-group person-group-type="allauthors"><name><surname>Gasteiger</surname><given-names>J.</given-names></name>; <name><surname>Groß</surname><given-names>J.</given-names></name>; <name><surname>Günnemann</surname><given-names>S.</given-names></name></person-group><article-title>Directional Message
Passing for Molecular Graphs</article-title>. <source>Proceedings
of the International Conference on Learning Representations</source>, <bold>2003</bold>, arXiv:2003.03123.</mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="report" id="cit3"><person-group person-group-type="allauthors"><name><surname>Zhang</surname><given-names>S.</given-names></name>; <name><surname>Liu</surname><given-names>Y.</given-names></name>; <name><surname>Xie</surname><given-names>L.</given-names></name></person-group><article-title>Molecular Mechanics-Driven Graph Neural
Network with Multiplex Graph for Molecular Structures</article-title>. <source>Machine Learning for Molecules Workshop at NeurIPS</source>, <bold>2020</bold>, arXiv:2011.07457.</mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting Solubility Limits of Organic
Solutes for a Wide Range of Solvents and Temperatures</article-title>. <source>J. Am. Chem. Soc.</source><year>2022</year>, <volume>144</volume>, <fpage>10785</fpage>–<lpage>10797</lpage>. <pub-id pub-id-type="doi">10.1021/jacs.2c01768</pub-id>.<pub-id pub-id-type="pmid">35687887</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="journal" id="cit5"><name><surname>Dobbelaere</surname><given-names>M. R.</given-names></name>; <name><surname>Ureel</surname><given-names>Y.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Tomme</surname><given-names>L.</given-names></name>; <name><surname>Stevens</surname><given-names>C. V.</given-names></name>; <name><surname>Van Geem</surname><given-names>K. M.</given-names></name><article-title>Machine Learning for Physicochemical Property Prediction
of Complex Hydrocarbon Mixtures</article-title>. <source>Ind. Eng. Chem.
Res.</source><year>2022</year>, <volume>61</volume>, <fpage>8581</fpage>–<lpage>8594</lpage>. <pub-id pub-id-type="doi">10.1021/acs.iecr.2c00442</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal" id="cit6"><name><surname>Dobbelaere</surname><given-names>M. R.</given-names></name>; <name><surname>Plehiers</surname><given-names>P. P.</given-names></name>; <name><surname>Van de Vijver</surname><given-names>R.</given-names></name>; <name><surname>Stevens</surname><given-names>C. V.</given-names></name>; <name><surname>Van Geem</surname><given-names>K. M.</given-names></name><article-title>Learning
Molecular Representations for Thermochemistry Prediction of Cyclic
Hydrocarbons and Oxygenates</article-title>. <source>J. Phys. Chem.
A</source><year>2021</year>, <volume>125</volume>, <fpage>5166</fpage>–<lpage>5179</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpca.1c01956</pub-id>.<pub-id pub-id-type="pmid">34081474</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal" id="cit7"><name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Ben Hicham</surname><given-names>K.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name><article-title>Graph Neural
Networks for Temperature-Dependent Activity Coefficient Prediction
of Solutes in Ionic Liquids</article-title>. <source>Comput. Chem. Eng.</source><year>2023</year>, <volume>171</volume>, <fpage>108153</fpage><pub-id pub-id-type="doi">10.1016/j.compchemeng.2023.108153</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal" id="cit8"><name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Ritzert</surname><given-names>M.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Winkler</surname><given-names>S.</given-names></name>; <name><surname>Weber</surname><given-names>J. M.</given-names></name>; <name><surname>Morsch</surname><given-names>P.</given-names></name>; <name><surname>Heufer</surname><given-names>K. A.</given-names></name>; <name><surname>Grohe</surname><given-names>M.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name><article-title>Graph Machine Learning
for Design of High-Octane Fuels</article-title>. <source>AIChE J.</source><year>2023</year>, <volume>69</volume>, <fpage>e17971</fpage><pub-id pub-id-type="doi">10.1002/aic.17971</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="journal" id="cit9"><name><surname>Fleitmann</surname><given-names>L.</given-names></name>; <name><surname>Ackermann</surname><given-names>P.</given-names></name>; <name><surname>Schilling</surname><given-names>J.</given-names></name>; <name><surname>Kleinekorte</surname><given-names>J.</given-names></name>; <name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>vom Lehn</surname><given-names>F.</given-names></name>; <name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Pitsch</surname><given-names>H.</given-names></name>; <name><surname>Leonhard</surname><given-names>K.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name>; <name><surname>Bardow</surname><given-names>A.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name><article-title>Molecular Design of Fuels for Maximum
Spark-Ignition
Engine Efficiency by Combining Predictive Thermodynamics and Machine
Learning</article-title>. <source>Energ. Fuel.</source><year>2023</year>, <volume>37</volume>, <fpage>2213</fpage>–<lpage>2229</lpage>. <pub-id pub-id-type="doi">10.1021/acs.energyfuels.2c03296</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="journal" id="cit10"><name><surname>Stokes</surname><given-names>J. M.</given-names></name>; <name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Cubillos-Ruiz</surname><given-names>A.</given-names></name>; <name><surname>Donghia</surname><given-names>N. M.</given-names></name>; <name><surname>MacNair</surname><given-names>C. R.</given-names></name>; <name><surname>French</surname><given-names>S.</given-names></name>; <name><surname>Carfrae</surname><given-names>L. A.</given-names></name>; <name><surname>Bloom-Ackermann</surname><given-names>Z.</given-names></name>; et al. <article-title>A Deep Learning Approach to Antibiotic
Discovery</article-title>. <source>Cell</source><year>2020</year>, <volume>180</volume>, <fpage>688</fpage>–<lpage>702</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2020.01.021</pub-id>.<pub-id pub-id-type="pmid">32084340</pub-id></mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="report" id="cit11"><person-group person-group-type="allauthors"><name><surname>De Cao</surname><given-names>N.</given-names></name>; <name><surname>Kipf</surname><given-names>T.</given-names></name></person-group><article-title>MolGAN: An Implicit Generative
Model for Small Molecular Graphs</article-title>. <source>arXiv Preprint</source>, <bold>2022</bold>, arXiv:1805.11973.</mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="journal" id="cit12"><name><surname>Dan</surname><given-names>Y.</given-names></name>; <name><surname>Zhao</surname><given-names>Y.</given-names></name>; <name><surname>Li</surname><given-names>X.</given-names></name>; <name><surname>Li</surname><given-names>S.</given-names></name>; <name><surname>Hu</surname><given-names>M.</given-names></name>; <name><surname>Hu</surname><given-names>J.</given-names></name><article-title>Generative Adversarial
Networks (GAN) Based Efficient Sampling of
Chemical Composition Space for Inverse Design of Inorganic Materials</article-title>. <source>Npj Comput. Mater.</source><year>2020</year>, <volume>6</volume>, <fpage>1</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1038/s41524-020-00352-0</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="journal" id="cit13"><name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name>; <name><surname>Wei</surname><given-names>J. N.</given-names></name>; <name><surname>Duvenaud</surname><given-names>D.</given-names></name>; <name><surname>Hernández-Lobato</surname><given-names>J. M.</given-names></name>; <name><surname>Sánchez-Lengeling</surname><given-names>B.</given-names></name>; <name><surname>Sheberla</surname><given-names>D.</given-names></name>; <name><surname>Aguilera-Iparraguirre</surname><given-names>J.</given-names></name>; <name><surname>Hirzel</surname><given-names>T. D.</given-names></name>; <name><surname>Adams</surname><given-names>R. P.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name><article-title>Automatic
Chemical Design using a Data-Driven Continuous Representation of Molecules</article-title>. <source>ACS Cent. Sci.</source><year>2018</year>, <volume>4</volume>, <fpage>268</fpage>–<lpage>276</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.7b00572</pub-id>.<pub-id pub-id-type="pmid">29532027</pub-id></mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="journal" id="cit14"><name><surname>Wei</surname><given-names>J. N.</given-names></name>; <name><surname>Duvenaud</surname><given-names>D.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name><article-title>Neural Networks
for the Prediction
of Organic Chemistry Reactions</article-title>. <source>ACS Cent. Sci.</source><year>2016</year>, <volume>2</volume>, <fpage>725</fpage>–<lpage>732</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.6b00219</pub-id>.<pub-id pub-id-type="pmid">27800555</pub-id></mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="journal" id="cit15"><name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Waller</surname><given-names>M. P.</given-names></name><article-title>Neural-Symbolic
Machine Learning for Retrosynthesis
and Reaction Prediction</article-title>. <source>Chem. Eur. J.</source><year>2017</year>, <volume>23</volume>, <fpage>5966</fpage>–<lpage>5971</lpage>. <pub-id pub-id-type="doi">10.1002/chem.201605499</pub-id>.<pub-id pub-id-type="pmid">28134452</pub-id></mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal" id="cit16"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Prediction
of Organic Reaction Outcomes using Machine Learning</article-title>. <source>ACS Cent. Sci.</source><year>2017</year>, <volume>3</volume>, <fpage>434</fpage>–<lpage>443</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.7b00064</pub-id>.<pub-id pub-id-type="pmid">28573205</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="journal" id="cit17"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Machine Learning
in Computer-Aided
Synthesis Planning</article-title>. <source>Acc. Chem. Res.</source><year>2018</year>, <volume>51</volume>, <fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1021/acs.accounts.8b00087</pub-id>.<pub-id pub-id-type="pmid">29715002</pub-id></mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="journal" id="cit18"><name><surname>Szymkuć</surname><given-names>S.</given-names></name>; <name><surname>Gajewska</surname><given-names>E. P.</given-names></name>; <name><surname>Klucznik</surname><given-names>T.</given-names></name>; <name><surname>Molga</surname><given-names>K.</given-names></name>; <name><surname>Dittwald</surname><given-names>P.</given-names></name>; <name><surname>Startek</surname><given-names>M.</given-names></name>; <name><surname>Bajczyk</surname><given-names>M.</given-names></name>; <name><surname>Grzybowski</surname><given-names>B. A.</given-names></name><article-title>Computer-Assisted
Synthetic Planning: The End of the Beginning</article-title>. <source>Angew. Chem., Int. Ed.</source><year>2016</year>, <volume>55</volume>, <fpage>5904</fpage>–<lpage>5937</lpage>. <pub-id pub-id-type="doi">10.1002/anie.201506101</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="journal" id="cit19"><name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Preuss</surname><given-names>M.</given-names></name>; <name><surname>Waller</surname><given-names>M. P.</given-names></name><article-title>Planning Chemical Syntheses with
Deep Neural Networks and Symbolic AI</article-title>. <source>Nature</source><year>2018</year>, <volume>555</volume>, <fpage>604</fpage>–<lpage>610</lpage>. <pub-id pub-id-type="doi">10.1038/nature25978</pub-id>.<pub-id pub-id-type="pmid">29595767</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="journal" id="cit20"><name><surname>Kayala</surname><given-names>M. A.</given-names></name>; <name><surname>Azencott</surname><given-names>C.-A.</given-names></name>; <name><surname>Chen</surname><given-names>J. H.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>Learning to Predict
Chemical Reactions</article-title>. <source>J. Chem. Inf. Model.</source><year>2011</year>, <volume>51</volume>, <fpage>2209</fpage>–<lpage>2222</lpage>. <pub-id pub-id-type="doi">10.1021/ci200207y</pub-id>.<pub-id pub-id-type="pmid">21819139</pub-id></mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="journal" id="cit21"><name><surname>Kayala</surname><given-names>M. A.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>ReactionPredictor:
Prediction of Complex Chemical Reactions at the
Mechanistic Level using Machine Learning</article-title>. <source>J.
Chem. Inf. Model.</source><year>2012</year>, <volume>52</volume>, <fpage>2526</fpage>–<lpage>2540</lpage>. <pub-id pub-id-type="doi">10.1021/ci3003039</pub-id>.<pub-id pub-id-type="pmid">22978639</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="journal" id="cit22"><name><surname>Fooshee</surname><given-names>D.</given-names></name>; <name><surname>Mood</surname><given-names>A.</given-names></name>; <name><surname>Gutman</surname><given-names>E.</given-names></name>; <name><surname>Tavakoli</surname><given-names>M.</given-names></name>; <name><surname>Urban</surname><given-names>G.</given-names></name>; <name><surname>Liu</surname><given-names>F.</given-names></name>; <name><surname>Huynh</surname><given-names>N.</given-names></name>; <name><surname>Van Vranken</surname><given-names>D.</given-names></name>; <name><surname>Baldi</surname><given-names>P.</given-names></name><article-title>Deep Learning for Chemical
Reaction Prediction</article-title>. <source>Mol. Syst. Des. Eng.</source><year>2018</year>, <volume>3</volume>, <fpage>442</fpage>–<lpage>452</lpage>. <pub-id pub-id-type="doi">10.1039/C7ME00107J</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="report" id="cit23"><person-group person-group-type="allauthors"><name><surname>Bradshaw</surname><given-names>J.</given-names></name>; <name><surname>Kusner</surname><given-names>M. J.</given-names></name>; <name><surname>Paige</surname><given-names>B.</given-names></name>; <name><surname>Segler</surname><given-names>M. H.</given-names></name>; <name><surname>Hernández-Lobato</surname><given-names>J. M.</given-names></name></person-group><article-title>A Generative Model for Electron Paths</article-title>. <source>Proceedings
of the International Conference on Learning Representations</source>, <bold>2019</bold>, arXiv:1805.10970.</mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="journal" id="cit24"><name><surname>Bi</surname><given-names>H.</given-names></name>; <name><surname>Wang</surname><given-names>H.</given-names></name>; <name><surname>Shi</surname><given-names>C.</given-names></name>; <name><surname>Coley</surname><given-names>C.</given-names></name>; <name><surname>Tang</surname><given-names>J.</given-names></name>; <name><surname>Guo</surname><given-names>H.</given-names></name><article-title>Non-Autoregressive
Electron Redistribution Modeling for Reaction
Prediction</article-title>. <source>Proceedings of the International
Conference on Machine Learning</source><year>2021</year>, <volume>139</volume>, <fpage>904</fpage>–<lpage>913</lpage>.</mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="journal" id="cit25"><name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Rogers</surname><given-names>L.</given-names></name>; <name><surname>Jamison</surname><given-names>T. F.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>A Graph-Convolutional Neural Network
Model for the Prediction of Chemical Reactivity</article-title>. <source>Chem. Sci.</source><year>2019</year>, <volume>10</volume>, <fpage>370</fpage>–<lpage>377</lpage>. <pub-id pub-id-type="doi">10.1039/C8SC04228D</pub-id>.<pub-id pub-id-type="pmid">30746086</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <mixed-citation publication-type="journal" id="cit26"><name><surname>Sacha</surname><given-names>M.</given-names></name>; <name><surname>Błaz</surname><given-names>M.</given-names></name>; <name><surname>Byrski</surname><given-names>P.</given-names></name>; <name><surname>Dabrowski-Tumanski</surname><given-names>P.</given-names></name>; <name><surname>Chrominski</surname><given-names>M.</given-names></name>; <name><surname>Loska</surname><given-names>R.</given-names></name>; <name><surname>Włodarczyk-Pruszynski</surname><given-names>P.</given-names></name>; <name><surname>Jastrzebski</surname><given-names>S.</given-names></name><article-title>Molecule Edit Graph Attention Network: Modeling Chemical
Reactions as Sequences of Graph Edits</article-title>. <source>J. Chem.
Inf. Model.</source><year>2021</year>, <volume>61</volume>, <fpage>3273</fpage>–<lpage>3284</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00537</pub-id>.<pub-id pub-id-type="pmid">34251814</pub-id></mixed-citation>
    </ref>
    <ref id="ref27">
      <mixed-citation publication-type="journal" id="cit27"><name><surname>Schwaller</surname><given-names>P.</given-names></name>; <name><surname>Gaudin</surname><given-names>T.</given-names></name>; <name><surname>Lanyi</surname><given-names>D.</given-names></name>; <name><surname>Bekas</surname><given-names>C.</given-names></name>; <name><surname>Laino</surname><given-names>T.</given-names></name><article-title>“Found
in Translation”: Predicting Outcomes of Complex Organic Chemistry
Reactions using Neural Sequence-to-Sequence Models</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>6091</fpage>–<lpage>6098</lpage>. <pub-id pub-id-type="doi">10.1039/C8SC02339E</pub-id>.<pub-id pub-id-type="pmid">30090297</pub-id></mixed-citation>
    </ref>
    <ref id="ref28">
      <mixed-citation publication-type="journal" id="cit28"><name><surname>McGill</surname><given-names>C.</given-names></name>; <name><surname>Forsuelo</surname><given-names>M.</given-names></name>; <name><surname>Guan</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting Infrared
Spectra with Message Passing Neural Networks</article-title>. <source>J. Chem. Inf. Model.</source><year>2021</year>, <volume>61</volume>, <fpage>2594</fpage>–<lpage>2609</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00055</pub-id>.<pub-id pub-id-type="pmid">34048221</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <mixed-citation publication-type="journal" id="cit29"><name><surname>Greenman</surname><given-names>K. P.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></name><article-title>Multi-Fidelity
Prediction
of Molecular Optical Peaks with Deep Learning</article-title>. <source>Chem. Sci.</source><year>2022</year>, <volume>13</volume>, <fpage>1152</fpage>–<lpage>1162</lpage>. <pub-id pub-id-type="doi">10.1039/D1SC05677H</pub-id>.<pub-id pub-id-type="pmid">35211282</pub-id></mixed-citation>
    </ref>
    <ref id="ref30">
      <mixed-citation publication-type="journal" id="cit30"><name><surname>Dührkop</surname><given-names>K.</given-names></name><article-title>Deep Kernel
Learning Improves Molecular Fingerprint Prediction from Tandem Mass
Spectra</article-title>. <source>Bioinf.</source><year>2022</year>, <volume>38</volume>, <fpage>i342</fpage>–<lpage>i349</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btac260</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref31">
      <mixed-citation publication-type="journal" id="cit31"><name><surname>Nguyen</surname><given-names>D. H.</given-names></name>; <name><surname>Nguyen</surname><given-names>C. H.</given-names></name>; <name><surname>Mamitsuka</surname><given-names>H.</given-names></name><article-title>ADAPTIVE:
leArning DAta-dePendenT,
concIse molecular VEctors for Fast, Accurate Metabolite Identification
from Tandem Mass Spectra</article-title>. <source>Bioinf.</source><year>2019</year>, <volume>35</volume>, <fpage>i164</fpage>–<lpage>i172</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz319</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref32">
      <mixed-citation publication-type="journal" id="cit32"><name><surname>Nguyen</surname><given-names>D. H.</given-names></name>; <name><surname>Nguyen</surname><given-names>C. H.</given-names></name>; <name><surname>Mamitsuka</surname><given-names>H.</given-names></name><article-title>Recent Advances
and Prospects of
Computational Methods for Metabolite Identification: A Review with
Emphasis on Machine Learning Approaches</article-title>. <source>Brief.
Bioinf.</source><year>2019</year>, <volume>20</volume>, <fpage>2028</fpage>–<lpage>2043</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bby066</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <mixed-citation publication-type="journal" id="cit33"><name><surname>Stravs</surname><given-names>M.
A.</given-names></name>; <name><surname>Dührkop</surname><given-names>K.</given-names></name>; <name><surname>Böcker</surname><given-names>S.</given-names></name>; <name><surname>Zamboni</surname><given-names>N.</given-names></name><article-title>MSNovelist: De Novo
Structure Generation from Mass Spectra</article-title>. <source>Nat.
Methods</source><year>2022</year>, <volume>19</volume>, <fpage>865</fpage>–<lpage>870</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-022-01486-3</pub-id>.<pub-id pub-id-type="pmid">35637304</pub-id></mixed-citation>
    </ref>
    <ref id="ref34">
      <mixed-citation publication-type="journal" id="cit34"><name><surname>Muratov</surname><given-names>E. N.</given-names></name>; <name><surname>Bajorath</surname><given-names>J.</given-names></name>; <name><surname>Sheridan</surname><given-names>R. P.</given-names></name>; <name><surname>Tetko</surname><given-names>I. V.</given-names></name>; <name><surname>Filimonov</surname><given-names>D.</given-names></name>; <name><surname>Poroikov</surname><given-names>V.</given-names></name>; <name><surname>Oprea</surname><given-names>T. I.</given-names></name>; <name><surname>Baskin</surname><given-names>I. I.</given-names></name>; <name><surname>Varnek</surname><given-names>A.</given-names></name>; <name><surname>Roitberg</surname><given-names>A.</given-names></name>; et al. <article-title>QSAR without Borders</article-title>. <source>Chem. Soc. Rev.</source><year>2020</year>, <volume>49</volume>, <fpage>3525</fpage>–<lpage>3564</lpage>. <pub-id pub-id-type="doi">10.1039/D0CS00098A</pub-id>.<pub-id pub-id-type="pmid">32356548</pub-id></mixed-citation>
    </ref>
    <ref id="ref35">
      <mixed-citation publication-type="journal" id="cit35"><name><surname>Kearnes</surname><given-names>S.</given-names></name>; <name><surname>McCloskey</surname><given-names>K.</given-names></name>; <name><surname>Berndl</surname><given-names>M.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Riley</surname><given-names>P.</given-names></name><article-title>Molecular
Graph Convolutions: Moving Beyond Fingerprints</article-title>. <source>J. Comput. Aided Mol. Des.</source><year>2016</year>, <volume>30</volume>, <fpage>595</fpage>–<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>.<pub-id pub-id-type="pmid">27558503</pub-id></mixed-citation>
    </ref>
    <ref id="ref36">
      <mixed-citation publication-type="journal" id="cit36"><name><surname>Yang</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Coley</surname><given-names>C.</given-names></name>; <name><surname>Eiden</surname><given-names>P.</given-names></name>; <name><surname>Gao</surname><given-names>H.</given-names></name>; <name><surname>Guzman-Perez</surname><given-names>A.</given-names></name>; <name><surname>Hopper</surname><given-names>T.</given-names></name>; <name><surname>Kelley</surname><given-names>B.</given-names></name>; <name><surname>Mathea</surname><given-names>M.</given-names></name>; <name><surname>Palmer</surname><given-names>A.</given-names></name>; <name><surname>Settels</surname><given-names>V.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T.</given-names></name>; <name><surname>Jensen</surname><given-names>K.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name><article-title>Analyzing
Learned Molecular Representations for Property
Prediction</article-title>. <source>J. Chem. Inf. Model.</source><year>2019</year>, <volume>59</volume>, <fpage>3370</fpage>–<lpage>3388</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id>.<pub-id pub-id-type="pmid">31361484</pub-id></mixed-citation>
    </ref>
    <ref id="ref37">
      <mixed-citation publication-type="report" id="cit37"><person-group person-group-type="allauthors"><name><surname>Maziarka</surname><given-names>Ł.</given-names></name>; <name><surname>Danel</surname><given-names>T.</given-names></name>; <name><surname>Mucha</surname><given-names>S.</given-names></name>; <name><surname>Rataj</surname><given-names>K.</given-names></name>; <name><surname>Tabor</surname><given-names>J.</given-names></name>; <name><surname>Jastrzebski</surname><given-names>S.</given-names></name></person-group><article-title>Molecule Attention Transformer</article-title>. <source>arXiv Preprint</source>, <bold>2020</bold>, arXiv:2002.08264.</mixed-citation>
    </ref>
    <ref id="ref38">
      <mixed-citation publication-type="journal" id="cit38"><name><surname>Kreuzer</surname><given-names>D.</given-names></name>; <name><surname>Beaini</surname><given-names>D.</given-names></name>; <name><surname>Hamilton</surname><given-names>W.</given-names></name>; <name><surname>Létourneau</surname><given-names>V.</given-names></name>; <name><surname>Tossou</surname><given-names>P.</given-names></name><article-title>Rethinking Graph Transformers with Spectral Attention</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2021</year>, <volume>34</volume>, <fpage>21618</fpage>–<lpage>21629</lpage>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <mixed-citation publication-type="journal" id="cit39"><name><surname>Schütt</surname><given-names>K. T.</given-names></name>; <name><surname>Sauceda</surname><given-names>H. E.</given-names></name>; <name><surname>Kindermans</surname><given-names>P.-J.</given-names></name>; <name><surname>Tkatchenko</surname><given-names>A.</given-names></name>; <name><surname>Müller</surname><given-names>K.-R.</given-names></name><article-title>Schnet–A Deep Learning Architecture
for Molecules
and Materials</article-title>. <source>J. Chem. Phys.</source><year>2018</year>, <volume>148</volume>, <fpage>241722</fpage><pub-id pub-id-type="doi">10.1063/1.5019779</pub-id>.<pub-id pub-id-type="pmid">29960322</pub-id></mixed-citation>
    </ref>
    <ref id="ref40">
      <mixed-citation publication-type="journal" id="cit40"><name><surname>Unke</surname><given-names>O. T.</given-names></name>; <name><surname>Meuwly</surname><given-names>M.</given-names></name><article-title>PhysNet: A Neural Network
for Predicting Energies,
Forces, Dipole Moments, and Partial Charges</article-title>. <source>J. Chem. Theory Comput.</source><year>2019</year>, <volume>15</volume>, <fpage>3678</fpage>–<lpage>3693</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.9b00181</pub-id>.<pub-id pub-id-type="pmid">31042390</pub-id></mixed-citation>
    </ref>
    <ref id="ref41">
      <mixed-citation publication-type="report" id="cit41"><person-group person-group-type="allauthors"><name><surname>Bigi</surname><given-names>F.</given-names></name>, <name><surname>Pozdnyakov</surname><given-names>S. N.</given-names></name>, <name><surname>Ceriotti</surname><given-names>M.</given-names></name></person-group><article-title>Wigner
Kernels:
Body-Ordered Equivariant Machine Learning without a Basis</article-title>. <source>arXiv Preprint</source>, <bold>2023</bold>, arXiv:2303.04124.</mixed-citation>
    </ref>
    <ref id="ref42">
      <mixed-citation publication-type="journal" id="cit42"><name><surname>Winter</surname><given-names>B.</given-names></name>; <name><surname>Winter</surname><given-names>C.</given-names></name>; <name><surname>Schilling</surname><given-names>J.</given-names></name>; <name><surname>Bardow</surname><given-names>A.</given-names></name><article-title>A Smile is All you
Need: Predicting Limiting Activity Coefficients from SMILES with Natural
Language Processing</article-title>. <source>Digital Discovery</source><year>2022</year>, <volume>1</volume>, <fpage>859</fpage>–<lpage>869</lpage>. <pub-id pub-id-type="doi">10.1039/D2DD00058J</pub-id>.<pub-id pub-id-type="pmid">36561987</pub-id></mixed-citation>
    </ref>
    <ref id="ref43">
      <mixed-citation publication-type="journal" id="cit43"><name><surname>Bagal</surname><given-names>V.</given-names></name>; <name><surname>Aggarwal</surname><given-names>R.</given-names></name>; <name><surname>Vinod</surname><given-names>P.</given-names></name>; <name><surname>Priyakumar</surname><given-names>U. D.</given-names></name><article-title>MolGPT:
Molecular Generation using a Transformer-Decoder Model</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>2064</fpage>–<lpage>2076</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00600</pub-id>.<pub-id pub-id-type="pmid">34694798</pub-id></mixed-citation>
    </ref>
    <ref id="ref44">
      <mixed-citation publication-type="report" id="cit44"><person-group person-group-type="allauthors"><name><surname>Honda</surname><given-names>S.</given-names></name>; <name><surname>Shi</surname><given-names>S.</given-names></name>; <name><surname>Ueda</surname><given-names>H.
R.</given-names></name></person-group><article-title>Smiles Transformer:
Pre-trained
Molecular Fingerprint for Low Data Drug Discovery</article-title>. <source>arXiv Preprint</source>, <bold>2019</bold>, arXiv:1911.04738.</mixed-citation>
    </ref>
    <ref id="ref45">
      <mixed-citation publication-type="conf-proc" id="cit45"><person-group person-group-type="allauthors"><name><surname>Chithrananda</surname><given-names>S.</given-names></name>; <name><surname>Grand</surname><given-names>G.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name></person-group><article-title>Chemberta: Large-Scale
Self-Supervised Pretraining for Molecular Property Prediction</article-title>. <source>Machine Learning for Molecules Workshop at NeurIPS</source>, <bold>2020</bold>, arXiv:2010.09885.</mixed-citation>
    </ref>
    <ref id="ref46">
      <mixed-citation publication-type="weblink" id="cit46"><person-group person-group-type="allauthors"><name><surname>Landrum</surname><given-names>G.</given-names></name></person-group><article-title>RDKit: Open-Source
Cheminformatics</article-title>. <bold>2006</bold>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.rdkit.org/">https://www.rdkit.org/</uri>.</mixed-citation>
    </ref>
    <ref id="ref47">
      <mixed-citation publication-type="conf-proc" id="cit47"><person-group person-group-type="allauthors"><name><surname>Mahé</surname><given-names>P.</given-names></name>; <name><surname>Ueda</surname><given-names>N.</given-names></name>; <name><surname>Akutsu</surname><given-names>T.</given-names></name>; <name><surname>Perret</surname><given-names>J.-L.</given-names></name>; <name><surname>Vert</surname><given-names>J.-P.</given-names></name></person-group><article-title>Extensions
of marginalized graph kernels</article-title>. <source>Proceedings
of the International Conference on Machine Learning</source>, <bold>2004</bold>, <volume>70</volume>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <mixed-citation publication-type="journal" id="cit48"><name><surname>Schweidtmann</surname><given-names>A. M.</given-names></name>; <name><surname>Rittig</surname><given-names>J. G.</given-names></name>; <name><surname>Weber</surname><given-names>J. M.</given-names></name>; <name><surname>Grohe</surname><given-names>M.</given-names></name>; <name><surname>Dahmen</surname><given-names>M.</given-names></name>; <name><surname>Leonhard</surname><given-names>K.</given-names></name>; <name><surname>Mitsos</surname><given-names>A.</given-names></name><article-title>Physical Pooling Functions in Graph
Neural Networks for Molecular Property Prediction</article-title>. <source>Comput. Chem. Eng.</source><year>2023</year>, <volume>172</volume>, <fpage>108202</fpage><pub-id pub-id-type="doi">10.1016/j.compchemeng.2023.108202</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref49">
      <mixed-citation publication-type="journal" id="cit49"><name><surname>Morgan</surname><given-names>H. L.</given-names></name><article-title>The Generation
of a Unique Machine Description for Chemical Structures – A
Technique Developed at Chemical Abstracts Service</article-title>. <source>J. Chem. Doc.</source><year>1965</year>, <volume>5</volume>, <fpage>107</fpage>–<lpage>113</lpage>. <pub-id pub-id-type="doi">10.1021/c160017a018</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref50">
      <mixed-citation publication-type="report" id="cit50"><person-group person-group-type="allauthors"><name><surname>Kingma</surname><given-names>D. P.</given-names></name>; <name><surname>Ba</surname><given-names>J.</given-names></name></person-group><article-title>Adam: A Method for Stochastic
Optimization</article-title>. <source>Proceedings of the International
Conference on Learning Representations</source>, <bold>2017</bold>, arXiv:1412.6980.</mixed-citation>
    </ref>
    <ref id="ref51">
      <mixed-citation publication-type="journal" id="cit51"><name><surname>Lim</surname><given-names>M. A.</given-names></name>; <name><surname>Yang</surname><given-names>S.</given-names></name>; <name><surname>Mai</surname><given-names>H.</given-names></name>; <name><surname>Cheng</surname><given-names>A. C.</given-names></name><article-title>Exploring Deep Learning
of Quantum Chemical Properties for Absorption, Distribution, Metabolism,
and Excretion Predictions</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>6336</fpage>–<lpage>6341</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c00245</pub-id>.<pub-id pub-id-type="pmid">35758421</pub-id></mixed-citation>
    </ref>
    <ref id="ref52">
      <mixed-citation publication-type="journal" id="cit52"><name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Stokes</surname><given-names>J. M.</given-names></name>; <name><surname>Eastman</surname><given-names>R. T.</given-names></name>; <name><surname>Itkin</surname><given-names>Z.</given-names></name>; <name><surname>Zakharov</surname><given-names>A. V.</given-names></name>; <name><surname>Collins</surname><given-names>J. J.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name><article-title>Deep Learning Identifies
Synergistic Drug Combinations for Treating COVID-19</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><year>2021</year>, <volume>118</volume>, <fpage>e2105070118</fpage><pub-id pub-id-type="doi">10.1073/pnas.2105070118</pub-id>.<pub-id pub-id-type="pmid">34526388</pub-id></mixed-citation>
    </ref>
    <ref id="ref53">
      <mixed-citation publication-type="journal" id="cit53"><name><surname>Liu</surname><given-names>G.</given-names></name>; <name><surname>Catacutan</surname><given-names>D. B.</given-names></name>; <name><surname>Rathod</surname><given-names>K.</given-names></name>; <name><surname>Swanson</surname><given-names>K.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Mohammed</surname><given-names>J. C.</given-names></name>; <name><surname>Chiappino-Pepe</surname><given-names>A.</given-names></name>; <name><surname>Syed</surname><given-names>S. A.</given-names></name>; <name><surname>Fragis</surname><given-names>M.</given-names></name>; <name><surname>Rachwalski</surname><given-names>K.</given-names></name>; et al. <article-title>Deep Learning-Guided Discovery of an Antibiotic
Targeting Acinetobacter Baumannii</article-title>. <source>Nat. Chem.
Biol.</source><year>2023</year>, <volume>19</volume>, <fpage>1342</fpage>–<lpage>1350</lpage>. <pub-id pub-id-type="doi">10.1038/s41589-023-01349-8</pub-id>.<pub-id pub-id-type="pmid">37231267</pub-id></mixed-citation>
    </ref>
    <ref id="ref54">
      <mixed-citation publication-type="journal" id="cit54"><name><surname>Larsson</surname><given-names>T.</given-names></name>; <name><surname>Vermeire</surname><given-names>F.</given-names></name>; <name><surname>Verhelst</surname><given-names>S.</given-names></name><article-title>Machine Learning for Fuel Property
Predictions: A Multi-Task and Transfer Learning Approach</article-title>. <source>SAE Technical Paper</source><year>2023</year>, <pub-id pub-id-type="doi">10.4271/2023-01-0337</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref55">
      <mixed-citation publication-type="journal" id="cit55"><name><surname>Guan</surname><given-names>Y.</given-names></name>; <name><surname>Coley</surname><given-names>C. W.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Ranasinghe</surname><given-names>D.</given-names></name>; <name><surname>Heid</surname><given-names>E.</given-names></name>; <name><surname>Struble</surname><given-names>T. J.</given-names></name>; <name><surname>Pattanaik</surname><given-names>L.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Regio-Selectivity Prediction with
a Machine-Learned Reaction Representation and On-the-Fly Quantum Mechanical
Descriptors</article-title>. <source>Chem. Sci.</source><year>2021</year>, <volume>12</volume>, <fpage>2198</fpage>–<lpage>2208</lpage>. <pub-id pub-id-type="doi">10.1039/D0SC04823B</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref56">
      <mixed-citation publication-type="journal" id="cit56"><name><surname>Biswas</surname><given-names>S.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Ramirez</surname><given-names>J.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting
Critical Properties and Acentric Factor of Fluids using Multi-Task
Machine Learning</article-title>. <source>J. Chem. Inf. Model.</source><year>2023</year>, <volume>63</volume>, <fpage>4574</fpage>–<lpage>4588</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.3c00546</pub-id>.<pub-id pub-id-type="pmid">37487557</pub-id></mixed-citation>
    </ref>
    <ref id="ref57">
      <mixed-citation publication-type="journal" id="cit57"><name><surname>Lenselink</surname><given-names>E. B.</given-names></name>; <name><surname>Stouten</surname><given-names>P. F. W.</given-names></name><article-title>Multitask machine
learning models for predicting lipophilicity
(logP) in the SAMPL7 challenge</article-title>. <source>Journal of Computer-Aided
Molecular Design</source><year>2021</year>, <volume>35</volume>, <fpage>901</fpage>–<lpage>909</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-021-00405-6</pub-id>.<pub-id pub-id-type="pmid">34273053</pub-id></mixed-citation>
    </ref>
    <ref id="ref58">
      <mixed-citation publication-type="journal" id="cit58"><name><surname>McNaughton</surname><given-names>A. D.</given-names></name>; <name><surname>Joshi</surname><given-names>R. P.</given-names></name>; <name><surname>Knutson</surname><given-names>C. R.</given-names></name>; <name><surname>Fnu</surname><given-names>A.</given-names></name>; <name><surname>Luebke</surname><given-names>K. J.</given-names></name>; <name><surname>Malerich</surname><given-names>J. P.</given-names></name>; <name><surname>Madrid</surname><given-names>P. B.</given-names></name>; <name><surname>Kumar</surname><given-names>N.</given-names></name><article-title>Machine Learning
Models
for Predicting Molecular UV–Vis Spectra with Quantum Mechanical
Properties</article-title>. <source>J. Chem. Inf. Model.</source><year>2023</year>, <volume>63</volume>, <fpage>1462</fpage>–<lpage>1471</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c01662</pub-id>.<pub-id pub-id-type="pmid">36847578</pub-id></mixed-citation>
    </ref>
    <ref id="ref59">
      <mixed-citation publication-type="journal" id="cit59"><name><surname>Heid</surname><given-names>E.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Machine Learning
of Reaction Properties via Learned
Representations of the Condensed Graph of Reaction</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>2101</fpage>–<lpage>2110</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c00975</pub-id>.<pub-id pub-id-type="pmid">34734699</pub-id></mixed-citation>
    </ref>
    <ref id="ref60">
      <mixed-citation publication-type="journal" id="cit60"><name><surname>Isert</surname><given-names>C.</given-names></name>; <name><surname>Kromann</surname><given-names>J. C.</given-names></name>; <name><surname>Stiefl</surname><given-names>N.</given-names></name>; <name><surname>Schneider</surname><given-names>G.</given-names></name>; <name><surname>Lewis</surname><given-names>R. A.</given-names></name><article-title>Machine Learning for Fast, Quantum Mechanics-Based
Approximation of Drug Lipophilicity</article-title>. <source>ACS Omega</source><year>2023</year>, <volume>8</volume>, <fpage>2046</fpage>–<lpage>2056</lpage>. <pub-id pub-id-type="doi">10.1021/acsomega.2c05607</pub-id>.<pub-id pub-id-type="pmid">36687099</pub-id></mixed-citation>
    </ref>
    <ref id="ref61">
      <mixed-citation publication-type="journal" id="cit61"><name><surname>Spiekermann</surname><given-names>K. A.</given-names></name>; <name><surname>Pattanaik</surname><given-names>L.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Fast Predictions of Reaction Barrier
Heights: Toward Coupled-Cluster Accuracy</article-title>. <source>J.
Phys. Chem. A</source><year>2022</year>, <volume>126</volume>, <fpage>3976</fpage>–<lpage>3986</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpca.2c02614</pub-id>.<pub-id pub-id-type="pmid">35727075</pub-id></mixed-citation>
    </ref>
    <ref id="ref62">
      <mixed-citation publication-type="report" id="cit62"><person-group person-group-type="allauthors"><name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W.
H.</given-names></name></person-group><article-title>Machine Learning
from Quantum Chemistry to Predict Experimental Solvent Effects on
Reaction Rates</article-title>. <source>ChemRxiv Preprint</source>, <bold>2023</bold>.</mixed-citation>
    </ref>
    <ref id="ref63">
      <mixed-citation publication-type="journal" id="cit63"><name><surname>Lansford</surname><given-names>J.
L.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name>; <name><surname>Barnes</surname><given-names>B. C.</given-names></name><article-title>Physics-informed Transfer Learning
for Out-of-sample Vapor Pressure Predictions</article-title>. <source>Propellants, Explosives, Pyrotechnics</source><year>2023</year>, <volume>48</volume>, <fpage>e202200265</fpage><pub-id pub-id-type="doi">10.1002/prep.202200265</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref64">
      <mixed-citation publication-type="journal" id="cit64"><name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Walker</surname><given-names>P. J.</given-names></name>; <name><surname>Abraham</surname><given-names>M. H.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Group Contribution and Machine Learning Approaches
to Predict Abraham Solute Parameters, Solvation Free Energy, and Solvation
Enthalpy</article-title>. <source>J. Chem. Inf. Model.</source><year>2022</year>, <volume>62</volume>, <fpage>433</fpage>–<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.1c01103</pub-id>.<pub-id pub-id-type="pmid">35044781</pub-id></mixed-citation>
    </ref>
    <ref id="ref65">
      <mixed-citation publication-type="journal" id="cit65"><name><surname>Koscher</surname><given-names>B. A.</given-names></name>; <name><surname>Canty</surname><given-names>R. B.</given-names></name>; <name><surname>McDonald</surname><given-names>M. A.</given-names></name>; <name><surname>Greenman</surname><given-names>K. P.</given-names></name>; <name><surname>McGill</surname><given-names>C. J.</given-names></name>; <name><surname>Bilodeau</surname><given-names>C. L.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Wu</surname><given-names>H.</given-names></name>; <name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Jin</surname><given-names>B.</given-names></name>; <name><surname>Hart</surname><given-names>T.</given-names></name>; <name><surname>Kulesza</surname><given-names>T.</given-names></name>; <name><surname>Li</surname><given-names>S.-C.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Gomez-Bombarelli</surname><given-names>R.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>Autonomous,
Multiproperty-Driven Molecular Discovery: From Predictions to Measurements
and Back</article-title>. <source>Science</source><year>2023</year>, <volume>382</volume> (<issue>6677</issue>), <fpage>eadi1407</fpage><pub-id pub-id-type="doi">10.1126/science.adi1407</pub-id>.<pub-id pub-id-type="pmid">38127734</pub-id>
</mixed-citation>
    </ref>
    <ref id="ref66">
      <mixed-citation publication-type="journal" id="cit66"><name><surname>Vermeire</surname><given-names>F. H.</given-names></name>; <name><surname>Chung</surname><given-names>Y.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Predicting
Solubility Limits of Organic
Solutes for a Wide Range of Solvents and Temperatures</article-title>. <source>J. Am. Chem. Soc.</source><year>2022</year>, <volume>144</volume>, <fpage>10785</fpage>–<lpage>10797</lpage>. <pub-id pub-id-type="doi">10.1021/jacs.2c01768</pub-id>.<pub-id pub-id-type="pmid">35687887</pub-id></mixed-citation>
    </ref>
    <ref id="ref67">
      <mixed-citation publication-type="journal" id="cit67"><name><surname>Wong</surname><given-names>F.</given-names></name>; <name><surname>Omori</surname><given-names>S.</given-names></name>; <name><surname>Donghia</surname><given-names>N. M.</given-names></name>; <name><surname>Zheng</surname><given-names>E. J.</given-names></name>; <name><surname>Collins</surname><given-names>J. J.</given-names></name><article-title>Discovering
Small-Molecule Senolytics with Deep Neural Networks</article-title>. <source>Nature Aging</source><year>2023</year>, <volume>3</volume>, <fpage>734</fpage>–<lpage>750</lpage>. <pub-id pub-id-type="doi">10.1038/s43587-023-00415-z</pub-id>.<pub-id pub-id-type="pmid">37142829</pub-id></mixed-citation>
    </ref>
    <ref id="ref68">
      <mixed-citation publication-type="conf-proc" id="cit68"><person-group><name><surname>Felton</surname><given-names>K. C.</given-names></name>; <name><surname>Ben-Safar</surname><given-names>H.</given-names></name>; <name><surname>Alexei</surname><given-names>A.</given-names></name></person-group><article-title>DeepGamma: A Deep Learning
Model for
Activity Coefficient Prediction</article-title>. <source>1st Annual
AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)</source>, <bold>2022</bold>.</mixed-citation>
    </ref>
    <ref id="ref69">
      <mixed-citation publication-type="journal" id="cit69"><name><surname>Colomba</surname><given-names>M.</given-names></name>; <name><surname>Benedetti</surname><given-names>S.</given-names></name>; <name><surname>Fraternale</surname><given-names>D.</given-names></name>; <name><surname>Guidarelli</surname><given-names>A.</given-names></name>; <name><surname>Coppari</surname><given-names>S.</given-names></name>; <name><surname>Freschi</surname><given-names>V.</given-names></name>; <name><surname>Crinelli</surname><given-names>R.</given-names></name>; <name><surname>Kass</surname><given-names>G. E. N.</given-names></name>; <name><surname>Gorassini</surname><given-names>A.</given-names></name>; <name><surname>Verardo</surname><given-names>G.</given-names></name>; <name><surname>Roselli</surname><given-names>C.</given-names></name>; <name><surname>Meli</surname><given-names>M. A.</given-names></name>; <name><surname>Di Giacomo</surname><given-names>B.</given-names></name>; <name><surname>Albertini</surname><given-names>M. C.</given-names></name><article-title>Nrf2-Mediated Pathway Activated by
Prunus spinosa L. (Rosaceae) Fruit Extract: Bioinformatics Analyses
and Experimental Validation</article-title>. <source>Nutrients</source><year>2023</year>, <volume>15</volume>, <fpage>2132</fpage><pub-id pub-id-type="doi">10.3390/nu15092132</pub-id>.<pub-id pub-id-type="pmid">37432298</pub-id></mixed-citation>
    </ref>
    <ref id="ref70">
      <mixed-citation publication-type="journal" id="cit70"><name><surname>Chang</surname><given-names>C.-I.</given-names></name><article-title>An Information-Theoretic
Approach to Spectral Variability, Similarity, and Discrimination for
Hyperspectral Image Analysis</article-title>. <source>IEEE Transactions
on Information Theory</source><year>2000</year>, <volume>46</volume>, <fpage>1927</fpage>–<lpage>1932</lpage>. <pub-id pub-id-type="doi">10.1109/18.857802</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref71">
      <mixed-citation publication-type="journal" id="cit71"><name><surname>Nix</surname><given-names>D. A.</given-names></name>; <name><surname>Weigend</surname><given-names>A. S.</given-names></name><article-title>Estimating the Mean and Variance of the Target Probability
Distribution</article-title>. <source>Proceedings of IEEE International
Conference on Neural Networks</source><year>1994</year>, <volume>1</volume>, <fpage>55</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="ref72">
      <mixed-citation publication-type="journal" id="cit72"><name><surname>Amini</surname><given-names>A.</given-names></name>; <name><surname>Schwarting</surname><given-names>W.</given-names></name>; <name><surname>Soleimany</surname><given-names>A.</given-names></name>; <name><surname>Rus</surname><given-names>D.</given-names></name><article-title>Deep Evidential Regression</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><year>2020</year>, <volume>33</volume>, <fpage>14927</fpage>–<lpage>14937</lpage>.</mixed-citation>
    </ref>
    <ref id="ref73">
      <mixed-citation publication-type="journal" id="cit73"><name><surname>Sensoy</surname><given-names>M.</given-names></name>; <name><surname>Kaplan</surname><given-names>L.</given-names></name>; <name><surname>Kandemir</surname><given-names>M.</given-names></name><article-title>Evidential Deep Learning
to Quantify
Classification Uncertainty</article-title>. <source>Adv. Neural Inf.
Process. Syst.</source><year>2018</year>, <volume>31</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref74">
      <mixed-citation publication-type="book" id="cit74"><person-group person-group-type="allauthors"><name><surname>Villani</surname><given-names>C.</given-names></name></person-group><source>Optimal Transport: Old
and New</source>; <publisher-name>Springer</publisher-name>, <bold>2009</bold>; Vol. <volume>338</volume>.</mixed-citation>
    </ref>
    <ref id="ref75">
      <mixed-citation publication-type="journal" id="cit75"><name><surname>Bergstra</surname><given-names>J.</given-names></name>; <name><surname>Bardenet</surname><given-names>R.</given-names></name>; <name><surname>Bengio</surname><given-names>Y.</given-names></name>; <name><surname>Kégl</surname><given-names>B.</given-names></name><article-title>Algorithms
for Hyper-Parameter Optimization</article-title>. <source>Adv. Neural
Inf. Process. Syst.</source><year>2011</year>, <volume>24</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref76">
      <mixed-citation publication-type="journal" id="cit76"><name><surname>Bergstra</surname><given-names>J.</given-names></name>; <name><surname>Yamins</surname><given-names>D.</given-names></name>; <name><surname>Cox</surname><given-names>D.</given-names></name><article-title>Making a Science
of Model Search:
Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures</article-title>. <source>Proceedings of the International Conference on Machine Learning</source><year>2013</year>, <fpage>115</fpage>–<lpage>123</lpage>.</mixed-citation>
    </ref>
    <ref id="ref77">
      <mixed-citation publication-type="journal" id="cit77"><name><surname>Lakshminarayanan</surname><given-names>B.</given-names></name>; <name><surname>Pritzel</surname><given-names>A.</given-names></name>; <name><surname>Blundell</surname><given-names>C.</given-names></name><article-title>Simple and Scalable Predictive Uncertainty
Estimation using Deep Ensembles</article-title>. <source>Adv. Neural
Inf. Process. Syst</source><year>2017</year>, <volume>30</volume>, <fpage>na</fpage>.</mixed-citation>
    </ref>
    <ref id="ref78">
      <mixed-citation publication-type="journal" id="cit78"><name><surname>Gal</surname><given-names>Y.</given-names></name>; <name><surname>Ghahramani</surname><given-names>Z.</given-names></name><article-title>Dropout as
a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning</article-title>. <source>Proceedings of
the International Conference on Machine Learning</source><year>2016</year>, <volume>48</volume>, <fpage>1050</fpage>–<lpage>1059</lpage>.</mixed-citation>
    </ref>
    <ref id="ref79">
      <mixed-citation publication-type="journal" id="cit79"><name><surname>Levi</surname><given-names>D.</given-names></name>; <name><surname>Gispan</surname><given-names>L.</given-names></name>; <name><surname>Giladi</surname><given-names>N.</given-names></name>; <name><surname>Fetaya</surname><given-names>E.</given-names></name><article-title>Evaluating
and calibrating
uncertainty prediction in regression tasks</article-title>. <source>Sensors</source><year>2022</year>, <volume>22</volume>, <fpage>5540</fpage><pub-id pub-id-type="doi">10.3390/s22155540</pub-id>.<pub-id pub-id-type="pmid">35898047</pub-id></mixed-citation>
    </ref>
    <ref id="ref80">
      <mixed-citation publication-type="report" id="cit80"><person-group person-group-type="allauthors"><name><surname>Zelikman</surname><given-names>E.</given-names></name>; <name><surname>Healy</surname><given-names>C.</given-names></name>; <name><surname>Zhou</surname><given-names>S.</given-names></name>; <name><surname>Avati</surname><given-names>A.</given-names></name></person-group><article-title>CRUDE: Calibrating
Regression Uncertainty Distributions
Empirically</article-title>. <source>arXiv Preprint</source>, <bold>2020</bold>, arXiv:2005.12496.</mixed-citation>
    </ref>
    <ref id="ref81">
      <mixed-citation publication-type="journal" id="cit81"><name><surname>Wang</surname><given-names>D.</given-names></name>; <name><surname>Yu</surname><given-names>J.</given-names></name>; <name><surname>Chen</surname><given-names>L.</given-names></name>; <name><surname>Li</surname><given-names>X.</given-names></name>; <name><surname>Jiang</surname><given-names>H.</given-names></name>; <name><surname>Chen</surname><given-names>K.</given-names></name>; <name><surname>Zheng</surname><given-names>M.</given-names></name>; <name><surname>Luo</surname><given-names>X.</given-names></name><article-title>A Hybrid Framework for Improving
Uncertainty Quantification in Deep Learning-Based QSAR Regression
Modeling</article-title>. <source>J. Cheminf.</source><year>2021</year>, <volume>13</volume>, <fpage>1</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1186/s13321-021-00551-x</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref82">
      <mixed-citation publication-type="journal" id="cit82"><name><surname>Guo</surname><given-names>C.</given-names></name>; <name><surname>Pleiss</surname><given-names>G.</given-names></name>; <name><surname>Sun</surname><given-names>Y.</given-names></name>; <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name><article-title>On Calibration
of Modern Neural Networks</article-title>. <source>Proceedings of the
International Conference on Machine Learning</source><year>2017</year>, <fpage>1321</fpage>–<lpage>1330</lpage>.</mixed-citation>
    </ref>
    <ref id="ref83">
      <mixed-citation publication-type="journal" id="cit83"><name><surname>Zadrozny</surname><given-names>B.</given-names></name>; <name><surname>Elkan</surname><given-names>C.</given-names></name><article-title>Transforming Classifier Scores into Accurate Multiclass Probability
Estimates</article-title>. <source>Proceedings of the International
Conference on Knowledge Discovery and Data Mining</source><year>2002</year>, <fpage>694</fpage>–<lpage>699</lpage>.</mixed-citation>
    </ref>
    <ref id="ref84">
      <mixed-citation publication-type="journal" id="cit84"><name><surname>Scalia</surname><given-names>G.</given-names></name>; <name><surname>Grambow</surname><given-names>C. A.</given-names></name>; <name><surname>Pernici</surname><given-names>B.</given-names></name>; <name><surname>Li</surname><given-names>Y.-P.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name><article-title>Evaluating
Scalable Uncertainty Estimation Methods for Deep Learning-Based Molecular
Property Prediction</article-title>. <source>J. Chem. Inf. Model.</source><year>2020</year>, <volume>60</volume>, <fpage>2697</fpage>–<lpage>2717</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00975</pub-id>.<pub-id pub-id-type="pmid">32243154</pub-id></mixed-citation>
    </ref>
    <ref id="ref85">
      <mixed-citation publication-type="weblink" id="cit85">Chemprop. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://chemprop.readthedocs.io/en/latest/">https://chemprop.readthedocs.io/en/latest/</uri> (accessed April 6 2023).</mixed-citation>
    </ref>
    <ref id="ref86">
      <mixed-citation publication-type="weblink" id="cit86">Chemprop
Workshop. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.youtube.com/watch?v=TeOl5E8Wo2M">https://www.youtube.com/watch?v=TeOl5E8Wo2M</uri> (accessed April
6 2023).</mixed-citation>
    </ref>
    <ref id="ref87">
      <mixed-citation publication-type="journal" id="cit87"><name><surname>Reuther</surname><given-names>A.</given-names></name>; <name><surname>Kepner</surname><given-names>J.</given-names></name>; <name><surname>Byun</surname><given-names>C.</given-names></name>; <name><surname>Samsi</surname><given-names>S.</given-names></name>; <name><surname>Arcand</surname><given-names>W.</given-names></name>; <name><surname>Bestor</surname><given-names>D.</given-names></name>; <name><surname>Bergeron</surname><given-names>B.</given-names></name>; <name><surname>Gadepally</surname><given-names>V.</given-names></name>; <name><surname>Houle</surname><given-names>M.</given-names></name>; <name><surname>Hubbell</surname><given-names>M.</given-names></name>; et al. <article-title>Interactive
Supercomputing
on 40,000 Cores for Machine Learning and Data Analysis</article-title>. <source>Proceedings of the IEEE High Performance Extreme Computing
Conference</source><year>2018</year>, <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
