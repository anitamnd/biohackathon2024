<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 201905//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brain Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brain Sci</journal-id>
    <journal-id journal-id-type="publisher-id">brainsci</journal-id>
    <journal-title-group>
      <journal-title>Brain Sciences</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2076-3425</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8301917</article-id>
    <article-id pub-id-type="pmid">34209754</article-id>
    <article-id pub-id-type="doi">10.3390/brainsci11070864</article-id>
    <article-id pub-id-type="publisher-id">brainsci-11-00864</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Acoustilytix™: A Web-Based Automated Ultrasonic Vocalization Scoring Platform</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8353-7668</contrib-id>
        <name>
          <surname>Ashley</surname>
          <given-names>Catherine B.</given-names>
        </name>
        <xref ref-type="aff" rid="af1-brainsci-11-00864">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Snyder</surname>
          <given-names>Ryan D.</given-names>
        </name>
        <xref ref-type="aff" rid="af1-brainsci-11-00864">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shepherd</surname>
          <given-names>James E.</given-names>
        </name>
        <xref ref-type="aff" rid="af1-brainsci-11-00864">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cervantes</surname>
          <given-names>Catalina</given-names>
        </name>
        <xref ref-type="aff" rid="af2-brainsci-11-00864">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4934-3157</contrib-id>
        <name>
          <surname>Mittal</surname>
          <given-names>Nitish</given-names>
        </name>
        <xref ref-type="aff" rid="af3-brainsci-11-00864">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fleming</surname>
          <given-names>Sheila</given-names>
        </name>
        <xref ref-type="aff" rid="af4-brainsci-11-00864">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bailey</surname>
          <given-names>Jaxon</given-names>
        </name>
        <xref ref-type="aff" rid="af2-brainsci-11-00864">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nievera</surname>
          <given-names>Maisie D.</given-names>
        </name>
        <xref ref-type="aff" rid="af2-brainsci-11-00864">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Souleimanova</surname>
          <given-names>Sharmin Islam</given-names>
        </name>
        <xref ref-type="aff" rid="af2-brainsci-11-00864">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nyaoga</surname>
          <given-names>Bill</given-names>
        </name>
        <xref ref-type="aff" rid="af2-brainsci-11-00864">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lichtenfeld</surname>
          <given-names>Lauren</given-names>
        </name>
        <xref ref-type="aff" rid="af2-brainsci-11-00864">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Alicia R.</given-names>
        </name>
        <xref ref-type="aff" rid="af2-brainsci-11-00864">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Maddox</surname>
          <given-names>W. Todd</given-names>
        </name>
        <xref ref-type="aff" rid="af5-brainsci-11-00864">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Duvauchelle</surname>
          <given-names>Christine L.</given-names>
        </name>
        <xref ref-type="aff" rid="af2-brainsci-11-00864">2</xref>
        <xref ref-type="aff" rid="af6-brainsci-11-00864">6</xref>
        <xref rid="c1-brainsci-11-00864" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Brudzynski</surname>
          <given-names>Stefan M.</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Burgdorf</surname>
          <given-names>Jeffrey</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <aff id="af1-brainsci-11-00864"><label>1</label>Cornerstone Research Group, Miamisburg, OH 45342, USA; <email>ashleycb@crgrp.com</email> (C.B.A.); <email>snyderrd@crgrp.com</email> (R.D.S.); <email>shepherdje@crgrp.com</email> (J.E.S.)</aff>
    <aff id="af2-brainsci-11-00864"><label>2</label>Division of Pharmacology and Toxicology, College of Pharmacy, The University of Texas at Austin, 2409 University Avenue, Stop A1915, Austin, TX 78712, USA; <email>catalinacervantes@austin.utexas.edu</email> (C.C.); <email>jaxon.bailey@utexas.edu</email> (J.B.); <email>maisie.nievera@utexas.edu</email> (M.D.N.); <email>sharmin.souleimanova@austin.utexas.edu</email> (S.I.S.); <email>bill.nyaoga@austin.utexas.edu</email> (B.N.); <email>lauren.lichtenfeld@austin.utexas.edu</email> (L.L.); <email>alicia.chen@austin.utexas.edu</email> (A.R.C.)</aff>
    <aff id="af3-brainsci-11-00864"><label>3</label>ZS Associates, Empire State Building, 350 Fifth Avenue, Suite 5100, New York, NY 10118, USA; <email>niimits1@gmail.com</email></aff>
    <aff id="af4-brainsci-11-00864"><label>4</label>Department of Pharmaceutical Sciences, Northeast Ohio Medical University, 4209 State Route 44, RGE, Rootstown, OH 44272, USA; <email>sfleming1@neomed.edu</email></aff>
    <aff id="af5-brainsci-11-00864"><label>5</label>Cognitive Design and Statistical Consulting, LLC, Austin, TX 78746, USA; <email>wtoddmaddox@gmail.com</email></aff>
    <aff id="af6-brainsci-11-00864"><label>6</label>Waggoner Center for Alcohol and Addiction Research, The University of Texas at Austin, 2500 Speedway, Stop A4800, Austin, TX 78712, USA</aff>
    <author-notes>
      <corresp id="c1-brainsci-11-00864"><label>*</label>Correspondence: <email>duvauchelle@mail.utexas.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>11</volume>
    <issue>7</issue>
    <elocation-id>864</elocation-id>
    <history>
      <date date-type="received">
        <day>02</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>6</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 by the authors.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Ultrasonic vocalizations (USVs) are known to reflect emotional processing, brain neurochemistry, and brain function. Collecting and processing USV data is manual, time-intensive, and costly, creating a significant bottleneck by limiting researchers’ ability to employ fully effective and nuanced experimental designs and serving as a barrier to entry for other researchers. In this report, we provide a snapshot of the current development and testing of Acoustilytix™, a web-based automated USV scoring tool. Acoustilytix implements machine learning methodology in the USV detection and classification process and is recording-environment-agnostic. We summarize the user features identified as desirable by USV researchers and how these were implemented. These include the ability to easily upload USV files, output a list of detected USVs with associated parameters in csv format, and the ability to manually verify or modify an automatically detected call. With no user intervention or tuning, Acoustilytix achieves 93% sensitivity (a measure of how accurately Acoustilytix detects true calls) and 73% precision (a measure of how accurately Acoustilytix avoids false positives) in call detection across four unique recording environments and was superior to the popular DeepSqueak algorithm (sensitivity = 88%; precision = 41%). Future work will include integration and implementation of machine-learning-based call type classification prediction that will recommend a call type to the user for each detected call. Call classification accuracy is currently in the 71–79% accuracy range, which will continue to improve as more USV files are scored by expert scorers, providing more training data for the classification model. We also describe a recently developed feature of Acoustilytix that offers a fast and effective way to train hand-scorers using automated learning principles without the need for an expert hand-scorer to be present and is built upon a foundation of learning science. The key is that trainees are given practice classifying hundreds of calls with immediate corrective feedback based on an expert’s USV classification. We showed that this approach is highly effective with inter-rater reliability (i.e., kappa statistics) between trainees and the expert ranging from 0.30–0.75 (average = 0.55) after only 1000–2000 calls of training. We conclude with a brief discussion of future improvements to the Acoustilytix platform.</p>
    </abstract>
    <kwd-group>
      <kwd>ultrasonic vocalization</kwd>
      <kwd>automated scoring</kwd>
      <kwd>dopamine</kwd>
      <kwd>addiction</kwd>
      <kwd>mental health</kwd>
      <kwd>machine learning</kwd>
      <kwd>drug discovery</kwd>
      <kwd>drug development</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-brainsci-11-00864">
    <title>1. Introduction</title>
    <p>Identifying neural mechanisms underlying mental health and addiction disorders is necessary to develop successful interventions, but moving from basic science findings to clinical trials is a lengthy and high-risk process. Although not the sole determinant, emotional processing is directly involved in many mental health disorders and addictions. Animal models allow direct measurement and manipulation of the neural circuitry that drives emotional processing, circuitry that often can only be inferred in human studies [<xref rid="B1-brainsci-11-00864" ref-type="bibr">1</xref>,<xref rid="B2-brainsci-11-00864" ref-type="bibr">2</xref>,<xref rid="B3-brainsci-11-00864" ref-type="bibr">3</xref>,<xref rid="B4-brainsci-11-00864" ref-type="bibr">4</xref>,<xref rid="B5-brainsci-11-00864" ref-type="bibr">5</xref>].</p>
    <p>Rodent ultrasonic vocalizations (USVs) are known to reflect emotional processing, brain neurochemistry, and brain function-key observations in animal model studies. USVs in the 22–28 kHz and 50–55 kHz frequency range are widely recognized forms of social and emotional expression in rats [<xref rid="B6-brainsci-11-00864" ref-type="bibr">6</xref>]. USVs in the 22–28 kHz range are provoked by external threats (e.g., danger) or unpleasant internal conditions (e.g., sickness, pain) [<xref rid="B6-brainsci-11-00864" ref-type="bibr">6</xref>,<xref rid="B7-brainsci-11-00864" ref-type="bibr">7</xref>,<xref rid="B8-brainsci-11-00864" ref-type="bibr">8</xref>]. These negative-affect associated 22–28 kHz USVs are initiated through the ascending mesolimbic cholinergic pathway, originating in the laterodorsal tegmental nucleus (LDT) and extending through the midbrain, anterior hypothalamic, preoptic, and septal regions [<xref rid="B7-brainsci-11-00864" ref-type="bibr">7</xref>,<xref rid="B9-brainsci-11-00864" ref-type="bibr">9</xref>,<xref rid="B10-brainsci-11-00864" ref-type="bibr">10</xref>]. Positive-affect associated USVs, commonly referred to as 50–55 kHz FM USVs, are elicited by activation of sites all along the mesolimbic dopaminergic pathway, including the VTA, anteromedial hypothalamus, preoptic area, and the NAc [<xref rid="B11-brainsci-11-00864" ref-type="bibr">11</xref>,<xref rid="B12-brainsci-11-00864" ref-type="bibr">12</xref>]. Although the frequency ranges and spectral dynamics of the emitted USVs differ, similar mechanisms also exist in mice [<xref rid="B13-brainsci-11-00864" ref-type="bibr">13</xref>,<xref rid="B14-brainsci-11-00864" ref-type="bibr">14</xref>]. Animal models using both mice and rats are utilized in a broad number of research areas, including mental health and addiction [<xref rid="B15-brainsci-11-00864" ref-type="bibr">15</xref>].</p>
    <p>Analysis of rodent USVs is a foundational technique that underlies a diverse cross-section of mental health and addiction research [<xref rid="B16-brainsci-11-00864" ref-type="bibr">16</xref>,<xref rid="B17-brainsci-11-00864" ref-type="bibr">17</xref>,<xref rid="B18-brainsci-11-00864" ref-type="bibr">18</xref>,<xref rid="B19-brainsci-11-00864" ref-type="bibr">19</xref>,<xref rid="B20-brainsci-11-00864" ref-type="bibr">20</xref>,<xref rid="B21-brainsci-11-00864" ref-type="bibr">21</xref>,<xref rid="B22-brainsci-11-00864" ref-type="bibr">22</xref>,<xref rid="B23-brainsci-11-00864" ref-type="bibr">23</xref>,<xref rid="B24-brainsci-11-00864" ref-type="bibr">24</xref>,<xref rid="B25-brainsci-11-00864" ref-type="bibr">25</xref>,<xref rid="B26-brainsci-11-00864" ref-type="bibr">26</xref>,<xref rid="B27-brainsci-11-00864" ref-type="bibr">27</xref>,<xref rid="B28-brainsci-11-00864" ref-type="bibr">28</xref>]. However, this technique is currently underutilized because USV tabulation is time-intensive, manual, and costly—each two minutes of recorded rodent vocalizations can require up to an hour to hand-score depending upon the number of calls present. Hand-scoring requires extensive prior training before a scorer reaches the necessary level of proficiency, and all too often, this training is sub-optimal, reliability is left unchecked, and large individual differences in hand-scoring results. Notably, even with proficient hand-scorers, the intensive manual nature of the analysis limits researchers’ ability to employ the effective and nuanced experimental designs (e.g., multi-hour and/or multi-session experiments) needed to fully explore mental health and addiction related topics. In addition, with the increased predictive success of acoustic parameters of USVs over discrete call classification, tools are needed to automate this process [<xref rid="B15-brainsci-11-00864" ref-type="bibr">15</xref>,<xref rid="B29-brainsci-11-00864" ref-type="bibr">29</xref>,<xref rid="B30-brainsci-11-00864" ref-type="bibr">30</xref>,<xref rid="B31-brainsci-11-00864" ref-type="bibr">31</xref>,<xref rid="B32-brainsci-11-00864" ref-type="bibr">32</xref>]. This represents a bottleneck for current USV researchers and a barrier to entry for other researchers as well.</p>
    <p>Recognizing that automation can alleviate the USV analysis bottleneck, several research groups have developed automated USV detection and classification algorithms, including WAAVES [<xref rid="B32-brainsci-11-00864" ref-type="bibr">32</xref>], XBAT [<xref rid="B33-brainsci-11-00864" ref-type="bibr">33</xref>], DeepSqueak [<xref rid="B34-brainsci-11-00864" ref-type="bibr">34</xref>], and MUPET [<xref rid="B35-brainsci-11-00864" ref-type="bibr">35</xref>]. WAAVES has enabled Duvauchelle and colleagues to rapidly examine emotional profiles in rats across a broad range of experimental settings in a fraction of the time required with manual analyses [<xref rid="B30-brainsci-11-00864" ref-type="bibr">30</xref>,<xref rid="B32-brainsci-11-00864" ref-type="bibr">32</xref>,<xref rid="B36-brainsci-11-00864" ref-type="bibr">36</xref>,<xref rid="B37-brainsci-11-00864" ref-type="bibr">37</xref>,<xref rid="B38-brainsci-11-00864" ref-type="bibr">38</xref>,<xref rid="B39-brainsci-11-00864" ref-type="bibr">39</xref>]. However, the main weakness of all of these automated USV scoring algorithms is that each is customized to specific environmental recording conditions and types of animals (e.g., rats vs. mice) and requires end user trial-and-error-driven thresholding and/or extensive training data to ensure accurate results. These algorithms cannot be broadly applied to data collected in other sound environments without tailoring, thus, impeding research progress.</p>
    <p>In this manuscript, we present a snapshot of the development and testing of a new web-based automated USV scoring tool called Acoustilytix (see <uri xlink:href="http://acoustilytix.com/">http://acoustilytix.com/</uri>; 30 May 2021). Acoustilytix is environment-agnostic and implements machine learning methodology in the USV detection and classification process (We are in the process of applying this approach in mice). The ongoing development of Acoustilytix is funded through the National Institutes of Mental Health Small Business Technology Transfer (STTR) program, whose mission is to support commercialization of products developed through a partnership between research institutions and industry (It is for this reason that many algorithmic details cannot be presented). The ongoing development of Acoustilytix derives from a unique collaboration between the Cornerstone Research Group (CRG), a high-technology research and development company near Dayton Ohio, and Dr. Christine Duvauchelle, an expert in USV animal research in the College of Pharmacy at the University of Texas (UT). The CRG and UT teams were complemented with an international consortium of USV researchers who generously provided their expertise and input, as well as their USV data to be used during software development. The consortium included a dozen USV researchers from around the world who were interested in helping us develop the most effective automated USV scoring solution.</p>
    <p>The remainder of the manuscript is organized into two major sections.</p>
    <p>The first focuses on the current state of development of the Acoustilytix platform and the rigorous quantitative testing and evaluation of the platform. The current version of Acoustilytix is quite effective at automated call detection, being environment-agnostic and offering a quantitative measure of call detection uncertainty (see <xref ref-type="sec" rid="sec3-brainsci-11-00864">Section 3</xref> below). Acoustilytix is less effective when it comes to automated call classification with this still being a work in progress. In this first section, we briefly summarize some of the user features suggested by our consortium members that have been implemented into Acoustilytix, such as rapid automated USV detection, call parsing, detection confidence, call classification, and parameterization from rats in any USV recording environment. For ease of exposition, the main body of this section will offer a high-level summary, with many of the algorithmic and architectural details in <xref ref-type="app" rid="app1-brainsci-11-00864">Appendix A</xref>, <xref ref-type="app" rid="app2-brainsci-11-00864">Appendix B</xref> and <xref ref-type="app" rid="app3-brainsci-11-00864">Appendix C</xref>. Next a detailed analysis of the platform output and a comparison of its effectiveness at automated call detection with the popular DeepSqueak algorithm is presented. Briefly, with no user intervention or tuning, Acoustilytix achieves 93% sensitivity (a measure of how accurately Acoustilytix detects true calls) and 73% precision (a measure of how accurately Acoustilytix avoids false positives) in call detection across four unique recording environments and was superior to the popular DeepSqueak algorithm (sensitivity = 88%; precision = 41%). Acoustilytix is also much less prone to detecting false positives than DeepSqueak. We conclude with a discussion of automated call classification and our progress to date. To anticipate, automated call classification has been challenging. We had to balance the desire to classify multiple distinct call types with the need for large numbers of hand-scored calls for model testing. To date, we have developed a machine learning model that automates the classification of a five-call scheme suggested by one of our consortium members. Call classification accuracy is in the 71–79% range (well above chance) and will continue to improve as more USV files are scored, which will provide more training data for improving model fit. Perhaps more importantly, discussions with our consortium members suggest that they are less interested in a product that is well calibrated to a single call classification scheme but rather would like a product that allows them to upload a large number of hand-scored calls from a classification scheme of their choice that will then be used to train a machine learning algorithm for subsequent automated scoring. In the General Discussion section, we briefly summarize what a product offering like this might look like.</p>
    <p>The second major section focuses on a recently developed feature of Acoustilytix that offers a fast and effective automated method for training hand-scorers without the need for an expert hand-scorer to be present. Building upon the fact that some level of hand-scoring competency is needed in any USV lab, even with Acoustilytix, we describe a new feature that leverages learning science principles to optimize hand-scoring training [<xref rid="B40-brainsci-11-00864" ref-type="bibr">40</xref>,<xref rid="B41-brainsci-11-00864" ref-type="bibr">41</xref>]. In short, hand-scorers in training can select the “training” feature in Acoustilytix. They are presented with a list of training files that contain calls that have been hand-scored by an expert in the laboratory whose classification serves as the teaching signal. The trainee can select a file and begin by selecting prototypical calls from a menu that they can visualize using a spectrogram or listen to at a reduced rate. Once the trainee is ready to begin the formal call-by-call training process, they are presented, one by one, with a large number of calls (e.g., 1000), which they may analyze visually using a spectrogram or audibly by listening to the audio clip at a reduced rate. For each call, the trainee selects a call classification and is given immediate corrective feedback (based on the expert scorer’s classification). If in error, the trainee is allowed to examine any prototypical calls again and/or make another classification judgement. This process continues until the trainee provides the correct classification for each call and until all calls in the training set have been correctly classified. Immediate corrective feedback of this sort is known to speed learning and enhance proficiency [<xref rid="B40-brainsci-11-00864" ref-type="bibr">40</xref>,<xref rid="B41-brainsci-11-00864" ref-type="bibr">41</xref>]. This increases the likelihood that all hand-scorers in the laboratory mimic the scoring behavior of the expert, thus increasing consistency and reliability of USV hand-scoring. We present the results from a reliability study that verifies the effectiveness of the Acoustilytix hand-scorer training approach.</p>
    <p>Finally, we conclude the manuscript with a discussion of future Acoustilytix software development plans and how Acoustilytix might be incorporated into and affect the USV research landscape.</p>
  </sec>
  <sec id="sec2-brainsci-11-00864">
    <title>2. Acoustilytix Program Structure</title>
    <p>In this section, we provide a high-level description of the Acoustilytix program structure. Acoustilytix is a web-based platform that allows USV researchers to easily upload wav files; to trigger automated USV call detection and classification; and to output call classifications, counts, and a number of acoustic characteristics of each call to a csv file for the users’ subsequent analysis.</p>
    <sec id="sec2dot1-brainsci-11-00864">
      <title>2.1. Acoustilytix Web-Based User Interface and Features</title>
      <p>An overriding aim of the Acoustilytix web-based interface is to offer a user-friendly experience for USV researchers. Based on extensive discussions with USV consortium members, the website includes separate pages or drop-down menu functions for wav file uploading, sorting, and tracking of wav files; selecting a USV classification scheme (e.g., the 14 call types identified in Wright, et al., 2010) [<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>]; initiating the automatic detection; manual verification of automated call detection and classification; and download of tabulated results to a csv file. We briefly summarize each of these functions below.</p>
      <sec id="sec2dot1dot1-brainsci-11-00864">
        <title>2.1.1. User Interface</title>
        <p>Once a research group gains access to Acoustilytix, the principal investigator (PI) in a laboratory creates a username and password, is assigned their own research group, and receives “administrative” privileges. Once logged in to the Acoustilytix platform, a user can select the “upload file” feature and can either drag and drop or use an upload button to select a wav file for upload to the research group. All research group members can access and score files that have been uploaded by any member of the group. Files can be sorted in any number of ways, for example, alphabetically by file name, by file uploader, or by file scorer. In the current version of the platform, users can select one of the default USV classification schemes from a drop-down menu. (As detailed in the General Discussion section, we are currently building the capability of allowing a researcher to create their own call classification scheme. A large number of hand-scored calls from this scheme will be uploaded to Acoustilytix and will be used to train an automated call classification model.) Once a call classification scheme is selected, it will be applied for all subsequent files until a new call classification scheme is selected. A manual verification process has been implemented that allows researchers to review the automated call classification for any calls they choose. Once a wav file is scored by Acoustilytix, it is available for download as a csv. The file will include the start and stop time, user selected call type classification, and a large number of acoustic parameters associated with each call.</p>
      </sec>
      <sec id="sec2dot1dot2-brainsci-11-00864">
        <title>2.1.2. Acoustilytix Features</title>
        <p>The two primary features of Acoustilytix are automated call detection and call classification. Because different laboratories or individuals within a laboratory might implement a different call classification scheme, users can select unique call classification schemas to be used during the automated call classification and/or manual verification process.</p>
        <p>Acoustilytix automatically begins parsing the audio file and detecting calls upon file upload. Automatic call detection requires three distinct processes—initial detection and file parsing, call isolation and detection confidence, and call parameterization.</p>
        <p><italic>Initial Detection and File Parsing</italic>: Two of the primary guiding tenets of the Acoustilytix automated detection algorithm are: (1) most of the recording time and frequency bins can be attributed to background noise and (2) USVs are connected in time, frequency, acoustic power, or some combination of these. Thus, Acoustilytix begins by estimating an overall measure of the background noise and identifies likely calls as those segments in the file with connected time, frequency, acoustic power, or some combination that is distinct from background noise. Possible calls are expanded by several milliseconds to capture both ends of any potential calls. These expanded segments are organized into a list where any overlapping segments are coalesced together. These segments with potential calls are then passed into the call isolation module. A detailed explanation of this process is included in <xref ref-type="app" rid="app1-brainsci-11-00864">Appendix A</xref>.</p>
        <p><italic>Call Isolation and Detection Confidence</italic>: To isolate calls and differentiate them from background and noise, Acoustilytix uses a combinatorial approach applying several filters and algorithms. These are combined in various ways to differentiate the call from the background and to assign a detection confidence value (percent certainty). The details can be found in <xref ref-type="app" rid="app2-brainsci-11-00864">Appendix B</xref>.</p>
        <p><italic>Parameterization</italic>: Once the calls have been isolated, descriptive parameters can be calculated; the current version of Acoustilytix calculates over 200 parameters for each call. These include parameters such as maximum, minimum, and median frequency; maximum acoustic power (and frequency at which this occurs); and median acoustic power along the call maximal ridge. A review of commonly referenced call types for both rat and mice reveals that temporal variations, i.e., slopes, jumps, etc., appear indicative of many call types [<xref rid="B14-brainsci-11-00864" ref-type="bibr">14</xref>,<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>]. Using the ridges from the isolation step as an initial guide, many temporal-varying parameters are also calculated, including max, min, median, and standard deviation of slopes for not only the whole call but also the beginning, middle, and end of the call. Jumps and gaps are also tabulated, as well as a few measures of how quickly slopes change over time (slope variability), which relates to “trill” type calls. The current iteration of the software outputs a sub-set of these parameters in the csv file for researchers. We fully expect the use of these parameters in USV research will become more common [<xref rid="B29-brainsci-11-00864" ref-type="bibr">29</xref>,<xref rid="B30-brainsci-11-00864" ref-type="bibr">30</xref>,<xref rid="B43-brainsci-11-00864" ref-type="bibr">43</xref>].</p>
        <p>Manual verification of automated call detection is not a mandatory step in the Acoustilytix process. Even so, and especially since software development is ongoing, we urge researchers to incorporate manual verification into their workflow. During software development, the CRG and UT teams worked iteratively to ensure that the manual verification process was easy to implement and streamlined. The result of this development effort is that the platform currently offers three data formats for exploring and interacting with the data in each file-card, tile, and table. Briefly, the “card” format presents a single segment/call combination with the ability for users to play back the audio segment while a line moves through a responsive spectrogram. From this screen, users can modify the call bounds, mark an auto-detected call as a false positive, or provide a USV classification type. The “tile” format provides an infinite scrolling list of the segment and call combinations for a broader overview. The call parameters and any user-selected call classification are shown on the tile. Tiles can be filtered to include all calls or only calls above a threshold detection confidence (i.e., percent certainty) for detection. Finally, the “table” format provides parameterization details for each of the calls, as well as any user added comments or classifications. These data are able to be downloaded as a csv file for each scoring session. Scoring data are organized in “scoring sessions” with data linked to the user, wav file, and call classification scheme. Progress for each scoring session is tracked, and users can easily resume scoring where they left off. In addition, each file can be scored separately by multiple users or using multiple classification schemes. Multiple users can score the same file, and the results can be easily compared to assess inter-rater reliability.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec3-brainsci-11-00864">
    <title>3. Acoustilytix Validation Results</title>
    <sec id="sec3dot1-brainsci-11-00864">
      <title>3.1. Acoustilytix: Detection Statistics</title>
      <p>To generate detection statistics, the output from Acoustilytix was compared to compiled data from hand-scored USV files and to the output from DeepSqueak. The time stamps were matched between datasets, and then, calls from the Acoustilytix or DeepSqueak dataset were given one of the following three designations:<list list-type="bullet"><list-item><p>Detected: These are defined as calls detected by hand-scorers and by Acoustilytix (or DeepSqueak).</p></list-item><list-item><p>Missed: These are defined as calls detected by hand-scorers but not detected by Acoustilytix (or DeepSqueak).</p></list-item><list-item><p>False Positive: These are defined as calls not detected by hand-scorers that were detected by Acoustilytix (or DeepSqueak).</p></list-item></list></p>
      <p>Two metrics were used to quantify the success of Acoustilytix (or DeepSqueak) at matching the hand-scored data.</p>
      <list list-type="bullet">
        <list-item>
          <p>Sensitivity = Detected/(Detected + Missed): This is a measure of how many of the hand-scored calls are correctly detected by Acoustilytix (or DeepSqueak).</p>
        </list-item>
        <list-item>
          <p>Precision = Detected/(Detected + False Positive): This is a measure of how many hand-scored calls Acoustilytix (or DeepSqueak) detects but also the number detected that are not detected by the hand-scorers.</p>
        </list-item>
      </list>
      <p>We examined the sensitivity and precision from different experimental conditions across different recording environments. These results are summarized in <xref rid="brainsci-11-00864-t001" ref-type="table">Table 1</xref>.</p>
      <p>For Acoustilytix, the sensitivity (accurate detection of calls compared to hand-scored) was above 90% in all cases, with an average of 92.7%. This means that Acoustilytix correctly detected over 90% of the calls noted via hand-scoring in a variety of experimental and recording conditions without any user intervention or tuning. For DeepSqueak, the sensitivity was 87.7% on average for 18,770 USV calls. Compared with Acoustilytix, DeepSqueak’s sensitivity had a <italic>p</italic>-value less than 0.00001, which showed a significant difference.</p>
      <p>Precision is a critical metric for automated detection success, as the number of false positive USVs should be minimized. As the results in <xref rid="brainsci-11-00864-t001" ref-type="table">Table 1</xref> suggest, precision was lower than sensitivity in all cases, ranging from 59.5–83.6% with a weighted average of 73.2%. DeepSqueak had an average precision of 41.0% with a <italic>p</italic>-value less than 0.00001 when compared with Acoustilytix’s 73.2% average precision.</p>
      <p>In developing and evaluating the USV detection algorithm, we realized that some metric of call detection certainty (or confidence) could be of value to USV researchers. We developed such an algorithm, and these values are part of the csv output file. The details of the algorithm are being evaluated as potential intellectual property (IP) and, thus, cannot be presented in detail here. At a high level, call detection certainty is a function of acoustic power of the parsed signal relative to the background power, with greater acoustic power for the parsed signal relative to the background being associated with greater call detection certainty.</p>
      <p>In the next iteration of Acoustilytix, we plan to incorporate a new feature into the detection workflow that will allow the USV researcher to set a threshold on the call detection certainty so that all calls below the research-selected threshold will be automatically made available for manual verification.</p>
    </sec>
    <sec id="sec3dot2-brainsci-11-00864">
      <title>3.2. Acoustilytix: Call Classification Statistics</title>
      <p>The goal of this section is to report the progress toward developing an automated USV call classification machine learning algorithm. Our original goal was to develop an automated USV call classification machine learning algorithm that could accurately classify 20 or more unique call types that are used in the literature. These might include the 15 call types identified by Wright et al. [<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>] but also more complex waveforms, including dysphonation and frequency shifts, which may have particular relevance to translational research [<xref rid="B44-brainsci-11-00864" ref-type="bibr">44</xref>,<xref rid="B45-brainsci-11-00864" ref-type="bibr">45</xref>]. After some initial attempts, this was not deemed feasible in a reasonable amount of time. In addition, and based on discussions with our USV consortium members, we were informed that researchers were more interested in having the ability to upload their own hand-scored USV files that could be used to train an automated scoring algorithm that they could use exclusively.</p>
      <p>The first step in achieving this aim was to identify which of many machine learning algorithms to implement. Although a number of machine learning algorithms were explored, we settled upon a random forest algorithm whose parameters were estimated from hand-scored USV call classification data. For the initial validation test, we wanted to balance the need to classify a broad range of calls with the need to have the statistical power to train and test the model. We settled on a five-call classification scheme that has been used by one of our consortium members and represents a composite of an existing call classification scheme by Wright et al. [<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>]. The mapping from the Wright categories to the five-call composite is presented in <xref rid="brainsci-11-00864-t002" ref-type="table">Table 2</xref>. Representative call spectrograms are displayed in <xref ref-type="fig" rid="brainsci-11-00864-f001">Figure 1</xref>. It is important to note that we are not arguing that this is the best call classification scheme to be used in translational research. Rather, the goal is to verify that a random forest algorithm can be used to build an automated hand-scoring tool with reasonable accuracy.</p>
      <sec sec-type="results">
        <title>Results</title>
        <p>A machine learning algorithm was trained using a supervised random forest classification model where Dr. Duvauchelle’s most experienced rater’s call classifications were used as the supervised teaching signal for the model. The dataset consisted of 931 rat calls. <xref rid="brainsci-11-00864-t003" ref-type="table">Table 3</xref> shows the number of each call type in the dataset.</p>
        <p>The experimental dataset was split into a training dataset consisting of 70% of the calls and a test dataset with the remaining 30% to validate the model’s performance and to prevent overfitting. The model features consisted of the parameters that were calculated by the call detection algorithm discussed in the Parameterization section of this paper. Feature selection methods were employed to down-select the number of predictors in the model and to prevent overfitting. A grid-based approach was used to tune the random forest model parameters and to identify the model that yielded the highest test data accuracy.</p>
        <p>We began the evaluation process by tallying the number of calls of each type that fall in each of the following four categories:<list list-type="bullet"><list-item><p>True Positive (TP): The hand-scorer selected call type “A”, and the classifier selected call type “A”. The hand-scorer and classifier agreed that call type “A” was present.</p></list-item><list-item><p>True Negative (TN): The hand-scorer did not select call type “A”, and the classifier also did not select call type “A.” The hand-scorer and classifier agreed that call type “A” was absent.</p></list-item><list-item><p>False Positive (FP): The hand-scorer did not select call type “A”, but the classifier selected call type “A.” The hand-scorer and classifier disagreed and the hand-scorer’s call type selection was considered the true call type.</p></list-item><list-item><p>False Negative (FN): The hand-scorer selected call type “A”, but the classifier did not select call type “A.” The hand-scorer and classifier disagreed and the hand-scorer’s call type selection was considered the true call type.</p></list-item></list></p>
        <p>These were then used to compute the five model performance metrics displayed in <xref rid="brainsci-11-00864-t004" ref-type="table">Table 4</xref>: These include precision, recall, F1-score, support, and accuracy, which are calculated from the TP, TN, FP, and FN values as follows. The F1-Score is the harmonic mean of precision and recall and gives a better measure of the incorrectly classified cases than the Accuracy metric.
<list list-type="bullet"><list-item><p>Precision = TP/(TP + FP)</p></list-item><list-item><p>Recall = TP/(TP + FN)</p></list-item><list-item><p>F1-score = 2 * (Precision * Recall)/(Precision + Recall)</p></list-item><list-item><p>Support = count</p></list-item><list-item><p>Accuracy = (TP + TN)/Total Population</p></list-item></list></p>
        <table-wrap id="brainsci-11-00864-t004" orientation="portrait" position="float">
          <object-id pub-id-type="pii">brainsci-11-00864-t004_Table 4</object-id>
          <label>Table 4</label>
          <caption>
            <p>Evaluation of the five-call composite classification model performance on the test dataset.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Five-Call Type</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th>
                <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Support</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed Frequency 50</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.77</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.69</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.73</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49 </td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated 50</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.73</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.80</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12 </td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated with Trill 50</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.81</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.62</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.70</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42 </td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Long 22-kHz</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.89</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53 </td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short 22-kHz</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.82</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.75</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.78</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">24 </td>
              </tr>
              <tr>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                  <bold>Overall Accuracy/Support (N-size)</bold>
                </td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                  <bold>0.79</bold>
                </td>
                <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                  <bold>280</bold>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>Precision, recall, F1-score, and accuracy fall between 0 and 1 where values close to 1 indicate strong model performance. <xref rid="brainsci-11-00864-t004" ref-type="table">Table 4</xref> shows good performance on all of the five call types across all performance metrics with an average accuracy of 79%, which is well above chance level (20%).</p>
        <p>For comparison, <xref rid="brainsci-11-00864-t005" ref-type="table">Table 5</xref> shows how well the model performed on the training data.</p>
        <p>Again, model performance was quite good on all performance metrics with an accuracy for the training dataset of 87%. We used a kappa statistic to compare the model to the expert hand-scorer’s full data set. Kappa statistics range from 0 to 1 with higher values denoting greater agreement. The model yielded a kappa = 0.79, suggesting strong agreement between the model and the expert hand-scorer. As new data are scored, the model fit will continue to improve and further optimize (We also fit the model with over 30,000 hand-scored calls from a team of six hand-scorers. Seventy percent of the data were used to train the model and 30% to test the model. Test data accuracy was 71% as compared to the 79% with the single expert hand-scorer, and training accuracy was 88% as compared to 87% with the single expert hand-scorer.).</p>
      </sec>
    </sec>
    <sec id="sec3dot3-brainsci-11-00864">
      <title>3.3. Acoustilytix Validation Results General Summary and Future Directions</title>
      <p>In this section we evaluated the effectiveness of Acoustilytix as an automated USV call detection tool and compared it with the currently popular DeepSqueak algorithm. We also examined the ability of a random forest machine learning algorithm to serve as the foundation for automated call classification. We gained a number of important insights from this evaluation.</p>
      <p>First, our automated USV detection algorithm was not significantly impacted by background noise. Background noise can obfuscate USVs from both hand-scorers and existing automated call classification solutions. Acoustilytix’s proprietary detection algorithm maintained excellent performance even in the presence of background noise. In fact, we showed that USVs missed by hand-scorers during initial scoring were subsequently identified by Acoustilytix and confirmed as true calls by hand-scorers. With no user intervention or tuning, Acoustilytix achieved 93% sensitivity (a measure of how accurately Acoustilytix detects true calls) and 73% precision (a measure of how accurately Acoustilytix avoids false positives) in call detection accuracy across four unique recording environments. Critically, Acoustilytix also generated a call detection certainty metric that reflected the confidence in the model call detection decision. In future iterations of Acoustilytix, we plan to incorporate this into the optional manual call verification process so that USV researchers have the option to manually verify any calls with a detection certainty below some threshold selected by the USV researcher.</p>
      <p>Second, Acoustilytix had significantly better sensitivity and precision than DeepSqueak (sensitivity = 88%; precision = 41%) across the four studies analyzed, which included four different environments. DeepSqueak had a high false positive rate across the four studies, which posed a challenge to hand-scoring.</p>
      <p>Third, our automated USV call classification algorithm performed quite well on the five-call composite outlined in <xref rid="brainsci-11-00864-t002" ref-type="table">Table 2</xref>. While there is room for continuous improvement, and we have a plan for implementing this (see future directions), Acoustilytix accurately classified 79% of the hand-scored data from nearly 1000 calls classified by an expert into the five-call types. This is well above chance (20%). As additional data are uploaded to the Acoustilytix platform, and as hand-scorers modify or endorse the Acoustilytix classification, model parameters will continue to be optimized in the interest of increased accuracy.</p>
      <p>Fourth, and based on conversations with our USV consortium members, it is clear that they prefer an automated call classification platform that allows them to upload a large number of hand-scored USV files that could be used to train an automated scoring algorithm that they could use exclusively. Our progress with the random forest algorithm displayed in <xref rid="brainsci-11-00864-t004" ref-type="table">Table 4</xref> and <xref rid="brainsci-11-00864-t005" ref-type="table">Table 5</xref> suggest that this feature can be effectively implemented in the next iteration of Acoustilytix.</p>
      <p>Fifth, and perhaps most exciting, is the fact that over 200 acoustic parameters associated with each detected USV are being computed and can be outputted in the csv files. As many research groups have shown, including our own [<xref rid="B14-brainsci-11-00864" ref-type="bibr">14</xref>,<xref rid="B30-brainsci-11-00864" ref-type="bibr">30</xref>,<xref rid="B46-brainsci-11-00864" ref-type="bibr">46</xref>,<xref rid="B47-brainsci-11-00864" ref-type="bibr">47</xref>,<xref rid="B48-brainsci-11-00864" ref-type="bibr">48</xref>,<xref rid="B49-brainsci-11-00864" ref-type="bibr">49</xref>,<xref rid="B50-brainsci-11-00864" ref-type="bibr">50</xref>,<xref rid="B51-brainsci-11-00864" ref-type="bibr">51</xref>], the USV acoustic parameter values are highly predictive of a number of addiction and other health related outcomes. It is very likely that the ease of acquisition of these parameters when using Acoustilytix will quicken the pace of research focused on acoustic parameters.</p>
    </sec>
  </sec>
  <sec id="sec4-brainsci-11-00864">
    <title>4. Acoustilytix Hand-Scoring Training Feature</title>
    <p>As outlined above, the need for highly proficient, fast, and consistent hand-scoring is a clear bottleneck for the use of USVs in research. Few USV researchers are satisfied with their current hand-scoring training procedures, and most report their desire for an automated approach to hand-scorer training.</p>
    <p>To accurately hand-score a USV, the scorer must detect the call as distinct from noise in a spectrogram, then, must visually inspect the USV and listen to it to accurately classify the call. This mental representation must then be compared with either visual and auditory mental representations of prototypical calls of each type or actual visual and auditory spectrograms of prototypical calls from each type. The scorer must then assign the call to a USV category.</p>
    <p>The best way to train this type of complex visual/auditory matching task is to have a trainee inspect a potential call, classify it, then receive immediate feedback from an expert hand-scorer on the correctness of their classification. This process should be repeated several hundred times to build expertise. Unfortunately, this is time-consuming and costly and, thus, rarely occurs. More commonly, a hand-scorer in training is shown examples along with verbal descriptions of each call type. They are then asked to score a large number of calls (e.g., several hundred), and they only receive corrective feedback after the fact. This is suboptimal, as the best time to strengthen or weaken the neural connection between the stimulus (call to classify) and response (classification) is to provide feedback immediately following the classification [<xref rid="B40-brainsci-11-00864" ref-type="bibr">40</xref>,<xref rid="B41-brainsci-11-00864" ref-type="bibr">41</xref>,<xref rid="B52-brainsci-11-00864" ref-type="bibr">52</xref>].</p>
    <p>We recently developed a new function in Acoustilytix to automate the hand-scoring training process that builds upon what is known about learning to optimize the speed, accuracy, and consistency of the resulting hand-scoring capability. This function includes an initial USV call familiarization phase in which a hand-scorer in training can view and listen to spectrograms of numerous calls from any selected call category. Learners can do this at their own pace for as long as they want. This familiarization tool remains available to hand-scorers at all points in time. Once a hand-scorer is ready to practice hand-scoring, they retrieve a USV file from the Acoustilytix platform that has been “tagged” as a training file. This training file has been hand-scored by an expert from the USV laboratory. The expert simply generates a csv file that includes the correct response, uploads it to the platform, and tags it as a training file. Once the hand-scorer in training begins, the first call is presented. The hand-scorer can look at the call and use all of the tools outlined above (e.g., zoom), as well as listen to the call. “Flashcards” of prototypical calls are available for the hand-scorer to view. Once they are ready to classify, they select a call category from a drop-down menu, and they receive immediate feedback from the platform. If their classification is correct, they go on to the next call. If their classification is incorrect, they can either select a new call category, or they can view the prototypical call flashcards and can compare them with the to-be-classified call until they are ready to generate their own classification. This process continues until they classify all calls in the training file. Once complete, an accuracy rate (based only on the first call classification) is displayed. Multiple training files can be scored until the hand-scorer is proficient.</p>
    <sec>
      <title>Acoustilytix: Hand-Scoring Trainer Inter-Rater Reliability</title>
      <p>A group of five individuals were given traditional (unstructured training), with each subsequently classifying several thousand calls into the five call types described above over the course of several months (ranging from 6000–31,000 calls classified by each hand-scorer). At a later date, all of these individuals were asked to individually score a single test file with approximately 1000 calls that had been hand-scored by an expert (test following unstructured training). They were given one week to complete this task.</p>
      <p>We used a kappa statistic to compare each hand-scorer with the expert hand-scorer. An example of the kappa statistic calculation can be found in <xref ref-type="app" rid="app3-brainsci-11-00864">Appendix C</xref>. The kappa statistics for the initial group of five individuals in the test following unstructured training is presented in the top portion of <xref rid="brainsci-11-00864-t006" ref-type="table">Table 6</xref>. Notice that the kappa values are moderate following unstructured training ranging from 0.30–0.55 with a mean of 0.42.</p>
      <p>Next, we asked two individuals with no prior USV hand-scoring experience to complete Acoustilytix training with approximately 1000 calls, and they were given the same Acoustilytix test. The kappa statistics for the two Acoustilytix-only trained individuals are presented at the bottom of <xref rid="brainsci-11-00864-t006" ref-type="table">Table 6</xref> (Hand-scorer 6–7). The kappa values were 0.42 and 0.47 with an average of 0.445. This is comparable to the average kappa value for the five extensively trained hand-scorers and, in fact, is slightly higher (0.445 vs. 0.42).</p>
      <p>Given the success of Acoustilytix training with the two novice hand-scorers, we asked the five experienced hand-scorers to complete Acoustilytix training with approximately 2000 calls, and they were given the same Acoustilytix test. The kappa statistics for the five Acoustilytix-only trained individuals are presented in the right-most column of <xref rid="brainsci-11-00864-t006" ref-type="table">Table 6</xref>. The kappa values ranged from 0.30–0.75 with an average kappa value of 0.60. Four out of five experienced hand-scorers had significantly better kappa statistics after the Acoustilytix training.</p>
      <p>These findings suggest that hand-scoring training in which the learner is given immediate call-by-call feedback, and where the teaching signal is based on the call classification from an expert hand-scorer, leads to more accurate and consistent hand-scoring. Across all seven hand-scorers, and following Acoustilytix training, the average kappa value was 0.55. Further training would likely increase this value. USV researchers could set a minimum threshold on this value if they like and deem hand-scorers “qualified” only when they exceed that threshold. Refresher training could also be incorporated to ensure consistency and accuracy of hand-scoring.</p>
    </sec>
  </sec>
  <sec id="sec5-brainsci-11-00864">
    <title>5. General Discussion and Future Directions</title>
    <p>Ultrasonic vocalizations in rodents provide a window into emotional and neural processing. These processes are directly involved in many mental health disorders and addictions. Despite the potential of USV analysis in mental health and addiction research, one huge bottleneck exists—namely, the time-intensive, manual and costly nature of USV hand-scoring—specifically, call detection and call classification.</p>
    <p>In this report, we provide a snapshot of our current development and testing of Acoustilytix, a web-based automated USV scoring tool that is currently environment-agnostic and implements machine learning methodology in the USV detection and classification process.</p>
    <p>Based on discussions with our USV consortium, we identified and implemented a number of features to make the platform easy to use. These include the ability to easily upload USV files, output csv files, and the ability to manually verify or modify an automatically detected call. The initial test of the call detection algorithm in Acoustilytix was promising. With no user intervention or tuning, Acoustilytix achieved 93% sensitivity (a measure of how accurately Acoustilytix detects true calls) and 73% precision (a measure of how accurately Acoustilytix avoids false positives) in call detection accuracy across four unique recording environments and was superior to the popular DeepSqueak algorithm (sensitivity = 88%; precision = 41%). Long calls and short calls are scored separately in DeepSqueak because the functions for each algorithm work differently. While analyzing and comparing the long and short call files, we noticed that DeepSqueak can detect long and short calls that overlap either partially or fully in time. This can create confusion regarding the spectral dynamics of each USV and make analysis more challenging. Because Acoustilytix’s call detection algorithm does not require separate analyses of the USV recording files, this problem is avoided. The ability to accurately detect USVs without any user intervention or tuning addresses a major barrier to broad adoption of USV models in addiction and mental health research. Once we incorporate the quantitative metric of call detection certainty into the user facing Acoustilytix dashboard, researchers will be able to set a threshold for manual verification that is fast, accurate, and flexible for users.</p>
    <p>We also examined USV call classification accuracy in a five-call composite classification scheme derived from the Wright et al. [<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>] call categories. Automated call classification accuracy using a random forest machine learning algorithm was 79% for the five-call composite. These accuracy rates are well above chance, but as additional data are uploaded to the Acoustilytix platform, and as hand-scorers modify or endorse the Acoustilytix classification, model parameters will continue to be optimized in the interest of increased USV call classification accuracy. The validation of the random forest model sets the stage for the next iteration of the automated USV call classification algorithm. As requested by our USV consortium members, we plan to incorporate a new feature that allows researchers to upload a large number of hand-scored USV files that could be used to train an automated scoring algorithm that they could use exclusively.</p>
    <p>Perhaps most exciting is the fact that over 200 acoustic parameters associated with each detected USV are being computed and can be outputted in the csv files. As our research group and others have shown [<xref rid="B31-brainsci-11-00864" ref-type="bibr">31</xref>,<xref rid="B37-brainsci-11-00864" ref-type="bibr">37</xref>,<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>,<xref rid="B53-brainsci-11-00864" ref-type="bibr">53</xref>,<xref rid="B54-brainsci-11-00864" ref-type="bibr">54</xref>,<xref rid="B55-brainsci-11-00864" ref-type="bibr">55</xref>,<xref rid="B56-brainsci-11-00864" ref-type="bibr">56</xref>,<xref rid="B57-brainsci-11-00864" ref-type="bibr">57</xref>,<xref rid="B58-brainsci-11-00864" ref-type="bibr">58</xref>,<xref rid="B59-brainsci-11-00864" ref-type="bibr">59</xref>], the USV acoustic parameters are highly predictive of a number of addiction and other health related outcomes. We expect that the ease of acquisition of these parameters when using Acoustilytix will quicken the pace of research focused on acoustic parameters.</p>
    <p>We also introduced a recently developed feature of Acoustilytix that offers a fast and effective way to train hand-scorers and is built upon a foundation of learning science [<xref rid="B40-brainsci-11-00864" ref-type="bibr">40</xref>,<xref rid="B41-brainsci-11-00864" ref-type="bibr">41</xref>]. Critically, trainees are presented with several hundred USVs one at a time, are asked to classify the call, and receive immediate corrective feedback based on the hand-scoring from an expert in the lab. We tested this approach in seven novice hand-scorers and showed that their post training call classifications were highly correlated with the expert (inter-rater reliability: kappa statistic ranged from 0.30–0.75, average = 0.55). This approach increases the likelihood that all hand-scorers in the laboratory mimic the scoring behavior of the expert, thus, increasing consistency and reliability of hand-scoring.</p>
    <p>We now briefly discuss some future directions and further refinements planned for Acoustilytix.</p>
    <p><italic>Call Detection</italic>: With no user intervention or tuning, we were able to achieve high levels of call detection accuracy that were superior to the popular DeepSqueak algorithm. Even so, we are exploring the application of machine learning models in the USV call detection process to further eliminate false positives. As more USV wav files from other research groups are uploaded to the platform and are run through the detection algorithm, this model can be further tuned to provide better separation of valid calls from false positives.</p>
    <p><italic>Call Classification</italic>: Although automated call classification for the five-call type classification scheme was well above chance, there is still room for improvement. Having explored numerous machine learning types, we are confident that the supervised random forest approach is a good approach, but we need a significantly larger pool of data with hand-scored calls from experts to further improve the predictions from the algorithm. This process is ongoing. As we outline in this manuscript, we began with an expert hand-scorer scoring 1000 calls or so. We used that to train the model. Every time the expert scored a new file, that triggered a model refit so that the fit improved based on the larger set of data. Users can use the model developed from our expert, or they can use their own expert to train the model on a specific call classification scheme. Ultimately, we will use all of these data to train the model using expert scorers from multiple research groups using the same classification scheme. This is beneficial to the whole USV community because it will lead to more uniformity and consistence in scoring within and across research groups, making the research findings more reliable and reproducible.</p>
    <p><italic>Acoustic Parameters</italic>: Several research groups [<xref rid="B10-brainsci-11-00864" ref-type="bibr">10</xref>,<xref rid="B14-brainsci-11-00864" ref-type="bibr">14</xref>,<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>,<xref rid="B46-brainsci-11-00864" ref-type="bibr">46</xref>,<xref rid="B47-brainsci-11-00864" ref-type="bibr">47</xref>,<xref rid="B48-brainsci-11-00864" ref-type="bibr">48</xref>,<xref rid="B49-brainsci-11-00864" ref-type="bibr">49</xref>,<xref rid="B50-brainsci-11-00864" ref-type="bibr">50</xref>,<xref rid="B51-brainsci-11-00864" ref-type="bibr">51</xref>,<xref rid="B53-brainsci-11-00864" ref-type="bibr">53</xref>,<xref rid="B54-brainsci-11-00864" ref-type="bibr">54</xref>,<xref rid="B56-brainsci-11-00864" ref-type="bibr">56</xref>,<xref rid="B57-brainsci-11-00864" ref-type="bibr">57</xref>,<xref rid="B59-brainsci-11-00864" ref-type="bibr">59</xref>,<xref rid="B60-brainsci-11-00864" ref-type="bibr">60</xref>], including our own [<xref rid="B29-brainsci-11-00864" ref-type="bibr">29</xref>,<xref rid="B30-brainsci-11-00864" ref-type="bibr">30</xref>,<xref rid="B31-brainsci-11-00864" ref-type="bibr">31</xref>,<xref rid="B32-brainsci-11-00864" ref-type="bibr">32</xref>,<xref rid="B36-brainsci-11-00864" ref-type="bibr">36</xref>,<xref rid="B37-brainsci-11-00864" ref-type="bibr">37</xref>,<xref rid="B38-brainsci-11-00864" ref-type="bibr">38</xref>,<xref rid="B39-brainsci-11-00864" ref-type="bibr">39</xref>,<xref rid="B43-brainsci-11-00864" ref-type="bibr">43</xref>,<xref rid="B61-brainsci-11-00864" ref-type="bibr">61</xref>,<xref rid="B62-brainsci-11-00864" ref-type="bibr">62</xref>,<xref rid="B63-brainsci-11-00864" ref-type="bibr">63</xref>,<xref rid="B64-brainsci-11-00864" ref-type="bibr">64</xref>], have shown that acoustic parameters are predictive of addiction and mental health related outcomes. For example, our own research group has shown that acoustic parameter values are predictive of whether a call is emitted by a high vs. low alcohol drinking rat or whether that rat is male or female [<xref rid="B37-brainsci-11-00864" ref-type="bibr">37</xref>,<xref rid="B43-brainsci-11-00864" ref-type="bibr">43</xref>], as well as a number of other interesting distinctions. Acoustilytix currently computes over 200 acoustic parameters for each call that can be analyzed by USV researchers. Future work should consider applying multi-dimensional similarity-based approaches to create novel call classification schemes that are strictly data driven, in addition to exploring the predictive nature of specific or groups of parameters.</p>
  </sec>
  <sec sec-type="conclusions" id="sec6-brainsci-11-00864">
    <title>6. Conclusions</title>
    <p>This report offers a snapshot of the development of a web-based automated USV detection and classification software solution called Acoustilytix. Acoustilytix accelerates throughput of USV scoring by automating the call detection and classification process. Acoustilytix achieves high detection accuracy across four distinct environments without manual tuning or calibration and is superior to the currently popular DeepSqueak algorithm in sensitivity for three out of four environments and superior in precision in all four environments. Automated call classification was adequate yielding 79% accuracy, but more work is needed. A feature that allows researchers to upload a large number of hand-scored USV files that could be used to train an automated scoring algorithm that they could use exclusively is in demand and under development. The development of an automated hand-scoring feature is innovative and has been well received by the USV community. Taken together, Acoustilytix has the potential to speed the process of USV detection and classification in the interest of developing and testing animal models of mental health and addiction.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. </p>
  </ack>
  <fn-group>
    <fn>
      <p><bold>Publisher’s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes>
    <title>Author Contributions</title>
    <p>Conceptualization: C.B.A., R.D.S., J.E.S., C.L.D., and W.T.M.; methodology: C.B.A., R.D.S., J.E.S., C.L.D., and W.T.M.; validation: C.B.A., R.D.S., J.E.S., C.C., N.M., S.F., J.B., M.D.N., S.I.S., B.N., L.L., A.R.C., and C.L.D.; formal analysis: C.B.A., R.D.S., and J.E.S.; investigation: C.B.A., R.D.S., J.E.S., C.C., N.M., W.T.M., and C.L.D.; resources: R.D.S. and C.L.D.; data curation: C.B.A., R.D.S., J.E.S., C.C., S.F., W.T.M., and C.L.D.; writing—review and editing: C.B.A., R.D.S., N.M., W.T.M., and C.L.D.; supervision: C.B.A., R.D.S., C.C., W.T.M., and C.L.D.; project Administration: C.B.A., R.D.S., C.C., W.T.M., and C.L.D.; funding acquisition: R.D.S. and C.L.D. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>This research was funded by NIMH of the National Institutes of Health under award number R41MH121119.</p>
  </notes>
  <notes>
    <title>Institutional Review Board Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Informed Consent Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <app-group>
    <app id="app1-brainsci-11-00864">
      <title>Appendix A. Details of Initial Detection and File Parsing</title>
      <p>To determine where a call is most likely to exist, a coarse spectrogram (STFT) of the entire file is calculated. This coarse spectrogram is processed to account for differences due to the recording setup, which can result in varying acoustic power with respect to frequency bins. A maximum acoustic power is then derived from this processed spectrogram at each time interval, resulting in a one-dimensional (1D) signal. To determine where possible calls are located, calls are treated as outliers in this 1D signal.</p>
    </app>
    <app id="app2-brainsci-11-00864">
      <title>Appendix B. Call Isolation and Detection Confidence</title>
      <p>To isolate calls and differentiate them from background and noise, Acoustilytix uses a combinatorial approach, applying several filters and algorithms. The details of the algorithm are being evaluated as potential IP and, thus, cannot be presented in detail here. These filters are combined in various ways to extract the calls out of the background and provide a detection confidence value (percent certainty)</p>
    </app>
    <app id="app3-brainsci-11-00864">
      <title>Appendix C. Example Kappa Statistic Calculation</title>
      <p>In calculating the kappa statistic between two scorers, USVs that Acoustilytix missed were not included in the calculation. We were unable to include user-added calls in the analysis because the expert scorer did not add calls when analyzing calls in some recording files. Acoustilytix, at times, detects false USVs. Both scorers can assign the “false positive” category to these USVs. The false positive category was included in the kappa statistic calculation. The method used to calculate the kappa statistic can be found in the University of Nebraska-Lincoln Bivariate Statistics Hand-Computation Cache (2021, February). To begin calculating the kappa statistic, a confusion matrix is generated. The confusion matrix is a 5 × 5 matrix where each row represents one of the five call types, and each column represents the same five call types. The rows denote the classification for the individual hand-scorer and the columns for the expert hand-scorer. For each call classified, the value in one cell of the 5 × 5 confusion matrix is incremented. For example, if both the individual and expert classify a call as a Short 22-kHz then the cell in the confusion matrix that represents Short 22-kHz for the individual hand-scorer and Short 22-kHz for the expert is incremented by 1. This process is repeated until all calls are represented in the confusion matrix.</p>
      <p><xref rid="brainsci-11-00864-t0A1" ref-type="table">Table A1</xref> shows an example of a confusion matrix for 748 rat calls that were each scored by one individual and by the expert.</p>
      <table-wrap id="brainsci-11-00864-t0A1" orientation="portrait" position="anchor">
        <object-id pub-id-type="pii">brainsci-11-00864-t0A1_Table A1</object-id>
        <label>Table A1</label>
        <caption>
          <p>Example of a confusion matrix comparing call types between two raters.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th>
              <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th>
              <th colspan="6" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1">Rater 1</th>
            </tr>
            <tr>
              <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th>
              <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th>
              <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed Frequency 50</th>
              <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated 50</th>
              <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated with Trill 50</th>
              <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Long 22</th>
              <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short 22</th>
              <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Row Totals</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="5" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Rater 2</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed Frequency 50</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">110</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">189</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated 50</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">32</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">305</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">16</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">353</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated with Trill 50</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">126</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">60</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">192</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Long 22</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short 22</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">10</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                <bold>Column Totals</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                <bold>150</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                <bold>507</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                <bold>79</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                <bold>1</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                <bold>11</bold>
              </td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
                <bold>748</bold>
              </td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Interpreting the confusion matrix is straightforward. On-diagonal elements represent calls where the two raters are in agreement, and off-diagonal elements represent calls where the two raters are not in agreement. For example, Rater 1 and Rater 2 agreed that 110 calls were the “Fixed Frequency 50” call type, but Rater 1 assigned 75 calls to the “Frequency Modulated 50” call type that Rater 2 assigned to “Fixed Frequency 50.”</p>
      <p>The kappa statistic calculation uses the row sums, column sums, and overall call count. These are shown in the final column and final row in the example in <xref rid="brainsci-11-00864-t0A1" ref-type="table">Table A1</xref>. The overall total is the same whether you sum the row totals or column totals and is 748 in this example.</p>
      <p>Next, the total number of agreements are found by summing the values in the diagonal cells of the confusion matrix. In this example,
<disp-formula><italic>Σ agreements</italic> = <italic>Σ a</italic> = 110 + 305 + 60 + 1 + 10 = 486</disp-formula></p>
      <p>The expected frequency for the number of agreements is now calculated for each agreement along the diagonal by multiplying the row total by the column total and dividing by the overall total (see <xref rid="brainsci-11-00864-t0A2" ref-type="table">Table A2</xref>). For example, row #1 multiplied by column #1 and divided by the overall total is:<disp-formula><italic>expected frequency</italic> = <italic>ef</italic> = 189 ∗ 150/748 = 37.90</disp-formula></p>
      <table-wrap id="brainsci-11-00864-t0A2" orientation="portrait" position="anchor">
        <object-id pub-id-type="pii">brainsci-11-00864-t0A2_Table A2</object-id>
        <label>Table A2</label>
        <caption>
          <p>Expected frequencies calculated from confusion matrix in <xref rid="brainsci-11-00864-t0A1" ref-type="table">Table A1</xref>.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th>
              <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Expected Frequency</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed Frequency 50</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">37.90</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated 50</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">239.27</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated with Trill 50</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">20.28</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Long 22</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0053</td>
            </tr>
            <tr>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short 22</td>
              <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1470</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The sum of the expected frequencies of agreement by chance is the sum of the expected frequencies. For example:<disp-formula><italic>Σ expected frequencies</italic> = <italic>Σ ef</italic> = 37.90 + 239.27 + 20.28 + 0.0053 + 0.1470 = 297.60</disp-formula></p>
      <p>The kappa statistic can now be calculated with the Formula:<disp-formula><italic>K</italic> = (<italic>Σ a</italic> − <italic>Σ ef</italic>)/(<italic>N</italic> − <italic>Σ ef</italic>) = (486 − 297.60)/(748 − 297.60) = 0.418</disp-formula></p>
    </app>
  </app-group>
  <ref-list>
    <title>References</title>
    <ref id="B1-brainsci-11-00864">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tóth</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Arnberg</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Häggkvist</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Mulder</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Halldin</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Gulyás</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Holmin</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Acute neuroinflammation in a clinically relevant focal cortical ischemic stroke model in rat: Longitudinal positron emission tomography and immunofluorescent tracking</article-title>
        <source>Brain Struct. Funct.</source>
        <year>2016</year>
        <volume>221</volume>
        <fpage>1279</fpage>
        <lpage>1290</lpage>
        <pub-id pub-id-type="doi">10.1007/s00429-014-0970-y</pub-id>
        <?supplied-pmid 25601153?>
        <pub-id pub-id-type="pmid">25601153</pub-id>
      </element-citation>
    </ref>
    <ref id="B2-brainsci-11-00864">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Homberg</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Kyzar</surname>
            <given-names>E.J.</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Norton</surname>
            <given-names>W.H.</given-names>
          </name>
          <name>
            <surname>Pittman</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Poudel</surname>
            <given-names>M.K.</given-names>
          </name>
          <name>
            <surname>Gaikwad</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Nakamura</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Koshiba</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yamanouchi</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Understanding autism and other neurodevelopmental disorders through experimental translational neurobehavioral models</article-title>
        <source>Neurosci. Biobehav. Rev.</source>
        <year>2016</year>
        <volume>65</volume>
        <fpage>292</fpage>
        <lpage>312</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.03.013</pub-id>
        <?supplied-pmid 27048961?>
        <pub-id pub-id-type="pmid">27048961</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-brainsci-11-00864">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stewart</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Kalueff</surname>
            <given-names>A.V.</given-names>
          </name>
        </person-group>
        <article-title>Developing better and more valid animal models of brain disorders</article-title>
        <source>Behav. Brain Res.</source>
        <year>2015</year>
        <volume>276</volume>
        <fpage>28</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2013.12.024</pub-id>
        <?supplied-pmid 24384129?>
        <pub-id pub-id-type="pmid">24384129</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-brainsci-11-00864">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kalueff</surname>
            <given-names>A.V.</given-names>
          </name>
          <name>
            <surname>Stewart</surname>
            <given-names>A.M.</given-names>
          </name>
        </person-group>
        <article-title>Modeling neuropsychiatric spectra to empower translational biological psychiatry</article-title>
        <source>Behav. Brain Res.</source>
        <year>2015</year>
        <volume>276</volume>
        <fpage>1</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2014.01.038</pub-id>
        <pub-id pub-id-type="pmid">24508241</pub-id>
      </element-citation>
    </ref>
    <ref id="B5-brainsci-11-00864">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Czéh</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Fuchs</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Wiborg</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Animal models of major depression and their clinical implications</article-title>
        <source>Prog. Neuropsychopharmacol. Biol. Psychiatry</source>
        <year>2016</year>
        <volume>64</volume>
        <fpage>293</fpage>
        <lpage>310</lpage>
        <pub-id pub-id-type="doi">10.1016/j.pnpbp.2015.04.004</pub-id>
        <pub-id pub-id-type="pmid">25891248</pub-id>
      </element-citation>
    </ref>
    <ref id="B6-brainsci-11-00864">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>Ultrasonic calls of rats as indicator variables of negative or positive states: Acetylcholine-dopamine interaction and acoustic coding</article-title>
        <source>Behav. Brain Res.</source>
        <year>2007</year>
        <volume>182</volume>
        <fpage>261</fpage>
        <lpage>273</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2007.03.004</pub-id>
        <pub-id pub-id-type="pmid">17467067</pub-id>
      </element-citation>
    </ref>
    <ref id="B7-brainsci-11-00864">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>Ethotransmission: Communication of emotional states through ultrasonic vocalization in rats</article-title>
        <source>Curr. Opin. Neurobiol.</source>
        <year>2013</year>
        <volume>23</volume>
        <fpage>310</fpage>
        <lpage>317</lpage>
        <pub-id pub-id-type="doi">10.1016/j.conb.2013.01.014</pub-id>
        <pub-id pub-id-type="pmid">23375168</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-brainsci-11-00864">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burgdorf</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wood</surname>
            <given-names>P.L.</given-names>
          </name>
          <name>
            <surname>Kroes</surname>
            <given-names>R.A.</given-names>
          </name>
          <name>
            <surname>Moskal</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Panksepp</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Neurobiology of 50-kHz ultrasonic vocalizations in rats: Electrode mapping, lesion, and pharmacology studies</article-title>
        <source>Behav. Brain Res.</source>
        <year>2007</year>
        <volume>182</volume>
        <fpage>274</fpage>
        <lpage>283</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2007.03.010</pub-id>
        <pub-id pub-id-type="pmid">17449117</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-brainsci-11-00864">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>Communication of adult rats by ultrasonic vocalization: Biological, sociobiological, and neuroscience approaches</article-title>
        <source>ILAR J.</source>
        <year>2009</year>
        <volume>50</volume>
        <fpage>43</fpage>
        <lpage>50</lpage>
        <pub-id pub-id-type="doi">10.1093/ilar.50.1.43</pub-id>
        <pub-id pub-id-type="pmid">19106451</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-brainsci-11-00864">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bihari</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hrycyshyn</surname>
            <given-names>A.W.</given-names>
          </name>
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>Role of the mesolimbic cholinergic projection to the septum in the production of 22 kHz alarm calls in rats</article-title>
        <source>Brain Res. Bull.</source>
        <year>2003</year>
        <volume>60</volume>
        <fpage>263</fpage>
        <lpage>274</lpage>
        <pub-id pub-id-type="doi">10.1016/S0361-9230(03)00041-8</pub-id>
        <pub-id pub-id-type="pmid">12754088</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-brainsci-11-00864">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>Pharmacological and behavioral characteristics of 22kHz alarm calls in rats</article-title>
        <source>Neurosci. Biobehav. Rev.</source>
        <year>2001</year>
        <volume>25</volume>
        <fpage>611</fpage>
        <lpage>617</lpage>
        <pub-id pub-id-type="doi">10.1016/S0149-7634(01)00058-6</pub-id>
        <pub-id pub-id-type="pmid">11801286</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-brainsci-11-00864">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thompson</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Leonard</surname>
            <given-names>K.C.</given-names>
          </name>
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>Amphetamine-induced 50 kHz calls from rat nucleus accumbens: A quantitative mapping study and acoustic analysis</article-title>
        <source>Behav. Brain Res.</source>
        <year>2006</year>
        <volume>168</volume>
        <fpage>64</fpage>
        <lpage>73</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2005.10.012</pub-id>
        <pub-id pub-id-type="pmid">16343652</pub-id>
      </element-citation>
    </ref>
    <ref id="B13-brainsci-11-00864">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grant</surname>
            <given-names>L.M.</given-names>
          </name>
          <name>
            <surname>Richter</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>J.E.</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Fox</surname>
            <given-names>C.M.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Chesselet</surname>
            <given-names>M.-F.</given-names>
          </name>
          <name>
            <surname>Ciucci</surname>
            <given-names>M.R.</given-names>
          </name>
        </person-group>
        <article-title>Vocalization deficits in mice over-expressing alpha-synuclein, a model of pre-manifest Parkinson’s disease</article-title>
        <source>Behav. Neurosci.</source>
        <year>2014</year>
        <volume>128</volume>
        <fpage>110</fpage>
        <lpage>121</lpage>
        <pub-id pub-id-type="doi">10.1037/a0035965</pub-id>
        <pub-id pub-id-type="pmid">24773432</pub-id>
      </element-citation>
    </ref>
    <ref id="B14-brainsci-11-00864">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Portfors</surname>
            <given-names>C.V.</given-names>
          </name>
        </person-group>
        <article-title>Types and functions of ultrasonic vocalizations in laboratory rats and mice</article-title>
        <source>J. Am. Assoc. Lab. Anim. Sci.</source>
        <year>2007</year>
        <volume>46</volume>
        <fpage>28</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="pmid">17203913</pub-id>
      </element-citation>
    </ref>
    <ref id="B15-brainsci-11-00864">
      <label>15.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <source>Handbook of Ultrasonic Vocalization</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <publisher-name>Academic Press</publisher-name>
        <publisher-loc>Cambridge, MA, USA</publisher-loc>
        <year>2018</year>
        <isbn>9780128096000</isbn>
      </element-citation>
    </ref>
    <ref id="B16-brainsci-11-00864">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burgdorf</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Moskal</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Panksepp</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Rats selectively bred for low levels of play-induced 50 kHz vocalizations as a model for autism spectrum disorders: A role for NMDA receptors</article-title>
        <source>Behav. Brain Res.</source>
        <year>2013</year>
        <volume>251</volume>
        <fpage>18</fpage>
        <lpage>24</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2013.04.022</pub-id>
        <pub-id pub-id-type="pmid">23623884</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-brainsci-11-00864">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Clinton</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Watson</surname>
            <given-names>S.J.</given-names>
          </name>
          <name>
            <surname>Akil</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>High novelty-seeking rats are resilient to negative physiological effects of the early life stress</article-title>
        <source>Stress</source>
        <year>2014</year>
        <volume>17</volume>
        <fpage>97</fpage>
        <lpage>107</lpage>
        <pub-id pub-id-type="doi">10.3109/10253890.2013.850670</pub-id>
        <pub-id pub-id-type="pmid">24090131</pub-id>
      </element-citation>
    </ref>
    <ref id="B18-brainsci-11-00864">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scattoni</surname>
            <given-names>M.L.</given-names>
          </name>
          <name>
            <surname>Crawley</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ricceri</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Ultrasonic vocalizations: A tool for behavioural phenotyping of mouse models of neurodevelopmental disorders</article-title>
        <source>Neurosci. Biobehav. Rev.</source>
        <year>2009</year>
        <volume>33</volume>
        <fpage>508</fpage>
        <lpage>515</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neubiorev.2008.08.003</pub-id>
        <pub-id pub-id-type="pmid">18771687</pub-id>
      </element-citation>
    </ref>
    <ref id="B19-brainsci-11-00864">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Pribiag</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Lilascharoen</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Knowland</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.-Y.</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>B.K.</given-names>
          </name>
        </person-group>
        <article-title>Drd3 Signaling in the Lateral Septum Mediates Early Life Stress-Induced Social Dysfunction</article-title>
        <source>Neuron</source>
        <year>2018</year>
        <volume>97</volume>
        <fpage>195</fpage>
        <lpage>208.e6</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.040</pub-id>
        <pub-id pub-id-type="pmid">29276054</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-brainsci-11-00864">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Silverman</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lord</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Crawley</surname>
            <given-names>J.N.</given-names>
          </name>
        </person-group>
        <article-title>Behavioural phenotyping assays for mouse models of autism</article-title>
        <source>Nat. Rev. Neurosci.</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>490</fpage>
        <lpage>502</lpage>
        <pub-id pub-id-type="doi">10.1038/nrn2851</pub-id>
        <pub-id pub-id-type="pmid">20559336</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-brainsci-11-00864">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Steele</surname>
            <given-names>F.F.</given-names>
          </name>
          <name>
            <surname>Whitehouse</surname>
            <given-names>S.C.</given-names>
          </name>
          <name>
            <surname>Aday</surname>
            <given-names>J.S.</given-names>
          </name>
          <name>
            <surname>Prus</surname>
            <given-names>A.J.</given-names>
          </name>
        </person-group>
        <article-title>Neurotensin NTS1 and NTS2 receptor agonists produce anxiolytic-like effects in the 22-kHz ultrasonic vocalization model in rats</article-title>
        <source>Brain Res.</source>
        <year>2017</year>
        <volume>1658</volume>
        <fpage>31</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1016/j.brainres.2017.01.012</pub-id>
        <pub-id pub-id-type="pmid">28089664</pub-id>
      </element-citation>
    </ref>
    <ref id="B22-brainsci-11-00864">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wöhr</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Ultrasonic vocalizations in Shank mouse models for autism spectrum disorders: Detailed spectrographic analyses and developmental profiles</article-title>
        <source>Neurosci. Biobehav. Rev.</source>
        <year>2014</year>
        <volume>43</volume>
        <fpage>199</fpage>
        <lpage>212</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neubiorev.2014.03.021</pub-id>
        <pub-id pub-id-type="pmid">24726578</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-brainsci-11-00864">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wöhr</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Silverman</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Scattoni</surname>
            <given-names>M.L.</given-names>
          </name>
          <name>
            <surname>Turner</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Harris</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Saxena</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Crawley</surname>
            <given-names>J.N.</given-names>
          </name>
        </person-group>
        <article-title>Developmental delays and reduced pup ultrasonic vocalizations but normal sociability in mice lacking the postsynaptic cell adhesion protein neuroligin2</article-title>
        <source>Behav. Brain Res.</source>
        <year>2013</year>
        <volume>251</volume>
        <fpage>50</fpage>
        <lpage>64</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2012.07.024</pub-id>
        <pub-id pub-id-type="pmid">22820233</pub-id>
      </element-citation>
    </ref>
    <ref id="B24-brainsci-11-00864">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Loureiro</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Kalikhman</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Crawley</surname>
            <given-names>J.N.</given-names>
          </name>
        </person-group>
        <article-title>Male mice emit distinct ultrasonic vocalizations when the female leaves the social interaction arena</article-title>
        <source>Front. Behav. Neurosci.</source>
        <year>2013</year>
        <volume>7</volume>
        <fpage>159</fpage>
        <pub-id pub-id-type="doi">10.3389/fnbeh.2013.00159</pub-id>
        <pub-id pub-id-type="pmid">24312027</pub-id>
      </element-citation>
    </ref>
    <ref id="B25-brainsci-11-00864">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Faure</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Nosjean</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pittaras</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Duchêne</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Andrieux</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gory-Fauré</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Charvériat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Granon</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Dissociated features of social cognition altered in mouse models of schizophrenia: Focus on social dominance and acoustic communication</article-title>
        <source>Neuropharmacology</source>
        <year>2019</year>
        <volume>159</volume>
        <fpage>107334</fpage>
        <pub-id pub-id-type="doi">10.1016/j.neuropharm.2018.09.009</pub-id>
        <pub-id pub-id-type="pmid">30236964</pub-id>
      </element-citation>
    </ref>
    <ref id="B26-brainsci-11-00864">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Knutson</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Burgdorf</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Panksepp</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>High-frequency ultrasonic vocalizations index conditioned pharmacological reward in rats</article-title>
        <source>Physiol. Behav.</source>
        <year>1999</year>
        <volume>66</volume>
        <fpage>639</fpage>
        <lpage>643</lpage>
        <pub-id pub-id-type="doi">10.1016/S0031-9384(98)00337-0</pub-id>
        <pub-id pub-id-type="pmid">10386908</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-brainsci-11-00864">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Prus</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Hillhouse</surname>
            <given-names>T.M.</given-names>
          </name>
          <name>
            <surname>LaCrosse</surname>
            <given-names>A.L.</given-names>
          </name>
        </person-group>
        <article-title>Acute, but not repeated, administration of the neurotensin NTS1 receptor agonist PD149163 decreases conditioned footshock-induced ultrasonic vocalizations in rats</article-title>
        <source>Prog. Neuropsychopharmacol. Biol. Psychiatry</source>
        <year>2014</year>
        <volume>49</volume>
        <fpage>78</fpage>
        <lpage>84</lpage>
        <pub-id pub-id-type="doi">10.1016/j.pnpbp.2013.11.011</pub-id>
        <pub-id pub-id-type="pmid">24275076</pub-id>
      </element-citation>
    </ref>
    <ref id="B28-brainsci-11-00864">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Saunders</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Tatard-Leitman</surname>
            <given-names>V.M.</given-names>
          </name>
          <name>
            <surname>Suh</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Billingslea</surname>
            <given-names>E.N.</given-names>
          </name>
          <name>
            <surname>Roberts</surname>
            <given-names>T.P.</given-names>
          </name>
          <name>
            <surname>Siegel</surname>
            <given-names>S.J.</given-names>
          </name>
        </person-group>
        <article-title>Knockout of NMDA receptors in parvalbumin interneurons recreates autism-like phenotypes</article-title>
        <source>Autism Res.</source>
        <year>2013</year>
        <volume>6</volume>
        <fpage>69</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1002/aur.1264</pub-id>
        <pub-id pub-id-type="pmid">23441094</pub-id>
      </element-citation>
    </ref>
    <ref id="B29-brainsci-11-00864">
      <label>29.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Reno</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Thakore</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Mittal</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Cormack</surname>
            <given-names>L.K.</given-names>
          </name>
          <name>
            <surname>Bell</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Gonzales</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Alcohol-Preferring Rats and 22-kHz Negative-Affect Ultrasonic Vocalizations</article-title>
        <source>Handbook of Behavioral Neuroscience</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <publisher-name>Academic Press</publisher-name>
        <publisher-loc>Cambridge, MA, USA</publisher-loc>
        <year>2018</year>
        <volume>Volume 25</volume>
        <fpage>401</fpage>
        <lpage>411</lpage>
      </element-citation>
    </ref>
    <ref id="B30-brainsci-11-00864">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mittal</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Todd Maddox</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Rodent ultrasonic vocalizations as biomarkers of future alcohol use: A predictive analytic approach</article-title>
        <source>Cogn. Affect. Behav. Neurosci.</source>
        <year>2018</year>
        <volume>18</volume>
        <fpage>88</fpage>
        <lpage>98</lpage>
        <pub-id pub-id-type="doi">10.3758/s13415-017-0554-4</pub-id>
        <?supplied-pmid 29209998?>
        <pub-id pub-id-type="pmid">29209998</pub-id>
      </element-citation>
    </ref>
    <ref id="B31-brainsci-11-00864">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mittal</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Spontaneous Ultrasonic Vocalization Transmission in Adult, Male Long-Evans Rats Is Age-Dependent and Sensitive to EtOH Modulation</article-title>
        <source>Brain Sci.</source>
        <year>2020</year>
        <volume>10</volume>
        <elocation-id>890</elocation-id>
        <pub-id pub-id-type="doi">10.3390/brainsci10110890</pub-id>
        <?supplied-pmid 33266373?>
        <pub-id pub-id-type="pmid">33266373</pub-id>
      </element-citation>
    </ref>
    <ref id="B32-brainsci-11-00864">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reno</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Marker</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Cormack</surname>
            <given-names>L.K.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Automating ultrasonic vocalization analyses: The WAAVES program</article-title>
        <source>J. Neurosci. Methods</source>
        <year>2013</year>
        <volume>219</volume>
        <fpage>155</fpage>
        <lpage>161</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.06.006</pub-id>
        <?supplied-pmid 23832016?>
        <pub-id pub-id-type="pmid">23832016</pub-id>
      </element-citation>
    </ref>
    <ref id="B33-brainsci-11-00864">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barker</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Herrera</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>West</surname>
            <given-names>M.O.</given-names>
          </name>
        </person-group>
        <article-title>Automated detection of 50-kHz ultrasonic vocalizations using template matching in XBAT</article-title>
        <source>J. Neurosci. Methods</source>
        <year>2014</year>
        <volume>236</volume>
        <fpage>68</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.08.007</pub-id>
        <pub-id pub-id-type="pmid">25128724</pub-id>
      </element-citation>
    </ref>
    <ref id="B34-brainsci-11-00864">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Coffey</surname>
            <given-names>K.R.</given-names>
          </name>
          <name>
            <surname>Marx</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Neumaier</surname>
            <given-names>J.F.</given-names>
          </name>
        </person-group>
        <article-title>DeepSqueak: A deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title>
        <source>Neuropsychopharmacology</source>
        <year>2019</year>
        <volume>44</volume>
        <fpage>859</fpage>
        <lpage>868</lpage>
        <pub-id pub-id-type="doi">10.1038/s41386-018-0303-6</pub-id>
        <pub-id pub-id-type="pmid">30610191</pub-id>
      </element-citation>
    </ref>
    <ref id="B35-brainsci-11-00864">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Segbroeck</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Knoll</surname>
            <given-names>A.T.</given-names>
          </name>
          <name>
            <surname>Levitt</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Narayanan</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>MUPET-Mouse Ultrasonic Profile ExTraction: A Signal Processing Tool for Rapid and Unsupervised Analysis of Ultrasonic Vocalizations</article-title>
        <source>Neuron</source>
        <year>2017</year>
        <volume>94</volume>
        <fpage>465</fpage>
        <lpage>485.e5</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.005</pub-id>
        <pub-id pub-id-type="pmid">28472651</pub-id>
      </element-citation>
    </ref>
    <ref id="B36-brainsci-11-00864">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ahrens</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S.T.</given-names>
          </name>
          <name>
            <surname>Maier</surname>
            <given-names>E.Y.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Repeated intravenous amphetamine exposure: Rapid and persistent sensitization of 50-kHz ultrasonic trill calls in rats</article-title>
        <source>Behav. Brain Res.</source>
        <year>2009</year>
        <volume>197</volume>
        <fpage>205</fpage>
        <lpage>209</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2008.08.037</pub-id>
        <pub-id pub-id-type="pmid">18809437</pub-id>
      </element-citation>
    </ref>
    <ref id="B37-brainsci-11-00864">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mittal</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Thakore</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Bell</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Sex-specific ultrasonic vocalization patterns and alcohol consumption in high alcohol-drinking (HAD-1) rats</article-title>
        <source>Physiol. Behav.</source>
        <year>2019</year>
        <volume>203</volume>
        <fpage>81</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/j.physbeh.2017.11.012</pub-id>
        <pub-id pub-id-type="pmid">29146494</pub-id>
      </element-citation>
    </ref>
    <ref id="B38-brainsci-11-00864">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mittal</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Thakore</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Reno</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Bell</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Alcohol-Naïve USVs Distinguish Male HAD-1 from LAD-1 Rat Strains</article-title>
        <source>Alcohol</source>
        <year>2017</year>
        <volume>68</volume>
        <fpage>9</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1016/j.alcohol.2017.09.003</pub-id>
        <pub-id pub-id-type="pmid">29427829</pub-id>
      </element-citation>
    </ref>
    <ref id="B39-brainsci-11-00864">
      <label>39.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Mittal</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Martinez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Fleming</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Sex differences in alcohol consumption, object recognition and rearing in male and female high alcohol drinking (had1) rats</article-title>
        <source>Proceedings of the Society for Neuroscience</source>
        <conf-loc>Washington, DC, USA</conf-loc>
        <conf-date>11–15 November 2017</conf-date>
      </element-citation>
    </ref>
    <ref id="B40-brainsci-11-00864">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Ing</surname>
            <given-names>A.D.</given-names>
          </name>
        </person-group>
        <article-title>Delayed feedback disrupts the procedural-learning system but not the hypothesis-testing system in perceptual category learning</article-title>
        <source>J. Exp. Psychol. Learn. Mem. Cogn.</source>
        <year>2005</year>
        <volume>31</volume>
        <fpage>100</fpage>
        <lpage>107</lpage>
        <pub-id pub-id-type="doi">10.1037/0278-7393.31.1.100</pub-id>
        <pub-id pub-id-type="pmid">15641908</pub-id>
      </element-citation>
    </ref>
    <ref id="B41-brainsci-11-00864">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Ashby</surname>
            <given-names>F.G.</given-names>
          </name>
          <name>
            <surname>Bohil</surname>
            <given-names>C.J.</given-names>
          </name>
        </person-group>
        <article-title>Delayed feedback effects on rule-based and information-integration category learning</article-title>
        <source>J. Exp. Psychol. Learn. Mem. Cogn.</source>
        <year>2003</year>
        <volume>29</volume>
        <fpage>650</fpage>
        <lpage>662</lpage>
        <pub-id pub-id-type="doi">10.1037/0278-7393.29.4.650</pub-id>
        <pub-id pub-id-type="pmid">12924865</pub-id>
      </element-citation>
    </ref>
    <ref id="B42-brainsci-11-00864">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wright</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Gourdon</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Clarke</surname>
            <given-names>P.B.S.</given-names>
          </name>
        </person-group>
        <article-title>Identification of multiple call categories within the rich repertoire of adult rat 50-kHz ultrasonic vocalizations: Effects of amphetamine and social context</article-title>
        <source>Psychopharmacology</source>
        <year>2010</year>
        <volume>211</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1007/s00213-010-1859-y</pub-id>
        <pub-id pub-id-type="pmid">20443111</pub-id>
      </element-citation>
    </ref>
    <ref id="B43-brainsci-11-00864">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thakore</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Reno</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Gonzales</surname>
            <given-names>R.A.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Bell</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Alcohol enhances unprovoked 22-28 kHz USVs and suppresses USV mean frequency in High Alcohol Drinking (HAD-1) male rats</article-title>
        <source>Behav. Brain Res.</source>
        <year>2016</year>
        <volume>302</volume>
        <fpage>228</fpage>
        <lpage>236</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2016.01.042</pub-id>
        <pub-id pub-id-type="pmid">26802730</pub-id>
      </element-citation>
    </ref>
    <ref id="B44-brainsci-11-00864">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Kehoe</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Callahan</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Sonographic structure of isolation-induced ultrasonic calls of rat pups</article-title>
        <source>Dev. Psychobiol.</source>
        <year>1999</year>
        <volume>34</volume>
        <fpage>195</fpage>
        <lpage>204</lpage>
        <pub-id pub-id-type="doi">10.1002/(SICI)1098-2302(199904)34:3&lt;195::AID-DEV4&gt;3.0.CO;2-S</pub-id>
        <pub-id pub-id-type="pmid">10204095</pub-id>
      </element-citation>
    </ref>
    <ref id="B45-brainsci-11-00864">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeskind</surname>
            <given-names>P.S.</given-names>
          </name>
          <name>
            <surname>McMurray</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Garber</surname>
            <given-names>K.A.</given-names>
          </name>
          <name>
            <surname>Neuspiel</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>E.T.</given-names>
          </name>
          <name>
            <surname>Grewen</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>Mayes</surname>
            <given-names>L.C.</given-names>
          </name>
          <name>
            <surname>Johns</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Development of translational methods in spectral analysis of human infant crying and rat pup ultrasonic vocalizations for early neurobehavioral assessment</article-title>
        <source>Front. Psychiatry</source>
        <year>2011</year>
        <volume>2</volume>
        <fpage>56</fpage>
        <pub-id pub-id-type="doi">10.3389/fpsyt.2011.00056</pub-id>
        <pub-id pub-id-type="pmid">22028695</pub-id>
      </element-citation>
    </ref>
    <ref id="B46-brainsci-11-00864">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Silkstone</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Komadoski</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Scullion</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Duffus</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Burgdorf</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kroes</surname>
            <given-names>R.A.</given-names>
          </name>
          <name>
            <surname>Moskal</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Panksepp</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Effects of intraaccumbens amphetamine on production of 50kHz vocalizations in three lines of selectively bred Long-Evans rats</article-title>
        <source>Behav. Brain Res.</source>
        <year>2011</year>
        <volume>217</volume>
        <fpage>32</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bbr.2010.10.006</pub-id>
        <pub-id pub-id-type="pmid">20937325</pub-id>
      </element-citation>
    </ref>
    <ref id="B47-brainsci-11-00864">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>Ultrasonic vocalization induced by intracerebral carbachol in rats: Localization and a dose-response study</article-title>
        <source>Behav. Brain Res.</source>
        <year>1994</year>
        <volume>63</volume>
        <fpage>133</fpage>
        <lpage>143</lpage>
        <pub-id pub-id-type="doi">10.1016/0166-4328(94)90084-1</pub-id>
        <pub-id pub-id-type="pmid">7999296</pub-id>
      </element-citation>
    </ref>
    <ref id="B48-brainsci-11-00864">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ciucci</surname>
            <given-names>M.R.</given-names>
          </name>
          <name>
            <surname>Ahrens</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S.T.</given-names>
          </name>
          <name>
            <surname>Kane</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Windham</surname>
            <given-names>E.B.</given-names>
          </name>
          <name>
            <surname>Woodlee</surname>
            <given-names>M.T.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Reduction of dopamine synaptic activity: Degradation of 50-kHz ultrasonic vocalization in rats</article-title>
        <source>Behav. Neurosci.</source>
        <year>2009</year>
        <volume>123</volume>
        <fpage>328</fpage>
        <lpage>336</lpage>
        <pub-id pub-id-type="doi">10.1037/a0014593</pub-id>
        <pub-id pub-id-type="pmid">19331456</pub-id>
      </element-citation>
    </ref>
    <ref id="B49-brainsci-11-00864">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Inagaki</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Takeuchi</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Mori</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Close relationship between the frequency of 22-kHz calls and vocal tract length in male rats</article-title>
        <source>Physiol. Behav.</source>
        <year>2012</year>
        <volume>106</volume>
        <fpage>224</fpage>
        <lpage>228</lpage>
        <pub-id pub-id-type="doi">10.1016/j.physbeh.2012.01.018</pub-id>
        <pub-id pub-id-type="pmid">22326645</pub-id>
      </element-citation>
    </ref>
    <ref id="B50-brainsci-11-00864">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wöhr</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Borta</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Schwarting</surname>
            <given-names>R.K.W.</given-names>
          </name>
        </person-group>
        <article-title>Overt behavior and ultrasonic vocalization in a fear conditioning paradigm: A dose-response study in the rat</article-title>
        <source>Neurobiol. Learn. Mem.</source>
        <year>2005</year>
        <volume>84</volume>
        <fpage>228</fpage>
        <lpage>240</lpage>
        <pub-id pub-id-type="doi">10.1016/j.nlm.2005.07.004</pub-id>
        <pub-id pub-id-type="pmid">16115784</pub-id>
      </element-citation>
    </ref>
    <ref id="B51-brainsci-11-00864">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simola</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Fenu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Pinna</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Plumitallo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Morelli</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Pharmacological characterization of 50-kHz ultrasonic vocalizations in rats: Comparison of the effects of different psychoactive drugs and relevance in drug-induced reward</article-title>
        <source>Neuropharmacology</source>
        <year>2012</year>
        <volume>63</volume>
        <fpage>224</fpage>
        <lpage>234</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuropharm.2012.03.013</pub-id>
        <pub-id pub-id-type="pmid">22465816</pub-id>
      </element-citation>
    </ref>
    <ref id="B52-brainsci-11-00864">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Worthy</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Markman</surname>
            <given-names>A.B.</given-names>
          </name>
          <name>
            <surname>Todd Maddox</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>Feedback and stimulus-offset timing effects in perceptual category learning</article-title>
        <source>Brain Cogn.</source>
        <year>2013</year>
        <volume>81</volume>
        <fpage>283</fpage>
        <lpage>293</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bandc.2012.11.006</pub-id>
        <pub-id pub-id-type="pmid">23313835</pub-id>
      </element-citation>
    </ref>
    <ref id="B53-brainsci-11-00864">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Basken</surname>
            <given-names>J.N.</given-names>
          </name>
          <name>
            <surname>Connor</surname>
            <given-names>N.P.</given-names>
          </name>
          <name>
            <surname>Ciucci</surname>
            <given-names>M.R.</given-names>
          </name>
        </person-group>
        <article-title>Effect of aging on ultrasonic vocalizations and laryngeal sensorimotor neurons in rats</article-title>
        <source>Exp. Brain Res.</source>
        <year>2012</year>
        <volume>219</volume>
        <fpage>351</fpage>
        <lpage>361</lpage>
        <pub-id pub-id-type="doi">10.1007/s00221-012-3096-6</pub-id>
        <pub-id pub-id-type="pmid">22562586</pub-id>
      </element-citation>
    </ref>
    <ref id="B54-brainsci-11-00864">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simola</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S.T.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Influence of acute caffeine on 50-kHz ultrasonic vocalizations in male adult rats and relevance to caffeine-mediated psychopharmacological effects</article-title>
        <source>Int. J. Neuropsychopharmacol.</source>
        <year>2010</year>
        <volume>13</volume>
        <fpage>123</fpage>
        <lpage>132</lpage>
        <pub-id pub-id-type="doi">10.1017/S1461145709990113</pub-id>
        <?supplied-pmid 19545474?>
        <pub-id pub-id-type="pmid">19545474</pub-id>
      </element-citation>
    </ref>
    <ref id="B55-brainsci-11-00864">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brudzynski</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>Pharmacology of Ultrasonic Vocalizations in Adult Rats: Significance, Call Classification and Neural Substrate</article-title>
        <source>Curr. Neuropharmacol.</source>
        <year>2015</year>
        <volume>13</volume>
        <fpage>180</fpage>
        <lpage>192</lpage>
        <pub-id pub-id-type="doi">10.2174/1570159X13999150210141444</pub-id>
        <?supplied-pmid 26411761?>
        <pub-id pub-id-type="pmid">26411761</pub-id>
      </element-citation>
    </ref>
    <ref id="B56-brainsci-11-00864">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Grant</surname>
            <given-names>L.M.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ciucci</surname>
            <given-names>M.R.</given-names>
          </name>
        </person-group>
        <article-title>Changes in rat 50-kHz ultrasonic vocalizations during dopamine denervation and aging: Relevance to neurodegeneration</article-title>
        <source>Curr. Neuropharmacol.</source>
        <year>2015</year>
        <volume>13</volume>
        <fpage>211</fpage>
        <lpage>219</lpage>
        <pub-id pub-id-type="doi">10.2174/1570159X1302150525122416</pub-id>
        <?supplied-pmid 26411763?>
        <pub-id pub-id-type="pmid">26411763</pub-id>
      </element-citation>
    </ref>
    <ref id="B57-brainsci-11-00864">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simola</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Rat Ultrasonic Vocalizations and Behavioral Neuropharmacology: From the Screening of Drugs to the Study of Disease</article-title>
        <source>Curr. Neuropharmacol.</source>
        <year>2015</year>
        <volume>13</volume>
        <fpage>164</fpage>
        <lpage>179</lpage>
        <pub-id pub-id-type="doi">10.2174/1570159X13999150318113800</pub-id>
        <pub-id pub-id-type="pmid">26411760</pub-id>
      </element-citation>
    </ref>
    <ref id="B58-brainsci-11-00864">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Knutson</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Burgdorf</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Panksepp</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Ultrasonic vocalizations as indices of affective states in rats</article-title>
        <source>Psychol. Bull.</source>
        <year>2002</year>
        <volume>128</volume>
        <fpage>961</fpage>
        <lpage>977</lpage>
        <pub-id pub-id-type="doi">10.1037/0033-2909.128.6.961</pub-id>
        <pub-id pub-id-type="pmid">12405139</pub-id>
      </element-citation>
    </ref>
    <ref id="B59-brainsci-11-00864">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ciucci</surname>
            <given-names>M.R.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S.T.</given-names>
          </name>
          <name>
            <surname>Kane</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Ahrens</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Limb use and complex ultrasonic vocalization in a rat model of Parkinson’s disease: Deficit-targeted training</article-title>
        <source>Park. Relat. Disord.</source>
        <year>2008</year>
        <volume>14</volume>
        <fpage>172</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1016/j.parkreldis.2008.04.027</pub-id>
      </element-citation>
    </ref>
    <ref id="B60-brainsci-11-00864">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barker</surname>
            <given-names>D.J.</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>A.M.</given-names>
          </name>
        </person-group>
        <article-title>Automated acoustic analysis of 50-kHz ultrasonic vocalizations using template matching and contour analysis</article-title>
        <source>J. Acoust. Soc. Am.</source>
        <year>2017</year>
        <volume>141</volume>
        <fpage>EL281</fpage>
        <lpage>EL286</lpage>
        <pub-id pub-id-type="doi">10.1121/1.4977990</pub-id>
        <pub-id pub-id-type="pmid">28372124</pub-id>
      </element-citation>
    </ref>
    <ref id="B61-brainsci-11-00864">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reno</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Response to: Making WAAVES in the vocalization community: How big is the splash?</article-title>
        <source>J. Neurosci. Methods</source>
        <year>2014</year>
        <volume>221</volume>
        <fpage>230</fpage>
        <lpage>232</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.09.008</pub-id>
        <pub-id pub-id-type="pmid">24091135</pub-id>
      </element-citation>
    </ref>
    <ref id="B62-brainsci-11-00864">
      <label>62.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reno</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Thakore</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Cormack</surname>
            <given-names>L.K.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Bell</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Negative Affect-Associated USV Acoustic Characteristics Predict Future Excessive Alcohol Drinking and Alcohol Avoidance in Male P and NP Rats</article-title>
        <source>Alcohol. Clin. Exp. Res.</source>
        <year>2017</year>
        <volume>41</volume>
        <fpage>786</fpage>
        <lpage>797</lpage>
        <pub-id pub-id-type="doi">10.1111/acer.13344</pub-id>
        <pub-id pub-id-type="pmid">28118495</pub-id>
      </element-citation>
    </ref>
    <ref id="B63-brainsci-11-00864">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maier</surname>
            <given-names>E.Y.</given-names>
          </name>
          <name>
            <surname>Abdalla</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ahrens</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>The missing variable: Ultrasonic vocalizations reveal hidden sensitization and tolerance-like effects during long-term cocaine administration</article-title>
        <source>Psychopharmacology</source>
        <year>2012</year>
        <volume>219</volume>
        <fpage>1141</fpage>
        <lpage>1152</lpage>
        <pub-id pub-id-type="doi">10.1007/s00213-011-2445-7</pub-id>
        <pub-id pub-id-type="pmid">21870038</pub-id>
      </element-citation>
    </ref>
    <ref id="B64-brainsci-11-00864">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reno</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Thakore</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Gonzales</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Schallert</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Bell</surname>
            <given-names>R.L.</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Duvauchelle</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Alcohol-Preferring P rats emit spontaneous 22-28 khz ultrasonic vocalizations that are altered by acute and chronic alcohol experience</article-title>
        <source>Alcohol. Clin. Exp. Res.</source>
        <year>2015</year>
        <volume>39</volume>
        <fpage>843</fpage>
        <lpage>852</lpage>
        <pub-id pub-id-type="doi">10.1111/acer.12706</pub-id>
        <pub-id pub-id-type="pmid">25827842</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="brainsci-11-00864-f001" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <p>Representative spectrograms for each of the call types in <xref rid="brainsci-11-00864-t002" ref-type="table">Table 2</xref>.</p>
    </caption>
    <graphic xlink:href="brainsci-11-00864-g001"/>
  </fig>
  <table-wrap id="brainsci-11-00864-t001" orientation="portrait" position="float">
    <object-id pub-id-type="pii">brainsci-11-00864-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Sensitivity and precision of Acoustilytix and DeepSqueak for four experimental conditions across four recording environments.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Sensitivity</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Precision</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Recording Environment</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Brief Description of Study Manipulation</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"># of Hand-Score <break/>Detected USVs</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustilytix</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DeepSqueak</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><italic>p</italic>-Value</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Acoustilytix</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DeepSqueak</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><italic>p</italic>-Value</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Cocaine</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">8552</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.5</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.5</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.016</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">74.5</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">42.0</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.00001</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Ethanol</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1720</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">94.0</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">92.1</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.029</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">72.0</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">65.8</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.00008</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Morphine</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5771</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90.6</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75.8</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.00001</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83.6</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">33.1</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.00001</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Sex</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3462</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">96.4</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">97.5</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0078</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">59.5</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">46.1</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.00001</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="brainsci-11-00864-t002" orientation="portrait" position="float">
    <object-id pub-id-type="pii">brainsci-11-00864-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>Call types in the Wright et al. [<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>] classification scheme and their mapping to the five-call composite with representative spectrograms from Acoustilytix.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Wright et al. [<xref rid="B42-brainsci-11-00864" ref-type="bibr">42</xref>]</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Five-Call Composite</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flat</td>
          <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Fixed Frequency 50</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Upward Ramp</td>
          <td rowspan="9" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Frequency Modulated 50</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Downward Ramp</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Split</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Step Up</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Step Down</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Multi-step</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Inverted-U</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Complex</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Composite</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trill</td>
          <td rowspan="3" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">Frequency Modulated with Trills 50</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Flat-Trill Combo</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Trill with jumps</td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">22-kHz call</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Long 22-kHz </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short 22-kHz (&gt;300 ms)</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="brainsci-11-00864-t003" orientation="portrait" position="float">
    <object-id pub-id-type="pii">brainsci-11-00864-t003_Table 3</object-id>
    <label>Table 3</label>
    <caption>
      <p>Number of each of five call types in the five-call composite dataset classified by an expert scorer.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Call Types</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Count</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed Frequency 50</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">202</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated 50</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">342</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated with Trill 50</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">132</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Long 22-kHz</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">184</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short 22-kHz</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">71</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="brainsci-11-00864-t005" orientation="portrait" position="float">
    <object-id pub-id-type="pii">brainsci-11-00864-t005_Table 5</object-id>
    <label>Table 5</label>
    <caption>
      <p>Evaluation of the five-call composite classification model performance on the training dataset.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Five-Call Type</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Precision</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">F1-Score</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Support</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Fixed Frequency 50</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.89</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.80</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.84</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">153 </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated 50</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.82</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.93</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">230 </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frequency Modulated with Trill 50</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.81</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">90 </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Long 22-kHz</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.92</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">131 </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Short 22-kHz</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.77</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.81</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">47 </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>Overall Accuracy/Support (N-size)</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>0.87</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>651</bold>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="brainsci-11-00864-t006" orientation="portrait" position="float">
    <object-id pub-id-type="pii">brainsci-11-00864-t006_Table 6</object-id>
    <label>Table 6</label>
    <caption>
      <p>Kappa statistics for experienced and novice hand-scorers relative to an expert scorer.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Hand-scorer</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Following Unstructured Training</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Following Acoustilytix Training</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><italic>p</italic>-Value</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td colspan="4" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Experienced Scorers</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.42</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.29</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.00001</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.55</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.60</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.031</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.30</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.75</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.00001</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.36</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.69</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.00001</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.49</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.64</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.00001</td>
        </tr>
        <tr>
          <td colspan="4" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">Novice Scorers</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.47</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.42</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">NA</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
