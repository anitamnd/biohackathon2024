<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10112952</article-id>
    <article-id pub-id-type="pmid">37018156</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btad174</article-id>
    <article-id pub-id-type="publisher-id">btad174</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>K-RET: knowledgeable biomedical relation extraction system</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0597-9273</contrib-id>
        <name>
          <surname>Sousa</surname>
          <given-names>Diana F</given-names>
        </name>
        <aff><institution>Departamento de Informática, Faculdade de Ciências, Universidade de Lisboa</institution>, Lisboa 1749-016, <country country="PT">Portugal</country></aff>
        <xref rid="btad174-cor1" ref-type="corresp"/>
        <!--dfsousa@lasige.di.fc.ul.pt-->
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0627-1496</contrib-id>
        <name>
          <surname>Couto</surname>
          <given-names>Francisco M</given-names>
        </name>
        <aff><institution>Departamento de Informática, Faculdade de Ciências, Universidade de Lisboa</institution>, Lisboa 1749-016, <country country="PT">Portugal</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btad174-cor1">Corresponding author. Departamento de Informática, Faculdade de Ciências, Universidade de Lisboa, Lisboa 1749-016, Portugal. E-mail: <email>dfsousa@lasige.di.fc.ul.pt</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2023-04-05">
      <day>05</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>4</month>
      <year>2023</year>
    </pub-date>
    <volume>39</volume>
    <issue>4</issue>
    <elocation-id>btad174</elocation-id>
    <history>
      <date date-type="received">
        <day>08</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>25</day>
        <month>2</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>3</month>
        <year>2023</year>
      </date>
      <date date-type="corrected-typeset">
        <day>18</day>
        <month>4</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btad174.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Relation extraction (RE) is a crucial process to deal with the amount of text published daily, e.g. to find missing associations in a database. RE is a text mining task for which the state-of-the-art approaches use bidirectional encoders, namely, BERT. However, state-of-the-art performance may be limited by the lack of efficient external knowledge injection approaches, with a larger impact in the biomedical area given the widespread usage and high quality of biomedical ontologies. This knowledge can propel these systems forward by aiding them in predicting more explainable biomedical associations. With this in mind, we developed K-RET, a novel, knowledgeable biomedical RE system that, for the first time, injects knowledge by handling different types of associations, multiple sources and where to apply it, and multi-token entities.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We tested K-RET on three independent and open-access corpora (DDI, BC5CDR, and PGR) using four biomedical ontologies handling different entities. K-RET improved state-of-the-art results by 2.68% on average, with the DDI Corpus yielding the most significant boost in performance, from 79.30% to 87.19% in F-measure, representing a <italic toggle="yes">P</italic>-value of <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mn>2.91</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p><ext-link xlink:href="https://github.com/lasigeBioTM/K-RET" ext-link-type="uri">https://github.com/lasigeBioTM/K-RET</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>FCT</institution>
            <institution-id institution-id-type="DOI">10.13039/100006129</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>PTDC/CCIBIO/28685/2017</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>With the exponential increase in the number of research articles published in the last decades, researchers find it hard to keep up with all relevant information for their respective fields. The overwhelming number of research papers, particularly in the biomedical field, frequently makes it impossible for researchers and clinicians to be aware of all established entity associations and dissociations. This unawareness often leads to experimental repetition to prove or disprove hypotheses already studied. Further, even if the hypothesis is unique or new, the same inference could often be retrieved from knowledge about experiments done on similar entities. Automated biomedical relation extraction (RE) is a fundamental step toward aiding these researchers and clinicians in saving time and focusing their work on genuinely novel associations.</p>
    <p>Through the years, several approaches have been employed to tackle biomedical RE, from straightforward rule-based approaches (<xref rid="btad174-B22" ref-type="bibr">Rinaldi et al. 2007</xref>, <xref rid="btad174-B15" ref-type="bibr">Kilicoglu et al. 2020</xref>) to entire machine learning dedicated systems (<xref rid="btad174-B1" ref-type="bibr">Abdelkader et al. 2021</xref>, <xref rid="btad174-B12" ref-type="bibr">Houssein et al. 2021</xref>), in which deep learning played a significant role (<xref rid="btad174-B6" ref-type="bibr">Dash et al. 2020</xref>). Successively, these approaches became more specialized in targeting multiple types of biomedical associations from protein–protein relations (<xref rid="btad174-B16" ref-type="bibr">Kim et al. 2006</xref>) to human phenotype-gene causality (<xref rid="btad174-B26" ref-type="bibr">Song et al. 2019</xref>, <xref rid="btad174-B28" ref-type="bibr">Sousa and Couto 2022</xref>). BERT (<xref rid="btad174-B14" ref-type="bibr">Kenton and Toutanova 2019</xref>) and their domain derivatives, such as BioBERT (<xref rid="btad174-B18" ref-type="bibr">Lee et al. 2020</xref>), SciBERT (<xref rid="btad174-B3" ref-type="bibr">Beltagy et al. 2019</xref>), and PubMedBERT (<xref rid="btad174-B9" ref-type="bibr">Gu et al. 2022</xref>) are the current state-of-the-art employed solutions. They not only successfully extract multiple types of relations from the text but can also, using their pretrained models, be easily integrated into newer systems targeting different Natural Language Processing tasks, such as Named-Entity Recognition (<xref rid="btad174-B21" ref-type="bibr">Nasar et al. 2022</xref>), Named-Entity Linking or Normalization (<xref rid="btad174-B23" ref-type="bibr">Ruas and Couto 2022</xref>), and Question Answering (<xref rid="btad174-B8" ref-type="bibr">Do and Phan 2022</xref>).</p>
    <p>However, there are some caveats for most biomedical RE systems. The first limitation is their focus and specialization on one specific type of association (<xref rid="btad174-B13" ref-type="bibr">Hu et al. 2021</xref>), with some notable exceptions that are flexible to target more than one type (<xref rid="btad174-B26" ref-type="bibr">Song et al. 2019</xref>, <xref rid="btad174-B27" ref-type="bibr">Sousa and Couto 2020</xref>, <xref rid="btad174-B28" ref-type="bibr">2022</xref>). The second limitation is that systems’ results frequently lack an explanation. One can not easily understand how a relation was found and what inferences a particular model made to reach an output. Finally, in close connection with making systems explainable, the third more relevant limitation is their general disregard for the vast repertoire of biomedical dedicated knowledge bases, particularly in the form of ontologies. This multitude of organized biomedical knowledge is freely available, but most systems still rely only on the information from the training data. Some exemptions recently made some strides into using knowledge in the general domain (<xref rid="btad174-B20" ref-type="bibr">Liu et al. 2020</xref>) and the biomedical/clinical domain (<xref rid="btad174-B10" ref-type="bibr">Hao et al. 2020</xref>). However, these are still limited in the domain’s specificity and inflexible to adding different or more structured knowledge.</p>
    <p>Some of the most notary examples of organized biomedical knowledge are the Gene Ontology (GO; <xref rid="btad174-B2" ref-type="bibr">Ashburner et al. 2000</xref>, <xref rid="btad174-B5" ref-type="bibr">The Gene Ontology Consortium 2019</xref>), the Human Phenotype Ontology (HPO; <xref rid="btad174-B17" ref-type="bibr">Köhler et al. 2021</xref>), the Human Disease Ontology (DO; <xref rid="btad174-B24" ref-type="bibr">Schriml et al. 2022</xref>), the Chemical Entities of Biological Interest (ChEBI; <xref rid="btad174-B7" ref-type="bibr">Degtyarenko et al. 2008</xref>), and the Unified Medical Language System (UMLS; <xref rid="btad174-B4" ref-type="bibr">Bodenreider 2004</xref>). Respectively, these organized resources described connections between gene function descriptors, human phenotypes, human diseases, chemicals of biological interest, and clinical entities.</p>
    <p>The work of <xref rid="btad174-B20" ref-type="bibr">Liu et al. (2020)</xref> is a recent and successful attempt for knowledge incorporation into multiple open and specific-domain tasks. Their K-BERT system was able to incorporate domain knowledge without creating a heterogeneous embedding space. The novelty in their approach is the creation of a knowledge layer that injects knowledge directly into the sentence. Then, they perform sentence tree re-arrangement before the embedding layer, with the addition of a soft-position embedding and a visible matrix to target possible readability problems from the re-arrangement. Also, it does not require pretraining of the BERT models it supports, making it suitable for users with limited computational resources (<xref rid="btad174-B31" ref-type="bibr">Zhao et al. 2019</xref>). However, their system has some major constrictions. First, the authors do not apply their approach to the RE task for the open or biomedical-specific domains. Second, the knowledge injection can only be used for single tokens within the sentence tree, making it miss knowledge associated with multi-token entities that are most prevalent in the biomedical domain. Third, the application of their system only allows for the injection of knowledge from one knowledge source, constraining data that intersects one or more domains (e.g. patient case reports that mention both the procedures done and the phenotypes associated with the patients). Finally, there is no possibility of injecting target knowledge into the entities of interest; one must always apply knowledge to all possible sentence tokens.</p>
    <p>In this work, we designed a new approach to knowledge injection that is able to address the challenges stated in the last paragraph to create K-RET, a knowledgeable biomedical RE BERT-based system. K-RET is a flexible biomedical RE system, allowing for the use of any pretrained BERT-based system (e.g. SciBERT and BioBERT) to inject knowledge in the form of knowledge bases from a single source or multiple sources simultaneously. This knowledge can be applied to various contextualizing tokens or just to the tokens of the candidate relation for single and multi-token entities.</p>
    <p>K-RET effectively improves the performance of baseline biomedical BERT-based models by an average of 2.68% on all datasets (DDI; <xref rid="btad174-B11" ref-type="bibr">Herrero-Zazo et al. 2013</xref>, <xref rid="btad174-B25" ref-type="bibr">Segura-Bedmar et al. 2014</xref>), BC5CDR (<xref rid="btad174-B19" ref-type="bibr">Li et al. 2016</xref>), and PGR-crowd (<xref rid="btad174-B29" ref-type="bibr">Sousa et al. 2019</xref>, <xref rid="btad174-B30" ref-type="bibr">2020</xref>; Corpora). The most successful configuration is applying contextualized knowledge by adding two knowledge base entities per possible domain entity within each sentence tree (i.e. DDI Corpus plus ChEBI). This work resulted in the following main contributions:</p>
    <list list-type="bullet">
      <list-item>
        <p>K-RET, a knowledgeable biomedical RE system that allows for the integration of any pre-trained BERT-based system.</p>
      </list-item>
      <list-item>
        <p>Approach to flexible injection of knowledge with multi-options:</p>
        <list list-type="order">
          <list-item>
            <p>association of knowledge to single or multi-token entities in the sentence tree;</p>
          </list-item>
          <list-item>
            <p>add more than one knowledge source to inject knowledge of different domains;</p>
          </list-item>
          <list-item>
            <p>injection of knowledge into targeted or multiple contextualizing tokens in a sentence tree.</p>
          </list-item>
        </list>
      </list-item>
    </list>
    <p>In Section 2, we describe K-RET, formally presenting the main architectural features. In Section 3, we describe the resources used (datasets and knowledge bases), parameters, training details, and the main results of different system configurations. These configurations are the baseline (i.e. without knowledge) and the use of target versus contextualized knowledge within three different biomedical RE datasets. Sections 4 and 5 discuss the previous results with targeted ablation studies and present the main conclusions and future work.</p>
  </sec>
  <sec>
    <title>2 System and methods</title>
    <p>In this section, we will present our system K-RET, represented in <xref rid="btad174-F1" ref-type="fig">Fig. 1</xref>. We will describe each new system component and how they differ from the implementations from both BERT (<xref rid="btad174-B14" ref-type="bibr">Kenton and Toutanova 2019</xref>) and K-BERT (<xref rid="btad174-B20" ref-type="bibr">Liu et al. 2020</xref>), which was built using the UER platform (<xref rid="btad174-B31" ref-type="bibr">Zhao et al. 2019</xref>), starting by defining the general Notation and then detailing our additions in Architecture.</p>
    <fig position="float" id="btad174-F1">
      <label>Figure 1.</label>
      <caption>
        <p>The structured layer representation of K-RET. In the pipeline, we retrieved a sentence from a biomedical relation extraction dataset with delimited target entities: venlafaxine and seizure. We added a Knowledge layer to these entities from associations made with their respective domain ontologies, the ChEBI and the Human DO. In the Embedding layer, the tokens are flattened into a sequence for token embedding. Then, the soft-position embedding is used along with the token embedding, and the tokens are tagged with A for segment embedding. The orange circles correspond to visible tokens in the Seeing layer, whereas the gray circles correspond to invisible ones. For instance, in row five venlafaxine (5) is visible to all tokens except the last two: is_a (15) and abnormal nervous system physiology (16). Finally, both layers, Embedding and Seeing, are fed to the Mask-transformer, corresponding to a stack of multiple mask-self-attention blocks. This last layer is masked to prevent the transformer from receiving structural information of the sentence tree. The sentence was simplified for readability purposes</p>
      </caption>
      <graphic xlink:href="btad174f1" position="float"/>
    </fig>
    <sec>
      <title>2.1 Notation</title>
      <p>We define a sentence <italic toggle="yes">s</italic> as a sequence of tokens <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, with length <italic toggle="yes">n</italic>:
</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1" display="block" overflow="scroll">
          <mml:mrow>
            <mml:mi>s</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo>{</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mi>t</mml:mi>
              </mml:mrow>
              <mml:mn>0</mml:mn>
            </mml:msub>
            <mml:mo>,</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mi>t</mml:mi>
              </mml:mrow>
              <mml:mn>1</mml:mn>
            </mml:msub>
            <mml:mo>,</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mi>t</mml:mi>
              </mml:mrow>
              <mml:mn>2</mml:mn>
            </mml:msub>
            <mml:mo>,</mml:mo>
            <mml:mo>…</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mi>t</mml:mi>
              </mml:mrow>
              <mml:mi>n</mml:mi>
            </mml:msub>
            <mml:mo>}</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
      <p>Each token can represent one or more words that are included in the vocabulary <italic toggle="yes">V</italic>, <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula>. The Knowledge Base, <italic toggle="yes">K</italic>, consists of a collection of triples, <italic toggle="yes">e</italic>:
where <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the descriptors of the entities, <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the relation between them, <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi>e</mml:mi><mml:mo>∈</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2" display="block" overflow="scroll">
          <mml:mrow>
            <mml:mi>e</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo stretchy="false">(</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mi>t</mml:mi>
              </mml:mrow>
              <mml:mi>i</mml:mi>
            </mml:msub>
            <mml:mo>,</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mi>r</mml:mi>
              </mml:mrow>
              <mml:mi>j</mml:mi>
            </mml:msub>
            <mml:mo>,</mml:mo>
            <mml:msub>
              <mml:mrow>
                <mml:mi>t</mml:mi>
              </mml:mrow>
              <mml:mi>k</mml:mi>
            </mml:msub>
            <mml:mo stretchy="false">)</mml:mo>
          </mml:mrow>
        </mml:math>
      </disp-formula>
    </sec>
    <sec>
      <title>2.2 Architecture</title>
      <p>The model architecture of K-RET is divided into four modules. The first module, a Knowledge layer, injects knowledge from the knowledge base in triples, expanding the original sentence into a knowledgeable sentence tree. The sentence tree is fed into simultaneously the Embedding layer (second module) and the Seeing layer (third module). Then, it is converted to token-level embedding representation and a visible matrix, as presented in <xref rid="btad174-F1" ref-type="fig">Fig. 1</xref>. This matrix acts as a control to prevent the injected knowledge from altering the meaning of the original sentence with excessive knowledge. The embedding representation can then be fed into the Mask-transformer (fourth module).</p>
      <p>K-BERT created a knowledge layer that injects knowledge and performs sentence tree conversion. On the other hand, our new knowledge layer was designed to address the three previously mentioned challenges. We detail how we tackled those limitations in the next sections: Multi-token entities, Multiple knowledge bases, and Contextual and targeted knowledge. Beyond the knowledge layer, we made changes to the original predictive pipeline by allowing the addition of class weights, a new type of data format, and the inclusion of any BERT-based pretrained model using the UER framework.</p>
      <sec>
        <title>2.2.1 Multi-token entities</title>
        <p>For multi-token entities, we first considered all combinations of single tokens to generate all possible multi-tokens in the same input sentence. Thus, if a sentence has a number of tokens of 10, the number of combinations possible would be 55. This number can vary if the sentence has punctuation or other special characters. Then, through a lookup table, we can add knowledge to all combinations we can match in the chosen knowledge base, keeping the longest combinations of tokens (with increased specificity) when there is overlap. Finally, we reconstruct the sentence tree through a sliding window that goes through all the combinations with and without knowledge.</p>
        <p>We introduced a multi-token entities option to address the limitation of only using one token, both in the sentence itself and in the knowledge to be injected. As it stands, the model did not allow associating knowledge to more than one token [e.g. ‘aralkylamino compound’ (original sentence tokens) <italic toggle="yes">is_a</italic> organic amino compound (added knowledgeable token)] nor associate multiple word knowledge to one or more words in the original sentence [e.g., dopamine (original sentence token) <italic toggle="yes">is_a</italic> ‘aralkylamino compound<bold>’</bold> (added knowledgeable tokens)].</p>
      </sec>
      <sec>
        <title>2.2.2 Multiple knowledge bases</title>
        <p>To accommodate multiple knowledge bases, we expanded the number of lookup tables mentioned in the previous section. Thus, if we use more than one knowledge base, K-RET will look at the sentence a number of times corresponding to the number of knowledge bases. If there is complete or partial overlap between two or more competing knowledgeable tokens associated with an entity sentence and the number of association tokens surpasses the number of maximum tokens defined at the start, we keep the ones with the highest information content.</p>
        <p>The multiple knowledge bases option resolves the limitation of using only one knowledge base at a time. Therefore if we have, as in the biomedical domain, knowledge bases targeting different types of entities, we have to choose which one we want to use to inject knowledge into the original sentence.</p>
      </sec>
      <sec>
        <title>2.2.3 Contextual and targeted knowledge</title>
        <p>Finally, we decided to add the possibility of only injecting knowledge into the entities in consideration for relation assessment, maintaining the native option of adding knowledge to all entities present in the sentence.</p>
        <p>To define the targeted entities, we used the tags <monospace>&lt; e &gt;</monospace> and <monospace>&lt;/e &gt;</monospace> to delimit the entities in the candidate relation. The tags allowed injecting knowledge directly into the candidate entities. K-RET ignores those tags when using contextual knowledge for knowledge injection, adding knowledge to entities as described previously.</p>
        <p>Hence, as a result of the three additions, given an input sentence <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> and one or more knowledge bases, our knowledge layer outputs a sentence tree:
which results from (i) querying all entity names involved in the sentence <italic toggle="yes">s</italic>, independently from their length and selecting correspondent triples from <italic toggle="yes">K</italic>, and (ii) adding the triples to their correspondent position. <xref rid="btad174-F1" ref-type="fig">Figure 1</xref> illustrates the structure of the sentence tree <italic toggle="yes">st</italic> and an example retrieved from our data. While the sentence tree can have multiple branches, the depth is fixed to 1, not deriving branches iteratively to better preserve the original sentence meaning.</p>
        <disp-formula id="E3">
          <label>(3)</label>
          <mml:math id="M3" display="block" overflow="scroll">
            <mml:mrow>
              <mml:mi>s</mml:mi>
              <mml:mi>t</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:mo>{</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
                <mml:mn>0</mml:mn>
              </mml:msub>
              <mml:mo>,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
                <mml:mn>1</mml:mn>
              </mml:msub>
              <mml:mo>,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
                <mml:mn>2</mml:mn>
              </mml:msub>
              <mml:mo>,</mml:mo>
              <mml:mo>…</mml:mo>
              <mml:mo>,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
                <mml:mi>i</mml:mi>
              </mml:msub>
              <mml:mo>{</mml:mo>
              <mml:mo stretchy="false">(</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>r</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mn>0</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo>,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mn>0</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo stretchy="false">)</mml:mo>
              <mml:mo>,</mml:mo>
              <mml:mo>…</mml:mo>
              <mml:mo>,</mml:mo>
              <mml:mo stretchy="false">(</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>r</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo stretchy="false">)</mml:mo>
              <mml:mo>}</mml:mo>
              <mml:mo>,</mml:mo>
              <mml:mo>…</mml:mo>
              <mml:mo>,</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
                <mml:mi>n</mml:mi>
              </mml:msub>
              <mml:mo>}</mml:mo>
            </mml:mrow>
          </mml:math>
        </disp-formula>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Implementation</title>
    <p>In this article, we used three openly available RE datasets to train K-RET: DDI Corpus (<xref rid="btad174-B11" ref-type="bibr">Herrero-Zazo et al. 2013</xref>, <xref rid="btad174-B25" ref-type="bibr">Segura-Bedmar et al. 2014</xref>; drug–drug interactions), PGR-crowd Corpus (<xref rid="btad174-B29" ref-type="bibr">Sousa et al. 2019</xref>, <xref rid="btad174-B30" ref-type="bibr">2020</xref>; human phenotype–gene interactions), and BC5CDR Corpus (<xref rid="btad174-B19" ref-type="bibr">Li et al. 2016</xref>; chemical–disease associations). Along with the written information in these RE datasets, we used four knowledge bases related to the four types of entities identified within those datasets to add extra entity information to the K-RET system. These knowledge bases are HPO (<xref rid="btad174-B17" ref-type="bibr">Köhler et al. 2021</xref>), DO (<xref rid="btad174-B24" ref-type="bibr">Schriml et al. 2022</xref>), CHEBI (<xref rid="btad174-B7" ref-type="bibr">Degtyarenko et al. 2008</xref>), and GO (<xref rid="btad174-B2" ref-type="bibr">Ashburner et al. 2000</xref>, <xref rid="btad174-B5" ref-type="bibr">The Gene Ontology Consortium 2019</xref>). In this section, we present the different experiments made to access our system using the aforementioned datasets and knowledge bases, with parameters and training details tuned for the specificities of each dataset. We also explore and present the results for the usage of full knowledge or just entity knowledge, as described in the previous section.</p>
    <sec>
      <title>3.1 Datasets</title>
      <p><xref rid="btad174-T1" ref-type="table">Table 1</xref> represents the relations types and counts for each dataset. The <italic toggle="yes">no_relation</italic> label accounts for entities present in the same sentence but that do not share a relation. The commonly used DDI Corpus presents four types of relations: <italic toggle="yes">effect</italic> to describe an effect or pharmacodynamics mechanism, <italic toggle="yes">mechanism</italic> to describe a pharmacokinetic mechanism, <italic toggle="yes">advice</italic> to describe semantic relations between drugs regarding recommendation, and <italic toggle="yes">int</italic> for relations where there is no further information. Both the PGR-crowd and BC5CDR Corpora are binary in terms of relation classification. These two datasets only classify if the relation is present (<italic toggle="yes">true</italic>) or not (<italic toggle="yes">false</italic>). <xref rid="btad174-F2" ref-type="fig">Figure 2</xref> presents an example sentence for each dataset.</p>
      <fig position="float" id="btad174-F2">
        <label>Figure 2.</label>
        <caption>
          <p>Three sentence examples for each dataset, DDI Corpus, PGR-crowd Corpus, and BC5CDR Corpus. The entities in each candidate relation are linked to corresponding knowledge bases</p>
        </caption>
        <graphic xlink:href="btad174f2" position="float"/>
      </fig>
      <table-wrap position="float" id="btad174-T1">
        <label>Table 1.</label>
        <caption>
          <p>The main statistics of DDI Corpus, PGR-crowd Corpus, and BC5CDR Corpus regarding the RE task.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="3" colspan="1">Dataset</th>
              <th colspan="5" rowspan="1">Relation type<hr/></th>
            </tr>
            <tr>
              <th colspan="4" rowspan="1">True<hr/></th>
              <th rowspan="2" colspan="1">No-relation/False</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Effect</th>
              <th rowspan="1" colspan="1">Advice</th>
              <th rowspan="1" colspan="1">Mechanism</th>
              <th rowspan="1" colspan="1">Int</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">DDI</td>
              <td rowspan="1" colspan="1">2026</td>
              <td rowspan="1" colspan="1">1616</td>
              <td rowspan="1" colspan="1">1047</td>
              <td rowspan="1" colspan="1">278</td>
              <td rowspan="1" colspan="1">29 245</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PGR-crowd</td>
              <td rowspan="1" colspan="1">5498</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">626</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BC5CDR</td>
              <td rowspan="1" colspan="1">1448</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">2294</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>We applied class weights to all datasets to normalize the different types of relations and distributions. Additionally, to train and evaluate all models on the same cross-validation splits, we split each dataset into training (60%), validation (10%), and test (30%) sets. Also, our metrics presented below are displayed considering the weighted average for each type of relation to further account for the datasets’ imbalances.</p>
    </sec>
    <sec>
      <title>3.2 Knowledge bases</title>
      <p>Several knowledge bases target biomedical entities, expanding on our information about them and their inherent relationships. Some of the knowledge bases or ontologies we can link to our target entities were mentioned above and are characterized in <xref rid="btad174-T2" ref-type="table">Table 2</xref>.</p>
      <table-wrap position="float" id="btad174-T2">
        <label>Table 2.</label>
        <caption>
          <p>The main characteristics of the following knowledge bases: HPO, DO, ChEBI, and GO. All types of relations are transitive.<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Knowledge bases</th>
              <th rowspan="1" colspan="1">Number of concepts</th>
              <th rowspan="1" colspan="1">Type of relations</th>
              <th rowspan="1" colspan="1">Specific characteristics</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">HPO</td>
              <td rowspan="1" colspan="1">15 670</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-a</italic>
              </td>
              <td rowspan="1" colspan="1">Five sub-ontologies, from which <italic toggle="yes">phenotypic abnormality</italic> is the most prevalent</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DO</td>
              <td rowspan="1" colspan="1">13 355</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-a</italic>
              </td>
              <td rowspan="1" colspan="1">Human-specific</td>
            </tr>
            <tr>
              <td rowspan="10" colspan="1">ChEBI</td>
              <td rowspan="10" colspan="1">153 795</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-a</italic>
              </td>
              <td rowspan="10" colspan="1">Refers to small molecular entities</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">has-part</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-conjugate-base-of</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-conjugate-acid-of</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-tautomer-of</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-enantiomer-of</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">has-functional-parent</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">has-parent-hydride</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-substituent-group-from</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">has-role</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="6" colspan="1">GO</td>
              <td rowspan="6" colspan="1">43 613</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">is-a</italic>
              </td>
              <td rowspan="6" colspan="1">Three subontologies, molecular function, biological process, and cellular component</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">part-of</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">has-part</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">regulates</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">negatively-regulates</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">positively-regulates</italic>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <label>a</label>
            <p>All knowledge bases were consulted on 20 April 2022.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>With few code adjustments, one can easily integrate more knowledge into K-RET in the form of other knowledge bases and has the flexibility to test different combinations swiftly.</p>
    </sec>
    <sec>
      <title>3.3 Parameters and training details</title>
      <p>For each dataset, we empirically determined the best set of parameters. K-RET used a batch size of 32 for all datasets. The number of epochs ranged from 30 for DDI Corpus to 20 for PGR-crowd and BC5CDR Corpora (due to differences in dataset size). All other parameter settings were maintained from the original BERT model. The added knowledge only played a role in the fine-tuning and prediction stage.</p>
      <p>For the added knowledge, we varied the number of knowledge base entities allowed to link to each dataset entity in a candidate relation (or not). This variation ranged from two to five knowledge base entities per dataset entity. Our baseline represents the performance of the BERT-based systems without any added knowledge. When we refer to targeted knowledge, we mean just adding knowledge to entities in a candidate relation versus contextual knowledge, where we can add knowledge to all entities in the dataset. <xref rid="btad174-F3" ref-type="fig">Figure 3</xref> further elucidates the differences in the addition of knowledge. For the DDI Corpus, we used the ChEBI ontology; for the PGR-, we used the HPO and the GO ontologies; and for the BC5CDR Corpus, we used the DO and ChEBI ontologies, as represented in <xref rid="btad174-F2" ref-type="fig">Fig. 2</xref>.</p>
      <fig position="float" id="btad174-F3">
        <label>Figure 3.</label>
        <caption>
          <p>Novelties of K-RET’s knowledge injection layer. Example 1 presents a modality where we only add knowledge to entities in the candidate relation (targeted knowledge) and limit the number of knowledge base entities assigned to each entity (maximum number of Entities 3). In Example 2, we present the same sentence with the option of adding knowledge to all entities regardless if they are in the candidate relation (contextual knowledge) and limit the number of knowledge base entities assigned to each entity to two (maximum number of Entities 2). From Examples 1 to 2, we add a third knowledgeable entity (terodiline) and remove one of the knowledge base entities added (aromatic compound). The first entity can only be linked to one knowledge entity because (ventricular arrhythmia) no other association is established in the knowledge base</p>
        </caption>
        <graphic xlink:href="btad174f3" position="float"/>
      </fig>
      <p>All models were trained on three Tesla M10 GPUs, taking, on average for each model, 24 h (DDI Corpus), 1 h (PGR-crowd Corpus), and 2 h (BC5CDR Corpus). Each result presented in the following sections represents the averaged metrics for three runs except when it explicitly says otherwise, and each metric represents the weighted-averaged of the different labels.</p>
    </sec>
    <sec>
      <title>3.4 Results</title>
      <p>As mentioned in the previous sections, we divided our K-RET experiments into baseline, where we ran the BERT-based models without additional knowledge, targeted knowledge added to the entities in the candidate relation, and contextual knowledge added to all relevant entities in the sentence. We considered an entity relevant if present in the chosen knowledge source (i.e., a part of the domain knowledge considered).</p>
      <sec>
        <title>3.4.1 Baseline</title>
        <p>To choose the best-performing BERT-based biomedical model for each of the three datasets, we used four of the most widely used systems: BERT (<xref rid="btad174-B14" ref-type="bibr">Kenton and Toutanova 2019</xref>), BioBERT (<xref rid="btad174-B18" ref-type="bibr">Lee et al. 2020</xref>), SciBERT (<xref rid="btad174-B3" ref-type="bibr">Beltagy et al. 2019</xref>), and PubMedBERT (<xref rid="btad174-B9" ref-type="bibr">Gu et al. 2022</xref>) integrated into K-BERT (<xref rid="btad174-B20" ref-type="bibr">Liu et al. 2020</xref>) modified to perform RE.</p>
        <p><xref rid="btad174-T3" ref-type="table">Table 3</xref> reports the main results of testing the three datasets over the different BERT-based systems. For all three datasets, SciBERT is the best-performing system. Therefore, in our following experiments, we used SciBERT as the baseline system to which we added a knowledge layer.</p>
        <table-wrap position="float" id="btad174-T3">
          <label>Table 3.</label>
          <caption>
            <p>The baseline performance of the four models for each dataset.<xref rid="tblfn2" ref-type="table-fn"><sup>a</sup></xref></p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Dataset</th>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Precision</th>
                <th rowspan="1" colspan="1">Recall</th>
                <th rowspan="1" colspan="1">F-measure</th>
                <th rowspan="1" colspan="1">Accuracy</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="4" colspan="1">PGR-crowd</td>
                <td rowspan="1" colspan="1">BERT</td>
                <td rowspan="1" colspan="1">0.7247</td>
                <td rowspan="1" colspan="1">0.7766</td>
                <td rowspan="1" colspan="1">0.7332</td>
                <td rowspan="1" colspan="1">0.7765</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SciBERT</td>
                <td rowspan="1" colspan="1">
                  <bold>0.7680</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.8002</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.7462</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.7999</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">BioBERT</td>
                <td rowspan="1" colspan="1">0.6224</td>
                <td rowspan="1" colspan="1">0.7888</td>
                <td rowspan="1" colspan="1">0.6957</td>
                <td rowspan="1" colspan="1">0.7888</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PubMedBERT</td>
                <td rowspan="1" colspan="1">0.7201</td>
                <td rowspan="1" colspan="1">0.7834</td>
                <td rowspan="1" colspan="1">0.7191</td>
                <td rowspan="1" colspan="1">0.7833</td>
              </tr>
              <tr>
                <td rowspan="4" colspan="1">DDI</td>
                <td rowspan="1" colspan="1">BERT</td>
                <td rowspan="1" colspan="1">0.7764</td>
                <td rowspan="1" colspan="1">0.7798</td>
                <td rowspan="1" colspan="1">0.7775</td>
                <td rowspan="1" colspan="1">0.7796</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SciBERT</td>
                <td rowspan="1" colspan="1">
                  <bold>0.7906</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.7964</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.7930</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.7960</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">BioBERT</td>
                <td rowspan="1" colspan="1">0.7796</td>
                <td rowspan="1" colspan="1">0.7648</td>
                <td rowspan="1" colspan="1">0.7680</td>
                <td rowspan="1" colspan="1">0.7647</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PubMedBERT</td>
                <td rowspan="1" colspan="1">0.7801</td>
                <td rowspan="1" colspan="1">0.7227</td>
                <td rowspan="1" colspan="1">0.7473</td>
                <td rowspan="1" colspan="1">0.7227</td>
              </tr>
              <tr>
                <td rowspan="4" colspan="1">BC5CDR</td>
                <td rowspan="1" colspan="1">BERT</td>
                <td rowspan="1" colspan="1">0.5804</td>
                <td rowspan="1" colspan="1">0.5614</td>
                <td rowspan="1" colspan="1">0.5670</td>
                <td rowspan="1" colspan="1">0.5615</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">SciBERT</td>
                <td rowspan="1" colspan="1">
                  <bold>0.6266</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6364</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6289</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6363</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">BioBERT</td>
                <td rowspan="1" colspan="1">0.6150</td>
                <td rowspan="1" colspan="1">0.6132</td>
                <td rowspan="1" colspan="1">0.6142</td>
                <td rowspan="1" colspan="1">0.6131</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PubMedBERT</td>
                <td rowspan="1" colspan="1">0.6091</td>
                <td rowspan="1" colspan="1">0.6218</td>
                <td rowspan="1" colspan="1">0.6113</td>
                <td rowspan="1" colspan="1">0.6217</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn2">
              <label>a</label>
              <p>The specific models used were bert-base-uncased (BERT), scibert_scivocab_uncased (SciBERT), biobert-base-cased-v1.2 (BioBERT), and BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext (PubMedBERT).</p>
            </fn>
            <fn id="tblfn3">
              <p>The highest scores for each metric are presented in bold.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <title>3.4.2 Targeted knowledge</title>
        <p>For targeted knowledge, we added from two to five knowledge base entities to each entity in the candidate relation, as shown previously in Example 1 (<xref rid="btad174-F3" ref-type="fig">Fig. 3</xref>). Although we define the number of knowledge base entities to add, we are always limited by how many entities each entity is linked to in the knowledge base itself. <xref rid="btad174-F4" ref-type="fig">Figure 4</xref> presents the performance of the three datasets from no added knowledge (baseline) to five added entities per entity in the candidate relation.</p>
        <fig position="float" id="btad174-F4">
          <label>Figure 4.</label>
          <caption>
            <p>The performance of the targeted knowledge K-RET configuration for the three datasets in Accuracy and F-Measure (top and bottom graphs, respectively) regarding the different number of added knowledge base entities</p>
          </caption>
          <graphic xlink:href="btad174f4" position="float"/>
        </fig>
        <p><xref rid="btad174-F4" ref-type="fig">Figure 4</xref> shows that none of the K-RET models trained performs better than the baseline at any number of knowledge base-added entities. For all datasets, there is a slight decrease in performance, with the BC5CDR Corpus having a small increase in performance when the number of added knowledge base entities equals three.</p>
      </sec>
      <sec>
        <title>3.4.3 Contextual knowledge</title>
        <p>We followed the same configuration mentioned above for contextual knowledge from two to five knowledge base entities added to each relevant entity in the sentence. <xref rid="btad174-F5" ref-type="fig">Figure 5</xref> presents the performance of the three datasets from no added knowledge (baseline) to five added entities per relevant entity.</p>
        <fig position="float" id="btad174-F5">
          <label>Figure 5.</label>
          <caption>
            <p>The performance of the contextual knowledge K-RET configuration for the three datasets n Accuracy and F-Measure (top and bottom graphs, respectively) regarding the different number of added knowledge base entities</p>
          </caption>
          <graphic xlink:href="btad174f5" position="float"/>
        </fig>
        <p><xref rid="btad174-F5" ref-type="fig">Figure 5</xref> shows a performance increase from baseline to K-RET on the DDI and BC5CDR Corpora. For the PGR-crowd Corpus, while the performance is better than for targeted knowledge, the baseline performance still outperforms K-RET. Similarly, the BC5CDR’s best performance is when the number of added knowledge base entities equals three, but this time surpasses the baseline.</p>
        <p><xref rid="btad174-T4" ref-type="table">Table 4</xref> presents the best results for each main model configuration (targeted and contextual), including the contextual knowledge configuration over 10 runs to determine the statistical significance of the best configuration accurately and the corresponding baseline considered previously and regarding the majority label. The baseline<inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mtext>ML</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the results if we assign the same label (majority label) to all the test relations.</p>
        <table-wrap position="float" id="btad174-T4">
          <label>Table 4.</label>
          <caption>
            <p>The final K-RET performance results.<xref rid="tblfn3" ref-type="table-fn"><sup>a</sup></xref></p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Dataset</th>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Precision</th>
                <th rowspan="1" colspan="1">Recall</th>
                <th rowspan="1" colspan="1">F-measure</th>
                <th rowspan="1" colspan="1">Accuracy</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="5" colspan="1">PGR-crowd</td>
                <td rowspan="1" colspan="1">Baseline<inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mtext>ML</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td>
                <td rowspan="1" colspan="1">0.6222</td>
                <td rowspan="1" colspan="1">0.7888</td>
                <td rowspan="1" colspan="1">0.6957</td>
                <td rowspan="1" colspan="1">0.7888</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Baseline</td>
                <td rowspan="1" colspan="1">
                  <bold>0.7680</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.8002</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.7462</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.7999</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TK-RET</td>
                <td rowspan="1" colspan="1">0.7891</td>
                <td rowspan="1" colspan="1">0.7914</td>
                <td rowspan="1" colspan="1">0.7100</td>
                <td rowspan="1" colspan="1">0.7917</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET</td>
                <td rowspan="1" colspan="1">0.7614</td>
                <td rowspan="1" colspan="1">0.7960</td>
                <td rowspan="1" colspan="1">0.7367</td>
                <td rowspan="1" colspan="1">0.7962</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET<sub>10</sub></td>
                <td rowspan="1" colspan="1">0.7441</td>
                <td rowspan="1" colspan="1">0.7933</td>
                <td rowspan="1" colspan="1">0.7206</td>
                <td rowspan="1" colspan="1">0.7934</td>
              </tr>
              <tr>
                <td rowspan="5" colspan="1">DDI</td>
                <td rowspan="1" colspan="1">Baseline<inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td>
                <td rowspan="1" colspan="1">0.7267</td>
                <td rowspan="1" colspan="1">0.8525</td>
                <td rowspan="1" colspan="1">0.7846</td>
                <td rowspan="1" colspan="1">0.8525</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Baseline</td>
                <td rowspan="1" colspan="1">0.7906</td>
                <td rowspan="1" colspan="1">0.7964</td>
                <td rowspan="1" colspan="1">0.7930</td>
                <td rowspan="1" colspan="1">0.7960</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TK-RET</td>
                <td rowspan="1" colspan="1">0.8132</td>
                <td rowspan="1" colspan="1">0.7820</td>
                <td rowspan="1" colspan="1">0.7930</td>
                <td rowspan="1" colspan="1">0.7823</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET</td>
                <td rowspan="1" colspan="1">0.8674</td>
                <td rowspan="1" colspan="1">0.8791</td>
                <td rowspan="1" colspan="1">0.8690</td>
                <td rowspan="1" colspan="1">0.8788</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET<sub>10</sub></td>
                <td rowspan="1" colspan="1">
                  <bold>0.8704</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.8805</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.8719</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.8804</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="5" colspan="1">BC5CDR</td>
                <td rowspan="1" colspan="1">Baseline<inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></td>
                <td rowspan="1" colspan="1">0.3803</td>
                <td rowspan="1" colspan="1">0.6167</td>
                <td rowspan="1" colspan="1">0.4705</td>
                <td rowspan="1" colspan="1">0.6167</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Baseline</td>
                <td rowspan="1" colspan="1">0.6266</td>
                <td rowspan="1" colspan="1">0.6364</td>
                <td rowspan="1" colspan="1">0.6289</td>
                <td rowspan="1" colspan="1">0.6363</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TK-RET</td>
                <td rowspan="1" colspan="1">0.5838</td>
                <td rowspan="1" colspan="1">0.5977</td>
                <td rowspan="1" colspan="1">0.5816</td>
                <td rowspan="1" colspan="1">0.5980</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET</td>
                <td rowspan="1" colspan="1">
                  <bold>0.6412</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6499</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6428</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.6498</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET<sub>10</sub></td>
                <td rowspan="1" colspan="1">0.6317</td>
                <td rowspan="1" colspan="1">0.6348</td>
                <td rowspan="1" colspan="1">0.6309</td>
                <td rowspan="1" colspan="1">0.6347</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn4">
              <label>a</label>
              <p>The best-performing models for each dataset concerning baseline considering the majority label, baseline, targeted knowledge, contextual knowledge, and contextual knowledge over ten runs. To facilitate table readability, we omitted the standard deviation values that range from 0.005 to 0.029 across all datasets due to the low impact these have on the interpretation of the final results.</p>
            </fn>
            <fn id="tblfn5">
              <p>The highest scores for each metric are presented in bold.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>The results show that the contextual knowledge configuration over three runs prevails as the best model in two datasets (DDI and BC5CDR), with the targeted knowledge configuration not surpassing the baseline for any dataset. However, the BC5CDR Corpus presents a p-value of 0.8108 for F-measure and 0.8681 for accuracy (<italic toggle="yes">α</italic> = .05), demonstrating a lack of significant difference between the means of Baseline and CK-RET<sub>10</sub>. Nonetheless, the DDI Corpus presents a p-value of <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mn>2.91</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for F-measure and <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mn>1.44</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> for accuracy (<italic toggle="yes">α</italic> = .05), validating a significant difference between the means of Baseline and CK-RET<sub>10</sub>. The <italic toggle="yes">P</italic>-values were determined using a one-tailed <italic toggle="yes">t</italic>-test.</p>
        <p><xref rid="btad174-T5" ref-type="table">Table 5</xref> reflects the specific performance of the DDI Corpus per type of relation (<italic toggle="yes">effect</italic>, <italic toggle="yes">advice</italic>, <italic toggle="yes">mechanism</italic>, <italic toggle="yes">int</italic>, and <italic toggle="yes">false</italic>) for each main model configuration. <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables SA1 and SA2</xref> present the same results for PGR-crowd and BC5CDR Corpora. For the DDI Corpus, we also performed ablation regarding multi-token entities by limiting the association of knowledge to the first entity within the multi-token considered. We obtained an F-measure of 0.8663 and an accuracy of 0.8732.</p>
        <table-wrap position="float" id="btad174-T5">
          <label>Table 5.</label>
          <caption>
            <p>DDI Corpus performance per type of relation.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="2" colspan="1">Metrics</th>
                <th rowspan="2" colspan="1">Model</th>
                <th colspan="5" rowspan="1">Type<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">Effect</th>
                <th rowspan="1" colspan="1">Advice</th>
                <th rowspan="1" colspan="1">Mechanism</th>
                <th rowspan="1" colspan="1">Int</th>
                <th rowspan="1" colspan="1">False</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="4" colspan="1">Precision</td>
                <td rowspan="1" colspan="1">Baseline</td>
                <td rowspan="1" colspan="1">0.1770</td>
                <td rowspan="1" colspan="1">0.0890</td>
                <td rowspan="1" colspan="1">0.2720</td>
                <td rowspan="1" colspan="1">0.2800</td>
                <td rowspan="1" colspan="1">0.8060</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TK-RET</td>
                <td rowspan="1" colspan="1">0.2747</td>
                <td rowspan="1" colspan="1">0.3053</td>
                <td rowspan="1" colspan="1">0.3623</td>
                <td rowspan="1" colspan="1">0.2360</td>
                <td rowspan="1" colspan="1">0.9020</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET</td>
                <td rowspan="1" colspan="1">0.5137</td>
                <td rowspan="1" colspan="1">
                  <bold>0.6720</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.5757</bold>
                </td>
                <td rowspan="1" colspan="1">0.5963</td>
                <td rowspan="1" colspan="1">0.9163</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET<sub>10</sub></td>
                <td rowspan="1" colspan="1">
                  <bold>0.5524</bold>
                </td>
                <td rowspan="1" colspan="1">0.6370</td>
                <td rowspan="1" colspan="1">0.5411</td>
                <td rowspan="1" colspan="1">
                  <bold>0.6659</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9194</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="4" colspan="1">Recall</td>
                <td rowspan="1" colspan="1">Baseline</td>
                <td rowspan="1" colspan="1">0.2000</td>
                <td rowspan="1" colspan="1">0.0730</td>
                <td rowspan="1" colspan="1">0.1810</td>
                <td rowspan="1" colspan="1">0.3850</td>
                <td rowspan="1" colspan="1">0.9040</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TK-RET</td>
                <td rowspan="1" colspan="1">0.4677</td>
                <td rowspan="1" colspan="1">0.3490</td>
                <td rowspan="1" colspan="1">0.5127</td>
                <td rowspan="1" colspan="1">0.0160</td>
                <td rowspan="1" colspan="1">0.8487</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET</td>
                <td rowspan="1" colspan="1">0.4533</td>
                <td rowspan="1" colspan="1">0.2937</td>
                <td rowspan="1" colspan="1">0.4950</td>
                <td rowspan="1" colspan="1">
                  <bold>0.4340</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9600</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET<sub>10</sub></td>
                <td rowspan="1" colspan="1">
                  <bold>0.4758</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.3226</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.5318</bold>
                </td>
                <td rowspan="1" colspan="1">0.3946</td>
                <td rowspan="1" colspan="1">0.9579</td>
              </tr>
              <tr>
                <td rowspan="4" colspan="1">F-measure</td>
                <td rowspan="1" colspan="1">Baseline</td>
                <td rowspan="1" colspan="1">0.1880</td>
                <td rowspan="1" colspan="1">0.0800</td>
                <td rowspan="1" colspan="1">0.2170</td>
                <td rowspan="1" colspan="1">0.3250</td>
                <td rowspan="1" colspan="1">0.9000</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">TK-RET</td>
                <td rowspan="1" colspan="1">0.3423</td>
                <td rowspan="1" colspan="1">0.3240</td>
                <td rowspan="1" colspan="1">0.4187</td>
                <td rowspan="1" colspan="1">0.0293</td>
                <td rowspan="1" colspan="1">0.8743</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET</td>
                <td rowspan="1" colspan="1">0.4810</td>
                <td rowspan="1" colspan="1">0.4077</td>
                <td rowspan="1" colspan="1">0.5317</td>
                <td rowspan="1" colspan="1">
                  <bold>0.5017</bold>
                </td>
                <td rowspan="1" colspan="1">0.9377</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CK-RET<sub>10</sub></td>
                <td rowspan="1" colspan="1">
                  <bold>0.5099</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.4266</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.5329</bold>
                </td>
                <td rowspan="1" colspan="1">0.4924</td>
                <td rowspan="1" colspan="1">
                  <bold>0.9384</bold>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>From the experiences conducted throughout the previous section, we can infer that adding knowledge can significantly improve the state-of-the-art performance of one of the most used biomedical RE datasets, mainly when using contextual knowledge. However, there is a difference in how significantly we can improve performance, from nothing (PGR-crowd Corpus) to over 0.076 percentage points (DDI Corpus). Several factors can explain the differences in performance for the different datasets, such as dataset size and label distribution, knowledge base size and coverage of the dataset entities, or the average number of knowledge base entities that can be linked to each dataset entity.</p>
    <p>For the PGR-crowd Corpus, while all entities are linked to one of two knowledge bases, the HPO or the GO, the linking to the GO is already second handed. In the PGR-crowd Corpus, the authors recognized gene entities and posteriorly linked them to their most representative GO term. We do not use their gene identifiers directly because these biomedical entities are not directly represented in any hierarchic knowledge base. Thus, we used the GO terms to add further information and, consequently, distanced ourselves from the original gene entity. However, what made the most significant impact in the lack of increase in performance is the distribution of labels being predominantly <italic toggle="yes">true</italic>. Identifying undetected <italic toggle="yes">true</italic> relations is more challenging than if the dataset was more balanced (e.g. the BC5CDR Corpus) or with the inverse distribution (e.g. the DDI Corpus). Also, for the specific case of PGR-crowd Corpus, there could be ambiguity if an acronym describes a human phenotype or a gene since these can often overlap. This ambiguity is an open problem, and in this work, we opted not to consider human phenotype entities acronyms for contextual knowledge, which could be made us lose valuable information.</p>
    <p>With the BC5CDR Corpus, K-RET only surpassed the baseline when adding contextual knowledge by slightly over 1% in both F-measure and accuracy and was unsuccessful at demonstrating a significant difference between the baseline and the best-performing configuration. Although the impact on performance is not preeminent, the fact that this dataset is denser in the number of relevant entities per sentence and more evenly distributed than the PGR-crowd Corpus increases the impact of the addition of knowledge. This behavior occurs when knowledge is added directly to the candidate relation’s target entities and peripherally to other relevant entities.</p>
    <p>The DDI Corpus is unbalanced in favor of <italic toggle="yes">no_relation</italic>/<italic toggle="yes">false</italic> labels. Therefore, finding <italic toggle="yes">true</italic> relations is more challenging. This dataset’s distribution of labels is ideal for increasing the impact of adding knowledge. The label distribution, the highly dense text in relevant entities, and the ampler size of the knowledge base used, ChEBI, all contribute to the rise in performance by adding contextual knowledge. The improvements from baseline were 7.60% for contextual knowledge in F-measure. We also verified that adding knowledge made a bigger impact on system performance than using multi-token entities by contributing to an average improvement of 0.0704 data points in F-measure and 0.0756 in accuracy.</p>
    <p>K-RET was able to design a more efficient and flexible knowledge layer than the previous attempt in the work of <xref rid="btad174-B20" ref-type="bibr">Liu et al. (2020)</xref> by applying knowledge injection to the biomedical RE task. First, by proving the utility of contextualizing tokens, with substantial improvements in one of the three datasets described above from targeted to contextual token usage. Second is the possibility of adding more than one knowledge source to cover the domains of all relevant entities mentioned in the training data. Third, by incorporating the option of adding knowledge to multi-token entities instead of only single token. We determined experimentally for the DDI Corpus the multi-token approach to be more thorough, which can be explained by those entities constituting the majority of biomedical entities described in the datasets used for testing.</p>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>This article proposed a new biomedical RE system, K-RET, that incorporates knowledge in the form of ontologies into BERT-based systems to enrich and complement the data used for training. K-RET is a flexible system that can handle different associations, integrate knowledge from multiple sources, define where to apply the knowledge, and deal with multi-token entities. We used three independent and open-access RE datasets to test our system concerning different types of biomedical entities with different characteristics (i.e. label distribution). Allied with the three datasets, we used four knowledge bases to inject knowledge according to the type of entities involved in the candidate relations. The best-performing dataset was the DDI Corpus allied with the ChEBI knowledge base, with significant average improvements compared with the baseline in the order of 7.60% and 8.28% for F-measure and accuracy, respectively. For the BC5CDR Corpus, we also had modest improvements (an average of 1.39% and 1.35% for F-measure and accuracy), even though we did not find these results significant. Differently, for the PGR-crowd Corpus, there was no significant change in performance from the addition of knowledge. Thus, we concluded that the label distribution and relevant entity density within the training data significantly affect how our system performs. Nevertheless, we successfully demonstrated the power of adding external knowledge to training data and how to accommodate it to different domain data.</p>
    <p>In a nutshell, the K-RET system successfully added contextual knowledge, more than one knowledge source, and knowledge to multi-token entities.</p>
    <p>For future work, we would like to explore the impact of different knowledge bases and combinations within the ones we already used. Also, we would like to analyze further and examine the implications of peripherical contextual entities on assigning a label for a candidate relation and entities overlapping within different knowledge bases.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btad174_Supplementary_Data</label>
      <media xlink:href="btad174_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <sec>
    <title>Supplementary data</title>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> is available at <italic toggle="yes">Bioinformatics</italic> online.</p>
    <p>Conflict of interest: None declared.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by FCT through Deep Semantic Tagger (DeST) Project under [Grant PTDC/CCIBIO/28685/2017 (<ext-link xlink:href="http://dest.rd.ciencias.ulisboa.pt/" ext-link-type="uri">http://dest.rd.ciencias.ulisboa.pt/</ext-link>)], in part by LASIGE Research Unit under [Grants UIDB/00408/2020 and UIDP/00408/2020], and in part by FCT and FSE through Ph.D. Scholarship under [Grant SFRH/BD/145221/2019].</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>The datasets and biomedical ontologies were derived from sources in the public domain:</p>
    <p>PGR-crowd Corpus: <ext-link xlink:href="https://github.com/lasigeBioTM/PGR-crowd" ext-link-type="uri">https://github.com/lasigeBioTM/PGR-crowd</ext-link>.</p>
    <p>DDI Corpus: <ext-link xlink:href="https://github.com/isegura/DDICorpus" ext-link-type="uri">https://github.com/isegura/DDICorpus</ext-link>.</p>
    <p>BC5CDR Corpus: <ext-link xlink:href="https://github.com/JHnlp/BioCreative-V-CDR-Corpus" ext-link-type="uri">https://github.com/isegura/DDICorpus</ext-link>.</p>
    <p>Human Phenotype Ontology (HPO): <ext-link xlink:href="https://hpo.jax.org/app/data/ontology" ext-link-type="uri">https://hpo.jax.org/app/data/ontology</ext-link>.</p>
    <p>Human Disease Ontology (DO): <ext-link xlink:href="https://www.ebi.ac.uk/ols/ontologies/doid" ext-link-type="uri">https://www.ebi.ac.uk/ols/ontologies/doid</ext-link>.</p>
    <p>Chemical Entities of Biological Interest (ChEBI): <ext-link xlink:href="https://www.ebi.ac.uk/chebi/" ext-link-type="uri">https://www.ebi.ac.uk/chebi/</ext-link>.</p>
    <p>Gene Ontology (GO): <ext-link xlink:href="http://geneontology.org/" ext-link-type="uri">http://geneontology.org/</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btad174-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abdelkader</surname><given-names>W</given-names></string-name>, <string-name><surname>Navarro</surname><given-names>T</given-names></string-name>, <string-name><surname>Parrish</surname><given-names>R</given-names></string-name></person-group><etal>et al</etal><article-title>Machine learning approaches to retrieve high-quality, clinically relevant evidence from the biomedical literature: systematic review</article-title>. <source>JMIR Med Inform</source><year>2021</year>;<volume>9</volume>:<fpage>e30401</fpage>.<pub-id pub-id-type="pmid">34499041</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ashburner</surname><given-names>M</given-names></string-name>, <string-name><surname>Ball</surname><given-names>CA</given-names></string-name>, <string-name><surname>Blake</surname><given-names>JA</given-names></string-name></person-group><etal>et al</etal><article-title>Gene ontology: tool for the unification of biology</article-title>. <source>Nat Genet</source><year>2000</year>;<volume>25</volume>:<fpage>25</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">10802651</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Beltagy</surname><given-names>I</given-names></string-name>, <string-name><surname>Lo</surname><given-names>K</given-names></string-name>, <string-name><surname>Cohan</surname><given-names>A.</given-names></string-name></person-group> SciBERT: A pretrained language model for scientific text. In: <italic toggle="yes">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</italic>, pp. <fpage>3615</fpage>–<lpage>20</lpage>, Hong Kong, China: Association for Computational Linguistics, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="btad174-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bodenreider</surname><given-names>O.</given-names></string-name></person-group><article-title>The unified medical language system (UMLS): integrating biomedical terminology</article-title>. <source>Nucleic Acids Res</source><year>2004</year>;<volume>32</volume>:<fpage>D267</fpage>–<lpage>70</lpage>.<pub-id pub-id-type="pmid">14681409</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B5">
      <mixed-citation publication-type="journal"><collab>The Gene Ontology Consortium</collab>. <article-title>The gene ontology resource: 20 years and still going strong</article-title>. <source>Nucleic Acids Res</source><year>2019</year>;<volume>47</volume>:<fpage>D330</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">30395331</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Dash</surname><given-names>S</given-names></string-name>, <string-name><surname>Acharya</surname><given-names>BR</given-names></string-name>, <string-name><surname>Mittal</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><year>2020</year>. <source>Deep Learning Techniques for Biomedical and Health Informatics</source>. Cham, Switzerland: <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btad174-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Degtyarenko</surname><given-names>K</given-names></string-name>, <string-name><surname>De Matos</surname><given-names>P</given-names></string-name>, <string-name><surname>Ennis</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>ChEBI: a database and ontology for chemical entities of biological interest</article-title>. <source>Nucleic Acids Res</source><year>2008</year>;<volume>36</volume>:<fpage>D344</fpage>–<lpage>50</lpage>.<pub-id pub-id-type="pmid">17932057</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Do</surname><given-names>P</given-names></string-name>, <string-name><surname>Phan</surname><given-names>TH.</given-names></string-name></person-group><article-title>Developing a BERT based triple classification model using knowledge graph embedding for question answering system</article-title>. <source>Appl Intell</source><year>2022</year>;<volume>52</volume>:<fpage>636</fpage>–<lpage>51</lpage>.</mixed-citation>
    </ref>
    <ref id="btad174-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tinn</surname><given-names>R</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>H</given-names></string-name></person-group><etal>et al</etal><article-title>Domain-specific language model pretraining for biomedical natural language processing</article-title>. <source>ACM Trans Comput Healthc</source><year>2022</year>;<volume>3</volume>:<fpage>1</fpage>–<lpage>23</lpage>.</mixed-citation>
    </ref>
    <ref id="btad174-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Hao</surname><given-names>B</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>H</given-names></string-name>, <string-name><surname>Paschalidis</surname><given-names>IC.</given-names></string-name></person-group> Enhancing clinical bert embedding using a biomedical knowledge base. In: <italic toggle="yes">28th International Conference on Computational Linguistics (COLING 2020)</italic>, <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="btad174-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Herrero-Zazo</surname><given-names>M</given-names></string-name>, <string-name><surname>Segura-Bedmar</surname><given-names>I</given-names></string-name>, <string-name><surname>Martínez</surname><given-names>P</given-names></string-name></person-group><etal>et al</etal><article-title>The ddi corpus: an annotated corpus with pharmacological substances and drug–drug interactions</article-title>. <source>J Biomed Inform</source><year>2013</year>;<volume>46</volume>:<fpage>914</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">23906817</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Houssein</surname><given-names>EH</given-names></string-name>, <string-name><surname>Mohamed</surname><given-names>RE</given-names></string-name>, <string-name><surname>Ali</surname><given-names>AA.</given-names></string-name></person-group><article-title>Machine learning techniques for biomedical natural language processing: a comprehensive review</article-title>. <source>IEEE Access</source><year>2021</year>;<volume>9</volume>:<fpage>140628</fpage>–<lpage>53</lpage>.</mixed-citation>
    </ref>
    <ref id="btad174-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname><given-names>L</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y-A</given-names></string-name></person-group><etal>et al</etal><article-title>A survey on computational models for predicting protein–protein interactions</article-title>. <source>Brief Bioinform</source><year>2021</year>;<volume>22</volume>:<fpage>bbab036</fpage>.<pub-id pub-id-type="pmid">33693513</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kenton</surname><given-names>JDM-WC</given-names></string-name>, <string-name><surname>Toutanova</surname><given-names>LK.</given-names></string-name></person-group> BERT: Pre-training of deep bidirectional transformers for language understanding. In: <italic toggle="yes">Proceedings of NAACL-HLT</italic>, pp. <fpage>4171</fpage>–<lpage>86,</lpage><year>2019</year>.</mixed-citation>
    </ref>
    <ref id="btad174-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kilicoglu</surname><given-names>H</given-names></string-name>, <string-name><surname>Rosemblat</surname><given-names>G</given-names></string-name>, <string-name><surname>Fiszman</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>Broad-coverage biomedical relation extraction with semrep</article-title>. <source>BMC Bioinformatics</source><year>2020</year>;<volume>21</volume>:<fpage>1</fpage>–<lpage>28</lpage>.<pub-id pub-id-type="pmid">31898485</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kim</surname><given-names>J-J</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z</given-names></string-name>, <string-name><surname>Park</surname><given-names>JC</given-names></string-name></person-group><etal>et al</etal><article-title>Biocontrasts: extracting and exploiting protein–protein contrastive relations from biomedical literature</article-title>. <source>Bioinformatics</source><year>2006</year>;<volume>22</volume>:<fpage>597</fpage>–<lpage>605</lpage>.<pub-id pub-id-type="pmid">16368768</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Köhler</surname><given-names>S</given-names></string-name>, <string-name><surname>Gargano</surname><given-names>M</given-names></string-name>, <string-name><surname>Matentzoglu</surname><given-names>N</given-names></string-name></person-group><etal>et al</etal><article-title>The human phenotype ontology in 2021</article-title>. <source>Nucleic Acids Res</source><year>2021</year>;<volume>49</volume>:<fpage>D1207</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">33264411</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>J</given-names></string-name>, <string-name><surname>Yoon</surname><given-names>W</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S</given-names></string-name></person-group><etal>et al</etal><article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>. <source>Bioinformatics</source><year>2020</year>;<volume>36</volume>:<fpage>1234</fpage>–<lpage>40</lpage>.<pub-id pub-id-type="pmid">31501885</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>J</given-names></string-name>, <string-name><surname>Sun</surname><given-names>Y</given-names></string-name>, <string-name><surname>Johnson</surname><given-names>RJ</given-names></string-name></person-group><etal>et al</etal><article-title>Biocreative v cdr task corpus: a resource for chemical disease relation extraction</article-title>. <source>Database</source><year>2016</year>;<volume>2016</volume>:<fpage>baw068</fpage>.<pub-id pub-id-type="pmid">27161011</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>W</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>P</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>Z</given-names></string-name></person-group><etal>et al</etal> K-BERT: Enabling language representation with knowledge graph. In: <italic toggle="yes">Proceedings of the AAAI Conference on Artificial Intelligence</italic>, Vol. <volume>34</volume>, pp. <fpage>2901</fpage>–<lpage>8</lpage>, <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="btad174-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nasar</surname><given-names>Z</given-names></string-name>, <string-name><surname>Jaffry</surname><given-names>SW</given-names></string-name>, <string-name><surname>Malik</surname><given-names>MK.</given-names></string-name></person-group><article-title>Named entity recognition and relation extraction: state-of-the-art</article-title>. <source>ACM Comput Surv</source><year>2022</year>;<volume>54</volume>:<fpage>1</fpage>–<lpage>39</lpage>.</mixed-citation>
    </ref>
    <ref id="btad174-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rinaldi</surname><given-names>F</given-names></string-name>, <string-name><surname>Schneider</surname><given-names>G</given-names></string-name>, <string-name><surname>Kaljurand</surname><given-names>K</given-names></string-name></person-group><etal>et al</etal><article-title>Mining of relations between proteins over biomedical scientific literature using a deep-linguistic approach</article-title>. <source>Artif Intell Med</source><year>2007</year>;<volume>39</volume>:<fpage>127</fpage>–<lpage>36</lpage>.<pub-id pub-id-type="pmid">17052900</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ruas</surname><given-names>P</given-names></string-name>, <string-name><surname>Couto</surname><given-names>FM.</given-names></string-name></person-group><article-title>NILINKER: attention-based approach to NIL entity linking</article-title>. <source>J Biomed Inform</source><year>2022</year>;<volume>132</volume>:<fpage>104137</fpage>.<pub-id pub-id-type="pmid">35811025</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schriml</surname><given-names>LM</given-names></string-name>, <string-name><surname>Munro</surname><given-names>JB</given-names></string-name>, <string-name><surname>Schor</surname><given-names>M</given-names></string-name></person-group><etal>et al</etal><article-title>The human disease ontology 2022 update</article-title>. <source>Nucleic Acids Res</source><year>2022</year>;<volume>50</volume>:<fpage>D1255</fpage>–<lpage>61</lpage>.<pub-id pub-id-type="pmid">34755882</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segura-Bedmar</surname><given-names>I</given-names></string-name>, <string-name><surname>Martínez</surname><given-names>P</given-names></string-name>, <string-name><surname>Herrero-Zazo</surname><given-names>M.</given-names></string-name></person-group><article-title>Lessons learnt from the ddiextraction-2013 shared task</article-title>. <source>J Biomed Inform</source><year>2014</year>;<volume>51</volume>:<fpage>152</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">24858490</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Song</surname><given-names>L</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Gildea</surname><given-names>D</given-names></string-name></person-group><etal>et al</etal> Leveraging dependency forest for neural medical relation extraction. In: <italic toggle="yes">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</italic>, pp. <fpage>208</fpage>–<lpage>18,</lpage><year>2019</year>.</mixed-citation>
    </ref>
    <ref id="btad174-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Sousa</surname><given-names>D</given-names></string-name>, <string-name><surname>Couto</surname><given-names>FM.</given-names></string-name></person-group> BiOnt: deep learning using multiple biomedical ontologies for relation extraction. In: <italic toggle="yes">European Conference on Information Retrieval</italic>, pp. <fpage>367</fpage>–<lpage>74</lpage>. Springer, <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="btad174-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sousa</surname><given-names>D</given-names></string-name>, <string-name><surname>Couto</surname><given-names>FM.</given-names></string-name></person-group><article-title>Biomedical relation extraction with knowledge graph-based recommendations</article-title>. <source>IEEE J Biomed Health Inform</source><year>2022</year>;<volume>26</volume>:<fpage>4207</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">35536818</pub-id></mixed-citation>
    </ref>
    <ref id="btad174-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Sousa</surname><given-names>D</given-names></string-name>, <string-name><surname>Lamurias</surname><given-names>A</given-names></string-name>, <string-name><surname>Couto</surname><given-names>FM.</given-names></string-name></person-group> A silver standard corpus of human phenotype-gene relations. In: <italic toggle="yes">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</italic>, Vol. 1 (Long and Short Papers), pp. <fpage>1487</fpage>–<lpage>92</lpage>, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="btad174-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sousa</surname><given-names>D</given-names></string-name>, <string-name><surname>Lamurias</surname><given-names>A</given-names></string-name>, <string-name><surname>Couto</surname><given-names>FM.</given-names></string-name></person-group><article-title>A hybrid approach toward biomedical relation extraction training corpora: combining distant supervision with crowdsourcing</article-title>. <source>Database</source>, pp. 1–15. <year>2020</year>;<volume>2020</volume>.</mixed-citation>
    </ref>
    <ref id="btad174-B31">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Zhao</surname><given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname><given-names>H</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>J</given-names></string-name></person-group><etal>et al</etal> UER: An open-source toolkit for pre-training models. In: <italic toggle="yes">EMNLP-IJCNLP 2019</italic>, p. <fpage>241,</fpage><year>2019</year>.</mixed-citation>
    </ref>
  </ref-list>
</back>
