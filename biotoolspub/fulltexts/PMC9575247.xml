<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9575247</article-id>
    <article-id pub-id-type="publisher-id">4980</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-04980-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep learning approach for cancer subtype classification using high-dimensional gene expression data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Shen</surname>
          <given-names>Jiquan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shi</surname>
          <given-names>Jiawei</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Luo</surname>
          <given-names>Junwei</given-names>
        </name>
        <address>
          <email>luojunwei@hpu.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhai</surname>
          <given-names>Haixia</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Xiaoyan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Zhengjiang</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yan</surname>
          <given-names>Chaokun</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Luo</surname>
          <given-names>Huimin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.412097.9</institution-id><institution-id institution-id-type="ISNI">0000 0000 8645 6375</institution-id><institution>School of Software, </institution><institution>Henan Polytechnic University, </institution></institution-wrap>Jiaozuo, 454003 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.256922.8</institution-id><institution-id institution-id-type="ISNI">0000 0000 9139 560X</institution-id><institution>School of Computer and Information Engineering, </institution><institution>Henan University, </institution></institution-wrap>Kaifeng, 475001 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>17</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>430</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Motivation</title>
        <p id="Par1">Studies have shown that classifying cancer subtypes can provide valuable information for a range of cancer research, from aetiology and tumour biology to prognosis and personalized treatment. Current methods usually adopt gene expression data to perform cancer subtype classification. However, cancer samples are scarce, and the high-dimensional features of their gene expression data are too sparse to allow most methods to achieve desirable classification results.
</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we propose a deep learning approach by combining a convolutional neural network (CNN) and bidirectional gated recurrent unit (BiGRU): our approach, DCGN, aims to achieve nonlinear dimensionality reduction and learn features to eliminate irrelevant factors in gene expression data. Specifically, DCGN first uses the synthetic minority oversampling technique algorithm to equalize data. The CNN can handle high-dimensional data without stress and extract important local features, and the BiGRU can analyse deep features and retain their important information; the DCGN captures key features by combining both neural networks to overcome the challenges of small sample sizes and sparse, high-dimensional features. In the experiments, we compared the DCGN to seven other cancer subtype classification methods using breast and bladder cancer gene expression datasets. The experimental results show that the DCGN performs better than the other seven methods and can provide more satisfactory classification results.</p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-022-04980-9.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Cancer subtype</kwd>
      <kwd>Classification</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Natural Science Foundation of China</institution>
        </funding-source>
        <award-id>61972134</award-id>
        <principal-award-recipient>
          <name>
            <surname>Luo</surname>
            <given-names>Junwei</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Young Elite Teachers in Henan Province</institution>
        </funding-source>
        <award-id>2020GGJS050</award-id>
        <principal-award-recipient>
          <name>
            <surname>Luo</surname>
            <given-names>Junwei</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Doctor Foundation of Henan Polytechnic University</institution>
        </funding-source>
        <award-id>B2018-36</award-id>
        <principal-award-recipient>
          <name>
            <surname>Luo</surname>
            <given-names>Junwei</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Innovative and Scientific Research Team of Henan Polytechnic University</institution>
        </funding-source>
        <award-id>T2021-3</award-id>
        <principal-award-recipient>
          <name>
            <surname>Luo</surname>
            <given-names>Junwei</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>Â© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <sec id="Sec2">
      <title>Background</title>
      <p id="Par11">Human cancer is a heterogeneous disease triggered by random somatic mutations and driven by multiple genomic alterations due to uncontrolled abnormal cell proliferation and spreading to other cells and tissues [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]; cancer disrupts the intracellular homeostasis of an individual and thus seriously threatens the lives of humans. To shift to personalized treatment plans, cancers in specific tissues can be classified into subtypes based on the molecular characteristics of the primary tumour. These subtypes are the key basis for providing personalized and precise treatment [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>] to cancer patients and have important implications for aetiology, tumour biology and prognoses in a range of cancer research.</p>
      <p id="Par12">Gene expression data reflect the direct or indirect measurement of the abundance of gene transcript mRNA in cells. These data can be used to analyse which gene expression characteristics have changed, the correlations among genes, and how the activities of genes are affected under varying conditions. These data thus have important applications in medical clinical diagnoses, drug efficacy judgements, and in revealing disease mechanisms. Hence, gene expression data can be used in cancer subtype classification research, and many methods based on gene expression data have been presented.</p>
    </sec>
    <sec id="Sec3">
      <title>Related work</title>
      <p id="Par13">The cancer subtype classification task can be formulated as a supervised learning problem in which established tumour subtypes are used as category labels to ensure that the features learned by the model are relevant to prior biological knowledge [<xref ref-type="bibr" rid="CR5">5</xref>]. Early research on cancer subtype classification methods usually used traditional machine learning methods. In recent years, deep learning methods have been widely used in many fields with good results, and many deep learning models are now applied in this field. Current methods can be divided into two categories.</p>
      <sec id="Sec4">
        <title>Methods based on traditional machine learning approaches</title>
        <p id="Par14">In 2017, Soh et al. [<xref ref-type="bibr" rid="CR6">6</xref>] developed tumour classification models based on random forests, logistic regression and support vector machines (SVMs) [<xref ref-type="bibr" rid="CR7">7</xref>]. The data they used included somatic mutations, copy number variants and a combination of both, and their methods achieved a 77.7% accuracy with only 50 features. Ye et al. [<xref ref-type="bibr" rid="CR8">8</xref>], in 2018, proposed a classification method using gene expression data based on the artificial bee colony (ABC) algorithm [<xref ref-type="bibr" rid="CR9">9</xref>] and SVM. They used the ABC algorithm to optimize the kernel function parameters and penalty factors of SVM and obtained a relatively high classification accuracy by comparing and analysing other classification methods on a public dataset. In 2021, Duan et al. [<xref ref-type="bibr" rid="CR10">10</xref>] used an extreme random tree model as a classifier to reduce the dimensionality of gene expression data using linear discriminant analysis, thus effectively improving the classification accuracy compared to several ensemble algorithms.</p>
      </sec>
      <sec id="Sec5">
        <title>Methods based on deep learning approaches</title>
        <p id="Par15">In 2019, Yang et al. [<xref ref-type="bibr" rid="CR11">11</xref>] used a stacked autoencoder (SAE) [<xref ref-type="bibr" rid="CR12">12</xref>] neural network to learn high-level representations of gene expression data and transcriptome selective splicing data separately and then integrated all these learned high-level representations to classify patients into different cancer subtype groups, thus providing an effective and useful method for integrating multiple types of transcriptomics data to identify cancer subtypes. Zhuang et al. [<xref ref-type="bibr" rid="CR13">13</xref>] proposed a method based on a combination of SAE and boosting to classify gene expression data in 2020. After detrending the gene expression data by principal component analysis, the SAE was used as a base classification algorithm for learning and training using boosting. Finally, multiple SAEs were combined for decision making. The authors found that the classification accuracy was improved by 5% to 10% over the use of SAE alone, exhibiting a good classification performance. Xiao et al. [<xref ref-type="bibr" rid="CR14">14</xref>] proposed a deep learning model based on the Wasserstein generative adversarial network for unbalanced gene expression data in the same year by increasing the sample sizes in a few categories to achieve balance and further expanding the samples to improve the model performance. Majumder et al. [<xref ref-type="bibr" rid="CR15">15</xref>] considered a combination of three deep learning models (the multilayer perceptron (MLP) and two convolutional networks) and two feature selection methods in 2022 and performed experimental analyses on four cancer datasets, achieving good performance.</p>
        <p id="Par16">In past studies applying the above methods, different measures have been taken to address the characteristics of cancer gene expression data through either linear dimensionality reductions or data equalizations. However, linear dimensionality reduction methods are easily affected by irrelevant information, some methods cannot handle high-dimensional gene table data well, and the classification results are thus not ideal [<xref ref-type="bibr" rid="CR5">5</xref>]. To address these existing problems, we first used the synthetic minority oversampling technique (SMOTE) [<xref ref-type="bibr" rid="CR16">16</xref>] algorithm to equalize samples. Cancer datasets are usually small, and the numbers of samples representing different categories can vary greatly. For example, in the breast cancer (BRCA) dataset used in this paper, there are only 150 samples in the sixth category, 202 in the fifth category, and 721 in the third category. Thus, we proposed a method, DCGN, by combining a convolutional neural network (CNN) [<xref ref-type="bibr" rid="CR17">17</xref>] and bidirectional gated recurrent unit (BiGRU) [<xref ref-type="bibr" rid="CR18">18</xref>] to achieve nonlinear dimensionality reduction in the process of learning important features. To extract key features from high-dimensional and sparse gene expression data, a relatively complex neural network is generally needed. CNNs have special structures with local weight sharing; the neuron weights (convolution kernels) on the same feature map are shared, so the neural network can learn in parallel, thus effectively reducing the complexity of the network [<xref ref-type="bibr" rid="CR19">19</xref>]. However, a simple CNN network is prone to losing some important features during the learning process. A BiGRU can bidirectionally analyse the feature matrix of a CNN in the middle of the neural network and retain the information that may be lost through the update gate. Moreover, compared to other networks, BiGRUs are more efficient and have fewer parameters, thus expanding the model while improving its efficiency [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. Such a combined network has low complexity, is mutually complementary, and can capture comprehensive and effective features.</p>
        <p id="Par17">The main contributions can be summarized as follows. (1) By combining CNN and BiGRU, we proposed a deep learning method named DCGN that can obtain more credible features from high-dimensional sparse gene expression data. The experimental results prove that the DCGN method can obtain ideal classification results. (2) The Gaussian error linear unit (gelu) activation function is applied to the cancer classification task, and we prove through experiments that its performance is superior to those of the rectified linear unit (relu), exponential linear unit (elu), etc. (3). The DCGN performs well when applied to five high-dimensional datasets and has a good generalization ability.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec6">
    <title>Methods</title>
    <p id="Par18">DCGN adopts gene expression data to perform cancer subtype classification. It consists of three modules (see Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>). 1. The data-processing module: first, the data are enhanced by the SMOTE algorithm to solve the sample imbalance problem. Then, feature normalization is performed. 2. The feature-learning module: the most important part of the neural network. This module captures key features in the gene expression data during training. 3. The classification module: this module performs multiclassification predictions using the feature learning module outputs and compares them with the true labels to calculate the classification loss.<fig id="Fig1"><label>Fig. 1</label><caption><p>DCGN architecture</p></caption><graphic xlink:href="12859_2022_4980_Fig1_HTML" id="MO1"/></fig></p>
    <sec id="Sec7">
      <title>Data processing module</title>
      <p id="Par19">First, considering the problem of small and unbalanced cancer samples, in this paper, we uses the SMOTE algorithm to enhance the utilized cancer datasets. The basic idea of the SMOTE algorithm is to analyseanalyze the minority class samples, artificially synthesize new samples according to the minority class samples and add them these new samples to the datasetdata set. Comparative experimental results of random undersampling and SMOTE algorithm results are provided in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1. The algorithm flow of SMOTE is described as follows:<list list-type="order"><list-item><p id="Par20">First, classes for which the sample size is less than 15% of the total sample size are specified as minority sample sets (<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{S}}_{\mathrm{min}}$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">min</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq1.gif"/></alternatives></inline-formula>). For each sample <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{i}=({d}_{1},{d}_{2}\dots {d}_{m})$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>â¯</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq2.gif"/></alternatives></inline-formula> in a class with few samples (<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{S}}_{\mathrm{min}}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">min</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq3.gif"/></alternatives></inline-formula>), m represents the dimension of the sample. We calculate the Euclidean distance to all samples in the minority class sample set <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{S}}_{\mathrm{min}}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">min</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq4.gif"/></alternatives></inline-formula> and obtain the K-nearest neighbours.</p></list-item><list-item><p id="Par21">Then, we determine the sampling multiplier N according to the sample imbalance ratio. For each minority class sample <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{i}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq5.gif"/></alternatives></inline-formula>, a number of samples <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{n}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>X</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq6.gif"/></alternatives></inline-formula> are randomly selected from its K nearest neighbours.</p></list-item><list-item><p id="Par22">For each randomly selected nearest neighbour <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{n}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq7.gif"/></alternatives></inline-formula>, a new sample is constructed separately from the original sample according to Eq.Â (<xref rid="Equ1" ref-type="">1</xref>), where <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{i}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq8.gif"/></alternatives></inline-formula> represents a sample point from the minority class and <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{n}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq9.gif"/></alternatives></inline-formula> represents a sample point randomly selected from the K nearest neighbours. The term rand(0,1) represents a randomly generated number between 0 and 1.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{new} = x_{i} + \left( {x_{n} - x_{i} } \right){*}rand\left( {0,1} \right)$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="italic">new</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mrow/><mml:mo>â</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p></list-item></list></p>
      <p id="Par23">Next is the feature normalization step. Feature normalization is performed on the gene expression data before model training so that all features in the dataset have means of zero and unit variance. Specifically, assuming that the mean of a feature is u and its standard deviation is <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\upsigma$$\end{document}</tex-math><mml:math id="M22"><mml:mi mathvariant="normal">Ï</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq10.gif"/></alternatives></inline-formula>, the utilized feature standardization formula is defined as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X^{\prime} = \frac{x - u}{\sigma }$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>â²</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mi>u</mml:mi></mml:mrow><mml:mi>Ï</mml:mi></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\upsigma } = \sqrt {\frac{{(x_{1} - u)^{2} + (x_{2} - u)^{2} + \ldots (x_{n} - u)^{2} }}{n}}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mi mathvariant="normal">Ï</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo>â¦</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where the calculation acts on each column, x represents the feature matrix of the data, u is the mean of the data, <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\upsigma$$\end{document}</tex-math><mml:math id="M28"><mml:mi mathvariant="normal">Ï</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq11.gif"/></alternatives></inline-formula> is the standard deviation of the data, and <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}^{^{\prime}}$$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>â²</mml:mo></mml:msup></mml:msup></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq12.gif"/></alternatives></inline-formula> represents the data after feature normalization.</p>
    </sec>
    <sec id="Sec8">
      <title>Feature learning module</title>
      <p id="Par24">The feature-learning module includes a fully connected layer (FC layer), a convolution layer, a BiGRU layer, and another convolution layer. The input layer first goes through the FC layer, and the FC layer computation process is essentially matrix multiplication. Then, the computational result is output by the activation function. The calculation formula is expressed as follows, where x represents the input matrix, W is the weight parameter, b represents the bias, GELU is the activation function of this layer, and Y represents the nonlinear output through the activation function.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Y}} = {\text{GELU}}\left( {{\text{W}}*{\text{x}} + {\text{b}}} \right)$$\end{document}</tex-math><mml:math id="M32" display="block"><mml:mrow><mml:mtext>Y</mml:mtext><mml:mo>=</mml:mo><mml:mtext>GELU</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>W</mml:mtext><mml:mrow/><mml:mo>â</mml:mo><mml:mtext>x</mml:mtext><mml:mo>+</mml:mo><mml:mtext>b</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <sec id="Sec9">
        <title>Activation function</title>
        <p id="Par25">The activation function is the "switch" that determines whether a neural network transmits information or not; this function is crucial in neural networks. Different activation functions greatly impact the training effect of neural networks. At present, the most commonly used activation functions are the relu(Recitified Linear Unit) function, sigmoid function, elu(Exponential Linear Unit) function, etc. In this paper, an activation function called gelu(Gaussian Error Linear Unit) is selected. Some experiments have shown that gelu is superior to other activation functions, such as relu and elu, for tasks ranging from computer vision to natural language processing [<xref ref-type="bibr" rid="CR22">22</xref>]. We conduct comparative experiments on these activation functions in âActivation function comparison experimentâ section.</p>
        <p id="Par26">Gelu can be seen as a combination of relu and dropout ideas. For high-dimensional gene expression data, excessive features may affect the feature-learning procedure. Sometimes we want to regularize unimportant information to zero, and the nonlinear variation feature of the gelu activation function can perform stochastic canonical transformation. It can be understood that a given input value should be multiplied by 1 or 0 according to its specific situation. For a more mathematical description, each input value x that obeys the standard normal distribution N(0,1) is multiplied by a Bernoulli distribution. In Fig.Â <xref rid="Fig2" ref-type="fig">2</xref>, for the gelu activation function, when x is larger, y is more likely to be retained, while the smaller x is, the more likely y is to be set to 0; however, but when x is less than 0, y has a certain probability not to be 0; under the relu function, outputs below 0 are set to 0. As x decreases, the probability of y being set to zero increases, and the limit is 0.<fig id="Fig2"><label>Fig. 2</label><caption><p>Function image of activation functions</p></caption><graphic xlink:href="12859_2022_4980_Fig2_HTML" id="MO2"/></fig></p>
        <p id="Par27">The gelu function has the following form:<inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{ GELU}}\left( {\text{X}} \right) = {\text{x}} \cdot \phi \left( {\text{x}} \right),{\text{x}}\sim{\text{N}}\left( {0,1} \right)$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>GELU</mml:mtext></mml:mrow><mml:mfenced close=")" open="("><mml:mtext>X</mml:mtext></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>x</mml:mtext><mml:mo>Â·</mml:mo><mml:mi>Ï</mml:mi><mml:mfenced close=")" open="("><mml:mtext>x</mml:mtext></mml:mfenced><mml:mo>,</mml:mo><mml:mtext>x</mml:mtext><mml:mo>â¼</mml:mo><mml:mtext>N</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq13.gif"/></alternatives></inline-formula> (5), where x is the input, <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi \left( {\text{x}} \right) = {\text{P}}\left( {{\text{X}} \le {\text{x}}} \right)$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>Ï</mml:mi><mml:mfenced close=")" open="("><mml:mtext>x</mml:mtext></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>P</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>X</mml:mtext><mml:mo>â¤</mml:mo><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq14.gif"/></alternatives></inline-formula>, X is a Gaussian random variable with zero mean and unit variance, and P(Xâ&lt;ââ=âx) is the probability that X is less than or equal to a given value x. The mathematical formula applied to obtain the approximate calculation is provided in the original paper [<xref ref-type="bibr" rid="CR22">22</xref>] as follows:<disp-formula id="Equ5"><label>6</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{GELU}}\left( {\text{x}} \right) = 0.5{\text{x}}\left( {1 + {\text{tanh}}\left( {\sqrt {2/{\uppi }} \left( {{\text{x}} + 0.044715{\text{x}}^{3} } \right)} \right)} \right)$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:mrow><mml:mtext>GELU</mml:mtext><mml:mfenced close=")" open="("><mml:mtext>x</mml:mtext></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mtext>x</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>tanh</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mi mathvariant="normal">Ï</mml:mi></mml:mrow></mml:msqrt><mml:mfenced close=")" open="("><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>+</mml:mo><mml:mn>0.044715</mml:mn><mml:msup><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec10">
        <title>Convolutional layer</title>
        <p id="Par28">The outputs of the FC layer are next subjected to convolution operations. Because CNN are mostly used for image processing, the calculation process typically involves a number of image channels (generally images have three channels: red, green, and blue (RGB)); however, the data used in this article are characterized by numerical matrices of gene expression data, which differ from image data, so it is necessary to describe the calculation process. The CNN calculation is different from that of the FC layer. The input matrix format includes four dimensions: the number of samples, height, width, and number of channels. The output matrix format has the same dimensional order and meaning as the input matrix, but the sizes of the last three dimensions are changed by the convolution operation. The meaning of the weight matrix (convolution kernel) dimensions is different from those of the above two matrices. These are the height of the convolution kernel, the width of the convolution kernel, the number of input channels (the number of channels in the input matrix), and the number of output channels (the number of convolution kernels).</p>
        <p id="Par29">Taking the first convolutional layer as an example, the FC layer output as (N,1024) becomes (N,32,32,1) after a Reshape operation is applied, where N represents the number of samples in the batch, the height and width are both 32, and the number of channels is only one. The first convolutional layer is the conv2D layer with 128 convolutional kernels of size (3,3,1), paddingâ=âsame, stridesâ=â2. The term "paddingâ=âsame" indicates that the output size is equal to the input size divided by the strides and rounded up. The calculation process is shown in Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>. Thus far, the conv2d layer has slid and multiplied the feature matrix through the convolution kernel to extract features and reduce the dimensionality. At the same time, the conv2d layer has a special structure with local weight value sharing, and this effectively reduces the training parameters of the neural network. To explore the impacts of these two convolutional layers on the DCGN, we conducted comparative experiments on different datasets, and the detailed experimental results are provided in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S2 and Fig. S1.</p>
        <p id="Par30">The convolution layer also contains a maximum pooling layer. This layer can reduce the size of the model, improve the calculation speed, and improve the robustness of the extracted features. However, the maximum pooling layer has no parameters to learn and can only takes the maximum value from the target area; thus, the numbers of input and output channels do not change..</p>
      </sec>
      <sec id="Sec11">
        <title>BiGRU layer</title>
        <p id="Par31">The gated recurrent unit (GRU) uses an update gate and a reset gate. The update gate determines how much information from the past should be allowed to pass through the gate, while the reset gate decides how much information from the past should be discarded. First, we obtain the two gating states from the last transmission down state <inline-formula id="IEq15"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${h}_{t-1}$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq15.gif"/></alternatives></inline-formula> and the input <inline-formula id="IEq16"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{t}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq16.gif"/></alternatives></inline-formula> of the current node. In Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>, <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z}_{t}$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq17.gif"/></alternatives></inline-formula> performs the update gate operation,<inline-formula id="IEq18"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{t}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq18.gif"/></alternatives></inline-formula> performs the reset gate operation, and the gate uses a sigmoid function to determine which values to let pass or discard.<disp-formula id="Equ6"><label>7</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_{t} = \sigma \left( {W_{z} \cdot \left[ {h_{t - 1} ,x_{t} } \right]} \right)$$\end{document}</tex-math><mml:math id="M48" display="block"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>Â·</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>8</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_{t} = \sigma \left( {W_{r} \cdot \left[ {h_{t - 1} ,x_{t} } \right]} \right)$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Ï</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>Â·</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><fig id="Fig3"><label>Fig. 3</label><caption><p>Structure of the GRU (where Ï represents the sigmoid function)</p></caption><graphic xlink:href="12859_2022_4980_Fig3_HTML" id="MO3"/></fig></p>
        <p id="Par32">When the gating signal is obtained, we first use reset gating to obtain the reset data <inline-formula id="IEq19"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t - 1} ^{\prime}$$\end{document}</tex-math><mml:math id="M52"><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>â²</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq19.gif"/></alternatives></inline-formula>; then, we splice <inline-formula id="IEq20"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t - 1} ^{\prime}$$\end{document}</tex-math><mml:math id="M54"><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>â²</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq20.gif"/></alternatives></inline-formula> with the input <inline-formula id="IEq21"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{t}$$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq21.gif"/></alternatives></inline-formula> and deflate the data to the range of âÂ 1~1 by the tanh activation function.<disp-formula id="Equ8"><label>9</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t - 1}^{^{\prime}} = h_{t - 1} *r_{t}$$\end{document}</tex-math><mml:math id="M58" display="block"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mrow/><mml:mo>â²</mml:mo></mml:msup></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow/><mml:mo>â</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>10</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t}^{^{\prime}} = {\text{tanh}}\left( {W \cdot \left[ {h_{t - 1}^{^{\prime}} ,x_{t} } \right]} \right)$$\end{document}</tex-math><mml:math id="M60" display="block"><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>â²</mml:mo></mml:msup></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>tanh</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mi>W</mml:mi><mml:mo>Â·</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mrow/><mml:mo>â²</mml:mo></mml:msup></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par33">Finally, the most critical step in GRU is the âupdate memoryâ step, in which we perform both forgetting and remembering tasks. The update gate <inline-formula id="IEq22"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_{t}$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq22.gif"/></alternatives></inline-formula> is equivalent to the forgetting gate, and 1-<inline-formula id="IEq23"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_{t}$$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq23.gif"/></alternatives></inline-formula> is equivalent to the input gate. The current moment memory unit can be expressed as follows:<disp-formula id="Equ10"><label>11</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{ h}}_{t} = \left( {1 - z_{t} } \right)h_{t}^{^{\prime}} + z_{t} h_{t - 1}$$\end{document}</tex-math><mml:math id="M66" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>h</mml:mtext></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>â²</mml:mo></mml:msup></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq24"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( {1 - z_{t} } \right)h_{t}^{^{\prime}}$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>â²</mml:mo></mml:msup></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq24.gif"/></alternatives></inline-formula> denotes the selective memory of <inline-formula id="IEq25"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t}^{^{\prime}}$$\end{document}</tex-math><mml:math id="M70"><mml:msubsup><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>â²</mml:mo></mml:msup></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq25.gif"/></alternatives></inline-formula> containing information about the current node, retaining some important information, and <inline-formula id="IEq26"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z_{t} h_{t - 1}$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq26.gif"/></alternatives></inline-formula> denotes the selective obliviousness of the originally hidden state. The operation of this step is to forget the information of some passed-down dimensions in <inline-formula id="IEq27"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{t - 1}$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq27.gif"/></alternatives></inline-formula> and add some dimensional information entered by the current node.</p>
        <p id="Par34">The above process represents forward delivery, and the BiGRU layer contains both forward and reverse gated recurrent units, as shown in Fig.Â <xref rid="Fig1" ref-type="fig">1</xref>b. At each moment t, the input is provided to the gated unit in both directions, while the output is determined by the joint bidirectional output.</p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Classification module</title>
      <p id="Par35">The DCGN takes the output of the feature-learning module into the classification module composed of four fully connected layers after a straightening operation is performed. We use a dropout layer after each fully connected layer to improve the generality of the network and to mitigate the interactions between neurons. The last layer directly outputs the tensor computed by the network, which may be optimally numerically stable.</p>
      <sec id="Sec13">
        <title>Loss function</title>
        <p id="Par36">The loss function of the neural network is the SparseCategoricalCrossentropy function; this function can convert digital coding into a one-hot coding format and then apply the cross entropy loss function to the data (real label value) and the predicted label value. The multicategorical cross-entropy loss function is actually an extension of the bicategorical cross-entropy loss function, as expressed in Eq.Â (<xref rid="Equ11" ref-type="">12</xref>), where M is the number of categories;<inline-formula id="IEq28"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{ic}$$\end{document}</tex-math><mml:math id="M76"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="italic">ic</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq28.gif"/></alternatives></inline-formula> is the symbolic function (0 or 1) (if the true class of sample i is equal to c, this term takes a value of 1; otherwise, it takes a value of 0); and <inline-formula id="IEq29"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{ic}$$\end{document}</tex-math><mml:math id="M78"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi mathvariant="italic">ic</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq29.gif"/></alternatives></inline-formula> is the predicted probability that sample i belongs to category c.<disp-formula id="Equ11"><label>12</label><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{ Loss}} = - \frac{1}{{\text{N}}}\mathop \sum \limits_{{\text{i}}} \mathop \sum \limits_{{{\text{c}} = 1}}^{{\text{M}}} {\text{y}}_{{{\text{ic}}}} {\text{log}}\left( {{\text{p}}_{{{\text{ic}}}} } \right)$$\end{document}</tex-math><mml:math id="M80" display="block"><mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>Loss</mml:mtext></mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mtext>N</mml:mtext></mml:mfrac><mml:munder><mml:mo movablelimits="false">â</mml:mo><mml:mtext>i</mml:mtext></mml:munder><mml:munderover><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mrow><mml:mtext>c</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mtext>M</mml:mtext></mml:munderover><mml:msub><mml:mtext>y</mml:mtext><mml:mtext>ic</mml:mtext></mml:msub><mml:mtext>log</mml:mtext><mml:mfenced close=")" open="("><mml:msub><mml:mtext>p</mml:mtext><mml:mtext>ic</mml:mtext></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Experiment</title>
    <sec id="Sec15">
      <title>Experimental data</title>
      <p id="Par37">To demonstrate the validity of the proposed method, we conducted experiments on breast and bladder cancers. The breast cancer dataset used herein was obtained from a previous Molecular Taxonomy of Breast cancer International Consortium (METABRIC) [<xref ref-type="bibr" rid="CR3">3</xref>] study. Chen et al. [<xref ref-type="bibr" rid="CR5">5</xref>] processed this dataset to obtain gene expression profiles from 1989 primary breast tumour samples and 144 normal breast tissue samples with Prediction Analysis of Miroarray 50 (PAM50) subtypes [<xref ref-type="bibr" rid="CR4">4</xref>] used as classification labels. The bladder cancer dataset was derived from The Cancer Genome Atlas (TCGA) project and contains the gene expression profiles of 408 bladder cancer samples. Four currently published molecular classifications, MD Anderson (MDA) [<xref ref-type="bibr" rid="CR23">23</xref>], TCGA [<xref ref-type="bibr" rid="CR24">24</xref>], Curie Institute (CIT)-Curie [<xref ref-type="bibr" rid="CR25">25</xref>], and Lund [<xref ref-type="bibr" rid="CR26">26</xref>], have been widely used in bladder cancer classification studies. We used the R package BLCAsubtyping [<xref ref-type="bibr" rid="CR27">27</xref>] to label each cancer sample separately according to these four classification systems. The details are provided in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Cancers and their specific subtypes</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Cancer category</th><th align="left">Classification systems</th><th align="left">Specific typology</th><th align="left">Genes</th><th align="left">Number of samples</th></tr></thead><tbody><tr><td align="left">BRCA</td><td align="left">PAM50</td><td align="left">Basal, HER2â+â, luminal A/B, normal-like, normal</td><td char="." align="char">20,000</td><td char="." align="char">4221</td></tr><tr><td align="left">BLCA</td><td align="left">MDA</td><td align="left">Luminal, basal, p53-like</td><td char="." align="char">20,087</td><td char="." align="char">1010</td></tr><tr><td align="left"/><td align="left">TCGA</td><td align="left">Luminal_infiltrated, Luminal_papillary, Luminal, Neuronal, Basal_squamous</td><td char="." align="char">20,000</td><td char="." align="char">761</td></tr><tr><td align="left"/><td align="left">CIT-Curie</td><td align="left">MC1, MC2, MC3, MC4, MC5, MC6, MC7</td><td char="." align="char">20,087</td><td char="." align="char">909</td></tr><tr><td align="left"/><td align="left">Lund</td><td align="left">UroA-Prog, UroB, UroC, Uro-Inf, GU, GU-Inf, Mes-like, Ba/Sq, Sc/NE-like, Ba/Sq-Inf</td><td char="." align="char">20,000</td><td char="." align="char">1185</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec16">
      <title>Experimental Setting</title>
      <p id="Par38">The number of nodes in the feature learning module was set to 1024, 128, 64, and 64, while the number of nodes in the classification module was set to 128, 64, 32, and 10. For the model construction and performance evaluation steps, we randomly divided the data into a training set, a validation set, and a test set containing 80%, 10% and 10% of the samples, respectively. To improve the memory utilization and parallelization efficiency in the large matrix multiplication process, we set the batch size to 256. Then, we used the Adam method [<xref ref-type="bibr" rid="CR28">28</xref>] to tune the model parameters with the learning rate set to 1e-3 and the dropout layer ratios set to 0.6 and 0.7.</p>
    </sec>
    <sec id="Sec17">
      <title>Comparative methods</title>
      <p id="Par39">To prove the robustness of the method proposed in this paper, we selected seven methods based on machine learning or deep learning approaches to conduct experiments. The following text briefly introduces the compared methods, among which gcForest [<xref ref-type="bibr" rid="CR29">29</xref>], SAE, BiGRU, and DCNN belong to the deep learning methods.</p>
      <p id="Par40">The basic SVM model is a linear classifier with the largest interval defined on the feature space; in the gradient boost decision tree (GBDT) [<xref ref-type="bibr" rid="CR30">30</xref>], the decision tree (DT) and gradient boosting (GB) components provide the learning strategy, and the DT model is trained with the gradient boosting strategy. In the light gradient boosting machine (LightGBM) [<xref ref-type="bibr" rid="CR31">31</xref>], a distributed gradient boosting framework based on a decision tree algorithm provides a framework based on the GBDT algorithm to support efficient parallel training, low memory usage and high accuracy. The gcForest method, involving a deep forest model with a cascading function, applies the principles of deep neural networks to the traditional "random forest" machine learning algorithm. SAE is a deep neural network model composed of multiple layers of Sparse AutoEncoder (sparse self-encoder); in this model, the output of the previous self-encoder layer is used as the input to the subsequent self-encoder layer, and the last layer is a classifier. In BiGRU, the bidirectional GRU layer in a recurrent neural network (RNN) retains important features through various gate functions. Compared to the long short-term memory (LSTM) model, BiGRU has fewer parameters and a better effect. DCNN is a deep learning method based on CNNs that has exhibited a good performance on gene expression datasets characterizing various cancers.</p>
    </sec>
    <sec id="Sec18">
      <title>Results and analysis</title>
      <sec id="Sec19">
        <title>Evaluation metrics</title>
        <p id="Par41">Evaluation metrics are critical criteria used to measure whether a method performs well when facing a given problem. For classification problems, metrics such as the accuracy, precision, recall, F1-score, and confusion matrix [<xref ref-type="bibr" rid="CR32">32</xref>] are normally used. The accuracy rate represents the proportion of the number of correct samples predicted by the method to the total number of samples, and the precision rate refers to the proportion of samples that are actually positive among the samples that are predicted to be positive. Recall refers to the proportion of samples that are predicted to be positive among the true positive samples. The F1-score is the harmonic mean of the precision and recall scores. By running the results of each model under the same experimental settings, we can intuitively identify the strengths and weaknesses of the model classification performance. When evaluating multiclassification problems, the problem is usually decomposed into multiple 2-classification problems, i.e., n classifications can be decomposed into n 2-classifications, with one of the classes set as the positive class and the remaining classes uniformly set as the negative class in each iteration; then, various 2-classification indexes are calculated. Finally, the multiclassification evaluation indexes are averaged. Although this paper uses the SMOTE algorithm to balance the data, it properly generates only some minority samples, so different proportions of categories may occur. Therefore, when calculating the multiclass evaluation metrics, we chose the weighted average method. When calculating the precision and recall scores, the scores of each category were multiplied by the percentage of the category in the total sample to obtain summed scores. In formulas (<xref rid="Equ12" ref-type="">13</xref>) and (<xref rid="Equ13" ref-type="">14</xref>), L represents the number of categories, <inline-formula id="IEq30"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$precision_{i}$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq30.gif"/></alternatives></inline-formula> and <inline-formula id="IEq31"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$recall_{i}$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq31.gif"/></alternatives></inline-formula> represent the precision and recall rates of class i, respectively, and <inline-formula id="IEq32"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_{i}$$\end{document}</tex-math><mml:math id="M86"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq32.gif"/></alternatives></inline-formula> represents the proportion of the i-th sample in the total sample.<disp-formula id="Equ12"><label>13</label><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Precision }} = { }\frac{{\mathop \sum \nolimits_{{{\text{i}} = 1}}^{{\text{L}}} {\text{precision}}_{{\text{i}}} {\text{*w}}_{{\text{i}}} }}{{\text{L}}}$$\end{document}</tex-math><mml:math id="M88" display="block"><mml:mrow><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow><mml:mo>=</mml:mo><mml:mrow/><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mtext>L</mml:mtext></mml:msubsup><mml:msub><mml:mtext>precision</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:msub><mml:mtext>*w</mml:mtext><mml:mtext>i</mml:mtext></mml:msub></mml:mrow><mml:mtext>L</mml:mtext></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ13"><label>14</label><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{ Recall }} = { }\frac{{\mathop \sum \nolimits_{{{\text{i}} = 1}}^{{\text{L}}} {\text{recall}}_{{\text{i}}} {\text{*w}}_{{\text{i}}} }}{{\text{L}}}$$\end{document}</tex-math><mml:math id="M90" display="block"><mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>Recall</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow><mml:mo>=</mml:mo><mml:mrow/><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>â</mml:mo><mml:mrow><mml:mrow><mml:mtext>i</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mtext>L</mml:mtext></mml:msubsup><mml:msub><mml:mtext>recall</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:msub><mml:mtext>*w</mml:mtext><mml:mtext>i</mml:mtext></mml:msub></mml:mrow><mml:mtext>L</mml:mtext></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ14"><label>15</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{F}}1 = { }\frac{2*Precision*Recall}{{Precision + Recall}}$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mtext>F</mml:mtext><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mrow/><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mrow/><mml:mo>â</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mrow/><mml:mo>â</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec20">
        <title>Activation function comparison experiment</title>
        <p id="Par42">To prove the influence of the gelu activation function selected in this paper on the model, we selected three other activation functions, relu, elu, and tanh, as the activation functions of the model and conducted comparative experiments on the bladder urothelial carcinoma (BLCA)-TCGA dataset. The experimental results are shown in Fig.Â <xref rid="Fig4" ref-type="fig">4</xref>. Gelu exhibited the fastest convergence speed, and its accuracy rate was as high as 99.3%. Both elu and relu performed slightly worse. Tanh had the worst training effect, with an accuracy of only 70%.<fig id="Fig4"><label>Fig. 4</label><caption><p>Activation functions accuracy curve (experimental results of different activation functions on other datasets are provided in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S2)</p></caption><graphic xlink:href="12859_2022_4980_Fig4_HTML" id="MO4"/></fig></p>
      </sec>
      <sec id="Sec21">
        <title>Ablation experiment</title>
        <p id="Par43">There are many robust network structures in deep learning, and different combinations of networks may perform differently. To demonstrate that the DCGN proposed in this paper can provide better results than other network structures, we used different neural networks to obtain free combinations and verified the networks on the BRCA 20,000-dimension dataset. The experimental results are shown in Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig5"><label>Fig. 5</label><caption><p>Ablation experiment results</p></caption><graphic xlink:href="12859_2022_4980_Fig5_HTML" id="MO5"/></fig></p>
        <p id="Par44">From Fig.Â <xref rid="Fig5" ref-type="fig">5</xref>, it can be found that the closer the curves of the three metrics are to the edge of the hexagonal graph, the better the classification performances of the networks are. The three evaluation metrics obtained for the DCGN are very close to the edge of the hexagon, and all are higher than those of the other network models, indicating that the classification performance of the model proposed in this paper is best compared to the other analysed deep learning neural networks. In addition, the number after each network combination in the figure indicates the total number of parameters trained by the deep learning model. During the backpropagation process, each model minimizes the loss by updating the parameters corresponding to each layer. The number of model parameters has a certain relationship with the model performance. The consideration of too few parameters may affect the classification performance of a model.</p>
      </sec>
      <sec id="Sec22">
        <title>Comparison experiments based on BRCA</title>
        <p id="Par45">To further validate the effectiveness of the proposed approach, we compared DCGN with the seven methods mentioned in âComparative methods: section on the BRCA 20,000-dimension dataset. The dimensionality of the data matrix after data enhancement was (4221, 20,000), and the specific results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>BRCA20000-dimension dataset results</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><italic>Dataset</italic></th><th align="left" colspan="8"><italic>BRCA</italic></th></tr><tr><th align="left">Methods</th><th align="left">DCNN</th><th align="left">SVM</th><th align="left">GBDT</th><th align="left">LightGBM</th><th align="left">gcForest</th><th align="left">SAE</th><th align="left">BiGRU</th><th align="left">DCGN</th></tr></thead><tbody><tr><td align="left" colspan="9"><italic>Highest level</italic></td></tr><tr><td align="left">Accuracy</td><td char="." align="char">90.2</td><td align="left">94.7</td><td char="." align="char">94.3</td><td char="." align="char">94.6</td><td char="." align="char">95.2</td><td char="." align="char">94.2</td><td align="left">95</td><td align="left">96</td></tr><tr><td align="left">Precision</td><td char="." align="char">95.1</td><td align="left">94.9</td><td char="." align="char">94.5</td><td char="." align="char">94.7</td><td char="." align="char">95.4</td><td char="." align="char">95.3</td><td align="left">95.4</td><td align="left">98.7</td></tr><tr><td align="left">Recall</td><td char="." align="char">94.8</td><td align="left">94.8</td><td char="." align="char">94.3</td><td char="." align="char">94.5</td><td char="." align="char">95.2</td><td char="." align="char">94.8</td><td align="left">94.7</td><td align="left">98.7</td></tr><tr><td align="left">F1-score</td><td char="." align="char">94.6</td><td align="left">94.8</td><td char="." align="char">94.3</td><td char="." align="char">94.6</td><td char="." align="char">95.3</td><td char="." align="char">94.7</td><td align="left">94.8</td><td align="left">98.6</td></tr><tr><td align="left" colspan="9"><italic>Average level</italic></td></tr><tr><td align="left">Accuracy</td><td char="." align="char">88.7</td><td align="left">94</td><td char="." align="char">93.2</td><td char="." align="char">93.6</td><td char="." align="char">94.1</td><td char="." align="char">93.7</td><td align="left">94</td><td align="left">94.8</td></tr><tr><td align="left">Precision</td><td char="." align="char">92.5</td><td align="left">94</td><td char="." align="char">93.4</td><td char="." align="char">93.7</td><td char="." align="char">94.2</td><td char="." align="char">93.1</td><td align="left">93.9</td><td align="left">96.8</td></tr><tr><td align="left">Recall</td><td char="." align="char">92.2</td><td align="left">94.1</td><td char="." align="char">93.2</td><td char="." align="char">93.5</td><td char="." align="char">94.2</td><td char="." align="char">93.2</td><td align="left">94</td><td align="left">96.7</td></tr><tr><td align="left">F1-score</td><td char="." align="char">92.3</td><td align="left">94</td><td char="." align="char">93.2</td><td char="." align="char">93.6</td><td char="." align="char">94.3</td><td char="." align="char">93.2</td><td align="left">94.1</td><td align="left">97</td></tr></tbody></table></table-wrap></p>
        <p id="Par46">From Table <xref rid="Tab2" ref-type="table">2</xref>, at the highest level, it can be seen that the DCGN exceeded the other seven models in all four metrics. In particular, DCGN achieved performance improvements ofâ~â3â5% in terms of the F1 scores; DCGN was the only model whose evaluation metrics exceeded 95% on average. The experimental results tentatively demonstrate that the classification performance of the DCGN is best when applied to high-dimensional datasets, indicating that the proposed method can exhibit a good classification performance on high-dimensional gene expression data. To further prove this conclusion, we verify the proposed model on the BLCA dataset.</p>
        <p id="Par47">A confusion matrix is a situation analysis table that summarizes the prediction results of a machine learning classification model. In the form of a matrix, the situation in the dataset can be summarized according to two standards: the real category and the category predicted by the classification model. A confusion matrix thus gives a more intuitive picture of how well a model performs, because all correct predictions are shown on the diagonal and all wrong predictions are shown off the diagonal; thus, it is very straightforward to identify incorrect predictions. FigureÂ <xref rid="Fig6" ref-type="fig">6</xref> shows the confusion matrix obtained for several models with relatively high classification performances on the BRCA 20,000-dimension dataset. It can be clearly seen that the DCGN has the fewest prediction results outside the diagonal line, with only 24 samples, while the fewest number of predictions outside the diagonal among the other models is 42. The result equally proves that the classification performance of the DCGN is optimal on the BRCA 20,000-dimensional dataset.<fig id="Fig6"><label>Fig. 6</label><caption><p>Confusion matrix derived for several well-performing methods</p></caption><graphic xlink:href="12859_2022_4980_Fig6_HTML" id="MO6"/></fig></p>
        <p id="Par48">In addition, two multiclassification metrics, the Kappa coefficient [<xref ref-type="bibr" rid="CR33">33</xref>] and Hamming distance [<xref ref-type="bibr" rid="CR34">34</xref>], can also reflect the classification performance of a model to some extent. The Kappa coefficient is a model evaluation parameter obtained based on the calculation of the confusion matrix with the following equation:<disp-formula id="Equ15"><label>16</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{ Kappa }} = { }\frac{{{\text{P}}_{0} - {\text{P}}_{{\text{e}}} }}{{1 - {\text{P}}_{{\text{e}}} }}$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:mrow><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>Kappa</mml:mtext><mml:mspace width="0.333333em"/></mml:mrow><mml:mo>=</mml:mo><mml:mrow/><mml:mfrac><mml:mrow><mml:msub><mml:mtext>P</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mtext>P</mml:mtext><mml:mtext>e</mml:mtext></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mtext>P</mml:mtext><mml:mtext>e</mml:mtext></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_4980_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq33"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{P}}_{0}$$\end{document}</tex-math><mml:math id="M96"><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq33.gif"/></alternatives></inline-formula> represents the overall classification accuracy and <inline-formula id="IEq34"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm{P}}_{\mathrm{e}}$$\end{document}</tex-math><mml:math id="M98"><mml:msub><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_4980_Article_IEq34.gif"/></alternatives></inline-formula> denotes the number of true samples in each category multiplied by the number of predicted samples in each category and then divided by the square of the total number of samples. The closer the Kappa coefficient is to 1, the better the classification performance is. The Hamming distance is also applicable to multiclassification problems and is simply a measure of the distance between the predicted label and the true label; this term takes a value between 0 and 1. A distance of 0 means that the predicted result is exactly the same as the true result, while a distance of 1 indicates that the model is exactly the opposite of the desired result. The experimental results are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. The Kappa coefficient of the DCGN reaches 0.984, indicating that the prediction results are very close, almost identical, to the actual classification results. Moreover, the Hamming distance of the DCGN is also the smallest, at only 0.013, the same as the expected experimental results. These results are strong proof that the DCGN has the best classification effect on high-dimensional datasets.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Kappa coefficient and Hamming distance values of each model on the BRCA 20,000-dimension dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">DCNN</th><th align="left">SVM</th><th align="left">GBDT</th><th align="left">LightGBM</th><th align="left">gcForest</th><th align="left">SAE</th><th align="left">BiGRU</th><th align="left">DCGN</th></tr></thead><tbody><tr><td align="left">Kappa</td><td char="." align="char">0.937</td><td char="." align="char">0.947</td><td char="." align="char">0.937</td><td char="." align="char">0.937</td><td char="." align="char">0.936</td><td char="." align="char">0.953</td><td char="." align="char">0.937</td><td char="." align="char">0.984</td></tr><tr><td align="left">Hamming distance</td><td char="." align="char">0.051</td><td char="." align="char">0.043</td><td char="." align="char">0.052</td><td char="." align="char">0.052</td><td char="." align="char">0.053</td><td char="." align="char">0.039</td><td char="." align="char">0.051</td><td char="." align="char">0.013</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="Sec23">
        <title>Comparison experiments based on the BLCA</title>
        <p id="Par49">As is widely known, the generalization ability of a model is an important criterion for judging its quality. Our proposed model performed well on the BRCA 20,000-dimension dataset. To demonstrate the generalization ability of the proposed model, we next conducted experiments on four high-dimensional BLCA datasets. The BLCA datasets were processed separately according to the four molecular typing systems as described in âExperimental dataâ section. After data enhancement, we loaded the data matrix and labels into each model to perform the experiments.</p>
        <p id="Par50">The experimental results of the BLCA datasets are shown in Tables <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>. From the results of the four datasets in Table <xref rid="Tab4" ref-type="table">4</xref>, it is obvious that the DCGN exhibits an excellent performance no matter which dataset is analysed. The DCGN performance is especially high for the BLCA-CIT-Curie and BLCA-TCGA datasets, and although the results of the other methods are also very good, the four indicators of DCGN at the highest level are nearly maximized, and the average results exceed 98%, indicating the best classification performance. Table <xref rid="Tab5" ref-type="table">5</xref> records the Kappa coefficient and Hamming distance values of all methods on the BLCA datasets. Table <xref rid="Tab5" ref-type="table">5</xref> shows that the DCGN has the highest Kappa coefficient and the smallest Hamming distance values on all datasets. On the TCGA and CIT-Curie datasets, the Kappa coefficient of the DCGN indicates that this was the only method to exceed 0.99, while the Hamming distances are only 0.005 and 0.006. In conclusion, the method proposed in this paper has a superior generalization ability and can learn different effective features to predict different classification tasks according to different datasets.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Experimental results of BLCA datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">DCNN</th><th align="left">SVM</th><th align="left">GBDT</th><th align="left">LightGBM</th><th align="left">gcForest</th><th align="left">SAE</th><th align="left">BiGRU</th><th align="left">DCGN</th></tr><tr><th align="left">Dataset</th><th align="left" colspan="8">BLCA-MDA</th></tr></thead><tbody><tr><td align="left">Accuracy</td><td char="." align="char">91.5 (90)</td><td char="(" align="char">93 (91.1)</td><td char="(" align="char">92.5 (91.4)</td><td char="(" align="char">92.5 (90)</td><td char="(" align="char">92.7 (89.5)</td><td char="(" align="char">93 (92.1)</td><td char="(" align="char">93.5 (92.7)</td><td char="(" align="char">95.5 (94.2)</td></tr><tr><td align="left">Precision</td><td char="." align="char">94.3 (92)</td><td char="(" align="char">93.4 (91.2)</td><td char="(" align="char">92.9 (90.6)</td><td char="(" align="char">93 (90.4)</td><td char="(" align="char">92.7 (90)</td><td char="(" align="char">93.2 (92.2)</td><td char="(" align="char">93.6 (92.6)</td><td char="(" align="char">97.4 (94.5)</td></tr><tr><td align="left">Recall</td><td char="." align="char">93 (91.8)</td><td char="(" align="char">93.3 (91)</td><td char="(" align="char">92.5 (90.4)</td><td char="(" align="char">93.2 (90)</td><td char="(" align="char">92.7 (90)</td><td char="(" align="char">93 (92)</td><td char="(" align="char">93.5 (92.4)</td><td char="(" align="char">97.3 (94.2)</td></tr><tr><td align="left">F1-score</td><td char="." align="char">93.3 (92)</td><td char="(" align="char">93.4 (91)</td><td char="(" align="char">92.6 (90.4)</td><td char="(" align="char">93.3 (90)</td><td char="(" align="char">92.6 (89.7)</td><td char="(" align="char">93 (92.1)</td><td char="(" align="char">93.4 (92.4)</td><td char="(" align="char">97.3 (94.2)</td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">DCNN</th><th align="left">SVM</th><th align="left">GBDT</th><th align="left">LightGBM</th><th align="left">gcForest</th><th align="left">SAE</th><th align="left">BiGRU</th><th align="left">DCGN</th></tr><tr><th align="left">Dataset</th><th align="left" colspan="8">BLCA-Lund</th></tr></thead><tbody><tr><td align="left">Accuracy</td><td char="(" align="char">91.8 (90)</td><td char="(" align="char">94 (91)</td><td char="(" align="char">93.2 (90.2)</td><td char="(" align="char">91.2 (90)</td><td char="(" align="char">92.4 (90)</td><td char="(" align="char">89.8 (88)</td><td char="(" align="char">93.6 (92.5)</td><td char="(" align="char">94.5 (93)</td></tr><tr><td align="left">Precision</td><td char="(" align="char">93 (91.5)</td><td char="(" align="char">94 (91.2)</td><td char="(" align="char">93.7 (91.8)</td><td char="(" align="char">91.7 (90)</td><td char="(" align="char">94 (91.5)</td><td char="(" align="char">89.7 (88.1)</td><td char="(" align="char">93.8 (92.7)</td><td char="(" align="char">94.9 (93.7)</td></tr><tr><td align="left">Recall</td><td char="(" align="char">92.5 (90.7)</td><td char="(" align="char">94 (91.1)</td><td char="(" align="char">93.2 (90.4)</td><td char="(" align="char">91.5 (89.6)</td><td char="(" align="char">92.4 (90)</td><td char="(" align="char">89.8 (88.4)</td><td char="(" align="char">93.6 (92.5)</td><td char="(" align="char">94.5 (93.4)</td></tr><tr><td align="left">F1-score</td><td char="(" align="char">92.6 (90.8)</td><td char="(" align="char">94 (91)</td><td char="(" align="char">92.9 (90.3)</td><td char="(" align="char">91.6 (89.5)</td><td char="(" align="char">92.6 (90)</td><td char="(" align="char">89.6 (88.2)</td><td char="(" align="char">93.7 (92.6)</td><td char="(" align="char">94.5 (93.4)</td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">DCNN</th><th align="left">SVM</th><th align="left">GBDT</th><th align="left">LightGBM</th><th align="left">gcForest</th><th align="left">SAE</th><th align="left">BiGRU</th><th align="left">DCGN</th></tr><tr><th align="left">Dataset</th><th align="left" colspan="8">BLCA-TCGA</th></tr></thead><tbody><tr><td align="left">Accuracy</td><td char="(" align="char">93.3 (91.5)</td><td char=" (" align="char">98.3 (97)</td><td char=" (" align="char">98.4 (96.7)</td><td char=" (" align="char">98.5 (96.4)</td><td char=" (" align="char">98.5 (97.3)</td><td char=" (" align="char">95.4 (94.7)</td><td char=" (" align="char">97.4 (96.2)</td><td char=" (" align="char">99.3 (98.2)</td></tr><tr><td align="left">Precision</td><td char="(" align="char">97.3 (95.8)</td><td char=" (" align="char">98.5 (97.2)</td><td char=" (" align="char">98.3 (97)</td><td char=" (" align="char">98.6 (96.4)</td><td char=" (" align="char">98.6 (97.5)</td><td char=" (" align="char">95.7 (95)</td><td char=" (" align="char">97.4 (96.5)</td><td char=" (" align="char">99.4 (98.4)</td></tr><tr><td align="left">Recall</td><td char="(" align="char">96 (94.2)</td><td char=" (" align="char">98.4 (97)</td><td char=" (" align="char">98.3 (96.7)</td><td char=" (" align="char">98.4 (96.3)</td><td char=" (" align="char">98.6 (97.3)</td><td char=" (" align="char">95.4 (94.3)</td><td char=" (" align="char">97.3 (96.4)</td><td char=" (" align="char">99.3 (98.2)</td></tr><tr><td align="left">F1-score</td><td char="(" align="char">96.4 (94.8)</td><td char=" (" align="char">98.4 (97)</td><td char=" (" align="char">98.2 (96.8)</td><td char=" (" align="char">98.4 (96.5)</td><td char=" (" align="char">98.4 (97.4)</td><td char=" (" align="char">95.5 (94.6)</td><td char=" (" align="char">97.4 (96.4)</td><td char=" (" align="char">99.3 (98.2)</td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left">Methods</th><th align="left">DCNN</th><th align="left">SVM</th><th align="left">GBDT</th><th align="left">LightGBM</th><th align="left">gcForest</th><th align="left">SAE</th><th align="left">BiGRU</th><th align="left">DCGN</th></tr><tr><th align="left">Dataset</th><th align="left" colspan="8">BLCA-CIT-Curie</th></tr></thead><tbody><tr><td align="left">Accuracy</td><td char="(" align="char">96.1 (94.3)</td><td char="." align="char">98.5 (97)</td><td char="(" align="char">98.2 (97.2)</td><td char="(" align="char">97.8 (95.7)</td><td char="." align="char">98.3 (97.3)</td><td char="(" align="char">98.3 (97.4)</td><td char="." align="char">97.8 (96.8)</td><td char="." align="char">99.4 (98.5)</td></tr><tr><td align="left">Precision</td><td char="(" align="char">98.5 (97)</td><td char="." align="char">98.5 (97.2)</td><td char="(" align="char">98.3 (97.5)</td><td char="(" align="char">98 (96)</td><td char="." align="char">98.4 (97.4)</td><td char="(" align="char">98.4 (97.3)</td><td char="." align="char">97.9 (97)</td><td char="." align="char">99.5 (98.7)</td></tr><tr><td align="left">Recall</td><td char="(" align="char">98 (96.9)</td><td char="." align="char">98.4 (97.1)</td><td char="(" align="char">98 (97)</td><td char="(" align="char">97.8 (95.7)</td><td char="." align="char">98.2 (97.4)</td><td char="(" align="char">98.2 (97.4)</td><td char="." align="char">97.6 (96.8)</td><td char="." align="char">99.4 (98.5)</td></tr><tr><td align="left">F1-score</td><td char="(" align="char">98.1 (96.8)</td><td char="." align="char">98.3 (97)</td><td char="(" align="char">98.1 (97.2)</td><td char="(" align="char">97.7 (95.6)</td><td char="." align="char">98.2 (97.3)</td><td char="(" align="char">98 (97.3)</td><td char="." align="char">97.8 (96.9)</td><td char="." align="char">99.4 (98.5)</td></tr></tbody></table><table-wrap-foot><p>The value in front of () represents the highest-level result, and the value in () represents the average result over ten iterations</p></table-wrap-foot></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Hamming distance and Kappa coefficient values obtained when applying models to BLCA datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="4">Kappa</th><th align="left" colspan="4">Hamming distance</th></tr><tr><th align="left">Dataset</th><th align="left">MDA</th><th align="left">Lund</th><th align="left">TCGA</th><th align="left">CIT-Curie</th><th align="left">MDA</th><th align="left">Lund</th><th align="left">TCGA</th><th align="left">CIT-Curie</th></tr></thead><tbody><tr><td align="left">DCNN</td><td char="." align="char">0.896</td><td char="." align="char">0.908</td><td char="." align="char">0.956</td><td char="." align="char">0.978</td><td char="." align="char">0.067</td><td char="." align="char">0.081</td><td char="." align="char">0.024</td><td char="." align="char">0.02</td></tr><tr><td align="left">SVM</td><td char="." align="char">0.91</td><td char="." align="char">0.93</td><td char="." align="char">0.98</td><td char="." align="char">0.984</td><td char="." align="char">0.059</td><td char="." align="char">0.06</td><td char="." align="char">0.019</td><td char="." align="char">0.014</td></tr><tr><td align="left">GBDT</td><td char="." align="char">0.895</td><td char="." align="char">0.924</td><td char="." align="char">0.982</td><td char="." align="char">0.979</td><td char="." align="char">0.069</td><td char="." align="char">0.067</td><td char="." align="char">0.016</td><td char="." align="char">0.017</td></tr><tr><td align="left">LightGBM</td><td char="." align="char">0.91</td><td char="." align="char">0.902</td><td char="." align="char">0.983</td><td char="." align="char">0.975</td><td char="." align="char">0.059</td><td char="." align="char">0.087</td><td char="." align="char">0.015</td><td char="." align="char">0.02</td></tr><tr><td align="left">gcForest</td><td char="." align="char">0.881</td><td char="." align="char">0.915</td><td char="." align="char">0.983</td><td char="." align="char">0.98</td><td char="." align="char">0.079</td><td char="." align="char">0.075</td><td char="." align="char">0.015</td><td char="." align="char">0.016</td></tr><tr><td align="left">SAE</td><td char="." align="char">0.895</td><td char="." align="char">0.898</td><td char="." align="char">0.941</td><td char="." align="char">0.98</td><td char="." align="char">0.069</td><td char="." align="char">0.096</td><td char="." align="char">0.045</td><td char="." align="char">0.016</td></tr><tr><td align="left">BiGRU</td><td char="." align="char">0.903</td><td char="." align="char">0.929</td><td char="." align="char">0.966</td><td char="." align="char">0.973</td><td char="." align="char">0.064</td><td char="." align="char">0.063</td><td char="." align="char">0.021</td><td char="." align="char">0.022</td></tr><tr><td align="left">DCGN</td><td char="." align="char">0.933</td><td char="." align="char">0.938</td><td char="." align="char">0.991</td><td char="." align="char">0.993</td><td char="." align="char">0.044</td><td char="." align="char">0.054</td><td char="." align="char">0.006</td><td char="." align="char">0.005</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec24">
    <title>Discussion</title>
    <p id="Par51">The method proposed herein still has some limitations. In this study, our method exhibits decent classification performances when applied to breast cancer and bladder cancer datasets. The subtype classifications of these cancers have been established; however, there are many cancers for which subtypes have not been accurately identified. Therefore, supervised learning of these other cancers is not yet possible. In addition, our approach to cancer classification is based solely on gene expression data. However, several recent studies have shown [<xref ref-type="bibr" rid="CR35">35</xref>] that combining genomic data from different platforms can reveal more valid information about cancer subtypes; thus, this will characterize the next step of our research.
</p>
  </sec>
  <sec id="Sec25">
    <title>Conclusions</title>
    <p id="Par52">In this paper, we proposed the DCGN, a deep learning method for cancer multiclassification tasks; this proposed model can better handle high-dimensional cancer data than other available models. In terms of classification evaluation indicators, such as the accuracy and precision, the DCGN performs well on all five analysed datasets, especially on the BLCA-TCGA and BLCA-CIT datasets, with values exceeding 99%. These results show that the method proposed in this paper can obtain ideal classification results and has a superior generalization ability. However, this paper considers only gene expression data, and we will integrate multiomics data to further study cancer subtype classification in future work.
</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec26">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2022_4980_MOESM1_ESM.docx">
            <caption>
              <p><bold>Additional file 1:</bold> Deep learning approach for cancer subtype classification using high-dimensional gene expression data.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par3">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>BiGRU</term>
        <def>
          <p id="Par4">Bidirectional gated recurrent unit</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par5">Support vector machines</p>
        </def>
      </def-item>
      <def-item>
        <term>ABC</term>
        <def>
          <p id="Par6">Artificial bee colony algorithm</p>
        </def>
      </def-item>
      <def-item>
        <term>SAE</term>
        <def>
          <p id="Par7">Stacked autoencoder</p>
        </def>
      </def-item>
      <def-item>
        <term>GBDT</term>
        <def>
          <p id="Par8">Gradient boost decision tree</p>
        </def>
      </def-item>
      <def-item>
        <term>BRCA</term>
        <def>
          <p id="Par9">Breast cancer susceptibility genes</p>
        </def>
      </def-item>
      <def-item>
        <term>BLCA</term>
        <def>
          <p id="Par10">Bladder urothelial carcinoma</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>JQS, JWS, and JWL participated in the design of the study and the analysis of the experimental results. JQS, JWS and ZJW performed the implementation. HML and CKY prepared the tables and figures. JQS, HXZ and XYL summarized the results of the study and checked the format of the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work has been supported in part by the National Natural Science Foundation of China under Grant No.61972134, Young Elite Teachers in Henan Province No. 2020GGJS050, Doctor Foundation of Henan Polytechnic University under Grant No.B2018-36, Innovative and Scientific Research Team of Henan Polytechnic University under No.T2021-3.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and material</title>
    <p>The BRCA unenhanced dataset can be obtained from <ext-link ext-link-type="uri" xlink:href="http://www.acsu.buffalo.edu/~yijunsun/lab/DeepType.html">http://www.acsu.buffalo.edu/~yijunsun/lab/DeepType.html</ext-link>. The BLCA unenhanced dataset is derived from the TCGA project. The software as part of this project is readily avail- able from GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/shijwe/DCGN">https://github.com/shijwe/DCGN</ext-link></p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par53">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par54">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par55">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanahan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Weinberg</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>Hallmarks of cancer: the next generation</article-title>
        <source>Cell</source>
        <year>2011</year>
        <volume>144</volume>
        <issue>5</issue>
        <fpage>646</fpage>
        <lpage>674</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2011.02.013</pub-id>
        <pub-id pub-id-type="pmid">21376230</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nowak</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Goodison</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Computational approach for deriving cancer progression roadmaps from static sample data</article-title>
        <source>Nucleic Acids Res</source>
        <year>2017</year>
        <volume>45</volume>
        <issue>9</issue>
        <fpage>e69</fpage>
        <?supplied-pmid 28108658?>
        <pub-id pub-id-type="pmid">28108658</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Curtis</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Chin</surname>
            <given-names>S-F</given-names>
          </name>
          <name>
            <surname>Turashvili</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rueda</surname>
            <given-names>OM</given-names>
          </name>
          <name>
            <surname>Dunning</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Speed</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lynch</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Samarajiwa</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The genomic and transcriptomic architecture of 2,000 breast tumours reveals novel subgroups</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>486</volume>
        <issue>7403</issue>
        <fpage>346</fpage>
        <lpage>352</lpage>
        <pub-id pub-id-type="doi">10.1038/nature10983</pub-id>
        <pub-id pub-id-type="pmid">22522925</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Parker</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Mullins</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Cheang</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Voduc</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Vickery</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Davies</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fauron</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Supervised risk predictor of breast cancer based on intrinsic subtypes</article-title>
        <source>J Clin Oncol</source>
        <year>2009</year>
        <volume>27</volume>
        <issue>8</issue>
        <fpage>1160</fpage>
        <lpage>1167</lpage>
        <pub-id pub-id-type="doi">10.1200/JCO.2008.18.1370</pub-id>
        <pub-id pub-id-type="pmid">19204204</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Goodison</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning approach to identifying cancer subtypes using high-dimensional genomic data</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>36</volume>
        <issue>5</issue>
        <fpage>1476</fpage>
        <lpage>1483</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz769</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Soh</surname>
            <given-names>KP</given-names>
          </name>
          <name>
            <surname>Szczurek</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Sakoparnig</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting cancer type from tumour DNA signatures</article-title>
        <source>Genome Med</source>
        <year>2017</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>104</fpage>
        <pub-id pub-id-type="doi">10.1186/s13073-017-0493-2</pub-id>
        <pub-id pub-id-type="pmid">29183400</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support-Vector Networks</article-title>
        <source>Mach Learn</source>
        <year>1995</year>
        <volume>20</volume>
        <issue>3</issue>
        <fpage>273</fpage>
        <lpage>297</lpage>
        <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ye</surname>
            <given-names>MQ</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>LY</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>CHY</given-names>
          </name>
        </person-group>
        <article-title>Gene expression data classification based on artificial bee colony and SVM</article-title>
        <source>J Shandong Univ (Engineering Edition)</source>
        <year>2018</year>
        <volume>48</volume>
        <issue>03</issue>
        <fpage>10</fpage>
        <lpage>16</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karaboga</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Artificial bee colony algorithm</article-title>
        <source>Scholarpedia</source>
        <year>2010</year>
        <volume>5</volume>
        <issue>3</issue>
        <fpage>6915</fpage>
        <pub-id pub-id-type="doi">10.4249/scholarpedia.6915</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Duan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>SH</given-names>
          </name>
        </person-group>
        <article-title>Study of cancer subtype classification model based on gene expression profile</article-title>
        <source>Math Model Appl</source>
        <year>2021</year>
        <volume>10</volume>
        <issue>3</issue>
        <fpage>7</fpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Identification of cancer subtypes by integrating multiple types of transcriptomics data with deep learning in breast cancer</article-title>
        <source>Neurocomputing</source>
        <year>2018</year>
        <volume>324</volume>
        <issue>9</issue>
        <fpage>20</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Bengio Y, Lamblin P, Popovici D, et al. Greedy layer-wise training of deep networks[C]// Advances in Neural Information Processing Systems 19, In: Proceedings of the twentieth annual conference on neural information processing systems, Vancouver, British Columbia, Canada, 2006. DBLP, 2007.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Classification of gene expression data based on Boosting</article-title>
        <source>Xi'an Univ Electron Sci Technol</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.27389/d.cnki.gxadu.2019.002388</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Research on cancer diagnosis based on deep learning of gene expression data</article-title>
        <source>Shanghai Jiaotong Univ</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.27307/d.cnki.gsjtu.2020.000051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Majumder</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Performance analysis of deep learning models for binary classification of cancer gene expression data</article-title>
        <source>J Healthc Eng</source>
        <year>2022</year>
        <volume>2022</volume>
        <fpage>1122536</fpage>
        <lpage>1122536</lpage>
        <pub-id pub-id-type="doi">10.1155/2022/1122536</pub-id>
        <pub-id pub-id-type="pmid">35310177</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>NV</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SMOTE: synthetic minority over-sampling technique</article-title>
        <source>J Artif Intell Res</source>
        <year>2002</year>
        <volume>16</volume>
        <fpage>321</fpage>
        <lpage>357</lpage>
        <pub-id pub-id-type="doi">10.1613/jair.953</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lecun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Boser</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Denker</surname>
            <given-names>JS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Backpropagation applied to handwritten zip code</article-title>
        <source>Neural Comput</source>
        <year>1989</year>
        <volume>1</volume>
        <fpage>541</fpage>
        <lpage>551</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Cho K, Merrienboer BV, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation. Comput Sci. 2014;1406.1078.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. CoRR, 2013, abs/1311.2901</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Chung J, Gulcehre C, Cho KH, et al. Empirical evaluation of gated recurrent neural networks on sequence modeling. Eprint Arxiv, 2014.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">YiÄit G, Amasyali MF, Simple but effective GRU variants. In: 2021 international conference on INnovations in intelligent SysTems and applications (INISTA), 2021, pp. 1â6. 10.1109/INISTA52262.2021.9548535</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Hendrycks D, Gimpel K. Gaussian error linear units (GELUs). 2016.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Porten</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of distinct basal and luminal subtypes of muscle-invasive bladder cancer with different sensitivities to frontline chemotherapy</article-title>
        <source>Cancer Cell</source>
        <year>2014</year>
        <volume>25</volume>
        <fpage>152</fpage>
        <lpage>165</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ccr.2014.01.009</pub-id>
        <pub-id pub-id-type="pmid">24525232</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robertson</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Al-Ahmadie</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive molecular characterization of muscle-invasive bladder cancer</article-title>
        <source>Cell</source>
        <year>2017</year>
        <volume>171</volume>
        <fpage>540</fpage>
        <lpage>56.e25</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2017.09.007</pub-id>
        <pub-id pub-id-type="pmid">28988769</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rebouissou</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bernard-Pierrot</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>de ReyniÃ¨s</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>EGFR as a potential therapeutic target for a subset of muscle-invasive bladder cancers presenting a basal-like phenotype</article-title>
        <source>Sci Transl Med</source>
        <year>2014</year>
        <volume>6</volume>
        <fpage>244ra91</fpage>
        <pub-id pub-id-type="doi">10.1126/scitranslmed.3008970</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marzouka</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Eriksson</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Rovira</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Liedberg</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>SjÃ¶dahl</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>HÃ¶glund</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A validation and extended description of the Lund taxonomy for urothelial carcinoma using the TCGA cohort</article-title>
        <source>Sci Rep</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>3737</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-22126-x</pub-id>
        <pub-id pub-id-type="pmid">29487377</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Kamoun A, De ReyniÃ¨s A, Allory Y, et al. A consensus molecular classification of muscle-invasive bladder cancer. Social Science Electronic Publishing.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: a method for stochastic optimization. In: International conference on learning representations, 2014. pp. 1â13.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Zhou ZH, Feng J. Deep forest: towards an alternative to deep neural networks. 2017.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>Greedy function approximation: a gradient boosting machine</article-title>
        <source>Ann Stat</source>
        <year>2001</year>
        <volume>29</volume>
        <issue>5</issue>
        <fpage>1189</fpage>
        <lpage>1232</lpage>
        <pub-id pub-id-type="doi">10.1214/aos/1013203451</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Qi M. LightGBM: a highly efficient gradient boosting decision tree[C]// Neural Information Processing Systems. Curran Associates Inc. 2017.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Deng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An improved method to construct basic probability assignment based on the confusion matrix for classification problem</article-title>
        <source>Inf Sci</source>
        <year>2016</year>
        <volume>340</volume>
        <fpage>250</fpage>
        <lpage>261</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2016.01.033</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wan</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Jun</surname>
            <given-names>HU</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Kappa coefficient: a popular measure of rater agreement</article-title>
        <source>Shanghai Arch Psychiatry</source>
        <year>2015</year>
        <volume>27</volume>
        <fpage>62</fpage>
        <pub-id pub-id-type="pmid">25852260</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sanchez-Reillo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Tamer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <source>Hamming distance</source>
        <year>2009</year>
        <publisher-loc>US</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mo</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Sparse integrative clustering of multiple omics data sets</article-title>
        <source>Ann Appl Stat</source>
        <year>2013</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>269</fpage>
        <lpage>294</lpage>
        <pub-id pub-id-type="doi">10.1214/12-AOAS578</pub-id>
        <pub-id pub-id-type="pmid">24587839</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
