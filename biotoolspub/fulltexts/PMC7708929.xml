<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_YNICL102499 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEgr8 jpg ?>
<?FILEgr9 jpg ?>
<?FILEgr10 jpg ?>
<?FILEmmc1 docx ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Neuroimage Clin</journal-id>
    <journal-id journal-id-type="iso-abbrev">Neuroimage Clin</journal-id>
    <journal-title-group>
      <journal-title>NeuroImage : Clinical</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2213-1582</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7708929</article-id>
    <article-id pub-id-type="pii">S2213-1582(20)30336-3</article-id>
    <article-id pub-id-type="doi">10.1016/j.nicl.2020.102499</article-id>
    <article-id pub-id-type="publisher-id">102499</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Regular Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Progressive multifocal leukoencephalopathy lesion and brain parenchymal segmentation from MRI using serial deep convolutional neural networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Al-Louzi</surname>
          <given-names>Omar</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af010" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>Roy</surname>
          <given-names>Snehashis</given-names>
        </name>
        <xref rid="af015" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au015">
        <name>
          <surname>Osuorah</surname>
          <given-names>Ikesinachi</given-names>
        </name>
        <xref rid="af010" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au020">
        <name>
          <surname>Parvathaneni</surname>
          <given-names>Prasanna</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au025">
        <name>
          <surname>Smith</surname>
          <given-names>Bryan R.</given-names>
        </name>
        <xref rid="af020" ref-type="aff">d</xref>
      </contrib>
      <contrib contrib-type="author" id="au030">
        <name>
          <surname>Ohayon</surname>
          <given-names>Joan</given-names>
        </name>
        <xref rid="af010" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au035">
        <name>
          <surname>Sati</surname>
          <given-names>Pascal</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af025" ref-type="aff">e</xref>
      </contrib>
      <contrib contrib-type="author" id="au040">
        <name>
          <surname>Pham</surname>
          <given-names>Dzung L.</given-names>
        </name>
        <xref rid="af030" ref-type="aff">f</xref>
      </contrib>
      <contrib contrib-type="author" id="au045">
        <name>
          <surname>Jacobson</surname>
          <given-names>Steven</given-names>
        </name>
        <xref rid="af035" ref-type="aff">g</xref>
      </contrib>
      <contrib contrib-type="author" id="au050">
        <name>
          <surname>Nath</surname>
          <given-names>Avindra</given-names>
        </name>
        <xref rid="af020" ref-type="aff">d</xref>
      </contrib>
      <contrib contrib-type="author" id="au055">
        <name>
          <surname>Reich</surname>
          <given-names>Daniel S.</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af010" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au060">
        <name>
          <surname>Cortese</surname>
          <given-names>Irene</given-names>
        </name>
        <email>corteseir@ninds.nih.gov</email>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="af005"><label>a</label>Translational Neuroradiology Section, National Institute of Neurological Disorders and Stroke, Bethesda, MD, USA</aff>
      <aff id="af010"><label>b</label>Neuroimmunology Clinic, National Institute of Neurological Disorders and Stroke, Bethesda, MD, USA</aff>
      <aff id="af015"><label>c</label>Section of Neural Function, National Institute of Mental Health, Bethesda, MD, USA</aff>
      <aff id="af020"><label>d</label>Section of Infections of the Nervous System, National Institute of Neurological Disorders and Stroke, Bethesda, MD, USA</aff>
      <aff id="af025"><label>e</label>Department of Neurology, Cedars-Sinai Medical Center, Los Angeles, CA, USA</aff>
      <aff id="af030"><label>f</label>Center for Neuroscience and Regenerative Medicine, The Henry M. Jackson Foundation for the Advancement of Military Medicine, Bethesda, MD, USA</aff>
      <aff id="af035"><label>g</label>Viral Immunology Section, National Institute of Neurological Disorders and Stroke, Bethesda, MD, USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author. <email>corteseir@ninds.nih.gov</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <volume>28</volume>
    <elocation-id>102499</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>2</day>
        <month>11</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>3</day>
        <month>11</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <license license-type="CC BY-NC-ND" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="author-highlights" id="ab005">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="l0055">
          <list-item id="o0195">
            <label>•</label>
            <p id="p0195">PML has characteristic dynamic changes in brain and lesion volume on MRI.</p>
          </list-item>
          <list-item id="o0200">
            <label>•</label>
            <p id="p0200">JCnet is an automated method for brain atrophy and lesion segmentation in PML.</p>
          </list-item>
          <list-item id="o0205">
            <label>•</label>
            <p id="p0205">JCnet improves PML lesion segmentation accuracy compared to conventional methods.</p>
          </list-item>
          <list-item id="o0210">
            <label>•</label>
            <p id="p0210">JCnet can accurately track PML lesion changes over time.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract id="ab010">
      <p>Progressive multifocal leukoencephalopathy (PML) is a rare opportunistic brain infection caused by the JC virus and associated with substantial morbidity and mortality. Accurate MRI assessment of PML lesion burden and brain parenchymal atrophy is of decisive value in monitoring the disease course and response to therapy. However, there are currently no validated automatic methods for quantification of PML lesion burden or associated parenchymal volume loss. Furthermore, manual brain or lesion delineations can be tedious, require the use of valuable time resources by radiologists or trained experts, and are often subjective. In this work, we introduce JCnet (named after the causative viral agent), an end-to-end, fully automated method for brain parenchymal and lesion segmentation in PML using consecutive 3D patch-based convolutional neural networks. The network architecture consists of multi-view feature pyramid networks with hierarchical residual learning blocks containing embedded batch normalization and nonlinear activation functions. The feature maps across the bottom-up and top-down pathways of the feature pyramids are merged, and an output probability membership generated through convolutional pathways, thus rendering the method fully convolutional. Our results show that this approach outperforms and improves longitudinal consistency compared to conventional, state-of-the-art methods of healthy brain and multiple sclerosis lesion segmentation, utilized here as comparators given the lack of available methods validated for use in PML. The ability to produce robust and accurate automated measures of brain atrophy and lesion segmentation in PML is not only valuable clinically but holds promise toward including standardized quantitative MRI measures in clinical trials of targeted therapies. Code is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/omarallouz/JCnet" id="ir005">https://github.com/omarallouz/JCnet</ext-link>.</p>
    </abstract>
    <kwd-group id="kg005">
      <title>Keywords</title>
      <kwd>Progressive multifocal leukoencephalopathy</kwd>
      <kwd>Magnetic resonance imaging</kwd>
      <kwd>Convolutional neural networks</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Lesion segmentation</kwd>
      <kwd>Brain parenchymal fraction</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0215">Progressive multifocal leukoencephalopathy (PML) is a rare opportunistic brain infection caused by the JC virus (JCV), a human polyoma virus. PML almost uniformly affects patients with significant immunocompromise, such as HIV/AIDS, lymphoproliferative or myeloproliferative disorders, inherited/acquired immunodeficiency, or drug-induced immunosuppression (<xref rid="b0130" ref-type="bibr">Major et al., 2018</xref>). The estimated prevalence of PML has been reported to be between 0.07% for patients with hematologic malignancies, and up to 5% in patients with HIV/AIDS (<xref rid="b0150" ref-type="bibr">Power et al., 2000</xref>). Magnetic resonance imaging (MRI) is considered the gold standard method for the identification and monitoring of PML lesions in vivo. On MRI, PML lesions appear as multifocal, patchy, and/or confluent areas of hyperintensity on T2-weighted sequences, often with corresponding hypointensity on T1-weighted images (<xref rid="b0200" ref-type="bibr">Tan and Koralnik, 2010</xref>). This infection is associated with a wide range of mortality rates (dependent on the cause of the underlying immunosuppression), with substantial persistent morbidity and disability amongst PML survivors (<xref rid="b0015" ref-type="bibr">Carson et al., 2009</xref>, <xref rid="b0040" ref-type="bibr">Eng et al., 2006</xref>, <xref rid="b0065" ref-type="bibr">Hadjadj et al., 2019</xref>).</p>
    <p id="p0220">There are no approved therapies for PML to-date, however recent studies employing immunomodulatory strategies have shown promising results (<xref rid="b0030" ref-type="bibr">Cortese et al., 2019</xref>, <xref rid="b0135" ref-type="bibr">Muftuoglu et al., 2018</xref>). The development of novel treatments for PML would be facilitated by widely available tools that can accurately track PML lesion and global/regional brain volume loss that occurs as part of this condition. This information is valuable for clinical applications and has the potential to be incorporated as an outcome measure for future investigational studies.</p>
    <p id="p0225">Despite the characteristic appearance of PML on MRI, a number of factors pose unique technical challenges when it comes to developing methods for automated lesion and brain volume segmentation, including the multifocal nature of PML and frequent involvement of infratentorial regions, an area prone to artifacts on commonly acquired MRI sequences (<xref rid="f0005" ref-type="fig">Fig. 1</xref>, Panels A and B). Many PML patients undergo brain biopsies as part of their diagnostic work-up, introducing further distortions to the cranium and outer aspects of the brain (<xref rid="f0005" ref-type="fig">Fig. 1</xref>, Panel C). Furthermore, the rarity of PML limits the availability of large, well-characterized imaging datasets for training and testing implementations.<fig id="f0005"><label>Fig. 1</label><caption><p>Illustration of the different challenges unique to progressive multifocal leukoencephalopathy (PML) lesion and brain segmentation on fluid attenuated inversion recovery (FLAIR) and T1-weighted MRI sequences. Given the multifocal nature of PML, there is often a preponderance of infratentorial structure involvement, including the middle cerebellar peduncles (Panel A, red arrows). PML lesions are often associated with confluent areas of T1 hypointensity with overlying cortical thinning, as seen in the left anterior frontal lobe in Panel B (red arrowheads), which can be readily misclassified as cerebrospinal fluid by conventional methods. Many patients with PML undergo brain biopsies as part of their diagnostic work-up, resulting in further cranial and outer brain parenchymal distortions on imaging, as illustrated in the right parietooccipital cortex and subcortical white matter in Panel C (asterisk). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr1"/></fig></p>
    <p id="p0230">Earlier studies attempting to quantify PML lesion volume on MRI have utilized methods based on region growing and adaptive thresholding (<xref rid="b0090" ref-type="bibr">Itti et al., 2001</xref>). These methods require manual input to set the seed point(s) and can work well with a limited number of lesions. However, this task can quickly become tedious when many discrete, non-contiguous lesions are present. This approach can also be particularly prone to image artifacts and brain- or lesion-shape irregularities, thereby limiting its generalizability to larger PML datasets. More recently, advances in supervised machine learning approaches for object detection and semantic segmentation have introduced significant improvements in segmentation accuracy on brain MRI of lesions in various neurological disorders, such as multiple sclerosis (MS) lesions (<xref rid="b0110" ref-type="bibr">La Rosa et al., 2020</xref>, <xref rid="b0160" ref-type="bibr">Roy et al., 2018a</xref>, <xref rid="b0205" ref-type="bibr">Valverde et al., 2017</xref>), HIV and human T-cell leukemia virus type 1 (HTLV-1) associated brain lesions (<xref rid="b0185" ref-type="bibr">Selvaganesan et al., 2019</xref>), brain gliomas (<xref rid="b0145" ref-type="bibr">Pereira et al., 2016</xref>, <xref rid="b0215" ref-type="bibr">Yi et al., 2016</xref>), and ischemic strokes (<xref rid="b0060" ref-type="bibr">Guerrero et al., 2018</xref>), as well as of brain substructure segmentation (<xref rid="b0210" ref-type="bibr">Wachinger et al., 2018</xref>).</p>
    <p id="p0235">Currently, no specific deep learning approaches have been tailored for PML lesion segmentation or the measurement of concomitant brain atrophy, an important paraclinical marker of disease progression and neuronal loss. Therefore, we designed a 3D deep convolutional neural network (CNN) that can be employed for robust, fully automated PML brain parenchymal and lesion segmentation on MRI scans using a serial approach that we have dubbed ‘JCnet,’ named after the causative viral agent. In PML, as well as in the segmentation of brain pathologies in general, healthy tissue can be present in greater abundance than that of the pathological target in segmentation applications, resulting in voxel-level class imbalance. An important methodological contribution of the work presented here is the serial architecture employed, whereby the first CNN performs candidate extraction of brain parenchymal voxels as foreground and the meninges as well as cerebrospinal fluid (CSF) spaces as background, followed by a second CNN trained to perform PML lesion segmentation on the extracted foreground voxels. Our design helps address the issue of class imbalance by excluding voxels corresponding to structures not relevant to the lesion segmentation task (namely the meninges and CSF spaces), while simultaneously allowing the generation of a brain parenchymal mask that can be utilized to track the degree of brain atrophy in PML, an important marker of neuronal degeneration and brain volume loss in neuroimmunological conditions (<xref rid="b0170" ref-type="bibr">Rudick et al., 1999</xref>).</p>
    <p id="p0240">Given the lack of widely available methods specific to PML, we evaluate JCnet against several approaches designed for normal-appearing brain and MS lesion segmentation. In both cases, we show significant improvements of performance in PML with an accuracy that approaches that achieved by a trained human rater. We outline our approach in this paper in chronological order. In <xref rid="s0010" ref-type="sec">Section 2</xref>, we describe patient recruitment, MRI data acquisition, and preprocessing. In <xref rid="s0030" ref-type="sec">Section 3</xref>, we describe the method, implementation, and training specifications. Experimental evaluation and results on the testing dataset are presented in <xref rid="s0035" ref-type="sec">Section 4</xref>, followed by a discussion of the proposed network architecture in the context of unmet needs in PML and future directions in <xref rid="s0060" ref-type="sec">Section 5</xref>.</p>
  </sec>
  <sec id="s0010">
    <label>2</label>
    <title>Materials</title>
    <sec id="s0015">
      <label>2.1</label>
      <title>Study population</title>
      <p id="p0245">Scans included in this analysis were selected via retrospective review of patients with PML who were seen at the NIH Neuroimmunology Clinic and evaluated under the Natural History Study of PML (ClinicalTrials.gov number, NCT01730131). The study protocol was approved by the NIH institutional review board, and all the participants provided written informed consent prior to study enrollment. Demographics and clinical characteristics were obtained through electronic chart review. Study participants underwent clinical evaluations, including neurologic examinations and disability measurement at each study visit. PML diagnosis was confirmed by the treating neurologists in accordance with the 2013 American Academy of Neurology Neuroinfectious Disease Section diagnostic criteria (<xref rid="b0010" ref-type="bibr">Berger et al., 2013</xref>). Scans from a total of 41 patients with PML were included in the final analysis.</p>
    </sec>
    <sec id="s0020">
      <label>2.2</label>
      <title>Image acquisition</title>
      <p id="p0250">To improve the generalizability of the trained models, MRI scans acquired on either a Siemens Skyra 3T scanner equipped with a body transmit and a 32-channel receive coils, or a 3T Philips MRI scanner (Philips Medical Systems, Netherlands) equipped with an 8- or 32-channel receive head coils were included in the analysis. Four whole-brain MRI sequences without gaps were used for training and testing implementations: whole brain 3D T1-weighted magnetization-prepared rapid acquisition of gradient echoes (T1-MPRAGE), 3D T2-weighted fluid-attenuated inversion recovery (FLAIR), and multislice T2-weighted (T2) and proton density (PD) sequences (acquired via a dual-echo fast-spin-echo sequence). The MR acquisition parameters for these sequences per scanner are detailed in <xref rid="t0005" ref-type="table">Table 1</xref>.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>MR acquisition parameters for the sequences utilized in the study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>3D-T1 MPRAGE*</th><th>3D-FLAIR</th><th>2D-FSE T2/PD</th></tr></thead><tbody><tr><td colspan="4"><italic>Siemens Skyra 3T MRI scanner (16/41 scans)*</italic></td></tr><tr><td>Slice thickness (mm)</td><td>1</td><td>1</td><td>3</td></tr><tr><td>Inversion time (ms)</td><td>900</td><td>1800</td><td>–</td></tr><tr><td>Echo time (ms)</td><td>1.7</td><td>352–354</td><td>18, 82</td></tr><tr><td>Repetition time (ms)</td><td>3000</td><td>4800</td><td>3000 or 5000</td></tr><tr><td>Flip angle (deg)</td><td>9</td><td>120</td><td>150</td></tr><tr><td>Number of repetitions</td><td>1</td><td>1</td><td>1</td></tr><tr><td colspan="4">  </td></tr><tr><td colspan="4"><italic>Philips 3T MRI scanner (25/41 scans)</italic></td></tr><tr><td>Slice thickness (mm)</td><td>0.73 or 1</td><td>0.75, 1, or 1.117</td><td>3</td></tr><tr><td>Inversion time (ms)</td><td>900</td><td>1600–1650</td><td>–</td></tr><tr><td>Echo time (ms)</td><td>3.2</td><td>276–365</td><td>15.38, 100</td></tr><tr><td>Repetition time (ms)</td><td>7</td><td>4800</td><td>3410–3763</td></tr><tr><td>Flip angle (deg)</td><td>9</td><td>90</td><td>90</td></tr><tr><td>Number of repetitions</td><td>1</td><td>1</td><td>1</td></tr></tbody></table><table-wrap-foot><fn id="sp0070"><p>* A total of 6 scans on the Siemens Skyra were acquired using the T1-weighted MP2RAGE protocol (repetition time = 5000 ms, echo time = 2.9 ms, inversion time = 700 ms/2500 ms, flip angle = 4˚/5˚).</p></fn><fn id="sp0075"><p>Abbreviations: 3D = 3 dimensional; deg = degrees; FLAIR = fluid-attenuated inversion recovery; MPRAGE = magnetization-prepared rapid acquisition of gradient echoes; FSE = fast spin echo; ms = millisecond; PD = proton density; T = tesla; T2 = T2-weighted sequence.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="s0025">
      <label>2.3</label>
      <title>Image preprocessing</title>
      <p id="p0255">MR image preprocessing was undertaken using the FMRIB Software (FSL) and Advanced Neuroimaging Tools (ANTs) open source software libraries (<xref rid="b0005" ref-type="bibr">Avants et al., 2011</xref>, <xref rid="b0095" ref-type="bibr">Jenkinson et al., 2012</xref>). The T1-weighted images were initially rigidly registered to the Montreal Neurological Institute (MNI)-152 and International Consortium for Brain Mapping (ICBM) nonlinear symmetric 1x1x1mm atlas template that is publicly available for download (<ext-link ext-link-type="uri" xlink:href="http://nist.mni.mcgill.ca/%3fp%3d904" id="ir010">http://nist.mni.mcgill.ca/?p=904</ext-link>) (<xref rid="b0055" ref-type="bibr">Fonov et al., 2009</xref>). The skull and extracranial tissues were removed using the MONSTR algorithm (<xref rid="b0155" ref-type="bibr">Roy et al., 2017</xref>) and corrected for any inhomogeneity using the Multiplicative Intrinsic Component Optimization (MICO) method for bias-field estimation (<xref rid="b0115" ref-type="bibr">Li et al., 2014</xref>). Subsequently, the other contrasts, i.e. FLAIR, T2, and PD images, were rigidly co-registered to the T1-weighted image in MNI space, skull-stripped with the same binary mask, and corrected by MICO in a similar fashion.</p>
    </sec>
  </sec>
  <sec id="s0030">
    <label>3</label>
    <title>Reference labeled mask creation</title>
    <p id="p0260">To generate the ground truth brain parenchymal masks, we analyzed the T1-weighted and FLAIR images using the Lesion-TOADS (Topology-preserving Anatomy-Driven Segmentation) algorithm (<xref rid="b0190" ref-type="bibr">Shiee et al., 2010</xref>). The final brain parenchymal masks were generated by combining all the brain substructure labels into a single foreground label category and excluding the meninges, sulcal CSF, and ventricles by merging them with the background. All brain parenchymal masks were manually corrected by a single experienced rater (OA). These masks were subsequently used to train the first stage of JCnet as described in detail below in Section 3.1. Ground truth PML lesion masks were manually delineated by two raters (IO and OA) using the publicly available ITK-SNAP software (<xref rid="b0220" ref-type="bibr">Yushkevich et al., 2006</xref>). Inter-rater reproducibility was calculated on a subset of 3 subjects delineated by both raters.</p>
  </sec>
  <sec id="s0035">
    <label>4</label>
    <title>Methods</title>
    <sec id="s0040">
      <label>4.1</label>
      <title>Network architecture</title>
      <p id="p0265">CNNs have emerged as a powerful tool in performing object detection and semantic segmentation tasks on natural images in recent years. This is due to their ability to perform feature-extracting convolutions on images that are learned through iterative training cycles, obviating the need to design hand-crafted features as in classical machine learning approaches. This capability can be easily extended to medical image analysis, object detection, and lesion segmentation, where features are extracted from either multichannel 2D slices (<xref rid="b0165" ref-type="bibr">Roy et al., 2018b</xref>) or 3D patches (<xref rid="b0210" ref-type="bibr">Wachinger et al., 2018</xref>) sampled from the original input images. As detailed in <xref rid="b0060" ref-type="bibr">Guerrero et al. (2018)</xref>, the network learns a mapping function that transforms voxel-level image intensities to a desired label classification or segmentation category through a series of convolutions followed by nonlinear activation functions, with each component of this series being referred to as a layer. Feature pyramid networks further exploit the hierarchical architecture of CNNs along their depth by combining low-resolution, semantically strong features (from deeper layers) with high-resolution, semantically weak features (from shallow layers) via a top-down pathway and lateral connections (<xref rid="b0120" ref-type="bibr">Lin et al., 2016</xref>). The feature map outputs of each layer in the convolutional pathway are then concatenated to predict a voxel-wise membership function of the patch.</p>
      <p id="p0270">We implement feature pyramid networks in a two-staged approach, each consisting of 3D patch-based multi-view CNNs: the first stage aims at extracting the brain parenchymal voxels as foreground, while the second stage performs PML lesion segmentation. The output of the first stage can be used to exclude meningeal structures and CSF spaces, which are present in the skull-stripped input images, and allows the generation of a brain parenchymal mask to quantify parenchymal volume loss in PML. This strategy also mitigates class imbalance when used as input to the second network stage similar to the work presented by <xref rid="b0210" ref-type="bibr">Wachinger et al. (2018)</xref>. The overall structure of the stages for JCnet are illustrated in <xref rid="f0010" ref-type="fig">Fig. 2</xref>.<fig id="f0010"><label>Fig. 2</label><caption><p>Overview of the proposed two stage approach of JCnet. Three-dimensional patch samples are extracted from input skull-stripped contrast modalities, reoriented, and used to train three multi-view feature pyramid networks (FPNs) to identify the brain parenchyma as foreground, with meninges and cerebrospinal fluid spaces as background. The second stage utilizes a similar neural network architecture to perform PML lesion segmentation, illustrated in light red. Abbreviations: FPNs = feature pyramid networks; orient = orientation. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr2"/></fig></p>
      <p id="p0275">The details of the network architecture for the CNNs used in each stage of JCnet are listed in <xref rid="t0010" ref-type="table">Table 2</xref>. For each individual CNN, we adopt an FPN design with a ResNet-50 backbone as our baseline. We use deep residual learning given the improved optimization and training of deeper networks, as has been shown by He et al. (2016). We utilize a total of 4 ResNet levels with embedded residual bottleneck building blocks containing projection and identity shortcuts (<xref rid="b0075" ref-type="bibr">He et al., 2016b</xref>). The residual blocks consist of a series of three sequential convolution operations, each followed by a batch normalization step to correct for internal covariate shift (<xref rid="b0085" ref-type="bibr">Ioffe and Szegedy, 2015</xref>) and a rectified linear unit (ReLU) activation (<xref rid="b0140" ref-type="bibr">Nair and Hinton, 2010</xref>), with the exception of the last convolution in each block where the ReLU activation is applied after the shortcut connection and element-wise addition. Aside from a single max pooling operation after the first ResNet layer, down-sampling in the network is otherwise achieved through strided convolutions in the 3rd and 4th layers, as has been described by <xref rid="b0195" ref-type="bibr">Springenberg et al. (2014)</xref> and implemented by <xref rid="b0070" ref-type="bibr">He et al. (2016a)</xref>. For all the other convolutions within the network, we use zero padding in order to have uniform input and output sizes for all filters.<table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>Network architecture details for the CNNs used in each stage of JCnet. The default input 3D patch size we used is 80x80x80 with 16 base filters. The network architecture is identical for the three multi-view CNNs in each stage. Given the large number of layers, we describe the aggregate of the convolution specifications across each residual block. Projection shortcut blocks contain a 1x1x1 convolution in the shortcut compared to identity shortcuts, which are empty and parameter-free.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Level name</th><th>Output size</th><th>Layer components</th><th>Specification(s)</th><th>No. of parameters</th></tr></thead><tbody><tr><td>ResNet 1</td><td>80 × 80 × 80 × 16</td><td><list list-type="simple" id="l0005"><list-item id="o0005"><label>•</label><p id="p0005">Input layer</p></list-item><list-item id="o0010"><label>•</label><p id="p0010">Convolution</p></list-item><list-item id="o0015"><label>•</label><p id="p0015">Batch normalization</p></list-item><list-item id="o0020"><label>•</label><p id="p0020">ReLU activation</p></list-item></list></td><td>–<break/>3 × 3 × 3</td><td>–<break/>1744<break/>64</td></tr><tr><td rowspan="2">ResNet 2</td><td rowspan="2">40 × 40 × 40 × 32</td><td><list list-type="simple" id="l0010"><list-item id="o0025"><label>•</label><p id="p0025">Max pooling</p></list-item></list></td><td>3 × 3 × 3 pool size, stride 2</td><td>–</td></tr><tr><td><list list-type="simple" id="l0015"><list-item id="o0030"><label>•</label><p id="p0030">Projection shortcut block</p></list-item><list-item id="o0035"><label>•</label><p id="p0035">Identity shortcut block</p></list-item></list></td><td>1 × 1 × 1 → 3 × 3 × 3 → 1 × 1 × 1<break/>1 × 1 × 1 → 3 × 3 × 3 → 1 × 1 × 1</td><td>30,336<break/>30,176</td></tr><tr><td>ResNet 3</td><td>20 × 20 × 20 × 64</td><td><list list-type="simple" id="l0020"><list-item id="o0040"><label>•</label><p id="p0040">Projection shortcut block*</p></list-item><list-item id="o0045"><label>•</label><p id="p0045">Identity shortcut block</p></list-item></list></td><td>1 × 1 × 1 → 3 × 3 × 3 → 1 × 1 × 1<break/>1 × 1 × 1 → 3 × 3 × 3 → 1 × 1 × 1</td><td>120,064<break/>119,744</td></tr><tr><td>ResNet 4</td><td>10 × 10 × 10 × 128</td><td><list list-type="simple" id="l0025"><list-item id="o0050"><label>•</label><p id="p0050">Projection shortcut block*</p></list-item><list-item id="o0055"><label>•</label><p id="p0055">Identity shortcut block  × 6</p></list-item></list></td><td>1 × 1 × 1 → 3 × 3 × 3 → 1 × 1 × 1<break/>1 × 1 × 1 → 3 × 3 × 3 → 1 × 1 × 1</td><td>477,696<break/>2,870,592</td></tr><tr><td>Fully convolutional concatenation</td><td>All levels</td><td><list list-type="simple" id="l0030"><list-item id="o0060"><label>•</label><p id="p0060">Convolutional merging of bottom-up and top-down up-sampled feature maps</p></list-item><list-item id="o0065"><label>•</label><p id="p0065">Convolution appended on each merged feature map to reduce aliasing effect from up-sampling</p></list-item><list-item id="o0070"><label>•</label><p id="p0070">Convolutional pathway to predict membership function of the patch</p></list-item></list></td><td>1 × 1 × 1<break/><break/><break/>3 × 3 × 3<break/><break/><break/>3 × 3 × 3, sigmoid activation</td><td>7360<break/><break/><break/>442,624<break/><break/><break/>6916</td></tr></tbody></table><table-wrap-foot><fn id="sp0085"><p>* Denotes residual blocks that contain down-sampling convolutions with stride 2.</p></fn><fn id="sp0090"><p>Abbreviations: No. = number; ReLU = rectified linear unit.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0280">In contrast to the standard ResNet implementation, which uses a fully connected (FC) layer to generate label predictions, we employ a fully convolutional pathway to merge the feature maps across the bottom-up and top-down pathways in our FPN architecture, then append further convolutions to reduce aliasing effects from up-sampling, and subsequently predict the membership function across the patch (<xref rid="t0010" ref-type="table">Table 2</xref>). This approach has been shown to reduce false positives in semantic lesion segmentation tasks and limit the total number of parameters in the model (<xref rid="b0165" ref-type="bibr">Roy et al., 2018b</xref>). In addition, this also improves the prediction time of the network, as convolutions circumvent the need to perform voxel-wise predictions on each voxel of a new image during testing, which is the case with FC layers. We ensemble the outputs of the three CNNs after reorientation by averaging the voxel-wise probability memberships, which is then thresholded to obtain a hard segmentation of the brain parenchymal voxels (stage 1) and the PML lesional tissue (stage 2). The total number of parameters in our model is 4.1 M, with 7008 non-trainable parameters. This compares favorably with other types of ResNet-50 network models with preserved complexity, where the estimated number of parameters can approach ~25 M (<xref rid="b0080" ref-type="bibr">Hu et al., 2017</xref>).</p>
    </sec>
    <sec id="s0045">
      <label>4.2</label>
      <title>Class imbalance</title>
      <p id="p0285">Class imbalance is an important topic to consider in classification applications and refers to when the distribution of the target classes is skewed within the training dataset. The measures of voxel-level class imbalance across our entire PML dataset of 41 subjects pertaining to both stages are displayed in <xref rid="t0015" ref-type="table">Table 3</xref>. When examining this data, a few trends become clear. First, class imbalance exists in the input datasets for both stages of JCnet. As expected in the first stage, this imbalance is skewed toward brain parenchymal voxels occupying a larger volume when compared to the background class (meninges and CSF spaces), whereas in the second stage the converse is true: PML lesions occupy a much smaller volume (~2–5% on average) compared to the brain parenchyma. Secondly, whereas classifying lesions on the brain parenchymal foreground (instead of the entire stripped volume) helps correct the imbalance by 0.5–1%, this still does not nearly enough result in a sufficient reconciliation toward a balanced state. Therefore, we implement other measures in our patch sampling and loss function specification to mitigate this issue, as described in more detail in the Supplement.<table-wrap position="float" id="t0015"><label>Table 3</label><caption><p>Class level voxel data for stages 1 and 2 of JCnet extracted from the manually labelled masks across both the training and testing sets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Training (n = 31)</th><th>Testing (n = 10)</th><th>Total (n = 41)</th></tr></thead><tbody><tr><td>Proportion of brain parenchymal voxels out of all nonzero voxels; mean % across volumes (SD)</td><td>77.1 (5)</td><td>76.0 (3)</td><td>76.8 (5)</td></tr><tr><td>Proportion of lesion voxels out of all pre-processed voxels; mean % across volumes (SD)</td><td>4.2 (3)</td><td>2.0 (1)</td><td>3.6 (3)</td></tr><tr><td>Proportion of lesion voxels out of brain parenchymal voxels; mean % across volumes (SD)</td><td>5.3 (4)</td><td>2.7 (1)</td><td>4.7 (4)</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="s0050">
      <label>4.3</label>
      <title>Training implementation</title>
      <p id="p0290">JCnet was implemented using Python version 3.6 (<ext-link ext-link-type="uri" xlink:href="https://www.python.org/" id="ir015">https://www.python.org/</ext-link>) and Keras version 2.2.4 (<ext-link ext-link-type="uri" xlink:href="https://keras.io/about/" id="ir020">https://keras.io/about/</ext-link>), with TensorFlow as backend. For training purposes, 3D image patches were extracted from the available training dataset, yielding thousands of image samples and allowing for the sample-size necessary for training effective deep learning approaches. We split the 3D patches extracted from the training dataset (n = 31) into 80% used for training and 20% used for validation purposes. Adam optimization was used for training (<xref rid="b0105" ref-type="bibr">Kingma and Ba, 2015</xref>), with an initial learning rate of 0.0001 and a decay factor of 0.5 if learning plateaued for more than 2 epochs. In addition, we specified early stopping criteria if validation accuracy failed to improve by more than 0.0001 over 4 epochs during training. These specifications were found to produce sufficient convergence without overfitting in most of our training procedures. All experiments were conducted using the computational resources of the NIH HPC Biowulf cluster (<ext-link ext-link-type="uri" xlink:href="http://hpc.nih.gov" id="ir025">http://hpc.nih.gov</ext-link>), using nodes equipped with 4x NVIDIA V100-SXM2 GPUs (32 GB VRAM, 5120 cores, 640 Tensor cores). Training of our primary 80x80x80 patch model with 16 minibatches took approximately 20 hours to converge for all 3 multi-view models of a single JCnet stage. Testing JCnet on a single subject takes approximately 15 min per stage. The implementation details and code for JCnet have been made available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/omarallouz/JCnet" id="ir030">https://github.com/omarallouz/JCnet</ext-link>.</p>
    </sec>
    <sec id="s0055">
      <label>4.4</label>
      <title>Statistical analysis and comparison metrics</title>
      <p id="p0295">Statistical analyses were performed using Stata software (version 13; StataCorp LP, College Station, TX). The Shapiro-Wilk test was used to assess the normality of distributions. Comparisons between training and testing subsets were performed using the two-sample <italic>t</italic>-test. For age, Chi-square test for sex, Wilcoxon signed-ranks test for PML duration, and Fisher’s exact test for PML risk factors and brain biopsy designation. For comparisons between the different patch size models, we used a one-way analysis of variance with repeated measures (i.e. different models applied to the same PML testing set scans repeatedly).</p>
      <p id="p0300">We evaluated the accuracy of JCnet using Dice similarity coefficients (DSC), and absolute volume differences (AVD). For a manual segmentation (<italic>M</italic>) and an automated binary segmentation (<italic>A</italic>), DSC (<xref rid="b0035" ref-type="bibr">Dice, 1945</xref>) is defined as:<disp-formula id="e0005"><label>(3)</label><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mfenced><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi>M</mml:mi><mml:mo>∩</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0305">In cases where the target segmentation volume is small, DSC scores can be penalized more by absolute voxel disagreements between the manual and automated segmentations that may not necessarily be clinically significant. Therefore, we also compare AVD, defined as:<disp-formula id="e0010"><label>(4)</label><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:mi>A</mml:mi><mml:mi>V</mml:mi><mml:mi>D</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mfenced><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p id="p0310">All statistical tests between cross-sectional comparison metrics were performed with a non-parametric paired Wilcoxon signed-ranks test. To measure the longitudinal lesion segmentation consistency and agreement of the method with manual delineations, we calculated intraclass correlation coefficients derived from three-level random intercept models: automated and manual lesion volume measurements (level 1) nested within timepoints (level 2), which are in turn nested within PML test subjects (level 3). The intraclass correlation coefficients describe the agreement of the automated and manual delineations relative to the variability seen between different timepoints of the same subject, and between different test subjects. Statistical significance was defined as p &lt; 0.05.</p>
    </sec>
  </sec>
  <sec id="s0060">
    <label>5</label>
    <title>Results</title>
    <sec id="s0065">
      <label>5.1</label>
      <title>Clinical characteristics</title>
      <p id="p0315">The cohort of 41 patients with PML included in the analysis was empirically divided into 31 training and 10 testing cases sampled at random from the entire set. The testing dataset was unseen by any of the trained networks and used solely to assess the performance of JCnet and the comparator methods. The demographics and clinical characteristics of the patient population by training/testing designation are presented in <xref rid="t0020" ref-type="table">Table 4</xref>. The median number of months between PML symptom onset and image acquisition was 4.5 months (range 0.6–44.5 months), reflecting a wide spectrum of early and overt disease. The test group comprised a wide range of lesion size measurements with a mean of 29.1 cm<sup>3</sup> (SD 17, range 7.2–62.5 cm<sup>3</sup>). Excellent inter-rater reliability of PML lesion segmentation was noted on the sample of 3 scans that were segmented by both raters (mean DSC 0.95, SD 0.02, range 0.94–0.97).<table-wrap position="float" id="t0020"><label>Table 4</label><caption><p>Demographics and clinical characteristics of study participants. Disease duration was defined as the time between PML symptom onset and the acquisition date of the MRI scan.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Overall<break/>n = 41</th><th>Training set<break/>n = 31</th><th>Testing set<break/>n = 10</th><th>p-value</th></tr></thead><tbody><tr><td>Age, years; mean (SD)</td><td>55 (13)</td><td>54 (14)</td><td>57 (12)</td><td>0.62<sup>a</sup></td></tr><tr><td>Female; n (%)</td><td>18 (44)</td><td>12 (39)</td><td>6 (40)</td><td>0.24<sup>b</sup></td></tr><tr><td>PML risk factor category; n (%):</td><td/><td/><td/><td/></tr><tr><td><list list-type="simple" id="l0035"><list-item id="o0075"><label>•</label><p id="p0075">Hematological malignancy</p></list-item><list-item id="o0080"><label>•</label><p id="p0080">HIV</p></list-item><list-item id="o0085"><label>•</label><p id="p0085">Idiopathic CD4 lymphopenia</p></list-item><list-item id="o0090"><label>•</label><p id="p0090">Medication-related</p></list-item><list-item id="o0095"><label>•</label><p id="p0095">Other acquired immunodeficiency</p></list-item><list-item id="o0100"><label>•</label><p id="p0100">No known immunocompromise</p></list-item></list></td><td><list list-type="simple" id="l0040"><list-item id="o0105"><label>•</label><p id="p0105">15 (37)</p></list-item><list-item id="o0110"><label>•</label><p id="p0110">8 (20)</p></list-item><list-item id="o0115"><label>•</label><p id="p0115">4 (10)</p></list-item><list-item id="o0120"><label>•</label><p id="p0120">4 (10)</p></list-item><list-item id="o0125"><label>•</label><p id="p0125">8 (20)</p></list-item><list-item id="o0130"><label>•</label><p id="p0130">2 (5)</p></list-item></list></td><td><list list-type="simple" id="l0045"><list-item id="o0135"><label>•</label><p id="p0135">12 (39)</p></list-item><list-item id="o0140"><label>•</label><p id="p0140">6 (19)</p></list-item><list-item id="o0145"><label>•</label><p id="p0145">3 (10)</p></list-item><list-item id="o0150"><label>•</label><p id="p0150">2 (6)</p></list-item><list-item id="o0155"><label>•</label><p id="p0155">6 (19)</p></list-item><list-item id="o0160"><label>•</label><p id="p0160">2 (6)</p></list-item></list></td><td><list list-type="simple" id="l0050"><list-item id="o0165"><label>•</label><p id="p0165">3 (30)</p></list-item><list-item id="o0170"><label>•</label><p id="p0170">2 (20)</p></list-item><list-item id="o0175"><label>•</label><p id="p0175">1 (10)</p></list-item><list-item id="o0180"><label>•</label><p id="p0180">2 (20)</p></list-item><list-item id="o0185"><label>•</label><p id="p0185">2 (20)</p></list-item><list-item id="o0190"><label>•</label><p id="p0190">–</p></list-item></list></td><td>0.88<sup>c</sup></td></tr><tr><td>PML disease duration, months; median (Q1-3)</td><td>4.5 (2–10)</td><td>5.8 (2–10)</td><td>3.0 (1–8)</td><td>0.08<sup>d</sup></td></tr><tr><td>Underwent brain biopsy for diagnosis; n (%)</td><td>12 (29)</td><td>10 (32)</td><td>2 (20)</td><td>0.69<sup>c</sup></td></tr></tbody></table><table-wrap-foot><fn id="sp0105"><p><sup>a</sup> two-sample <italic>t</italic>-test.</p></fn><fn id="sp0110"><p><sup>b</sup> chi-square test.</p></fn><fn id="sp0115"><p><sup>c</sup> Fisher’s exact test.</p></fn><fn id="sp0120"><p><sup>d</sup> Wilcoxon rank-sum test.</p></fn><fn id="sp0125"><p>Abbreviations: HIV = human immunodeficiency virus; MRI = magnetic resonance imaging; PML = progressive multifocal leukoencephalopathy; Q = quartile; SD = standard deviation.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="s0070">
      <label>5.2</label>
      <title>Impact of patch size selection</title>
      <p id="p0320">To understand whether the accuracy of brain extraction or lesion segmentation was influenced by the input patch size, we compared three JCnet models that were trained on different input patch sizes of 32x32x32, 64x64x64, and 80x80x80 voxels. As the patch size increased, the number of base filters was reduced from 64 to 32 to 16 base filters, respectively, to allow the data to fit into the available video random access memory (VRAM). Therefore, this decrement provides insight on the trade-off between patch size and the number of base filters in model hyperparameter selection. The output voxel-wise probability membership for each stage of JCnet was segmented at regular thresholds between 0.1 and 0.9 (step size of 0.1), and the mean DSC were compared across models and hard segmentation thresholds keeping all other parameters in training and testing constant (<xref rid="f0015" ref-type="fig">Fig. 3</xref>).<fig id="f0015"><label>Fig. 3</label><caption><p>Mean Dice similarity coefficients and 95% confidence intervals of brain extraction and lesion segmentation displayed by input patch size across the PML testing set. For brain extraction, models with a variety of patch sizes performed similarly using a threshold range of 0.5–0.6, but a more rapid drop-off in accuracy at the tails of the membership distribution was noted for the 80x80x80 patch size model. For lesion segmentation at the best performing threshold for each model, the 64x64x64 model performed better than the smaller 32x32x32 patch size model (mean DSC difference 0.022; p = 0.01). Otherwise, pairwise comparisons between the lesion segmentation models at their best performing threshold were not statistically significant.</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0325">For brain extraction, there was no apparent difference in performance between different patch sized models at thresholds of 0.5–0.6, or with the best-performing threshold for each model using a one-way analysis of variance with repeated measures for all pairwise model comparisons (p &gt; 0.05). Interestingly, for the 80x80x80 patch size, there was a drop-off in accuracy at the tails of the membership distribution (particularly for voxels at the brain-sulcal CSF boundaries) indicating that the decrement in base filters during training may have impacted the accuracy of boundary voxel classification (<xref rid="f0015" ref-type="fig">Fig. 3</xref>, Panel A; purple curve). On the other hand, larger patch-sized models tended to outperform the smaller 32x32x32 one for lesion segmentation on average (<xref rid="f0015" ref-type="fig">Fig. 3</xref>, Panel B). After selecting the best performing threshold for lesion hard segmentation models, a one-way analysis of variance with repeated measures showed an improvement of the 64x64x64 model DSC scores on the test dataset compared to the 32x32x32 model (mean DSC difference 0.022; p = 0.01), but there were no significant differences between the 80x80x80 and 32x32x32 models (p = 0.06) or the 64x64x64 and 80x80x80 models (p = 0.49).</p>
    </sec>
    <sec id="s0075">
      <label>5.3</label>
      <title>Comparison to reference methods</title>
      <p id="p0330">We compared the performance of JCnet on PML test cases with FMRIB's Automated Segmentation Tool using FSL version 6.0.0 (FSL-FAST; <xref rid="b0225" ref-type="bibr">Zhang et al., 2001</xref>) and FreeSurfer version 6.0.0 (<xref rid="b0050" ref-type="bibr">Fischl et al., 2002</xref>) for global brain parenchymal segmentation. For lesion segmentation, JCnet was compared with two methods designed for general T2/FLAIR or MS lesion segmentation applied directly to the PML testing dataset: Lesion-TOADS (<xref rid="b0190" ref-type="bibr">Shiee et al., 2010</xref>) and the lesion prediction algorithm of the Lesion Segmentation Tool version 3.0.0 (LST-LPA), which is an open source toolbox for SPM12 (<xref rid="b0175" ref-type="bibr">Schmidt, 2017</xref>). Based on our initial testing, the LST-LPA performed better than its counterpart in the toolbox, the lesion growth algorithm (<xref rid="b0180" ref-type="bibr">Schmidt et al., 2012</xref>), in PML lesion segmentation; therefore, LST-LPA was included for all subsequent analyses. It is important to note that the reference methods we used for comparison have not been developed or validated for use specifically in PML, but their utilization here is driven primarily by the lack of other validated methods for PML MRI analysis. The input specifications and parameter details used to apply the comparator methods on the PML testing dataset are specified in Supplementary <xref rid="t0005" ref-type="table">Table 1</xref>. JCnet was also compared to another CNN-based method using U-Net architecture (<xref rid="b0025" ref-type="bibr">Çiçek et al., 2016</xref>) trained on the same dataset as described in detail in <xref rid="s0120" ref-type="sec">Supplementary Fig. 1</xref>.</p>
      <p id="p0335">The quantitative differences in PML brain extraction accuracy for JCnet, FSL-FAST, and FreeSurfer methods are described in <xref rid="t0025" ref-type="table">Table 5</xref> and <xref rid="f0020" ref-type="fig">Fig. 4</xref> (Panel A). Given that the input contrasts differ between the comparator methods, we display equivalent JCnet models where the T2- and/or PD-weighted input contrasts were omitted during both training and testing. JCnet was associated with improvement in voxel-wise classification as measured by DSC and AVD compared to both FSL-FAST and FreeSurfer (<xref rid="t0025" ref-type="table">Table 5</xref>). Qualitatively, this was in part driven by improved performance in areas of significant T1-hypointesity within PML lesions (particularly near the cortical mantle), as well as in regions of biopsy-related changes (<xref rid="f0025" ref-type="fig">Fig. 5</xref>, Rows A-B).<table-wrap position="float" id="t0025"><label>Table 5</label><caption><p>Brain extraction and lesion segmentation accuracy metrics between JCnet, FSL-FAST, FreeSurfer, Lesion-TOADS, and LST-LPA methods applied on the PML test dataset. The DSC and AVD scores were measured relative to the gold-standard manual delineations separately for each automated method. The DSC scores reflect the degree of overlap of each of the automated methods to that of the manual delineations, where 0 indicates no overlap whatsoever and 1 indicates exact voxel-wise agreement between both. Comparisons were conducted using methods with the same number and type of input contrasts using Wilcoxon signed-ranks test.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="10"><italic>Brain Extraction</italic><hr/></th></tr><tr><th>Comparison metric</th><th>JCnet (T1 + FL + T2 + PD)</th><th>JCnet (T1 + FL + T2)</th><th>JCnet (T1 + FL)</th><th>U-Net (T1 + FL + T2 + PD)</th><th>FSL-FAST</th><th>FreeSurfer</th><th>p-value, JCnet vs U-Net</th><th>p-value, JCnet (T1 + FL + T2 + PD) vs FSL-FAST</th><th>p-value, JCnet2 (T1 + FL) vs FreeSurfer</th></tr></thead><tbody><tr><td>Brain parenchymal volume, cm<sup>3</sup>, mean (SD)</td><td>1080 (100)</td><td>1079 (99)</td><td>1080 (98)</td><td>1081 (100)</td><td>1159 (108)</td><td>1129 (91)</td><td>0.33</td><td><bold>0.005*</bold></td><td><bold>0.009*</bold></td></tr><tr><td>DSC scores, mean (SD)</td><td>0.992 (0.005)</td><td>0.991 (0.006)</td><td>0.991 (0.005)</td><td>0.991 (0.007)</td><td>0.952 (0.010)</td><td>0.933 (0.010)</td><td>0.07</td><td><bold>0.005*</bold></td><td><bold>0.005*</bold></td></tr><tr><td>AVD, cm<sup>3</sup>, mean (SD)</td><td>8.5 (15)</td><td>8.2 (16)</td><td>8.0 (13)</td><td>9.7 (17)</td><td>72.8 (36)</td><td>48.9 (27)</td><td>0.33</td><td><bold>0.01*</bold></td><td><bold>0.02*</bold></td></tr><tr><td colspan="10">  </td></tr><tr><td colspan="10"><italic>Lesion segmentation</italic></td></tr><tr><td colspan="10"><hr/></td></tr><tr><td>Comparison metric</td><td>JCnet (T1 + FL + T2 + PD)</td><td>JCnet (T1 + FL + T2)</td><td>JCnet (T1 + FL)</td><td>U-Net (T1 + FL + T2 + PD)</td><td>LTOADS</td><td>LST-LPA</td><td>p-value, JCnet vs U-Net</td><td>p-value, JCnet (T1 + FL) vs LTOADS</td><td>p-value, JCnet (T1 + FL) vs LST-LPA</td></tr><tr><td colspan="10"><hr/></td></tr><tr><td>Lesion volume, cm<sup>3</sup>, mean (SD)</td><td>29 (17)</td><td>29 (17)</td><td>31 (20)</td><td>30 (17)</td><td>7 (6)</td><td>12 (9)</td><td><bold>0.037*</bold></td><td><bold>0.005*</bold></td><td><bold>0.005*</bold></td></tr><tr><td>DSC scores, mean (SD)</td><td>0.848 (0.14)</td><td>0.850 (0.15)</td><td>0.830 (0.18)</td><td>0.827 (0.15)</td><td>0.300 (0.24)</td><td>0.415 (0.24)</td><td><bold>0.007*</bold></td><td><bold>0.005*</bold></td><td><bold>0.005*</bold></td></tr><tr><td>AVD, cm<sup>3</sup>, mean (SD)</td><td>2.4 (2)</td><td>1.8 (2)</td><td>3.1 (3)</td><td>3.0 (3)</td><td>21.7 (14)</td><td>17.1 (12)</td><td>0.3</td><td><bold>0.005*</bold></td><td><bold>0.005*</bold></td></tr></tbody></table><table-wrap-foot><fn id="sp0135"><p>* p &lt; 0.05.</p></fn><fn id="sp0140"><p>Abbreviations: AVD = absolute volume difference; DSC = Dice Similarity coefficient; FL = fluid-attenuated inversion recovery image; FN = false negative; FP = false positive; FSL-FAST = FMRIB's Automated Segmentation Tool; LTOADS = Lesion-TOpology-preserving Anatomical Segmentation; LST-LPA = Lesion Segmentation Tool - Lesion prediction algorithm; PD = proton density image; T1 = T1-weighted image; T2 = T2-weighted image.</p></fn></table-wrap-foot></table-wrap><fig id="f0020"><label>Fig. 4</label><caption><p>Box plots of Dice similarity coefficients (DSC) between JCnet with different input contrast specifications and the comparator methods for brain extraction (Panel A) and lesion segmentation (Panel B) across 10 PML subject test cases. The single outlier subject with a DSC &lt; 0.5 using JCnet, and DSC &lt; 0.05 on LST-LPA and LTOADS, had the smallest lesion size of all the test subjects (7.2 cm<sup>3</sup>). Abbreviations: FL = fluid-attenuated inversion recovery image; FSL-FAST = FMRIB's Automated Segmentation Tool; Lesion-TOADS = Lesion-TOpology-preserving Anatomical Segmentation; LST-LPA = Lesion Segmentation Tool - Lesion prediction algorithm; PD = proton density image; T1 = T1-weighted image; T2 = T2-weighted image.</p></caption><graphic xlink:href="gr4"/></fig><fig id="f0025"><label>Fig. 5</label><caption><p>Visual depictions of the performance of the proposed and comparator methods on 2 PML test subjects. Rows A and B demonstrate T1-weighted images with binary brain parenchymal masks overlaid in green, whereas Rows C and D demonstrate FLAIR images with lesion segmentation results overlaid in light red. JCnet displayed improved brain extraction results in areas of underlying T1-hypointensity, particularly near the cortical mantle (Row A, red arrows). Similarly, regions of post-biopsy related signal changes, as seen in the left cerebellum in Row B, showed a reduction of false negative voxels within the biopsy bed compared to FSL-FAST and false positive voxels outside the meningeal folds compared to FreeSurfer (red arrowheads). An improvement in PML lesion delineation was seen across the spectrum of supratentorial and infratentorial lesions (Rows C and D, blue arrows). There was also a concomitant improvement in the detection of lesions that were entirely missed by the other methods (blue arrowheads). Abbreviations: FLAIR = fluid-attenuated inversion recovery; PML = progressive multifocal leukoencephalopathy. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr5"/></fig></p>
      <p id="p0340"><xref rid="f0030" ref-type="fig">Fig. 6</xref> shows the correlation between the manual PML brain parenchymal masks and automatically generated ones using JCnet models with an equivalent number and type of input contrasts to FSL-FAST and FreeSurfer. In both cases, an improvement of 9–13% was noted in the R<sup>2</sup> values, reflecting the strength of linear association between automated and manual measurements. We further inspected the regions of voxel mismatch between the manual brain parenchymal masks and FSL-FAST/FreeSurfer automated masks using binary mask subtraction, which showed that most voxel misclassifications occurred due to false positive voxels within sulcal CSF spaces near brain boundaries (<xref rid="s0120" ref-type="sec">Supplementary Fig. 2</xref>).<fig id="f0030"><label>Fig. 6</label><caption><p>Scatter plots of automated versus manual brain parenchymal volumes for JCnet with 4 input contrasts compared to FSL-FAST, and JCnet with 2 input contrasts compared to FreeSurfer. Solid black lines represent the identity lines. Dashed lines represent the linear regression fit for each method. Abbreviations: FL = fluid-attenuated inversion recovery image; FSL-FAST = FMRIB's Automated Segmentation Tool; PD = proton density image; T1 = T1-weighted image; T2 = T2-weighted image.</p></caption><graphic xlink:href="gr6"/></fig></p>
      <p id="p0345">Lesion segmentation comparisons in the unseen PML testing set were undertaken with both Lesion-TOADS and the LST-LPA algorithms, as presented in <xref rid="f0020" ref-type="fig">Fig. 4</xref> (Panel B) and <xref rid="t0030" ref-type="table">Table 6</xref>. JCnet achieved a 42–55% absolute improvement in DSC scores compared to either method, which was driven by an increased sensitivity in PML lesion detection and boundary segmentation (<xref rid="f0025" ref-type="fig">Fig. 5</xref>, Rows C and D). This is also illustrated in <xref rid="f0035" ref-type="fig">Fig. 7</xref>, which shows volume comparisons between manually delineated lesion masks and automated ones from JCnet, Lesion-TOADS, and LST-LPA. Bland-Altman plots comparing manual and automated values for each method included in the brain extraction and lesion segmentation analyses are displayed in <xref rid="f0040" ref-type="fig">Fig. 8</xref> (Panels A and B respectively).<table-wrap position="float" id="t0030"><label>Table 6</label><caption><p>Intraclass correlation coefficients (ICCs) of longitudinal lesion segmentation for 17 timepoints of a subset of 4 PML test cases. Given a three-level nested model of two lesion measurements (manual and automated) nested within timepoints, nested within subjects, we can calculate the ICCs at the subject (level-3) and timepoints-within-subjects (level-2) parts of the model. The subject level ICC (level-3) estimates the total residual variance explained by between-subject differences (ignoring the nested timepoint structure), whereas the timepoints-within-subjects ICC (level-2) describes the total residual variance explained by differences between timepoints within the subjects. Any residual variance not explained by the level-2 part of the model (1- level 2 ICC) reflects the degree of disagreement between the manual and automated measures relative to the total residual variance.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Automated method</th><th>Subject level ICC (Level-3)</th><th>Subject level ICC95% CI</th><th>Timepoints-within-subjects ICC (Level-2)</th><th>Timepoint level ICC 95% CI</th></tr></thead><tbody><tr><td>JCnet</td><td align="char">0.64</td><td>0.21, 0.92</td><td align="char">0.99</td><td>0.98, 0.99</td></tr><tr><td>LST-LPA</td><td align="char">0.58</td><td>0.20, 0.88</td><td align="char">0.64</td><td>0.25, 0.90</td></tr><tr><td>LTOADS</td><td align="char">0.60</td><td>0.22, 0.89</td><td align="char">0.60</td><td>0.22, 0.89</td></tr></tbody></table><table-wrap-foot><fn><p>Abbreviations: CI = confidence interval; ICCs = intraclass correlation coefficient; LTOADS = Lesion-TOpology-preserving Anatomical Segmentation; LST-LPA = Lesion Segmentation Tool - Lesion prediction algorithm; MS = multiple sclerosis.</p></fn></table-wrap-foot></table-wrap><fig id="f0035"><label>Fig. 7</label><caption><p>Scatter plots of automated versus manual lesion masks comparing JCnet, LST-LPA, and Lesion-TOADS . Solid black lines represent y = x identity lines. Dashed lines represent linear regression fit for each method. Abbreviations: FL = fluid-attenuated inversion recovery image; Lesion-TOADS = Lesion-TOpology-preserving Anatomical Segmentation; LST-LPA = Lesion Segmentation Tool - Lesion prediction algorithm; PD = proton density image; T1 = T1-weighted image; T2 = T2-weighted image.</p></caption><graphic xlink:href="gr7"/></fig><fig id="f0040"><label>Fig. 8</label><caption><p>Bland-Altman plots comparing manual delineations with all other methods included in our analysis for brain extraction (Panel A) and lesion segmentation (Panel B). The red horizontal line represents the mean of the differences and the black dashed horizontal lines represent the upper and lower limits of agreement, calculated as the mean ± 1.96SD. The dashed colored lines for each method represent the linear regression fit and 95% confidence intervals (shaded gray region), with the regression parameters and 95% confidence interval of the slope (i.e. β coefficient) included in the inset for each method. Abbreviations: FL = fluid-attenuated inversion recovery image; FSL-FAST = FMRIB's Automated Segmentation Tool; Lesion-TOADS = Lesion-TOpology-preserving Anatomical Segmentation; LST-LPA = Lesion Segmentation Tool - Lesion prediction algorithm; PD = proton density image; T1 = T1-weighted image; T2 = T2-weighted image. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr8"/></fig></p>
      <p id="p0350">When compared to a U-Net based model architecture with an equivalent number of input contrasts, base filters, and focal loss function (<xref rid="t0025" ref-type="table">Table 5</xref> and <xref rid="s0120" ref-type="sec">Supplementary Fig. 1</xref>), there were no statistically significant differences in brain extraction DSC and AVD results using JCnet’s FPN architecture (mean DSC difference 0.001, p = 0.07; mean AVD difference 1.2 cm<sup>3</sup>, p = 0.33). However, a significant improvement in lesion segmentation DSC scores was noted utilizing the FPN design (mean DSC difference 0.02, p = 0.007).</p>
    </sec>
    <sec id="s0080">
      <label>5.4</label>
      <title>Longitudinal lesion segmentation performance</title>
      <p id="p0355">Longitudinal lesion segmentation assessment across follow-up scans was performed on a total of 17 timepoints from a subset of 4 PML test subjects. Median number of timepoints per subject was 4 (range 2–7), spanning a median of 2.3 months (range 0.7 – 4.0). Manual lesion delineations were performed on all 17 timepoints, and the consistency of the automated methods was compared to the manual delineations at each timepoint, as discussed in sections 3.4 and 4.3 (<xref rid="f0045" ref-type="fig">Fig. 9</xref>).<fig id="f0045"><label>Fig. 9</label><caption><p>Longitudinal lesion profile plots of 4 PML test subjects comparing the consistency of JCnet (purple), LST-LPA (blue), and LTOADS (orange) with those of manual delineations (black). Dynamic lesion volume changes over time were better captured using convolutional neural networks trained on PML cases (JCnet), compared to other methods developed for multiple sclerosis lesion segmentation (LST-LPA and LTOADS) which did not fully reflect the extent of lesion accumulation over time in Subjects 1, 2, and 4. Abbreviations: LTOADS = Lesion-TOpology-preserving Anatomical Segmentation; LST-LPA = Lesion Segmentation Tool - Lesion prediction algorithm; PML = progressive multifocal leukoencephalopathy. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr9"/></fig></p>
      <p id="p0360">The intraclass correlation coefficients (ICCs) comparing the consistency of lesion segmentation between the manual delineations and different automated methods for the longitudinal PML subset are presented in <xref rid="t0030" ref-type="table">Table 6</xref>. Interestingly, between-subject variability in PML lesion volume accounts for only 58–64% of the total residual variance across the different methods, indicating that considerable variation in lesion volume occurs between different timepoints within subjects, highlighting the dynamic nature of PML lesions across time. JCnet showed an improved ability to track within-subject, between-timepoint variations in lesion volume segmentation, with 1% of the total residual variance being related to differences between manual and JCnet lesion volume measurements over time, compared to 36–40% for methods developed for general T2/FLAIR or MS lesion segmentation.</p>
    </sec>
    <sec id="s0085">
      <label>5.5</label>
      <title>Visualizing JCnet filter activation patterns</title>
      <p id="p0365">To gain a better understanding of the classification process taking place within JCnet, we inspected the filter activation patterns using the gradient ascent in input space method (<xref rid="b0020" ref-type="bibr">Chollet, 2017</xref>). This method enables the visualization of simulated patterns in input images to which filters in selected convolutional layers in the network respond maximally (<xref rid="f0050" ref-type="fig">Fig. 10</xref>). At shallow layers, the simulated FLAIR input of the lesion segmentation network consisted of hyperfine texture patterns, which evolved into checker-like and polka dot patterns in intermediate layers, and finally more abstract patterns in deeper, more semantically rich layers. Such patterns bear some resemblance to confluent PML lesions, meaning that the corresponding filters of these layers would respond maximally to signs of confluency in the input FLAIR volume.<fig id="f0050"><label>Fig. 10</label><caption><p>Examples of filter activation patterns within successive layers of increasing depth within the JCnet lesion segmentation convolutional neural network extracted from the midpoint slice of the 3D FLAIR input channel. Only 4 representative filters are displayed per layer from the entire set of available filters. In shallow layers, these resemble hyperfine texture patterns and then evolve to checker-like or polka-dot patterns in intermediate layers. In deeper layers (far right), more abstract visual patterns start to emerge, which arguably bear some resemblance to discrete or confluent PML lesions. Abbreviations: conv = convolutional layer.</p></caption><graphic xlink:href="gr10"/></fig></p>
    </sec>
  </sec>
  <sec id="s0090">
    <label>6</label>
    <title>Discussion</title>
    <p id="p0370">We describe a 3D patch-based, fully convolutional framework for brain extraction and lesion segmentation in PML. Accurate assessment of PML lesion burden and associated brain parenchymal atrophy is of critical importance for PML disease monitoring, assessing response to therapeutic measures, and obtaining a reliable MRI outcome measure for clinical trials of targeted therapies (<xref rid="b0030" ref-type="bibr">Cortese et al., 2019</xref>). One of the important findings of the work presented is that the application of standard methods of healthy brain extraction in the context of PML may not be optimal and should be approached with caution, given the increased incidence of brain parenchymal classification errors within T1 hypointense PML lesions and in the vicinity of biopsy-related changes. Similarly, we show that methods designed for lesion segmentation in MS are associated with an underestimation of PML lesion volume, particularly in patients with significant infratentorial disease burden (Figure, Row D). Collectively, these findings underscore the need for developing methods tailored to PML and capable of handling the unique morphological and intensity-based changes occurring in the PML brain on standard MRI sequences.</p>
    <p id="p0375">Generally speaking, CNNs have been implemented with varying degrees of success in a number of medical and neuroimaging applications, including automated motion detection (<xref rid="b0045" ref-type="bibr">Fantini et al., 2018</xref>), anatomical brain segmentation (<xref rid="b0210" ref-type="bibr">Wachinger et al., 2018</xref>), and accurate identification of pathologies such as ischemic strokes (<xref rid="b0060" ref-type="bibr">Guerrero et al., 2018</xref>) or MS lesions (<xref rid="b0110" ref-type="bibr">La Rosa et al., 2020</xref>, <xref rid="b0160" ref-type="bibr">Roy et al., 2018a</xref>, <xref rid="b0205" ref-type="bibr">Valverde et al., 2017</xref>). In the context of PML, CNNs offer several unique advantages suited to the task of brain extraction and lesion segmentation. CNNs can be viewed as powerful feature extractors able to learn local, translation-invariant features, which makes them highly data-efficient at solving perceptual problems and ideal for the task of multifocal lesion detection, which is seminal for PML analysis. Additionally, the hierarchical spatial representation of the feature maps within CNNs allows for combining local or hyperlocal patterns into higher level conceptual views (<xref rid="f0050" ref-type="fig">Fig. 10</xref>). FPNs, in particular, exploit this hierarchy by merging higher-level semantic features at different scales (<xref rid="b0120" ref-type="bibr">Lin et al., 2016</xref>), therefore their implementation in PML can help the network address the combination of larger confluent or more complex PML lesions as well as the smaller satellite lesions often encountered in practice, sometimes even in the same scan (<xref rid="f0025" ref-type="fig">Fig. 5</xref>, Row C). In addition to the FPN architecture, we utilize residual learning blocks in our network in order to improve the training convergence speed while, at the same time, allowing the training of networks with increasing depth and resultant accuracy gains (<xref rid="b0070" ref-type="bibr">He et al., 2016a</xref>).</p>
    <p id="p0380">Class imbalance is a prevalent issue in medical image segmentation (<xref rid="b0230" ref-type="bibr">Zhou et al., 2019</xref>), and we show in this work that PML datasets are no exception. Despite the larger size of PML lesions on average compared to those observed in MS, for example, there remains a significant voxel-wise imbalance, with PML lesions constituting only approximately 3.6% of the total nonzero voxels in a skull-stripped volume (<xref rid="t0015" ref-type="table">Table 3</xref>). This has important ramifications when it comes to network design, parameter selection, and data sampling. Prior studies have addressed this issue by proposing the use of specific loss functions, such as weighted or bootstrapped cross-entropy (<xref rid="b0060" ref-type="bibr">Guerrero et al., 2018</xref>), or by modifying the data sampling process such that all labels are equiprobable during sample (<xref rid="b0100" ref-type="bibr">Kamnitsas et al., 2017</xref>). In our implementation, we utilized a comparable strategy for data sampling, but opted to use the recently described focal loss function, which downscales the loss values for easily classified observations and allows the network to focus on poorly classified examples, which are arguably more pertinent for training (<xref rid="b0125" ref-type="bibr">Lin et al., 2017</xref>). Systematic comparisons between the use of different loss functions for PML lesion segmentation are out of the scope of the work presented but should be explored in future work to further assess which loss functions are better suited for lesion segmentation tasks.</p>
    <p id="p0385">The collective application of these specifications in our network design resulted in a significant improvement of voxel-wise classification accuracy of 4–6% and 42–55% for PML brain extraction and lesion segmentation, respectively, compared to reference comparator methods. Furthermore, we show that CNNs trained on PML data are able to capture the dynamic changes in PML lesions over time (using manual delineations as reference), and in a more consistent fashion compared to the comparator methods used in this study, with a level-2 ICC of 0.99. This is particularly important from the perspective of monitoring and clinical trials, where detection of change in lesion volume over time is of critical value. However, it is important to note that the comparator methods utilized here were not developed or validated for use in PML, but rather for segmentation of either normal-appearing brain tissue (FSL-FAST and Freesurfer) or T2-FLAIR hyperintense lesions in MS (LST-LPA and Lesion-TOADS). Their use in this context is primarily driven by the paucity of publicly available methods specific for PML.</p>
    <p id="p0390">One of the limitations of the work presented is that we have limited our analysis to a single label for the foreground brain parenchymal voxels in order to preserve simplicity and a focus on achieving accurate PML lesion segmentation when used as the input for the second stage of the method. With better availability of PML ground truth datasets, including those with manual brain substructure delineations, future work could investigate the possibility of extending our framework to include deep learning-based segmentation of brain substructures. The ability to discriminate PML lesions from those seen with concomitant or other neurological disorders is outside the scope of the current study given our primary focus on quantifying and tracking lesions in PML patients who have already been diagnosed. In patients with longitudinal imaging available prior to PML onset, this can be accomplished by masking out pre-existing lesions on imaging obtained prior to PML onset. However, in patients where this imaging is lacking, this remains a challenging task and would be an interesting target for future studies to investigate. It is also important to keep in mind that, although we have included MRI data from several different protocols acquired on two scanners, this analysis does not encompass the vast spectrum of available MRI scanners and protocols used in different medical centers. As a result, pretrained models should be applied with caution and rigorous quality control for imaging data acquired with different protocols or on different devices than those used in this study; alternatively, the models should be retrained.</p>
    <p id="p0395">In summary, we present an end-to-end framework capable of performing robust brain extraction and lesion segmentation in PML using a consecutive CNN strategy. We demonstrate significant improvements over current state-of-the-art comparator methods designed for normal-appearing brain and MS lesion segmentation. By tracking quantitative measures of PML-related brain and lesional changes, this approach can provide a window for clinicians and scientists to more accurately monitor PML in vivo, track its response to therapeutic strategies, and introduce standardized, quantitative MRI markers for use as outcome measures in clinical trials.</p>
  </sec>
  <sec id="s0095">
    <title>Study funding</title>
    <p id="p0400">O.A. is supported by a National Multiple Sclerosis Society-American Brain Foundation Clinician Scientist Development Award (FAN-1807-32163). This work was also supported by the Intramural Research Program of the National Institute of Neurological Disorders and Stroke of the National Institutes of Health.</p>
  </sec>
  <sec id="s0100">
    <title>Role of the funding source</title>
    <p id="p0405">The sponsors of this study had no role in study conceptualization, study design, data acquisition, analysis or interpretation, manuscript drafting or critical revision. The corresponding authors had full access to all data and accept responsibility for the decision to submit for publication.</p>
  </sec>
  <sec id="s0105">
    <title>Disclosures</title>
    <p id="p0410">Drs. Omar Al-Louzi, Snehashis Roy, Ikesinachi Osuorah, Prasanna Parvathaneni, Bryan Smith, Joan Ohayon, Pascal Sati, Dzung L. Pham, Steven Jacobson, Avindra Nath, and Irene Cortese have no disclosures pertaining to the work presented. Dr. Daniel S. Reich received research funding from Vertex Pharmaceuticals, unrelated to the current project.</p>
  </sec>
  <sec id="s0110">
    <title>CRediT authorship contribution statement</title>
    <p id="p0415"><bold>Omar Al-Louzi:</bold> Conceptualization, Methodology, Software, Validation, Data curation, Formal analysis, Writing - original draft. <bold>Snehashis Roy:</bold> Methodology, Software, Validation, Formal analysis. <bold>Ikesinachi Osuorah:</bold> Methodology, Data curation, Validation. <bold>Prasanna Parvathaneni:</bold> Methodology, Software. <bold>Bryan R. Smith:</bold> Investigation, Resources, Data curation. <bold>Joan Ohayon:</bold> Investigation, Resources, Data curation. <bold>Pascal Sati:</bold> Methodology, Investigation, Resources, Data curation, Supervision. <bold>Dzung L. Pham:</bold> Methodology, Software, Validation, Supervision. <bold>Steven Jacobson:</bold> Investigation, Resources, Data curation, Supervision. <bold>Avindra Nath:</bold> Investigation, Resources, Data curation, Project administration, Supervision. <bold>Daniel S. Reich:</bold> Conceptualization, Methodology, Investigation, Resources, Data curation, Formal analysis, Writing - review &amp; editing. <bold>Irene Cortese:</bold> Conceptualization, Methodology, Investigation, Resources, Data curation, Formal analysis, Writing - review &amp; editing, Project administration.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <element-citation publication-type="journal" id="h0005">
        <person-group person-group-type="author">
          <name>
            <surname>Avants</surname>
            <given-names>B.B.</given-names>
          </name>
          <name>
            <surname>Tustison</surname>
            <given-names>N.J.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Cook</surname>
            <given-names>P.A.</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>J.C.</given-names>
          </name>
        </person-group>
        <article-title>A reproducible evaluation of ANTs similarity metric performance in brain image registration</article-title>
        <source>NeuroImage</source>
        <volume>54</volume>
        <issue>3</issue>
        <year>2011</year>
        <fpage>2033</fpage>
        <lpage>2044</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.025</pub-id>
        <pub-id pub-id-type="pmid">20851191</pub-id>
      </element-citation>
    </ref>
    <ref id="b0010">
      <element-citation publication-type="journal" id="h0010">
        <person-group person-group-type="author">
          <name>
            <surname>Berger</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Aksamit</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Clifford</surname>
            <given-names>D.B.</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Koralnik</surname>
            <given-names>I.J.</given-names>
          </name>
          <name>
            <surname>Sejvar</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Bartt</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Major</surname>
            <given-names>E.O.</given-names>
          </name>
          <name>
            <surname>Nath</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>PML diagnostic criteria: Consensus statement from the AAN Neuroinfectious Disease Section</article-title>
        <source>Neurology</source>
        <volume>80</volume>
        <issue>15</issue>
        <year>2013</year>
        <fpage>1430</fpage>
        <lpage>1438</lpage>
        <pub-id pub-id-type="doi">10.1212/WNL.0b013e31828c2fa1</pub-id>
        <pub-id pub-id-type="pmid">23568998</pub-id>
      </element-citation>
    </ref>
    <ref id="b0015">
      <mixed-citation publication-type="other" id="h0015">Carson, K.R., Evens, A.M., Richey, E.A., Habermann, T.M., Focosi, D., Seymour, J.F., Laubach, J., Bawn, S.D., Gordon, L.I., Winter, J.N., Furman, R.R., Vose, J.M., Zelenetz, A.D., Mamtani, R., Raisch, D.W., Dorshimer, G.W., Rosen, S.T., Muro, K., Gottardi-Littell, N.R., Talley, R.L., Sartor, O., Green, D., Major, E.O., Bennett, C.L., 2009. Progressive multifocal leukoencephalopathy after rituximab therapy in HIV-negative patients: A report of 57 cases from the Research on Adverse Drug Events and Reports project. Blood 113, 4834–4840. https://doi.org/10.1182/blood-2008-10-186999.</mixed-citation>
    </ref>
    <ref id="b0020">
      <element-citation publication-type="book" id="h0020">
        <person-group person-group-type="author">
          <name>
            <surname>Chollet</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <chapter-title>Deep Learning with Python</chapter-title>
        <year>2017</year>
        <publisher-name>Manning. Manning Publications Co.</publisher-name>
        <publisher-loc>Shelter Island NY</publisher-loc>
      </element-citation>
    </ref>
    <ref id="b0025">
      <element-citation publication-type="journal" id="h0025">
        <person-group person-group-type="author">
          <name>
            <surname>Çiçek</surname>
            <given-names>Ö.</given-names>
          </name>
          <name>
            <surname>Abdulkadir</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lienkamp</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</article-title>
        <source>Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)</source>
        <volume>9901 LNCS</volume>
        <year>2016</year>
        <fpage>424</fpage>
        <lpage>432</lpage>
      </element-citation>
    </ref>
    <ref id="b0030">
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Cortese</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Muranski</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Enose-Akahata</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ha</surname>
            <given-names>S.-K.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Monaco</surname>
            <given-names>MariaChiara</given-names>
          </name>
          <name>
            <surname>Ryschkewitsch</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Major</surname>
            <given-names>E.O.</given-names>
          </name>
          <name>
            <surname>Ohayon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Schindler</surname>
            <given-names>M.K.</given-names>
          </name>
          <name>
            <surname>Beck</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Reoma</surname>
            <given-names>L.B.</given-names>
          </name>
          <name>
            <surname>Jacobson</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Reich</surname>
            <given-names>D.S.</given-names>
          </name>
          <name>
            <surname>Nath</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Pembrolizumab treatment for progressive multifocal leukoencephalopathy</article-title>
        <source>N. Engl. J. Med.</source>
        <volume>380</volume>
        <issue>17</issue>
        <year>2019</year>
        <fpage>1597</fpage>
        <lpage>1605</lpage>
        <pub-id pub-id-type="doi">10.1056/NEJMoa1815039</pub-id>
        <pub-id pub-id-type="pmid">30969503</pub-id>
      </element-citation>
    </ref>
    <ref id="b0035">
      <mixed-citation publication-type="other" id="h0035">Dice, L.R., 1945. Measures of the Amount of Ecologic Association Between Species. Ecology 26, 297–302. https://doi.org/10.2307/1932409.</mixed-citation>
    </ref>
    <ref id="b0040">
      <element-citation publication-type="journal" id="h0040">
        <person-group person-group-type="author">
          <name>
            <surname>Eng</surname>
            <given-names>P.M.</given-names>
          </name>
          <name>
            <surname>Turnbull</surname>
            <given-names>B.R.</given-names>
          </name>
          <name>
            <surname>Cook</surname>
            <given-names>S.F.</given-names>
          </name>
          <name>
            <surname>Davidson</surname>
            <given-names>J.E.</given-names>
          </name>
          <name>
            <surname>Kurth</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Seeger</surname>
            <given-names>J.D.</given-names>
          </name>
        </person-group>
        <article-title>Characteristics and antecedents of progressive multifocal leukoencephalopathy in an insured population</article-title>
        <source>Neurology</source>
        <volume>67</volume>
        <issue>5</issue>
        <year>2006</year>
        <fpage>884</fpage>
        <lpage>886</lpage>
        <pub-id pub-id-type="doi">10.1212/01.wnl.0000233918.21986.9c</pub-id>
        <pub-id pub-id-type="pmid">16966559</pub-id>
      </element-citation>
    </ref>
    <ref id="b0045">
      <mixed-citation publication-type="other" id="h0045">Fantini, I., Rittner, L., Yasuda, C., Lotufo, R., 2018. Automatic detection of motion artifacts on MRI using Deep CNN, in: 2018 International Workshop on Pattern Recognition in Neuroimaging, PRNI 2018. Institute of Electrical and Electronics Engineers Inc. https://doi.org/10.1109/PRNI.2018.8423948.</mixed-citation>
    </ref>
    <ref id="b0050">
      <element-citation publication-type="journal" id="h0050">
        <person-group person-group-type="author">
          <name>
            <surname>Fischl</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Salat</surname>
            <given-names>D.H.</given-names>
          </name>
          <name>
            <surname>Busa</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Albert</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Dieterich</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Haselgrove</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>van der Kouwe</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Killiany</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Kennedy</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Klaveness</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Montillo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Makris</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Rosen</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Dale</surname>
            <given-names>A.M.</given-names>
          </name>
        </person-group>
        <article-title>Whole brain segmentation</article-title>
        <source>Neuron</source>
        <volume>33</volume>
        <issue>3</issue>
        <year>2002</year>
        <fpage>341</fpage>
        <lpage>355</lpage>
        <pub-id pub-id-type="doi">10.1016/S0896-6273(02)00569-X</pub-id>
        <pub-id pub-id-type="pmid">11832223</pub-id>
      </element-citation>
    </ref>
    <ref id="b0055">
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Fonov</surname>
            <given-names>V.S.</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>A.C.</given-names>
          </name>
          <name>
            <surname>McKinstry</surname>
            <given-names>R.C.</given-names>
          </name>
          <name>
            <surname>Almli</surname>
            <given-names>C.R.</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>D.L.</given-names>
          </name>
        </person-group>
        <article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title>
        <source>NeuroImage</source>
        <volume>47</volume>
        <year>2009</year>
        <fpage>S102</fpage>
        <pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id>
      </element-citation>
    </ref>
    <ref id="b0060">
      <element-citation publication-type="journal" id="h0060">
        <person-group person-group-type="author">
          <name>
            <surname>Guerrero</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Oktay</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Bowles</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Joules</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Wolz</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Valdés-Hernández</surname>
            <given-names>M.C.</given-names>
          </name>
          <name>
            <surname>Dickie</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Wardlaw</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>White matter hyperintensity and stroke lesion segmentation and differentiation using convolutional neural networks</article-title>
        <source>NeuroImage: Clinical</source>
        <volume>17</volume>
        <year>2018</year>
        <fpage>918</fpage>
        <lpage>934</lpage>
        <pub-id pub-id-type="doi">10.1016/j.nicl.2017.12.022</pub-id>
        <pub-id pub-id-type="pmid">29527496</pub-id>
      </element-citation>
    </ref>
    <ref id="b0065">
      <element-citation publication-type="journal" id="h0065">
        <person-group person-group-type="author">
          <name>
            <surname>Hadjadj</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Guffroy</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Delavaud</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Taieb</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Meyts</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Fresard</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Streichenberger</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>L’Honneur</surname>
            <given-names>A.-S.</given-names>
          </name>
          <name>
            <surname>Rozenberg</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>D’Aveni</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Aguilar</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Rosain</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Picard</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Mahlaoui</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Lecuit</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hermine</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Lortholary</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Suarez</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Progressive multifocal leukoencephalopathy in primary immunodeficiencies</article-title>
        <source>J. Clin. Immunol.</source>
        <volume>39</volume>
        <issue>1</issue>
        <year>2019</year>
        <fpage>55</fpage>
        <lpage>64</lpage>
        <pub-id pub-id-type="doi">10.1007/s10875-018-0578-8</pub-id>
        <pub-id pub-id-type="pmid">30552536</pub-id>
      </element-citation>
    </ref>
    <ref id="b0070">
      <element-citation publication-type="book" id="h0070">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <chapter-title>Deep residual learning for image recognition, in</chapter-title>
        <source>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society</source>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>778</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>
      </element-citation>
    </ref>
    <ref id="b0075">
      <element-citation publication-type="book" id="h0075">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <chapter-title>Identity mappings in deep residual networks</chapter-title>
        <source>Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</source>
        <year>2016</year>
        <publisher-name>Springer Verlag</publisher-name>
        <fpage>630</fpage>
        <lpage>645</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-46493-0_38</pub-id>
      </element-citation>
    </ref>
    <ref id="b0080">
      <element-citation publication-type="journal" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Albanie</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>Squeeze-and-excitation networks</article-title>
        <source>Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</source>
        <volume>7132–7141</volume>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="b0085">
      <mixed-citation publication-type="other" id="h0085">Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift, in: 32nd International Conference on Machine Learning, ICML 2015. International Machine Learning Society (IMLS), pp. 448–456.</mixed-citation>
    </ref>
    <ref id="b0090">
      <mixed-citation publication-type="other" id="h0090">Itti, L., Chang, L., Ernst, T., 2001. Segmentation of progressive multifocal leukoencephalopathy lesions in fluid-attenuated inversion recovery magnetic resonance imaging. J. Neuroimaging 11, 412–7. https://doi.org/10.1111/j.1552-6569.2001.tb00071.x.</mixed-citation>
    </ref>
    <ref id="b0095">
      <element-citation publication-type="journal" id="h0095">
        <person-group person-group-type="author">
          <name>
            <surname>Jenkinson</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Beckmann</surname>
            <given-names>C.F.</given-names>
          </name>
          <name>
            <surname>Behrens</surname>
            <given-names>T.E.J.</given-names>
          </name>
          <name>
            <surname>Woolrich</surname>
            <given-names>M.W.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>S.M.</given-names>
          </name>
        </person-group>
        <article-title>FSL</article-title>
        <source>NeuroImage</source>
        <volume>62</volume>
        <issue>2</issue>
        <year>2012</year>
        <fpage>782</fpage>
        <lpage>790</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id>
        <pub-id pub-id-type="pmid">21979382</pub-id>
      </element-citation>
    </ref>
    <ref id="b0100">
      <element-citation publication-type="journal" id="h0100">
        <person-group person-group-type="author">
          <name>
            <surname>Kamnitsas</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Ledig</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Newcombe</surname>
            <given-names>V.F.J.</given-names>
          </name>
          <name>
            <surname>Simpson</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Kane</surname>
            <given-names>A.D.</given-names>
          </name>
          <name>
            <surname>Menon</surname>
            <given-names>D.K.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</article-title>
        <source>Med. Image Anal.</source>
        <volume>36</volume>
        <year>2017</year>
        <fpage>61</fpage>
        <lpage>78</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.10.004</pub-id>
        <pub-id pub-id-type="pmid">27865153</pub-id>
      </element-citation>
    </ref>
    <ref id="b0105">
      <element-citation publication-type="book" id="h0105">
        <person-group person-group-type="author">
          <name>
            <surname>Kingma</surname>
            <given-names>D.P.</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J.L.</given-names>
          </name>
        </person-group>
        <chapter-title>Adam: A method for stochastic optimization</chapter-title>
        <source>in: 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings. International Conference on Learning Representations</source>
        <year>2015</year>
      </element-citation>
    </ref>
    <ref id="b0110">
      <element-citation publication-type="journal" id="h0110">
        <person-group person-group-type="author">
          <name>
            <surname>La Rosa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Abdulkadir</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Fartaria</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Rahmanzadeh</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>P.-J.</given-names>
          </name>
          <name>
            <surname>Galbusera</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Barakovic</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Thiran</surname>
            <given-names>J.-P.</given-names>
          </name>
          <name>
            <surname>Granziera</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cuadra</surname>
            <given-names>M.B.</given-names>
          </name>
        </person-group>
        <article-title>Multiple sclerosis cortical and WM lesion segmentation at 3T MRI: a deep learning method based on FLAIR and MP2RAGE</article-title>
        <source>NeuroImage Clin.</source>
        <volume>27</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">102335</object-id>
        <pub-id pub-id-type="doi">10.1016/j.nicl.2020.102335</pub-id>
      </element-citation>
    </ref>
    <ref id="b0115">
      <element-citation publication-type="journal" id="h0115">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Gore</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Davatzikos</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Multiplicative intrinsic component optimization (MICO) for MRI bias field estimation and tissue segmentation</article-title>
        <source>Magn. Reson. Imaging</source>
        <volume>32</volume>
        <year>2014</year>
        <fpage>913</fpage>
        <lpage>923</lpage>
        <pub-id pub-id-type="doi">10.1016/j.mri.2014.03.010</pub-id>
        <pub-id pub-id-type="pmid">24928302</pub-id>
      </element-citation>
    </ref>
    <ref id="b0120">
      <mixed-citation publication-type="other" id="h0120">Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., 2016. Feature Pyramid Networks for Object Detection.</mixed-citation>
    </ref>
    <ref id="b0125">
      <element-citation publication-type="journal" id="h0125">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>T.-Y.</given-names>
          </name>
          <name>
            <surname>Goyal</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Girshick</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Dollár</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Focal loss for dense object detection</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <volume>42</volume>
        <year>2017</year>
        <fpage>318</fpage>
        <lpage>327</lpage>
      </element-citation>
    </ref>
    <ref id="b0130">
      <element-citation publication-type="journal" id="h0130">
        <person-group person-group-type="author">
          <name>
            <surname>Major</surname>
            <given-names>E.O.</given-names>
          </name>
          <name>
            <surname>Yousry</surname>
            <given-names>T.A.</given-names>
          </name>
          <name>
            <surname>Clifford</surname>
            <given-names>D.B.</given-names>
          </name>
        </person-group>
        <article-title>Pathogenesis of progressive multifocal leukoencephalopathy and risks associated with treatments for multiple sclerosis: a decade of lessons learned</article-title>
        <source>Lancet. Neurol.</source>
        <volume>17</volume>
        <year>2018</year>
        <fpage>467</fpage>
        <lpage>480</lpage>
        <pub-id pub-id-type="doi">10.1016/S1474-4422(18)30040-1</pub-id>
        <pub-id pub-id-type="pmid">29656742</pub-id>
      </element-citation>
    </ref>
    <ref id="b0135">
      <element-citation publication-type="journal" id="h0135">
        <person-group person-group-type="author">
          <name>
            <surname>Muftuoglu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Olson</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Marin</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ahmed</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Mulanovich</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Tummala</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chi</surname>
            <given-names>T.L.</given-names>
          </name>
          <name>
            <surname>Ferrajoli</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kaur</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Champlin</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Shpall</surname>
            <given-names>E.J.</given-names>
          </name>
          <name>
            <surname>Rezvani</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Allogeneic BK virus-specific T cells for progressive multifocal leukoencephalopathy</article-title>
        <source>N. Engl. J. Med.</source>
        <volume>379</volume>
        <year>2018</year>
        <fpage>1443</fpage>
        <lpage>1451</lpage>
        <pub-id pub-id-type="doi">10.1056/NEJMoa1801540</pub-id>
        <pub-id pub-id-type="pmid">30304652</pub-id>
      </element-citation>
    </ref>
    <ref id="b0140">
      <mixed-citation publication-type="other" id="h0140">Nair, V., Hinton, G.E., 2010. Rectified Linear Units Improve Restricted Boltzmann Machines, in: Proceedings of the 27th International Conference on International Conference on Machine Learning (ICML’10). pp. 807–814.</mixed-citation>
    </ref>
    <ref id="b0145">
      <element-citation publication-type="journal" id="h0145">
        <person-group person-group-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Pinto</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Alves</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Silva</surname>
            <given-names>C.A.</given-names>
          </name>
        </person-group>
        <article-title>Brain tumor segmentation using convolutional neural networks in MRI images</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>35</volume>
        <year>2016</year>
        <fpage>1240</fpage>
        <lpage>1251</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2016.2538465</pub-id>
        <pub-id pub-id-type="pmid">26960222</pub-id>
      </element-citation>
    </ref>
    <ref id="b0150">
      <element-citation publication-type="journal" id="h0150">
        <person-group person-group-type="author">
          <name>
            <surname>Power</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Gladden</surname>
            <given-names>J.G.B.</given-names>
          </name>
          <name>
            <surname>Halliday</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Del Bigio</surname>
            <given-names>M.R.</given-names>
          </name>
          <name>
            <surname>Nath</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Major</surname>
            <given-names>E.O.</given-names>
          </name>
          <name>
            <surname>Blanchard</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Mowat</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>AIDS- and non-AIDS-related PML association with distinct p53 polymorphism</article-title>
        <source>Neurology</source>
        <volume>54</volume>
        <year>2000</year>
        <fpage>743</fpage>
        <lpage>746</lpage>
        <pub-id pub-id-type="doi">10.1212/wnl.54.3.743</pub-id>
        <pub-id pub-id-type="pmid">10680816</pub-id>
      </element-citation>
    </ref>
    <ref id="b0155">
      <element-citation publication-type="journal" id="h0155">
        <person-group person-group-type="author">
          <name>
            <surname>Roy</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Butman</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Pham</surname>
            <given-names>D.L.</given-names>
          </name>
        </person-group>
        <article-title>Robust skull stripping using multiple MR image contrasts insensitive to pathology</article-title>
        <source>Neuroimage</source>
        <volume>146</volume>
        <year>2017</year>
        <fpage>132</fpage>
        <lpage>147</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.017</pub-id>
        <pub-id pub-id-type="pmid">27864083</pub-id>
      </element-citation>
    </ref>
    <ref id="b0160">
      <mixed-citation publication-type="other" id="h0160">Roy, S., Butman, J.A., Reich, D.S., Calabresi, P.A., Pham, D.L., 2018a. Multiple Sclerosis Lesion Segmentation from Brain MRI via Fully Convolutional Neural Networks.</mixed-citation>
    </ref>
    <ref id="b0165">
      <mixed-citation publication-type="other" id="h0165">Roy, S., Butman, J.A., Reich, D.S., Calabresi, P.A., Pham, D.L., 2018b. Multiple Sclerosis Lesion Segmentation from Brain MRI via Fully Convolutional Neural Networks. arXiv:1803.09172.</mixed-citation>
    </ref>
    <ref id="b0170">
      <element-citation publication-type="journal" id="h0170">
        <person-group person-group-type="author">
          <name>
            <surname>Rudick</surname>
            <given-names>R.A.</given-names>
          </name>
          <name>
            <surname>Fisher</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jacobs</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Use of the brain parenchymal fraction to measure whole brain atrophy in relapsing-remitting MS</article-title>
        <source>Multiple Sclerosis Collaborative Res. Group. Neurology</source>
        <volume>53</volume>
        <year>1999</year>
        <fpage>1698</fpage>
        <lpage>1704</lpage>
      </element-citation>
    </ref>
    <ref id="b0175">
      <mixed-citation publication-type="other" id="h0175">Schmidt, P., 2017. Bayesian inference for structured additive regression models for large-scale problems with applications to medical imaging. PhD thesis, Ludwig-Maximilians-Universität München; Ch 6.1, 115–116.</mixed-citation>
    </ref>
    <ref id="b0180">
      <element-citation publication-type="journal" id="h0180">
        <person-group person-group-type="author">
          <name>
            <surname>Schmidt</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Gaser</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Arsic</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Buck</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Förschler</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Berthele</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hoshi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ilg</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>V.J.</given-names>
          </name>
          <name>
            <surname>Zimmer</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Hemmer</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Mühlau</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>An automated tool for detection of FLAIR-hyperintense white-matter lesions in Multiple Sclerosis</article-title>
        <source>Neuroimage</source>
        <volume>59</volume>
        <year>2012</year>
        <fpage>3774</fpage>
        <lpage>3783</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.11.032</pub-id>
        <pub-id pub-id-type="pmid">22119648</pub-id>
      </element-citation>
    </ref>
    <ref id="b0185">
      <element-citation publication-type="journal" id="h0185">
        <person-group person-group-type="author">
          <name>
            <surname>Selvaganesan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Whitehead</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>DeAlwis</surname>
            <given-names>P.M.</given-names>
          </name>
          <name>
            <surname>Schindler</surname>
            <given-names>M.K.</given-names>
          </name>
          <name>
            <surname>Inati</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Saad</surname>
            <given-names>Z.S.</given-names>
          </name>
          <name>
            <surname>Ohayon</surname>
            <given-names>J.E.</given-names>
          </name>
          <name>
            <surname>Cortese</surname>
            <given-names>I.C.M.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Jacobson</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Nath</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Reich</surname>
            <given-names>D.S.</given-names>
          </name>
          <name>
            <surname>Inati</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Nair</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Robust, atlas-free, automatic segmentation of brain MRI in health and disease</article-title>
        <source>Heliyon</source>
        <volume>5</volume>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1016/j.heliyon.2019.e01226</pub-id>
      </element-citation>
    </ref>
    <ref id="b0190">
      <mixed-citation publication-type="other" id="h0190">Shiee, N., Bazin, P.-L., Ozturk, A., Reich, D.S., Calabresi, P. a, Pham, D.L., 2010. A topology-preserving approach to the segmentation of brain images with multiple sclerosis lesions. Neuroimage 49, 1524–35. https://doi.org/10.1016/j.neuroimage.2009.09.005.</mixed-citation>
    </ref>
    <ref id="b0195">
      <mixed-citation publication-type="other" id="h0195">Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M., 2014. Striving for Simplicity: The All Convolutional Net. 3rd Int. Conf. Learn. Represent. ICLR 2015 - Work. Track Proc.</mixed-citation>
    </ref>
    <ref id="b0200">
      <element-citation publication-type="journal" id="h0200">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>C.S.</given-names>
          </name>
          <name>
            <surname>Koralnik</surname>
            <given-names>I.J.</given-names>
          </name>
        </person-group>
        <article-title>Progressive multifocal leukoencephalopathy and other disorders caused by JC virus: clinical features and pathogenesis</article-title>
        <source>Lancet. Neurol.</source>
        <volume>9</volume>
        <year>2010</year>
        <fpage>425</fpage>
        <lpage>437</lpage>
        <pub-id pub-id-type="doi">10.1016/S1474-4422(10)70040-5</pub-id>
        <pub-id pub-id-type="pmid">20298966</pub-id>
      </element-citation>
    </ref>
    <ref id="b0205">
      <element-citation publication-type="journal" id="h0205">
        <person-group person-group-type="author">
          <name>
            <surname>Valverde</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Cabezas</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Roura</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>González-Villà</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Pareto</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Vilanova</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Ramió-Torrentà</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Rovira</surname>
            <given-names>À.</given-names>
          </name>
          <name>
            <surname>Oliver</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lladó</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Improving automated multiple sclerosis lesion segmentation with a cascaded 3D convolutional neural network approach</article-title>
        <source>Neuroimage</source>
        <volume>155</volume>
        <year>2017</year>
        <fpage>159</fpage>
        <lpage>168</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.034</pub-id>
        <pub-id pub-id-type="pmid">28435096</pub-id>
      </element-citation>
    </ref>
    <ref id="b0210">
      <element-citation publication-type="journal" id="h0210">
        <person-group person-group-type="author">
          <name>
            <surname>Wachinger</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Reuter</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>DeepNAT: Deep convolutional neural network for segmenting neuroanatomy</article-title>
        <source>Neuroimage</source>
        <volume>170</volume>
        <year>2018</year>
        <fpage>434</fpage>
        <lpage>445</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.02.035</pub-id>
        <pub-id pub-id-type="pmid">28223187</pub-id>
      </element-citation>
    </ref>
    <ref id="b0215">
      <mixed-citation publication-type="other" id="h0215">Yi, D., Zhou, M., Chen, Z., Gevaert, O., 2016. 3-D Convolutional Neural Networks for Glioblastoma Segmentation.</mixed-citation>
    </ref>
    <ref id="b0220">
      <element-citation publication-type="journal" id="h0220">
        <person-group person-group-type="author">
          <name>
            <surname>Yushkevich</surname>
            <given-names>P.A.</given-names>
          </name>
          <name>
            <surname>Piven</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hazlett</surname>
            <given-names>H.C.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Ho</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Gerig</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</article-title>
        <source>Neuroimage</source>
        <volume>31</volume>
        <year>2006</year>
        <fpage>1116</fpage>
        <lpage>1128</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.015</pub-id>
        <pub-id pub-id-type="pmid">16545965</pub-id>
      </element-citation>
    </ref>
    <ref id="b0225">
      <element-citation publication-type="journal" id="h0225">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Brady</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>20</volume>
        <year>2001</year>
        <fpage>45</fpage>
        <lpage>57</lpage>
        <pub-id pub-id-type="doi">10.1109/42.906424</pub-id>
        <pub-id pub-id-type="pmid">11293691</pub-id>
      </element-citation>
    </ref>
    <ref id="b0230">
      <element-citation publication-type="journal" id="h0230">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Canu</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A review: deep learning for medical image segmentation using multi-modality fusion</article-title>
        <source>Array</source>
        <volume>3–4</volume>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">100004</object-id>
        <pub-id pub-id-type="doi">10.1016/j.array.2019.100004</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="s0120" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary data</title>
    <p id="p0430">The following are the Supplementary data to this article:<supplementary-material content-type="local-data" id="m0005"><caption><title>Supplementary data 1</title></caption><media xlink:href="mmc1.docx"/></supplementary-material></p>
  </sec>
  <ack id="ak005">
    <title>Acknowledgments</title>
    <p id="p0420">We wish to thank all the patients that participated in the study as well as their family members. We also wish to acknowledge the contributions of Frances Andrada and Jennifer Dwyer from the National Institute of Neurological Disorders and Stroke (NINDS) Neuroimmunology Clinic to the recruitment, care, and collection of clinical data from the study participants. We wish to thank Dr. Hadar Kolb for reviewing the initial manual PML delineations and for her contributions to the project. We also wish to thank Dr. John Ostuni and the NINDS information technology department for help with servers and software maintenance, and the staff of the Functional Magnetic Resonance Facility (FMRIF) and radiology department technicians who have been instrumental with regards to image acquisition.</p>
  </ack>
  <fn-group>
    <fn id="s0115" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0425">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.nicl.2020.102499" id="ir035">https://doi.org/10.1016/j.nicl.2020.102499</ext-link>.</p>
    </fn>
  </fn-group>
</back>
