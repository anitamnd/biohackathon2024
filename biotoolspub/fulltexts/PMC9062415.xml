<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ1495 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEga1 jpg ?>
<?FILEmmc1 docx ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?FILEsi10 svg ?>
<?FILEsi11 svg ?>
<?FILEsi12 svg ?>
<?FILEsi13 svg ?>
<?FILEsi14 svg ?>
<?FILEsi15 svg ?>
<?FILEsi16 svg ?>
<?FILEsi17 svg ?>
<?FILEsi18 svg ?>
<?FILEsi19 svg ?>
<?FILEsi20 svg ?>
<?FILEsi21 svg ?>
<?FILEsi22 svg ?>
<?FILEsi23 svg ?>
<?FILEsi24 svg ?>
<?FILEsi25 svg ?>
<?FILEsi26 svg ?>
<?FILEsi27 svg ?>
<?FILEsi28 svg ?>
<?FILEsi29 svg ?>
<?FILEsi30 svg ?>
<?FILEsi31 svg ?>
<?FILEsi32 svg ?>
<?FILEsi33 svg ?>
<?FILEsi34 svg ?>
<?FILEsi35 svg ?>
<?FILEsi36 svg ?>
<?FILEsi37 svg ?>
<?FILEsi38 svg ?>
<?FILEsi39 svg ?>
<?FILEsi40 svg ?>
<?FILEsi41 svg ?>
<?FILEsi42 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9062415</article-id>
    <article-id pub-id-type="pii">S2001-0370(22)00142-8</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2022.04.024</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Improving the topology prediction of α-helical transmembrane proteins with deep transfer learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Wang</surname>
          <given-names>Lei</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af010" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>Zhong</surname>
          <given-names>Haolin</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au015">
        <name>
          <surname>Xue</surname>
          <given-names>Zhidong</given-names>
        </name>
        <email>zdxue@hust.edu.cn</email>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="af015" ref-type="aff">c</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au020">
        <name>
          <surname>Wang</surname>
          <given-names>Yan</given-names>
        </name>
        <email>yanw@hust.edu.cn</email>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="af005"><label>a</label>School of Life Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei 430074, China</aff>
      <aff id="af010"><label>b</label>Institute of Medical Artificial Intelligence, Binzhou Medical University, Yantai, Shandong 264003, China</aff>
      <aff id="af015"><label>c</label>School of Software Engineering, Huazhong University of Science and Technology, Wuhan, Hubei 430074, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding authors at: Institute of Medical Artificial Intelligence, Binzhou Medical University, Yantai, Shandong 264003, China. <email>zdxue@hust.edu.cn</email><email>yanw@hust.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>4</month>
      <year>2022</year>
    </pub-date>
    <volume>20</volume>
    <fpage>1993</fpage>
    <lpage>2000</lpage>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>11</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>9</day>
        <month>4</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>4</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="graphical" id="ab005">
      <title>Graphical abstract</title>
      <fig id="f0020" position="anchor">
        <graphic xlink:href="ga1"/>
      </fig>
    </abstract>
    <abstract id="ab010">
      <p>Transmembrane proteins (TMPs) are essential for cell recognition and communication, and they serve as important drug targets in humans. Transmembrane proteins' 3D structures are critical for determining their functions and drug design but are hard to determine even by experimental methods. Although some computational methods have been developed to predict transmembrane helices (TMHs) and orientation, there is still room for improvement. Considering that the pre-trained language model can make full use of massive unlabeled protein sequences to obtain latent feature representation for TMPs and reduce the dependence on evolutionary information, we proposed DeepTMpred, which used pre-trained self-supervised language models called ESM, convolutional neural networks, attentive neural network and conditional random fields for alpha-TMP topology prediction. Compared with the current state-of-the-art tools on a non-redundant dataset of TMPs, DeepTMpred demonstrated superior predictive performance in most evaluation metrics, especially at the TMH level. Furthermore, DeepTMpred could also obtain reliable prediction results for TMPs without much evolutionary feature in a few seconds. A tutorial on how to use DeepTMpred can be found in the colab notebook (<ext-link ext-link-type="uri" xlink:href="https://colab.research.google.com/github/ISYSLAB-HUST/DeepTMpred/blob/master/notebook/test.ipynb" id="ir005">https://colab.research.google.com/github/ISYSLAB-HUST/DeepTMpred/blob/master/notebook/test.ipynb</ext-link>).</p>
    </abstract>
    <kwd-group id="kg005">
      <title>Keywords</title>
      <kwd>Transmembrane protein</kwd>
      <kwd>Topology prediction</kwd>
      <kwd>Transfer learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0005">Biological membranes protect the vital contents of cells from the environment. Membrane proteins are an important part of biological membranes and play a diverse and important role in many processes such as molecule transport, immune system molecule recognition and metabolism <xref rid="b0005" ref-type="bibr">[1]</xref>, <xref rid="b0010" ref-type="bibr">[2]</xref>. Membrane proteins are fully embedded into the membrane lipid bilayer, and often comprise a substantial fraction of the membrane total mass, ranging from 18% in the insulating myelin membrane of neurons to 75% in the inner membrane of mitochondria <xref rid="b0015" ref-type="bibr">[3]</xref>. Membrane proteins are mainly divided into two structural classes: alpha-helical transmembrane proteins and beta-barrel transmembrane proteins. α-helical transmembrane proteins make up the majority of all known membrane proteins, which have been mostly found in the plasma membrane of eukaryotes and the inner membranes of bacterial cells. In humans, the importance of TMPs is reflected in the fact that they account for more than 50% of known drug targets even though they constitute a minority (between 20% and 30%) of all the proteins encoded in fully-sequenced genome <xref rid="b0020" ref-type="bibr">[4]</xref>, <xref rid="b0025" ref-type="bibr">[5]</xref>.</p>
    <p id="p0010">However, the structure determination of transmembrane proteins by X-ray crystallography and by NMR techniques is limited due to the difficulties in crystallizing them in an aqueous environment and by their relatively high molecular weight <xref rid="b0030" ref-type="bibr">[6]</xref>. The known structures of transmembrane proteins (TMPs) only comprise about 1.8% (∼5800) of all structures in the Protein Data Bank <xref rid="b0010" ref-type="bibr">[2]</xref>, but there are more than 30,000,000 unique TMP sequences in the UniRef100 (release 2020.06). Considering the importance of transmembrane protein structures for drug development, automatic computational methods predicting topology of transmembrane proteins has been paid great attention. In particular, some low-resolution structural information about transmembrane proteins (TMPs) can be served as the constraint information for custom experiments <xref rid="b0035" ref-type="bibr">[7]</xref> or modeling their 3D structures <xref rid="b0040" ref-type="bibr">[8]</xref>.</p>
    <p id="p0015">As far as we know, TopPred <xref rid="b0045" ref-type="bibr">[9]</xref> is one of the earliest methods to predict the topology of TMPs, which was developed based on hydrophobicity analysis. Jones et al. <xref rid="b0050" ref-type="bibr">[10]</xref> proposed a constrained dynamic programming to find the optimal location and orientation of transmembrane helices. In the following time, computational methods based on machine learning have been widely applied for predicting the topology of transmembrane proteins (TMPs) such as hidden Markov models <xref rid="b0055" ref-type="bibr">[11]</xref>, <xref rid="b0060" ref-type="bibr">[12]</xref>, <xref rid="b0065" ref-type="bibr">[13]</xref>, <xref rid="b0070" ref-type="bibr">[14]</xref>, SVMs <xref rid="b0075" ref-type="bibr">[15]</xref>, feed-forward neural network <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0085" ref-type="bibr">[17]</xref>, random forests <xref rid="b0090" ref-type="bibr">[18]</xref>, conditional random fields <xref rid="b0095" ref-type="bibr">[19]</xref>, <xref rid="b0100" ref-type="bibr">[20]</xref>, and K-nearest neighbor <xref rid="b0105" ref-type="bibr">[21]</xref>. Besides, the prediction methods like TOPCONS <xref rid="b0110" ref-type="bibr">[22]</xref>, <xref rid="b0115" ref-type="bibr">[23]</xref>, CCTOP <xref rid="b0120" ref-type="bibr">[24]</xref>, and CNTOP <xref rid="b0125" ref-type="bibr">[25]</xref> integrated the results of different computational methods into one consensus prediction and quantified the reliable TMHs based on the agreement between the different methods. Assaf et al. <xref rid="b0130" ref-type="bibr">[26]</xref> proposed a graphical algorithm, called TopGraph, which was based on the minimum energy, the positive-inside rule and showed high accuracy on large transporters without structural homologues. Compared to traditional machine learning methods, computational methods based on deep learning can have significant advantages on complex biology problems <xref rid="b0135" ref-type="bibr">[27]</xref>, <xref rid="b0140" ref-type="bibr">[28]</xref>. Recently, some methods based on deep learning have been developed to significantly improve the topology prediction of transmembrane proteins such as Membrain3.0 <xref rid="b0145" ref-type="bibr">[29]</xref>, DMCTOP <xref rid="b0150" ref-type="bibr">[30]</xref>. Most of the above methods used evolutionary information as the input features such as sequence profile or multiple sequence alignments <xref rid="b0155" ref-type="bibr">[31]</xref>, <xref rid="b0160" ref-type="bibr">[32]</xref>, therefore they do not work when there are not enough homologous sequences.</p>
    <p id="p0020">Transfer learning aims at improving the performance of task on target domains by transferring the knowledge contained in different but related source domains, which can reduce the dependence on a large number of target domain data <xref rid="b0165" ref-type="bibr">[33]</xref>. Transferring knowledge is particularly efficient when data are abundant in the source domain but scarce in the target domain. In the natural language processing (NLP) field, transfer learning in the form of pre-trained language models has become ubiquitous. Pre-trained language models mainly learn context-based word embeddings such as Bert <xref rid="b0170" ref-type="bibr">[34]</xref>. Since there are more than billion protein sequences in databases, one efficient way is self-supervised language model to learn the latent information from unlabeled sequences. Recently, Bepler et al. <xref rid="b0175" ref-type="bibr">[35]</xref> have reported competitive results with the help of transfer learning on secondary structure prediction and Rives et al. <xref rid="b0180" ref-type="bibr">[36]</xref> trained the pre-trained language model called ESM based on Transformer which includes more than 680 million parameters to predict protein contact map. It is extremely enlightening for us to get a better feature representation of transmembrane proteins with the help of transfer learning.</p>
    <p id="p0025">In this study, we proposed a transfer learning method, DeepTMpred, using self-supervised pre-trained language models called ESM <xref rid="b0180" ref-type="bibr">[36]</xref>, convolutional neural networks, attentive neural network and conditional random fields for TMP topology prediction. Compared with the current state-of-the-art tools on an independent dataset, DeepTMpred can achieve superior results in most evaluation metrics. Furthermore, with the help of pre-trained language model, it could produce reliable topology prediction for TMPs in a few seconds. Finally, all source code can be freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ISYSLAB-HUST/DeepTMpred" id="ir010">https://github.com/ISYSLAB-HUST/DeepTMpred</ext-link>.</p>
  </sec>
  <sec id="s0010">
    <label>2</label>
    <title>Materials and methods</title>
    <sec id="s0015">
      <label>2.1</label>
      <title>Dataset</title>
      <sec id="s0020">
        <label>2.1.1</label>
        <title>Data collection and preprocessing</title>
        <p id="p0030">We collected alpha-TMPs with known structures annotated in OPM(version: July 02, 2020) <xref rid="b0185" ref-type="bibr">[37]</xref>, which was an up-to-date experimental TMP structure database and provided spatial arrangements of membrane proteins with respect to the hydrocarbon core of the lipid bilayer. We first collected two classes of proteins from orientations of proteins in membranes (OPM) database such as the alpha-helical polytopic proteins and bitopic proteins. 8684 protein chains were collected in total. The protein chains were further chosen with the following conditions: (1) the chain is consecutive; (2) the length of the protein chain is less than 800 residues, and more than 20 residues; and (3) there is at least one TMH in the chain. Redundancy of the sequences was removed at 30% identity using CD-HIT and PSI-CD-HIT <xref rid="b0190" ref-type="bibr">[38]</xref>, <xref rid="b0195" ref-type="bibr">[39]</xref>. We chose 40 test proteins used in Membrain 3.0 as the independent test dataset, where the similarity between them is less than 20%, and all of them are solved by either NMR or X-ray technique with resolution less than 4.5 Å. To guarantee a fair comparison on the independent test dataset, the collected sequences in the OPM that are similar to the independent test proteins at a threshold of 30% were dropped. By the above steps, the remaining TMPs from training dataset included 582 protein chains and was randomly divided into training set and validation set according to the ratio of 4:1. The annotation of TMHs and the orientations was taken from the OPM database (<ext-link ext-link-type="uri" xlink:href="https://opm.phar.umich.edu/download" id="ir015">https://opm.phar.umich.edu/download</ext-link>). All training data and test data can be freely accessed at <ext-link ext-link-type="uri" xlink:href="https://github.com/ISYSLAB-HUST/DeepTMpred" id="ir020">https://github.com/ISYSLAB-HUST/DeepTMpred</ext-link>. <xref rid="t0005" ref-type="table">Table 1</xref> lists a summary of the residues and TMHs located on TM and non-TM in the training set and test set. Besides, the training set and test set have approximately the same distributions with respect to the TMH length (<xref rid="s0135" ref-type="sec">Fig. S1</xref>). To explore whether the model can distinguish between soluble proteins and TMPs, we collected two types of soluble proteins from the TMSEG SP1441 test set, including 113 proteins with signal peptides and 173 proteins without signal peptides.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>The summary of residues and TMHs located on TM and non-TM in training set and test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"><bold>Type</bold></th><th colspan="2"><bold>Training Set</bold><hr/></th><th colspan="2"><bold>Test Set</bold><hr/></th></tr><tr><th><bold>TM</bold></th><th><bold>non-TM</bold></th><th><bold>TM</bold></th><th><bold>non-TM</bold></th></tr></thead><tbody><tr><td>Residues</td><td>55,949</td><td>78,724</td><td>3126</td><td>3718</td></tr><tr><td>TMHs</td><td>2637</td><td>–</td><td>146</td><td>–</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="s0025">
        <label>2.1.2</label>
        <title>Construction of features based on evolutionary information</title>
        <p id="p0035">Position-specific scoring matrix (PSSM) is a commonly used protein sequence pattern representation. It contains evolutionary information and has been widely used in previous TMH prediction methods <xref rid="b0075" ref-type="bibr">[15]</xref>, <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0200" ref-type="bibr">[40]</xref>. PSI-BLAST <xref rid="b0205" ref-type="bibr">[41]</xref> is used to search against the non-redundant (NR) database with an iteration number equal to 3 and e-value threshold equal to 1e-3. For a protein sequence with length of L amino acids, the dimension of PSSM is <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo>∗</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
        <p id="p0040">Hidden Markov Model (HMM) profile is conducted by HHblits <xref rid="b0210" ref-type="bibr">[42]</xref> to search against the Uniclust30 database with an iteration number equal to 3 and e-value threshold to 1e-3. The HMM profile is a <inline-formula><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></inline-formula> matrix. For each residue column, it consists of 20 emission frequencies (EF), 7 transition probabilities, and 3 local diversities. PSSM and HMM profile was constructed for all training and test proteins.</p>
      </sec>
      <sec id="s0030">
        <label>2.1.3</label>
        <title>Label of the topology</title>
        <p id="p0045">For the TMH prediction task, (0, 1) is assigned for labeling the residues in the sequence. ‘0’ indicates a non-transmembrane residue and ‘1’ indicates a membrane residue. For the N-terminus orientation prediction in the TMPs, ‘0’ denotes an inner membrane side and ‘1′ denotes an outer membrane side.</p>
      </sec>
    </sec>
    <sec id="s0035">
      <label>2.2</label>
      <title>Model architecture</title>
      <p id="p0050">The model architecture shown in <xref rid="f0005" ref-type="fig">Fig. 1</xref> consists of 6 parts, pre-trained self-supervised language model, evolutionary feature preprocessing, convolutional layer, two position-wise fully-connected layers, Conditional Random Fields, and attentive neural network. After the treatment of the evolutionary feature, the preprocessed feature vectors are concatenated with the output of the pre-trained language model. The integrated feature is fed into a linear layer to reduce the dimensionality. Using the convolution layer can obtain local features of the residues. In the next stage, two position-wise fully-connected layers are used for the classification of transmembrane protein helix. Finally, conditional random fields are used to optimize the distribution of sequence labeling.<fig id="f0005"><label>Fig. 1</label><caption><p>The flowchart of DeepTMpred.</p></caption><graphic xlink:href="gr1"/></fig></p>
      <sec id="s0040">
        <label>2.2.1</label>
        <title>Pre-trained self-supervised language model</title>
        <p id="p0055">The development of pre-trained language models has brought the research in the protein representation to a new stage without artificial labels. Representation of protein sequences can be learned from massive unlabeled protein sequences, and downstream tasks can be significantly improved. Using pre-trained language model can reduce the risk of overfitting on small training data, which is equivalent to a kind of regularization method.</p>
        <p id="p0060">Here, we adopted a transformer-based self-supervised language model called ESM, which made full use of up to 250 million sequences of the Uniparc database <xref rid="b0215" ref-type="bibr">[43]</xref> and included more than 85 million parameters (12-layer transformer) <xref rid="b0180" ref-type="bibr">[36]</xref>. Here, the ESM model can accept a protein sequence and generate embedding matrices with <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>780</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
      <sec id="s0045">
        <label>2.2.2</label>
        <title>Feature fusion and convolutional neural network</title>
        <p id="p0065">All embeddings are projected into the same feature space by using a fully connected neural layer. Given protein sequence PSSM feature <inline-formula><mml:math id="M4" altimg="si4.svg"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, HMM profile feature <inline-formula><mml:math id="M5" altimg="si5.svg"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the embedding feature of the language model <inline-formula><mml:math id="M6" altimg="si6.svg"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we can obtain new embeddings <inline-formula><mml:math id="M7" altimg="si7.svg"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>via:<disp-formula id="e0005"><label>(1)</label><mml:math id="M8" altimg="si8.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mspace width=".25em"/><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">θ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M9" altimg="si9.svg"><mml:mrow><mml:mo>⊕</mml:mo></mml:mrow></mml:math></inline-formula> is the concatenation and FC represents a fully connected neural layer (<inline-formula><mml:math id="M10" altimg="si10.svg"><mml:mrow><mml:msub><mml:mi mathvariant="normal">θ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the learnable parameters) which is applied to each position separately. As such, a nonlinear ReLU activation function can be applied to the fusion embedding. Considering the strong local correlation of the topological structure of transmembrane proteins, we use a convolutional layer based on sparse connectivity to obtain the local vector representations of adjacent residues.</p>
      </sec>
      <sec id="s0050">
        <label>2.2.3</label>
        <title>Conditional random fields</title>
        <p id="p0070">Transmembrane protein topology prediction is a classical problem of sequence labeling. The aim is to determine whether the residue is embedded in the biological membrane or not. The residues are assigned by using Conditional Random Fields to strengthen the correlation between labels. Here, the use of a CRF layer is a helpful regularizer so that continuity of TMHs is encouraged. Conditional random fields were proposed by Lafferty et al. <xref rid="b0220" ref-type="bibr">[44]</xref> for labeling sequence data. Given a protein sequence (length is <inline-formula><mml:math id="M11" altimg="si11.svg"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>) of observations <inline-formula><mml:math id="M12" altimg="si12.svg"><mml:mrow><mml:mi>X</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, the most probable topology sequence is <inline-formula><mml:math id="M13" altimg="si13.svg"><mml:mrow><mml:mi>Y</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="M14" altimg="si14.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">argmax</mml:mi></mml:mrow><mml:mi>Y</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>. By the fundamental theorem of a random field, the joint distribution over label sequence Y given X can be given by the following conditional probability:<disp-formula id="e0010"><label>(2)</label><mml:math id="M15" altimg="si15.svg"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">Y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="normal">Z</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:msubsup><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi mathvariant="normal">ψ</mml:mi><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M16" altimg="si16.svg"><mml:mrow><mml:mi>h</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is the output of the convolutional neural network directly below the conditional random field, Z(h) is the normalization constant of the distribution <inline-formula><mml:math id="M17" altimg="si17.svg"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M18" altimg="si18.svg"><mml:mrow><mml:msub><mml:mi>ψ</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> is a two fully-connected layers model which accepts <inline-formula><mml:math id="M19" altimg="si19.svg"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as input and obtains outputs of m classes and <inline-formula><mml:math id="M20" altimg="si20.svg"><mml:mrow><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a learnable transition matrix with <inline-formula><mml:math id="M21" altimg="si21.svg"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> parameters.</p>
      </sec>
      <sec id="s0055">
        <label>2.2.4</label>
        <title>Orientation prediction model based on the attentive neural network</title>
        <p id="p0075">The inside/outside orientation is only annotated at the N-terminus orientation of proteins in membranes (OPM) database. Therefore, this prediction is made for the entire protein, and then the orientation of the other non-transmembrane residues is inferred since they are supposed to switch after each TMH. Firstly, the parameters of the fully connected neural layer of feature fusion were fine-tuned with a small learning rate(1e-5). Since only the inside/outside position of the N-terminal is predicted, 5 residues before and after the TMP were chosen to classify the position which was taken based on the performance of the validation set. Then, an attention layer <xref rid="b0225" ref-type="bibr">[45]</xref> with the ReLU activation function and a fully connected layer are used to map the features to the binary classification space. For the attention layer, the orientation representation <inline-formula><mml:math id="M22" altimg="si22.svg"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> of the TMP is formed by a weighted sum of these output vectors:<disp-formula id="e0015"><label>(3)</label><mml:math id="M23" altimg="si23.svg"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>M</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M24" altimg="si24.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msup><mml:mo>×</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the embeddings of 5 residues before and after the TMP, <inline-formula><mml:math id="M25" altimg="si25.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the dimension of the fusion layer, <inline-formula><mml:math id="M26" altimg="si26.svg"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula> is a trained parameter vector and<inline-formula><mml:math id="M27" altimg="si27.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>is a transpose. The dimension of <inline-formula><mml:math id="M28" altimg="si26.svg"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M29" altimg="si28.svg"><mml:mrow><mml:mi mathvariant="normal">α</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M30" altimg="si22.svg"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="M31" altimg="si25.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, 10, <inline-formula><mml:math id="M32" altimg="si25.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> separately.</p>
        <p id="p0080">For model optimization, the label smoothing <xref rid="b0230" ref-type="bibr">[46]</xref> method is adopted to avoid the overfitting of the prediction model.</p>
      </sec>
    </sec>
    <sec id="s0060">
      <label>2.3</label>
      <title>Evaluation criteria</title>
      <p id="p0085">To evaluate our model performance, we adopted the same evaluation criteria that were also applied in previous studies <xref rid="b0145" ref-type="bibr">[29]</xref>.</p>
      <p id="p0090">For TMH prediction on residues level, it should be predicted whether each residue belongs to a membrane residue. PRE(r) is defined as:<disp-formula id="e0020"><label>(4)</label><mml:math id="M33" altimg="si29.svg"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0095">The recall for residues level. REC(r) is defined as:<disp-formula id="e0025"><label>(5)</label><mml:math id="M34" altimg="si30.svg"><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0100">The F1 score for residues level. F1(r) is defined as:<disp-formula id="e0030"><label>(6)</label><mml:math id="M35" altimg="si31.svg"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>×</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0105">The precision for TMH level. PRE(h) is defined as:<disp-formula id="e0035"><label>(7)</label><mml:math id="M36" altimg="si32.svg"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0110">The recall for TMH level. REC(h) is defined as:<disp-formula id="e0040"><label>(8)</label><mml:math id="M37" altimg="si33.svg"><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">H</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0115">The F1 score for TMH level. F1(h) is defined as:<disp-formula id="e0045"><label>(9)</label><mml:math id="M38" altimg="si34.svg"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>×</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p id="p0120"><inline-formula><mml:math id="M39" altimg="si35.svg"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the number of proteins with correctly predicted TMH, which is defined as:<disp-formula id="e0050"><label>(10)</label><mml:math id="M40" altimg="si36.svg"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mfenced open="{"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="3.33333pt"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="3.33333pt"/><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfenced><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfenced><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="3.33333pt"/><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p id="p0125">A TMH segment is correctly predicted, if the segment satisfies both of the following criteria: <xref rid="e0005" ref-type="disp-formula">(1)</xref> the endpoints of the predicted TMH does not deviate from those of the observed TMH by more than 5 residues; <xref rid="e0010" ref-type="disp-formula">(2)</xref> the overlapped residues between the predicted TMH segment and the real TMH segment accounts for at least half of the longer one.</p>
      <p id="p0130">The inside/outside orientation prediction is a binary classification task. Thus, we adopt MCC, F1 score, and accuracy (ACC) criteria to evaluate prediction performance. These criteria are defined as:<disp-formula id="e0055"><label>(11)</label><mml:math id="M41" altimg="si37.svg"><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:msqrt><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="e0060"><label>(12)</label><mml:math id="M42" altimg="si38.svg"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mspace width="0.166667em"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.166667em"/><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="e0065"><label>(13)</label><mml:math id="M43" altimg="si39.svg"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mspace width="0.166667em"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where TP, TN, FP, FN represent the numbers of true positives, true negatives, false positives, and false negatives, respectively.</p>
      <p id="p0135"><inline-formula><mml:math id="M44" altimg="si40.svg"><mml:mrow><mml:mi mathvariant="italic">AC</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">sp</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is used to evaluate the accuracy of soluble protein prediction, which is defined as:<disp-formula id="e0070"><label>(14)</label><mml:math id="M45" altimg="si41.svg"><mml:mrow><mml:mi mathvariant="italic">AC</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">sp</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mspace width="0.166667em"/></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="0.333333em"/><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
    </sec>
  </sec>
  <sec id="s0065">
    <label>3</label>
    <title>Results</title>
    <p id="p0140">First of all, we showed how the hyperparameters of our model are tuned specifically. To make a comprehensive evaluation of transmembrane helix (TMHs) prediction, we compared DeepTMpred with three representative state-of-the-art methods including deep learning (MemBrain3.0), ensemble learning (CCTOP, TOPCONS), machine learning (MEMSAT-SVM) and graphical algorithm based on minimum energy (TopGraph) on the independent test data for TMH segments prediction. To demonstrate that DeepTMpred could work well even if TMPs are lack of evolutionary information, our proposed model was divided into two categories according to the input feature: (a) accepted the fusion feature of the evolutionary information and the output feature of ESM; (b) only accepted the output of ESM as input feature.</p>
    <sec id="s0070">
      <label>3.1</label>
      <title>Hyperparameters tuning</title>
      <p id="p0145">DeepTMpred was constructed in Python 3.8 with PyTorch 1.5.0. Considering the limitations of GPU memory, we only finetuned 12-layer ESM with a small learning rate (1e-5). There are some hyperparameters needing to be optimized, including fusion embedding dimension, convolution filter setting, the numbers of convolution layers, and the fully connected layer dimension. All hyperparameters were tuned with Neural Network Intelligence (<ext-link ext-link-type="uri" xlink:href="https://github.com/microsoft/nni" id="ir025">https://github.com/microsoft/nni</ext-link>). In this work, the optimized hyperparameters are as follows: fusion embedding dimension is 32, convolution filter is (16, 3) in which filter number is 16 and kernel size is 3, and the dimension of two fully connected layers are 16 and 2, respectively. For the inside/outside orientation prediction task, a small learning rate(1e-4) was applied for fine-tuning at the fully connected neural layer of feature fusion and the dimension of two fully connected layers for orientation classification are 8 and 2, respectively.</p>
    </sec>
    <sec id="s0075">
      <label>3.2</label>
      <title>Comparison to the state-of-the-art methods for TMH segments prediction</title>
      <p id="p0150">To make a fair comparison with the three-representative state-of-the-art methods, the same test data has been adopted. <xref rid="t0010" ref-type="table">Table 2</xref> shows the prediction performance of different methods based on the 40 independent test proteins (the results of MemBrain3.0, MEMSAT-SVM, TOPCONS, TopGraph and CCTOP are from MemBrain3.0′s paper). DeepTMpred-a adopted the characteristics of evolutionary information of PSSM and HMM profile, while DeepTMpred-b does not include evolutionary information which is replaced by an all-zero matrix of <inline-formula><mml:math id="M46" altimg="si42.svg"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>. For DeepTMpred training, above process was equivalent that DeepTMpred-b had 50*32 less parameters than DeepTMpred-a.<table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>Performance comparisons of DeepTMpred with the three representative state-of-the-art methods on the independent test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><bold>Methods</bold></th><th><bold>PRE(r)</bold></th><th><bold>REC(r)</bold></th><th><bold>F1(r)</bold></th><th><bold>PRE(h)</bold></th><th><bold>REC(h)</bold></th><th><bold>F1(h)</bold></th><th><bold>V(p)</bold></th></tr></thead><tbody><tr><td>MemBrain3.0</td><td align="char">0.892</td><td align="char">0.904</td><td align="char">0.898</td><td align="char">0.808</td><td align="char">0.819</td><td align="char">0.814</td><td>21</td></tr><tr><td>TopGraph</td><td align="char">0.873</td><td align="char">0.717</td><td align="char">0.787</td><td align="char">0.504</td><td align="char">0.464</td><td align="char">0.483</td><td>12</td></tr><tr><td>TOPCONS</td><td align="char">0.918</td><td align="char">0.722</td><td align="char">0.808</td><td align="char">0.573</td><td align="char">0.530</td><td align="char">0.551</td><td>12</td></tr><tr><td>MEMSAT-SVM</td><td align="char">0.926</td><td align="char">0.710</td><td align="char">0.804</td><td align="char">0.591</td><td align="char">0.547</td><td align="char">0.568</td><td>10</td></tr><tr><td>CCTOP</td><td align="char"><bold>0.931</bold></td><td align="char">0.679</td><td align="char">0.785</td><td align="char">0.511</td><td align="char">0.477</td><td align="char">0.493</td><td>10</td></tr><tr><td>DeepTMpred-a</td><td align="char">0.874</td><td align="char"><bold>0.918</bold></td><td align="char">0.895</td><td align="char">0.857</td><td align="char">0.863</td><td align="char">0.860</td><td>26</td></tr><tr><td>DeepTMpred-b</td><td align="char">0.889</td><td align="char">0.911</td><td align="char"><bold>0.900</bold></td><td align="char"><bold>0.864</bold></td><td align="char"><bold>0.870</bold></td><td align="char"><bold>0.867</bold></td><td><bold>28</bold></td></tr></tbody></table></table-wrap></p>
      <p id="p0155">From <xref rid="t0010" ref-type="table">Table 2</xref>, we see that DeepTMpred-b including 12-layer Transformer achieved the highest F1(r) score of 0.900, followed by MemBrain3.0 (0.898), DeepTMpred-a (0.895), TOPCONS (0.808), MEMSAT-SVM (0.804), TopGraph (0.787), and CCTOP (0.785). CCTOP achieved the highest PRE(r) (0.931), followed by DeepTMpred-a (0.918), but CCTOP had the lowest REC(r) (0.679). It is interesting to be noted that all the PRE(h), REC(h) and F1(h) scores of DeepTMpred were significantly higher than those of the other three state-of-the-art methods. For the TMH segments prediction, the values of PRE(h), REC(h) and F1(h) of DeepTMpred-a are 0.857, 0.863 and 0.860, respectively, which are 6.1%, 5.4% and 5.7% higher than those of the MemBrain3.0 which performs the best at the TMH level among the three comparing state-of-the-art methods. Meanwhile, without including evolutionary information, DeepTMpred-b achieved PRE(h) of 0.864, REC(h) of 0.870 and F1(h) of 0.867, which are 6.9%, 6.2% and 6.5% higher than those of the MemBrain3.0.</p>
      <p id="p0160">Among the 40 test proteins, DeepTMpred-b correctly predicts all the TMHs for 28 proteins, i.e., 28 of 40 test proteins are assigned with the correct TMHs, while the highest V(p) of the three comparing state-of-the-art methods is 21. The second-best predictor on V(p) is DeepTMpred-a, which correctly predicts 26 out of the 40 test proteins. These data imply that DeepTMpred based the pre-trained model achieves better performance than other state-of-the-art methods.</p>
      <p id="p0165">The superior performance of DeepTMpred is mainly attributed to the following two reasons. Firstly, transfer learning which does not rely on large-scale training data is very suitable for TMP topology prediction with small scale data. Secondly, CRFs used in the last step of DeepTMpred can learn long-range correlations of TMHs and non-TMHs, which further improves the prediction performance of the method.</p>
    </sec>
    <sec id="s0080">
      <label>3.3</label>
      <title>Representation of TMP residues in embedding space</title>
      <p id="p0170">DeepTMpred can be expected to capture meaningful patterns between TMH and non-TMH. To investigate whether the DeepTMpred model has learned to encode properties of TMH classification in its representations, we use 40 test TMPs from Membrain 3.0 and project the learned embedding of the fusion layer and convolutional layer of the DeepTMpred-b into two dimensions by applying the t-distributed stochastic neighbor embedding (t-SNE) algorithm. <xref rid="f0010" ref-type="fig">Fig. 2</xref> demonstrates that the embedding is already meaningful after training in the convolutional layer. Furthermore, the clustering distribution of the embedding space of the convolutional layer is denser and more separable than the fusion layer and it shows that the convolutional layer enhances patterns in the embedding space. Based on the results, it is clearly concluded that the fusion layer and convolutional layer could encode and capture the different level feature from ESM embeddings and evolution information.<fig id="f0010"><label>Fig. 2</label><caption><p>TMH properties of amino acids are represented in the output embeddings of the fusion layer(A) and convolutional layer (B), visualized here with t-SNE.</p></caption><graphic xlink:href="gr2"/></fig></p>
    </sec>
    <sec id="s0085">
      <label>3.4</label>
      <title>The inside/outside orientation prediction</title>
      <p id="p0175">Considering that other non-transmembrane positions can be inferred by the N-terminus position of a TMP, we only predicted the inside/outside orientation of the N-terminus by transfer learning. We adopt MCC, ACC, and F1 criteria to evaluate the effect of DeepTMpred. The performance of the two modules is shown in <xref rid="t0015" ref-type="table">Table 3</xref>. From <xref rid="t0015" ref-type="table">Table 3</xref>, we can see that DeepTMpred-a achieved F1 of 0.935, ACC of 0.900, and MCC of 0.722, which were better than those of DeepTMpred-b. The attentive neural network could efficiently learn and choose the import feature in 5 residues before and after the TMP. It also indicates that evolutionary information can significantly improve the performance of the orientation task and the latent information from ESM play a complementary role with evolutionary information, considering that the better performance of DeepTMpred-a than DeepTMpred-b. Regarding the prediction of orientation, we also compared DeepTMpred with the two tools (MEMSAT-SVM, TOPCONS). It can be seen from <xref rid="t0015" ref-type="table">Table 3</xref> that the scores of DeepTMpred-a and DeepTMpred-b in MCC, F1 and ACC were far higher than those of MEMSAT-SVM and TOPCONS. At present, our model only focuses on the orientation on the N-terminus, and it can be extended to the prediction of re-entrant loops in the future.<table-wrap position="float" id="t0015"><label>Table 3</label><caption><p>Performance of DeepTMpred for TMP orientation prediction with MEMSAT-SVM and TOPCONS on the independent test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><bold>Methods</bold></th><th><bold>F1</bold></th><th><bold>ACC</bold></th><th><bold>MCC</bold></th></tr></thead><tbody><tr><td>DeepTMpred-a</td><td align="char"><bold>0.935</bold></td><td align="char"><bold>0.900</bold></td><td align="char"><bold>0.722</bold></td></tr><tr><td>DeepTMpred-b</td><td align="char">0.915</td><td align="char">0.875</td><td align="char">0.680</td></tr><tr><td>TOPCONS</td><td align="char">0.717</td><td align="char">0.625</td><td align="char">0.204</td></tr><tr><td>MEMSAT-SVM</td><td align="char">0.769</td><td align="char">0.700</td><td align="char">0.406</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="s0090">
      <label>3.5</label>
      <title>Distinguishing TMPs with soluble proteins</title>
      <p id="p0180">Although DeepTMpred was not trained on soluble proteins, it is also important to distinguish TMPs with soluble proteins. 286 soluble proteins (two groups) from the SP1441 dataset were used. We mainly compared DeepTMpred with the other three methods (MEMSAT-SVM, TMHMM2.0 and TOPCONS) for this task. We adopted the same strategy as CCTOP and Phobius, that is, using SignalP software to predict the position information of signal peptides, which can greatly alleviate the problem of identifying signal peptides as TMH. In this study, SignalP 6.0 <xref rid="b9000" ref-type="bibr">[47]</xref> was used. The prediction results are shown in <xref rid="t0020" ref-type="table">Table 4</xref>. For the group A of 113 soluble proteins with signal peptides, TOPCONS achieved the highest ACC score of 0.929, followed by DeepTMpred (0.920), MEMSAT-SVM (0.699), and TMHMM2.0(0.655). For the group B of 173 soluble proteins without signal peptides, TMHMM2.0 achieved the highest ACC score of 0.977, followed by DeepTMpred (0.971), TOPCONS (0.971), and MEMSAT-SVM (0.965). Further comparison of the results showed that only DeepTMpred and TOPCONS could distinguish TMPs with soluble proteins well on both the two datasets.<table-wrap position="float" id="t0020"><label>Table 4</label><caption><p>Prediction performance of DeepTMpred and the other three methods on the SP1441 soluble protein test set (A: 113 proteins with signal peptides, B: 173 proteins without signal peptides.).</p></caption><table frame="hsides" rules="groups"><thead><tr><th><bold>Methods</bold></th><th><bold>ACC<sub>sp</sub> (A)</bold></th><th><bold>ACC<sub>sp</sub> (B)</bold></th></tr></thead><tbody><tr><td>DeepTMpred</td><td align="char">0.920</td><td align="char">0.971</td></tr><tr><td>MEMSAT-SVM</td><td align="char">0.699</td><td align="char">0.965</td></tr><tr><td>TMHMM2.0</td><td align="char">0.655</td><td align="char"><bold>0.977</bold></td></tr><tr><td>TOPCONS</td><td align="char"><bold>0.929</bold></td><td align="char">0.971</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="s0095">
      <label>3.6</label>
      <title>Analysis of time complexity</title>
      <p id="p0185">We also compared the time complexity for DeepTMpred and the other three methods on the 40 test proteins. TMHMM2.0, MEMSAT-SVM and TOPCONS were run as standalone package with default parameters and the pure running time for 40 test proteins was counted. Since the MemBrain3.0 model file cannot be run locally in our cluster, only the time of the feature generation step was computed. The results are shown in <xref rid="t0025" ref-type="table">Table 5</xref>. MemBrain3.0 was the most time-consuming method, followed by MEMSAT-SVM, TOPCONS, DeepTMpred, and TMHMM2.0. Among the top three most accurate methods (DeepTMpred, MemBrain3.0 and TOPCONS), DeepTMpred required the least amount of time and it only took 8 s to predict the topology of 40 TMPs on the CPU device (6 s on GPU device). CCTOP did not provide a downloadable version, but its paper reported that a sequence took anywhere from a few minutes to 30 min. Overall, DeepTMpred had good performance in terms of both accuracy and running time.<table-wrap position="float" id="t0025"><label>Table 5</label><caption><p>The time complexity comparison between DeepTMpred and other four methods on the 40 test proteins.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><bold>Methods</bold></th><th><bold>Running Time (s)</bold></th><th><bold>Database</bold></th></tr></thead><tbody><tr><td>TMHMM2.0</td><td>4</td><td>–</td></tr><tr><td>MEMSAT-SVM</td><td>12,557</td><td>UniRef-50</td></tr><tr><td>MemBrain3.0*</td><td>38,303</td><td>NR, UniClust-30</td></tr><tr><td>TOPCONS</td><td>869</td><td>Pfam, CDD</td></tr><tr><td>DeepTMpred</td><td>8/6</td><td>–</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="s0100">
      <label>3.7</label>
      <title>Case study</title>
      <p id="p0190">In <xref rid="f0015" ref-type="fig">Fig. 3</xref>, we showed an illustrative example from bacteriorhodopsin-I (PDB id:4pxkA) which is a kind of bacteriorhodopsin protein acting as a proton pump and including 7 TMHs. The prediction results of DeepTMpred-a and DeepTMpred-b were shown in <xref rid="f0015" ref-type="fig">Fig. 3</xref>A and <xref rid="f0015" ref-type="fig">Fig. 3</xref>B, respectively. From <xref rid="f0015" ref-type="fig">Fig. 3</xref>, it is easy to see that the first six TMHs of bacteriorhodopsin-I were correctly identified by both DeepTMpred-a and DeepTMpred-b. The predicted start and stop sites of the first six TMHs were all within the error range (&lt;=5 residues) according to the annotation of the OPM database (<xref rid="f0015" ref-type="fig">Fig. 3</xref>C). Only the termination site of the seventh TMH predicted by DeepTMpred-a and DeepTMpred-b exceeded the error range. The errors were 6 and 7 residues, respectively, according to the annotation of the OPM database. While it is found that the starting and ending sites of the seventh TMH of bacteriorhodopsin-I are 208 and 229, respectively, based on the PDBTM database annotation which are a little difference from the annotation of the OPM database which are 205 and 219, respectively. The termination site of the seventh TMH predicted by DeepTMpred-a and DeepTMpred-b was also within the error range (&lt;=5 residues), if judged by the annotation of the PDBTM database. Furthermore, the very similar performance of DeepTMpred-a and DeepTMpred-b on the case study of bacteriorhodopsin-I shows that DeepTMpred could also work well without using evolutionary information.<fig id="f0015"><label>Fig. 3</label><caption><p>Case study of bacteriorhodopsin-I. (A)Molecular structure of bacteriorhodopsin-I (PDB entry: 4pxk) annotated according to DeepTMpred-a prediction: TMHs in green. (B)Molecular structure of bacteriorhodopsin-I (PDB entry: 4pxk) annotated according to DeepTMpred-b prediction: TMHs in red. (C)Comparison between the real topology (OPM database) and prediction of DeepTMpred. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0195">We also compared the performance of three different tools (MEMSAT-SVM, MemBrain and TOPCONS) with DeepTMpred in hard-type TMH. For long-TMH prediction, 4b4aA was taken as an example, whose third TMH was of long type (&gt;30). As can be seen from <xref rid="t0030" ref-type="table">Table 6</xref>, both DeepTMpred, MemBrain, and MEMSAT-SVM could predict the long-TMH, but TOPCONS failed. Both MEMSAT-SVM and MemBrain showed a big error in the first TMH prediction, and DeepTMpred and TOPCONS also showed an error in the fourth TMH (more than 6 residues at the N-terminus). This example also showed that DeepTMpred was competitive with other tools in hard type TMH prediction.<table-wrap position="float" id="t0030"><label>Table 6</label><caption><p>Comparison between the true topology of 4b4aA (OPM database) and that predicted by DeepTMpred, MEMSAT-SVM, MemBrain, and TOPCONS.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><bold>Methods</bold></th><th><bold>TMH-1</bold></th><th><bold>TMH-2</bold></th><th><bold>TMH-3(hard)</bold></th><th><bold>TMH-4</bold></th><th><bold>TMH-5</bold></th><th><bold>TMH-6</bold></th></tr></thead><tbody><tr><td>OPM</td><td>(11–32)</td><td>(61–85)</td><td>(100–130)</td><td>(147–170)</td><td>(187–202)</td><td>(206–226)</td></tr><tr><td>MEMSAT-SVM</td><td>(15–40)</td><td>(59–87)</td><td>(101–130)</td><td>(150–173)</td><td>(190–205)</td><td>(210–225)</td></tr><tr><td>TOPCONS</td><td>(14–34)</td><td>(65–85)</td><td>(103–123)</td><td>(153–173)</td><td>(187–207)</td><td>(210–230)</td></tr><tr><td>MemBrain</td><td>(6–45)</td><td>(59–88)</td><td>(100–129)</td><td>(149–175)</td><td>(182–202)</td><td>(206–231)</td></tr><tr><td>DeepTMpred</td><td>(11–31)</td><td>(64–85)</td><td>(101–130)</td><td>(153–173)</td><td>(188–203)</td><td>(209–227)</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="s0105">
    <label>4</label>
    <title>Discussion and conclusion</title>
    <p id="p0200">In this work, we proposed a transfer learning method, DeepTMpred, using pre-trained self-supervised language model called ESM, convolutional neural networks, conditional random field and attentive neural network for alpha-TMP topology prediction. Based on the comparison results, the proposed DeepTMpred showed superior performance in predicting the topology of TMP compared with the state-of-the-art methods.</p>
    <p id="p0205">Protein representation is an important problem for downstream tasks such as protein contact map prediction. Although evolutionary information is the most extensive method of protein representation, its disadvantages are obvious, relying too much on homologous sequences. Most of the state-of-art machine-learning based methods for TMP topology prediction made use of sequence profile as the input features, but they are time-consuming find enough homologous sequences. Further, some methods (such as Membrain 3.0) rely on the predicted structural features and biochemical properties, but these predicted features caused bias and calculations are time-consuming (more than dozens of minutes). DeepTMpred model which does not rely on the evolution information showed that the topology of proteins could be predicted only depending on the output feature of ESM within a few seconds.</p>
    <p id="p0210">Besides, after comparing DeepTMpred-a with DeepTMpred-b, DeepTMpred-b was slightly better at the TMH level, which implied that the combination of ESM and evolutionary information would bring noise and ESM model has already included most evolutionary information or adding parameters was difficult to optimize on training data sets. Intuitively, it is difficult to optimize a complex network (more parameters) for a small-scale data set, so this does not mean that adding evolutionary information will not lead to improvements in other tasks. In our final model, the fine-tuning ESM was used to replace the artificially constructed features in terms of evaluation performance. In the proposed model, CRFs also play an important role in TMH prediction and construct the correlation of labels. For the orientation prediction task, DeepTMpred can predict the orientation of transmembrane proteins through a transfer model with the attentive neural network which could capture the important contribution residues.</p>
    <p id="p0215">In addition to its best performance, DeepTMpred is also recommended due to its speed. The method is readily available for free: online via colab notebook, and as a standalone package from GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/ISYSLAB-HUST/DeepTMpred" id="ir030">https://github.com/ISYSLAB-HUST/DeepTMpred</ext-link>). Apart from that, as a stand-alone software package, DeepTMpred does not require any biological sequence database. A tutorial on how to use DeepTMpred can be found in the colab notebook (<ext-link ext-link-type="uri" xlink:href="https://colab.research.google.com/github/ISYSLAB-HUST/DeepTMpred/blob/master/notebook/test.ipynb" id="ir035">https://colab.research.google.com/github/ISYSLAB-HUST/DeepTMpred/blob/master/notebook/test.ipynb</ext-link>), mainly providing three different prediction modes (batch sequence prediction, single TMP sequence prediction, extra-long TMP sequence prediction).</p>
    <p id="p0220">As far as we know, this is the first time that transfer learning algorithm has been applied in the topology prediction of TMPs. Transfer learning provides new ideas for predicting the topology of transmembrane proteins lacking sufficient evolutionary information. In the future, we intend to improve the performance of TMPs topology prediction by developing new deep learning methods.</p>
  </sec>
  <sec id="s0110">
    <title>Funding</title>
    <p id="p0225">This work was supported by <funding-source id="gp005"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source> under Grant 61772217 and 62172172, Scientific Research Start-up Foundation of Binzhou Medical University under Grant BY2020KYQD01, and <funding-source id="gp010"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100012226</institution-id><institution>Fundamental Research Funds for the Central Universities</institution></institution-wrap></funding-source> under Grant 2016YXMS104 and 2017KFYXJJ225.</p>
  </sec>
  <sec id="s0115">
    <title>CRediT authorship contribution statement</title>
    <p id="p0230"><bold>Lei Wang:</bold> Software, Formal analysis, Data curation, Writing – review &amp; editing. <bold>Haolin Zhong:</bold> Software, Formal analysis, Writing – review &amp; editing, Visualization. <bold>Zhidong Xue:</bold> Conceptualization, Methodology, Writing – review &amp; editing. <bold>Yan Wang:</bold> Conceptualization, Methodology, Writing – original draft, Writing – review &amp; editing.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <label>1</label>
      <element-citation publication-type="journal" id="h0005">
        <person-group person-group-type="author">
          <name>
            <surname>Heyden</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Freites</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Ulmschneider</surname>
            <given-names>M.B.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Assembly and stability of alpha-helical membrane proteins</article-title>
        <source>Soft Matter</source>
        <volume>8</volume>
        <year>2012</year>
        <fpage>7742</fpage>
        <lpage>7752</lpage>
        <pub-id pub-id-type="pmid">23166562</pub-id>
      </element-citation>
    </ref>
    <ref id="b0010">
      <label>2</label>
      <element-citation publication-type="journal" id="h0010">
        <person-group person-group-type="author">
          <name>
            <surname>Kozma</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Tusnady</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <article-title>PDBTM: Protein Data Bank of transmembrane proteins after 8 years</article-title>
        <source>Nucleic Acids Res</source>
        <volume>41</volume>
        <year>2013</year>
        <fpage>D524</fpage>
        <lpage>529</lpage>
        <pub-id pub-id-type="pmid">23203988</pub-id>
      </element-citation>
    </ref>
    <ref id="b0015">
      <label>3</label>
      <element-citation publication-type="journal" id="h0015">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>H.T.</given-names>
          </name>
          <name>
            <surname>Chung</surname>
            <given-names>M.C.</given-names>
          </name>
        </person-group>
        <article-title>Membrane proteins and membrane proteomics</article-title>
        <source>Proteomics</source>
        <volume>8</volume>
        <year>2008</year>
        <fpage>3924</fpage>
        <lpage>3932</lpage>
        <pub-id pub-id-type="pmid">18763712</pub-id>
      </element-citation>
    </ref>
    <ref id="b0020">
      <label>4</label>
      <element-citation publication-type="journal" id="h0020">
        <person-group person-group-type="author">
          <name>
            <surname>Hopkins</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Groom</surname>
            <given-names>C.R.</given-names>
          </name>
        </person-group>
        <article-title>The druggable genome</article-title>
        <source>Nat Rev Drug Discov</source>
        <volume>1</volume>
        <year>2002</year>
        <fpage>727</fpage>
        <lpage>730</lpage>
        <pub-id pub-id-type="pmid">12209152</pub-id>
      </element-citation>
    </ref>
    <ref id="b0025">
      <label>5</label>
      <element-citation publication-type="journal" id="h0025">
        <person-group person-group-type="author">
          <name>
            <surname>Wallin</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>von Heijne</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide analysis of integral membrane proteins from eubacterial, archaean, and eukaryotic organisms</article-title>
        <source>Protein Sci</source>
        <volume>7</volume>
        <year>1998</year>
        <fpage>1029</fpage>
        <lpage>1038</lpage>
        <pub-id pub-id-type="pmid">9568909</pub-id>
      </element-citation>
    </ref>
    <ref id="b0030">
      <label>6</label>
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Arora</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tamm</surname>
            <given-names>L.K.</given-names>
          </name>
        </person-group>
        <article-title>Biophysical approaches to membrane protein structure determination</article-title>
        <source>Curr Opin Struct Biol</source>
        <volume>11</volume>
        <year>2001</year>
        <fpage>540</fpage>
        <lpage>547</lpage>
        <pub-id pub-id-type="pmid">11785753</pub-id>
      </element-citation>
    </ref>
    <ref id="b0035">
      <label>7</label>
      <element-citation publication-type="journal" id="h0035">
        <person-group person-group-type="author">
          <name>
            <surname>Das</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hahn</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Walker</surname>
            <given-names>D.A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Topology of NGEP, a prostate-specific cell:cell junction protein widely expressed in many cancers of different grade level</article-title>
        <source>Cancer Res</source>
        <volume>68</volume>
        <year>2008</year>
        <fpage>6306</fpage>
        <lpage>6312</lpage>
        <pub-id pub-id-type="pmid">18676855</pub-id>
      </element-citation>
    </ref>
    <ref id="b0040">
      <label>8</label>
      <element-citation publication-type="journal" id="h0040">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Transmembrane protein alignment and fold recognition based on predicted topology</article-title>
        <source>PLoS ONE</source>
        <volume>8</volume>
        <year>2013</year>
        <object-id pub-id-type="publisher-id">e69744</object-id>
      </element-citation>
    </ref>
    <ref id="b0045">
      <label>9</label>
      <element-citation publication-type="journal" id="h0045">
        <person-group person-group-type="author">
          <name>
            <surname>von Heijne</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Membrane protein structure prediction. Hydrophobicity analysis and the positive-inside rule</article-title>
        <source>J Mol Biol</source>
        <volume>225</volume>
        <year>1992</year>
        <fpage>487</fpage>
        <lpage>494</lpage>
        <pub-id pub-id-type="pmid">1593632</pub-id>
      </element-citation>
    </ref>
    <ref id="b0050">
      <label>10</label>
      <element-citation publication-type="journal" id="h0050">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>D.T.</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>W.R.</given-names>
          </name>
          <name>
            <surname>Thornton</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>A model recognition approach to the prediction of all-helical membrane protein structure and topology</article-title>
        <source>Biochemistry</source>
        <volume>33</volume>
        <year>1994</year>
        <fpage>3038</fpage>
        <lpage>3049</lpage>
        <pub-id pub-id-type="pmid">8130217</pub-id>
      </element-citation>
    </ref>
    <ref id="b0055">
      <label>11</label>
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Krogh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Larsson</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>von Heijne</surname>
            <given-names>G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting transmembrane protein topology with a hidden Markov model: application to complete genomes</article-title>
        <source>J Mol Biol</source>
        <volume>305</volume>
        <year>2001</year>
        <fpage>567</fpage>
        <lpage>580</lpage>
        <pub-id pub-id-type="pmid">11152613</pub-id>
      </element-citation>
    </ref>
    <ref id="b0060">
      <label>12</label>
      <element-citation publication-type="journal" id="h0060">
        <person-group person-group-type="author">
          <name>
            <surname>Kall</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sonnhammer</surname>
            <given-names>E.L.</given-names>
          </name>
        </person-group>
        <article-title>A combined transmembrane topology and signal peptide prediction method</article-title>
        <source>J Mol Biol</source>
        <volume>338</volume>
        <year>2004</year>
        <fpage>1027</fpage>
        <lpage>1036</lpage>
        <pub-id pub-id-type="pmid">15111065</pub-id>
      </element-citation>
    </ref>
    <ref id="b0065">
      <label>13</label>
      <element-citation publication-type="journal" id="h0065">
        <person-group person-group-type="author">
          <name>
            <surname>Martelli</surname>
            <given-names>P.L.</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>An ENSEMBLE machine learning approach for the prediction of all-alpha membrane proteins</article-title>
        <source>Bioinformatics</source>
        <volume>19</volume>
        <issue>Suppl 1</issue>
        <year>2003</year>
        <fpage>i205</fpage>
        <lpage>211</lpage>
        <pub-id pub-id-type="pmid">12855459</pub-id>
      </element-citation>
    </ref>
    <ref id="b0070">
      <label>14</label>
      <element-citation publication-type="journal" id="h0070">
        <person-group person-group-type="author">
          <name>
            <surname>Tamposis</surname>
            <given-names>I.A.</given-names>
          </name>
          <name>
            <surname>Sarantopoulou</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Theodoropoulou</surname>
            <given-names>M.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hidden neural networks for transmembrane protein topology prediction</article-title>
        <source>Comput Struct Biotechnol J</source>
        <volume>19</volume>
        <year>2021</year>
        <fpage>6090</fpage>
        <lpage>6097</lpage>
        <pub-id pub-id-type="pmid">34849210</pub-id>
      </element-citation>
    </ref>
    <ref id="b0075">
      <label>15</label>
      <element-citation publication-type="journal" id="h0075">
        <person-group person-group-type="author">
          <name>
            <surname>Nugent</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>D.T.</given-names>
          </name>
        </person-group>
        <article-title>Transmembrane protein topology prediction using support vector machines</article-title>
        <source>BMC Bioinf</source>
        <volume>10</volume>
        <year>2009</year>
        <fpage>159</fpage>
      </element-citation>
    </ref>
    <ref id="b0080">
      <label>16</label>
      <element-citation publication-type="journal" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>D.T.</given-names>
          </name>
        </person-group>
        <article-title>Improving the accuracy of transmembrane protein topology prediction using evolutionary information</article-title>
        <source>Bioinformatics</source>
        <volume>23</volume>
        <year>2007</year>
        <fpage>538</fpage>
        <lpage>544</lpage>
        <pub-id pub-id-type="pmid">17237066</pub-id>
      </element-citation>
    </ref>
    <ref id="b0085">
      <label>17</label>
      <element-citation publication-type="journal" id="h0085">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Topology prediction for helical transmembrane proteins at 86% accuracy</article-title>
        <source>Protein Sci</source>
        <volume>5</volume>
        <year>1996</year>
        <fpage>1704</fpage>
        <lpage>1718</lpage>
        <pub-id pub-id-type="pmid">8844859</pub-id>
      </element-citation>
    </ref>
    <ref id="b0090">
      <label>18</label>
      <element-citation publication-type="journal" id="h0090">
        <person-group person-group-type="author">
          <name>
            <surname>Bernhofer</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kloppmann</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Reeb</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TMSEG: Novel prediction of transmembrane helices</article-title>
        <source>Proteins</source>
        <volume>84</volume>
        <year>2016</year>
        <fpage>1706</fpage>
        <lpage>1716</lpage>
        <pub-id pub-id-type="pmid">27566436</pub-id>
      </element-citation>
    </ref>
    <ref id="b0095">
      <label>19</label>
      <element-citation publication-type="journal" id="h0095">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep conditional random field approach to transmembrane topology prediction and application to GPCR three-dimensional structure modeling</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <volume>14</volume>
        <year>2017</year>
        <fpage>1106</fpage>
        <lpage>1114</lpage>
        <pub-id pub-id-type="pmid">27576262</pub-id>
      </element-citation>
    </ref>
    <ref id="b0100">
      <label>20</label>
      <mixed-citation publication-type="other" id="h0100">Lu W, Fu B, Wu H et al. CRF-TM: A conditional random field method for predicting transmembrane topology. Cham, 2015, p. 529-537. Springer International Publishing.</mixed-citation>
    </ref>
    <ref id="b0105">
      <label>21</label>
      <element-citation publication-type="journal" id="h0105">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>J.J.</given-names>
          </name>
        </person-group>
        <article-title>MemBrain: improving the accuracy of predicting transmembrane helices</article-title>
        <source>PLoS ONE</source>
        <volume>3</volume>
        <year>2008</year>
        <object-id pub-id-type="publisher-id">e2399</object-id>
      </element-citation>
    </ref>
    <ref id="b0110">
      <label>22</label>
      <element-citation publication-type="journal" id="h0110">
        <person-group person-group-type="author">
          <name>
            <surname>Bernsel</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Viklund</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Hennerdal</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TOPCONS: consensus prediction of membrane protein topology</article-title>
        <source>Nucleic Acids Res</source>
        <volume>37</volume>
        <year>2009</year>
        <fpage>W465</fpage>
        <lpage>468</lpage>
        <pub-id pub-id-type="pmid">19429891</pub-id>
      </element-citation>
    </ref>
    <ref id="b0115">
      <label>23</label>
      <element-citation publication-type="journal" id="h0115">
        <person-group person-group-type="author">
          <name>
            <surname>Tsirigos</surname>
            <given-names>K.D.</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Shu</surname>
            <given-names>N.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The TOPCONS web server for consensus prediction of membrane protein topology and signal peptides</article-title>
        <source>Nucleic Acids Res</source>
        <volume>43</volume>
        <year>2015</year>
        <fpage>W401</fpage>
        <lpage>407</lpage>
        <pub-id pub-id-type="pmid">25969446</pub-id>
      </element-citation>
    </ref>
    <ref id="b0120">
      <label>24</label>
      <element-citation publication-type="journal" id="h0120">
        <person-group person-group-type="author">
          <name>
            <surname>Dobson</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Remenyi</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Tusnady</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <article-title>CCTOP: a Consensus Constrained TOPology prediction web server</article-title>
        <source>Nucleic Acids Res</source>
        <volume>43</volume>
        <year>2015</year>
        <fpage>W408</fpage>
        <lpage>412</lpage>
        <pub-id pub-id-type="pmid">25943549</pub-id>
      </element-citation>
    </ref>
    <ref id="b0125">
      <label>25</label>
      <element-citation publication-type="journal" id="h0125">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improving transmembrane protein consensus topology prediction using inter-helical interaction</article-title>
        <source>Biochim Biophys Acta</source>
        <volume>1818</volume>
        <year>2012</year>
        <fpage>2679</fpage>
        <lpage>2686</lpage>
        <pub-id pub-id-type="pmid">22683598</pub-id>
      </element-citation>
    </ref>
    <ref id="b0130">
      <label>26</label>
      <element-citation publication-type="journal" id="h0130">
        <person-group person-group-type="author">
          <name>
            <surname>Elazar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Weinstein</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Prilusky</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Interplay between hydrophobicity and the positive-inside rule in determining membrane-protein topology</article-title>
        <source>Proc Natl Acad Sci U S A</source>
        <volume>113</volume>
        <year>2016</year>
        <fpage>10340</fpage>
        <lpage>10345</lpage>
        <pub-id pub-id-type="pmid">27562165</pub-id>
      </element-citation>
    </ref>
    <ref id="b0135">
      <label>27</label>
      <element-citation publication-type="journal" id="h0135">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>S.Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning for mining protein data</article-title>
        <source>Briefings Bioinf</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>194</fpage>
        <lpage>218</lpage>
      </element-citation>
    </ref>
    <ref id="b0140">
      <label>28</label>
      <element-citation publication-type="journal" id="h0140">
        <person-group person-group-type="author">
          <name>
            <surname>Min</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in bioinformatics</article-title>
        <source>Briefings Bioinf</source>
        <volume>18</volume>
        <year>2017</year>
        <fpage>851</fpage>
        <lpage>869</lpage>
      </element-citation>
    </ref>
    <ref id="b0145">
      <label>29</label>
      <element-citation publication-type="journal" id="h0145">
        <person-group person-group-type="author">
          <name>
            <surname>Feng</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W.X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Topology prediction improvement of alpha-helical transmembrane proteins through helix-tail modeling and multiscale deep learning fusion</article-title>
        <source>J Mol Biol</source>
        <volume>432</volume>
        <year>2020</year>
        <fpage>1279</fpage>
        <lpage>1296</lpage>
        <pub-id pub-id-type="pmid">31870850</pub-id>
      </element-citation>
    </ref>
    <ref id="b0150">
      <label>30</label>
      <element-citation publication-type="book" id="h0150">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>DMCTOP: topology prediction of alpha-helical transmembrane protein based on deep multi-scale convolutional neural network</part-title>
        <source>2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source>
        <year>2019</year>
        <fpage>36</fpage>
        <lpage>43</lpage>
      </element-citation>
    </ref>
    <ref id="b0155">
      <label>31</label>
      <element-citation publication-type="journal" id="h0155">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Refining neural network predictions for helical transmembrane proteins by dynamic programming</article-title>
        <source>Proc Int Conf Intell Syst Mol Biol</source>
        <volume>4</volume>
        <year>1996</year>
        <fpage>192</fpage>
        <lpage>200</lpage>
        <pub-id pub-id-type="pmid">8877519</pub-id>
      </element-citation>
    </ref>
    <ref id="b0160">
      <label>32</label>
      <element-citation publication-type="journal" id="h0160">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Casadio</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Fariselli</surname>
            <given-names>P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Transmembrane helices predicted at 95% accuracy</article-title>
        <source>Protein Sci</source>
        <volume>4</volume>
        <year>1995</year>
        <fpage>521</fpage>
        <lpage>533</lpage>
        <pub-id pub-id-type="pmid">7795533</pub-id>
      </element-citation>
    </ref>
    <ref id="b0165">
      <label>33</label>
      <element-citation publication-type="journal" id="h0165">
        <person-group person-group-type="author">
          <name>
            <surname>Zhuang</surname>
            <given-names>F.Z.</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>Z.Y.</given-names>
          </name>
          <name>
            <surname>Duan</surname>
            <given-names>K.Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A comprehensive survey on transfer learning</article-title>
        <source>Proc IEEE</source>
        <volume>109</volume>
        <year>2021</year>
        <fpage>43</fpage>
        <lpage>76</lpage>
      </element-citation>
    </ref>
    <ref id="b0170">
      <label>34</label>
      <element-citation publication-type="book" id="h0170">
        <person-group person-group-type="author">
          <name>
            <surname>Devlin</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>M.-W.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>K.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Pre-training of deep bidirectional transformers for language understanding</part-title>
        <source>Proceedings of the 2019 conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</source>
        <year>2019</year>
        <fpage>4171</fpage>
        <lpage>4186</lpage>
      </element-citation>
    </ref>
    <ref id="b0175">
      <label>35</label>
      <element-citation publication-type="book" id="h0175">
        <person-group person-group-type="author">
          <name>
            <surname>Bepler</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <part-title>Learning protein sequence embeddings using information from structure</part-title>
        <source>International conference on learning representations</source>
        <year>2018</year>
      </element-citation>
    </ref>
    <ref id="b0180">
      <label>36</label>
      <element-citation publication-type="journal" id="h0180">
        <person-group person-group-type="author">
          <name>
            <surname>Rives</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sercu</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>
        <source>PNAS</source>
        <volume>118</volume>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b0185">
      <label>37</label>
      <element-citation publication-type="journal" id="h0185">
        <person-group person-group-type="author">
          <name>
            <surname>Lomize</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Lomize</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Pogozheva</surname>
            <given-names>I.D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>OPM: orientations of proteins in membranes database</article-title>
        <source>Bioinformatics</source>
        <volume>22</volume>
        <year>2006</year>
        <fpage>623</fpage>
        <lpage>625</lpage>
        <pub-id pub-id-type="pmid">16397007</pub-id>
      </element-citation>
    </ref>
    <ref id="b0190">
      <label>38</label>
      <element-citation publication-type="journal" id="h0190">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <volume>28</volume>
        <year>2012</year>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="b0195">
      <label>39</label>
      <element-citation publication-type="journal" id="h0195">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>
        <source>Bioinformatics</source>
        <volume>22</volume>
        <year>2006</year>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="pmid">16731699</pub-id>
      </element-citation>
    </ref>
    <ref id="b0200">
      <label>40</label>
      <element-citation publication-type="journal" id="h0200">
        <person-group person-group-type="author">
          <name>
            <surname>Kall</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Krogh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sonnhammer</surname>
            <given-names>E.L.</given-names>
          </name>
        </person-group>
        <article-title>An HMM posterior decoder for sequence feature prediction that includes homology information</article-title>
        <source>Bioinformatics</source>
        <volume>21</volume>
        <issue>Suppl 1</issue>
        <year>2005</year>
        <fpage>i251</fpage>
        <lpage>257</lpage>
        <pub-id pub-id-type="pmid">15961464</pub-id>
      </element-citation>
    </ref>
    <ref id="b0205">
      <label>41</label>
      <element-citation publication-type="journal" id="h0205">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>S.F.</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>T.L.</given-names>
          </name>
          <name>
            <surname>Schaffer</surname>
            <given-names>A.A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>
        <source>Nucleic Acids Res</source>
        <volume>25</volume>
        <year>1997</year>
        <fpage>3389</fpage>
        <lpage>3402</lpage>
        <pub-id pub-id-type="pmid">9254694</pub-id>
      </element-citation>
    </ref>
    <ref id="b0210">
      <label>42</label>
      <element-citation publication-type="journal" id="h0210">
        <person-group person-group-type="author">
          <name>
            <surname>Remmert</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Biegert</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hauser</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</article-title>
        <source>Nat Methods</source>
        <volume>9</volume>
        <year>2011</year>
        <fpage>173</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="pmid">22198341</pub-id>
      </element-citation>
    </ref>
    <ref id="b0215">
      <label>43</label>
      <mixed-citation publication-type="other" id="h0215">Pundir S, Magrane M, Martin MJ et al. Searching and navigating UniProt databases, Curr Protoc Bioinformatics 2015;50:1 27 21-10.</mixed-citation>
    </ref>
    <ref id="b0220">
      <label>44</label>
      <mixed-citation publication-type="other" id="h0220">Lafferty JD, McCallum A, Pereira FCN. Conditional random fields: probabilistic models for segmenting and labeling sequence data. International conference on machine learning. 2001, 282-289.</mixed-citation>
    </ref>
    <ref id="b0225">
      <label>45</label>
      <element-citation publication-type="book" id="h0225">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Attention-based bidirectional long short-term memory networks for relation classification</part-title>
        <source>Proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: Short papers)</source>
        <year>2016</year>
        <fpage>207</fpage>
        <lpage>212</lpage>
      </element-citation>
    </ref>
    <ref id="b0230">
      <label>46</label>
      <element-citation publication-type="journal" id="h0230">
        <person-group person-group-type="author">
          <name>
            <surname>Müller</surname>
            <given-names>R.R.</given-names>
          </name>
          <name>
            <surname>Kornblith</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>When does label smoothing help</article-title>
        <source>Neural Inf Process Systems</source>
        <year>2019</year>
        <fpage>4694</fpage>
        <lpage>4703</lpage>
      </element-citation>
    </ref>
    <ref id="b9000">
      <label>47</label>
      <element-citation publication-type="journal" id="h9000">
        <person-group person-group-type="author">
          <name>
            <surname>Teufel</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Almagro Armenteros</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Johansen</surname>
            <given-names>A.R.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SignalP 6.0 predicts all five types of signal peptides using protein language models</article-title>
        <source>Nat Biotechnol</source>
        <year>2022</year>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="s0135" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary data</title>
    <p id="p0250">The following are the Supplementary data to this article:<supplementary-material content-type="local-data" id="m0005"><caption><title>Supplementary data 1</title></caption><media xlink:href="mmc1.docx"/></supplementary-material></p>
  </sec>
  <ack id="ak005">
    <sec id="s0120">
      <title>Acknowledgments</title>
      <p id="p0235">Thanks to the Facebook Research team for providing the pre-trained weights for the transformer protein language models.</p>
    </sec>
    <sec id="s0125">
      <title>Conflicts of interest</title>
      <p id="p0240">The authors declare no conflict of interest.</p>
    </sec>
  </ack>
  <fn-group>
    <fn id="s0130" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0245">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csbj.2022.04.024" id="ir040">https://doi.org/10.1016/j.csbj.2022.04.024</ext-link>.</p>
    </fn>
  </fn-group>
</back>
