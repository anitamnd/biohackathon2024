<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neural Circuits</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neural Circuits</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neural Circuits</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neural Circuits</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5110</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9732676</article-id>
    <article-id pub-id-type="doi">10.3389/fncir.2022.977700</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Igneous: Distributed dense 3D segmentation meshing, neuron skeletonization, and hierarchical downsampling</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Silversmith</surname>
          <given-names>William</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/601605/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zlateski</surname>
          <given-names>Aleksandar</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bae</surname>
          <given-names>J. Alexander</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1662986/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tartavull</surname>
          <given-names>Ignacio</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kemnitz</surname>
          <given-names>Nico</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Jingpeng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/609212/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Seung</surname>
          <given-names>H. Sebastian</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2120/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Princeton Neuroscience Institute, Princeton University</institution>, <addr-line>Princeton, NJ</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology</institution>, <addr-line>Cambridge, MA</addr-line>, <country>United States</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Department of Electrical and Computer Engineering, Princeton University</institution>, <addr-line>Princeton, NJ</addr-line>, <country>United States</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Department of Computer Science, Princeton University</institution>, <addr-line>Princeton, NJ</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Yoshiyuki Kubota, National Institute for Physiological Sciences (NIPS), Japan</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Mohan Raghavan, Indian Institute of Technology Hyderabad, India; Kevin Boergens, Paradromics, Inc., United States</p>
      </fn>
      <corresp id="c001">*Correspondence: William Silversmith <email>ws9@princeton.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>16</volume>
    <elocation-id>977700</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>09</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright Â© 2022 Silversmith, Zlateski, Bae, Tartavull, Kemnitz, Wu and Seung.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Silversmith, Zlateski, Bae, Tartavull, Kemnitz, Wu and Seung</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Three-dimensional electron microscopy images of brain tissue and their dense segmentations are now petascale and growing. These volumes require the mass production of dense segmentation-derived neuron skeletons, multi-resolution meshes, image hierarchies (for both modalities) for visualization and analysis, and tools to manage the large amount of data. However, open tools for large-scale meshing, skeletonization, and data management have been missing. Igneous is a Python-based distributed computing framework that enables economical meshing, skeletonization, image hierarchy creation, and data management using cloud or cluster computing that has been proven to scale horizontally. We sketch Igneous's computing framework, show how to use it, and characterize its performance and data storage.</p>
    </abstract>
    <kwd-group>
      <kwd>meshing</kwd>
      <kwd>skeletonization</kwd>
      <kwd>neuroscience</kwd>
      <kwd>connectomics</kwd>
      <kwd>image processing</kwd>
      <kwd>cloud computing</kwd>
      <kwd>distributed computing</kwd>
      <kwd>compression</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>National Institute of Mental Health</institution>
            <institution-id institution-id-type="doi">10.13039/100000025</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn002">RF1MH117815</award-id>
        <award-id award-type="contract" rid="cn002">U01MH114824</award-id>
        <award-id award-type="contract" rid="cn002">U01MH117072</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn003">
          <institution-wrap>
            <institution>National Institute of Neurological Disorders and Stroke</institution>
            <institution-id institution-id-type="doi">10.13039/100000065</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn003">R01NS104926</award-id>
        <award-id award-type="contract" rid="cn003">U19NS104648</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn004">
          <institution-wrap>
            <institution>National Eye Institute</institution>
            <institution-id institution-id-type="doi">10.13039/100000053</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn004">R01EY027036</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn005">
          <institution-wrap>
            <institution>Army Research Office</institution>
            <institution-id institution-id-type="doi">10.13039/100000183</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn005">W911NF-12-1-0594</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>Intelligence Advanced Research Projects Activity</institution>
            <institution-id institution-id-type="doi">10.13039/100011039</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="11"/>
      <table-count count="3"/>
      <equation-count count="0"/>
      <ref-count count="63"/>
      <page-count count="22"/>
      <word-count count="15514"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Over the past decade, advances in the dense reconstruction of microscale neural circuits, a field known as connectomics, have produced increasingly large stacks of electron microscopy images derived from thinly sliced plasticized brain tissue (Pfister et al., <xref rid="B37" ref-type="bibr">2014</xref>). In recent years, several large datasets have appeared, including the whole brain (Zheng et al., <xref rid="B63" ref-type="bibr">2018</xref>) and hemibrain (Xu et al., <xref rid="B58" ref-type="bibr">2020</xref>) versions of <italic>Drosophila melanogaster</italic>. In 2021, a cubic millimeter dataset of mouse primary visual cortex (MICrONS Consortium et al., <xref rid="B34" ref-type="bibr">2021</xref>) and a petascale fragment of human cerebral cortex (Shapson-Coe et al., <xref rid="B46" ref-type="bibr">2021</xref>) were made available as pre-prints, the largest datasets to date. There are even ongoing discussions on imaging a whole mouse brain, a volume hundreds of times larger than either of those (Abbott et al., <xref rid="B1" ref-type="bibr">2020</xref>; Rose Li and Associates Inc., <xref rid="B40" ref-type="bibr">2021</xref>).</p>
    <p>Such volumes are far larger than the capacity of existing single machines and so require the use of cloud storage or other large networked filesystems. Not only is hardware required to store these massive datasets, but also software to visualize, process, and manage them. Efficiently producing and managing datasets of this size is a key challenge for the connectomics field as it scales to acquire the complete wiring diagram of higher order organisms.</p>
    <p>There are many types of data involved in such investigations (Pfister et al., <xref rid="B37" ref-type="bibr">2014</xref>; Beyer et al., <xref rid="B10" ref-type="bibr">2022</xref>), but at the microscale, they typically consist of stacks of single-channel electron or multi-channel confocal microscopy images. Electron micrograph stacks are processed into (usually dense) per-voxel 32 or 64-bit labelings (âsegmentationsâ) and intermediate representations such as three-channel 32-bit floating point voxel affinities. From the segmentation are derived surface meshes for 3D visualization and skeletons (Tagliasacchi et al., <xref rid="B52" ref-type="bibr">2016</xref>) (stick figure centerline representations of geometries) which have analytical, visualization, and graphical user interface uses. Volumes may be annotated with points and lines (such as for describing synapses or other sub-cellular compartments).</p>
    <p>Many systems have been published that excel at handling large-scale image data. To our knowledge, there are no publicly available systems capable of large-scale dense segmentation meshing and skeletonization (see Related Work). Our innovation is the release of fully functional, open source, and easy-to-use software that mass produces meshes and skeletons directly from segmentation images and independently of each other. It is not necessary to produce a set of meshes before producing skeletons nor vice versa. These two innovations are embedded within a larger system that produces and manages Neuroglancer viewable volumes (Maitin-Shepard et al., <xref rid="B31" ref-type="bibr">2021</xref>).</p>
    <p>Neuroglancer is a viewer that has been gaining popularity (see <xref rid="F1" ref-type="fig">Figure 1</xref>). It is a lightweight static web page that is pointed at storage backends such as local web servers, Google Cloud Storage, and Amazon S3 (or S3 emulators) to pull in data. Neuroglancer's native format, Precomputed, is designed to make the client-side calculation of the necessary files' locations trivial without scanning a filesystem or querying a database. Precomputed divides the image into a regular grid of chunk files. It may also store groups of those files in a container âshardedâ format that retains the random read access property (but not random write access) to reduce the load on the filesystem. A resolution hierarchy is also defined so that the entire image can be visualized at low resolution and refined as the viewer is zoomed in. Neuroglancer also provides specifications for visualizing 3D meshes, skeletons, and annotations with multi-resolution formats defined for meshes and annotations. However useful Neuroglancer is, it nonetheless doesn't come with a way to create, manage, or programmatically read the Precomputed format.</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p>Screenshot of an Igneous generated dataset viewed in Neuroglancer. Igneous is designed to produce complete Neuroglancer viewable datasets from images and segmentations. Above, two cells are selected from an unproofread automatic segmentation of the S1 dataset and displayed in Neuroglancer. Three zoomed out cross-sectional views (XY, XZ, and YZ planes going clockwise) are shown with overlaid electron microscope images and segmentation labels. Image pyramids, meshes, and skeletons (in 2D) can be seen in the bottom left panel.</p>
      </caption>
      <graphic xlink:href="fncir-16-977700-g0001" position="float"/>
    </fig>
    <p>We report Igneous<xref rid="fn0001" ref-type="fn"><sup>1</sup></xref>, an open-source software Python program that provides a critically needed scalable, low-cost, and easy-to-use computational pipeline for generating and managing bulk Precomputed data such as image pyramids, meshes, and skeletons. Much like its sister software chunkflow (Wu et al., <xref rid="B56" ref-type="bibr">2021</xref>), which is used for generating segmentations, Igneous is robust to task failure and can be used with cheap unreliable cloud instances (sometimes called âpreemptibleâ or âspotâ instances). Igneous can be run completely locally or massively scaled in the cloud. It requires minimal setup and no maintenance between runs. As seen in <xref rid="F2" ref-type="fig">Figure 2</xref>, it is completely independent of Neuroglancer itself, though it produces datasets that comply with the Neuroglancer Precomputed specifications.<xref rid="fn0002" ref-type="fn"><sup>2</sup></xref></p>
    <fig position="float" id="F2">
      <label>Figure 2</label>
      <caption>
        <p>Data flow. The relationship between Igneous, CloudVolume, CloudFiles, and Neuroglancer. Arrows indicate the direction of data flow with reading flowing from top to bottom and writes flowing bottom to top. Neuroglancer is a separate application that only reads data. Igneous uses CloudVolume for high level primitives such as images, meshes, and skeletons and uses CloudFiles for low level file IO. The box âProgrammatic Accessâ serves to indicate that CloudVolume and CloudFiles also provide programmatic access to the dataset for many other situations outside of Igneous.</p>
      </caption>
      <graphic xlink:href="fncir-16-977700-g0002" position="float"/>
    </fig>
    <p>In this article, we will sketch Igneous' computational framework, show how to use it, describe our innovations in meshing and skeletonization of dense segmentation, and characterize the system's performance.</p>
    <sec>
      <title>1.1. Related work</title>
      <p>Many labs have developed separate solutions for storing, visualizing, and annotating datasets of ever-increasing size as contemporary commercial solutions were not adequately scalable, were missing features, or both. These tools operate on many different principles, and most of them have a method for importing images. However, there is limited support for meshing and skeletonization of dense segmentation in bulk as we will describe below.</p>
      <p>To give a brief sketch of the landscape, these systems can be broadly characterized by the maximum data size they support (in-memory, disk, network filesystem / cloud storage), method of neural representation (skeletons, per-voxel image segmentation, network representation), their methods of visualization [single resolution image, image pyramid (Pietzsch et al., <xref rid="B38" ref-type="bibr">2015</xref>; Sofroniew et al., <xref rid="B51" ref-type="bibr">2022</xref>), precomputed or dynamic surface meshes, skeletons, volume rendering (Peng et al., <xref rid="B36" ref-type="bibr">2010</xref>; Maitin-Shepard et al., <xref rid="B31" ref-type="bibr">2021</xref>), or other], method of data storage [single file, chunks, random-access consolidated files (âshardsâ), versioning, and serverlessness], method of proofreading [skeleton tracing (Saalfeld et al., <xref rid="B41" ref-type="bibr">2009</xref>; Boergens et al., <xref rid="B12" ref-type="bibr">2017</xref>) and segment merging and splitting (Kim et al., <xref rid="B27" ref-type="bibr">2014</xref>; Katz and Plaza, <xref rid="B26" ref-type="bibr">2019</xref>; Dorkenwald et al., <xref rid="B15" ref-type="bibr">2021</xref>), with variations within each category], and openness of access to reconstructions and proofreading prior to publication (public, semi-public, or internal only), and the different overlapping communities of proofreaders, viewers, and software developers centered around each tool. Proofreading systems for editing the annotations and segmentation may have additional storage requirements such as a graph of connected segments (Anderson et al., <xref rid="B4" ref-type="bibr">2011</xref>; Ai-Awami et al., <xref rid="B2" ref-type="bibr">2016</xref>; Dorkenwald et al., <xref rid="B15" ref-type="bibr">2021</xref>). An excellent overview of the visualization landscape can be found in a recent survey by Beyer et al. (<xref rid="B10" ref-type="bibr">2022</xref>) and in the VAST paper by Berger et al. (<xref rid="B8" ref-type="bibr">2018</xref>).</p>
      <p>To place Igneous in relation to these categories, it produces Neuroglancer Precomputed volumes that are stored in cloud or network storage. Neurons are represented by per-voxel annotations, precomputed multi-resolution surface meshes, and precomputed skeletons. The image data are stored as versionless chunks or shards (see Consolidating Files). Igneous is free software that can be used by anyone. See the <xref rid="SM4" ref-type="supplementary-material">Supplementary material</xref> for additional information.</p>
      <sec>
        <title>1.1.1. Meshing</title>
        <p>Meshing is frequently available at the single machine level for many tools. However, there are few publicly available tools that provide meshing at scale. This may be because most available implementations of Marching Cubes (Lorensen and Cline, <xref rid="B30" ref-type="bibr">1987</xref>) are applicable only to binary images or continuously valued images, which requires potentially thousands of iterative evaluations for a cutout of densely labeled segmentation. Despite the relative efficiency of Marching Cubes and similar algorithms, this repeated application to a cutout can result in long processing times as large amounts of redundant computation are incurred to produce a separate mesh for every label.</p>
        <p>Additionally, once a mesh is produced it is too detailed. Marching Cubes produces one or more triangle faces for every foreground voxel in a volume. This poses problems for both the storage and display of objects as the mesh is supposed to be a lighter-weight representation than the dense labeling. With three floats per vertex and three integers per triangle face compared with a single integer per voxel, an unrefined mesh may be larger than the object's image representation. A natural solution to this problem is mesh simplification, but many such algorithms are prone to change mesh topology and existing implementations are often hard to use for a variety of reasons including the availability of language bindings and performance.</p>
        <p>However, these problems are not intractable. Several large scale multi-resolution meshings have been published as pre-prints by the Google Connectomics team (Xu et al., <xref rid="B58" ref-type="bibr">2020</xref>; MICrONS Consortium et al., <xref rid="B34" ref-type="bibr">2021</xref>; Shapson-Coe et al., <xref rid="B46" ref-type="bibr">2021</xref>). At least one sparse segmentation published using WebKnossos (Boergens et al., <xref rid="B12" ref-type="bibr">2017</xref>) has precomputed meshes (Helmstaedter et al., <xref rid="B21" ref-type="bibr">2013</xref>) (larger densely labeled volumes appear to rely on on-demand meshing). However, the meshing engines that created these datasets are not publicly available.</p>
        <p>A distributed meshing tool called mesh-deco<xref rid="fn0003" ref-type="fn"><sup>3</sup></xref> by Matelsky works by passing extracted binary images to mesh workers. This approach is compatible with sparse meshing, but will be inefficient for dense meshing.</p>
        <p>Other tools have found workarounds for the difficulties of bulk meshing. Some proofreading tools (e.g., CATMAID, Saalfeld et al., <xref rid="B41" ref-type="bibr">2009</xref>) rely on skeleton tracing and therefore have a less pressing need for bulk meshing. NeuTu found a creative method for rapidly visualizing segments by rendering all surface points of an object as a set of spheres in cases where meshes are not available (Zhao et al., <xref rid="B61" ref-type="bibr">2018</xref>).</p>
        <p>A few projects have built Neuroglancer multi-resolution mesh generation capabilities that used pre-existing base meshes. Sidky's neurogen<xref rid="fn0004" ref-type="fn"><sup>4</sup></xref> and Ackerman's Multi-Resolution-Mesh-Creator<xref rid="fn0005" ref-type="fn"><sup>5</sup></xref> for DVID (Katz and Plaza, <xref rid="B26" ref-type="bibr">2019</xref>) demonstrated the viability of using a mesh simplification strategy. Jagannathan's pyroglancer<xref rid="fn0006" ref-type="fn"><sup>6</sup></xref> also has a multi-resolution mesh creation capability.</p>
        <p>Our sister project chunkflow (Wu et al., <xref rid="B56" ref-type="bibr">2021</xref>), which uses the same meshing library that we use, also has the ability to densely mesh, but at single resolution without shards.</p>
        <p>Thus, it could be said that the access and ability to perform large-scale meshing of dense segmentation has been very uneven even though in principle versions of the algorithm and open source software have been available. We report a method for efficiently and economically mass producing large, multi-resolution simplified meshes from dense segmentation that is publicly available and easy to install.</p>
      </sec>
      <sec>
        <title>1.1.2. Skeletonization</title>
        <p>Extracting skeletons from segmented neurons have obvious benefits for neuroanatomical analysis as they are simpler to manipulate and represent the structural connectivity of the interior of a neuron rather than of the surface. However, computing them in quantity has been a challenge for the field. The most popular skeletonization algorithm in connectomics studies is TEASAR (Sato et al., <xref rid="B42" ref-type="bibr">2000</xref>; Bitter et al., <xref rid="B11" ref-type="bibr">2001</xref>; Zhao and Plaza, <xref rid="B62" ref-type="bibr">2014</xref>). However, implementations of TEASAR are often memory hungry and slow. As with meshing, this is partly due to existing implementations which only accept binary images and thus need to iteratively evaluate a densely labeled volume. However, there are other elements of the algorithm that present problems, such as requiring the construction of large graphs of connected voxels and then performing operations on this graph. Thus, usually only a select fraction of objects in a segmentation can be practically skeletonized, and that volume is usually of limited size or resolution.</p>
        <p>Some existing C++ implementations of TEASAR can be found in NeuTu<xref rid="fn0007" ref-type="fn"><sup>7</sup></xref> and Skeletopyze.<xref rid="fn0008" ref-type="fn"><sup>8</sup></xref> A Python implementation by Bae is found in Skeletonization.<xref rid="fn0009" ref-type="fn"><sup>9</sup></xref> A particularly interesting Julia implementation by Wu et al. (<xref rid="B57" ref-type="bibr">2022</xref>) uses bit packing and sparse graph representations to enable the sparse skeletonization of large neurons with full context by making it practical to fit the whole neuron in memory.</p>
        <p>In the past few years, some alternative approaches which extract skeletons from meshes have appeared in tooling. The approach by Dorkenwald et al. in MeshParty (Dorkenwald et al., <xref rid="B16" ref-type="bibr">2020</xref>) could be described as âMesh TEASARâ as it uses the surface mesh triangle graph instead of a voxel connectivity graph. Skeletor by Schlegel and Kazimiers (<xref rid="B44" ref-type="bibr">2021</xref>) contains several published algorithms for extracting a skeleton from a mesh (including a âMesh TEASARâ implementation). These approaches are promising and warrant further exploration.</p>
        <p>A popular voxel thinning algorithm implementation, Skeletonize3d<xref rid="fn0010" ref-type="fn"><sup>10</sup></xref> by Ignacio Arganda-Carrera can be found in Fiji based on the algorithm by Lee and Kashyap (<xref rid="B29" ref-type="bibr">1994</xref>) and an ITK implementation by Hanno Homann.<xref rid="fn0011" ref-type="fn"><sup>11</sup></xref> Another innovative technique by Matejek et al. called SynapseAware (Matejek et al., <xref rid="B33" ref-type="bibr">2019</xref>) modifies a voxel thinning algorithm to preserve pathways between synapses specifically to optimize the Neural Reconstruction Integrity (NRI) metric for the resultant skeletons (Reilly et al., <xref rid="B39" ref-type="bibr">2018</xref>).</p>
        <p>We report a method of mass producing high-quality skeletons directly from dense segmentation images by using a fast memory optimized TEASAR implementation and a novel chunking and stitching strategy. To our knowledge, no other publicly available tool has demonstrated this capability.</p>
      </sec>
      <sec>
        <title>1.1.3. Images</title>
        <p>Image handling is so basic to connectomics and other kinds of investigations that there is a very large amount of prior work both within and outside of the field. Therefore, we will only briefly treat the most closely related systems to confine ourselves to the available space. Again, please consult Berger et al. (<xref rid="B8" ref-type="bibr">2018</xref>) and Beyer et al. (<xref rid="B10" ref-type="bibr">2022</xref>) for more information.</p>
        <p>Nearly all major connectomics systems support the display of image pyramids. Neuroglancer, CATMAID (Saalfeld et al., <xref rid="B41" ref-type="bibr">2009</xref>), NeuTu/DVID (Zhao et al., <xref rid="B61" ref-type="bibr">2018</xref>; Katz and Plaza, <xref rid="B26" ref-type="bibr">2019</xref>), Knossos (Helmstaedter et al., <xref rid="B20" ref-type="bibr">2011</xref>), WebKnossos (Boergens et al., <xref rid="B12" ref-type="bibr">2017</xref>), PyKnossos (Wanner et al., <xref rid="B54" ref-type="bibr">2016</xref>), BigDataViewer (Pietzsch et al., <xref rid="B38" ref-type="bibr">2015</xref>), Vaa3d (Peng et al., <xref rid="B36" ref-type="bibr">2010</xref>), BossDB (Hider et al., <xref rid="B22" ref-type="bibr">2019</xref>), Omni (Shearer, <xref rid="B47" ref-type="bibr">2009</xref>), VAST (Berger et al., <xref rid="B8" ref-type="bibr">2018</xref>), RECONSTRUCT (Fiala, <xref rid="B17" ref-type="bibr">2005</xref>), VikingViewer (Anderson et al., <xref rid="B4" ref-type="bibr">2011</xref>), Dojo (Haehn et al., <xref rid="B19" ref-type="bibr">2014</xref>), Ilastik (Berg et al., <xref rid="B7" ref-type="bibr">2019</xref>), IMOD (Kremer et al., <xref rid="B28" ref-type="bibr">1996</xref>), TrakEM2 (Cardona et al., <xref rid="B14" ref-type="bibr">2012</xref>), ITK-SNAP (Yushkevich et al., <xref rid="B60" ref-type="bibr">2006</xref>), and SSECRETT/NeuroTrace (Jeong et al., <xref rid="B24" ref-type="bibr">2010</xref>) all support the storage and display of image pyramids of electron micrographs. Most of these also support the display of segmentation overlays as well. Usually, each level of the pyramid is chunked so that subsets of an image can be retrieved efficiently.</p>
        <p>Eyewire (Kim et al., <xref rid="B27" ref-type="bibr">2014</xref>) does not support the display of the entire image at once and displays only small blocks at a time. It relies on a dynamically updated overview mesh for providing overall context for each cell. Omni is used in conjunction with tasks that require large image context.</p>
        <p>Many of these tools use a custom file format for storing the image pyramid. In some cases, the image viewers are compatible with multiple formats. For example, Neuroglancer is currently compatible with the following imaging formats<xref rid="fn0012" ref-type="fn"><sup>12</sup></xref>: BossDB, BrainMaps (Google's internal format), DVID, N5<xref rid="fn0013" ref-type="fn"><sup>13</sup></xref>, nifti<xref rid="fn0014" ref-type="fn"><sup>14</sup></xref>, Precomputed (Neuroglancer's native format), Render<xref rid="fn0015" ref-type="fn"><sup>15</sup></xref>, and zarr (Miles et al., <xref rid="B35" ref-type="bibr">2022</xref>).</p>
        <p>Several tools provide guidance or a tool for aiding in the import and processing of a new image dataset. CATMAID provides an importer tool<xref rid="fn0016" ref-type="fn"><sup>16</sup></xref>, DVID has a built-in downsampler<xref rid="fn0017" ref-type="fn"><sup>17</sup></xref> and can be run in clustered fashion. Ingest clients are provided by BossDB (<monospace>ingest-client</monospace><xref rid="fn0018" ref-type="fn"><sup>18</sup></xref>) and WebKnossos (<monospace>wkCuber</monospace><xref rid="fn0019" ref-type="fn"><sup>19</sup></xref>).</p>
        <p>We report a tool for downsampling and managing cloud storage hosted Precomputed format images and segmentations that is proven to scale to hundreds of teravoxels and supports sharded images (see Condensing Files). To our knowledge, Igneous (<italic>via</italic> CloudVolume) is the first publicly available tool to support the Compresso (Matejek et al., <xref rid="B32" ref-type="bibr">2017</xref>) dense segmentation compression codec.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>2. Methods</title>
    <p>Igneous uses a dependency-free task queue in order to assign and distribute work. A schematic of how this works can be seen in <xref rid="F3" ref-type="fig">Figure 3</xref>. Dependency freedom is possible because the production and management of image pyramids, meshes, and skeletons can be broken down into either spatially chunked tasks without significant overlap or into non-overlapping ranges of integer labels which enables efficient parallelization. Some operations, such as the production of a small image pyramid can be performed in a single pass. More complex operations, such as building larger image pyramids, meshing, or skeletonization, build on top of previous results either directly or in map and reduce passes (termed âforgingâ and âmerging,â respectively, within Igneous).</p>
    <fig position="float" id="F3">
      <label>Figure 3</label>
      <caption>
        <p>How Igneous works: task creation and distribution. Task distribution and execution are robust to failure. <bold>(A)</bold> Dependency free tasks are generated as JSON from a lightweight machine and inserted into a cloud queue (Amazon Web Services' SQS) or into a the local file system, which operates similarly <bold>(B)</bold> workers from a pre-configured cluster (usually controlled <italic>via</italic> Kubernetes or SLURM) continuously attempt to acquire a time-based lease on tasks. Once the lease expires, the task is again available to be leased. <bold>(C)</bold> Once a task lease is acquired, the worker uses the instructions in the task to fetch data from cloud storage and executes the task specified procedure against it <bold>(D)</bold> on finishing execution, the results are written back into the cloud. <bold>(E)</bold> The task is then deleted from the cloud queue to prevent redundant re-execution. When all tasks are deleted the job is complete.</p>
      </caption>
      <graphic xlink:href="fncir-16-977700-g0003" position="float"/>
    </fig>
    <p>Igneous's queue, python-task-queue<xref rid="fn0020" ref-type="fn"><sup>20</sup></xref>, converts annotated python functions or objects into a lightweight JSON stream that can be submitted to a cloud queue (such as Amazon SQS) or to our on-disk emulation of SQS we term FileQueue. These queues provide a guarantee of durability; a task will either be executed or recycled upon failure. If a task fails, eventually the time-based lease will expire and another worker will re-execute the task. This possibility of re-execution requires tasks to be idempotent. Dependency-free operation allows for the parallelization of task generation, insertion, and execution, which becomes convenient for very large datasets that may require hundreds of thousands or millions of tasks.</p>
    <p>Igneous is highly versatile and easily operates in different environments. For small jobs, usually running it locally from the command line is sufficient. For larger jobs, it has been successfully used with SLURM (Yoo et al., <xref rid="B59" ref-type="bibr">2003</xref>) in Princeton's Della cluster<xref rid="fn0021" ref-type="fn"><sup>21</sup></xref> and Docker<xref rid="fn0022" ref-type="fn"><sup>22</sup></xref>/Kubernetes<xref rid="fn0023" ref-type="fn"><sup>23</sup></xref> using Google Cloud Platform.<xref rid="fn0024" ref-type="fn"><sup>24</sup></xref> As the filesystem and SQS are always available and reasonably durable, jobs can be stopped and resumed at will.</p>
    <p>So long as workers can access the queue and datastore, they can be located anywhere in the world. This enables hybrid computing using local resources, university clusters, and cloud platforms simultaneously. FileQueue enables Igneous to also be used in limited environments as it only requires a filesystem that provides POSIX advisory file locking. For example, national supercomputing centers may restrict internet connectivity on worker nodes and may not allow the installation of a persistent queuing service, but nonetheless provide a common filesystem.</p>
    <p>Igneous can theoretically work with any Key-Value store. It uses CloudVolume (Silversmith et al., <xref rid="B50" ref-type="bibr">2021b</xref>) and CloudFiles<xref rid="fn0025" ref-type="fn"><sup>25</sup></xref> software which provide threaded IO to the filesystem, Google Cloud Storage, Amazon S3, S3 emulators, and static HTTP file servers. Thus, data can be stored and shuffled to the most convenient or cost-efficient location whether that's local, in the cloud, or at an on-premises network file system or S3 emulator.</p>
    <p>The relationship between Neuroglancer, Igneous, CloudVolume, and CloudFiles can be seen in <xref rid="F2" ref-type="fig">Figure 2</xref>. Igneous uses CloudVolume and CloudFiles to read and write data to cloud storage or a network filesystem in order to create Neuroglancer readable volumes. CloudVolume, used to manage higher order primitives like image cutouts, meshes, and skeletons, uses CloudFiles for file IO. Neuroglancer, a viewer for 3D datasets, is a web page that can independently read Igenous generated datasets directly from cloud storage or from an HTTP server. Neuroglancer is a completely separate code base from Igenous, CloudVolume, and CloudFiles. The âProgrammatic Accessâ element indicates that CloudVolume and CloudFiles provide programmatic access to the dataset outside of their use in Igneous (beyond the scope of this article).</p>
    <sec>
      <title>2.1. Condensing files (âshardingâ)</title>
      <p>Random read access to large datasets is often achieved by chunking large images into many smaller files or writing out each mesh, skeleton, or other derivatives of a label as one or more files. This results in problems for the filesystem when the number of files becomes large. Even with cloud storage, as of this writing, file creation costs between $5<xref rid="fn0026" ref-type="fn"><sup>26</sup></xref> and $6.50<xref rid="fn0027" ref-type="fn"><sup>27</sup></xref> per a million files on several cloud providers on active storage (usually the default tier). When hundreds of millions or billions of files are created, the initial upload costs can be more than the cost of the computation.</p>
      <p>While it's difficult to impute the financial cost directly to the stress put on the storage system, it is notable that object storage systems often use an erasure coding (Balaji et al., <xref rid="B5" ref-type="bibr">2018</xref>) or replication scheme that creates multiple copies or parity fragments to be able to reconstruct each file in the event of bit flips or damage to machines and hard disks. Frequently, the metadata used to locate distributed copies or file fragments are themselves replicated several times. Thus, there is a constant storage multiplier attached to each file uploaded. Sometimes, even if a large dataset can be created, deleting it can be difficult on some systems, incurring delays or additional costs.</p>
      <p>As a mitigation, Neuroglancer adopted an approach that condenses many individual files into random-read (but not random-write) files called âshardsâ<xref rid="fn0028" ref-type="fn"><sup>28</sup></xref> (see <xref rid="F4" ref-type="fig">Figure 4</xref>). Shards reduce the number of files in a dataset by several orders of magnitude. This kind of approach is becoming more popular and implementations are being discussed in prominent projects like Zarr.<xref rid="fn0029" ref-type="fn"><sup>29</sup></xref> Techniques similar to sharding can be implemented by a filesystem to cope with large numbers of small files, so shards could be viewed as a client-side emulation of a filesystem feature.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>Anatomy of a Shard. A shard condenses many files into a single read-only random access file and thereby relieves strain on filesystems. Integer labels are mapped to a given shard filename <italic>via</italic> a hash function. Random read access is achieved <italic>via</italic> a two level index that maps an integer label to a corresponding byte range. The index consists of the first fixed size âShard Indexâ that then maps to variable size âMinishard Indicesâ which contain the label to byte range mappings. In the drawing above, A,B,C,D are integers and are positioned over their corresponding byte ranges. At the bottom, 0 to <italic>b</italic><sub><italic>n</italic></sub> connote the byte offset with <italic>b</italic><sub><italic>n</italic></sub> being the end of the file. The roman numerals show the sequence of accesses. Prior to caching, three requests are needed to fetch label B in byte range <italic>b</italic><sub>2</sub> to <italic>b</italic><sub>3</sub>. (I) The shard index is accessed, which points to the (II) <italic>M</italic><sub>1</sub> minishard, which (III) locates label B between bytes <italic>b</italic><sub>2</sub> and <italic>b</italic><sub>3</sub>.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0004" position="float"/>
      </fig>
      <p>Igneous provides methods for creating shard files for each data type it supports (images, meshes, and skeletons) and CloudVolume is capable of reading them. As producing shards is more complicated and removes random access writes, it is usually hidden behind a flag (e.g., <monospace>--sharded</monospace>) or a separate command (e.g., <monospace>merge-sharded</monospace>).</p>
    </sec>
    <sec>
      <title>2.2. Supported data encodings</title>
      <p>Igneous supports all encoding methods that are currently supported in Neuroglancer Precomputed.</p>
      <sec>
        <title>2.2.1. Microscopy images</title>
        <p>Electron microscopy and other natural images are often represented as single-channel (grayscale) 8-bit or 16-bit unsigned integers. Igneous supports compressing chunk files using <monospace>raw+gzip</monospace>, <monospace>raw+brotli</monospace> (Alakuijala and Szabadka, <xref rid="B3" ref-type="bibr">2016</xref>), <monospace>png</monospace><xref rid="fn0030" ref-type="fn"><sup>30</sup></xref>, and <monospace>jpeg</monospace><xref rid="fn0031" ref-type="fn"><sup>31</sup></xref> (where <monospace>raw</monospace> means a serialized array).</p>
        <p>We currently use the SIMD<xref rid="fn0032" ref-type="fn"><sup>32</sup></xref> accelerated <monospace>deflate</monospace><xref rid="fn0033" ref-type="fn"><sup>33</sup></xref> library (based on <monospace>libdeflate</monospace><xref rid="fn0034" ref-type="fn"><sup>34</sup></xref>) to accelerate gzip, <monospace>pyspng-seunglab</monospace><xref rid="fn0035" ref-type="fn"><sup>35</sup></xref> (based on <monospace>libspng</monospace><xref rid="fn0036" ref-type="fn"><sup>36</sup></xref> and <monospace>pyspng</monospace><xref rid="fn0037" ref-type="fn"><sup>37</sup></xref>) to accelerate PNG, and SIMD accelerated <monospace>simplejpeg</monospace><xref rid="fn0038" ref-type="fn"><sup>38</sup></xref> (based on libjpeg-turbo<xref rid="fn0039" ref-type="fn"><sup>39</sup></xref>) to accelerate JPEG codec performance.</p>
      </sec>
      <sec>
        <title>2.2.2. Segmentation images</title>
        <p>Connectomics segmentation images have uncommon image statistics in that they are often 64-bit unsigned integers, densely labeled so that nearly every voxel is foreground and contain potentially billions of labels that are densely packed smoothly varying organic shapes. This creates a densely packed image with simple statistics where most adjacent voxels are equal which results in an amusing contradiction. The large data width of the segmentation means that it is eight times larger than the image it was derived from but it compresses so excellently, that it is much smaller on disk.</p>
        <p>Igneous and Neuroglancer support three compression schemes that can be layered with gzip or brotli compression for segmentation images: <monospace>raw</monospace>, <monospace>compressed_segmentation</monospace><xref rid="fn0040" ref-type="fn"><sup>40</sup></xref>, and <monospace>compresso</monospace>. The latter two codecs were designed specifically with connectomics segmentations in mind.</p>
        <p><monospace>compressed_segmentation</monospace> renumbers and bit packs small 3D regions within the image (often 8 Ã 8 Ã 8 voxels) to vastly reduce the size of the representation. It works extremely well with second-stage compression which often results in both smaller data and faster IO overall. It is also randomly accessible, and so is used by Neuroglancer in order to pack more image regions into the GPU. We use a Cython wrapper (Behnel et al., <xref rid="B6" ref-type="bibr">2011</xref>) around Jeremy Maitin-Shepard's C++ encoder and Stephen Plaza's C++ decoder for our codec.<xref rid="fn0041" ref-type="fn"><sup>41</sup></xref></p>
        <p>Compresso (Matejek et al., <xref rid="B32" ref-type="bibr">2017</xref>) was designed explicitly for two-stage high compression of connectomics segmentation. Compresso represents the boundary between segments as a boolean bit packed field. Long runs of zeros are run-length encoded. The 4-connected components within that field are mapped to a corresponding label. Boundary voxels are decoded with reference to their neighbors, or if the location is ambiguous, by storing their label. We adapted the Compresso algorithm and modified pre-existing code to create a Cython/C++ codec.<xref rid="fn0042" ref-type="fn"><sup>42</sup></xref>.</p>
        <p>We've found that high-resolution images (mips 0 and 1) typically compress better with <monospace>compresso</monospace> than with <monospace>compressed_segmentation</monospace>. Lower resolution images, since they become noisier, do not compress as well and eventually <monospace>compressed_segmentation</monospace> achieves higher compression. A hybrid strategy of using <monospace>compresso</monospace> for high-resolution layers and <monospace>compressed_segmentation</monospace> for lower resolution layers could be used to achieve higher compression overall.</p>
      </sec>
      <sec>
        <title>2.2.3. Meshes</title>
        <p>Meshes generated by Igneous are written in the triangle soup Precomputed format<xref rid="fn0043" ref-type="fn"><sup>43</sup></xref>. They can be compressed with gzip or brotli for single-resolution meshes, and with Draco<xref rid="fn0044" ref-type="fn"><sup>44</sup></xref> for multi-resolution meshes. It is possible to convert meshes to the popular Wavefront OBJ or PLY formats on demand using CloudVolume.</p>
      </sec>
      <sec>
        <title>2.2.4. Skeletons</title>
        <p>Skeletons are written in the Precomputed format<xref rid="fn0045" ref-type="fn"><sup>45</sup></xref> and can be compressed with gzip or brotli. They are stored as an undirected vertex graph with vertex attributes. They can be converted to the widely used tree-based SWC format on demand using CloudVolume (loops may cause errors).</p>
      </sec>
    </sec>
    <sec>
      <title>2.3. Downsampling</title>
      <p>Large volumetric images do not fit in RAM and are not practical to download for visualization. It is also desirable to perform some types of processing on lower resolution images. Thus, it is necessary to create an image pyramid of lower resolution images that can be downloaded selectively (e.g., depending on the viewer's current zoom level). Typically, each successive resolution level (or âmipâ<xref rid="fn0046" ref-type="fn"><sup>46</sup></xref>) is downsampled using 2 Ã 2 (2D) or 2 Ã 2 Ã 2 (3D) voxel pooling. For Neuroglancer, mip 0 is the highest resolution layer, with higher integers indicating successively lower resolution layers.</p>
      <p>In calculating these mip levels, we use average pooling for natural (electron microscopy) images and recursive mode pooling for labeled (segmentation) images. Uncompressed, a pyramid of 2 Ã 2 downsamples requires 33% additional storage over the base image. A pyramid of 2 Ã 2 Ã 2 downsamples will require 14% more. In both cases, mip 1 is the dominant contributor to the additional storage (25% for 2 Ã 2, 12.5% for 2 Ã 2 Ã 2).</p>
      <p>Multiple mip levels can be generated from a single task if the task is large enough. This is advantageous because (a) it takes fewer runs to generate all mip levels (b) fewer tasks need to be generated and managed (c) the average pooling algorithm can avoid integer truncation. Nonetheless, it's often not possible to generate all mip levels in a single task as each additional level exponentially multiplies the required memory by 4â8 times. Therefore, by default, five levels are generated and another set can be generated on top of them in a process we term âsuperdownsampling.â</p>
      <p>Integer truncation becomes significant in very large volumes that may have 10+ mip levels. If it is not accounted for, the lowest resolution levels are noticeably darker than the base level as up to 0.75 luminance units can be lost per voxel at each level in 2 Ã 2 pooling or 0.875 units in 2 Ã 2 Ã 2 pooling. At 10 mips, this amounts to a potential loss of 7.5 units in 2 Ã 2 and 8.75 for 2 Ã 2 Ã 2. Our average pooling implementation in the <monospace>tinybrain</monospace><xref rid="fn0047" ref-type="fn"><sup>47</sup></xref> library mitigates this issue by accumulating sums for each mip level before dividing.</p>
      <p>2 Ã 2 Ã 2 downsampling can lead to ghosting around the edges of a slice as extensions of one slice regions can be averaged with several zeros from a gap in an adjacent slice. We offer a sparse downsampling mode that skips counting zero-valued voxels in the average.</p>
      <p>For sharded volumes, due to the large memory requirement for holding a single shard in memory, Igneous can only generate a single mip level at a time. Generating additional mip levels would require holding exponentially larger numbers of shards in memory for each mip level unless the shards shrink at each mip level, reducing their utility. This requirement means that producing a sharded mip incurs integer truncation errors. However, it is possible to produce unsharded mips from sharded mips and later condense them.</p>
    </sec>
    <sec>
      <title>2.4. Meshing</title>
      <p>While visualizing cross sections of 3D segmentation are informative, it is difficult to understand the overall shape of a neuron outside of a series of local contexts without a visual representation of the whole object. Meshing, creating a 3D surface representation consisting of vertices and faces from a segmentation, provides a light-weight method of visualizing neurons that may span an entire dataset as only the surface of the object of interest needs to be represented.</p>
      <p>Neuroglancer offers three different formats for representing meshes: single-resolution unsharded, multi-resolution unsharded, and multi-resolution sharded (single-resolution sharded is not supported). The single resolution format allows for multiple mesh fragments for each segmentation label to be written as separate files and then pointed to by a manifest JSON file with a well-known filename (âSEGID:0â). The multi-resolution formats are different in that each mesh data file must contain the whole mesh and potentially additional lower resolution meshes addressable in an octree format. In the unsharded version, each label has a data file and an index file that gives instructions for reading the data file's octree. In the sharded format, a mesh's index file and data file are concatenated and grouped with many other labels's meshes. The benefit of multi-resolution files is that Neuroglancer can display lower resolution meshes when the viewer is zoomed out, which can allow for a higher performance display of large or numerous meshes.</p>
      <p>We produce meshes by applying a specialized variant of the marching cubes algorithm (Lorensen and Cline, <xref rid="B30" ref-type="bibr">1987</xref>) to segmentation images and then simplify the results. Two passes are needed. As it is impossible to produce a mesh spanning large datasets within the memory of a single machine, the image must be divided into smaller tasks and then merged. The first pass produces mesh fragments derived from small spatial areas and writes them to storage. The second pass aggregates the mesh fragments to create the final output (and in the case of multi-resolution meshes, also generates a hierarchy of simplified meshes).</p>
      <p>Typically we begin with downsampling the segmentation image to use a near-isotropic mip level for meshing. While any mip level can be used, downsampling vastly reduces the amount of data to be processed, often by 64 Ã , while retaining a reasonable amount of detail. The selected mip level's image is then divided into a regular grid of tasks that each overlap on their positive axis face by one voxel.</p>
      <p>We wrote the free software <monospace>zmesh</monospace><xref rid="fn0048" ref-type="fn"><sup>48</sup></xref> library to produce our meshes. <monospace>zmesh</monospace> is based on <monospace>zi_lib</monospace><xref rid="fn0049" ref-type="fn"><sup>49</sup></xref>, which was first used in the Omni (Shearer, <xref rid="B47" ref-type="bibr">2009</xref>) neural reconstruction proofreading software and then in Eyewire (Kim et al., <xref rid="B27" ref-type="bibr">2014</xref>), an online crowdsourced proofreading platform. It implements a specialized version of the marching cubes algorithm (Lorensen and Cline, <xref rid="B30" ref-type="bibr">1987</xref>) which efficiently handles multi-labeled images. This allows for all meshes to be generated in a single pass instead of thousands. The resultant mesh fragments are then simplified using the methods of Garland and Heckbert (<xref rid="B18" ref-type="bibr">1997</xref>) and Hoppe (<xref rid="B23" ref-type="bibr">1999</xref>) modified to retain topological integrity by filtering out unsound simplifications.</p>
      <p>The one voxel overlap causes marching cubes to output mesh fragments that can be trivially stitched. Each fragment is then simplified before being serialized and compressed with gzip. Single resolution meshes are serialized in the legacy Precomputed format. Meshes written to the multi-resolution format are Draco compressed with integer position attributes using DracoPy.<xref rid="fn0050" ref-type="fn"><sup>50</sup></xref> To generate a resolution hierarchy, we repeatedly apply pyfqmr<xref rid="fn0051" ref-type="fn"><sup>51</sup></xref> to the base mesh with increasingly aggressive settings.</p>
    </sec>
    <sec>
      <title>2.5. Skeletonization</title>
      <p>Skeletons are stick-figure centerline representations of neurons. They are often represented as trees or graphs with vertices that are localized in space. They have many uses in analyzing the anatomy and function of neurons. For example, they can be used to compute basic properties such as cable length and wire diameter (when the skeleton is a medial axis transform). They can also be used to define functional sections of neurite during analysis such as dendrite and axon or to model electrical compartments. Skeletons can be used to guide cameras in 3D renderings and in proofreading neural circuits (as in CATMAID and WebKnossos).</p>
      <p>However, mass-producing large skeletons from dense segmentation images is difficult due to both performance and memory requirements. Most image skeletonization algorithms require holding the entire image in memory, which is impossible at near petavoxel scale. The most popular skeletonization algorithm in the connectomics field is the TEASAR algorithm (Sato et al., <xref rid="B42" ref-type="bibr">2000</xref>; Bitter et al., <xref rid="B11" ref-type="bibr">2001</xref>), due to its flexibility in skeletonization different shapes with parameterization (Zhao and Plaza, <xref rid="B62" ref-type="bibr">2014</xref>). However, the previously available implementations of TEASAR are very slow and require high memory usage. Furthermore, like meshes, mass production of skeletons can produce hundreds of millions of files, which results in difficulties with the filesystem or high cost on cloud storage. These problems previously made the mass production of skeletons impractical. However, we have been able to overcome these limitations and can now mass produce skeletons directly from segmentation images and without the need for a meshing step.</p>
      <p>We developed Kimimaro (Silversmith et al., <xref rid="B49" ref-type="bibr">2021a</xref>), a fast Python, Cython, and C++ based TEASAR-like algorithm that can process densely labeled segmentation images at hundreds of kilovoxels per a second with memory usage low enough to use on consumer hardware with a 512<sup>3</sup> voxel cutout. However, even with improvements in the core algorithm, it was still not feasible to skeletonize the entire image in one shot due to memory constraints. Thus, we chunk the image into a regular grid of 512<sup>3</sup> voxel tasks and stitch the results together in a second pass, similar to meshing. For this approach to work, several conditions must be satisfied (a) the stitching process must be made reliable (in the sense that the right connection between tasks is always made) and efficient (b) the tasks must be large enough to encompass significant morphological features or else skeletonization may go awry from lack of context (c) stitched skeletons should be free of loops.</p>
      <p>Igneous ensures reliable connections between skeleton chunks by using single voxel overlap between tasks and ensuring that skeletons generated on each side of the border will meet at the same voxel. This property allows them to be trivially stitched before post-processing. This is accomplished by changing the TEASAR algorithm to first target borders before proceeding normally. A border target is defined for each 2D connected component extracted from the boundary cross-section of each shape. Each target is defined as the voxel containing the peak euclidean distance transform of each connected component (see <xref rid="F5" ref-type="fig">Figure 5</xref>). This metric ensures that, unlike a centroid, the target voxel resides within the component.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>Selecting skeletonization border targets. Selection of border targets on one face of a rectangular 3D segmentation cutout. This process is applied to all six faces. Each face is overlapped by one voxel with the neighboring task to ensure perfect mating of adjacent skeleton traces. <bold>(A)</bold> The segmentation on the first slice of a task's 3D image. <bold>(B)</bold> Per segment normalized distance transform. Red crosses indicate the peak transform values and are the border targets.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0005" position="float"/>
      </fig>
      <p>To break ties between peak voxels, we use topological features to ensure that the criteria are coordinate-frame independent as the connecting coordinate frames are mirrored on each side of the task (top &amp; bottom, left &amp; right, back &amp; front). The tie-breaking features in order of precedence:</p>
      <list list-type="order">
        <list-item>
          <p>Shape Centered: The peak value closest to the centroid of the current shape.</p>
        </list-item>
        <list-item>
          <p>âCenternessâ: Between values that are tied for shape centering, we choose the value closest to the center of the face.</p>
        </list-item>
        <list-item>
          <p>âCornernessâ: Between values that are equidistant from both the shape centroid and the image centroid, choose the values that are closest to the corners of the image.</p>
        </list-item>
        <list-item>
          <p>âEdgenessâ: Of the values equidistant from the shape center, the image center, and corners, pick the one closest to the edge of the image.</p>
        </list-item>
      </list>
      <p>These criteria work well for most shapes but a small number of shapes such as an annulus, X, square shape, or specially constructed irregular shapes centered at the image center will generate up to eight candidate points. We haven't implemented a final tie-breaker, but it could be resolved by introducing an asymmetric criterion (such as selecting the top-left peak). This can be made to work if it alternates between top-left and top-right depending on the orientation (e.g., up vs. down) of the face, otherwise, it will introduce a disconnection.</p>
      <p>If a shape contacts the edge or corner of the bounding box, it will generate two and three targets respectively. The adjacent tasks will also draw skeletons to each of these target points, resulting in small loops. These loops are later resolved during post-processing during which loops are removed, small extensions are pruned, and components closer together than the nearest boundary are joined.</p>
      <p>By using a fast and memory-efficient library, a chunked skeletonization strategy, and supporting sharded skeleton production (see Condensing Files), Igneous makes mass production of quality skeletons practical. Ensuring that Kimimaro has appropriate visual context is a somewhat more difficult problem that doesn't yet have a perfect solution as the arbitrary division of the image into a regular grid can split objects. A reasonable way to manage this problem is to ensure that the amount of visual context in the image is greater than the size of the largest objects of interest. Reducing image resolution and increasing the size of each task can both increase visual context.</p>
      <p>Similar to meshing, we often skeletonize volumes at a near-isotropic resolution. While our highest resolution segmentation is usually 4 Ã 4 Ã 40 or 8 Ã 8 Ã 40 <italic>nm</italic><sup>3</sup>, ordinarily skeletonization is processed at 32 Ã 32 Ã 40 <italic>nm</italic><sup>3</sup>. At lower resolutions, such as 64 Ã 64 Ã 40 <italic>nm</italic><sup>3</sup>, we find that skeletons become noticeably de-centered from neurites. At even lower resolutions, thin processes may become disconnected.</p>
      <p>By default, skeletonization tasks are 512<sup>3</sup> voxels, which at 32 Ã 32 Ã 40 <italic>nm</italic><sup>3</sup> equates to 16.4 Ã 16.4 Ã 20.5 Î¼<italic>m</italic><sup>3</sup> of physical context. The interconnection scheme described above works well for wire-like objects, but more bulbous objects such as somata, require full context to produce a reasonable skeleton. It is difficult to guarantee that large objects will have full context when crudely dividing the image, so future work will be needed to refine this aspect.</p>
    </sec>
    <sec>
      <title>2.6. Contrast correction</title>
      <p>Contrast correction <italic>via</italic> histogram equalization on a per-Z slice basis is supported for 8 and 16-bit images with optional right and left tail clipping. This operation is two pass as statistical information about the histogram of each slice must be known before the adjustment can be made. The first pass collects sample data from patches assigned in a regular grid at a configurable fraction of the image area (default 1%). It then writes a JSON file for each slice containing the sampled histogram. A second pass of contrast correction tasks then performs the histogram equalization on a regular grid of chunks to avoid downloading an entire slice (which may be dozens of gigabytes). Downsampling is integrated into the second pass to enable rapid visualization and save work.</p>
    </sec>
    <sec>
      <title>2.7. Dataset management</title>
      <p>Managing large datasets is a problem in its own right. Simply enumerating a billion files can become a challenge when only thousands or tens of thousands of filenames can be listed per second. Nonetheless, it is frequently advantageous to move datasets to share them with collaborators, to move the data closer to the site of computation, to use a more economical storage provider, or to re-encode them with a different compression algorithm.</p>
      <p>Igneous provides convenient commands to transfer, re-encode, re-chunk, delete, and condense large data for images, meshes, and skeletons. Transfers and deletions are often more efficient when working with sharded volumes as fewer files need to be manipulated.</p>
      <sec>
        <title>2.7.1. Transfer, re-chunking, and re-encoding</title>
        <p>Image transfers divide the image into a regular grid of tasks, each of which manages the transfer of its grid space. Except in special cases, transfers download the image region and render it into an array internally before constructing the files to write. This allows images to be downsampled as they are transferred, which saves future downsampling work and makes it easier to visualize the transfer in progress as Neuroglancer can be used even when the transfer is incomplete.</p>
        <p>At the same time, a new chunking scheme and/or encoding can be applied. For example, a 3D dataset can be converted into 2D slices or vice versa. 3D chunking is better for visualization and certain IO patterns, while 2D slices are very useful during the alignment phase. A <monospace>raw</monospace> encoding, for example, can be changed to a higher compression encoding such as <monospace>jpeg</monospace> or <monospace>compresso</monospace>.</p>
        <p>If no new encoding is applied, no downsamples are generated, and the chunk size is identical, the transfer is performed very efficiently without decompressing and recompressing each file.</p>
        <p>Mesh and skeleton transfers are simpler. They divide up the label or shard file prefix space and assign ranges to each task for transfer. In most cases this works well, but if the stored labels are clustered under a long prefix, a custom strategy may be needed.</p>
      </sec>
      <sec>
        <title>2.7.2. Deletion</title>
        <p>Igneous deletes image data in much the same way it performs other operations. The dataset is divided into a rectangular grid with a task assigned to each grid space. The delete tasks do not require downloading or uploading data, so each task can be much larger. By default, each task will also delete the five mip levels above it. For very large datasets, it might be necessary to âsuperdeleteâ the higher mip levels in a second or third pass.</p>
        <p>Unsharded skeletons and meshes are handled similarly to transfers. The prefix space is divided up and a prefix is assigned to each task. Shards can be deleted using Igneous, but they are usually few enough in number that an ordinary deletion command will suffice within a few minutes. For example, 10<sup>5</sup> shards deleted at a rate of 300 per second will be eliminated within 6 min.</p>
      </sec>
      <sec>
        <title>2.7.3. Sharded transfers</title>
        <p>Using a transfer command with the <monospace>--sharded</monospace> flag will automatically create tasks to aggregate unsharded data into sharded data. As of this writing, unsharded to unsharded, unsharded to sharded, and sharded to sharded transfers are all supported. Only sharded to unsharded is not yet supported, though it may be useful to restore random write access to a sharded dataset. Downsamples are not automatically generated for sharded image transfers.</p>
      </sec>
    </sec>
    <sec>
      <title>2.8. Using and installing igneous</title>
      <p>Igneous can be installed on any system meeting the following requirements (a) runs a supported cPython version (currently 3.7+) (b) runs a recent Linux, MacOS, or Windows operating system OR can run an Ubuntu Linux based Docker container (c) all workers can access a common queue <italic>via</italic> either FileQueue which requires a filesystem with consistent advisory file locks OR <italic>via</italic> internet access to AWS SQS. Installation with Python pip is simple: <monospace>pip</monospace><monospace> install</monospace><monospace> igneous-pipeline</monospace>.</p>
      <p>Igneous can be scripted in Python or run from the command line. A typical workflow is to first select a dataset and operation, and then enqueue a set of tasks in either FileQueue or Amazon SQS from a local workstation. Then, using the execute command, which may be used on the same workstation or a cluster, the queue is selected and executed against with one worker process per an available core. Execution continues until the queue is empty. Termination can be set to automatic in the case of FileQueue, but SQS only returns the approximate number of tasks enqueued and so requires either manual monitoring or repeated polling to verify the current job is finished. Examples of Igneous CLI commands can be seen in <xref rid="F12" ref-type="fig">Listing 1</xref> and an enumeration of the available commands is available in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <fig position="float" id="F12">
        <label>Listing 1</label>
        <caption>
          <p>Select examples of using igneous on the command line. Square brackets indicate optional arguments.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0012" position="float"/>
      </fig>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Igneous CLI commands and sub-commands.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Command</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Sub-commands</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">design</td>
              <td valign="top" align="left" rowspan="1" colspan="1">bounds, ds-memory, ds-shape</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">image</td>
              <td valign="top" align="left" rowspan="1" colspan="1">contrast, downsample, rm, xfer</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">mesh</td>
              <td valign="top" align="left" rowspan="1" colspan="1">forge, merge, merge-sharded, rm, spatial-index, xfer</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">skeleton</td>
              <td valign="top" align="left" rowspan="1" colspan="1">forge, merge, merge-sharded, rm, spatial-index, xfer</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">execute</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">view</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">license</td>
              <td rowspan="1" colspan="1"/>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>Some commands, such as spatial-index, have deeper trees.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Parallel execution can be accomplished by any runner that can run the command <monospace>igneous</monospace><monospace> execute</monospace><monospace>$QUEUE</monospace> where <monospace>$QUEUE</monospace> is a path (such as <monospace>./queue</monospace> or <monospace>sqs://my-queue</monospace>). On a local machine, parallel execution can be triggered by the <monospace>-p</monospace> flag which indicates the number of processes to start. For large distributed jobs, we have used Docker/Kubernetes and SLURM as runners, but many other platforms would be suitable. Queue progress can be monitored and managed <italic>via</italic> the co-installed <monospace>ptq</monospace> (short for âPython Task Queueâ) command line utility.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <p>We attempted to characterize the performance of several aspects of Igneous by fully processing a segmentation of the well-known S1 dataset, by reporting the computation required by large historical runs, and reporting the efficacy of compression algorithms as applied to CREMI data (<ext-link xlink:href="https://cremi.org/data/" ext-link-type="uri">https://cremi.org/data/</ext-link>). We also used CREMI data to roughly characterize the quality of Igneous generated meshes and skeletons.</p>
    <sec>
      <title>3.1. Evaluating image compression on CREMI data</title>
      <p>As images generally comprise the bulk of storage for a given connectomics dataset, we attempted to characterize the efficacy of different compression technologies supported by Neuroglancer. We downloaded three pairs of padded test image and training segmentation datasets. We converted the HDF5 files into uncompressed Neuroglancer Precomputed volumes using CloudVolume and then re-encoded the raw volume using the <monospace>igneous</monospace><monospace> xfer</monospace> command. For each re-encoding, we measured the computation time taken and the resultant disk space used. For decoding, we timed reading each resultant volume with CloudVolume 8.7.0 five times and reported the average.</p>
      <p>All measurements were taken on a 2021 M1 Macbook Pro with an SSD using one process. The measurements for CREMI volumes A+, B+, and C+ were averaged into a point estimate of computation time in megavoxels per second (MVx/s) and disk space required for each encoding. All three CREMI image volumes were 8-bit 3072 Ã 3072 Ã 200 voxels (1.9 gigavoxels each, 5.7 gigavoxels total). All three CREMI segmentation volumes were all rendered as 64-bit 1250 Ã 1250 Ã 125 voxels (195 MVx each, 586 MVx total).</p>
      <p>For microscopy images (see <xref rid="F6" ref-type="fig">Figure 6</xref>), <monospace>raw</monospace> (uncompressed), <monospace>raw+gzip</monospace>, <monospace>raw+brotli</monospace>, <monospace>jpeg</monospace>, and <monospace>png</monospace> transfer encodings were evaluated. PNG and gzip were evaluated at compression level 9, brotli at level 5, and JPEG at 85% quality. For segmentation images, we evaluated <monospace>raw</monospace> (uncompressed), <monospace>compresso</monospace>, and <monospace>compressed_segmentation</monospace> encodings layered in combination with gzip level 9 and brotli level 5.</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>Microscopy image compression performance. We evaluated Neuroglancer compatible compression codecs against CREMI microscopy images by re-encoding an uncompressed volume with Igneous. Averages across the three images are shown. <bold>(Top)</bold> Compression factor (original/compressed bytes), larger is better. The dashed line indicates the level of no compression. (<bold>Bottom</bold>, left/blue) Encoding speed in megavoxels per second (MVx/s) (<bold>Bottom</bold>, right/orange). Decoding speed in MVx/s. Larger is better for all three metrics.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0006" position="float"/>
      </fig>
      <p>The best compression and encoding speed for images was the lossy JPEG encoding (5.98 Ã , 288.6 MVx/s). The best lossless compression was PNG (1.89 Ã ) though it was the slowest (13.7 MVx/s). <monospace>raw+brotli</monospace> and <monospace>raw+gzip</monospace> had similar encoding speeds (25.7 and 25.8 MVx/s), but <monospace>raw+brotli</monospace> gave a slightly smaller file size (1.40 Ã gzip and 1.48 Ã brotli).</p>
      <p>Decoding was uniformly much faster than encoding for microscopy images. The fastest decoding time was naturally held by uncompressed data (753.4 MVx/s). <monospace>jpeg</monospace> was second (574.5 MVx/s). Of the lossless compression codecs, <monospace>raw+gzip</monospace> (363.0 MVx/s) and <monospace>raw+brotli</monospace> (361.8 MVx/s) were similar. <monospace>png</monospace> was the slowest (74.5 MVx/s).</p>
      <p>For segmentation (see <xref rid="F7" ref-type="fig">Figure 7</xref>) we evaluated <monospace>raw</monospace>, <monospace>compressed_segmentation</monospace>, and <monospace>compresso</monospace> each with no compression, gzip, and brotli second stage compression. The overall best compression was given by <monospace>compresso+brotli</monospace> (570 Ã , 64.7 MVx/s). <monospace>compresso+gzip</monospace> gave a similar compression ratio, but was slightly slower at encoding. <monospace>compressed_segmentation+brotli</monospace> was slightly faster (69.4 MVx/s) but only yielded a 333 Ã compression ratio (58%). The fastest overall method was writing uncompressed raw arrays (98.5 MVx/s), though it is not suitable for large datasets. Using <monospace>compresso+brotli</monospace> resulted in an 8 Ã speedup and a 4.8 Ã compression improvement vs. the naÃ¯ve <monospace>raw+gzip</monospace> approach.</p>
      <fig position="float" id="F7">
        <label>Figure 7</label>
        <caption>
          <p>Segmentation image compression performance. We evaluated Neuroglancer compatible compression codecs against CREMI training segmentation images by re-encoding an uncompressed volume with Igneous. Averages across the three images are shown. <bold>(Top)</bold> Compression factor (original/compressed bytes), larger is better. (<bold>Bottom</bold>, left/blue) Encoding speed in megavoxels per second (MVx/s) (<bold>Bottom</bold>, right/orange). Decoding speed in MVx/s. Larger is better for all three metrics. Key | cpso: Compresso; cseg: compressed_segmentation; br: Brotli; gz: Gzip; raw: Uncompressed.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0007" position="float"/>
      </fig>
      <p>Decoding speeds were faster than encoding speeds in all segmentation trials and showed less variability between codecs. Encoding and decoding speeds were also closer in magnitude than with microscopy image compression. <monospace>compresso</monospace> was slowest (96â97 MVx/s). Interestingly, it seemed to yield the same decoding speed regardless of second layer compression type (including none). <monospace>raw</monospace> compression was slightly faster and <monospace>compressed_segmentation</monospace> was the fastest encoding type (111â115 MVx/s). <monospace>raw</monospace> without any compression was fastest at 122.5 MVx/s.</p>
    </sec>
    <sec>
      <title>3.2. Processing mouse primary somatosensory cortex (S1) segmentation</title>
      <p>In order to characterize the computation and disk space required to process a dataset using Igneous, we processed a rough automatic segmentation of the well-known mouse S1 dataset (Kasthuri et al., <xref rid="B25" ref-type="bibr">2015</xref>) which is 283.1 gigavoxels (7.4 GB gzipped) and has a resolution of 6 Ã 6 Ã 30 <italic>nm</italic><sup>3</sup> (see <xref rid="F1" ref-type="fig">Figure 1</xref>). We downloaded the dataset from cloud storage to an 8-core 2021 M1 Macbook Pro with an SSD. The segmentation was then converted into a sharded <monospace>compresso+gzip</monospace> encoded image (shards do not currently support brotli) with 256 Ã 256 Ã 32 voxel chunks (Compresso is more effective on larger XY planes, hence the asymmetry). It was then unsharded downsampled to mip 5 in one step. The downsampled chunks were <monospace>compresso+brotli</monospace> compressed. It was then meshed in Draco compressed sharded format with a single resolution at image resolution 24 Ã 24 Ã 30 <italic>nm</italic><sup>3</sup>, and skeletonized using parameters intended to capture spines at the same resolution. The results can be seen in <xref rid="F8" ref-type="fig">Figure 8</xref>.</p>
      <fig position="float" id="F8">
        <label>Figure 8</label>
        <caption>
          <p>Igneous generated meshes and skeletons in S1. An example of an Igneous generated mesh and skeleton extracted from the S1 dataset displayed in Neuroglancer. <bold>(A)</bold> The skeleton is overlaid on a volumetric image cross section. The colors of an automatic segmentation can be faintly seen. <bold>(B)</bold> A close up of a semi-transparent 3D view of the same segment meshed and skeletonized. Accurate skeletonization of spines and reasonable branching behavior can be seen.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0008" position="float"/>
      </fig>
      <p><xref rid="T2" ref-type="table">Table 2</xref> shows the time and disk usage required by each stage of processing. In total, 36 core-hours were required, which works out to 7.9 gigavoxels/core-hr. The most computationally demanding tasks by far were generating mesh and skeleton fragments (âforgingâ) at all spatial grid points. However, in our experience, if parameters are poorly chosen or large mergers are present, skeleton merging can become demanding as well.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Igneous processing time and disk usage in the S1 dataset.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Job</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Core-hours</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Disk usage (GB)</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Convert segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.6</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.3 (one mip)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Downsample segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.5</td>
              <td valign="top" align="left" rowspan="1" colspan="1">5.4 (all mips)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Mesh forging (mip 2)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15.6</td>
              <td valign="top" align="left" rowspan="1" colspan="1">4.6 (fragment files)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Mesh merging</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.5</td>
              <td valign="top" align="left" rowspan="1" colspan="1">1.1 (shard files)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Skeleton forging (mip 2)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">15.5</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.3 (fragment files)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Skeleton merging</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.3</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.3 (shard files)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Totals</td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.0</td>
              <td valign="top" align="left" rowspan="1" colspan="1">6.8 (final files)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Quite sensibly, segmentation images including downsamples comprise the bulk of disk storage (5.4 GB), but thanks to compression, the highest resolution image (2.3 GB) isn't the single largest consumer of disk space. Instead, mesh intermediate files (4.6 GB) are the largest single item. While the intermediate fragments are simplified, they are gzip compressed while the final meshes will be merged and Draco compressed. Additionally, while meshes represent only the surface of many neighboring voxels, they do so by using three floating point numbers for the vertices plus three integers for each triangle face. For some geometries, this can inflate the size of the representation compared with the image, especially when the image is well-compressed. Skeleton fragments and final files (both 300 MB) are lightweight as expected.</p>
      <p>6.8 GB of final compressed files remained after processing. An additional 4.6 GB of intermediate mesh fragment files and 300 MB of intermediate skeleton fragment files were also present which can be deleted. If no intermediate files are deleted, the total size of the finished dataset is 12 GB including the spatial index (which can be useful for end-users beyond the generation of meshes and skeletons).</p>
    </sec>
    <sec>
      <title>3.3. Historical runs</title>
      <p>To help illustrate Igneous's ability to scale, we provide some illustrative examples of the performance of several large jobs that have been run over the years. These jobs were run on Google Cloud Platform and Google Cloud Storage using preemptible 16-core n1-standard-16 (104 GB RAM), n1-highmem-16 (128 GB RAM), and e2-highmem-16 (128 GB RAM) machine types with one Igneous process per a core. Igneous's performance may have improved since these jobs were run.</p>
      <p>Note that while we provide core-hours here, the total cost of these jobs depended on many factors such as the number of reads, writes, and bandwidth. Often the most expensive cost was writing unsharded meshes or skeletons as this would generate at least one or two files per a segmentation label resulting in hundreds of millions or billions of files generated. Hence, for large datasets we now recommend the sharded format.</p>
      <p><xref rid="T3" ref-type="table">Table 3</xref> was compiled retrospectively from contemporaneous notes. It shows that Igneous scales to jobs requiring at least 16,000 cores and 1,000 machines simultaneously for grayscale image downsampling on a 95 teravoxel volume which was completed in a little over an hour in wall-clock time. The largest volume processed was 298 teravoxels which was a segmentation downsampling job and was completed in a little over 3 h wall-clock time.</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Historical Igneous run characteristics.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Date</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Job</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Res. <italic>nm</italic><sup>3</sup></bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>TVx</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Tasks</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Nodes</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cores</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Core-hrs</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3/9/19</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Downsample image</td>
              <td valign="top" align="center" rowspan="1" colspan="1">16 Ã 16 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.5M</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">16,000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">19,200</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">4/13/19</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Meshing (primary)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 Ã 32 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50k</td>
              <td valign="top" align="center" rowspan="1" colspan="1">250</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4,000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">12,700</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">4/19/19</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Downsample segmentation</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8 Ã 8 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">298</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.1M</td>
              <td valign="top" align="center" rowspan="1" colspan="1">200</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3,200</td>
              <td valign="top" align="center" rowspan="1" colspan="1">10,400 (est.)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5/4/19</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Meshing (primary)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 Ã 32 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">175k</td>
              <td valign="top" align="center" rowspan="1" colspan="1">350</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5,600</td>
              <td valign="top" align="center" rowspan="1" colspan="1">99,800</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">5/5/19</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Meshing (merging)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 Ã 32 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3M</td>
              <td valign="top" align="center" rowspan="1" colspan="1">40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">640</td>
              <td valign="top" align="center" rowspan="1" colspan="1">2,450</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1/21/20</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Sharded skeletonization (primary)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 Ã 32 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74</td>
              <td valign="top" align="center" rowspan="1" colspan="1">143k</td>
              <td valign="top" align="center" rowspan="1" colspan="1">35</td>
              <td valign="top" align="center" rowspan="1" colspan="1">560</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91,000 (est.)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">12/29/20</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Meshing (primary)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 Ã 32 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">11.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">112k</td>
              <td valign="top" align="center" rowspan="1" colspan="1">100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,600</td>
              <td valign="top" align="center" rowspan="1" colspan="1">46,400</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">12/31/20</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Meshing (merging)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 Ã 32 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">11.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">300k</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">320</td>
              <td valign="top" align="center" rowspan="1" colspan="1">&lt; 4,800 (est.)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3/24/22</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Sharded skeletonization (primary)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 Ã 32 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4k</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20</td>
              <td valign="top" align="center" rowspan="1" colspan="1">320</td>
              <td valign="top" align="center" rowspan="1" colspan="1">3,700 (est.)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3/24/22</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Sharded skeletonization (merging)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 Ã 32 Ã 40</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,024</td>
              <td valign="top" align="center" rowspan="1" colspan="1">330 (est.)</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>All runs are unsharded except where noted. The rows are drawn from four different unpublished versions of datasets. Three are drawn from different datasets generated in the course of the cubic millimeter project and one from FAFB. The merging step of the corresponding January 2020 skeletonization run was an iteratively developed and so is omitted. The final sharded skeletonization run had a feature called âfix avocadosâ enabled that slowed it down. Core-Hours annotated with â(est.)â are upper bound estimates. TVx stands for teravoxels. Core-Hours additionally annotated with â &lt; â may be a significant overestimate.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>The most computationally expensive job listed was the 5/4/19 unsharded primary mesh fragment generation step for an unpublished draft automatic segmentation of the cubic millimeter dataset (MICrONS Consortium et al., <xref rid="B34" ref-type="bibr">2021</xref>) which took 99,800 core-hours. By comparison, the following merging step was much less intensive and took only 2,450 core-hours.</p>
      <p>Skeletonization was similarly demanding. The most computationally expensive job listed was the 1/21/20 sharded skeleton fragment generation at an estimated 91,000 core-hours for the same dataset. The skeletonization parameters were set to capture dendritic spines. That run produced 397,770,063 skeletons in 524,288 shard files which occupied 747.6 GB of disk space. An example skeleton can be seen in <xref rid="F9" ref-type="fig">Figure 9</xref>. The corresponding merging operation is not shown in <xref rid="T3" ref-type="table">Table 3</xref> as it was run almost a year later and at the time required an iterative development cycle to increase performance over a period of months making a point estimate unhelpful.</p>
      <fig position="float" id="F9">
        <label>Figure 9</label>
        <caption>
          <p>Large scale skeleton. An example skeleton extracted from an early automatic segmentation of a large subset of the cubic millimeter dataset (<ext-link xlink:href="https://www.microns-explorer.org/cortical-mm3" ext-link-type="uri">https://www.microns-explorer.org/cortical-mm3</ext-link>) displayed in Neuroglancer. This skeleton was mass produced alongside hundreds of millions more, though only a fraction of segments represent fairly complete cells. The semi-transparent silhouette of the cell's surface mesh can be seen. <bold>(A)</bold> A zoomed out view of the cell. <bold>(B)</bold> A closer view of the area around the cell body. <bold>(C)</bold> A close up view of one of its dendrites.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0009" position="float"/>
      </fig>
      <p>The skeletonization run on 3/24/22 was conducted on an unpublished dataset with spine capture and âavocado fixingâ enabled. This caused the primary skeletonization phase to be slower than would otherwise be expected. The merging run had the somewhat expensive âremove ticksâ feature disabled, and thus preserved small extensions.</p>
    </sec>
    <sec>
      <title>3.4. Characterizing mesh quality</title>
      <p>Though they are primarily for visualization, meshes may be used to derive scientific insights. Therefore, we endeavored to provide some basic characterization of mesh quality. Using the same CREMI A padded segmentation used in our image compression experiments, we compared Igneous/zmesh generated and stitched meshes to meshes generated by <monospace>scikit-image</monospace><xref rid="fn0052" ref-type="fn"><sup>52</sup></xref> using <monospace>skimage.measure.marching_cubes</monospace> version 0.18.1 using the lewiner algorithm. An example object that was meshed using both methods can be seen in <xref rid="F10" ref-type="fig">Figure 10</xref>.</p>
      <fig position="float" id="F10">
        <label>Figure 10</label>
        <caption>
          <p>Neuroglancer screenshot of scikit-image and Igneous versions of a mesh. Unsimplified meshes produced by <bold>(left)</bold> scikit-image and <bold>(right)</bold> Igneous.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0010" position="float"/>
      </fig>
      <p>There are 37,366 unique labels and 37,828 26-connected components in the segmentation. Of these labels, 36,902 (98.76%) are smaller than 1,000 voxels while 464 (1.24%) are greater than or equal to 1,000 voxels. We generated all meshes using <monospace>zmesh</monospace> as both unsimplified and simplified versions from the base resolution segmentation. Simplification was performed using <monospace>zmesh</monospace>'s built in algorithm using a triangle reduction target factor of 100 and a maximum error tolerance of 40 nanometers. <monospace>scikit-image</monospace> had a limitation where only objects 2 Ã 2 Ã 2 or larger could be meshed, so only 586 meshes could be generated.</p>
      <p>We then used Trimesh<xref rid="fn0053" ref-type="fn"><sup>53</sup></xref> version 3.10.8 to check the resultant meshes for topological integrity using the <monospace>mesh.is_volume</monospace> property. According to their documentation, this checks watertightness (every edge is included in two faces), having consistent winding (each shared edge going in an opposite direction), and outward facing normals. All Igneous/zmesh meshes passed this check. <monospace>scikit-image</monospace> meshes failed this check. It is possible that they contained degenerate triangles as they failed a convexity test. Visually, spot checks of the unsimplified <monospace>zmesh</monospace> and <monospace>scikit-image</monospace> meshes were almost indistinguishable and overlapped almost exactly in space.</p>
      <p>In order to quantify the degree to which the <monospace>zmesh</monospace> meshes fairly represented the underlying segmentation, we computed the volume and centroids of all labels and plotted histograms of the ratio of mesh volume (computed with Trimesh) to voxel volume for both simplified and unsimplified meshes as seen in <xref rid="SM4" ref-type="supplementary-material">Supplementary Figure 1</xref>.</p>
      <p>In this figure, it can be seen that small label volumes are often grossly underestimated by the mesh while large labels are usually underestimated within 5 or 10% of the voxel volume for unsimplified and simplified meshes respectively. Marching Cubes cuts voxel corners to create a reasonable manifold, so it is sensible that small meshes will show larger deviations while larger meshes will show smaller deviations. In the bottom right of the figure, it can be seen that many very small objects get simplified to near or actually zero volume.</p>
      <p>In <xref rid="SM4" ref-type="supplementary-material">Supplementary Figure 2</xref>, we computed the difference in voxels between mesh centroids and image label centroids for <monospace>zmesh</monospace> unsimplified and simplified and <monospace>scikit-image</monospace> meshes for all labels that were valid for <monospace>scikit-image</monospace>. We then evaluated the labels using connected-components-3d (Silversmith, <xref rid="B48" ref-type="bibr">2021</xref>) and the Trimesh centroid method (which does not rely on correct manifold topology). The maximum error for <monospace>zmesh</monospace> was 53.1 voxels, while for <monospace>scikit-image</monospace> it was 417.9 voxels. The mean error for <monospace>zmesh</monospace> is 4.9 with a standard deviation of 6.7 for both unsimplified and simplified meshes. For <monospace>scikit-image</monospace>, the mean is 8.4 and the standard deviation is 27.3. In the figure, it can be seen that all three groups are fairly similar, but <monospace>scikit-image</monospace> has a long tail of large errors.</p>
    </sec>
    <sec>
      <title>3.5. Characterizing skeleton quality</title>
      <p>Characterizing skeleton quality is somewhat more difficult than with meshes due to the different skeletonizations that can be proposed for a given object depending on the needs of the user. Therefore, skeletons are somewhat more subjective though there are proposed definitions for canonical skeletons based on a grass-fire analogy, the centers of maximally inscribed spheres, and other representations (Tagliasacchi et al., <xref rid="B52" ref-type="bibr">2016</xref>).</p>
      <p>We attempted to characterize Igneous produced skeletons by coarsely comparing them with other automatically traced skeletons on a well-known dataset. Unfortunately, datasets with manually traced skeletons generally do not have a per-voxel segmentation and vice-versa. Therefore, we decided to compare automatic skeletonizations of CREMI A as was done for meshes. We attempted to locate a TEASAR implementation that was able to be installed on our available hardware, that we were able to operate properly, and was not written by one of this article's authors. However, we were unable to do so. Therefore, we made our comparisons to skeletons generated by the popular binary image skeletonization procedure in Fiji (Schindelin et al., <xref rid="B43" ref-type="bibr">2012</xref>), which implements the 1994 voxel thinning algorithm by Lee and Kashyap (<xref rid="B29" ref-type="bibr">1994</xref>).</p>
      <p>We used Igneous to process CREMI A at 16 Ã 16 Ã 40 <italic>nm</italic><sup>3</sup> resolution using the parameters <monospace>const</monospace><monospace> 50</monospace>, <monospace>scale</monospace><monospace> 3</monospace>, <monospace>soma-accept</monospace><monospace> 3500</monospace>, <monospace>soma-detect</monospace><monospace> 1100</monospace>, <monospace>soma-const</monospace><monospace> 300</monospace>, and <monospace>soma-scale</monospace><monospace> 1</monospace> on all segments with greater than or equal to 1,000 voxels. We did not use the short extension (âtickâ) elimination feature (though it may have slightly improved some skeletons). For Fiji, we processed the segmentation using the same size threshold at the same resolution into a series of binary TIFF files and then batch processed them. The resultant thinned binary images were processed into SWC files and then converted into Neuroglancer Precomputed skeletons that were correctly offset into the same space as the Igneous skeletons. We visually confirmed <italic>via</italic> spot checks that both sets of skeletons appeared in Neuroglancer and seemed on-balance reasonable in their topology and location in space.</p>
      <p>We then made several comparisons between these skeletons to characterize them. First, we used the Trimesh 3.10.8 library to check the number of points that lay outside of their enclosing (<monospace>zmesh</monospace> unsimplified) mesh. Nine skeletons were unable to be compared due to Trimesh repeatedly crashing during the computation. In total, 332 segments were able to be compared out of 341. 0.3% of all vertices for both Fiji and Igneous skeletons lay outside the mesh. The existence of this small quantity may be due to small differences between the 16 Ã 16 Ã 40 <italic>nm</italic><sup>3</sup> resolution and the meshes created at 4 Ã 4 Ã 40 <italic>nm</italic><sup>3</sup>.</p>
      <p>In <xref rid="SM4" ref-type="supplementary-material">Supplementary Figure 3</xref>, we compared the difference in centroids, ratio of cable lengths, and difference in number of terminal points (vertices with only one connecting edge) for each set of skeletons. It can be seen that the maximum difference between centroids is 2,172 nm, though most are much less than that. The average distance between centroids is 188 nm with a standard deviation of 244 nm. We visually inspected the twelve segments more than 676 nm (two standard deviations larger) than the mean to determine their issues. In four cases, the thinning algorithm only skeletonized one of multiple connected components, leading to a much shorter cable length. In six cases, the thinning algorithm created a complex structure we referred to as a âbeehiveâ (see <xref rid="SM4" ref-type="supplementary-material">Supplementary Figure 4</xref>) that added extraneous path length. Three cases were more ambiguous as to which was the better skeleton, but the thinning algorithm preserved more branches and holes. A more typical case where both thinning and Igneous created reasonable skeletons can be seen in <xref rid="F11" ref-type="fig">Figure 11</xref>.</p>
      <fig position="float" id="F11">
        <label>Figure 11</label>
        <caption>
          <p>Comparing a similar voxel thinning and Igneous skeleton. Above is pictured a Neuroglancer screenshot of a meshed object in silhouette and its skeleton visualized as a bright line produced by (cyan) Igneous and (purple) voxel thinning <italic>via</italic> Fiji's Skeletonize3d routine.</p>
        </caption>
        <graphic xlink:href="fncir-16-977700-g0011" position="float"/>
      </fig>
      <p>The mean cable length was 4778.7 nm (stddev 3358.5 nm) for Igneous skeletons and 6613.5 nm (stddev 5584.4 nm) for thinned skeletons. 281 (85.49%) Igneous skeletons were shorter than their thinned skeleton counterpart and 51 (15.41%) were larger. The outlier on the right hand side of the middle subplot of <xref rid="SM4" ref-type="supplementary-material">Supplementary Figure 3</xref> is a near spherical object that was skeletonized across the diameter by Igneous but was reduced to almost a point by thinning.</p>
      <p>As can be seen in the last panel, the voxel thinning skeletons had many more terminal points. A terminal point is a vertex that has only one edge connecting it to the rest of the skeleton. In total, the thinned skeletons had 3541 terminal points while the Igneous skeletons had 1169 (or a 303% difference). The average Igneous skeleton had 3.46 terminal points (stddev 9.65), while thinned skeletons had on average 10.67 points (stddev 3.71).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>Igneous is a tool for processing large 3D images using a dependency-free distributed approach that scales to at least tens of thousands of cores operating simultaneously. While many labs may produce their data sets through one-off scripts, Igneous provides a proven, clean, efficient, scalable, and documented method for contrast correction, image pyramid construction, multi-resolution meshing, skeletonization, and data management.</p>
    <p>Igneous has been developed since 2017 and used within our lab to process a near petascale cubic millimeter of brain tissue. We have shown in practice this software processes millions of tasks and hundreds of teravoxels successfully. Our historical log shows that Igneous scales on cloud infrastructure very well, demonstrating the use of sixteen thousand cores on a thousand preemptible machines simultaneously to rapidly complete a large job. It has also been an important tool for moving and deleting copies of datasets. Large datasets often need to be moved to secure cheaper storage or to locate them closer to their next job's cluster.</p>
    <p>To provide some confidence in the output of Igneous, we compared its outputs to popular packages. For meshes, we compared Igneous to the <monospace>scikit-image</monospace> package. We found that the distribution of centroids appeared similar for both unsimplified and simplified Igneous meshes compared with <monospace>scikit-image</monospace>. Igneous's <monospace>zmesh</monospace> package was able to handle small meshes that <monospace>scikit-image</monospace> was not able to handle. We also checked the meshes and found that they were all watertight and reasonably volumetric objects, but some <monospace>scikit-image</monospace> meshes failed these tests. <monospace>zmesh</monospace> also had a smaller maximum centroid error than <monospace>scikit-image</monospace> when compared with image centroids.</p>
    <p>For skeletons, we compared Igneous to Fiji's Skeletonize3d routine, which is a voxel thinning type algorithm. It is more subjective to say one skeleton is better than another since that determination depends on what elements of a shape one is interested in. However, we can say broadly that Igneous produced simpler skeletons than Skeletonize3d and handled disconnected components. There were also fewer skeletons that appeared obviously wrong (e.g., no Igneous skeletons had a âbeehiveâ failure mode). On the other hand, the thinning algorithm preserved certain topological features that Igneous is unable to represent, such as holes, and more frequently found small extensions. However, Igneous's settings can be adjusted to find more extensions. Igenous skeletons could have been further simplified by using the short extension elimination feature.</p>
    <p>Igneous' output has already been used by neuroscientists. They have already made several scientific discoveries by both visualization and quantitative analysis (Wilson et al., <xref rid="B55" ref-type="bibr">2019</xref>; Turner et al., <xref rid="B53" ref-type="bibr">2022</xref>), with a few more uses documented in pre-prints (Schneider-Mizell et al., <xref rid="B45" ref-type="bibr">2020</xref>; Buchanan et al., <xref rid="B13" ref-type="bibr">2021</xref>). More papers are expected to be published in the future.</p>
    <p>In particular, to our knowledge, no other published connectomics tool is capable of mass producing multi-resolution meshes or skeletons economically at the scale of hundreds of millions or billions of objects (some non-public tools have been shown to scale, but their economy is unknown). Accomplishing this required optimizing both in-core operations (with <monospace>zmesh</monospace> for meshing and <monospace>kimimaro</monospace> for skeletonization) and out-of-core operations. A major advantage these in-core libraries have over other implementations is that they are natively multi-label and are able to process entire segmentation cutouts in a single pass of the algorithm. To this are added out-of-core improvements with the provision of dependency-free parallelism, reliable stitching, and resolution of issues at task boundaries. As meshes and skeletons are both produced directly from segmentation images, there exists no dependency between them and they can be produced independently of each other.</p>
    <p>It's a bit odd that quality multi-label versions of these algorithms have not previously appeared. However, this is less mysterious when it is considered that multi-label segmentation data only began appearing in bulk with the advent of the large-scale application of convolutional neural networks in the last decade. Connectomics segmentations are probably unique in both the size of the datasets and the density of labeled objects within each volume. Thus, it was previously possible to get sufficient performance out of binary versions of these algorithms.</p>
    <p>Igneous's focus on computational efficiency is important for not only visualizing the largest connectomics projects but also for making it possible for smaller labs to perform investigations that would otherwise be out of reach. Our experiments showed that the S1 dataset's segmentation can be fully processed in only 36 core-hours on a single machine. Igneous's unique ability to use <monospace>compresso</monospace> for segmentation compression makes it much easier to store and transmit segmentations. Its ability to condense datasets into a much smaller number of shard files for images, meshes, and skeletons means that many different filesystems will be able to cope with big data. The techniques utilized are general and thus they can be incorporated into other systems or allow Igneous to be extended to work with other systems.</p>
    <p>Nonetheless, there are areas that we wish to improve. Igneous does not yet support all Neuroglancer features. In particular, Igneous does not yet support annotations (though we intend to). Another problem concerns the import of raw data into Neuroglancer. As each initial dataset is often boutique in its organization and formatting, we have not yet found a standardized way to assist users in importing their raw data before Igneous can be used. However, by studying existing software, we hope to implement a method that will neatly fit into the connectomics ecosystem.</p>
    <p>While our skeletons are of mostly high quality in thin processes, in bulbous regions such as somata, they are less organized, especially if the region was not skeletonized with full context. Parallel traces can also occur along wires that run parallel to the edges of the task grid. We hope to address these issues with future refinements.</p>
    <p>This article also adds useful guidance for tuning both image and segmentation compression for datasets similar to CREMI. We found that for lossless compression, PNG was somewhat surprisingly better for image compression than the simple application of gzip or brotli though it was much slower for both encoding and decoding. For connectomics segmentation, we found that the overall best choice was <monospace>compresso+brotli</monospace> compression which to the extent of our knowledge has very low actual usage due in part to the relatively recent appearance of a practical implementation of the codec. <monospace>compresso+brotli</monospace> was slightly slower than <monospace>compressed_segmentation+brotli</monospace> at decoding but had a significantly higher compression ratio.</p>
    <p>Igneous provides a foundational tool for reliably scaling, visualizing, analyzing, and managing connectomics datasets. It provides unique publicly accessible capabilities for large-scale meshing and skeletonization and for segmentation compression. We expect it to be an indispensable tool as connectomics datasets, such as the whole mouse brain, attain exascale.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data availability statement</title>
    <p>The code for Igneous and required libraries can be found on PyPI (<ext-link xlink:href="https://pypi.org/project/igneous-pipeline/" ext-link-type="uri">https://pypi.org/project/igneous-pipeline/</ext-link>) and GitHub (<ext-link xlink:href="https://github.com/seung-lab/igneous" ext-link-type="uri">https://github.com/seung-lab/igneous</ext-link>) under GPL3+ licensure. A pre-built docker image can be found on DockerHub (<ext-link xlink:href="https://hub.docker.com/repository/docker/seunglab/igneous)" ext-link-type="uri">https://hub.docker.com/repository/docker/seunglab/igneous)</ext-link>. <monospace>zmesh</monospace> can be found at <ext-link xlink:href="https://github.com/seung-lab/zmesh" ext-link-type="uri">https://github.com/seung-lab/zmesh</ext-link>. Image data used for image compression, meshing, and skeletonization experiments can be found at <ext-link xlink:href="https://cremi.org/data/" ext-link-type="uri">https://cremi.org/data/</ext-link>. The S1 processing experiment was performed on an unpublished automatic segmentation of <ext-link xlink:href="https://neurodata.io/data/kasthuri15/" ext-link-type="uri">https://neurodata.io/data/kasthuri15/</ext-link>. The large scale skeletonization run was performed on an unpublished automatic segmentation of the âminnie65â subset of <ext-link xlink:href="https://www.microns-explorer.org/cortical-mm3" ext-link-type="uri">https://www.microns-explorer.org/cortical-mm3</ext-link>.</p>
  </sec>
  <sec id="s6">
    <title>Author contributions</title>
    <p>WS is the primary developer of Igneous and wrote the manuscript. AZ is the author of the C++ core algorithms and Cython wrapper that were further developed by WS, NK, AZ, and JW to become zmesh. WS and JB developed the skeletonization pipeline. IT and WS initiated the project in 2017. NK contributed improvements to the meshing code. JW contributed the design for task distribution <italic>via</italic> a dependency-free cloud queue. HS and JW contributed to the study design. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s7">
    <title>Funding</title>
    <p>This research was supported by the Intelligence Advanced Research Projects Activity (IARPA) <italic>via</italic> Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC0005, NIH/NIMH (U01MH114824, U01MH117072, and RF1MH117815), NIH/NINDS (U19NS104648 and R01NS104926), NIH/NEI (R01EY027036), and ARO (W911NF-12-1-0594). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>NK is employed by Zetta AI L.L.C. HS has financial interests in Zetta AI L.L.C. This study received assistance from Google, Amazon, and Intel. These companies were not involved in the study design, collection, analysis, interpretation of data, the writing of this article, or the decision to submit it for publication. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s8">
    <title>Publisher's note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec id="s9">
    <title>Author disclaimer</title>
    <p>The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.</p>
  </sec>
</body>
<back>
  <ack>
    <p>The authors are pleased to acknowledge that the work reported on in this paper was substantially performed using the Princeton Research Computing resources at Princeton University which is consortium of groups led by the Princeton Institute for Computational Science and Engineering (PICSciE) and Office of Information Technology's Research Computing. Forrest Collman and Sven Dorkenwald helped test early versions of the skeletonization pipeline and contributed helpful discussions. Sergiy Popovych articulated the need for FileQueue and collaborated on testing it. Manuel Castro wrote the original version of DracoPy and added Draco mesh compression to Igneous. We graciously thank everyone that has previously (or will have in the future) contributed code to Igneous. We are grateful for assistance from Google, Amazon, and Intel.</p>
  </ack>
  <fn-group>
    <fn id="fn0001">
      <p>
        <sup>1</sup>
        <ext-link xlink:href="https://github.com/seung-lab/igneous" ext-link-type="uri">https://github.com/seung-lab/igneous</ext-link>
      </p>
    </fn>
    <fn id="fn0002">
      <p>
        <sup>2</sup>
        <ext-link xlink:href="https://github.com/google/neuroglancer/tree/master/src/neuroglancer/datasource/precomputed" ext-link-type="uri">https://github.com/google/neuroglancer/tree/master/src/neuroglancer/datasource/precomputed</ext-link>
      </p>
    </fn>
    <fn id="fn0003">
      <p>
        <sup>3</sup>
        <ext-link xlink:href="https://github.com/aplbrain/mesh-deco" ext-link-type="uri">https://github.com/aplbrain/mesh-deco</ext-link>
      </p>
    </fn>
    <fn id="fn0004">
      <p>
        <sup>4</sup>
        <ext-link xlink:href="https://github.com/hsidky/neurogen" ext-link-type="uri">https://github.com/hsidky/neurogen</ext-link>
      </p>
    </fn>
    <fn id="fn0005">
      <p>
        <sup>5</sup>
        <ext-link xlink:href="https://github.com/janelia-cosem/multiresolution-mesh-creator" ext-link-type="uri">https://github.com/janelia-cosem/multiresolution-mesh-creator</ext-link>
      </p>
    </fn>
    <fn id="fn0006">
      <p>
        <sup>6</sup>
        <ext-link xlink:href="https://github.com/SridharJagannathan/pyroglancer" ext-link-type="uri">https://github.com/SridharJagannathan/pyroglancer</ext-link>
      </p>
    </fn>
    <fn id="fn0007">
      <p>
        <sup>7</sup>
        <ext-link xlink:href="https://github.com/janelia-flyem/NeuTu/blob/a913db28719e328875f5017019cfcac16517cd5b/neurolabi/gui/zstackskeletonizer.cpp" ext-link-type="uri">https://github.com/janelia-flyem/NeuTu/blob/a913db28719e328875f5017019cfcac16517cd5b/neurolabi/gui/zstackskeletonizer.cpp</ext-link>
      </p>
    </fn>
    <fn id="fn0008">
      <p>
        <sup>8</sup>
        <ext-link xlink:href="https://github.com/funkey/skeletopyze/" ext-link-type="uri">https://github.com/funkey/skeletopyze/</ext-link>
      </p>
    </fn>
    <fn id="fn0009">
      <p>
        <sup>9</sup>
        <ext-link xlink:href="https://github.com/seung-lab/skeletonization/" ext-link-type="uri">https://github.com/seung-lab/skeletonization/</ext-link>
      </p>
    </fn>
    <fn id="fn0010">
      <p>
        <sup>10</sup>
        <ext-link xlink:href="https://imagej.net/plugins/skeletonize3d" ext-link-type="uri">https://imagej.net/plugins/skeletonize3d</ext-link>
      </p>
    </fn>
    <fn id="fn0011">
      <p>
        <sup>11</sup>
        <ext-link xlink:href="https://github.com/T4mmi/ITKThickness3D" ext-link-type="uri">https://github.com/T4mmi/ITKThickness3D</ext-link>
      </p>
    </fn>
    <fn id="fn0012">
      <p><sup>12</sup>Neuroglancer also supports a few other formats but they are not precisely image file formats.</p>
    </fn>
    <fn id="fn0013">
      <p>
        <sup>13</sup>
        <ext-link xlink:href="https://github.com/saalfeldlab/n5" ext-link-type="uri">https://github.com/saalfeldlab/n5</ext-link>
      </p>
    </fn>
    <fn id="fn0014">
      <p>
        <sup>14</sup>
        <ext-link xlink:href="https://nifti.nimh.nih.gov/nifti-1/" ext-link-type="uri">https://nifti.nimh.nih.gov/nifti-1/</ext-link>
      </p>
    </fn>
    <fn id="fn0015">
      <p>
        <sup>15</sup>
        <ext-link xlink:href="https://github.com/saalfeldlab/render" ext-link-type="uri">https://github.com/saalfeldlab/render</ext-link>
      </p>
    </fn>
    <fn id="fn0016">
      <p>
        <sup>16</sup>
        <ext-link xlink:href="https://catmaid.readthedocs.io/en/stable/importing_data.html" ext-link-type="uri">https://catmaid.readthedocs.io/en/stable/importing_data.html</ext-link>
      </p>
    </fn>
    <fn id="fn0017">
      <p>
        <sup>17</sup>
        <ext-link xlink:href="https://github.com/janelia-flyem/dvid/wiki/Tile-Generation" ext-link-type="uri">https://github.com/janelia-flyem/dvid/wiki/Tile-Generation</ext-link>
      </p>
    </fn>
    <fn id="fn0018">
      <p>
        <sup>18</sup>
        <ext-link xlink:href="https://github.com/jhuapl-boss/ingest-client" ext-link-type="uri">https://github.com/jhuapl-boss/ingest-client</ext-link>
      </p>
    </fn>
    <fn id="fn0019">
      <p>
        <sup>19</sup>
        <ext-link xlink:href="https://github.com/scalableminds/webknossos-libs/tree/master/wkcuber" ext-link-type="uri">https://github.com/scalableminds/webknossos-libs/tree/master/wkcuber</ext-link>
      </p>
    </fn>
    <fn id="fn0020">
      <p>
        <sup>20</sup>
        <ext-link xlink:href="https://github.com/seung-lab/python-task-queue" ext-link-type="uri">https://github.com/seung-lab/python-task-queue</ext-link>
      </p>
    </fn>
    <fn id="fn0021">
      <p>
        <sup>21</sup>
        <ext-link xlink:href="https://researchcomputing.princeton.edu/systems/della" ext-link-type="uri">https://researchcomputing.princeton.edu/systems/della</ext-link>
      </p>
    </fn>
    <fn id="fn0022">
      <p>
        <sup>22</sup>
        <ext-link xlink:href="https://www.docker.com/" ext-link-type="uri">https://www.docker.com/</ext-link>
      </p>
    </fn>
    <fn id="fn0023">
      <p>
        <sup>23</sup>
        <ext-link xlink:href="https://kubernetes.io/" ext-link-type="uri">https://kubernetes.io/</ext-link>
      </p>
    </fn>
    <fn id="fn0024">
      <p>
        <sup>24</sup>
        <ext-link xlink:href="https://cloud.google.com/" ext-link-type="uri">https://cloud.google.com/</ext-link>
      </p>
    </fn>
    <fn id="fn0025">
      <p>
        <sup>25</sup>
        <ext-link xlink:href="https://github.com/seung-lab/cloud-files/" ext-link-type="uri">https://github.com/seung-lab/cloud-files/</ext-link>
      </p>
    </fn>
    <fn id="fn0026">
      <p><sup>26</sup><ext-link xlink:href="https://cloud.google.com/storage/pricing" ext-link-type="uri">https://cloud.google.com/storage/pricing</ext-link>, <ext-link xlink:href="https://aws.amazon.com/s3/pricing/" ext-link-type="uri">https://aws.amazon.com/s3/pricing/</ext-link></p>
    </fn>
    <fn id="fn0027">
      <p><sup>27</sup><ext-link xlink:href="https://azure.microsoft.com/en-us/pricing/details/storage/blobs/#pricing" ext-link-type="uri">https://azure.microsoft.com/en-us/pricing/details/storage/blobs/#pricing</ext-link>. Azure also has a Premium tier with higher storage costs and reduced transaction costs.</p>
    </fn>
    <fn id="fn0028">
      <p>
        <sup>28</sup>
        <ext-link xlink:href="https://github.com/google/neuroglancer/blob/056a3548abffc3c76c93c7a906f1603ce02b5fa3/src/neuroglancer/datasource/precomputed/sharded.md" ext-link-type="uri">https://github.com/google/neuroglancer/blob/056a3548abffc3c76c93c7a906f1603ce02b5fa3/src/neuroglancer/datasource/precomputed/sharded.md</ext-link>
      </p>
    </fn>
    <fn id="fn0029">
      <p><sup>29</sup>âAdd Sharding Supportâ <ext-link xlink:href="https://github.com/zarr-developers/zarr-python/issues/877" ext-link-type="uri">https://github.com/zarr-developers/zarr-python/issues/877</ext-link>.</p>
    </fn>
    <fn id="fn0030">
      <p>
        <sup>30</sup>
        <ext-link xlink:href="http://libpng.org/pub/png/spec/1.2/PNG-Contents.html" ext-link-type="uri">http://libpng.org/pub/png/spec/1.2/PNG-Contents.html</ext-link>
      </p>
    </fn>
    <fn id="fn0031">
      <p>
        <sup>31</sup>
        <ext-link xlink:href="https://jpeg.org/jpeg/" ext-link-type="uri">https://jpeg.org/jpeg/</ext-link>
      </p>
    </fn>
    <fn id="fn0032">
      <p><sup>32</sup>Single Instruction Multiple Data: Instruction level parallelism.</p>
    </fn>
    <fn id="fn0033">
      <p>
        <sup>33</sup>
        <ext-link xlink:href="https://github.com/dcwatson/deflate" ext-link-type="uri">https://github.com/dcwatson/deflate</ext-link>
      </p>
    </fn>
    <fn id="fn0034">
      <p>
        <sup>34</sup>
        <ext-link xlink:href="https://github.com/ebiggers/libdeflate" ext-link-type="uri">https://github.com/ebiggers/libdeflate</ext-link>
      </p>
    </fn>
    <fn id="fn0035">
      <p>
        <sup>35</sup>
        <ext-link xlink:href="https://github.com/seung-lab/pyspng-seunglab" ext-link-type="uri">https://github.com/seung-lab/pyspng-seunglab</ext-link>
      </p>
    </fn>
    <fn id="fn0036">
      <p>
        <sup>36</sup>
        <ext-link xlink:href="https://github.com/randy408/libspng" ext-link-type="uri">https://github.com/randy408/libspng</ext-link>
      </p>
    </fn>
    <fn id="fn0037">
      <p>
        <sup>37</sup>
        <ext-link xlink:href="https://github.com/nurpax/pyspng/" ext-link-type="uri">https://github.com/nurpax/pyspng/</ext-link>
      </p>
    </fn>
    <fn id="fn0038">
      <p>
        <sup>38</sup>
        <ext-link xlink:href="https://gitlab.com/jfolz/simplejpeg/" ext-link-type="uri">https://gitlab.com/jfolz/simplejpeg/</ext-link>
      </p>
    </fn>
    <fn id="fn0039">
      <p>
        <sup>39</sup>
        <ext-link xlink:href="https://libjpeg-turbo.org/" ext-link-type="uri">https://libjpeg-turbo.org/</ext-link>
      </p>
    </fn>
    <fn id="fn0040">
      <p>
        <sup>40</sup>
        <ext-link xlink:href="https://github.com/google/neuroglancer/tree/f411a53f3ebc14172eb20614da2ccf6d14cd4905/src/neuroglancer/sliceview/compressed_segmentation" ext-link-type="uri">https://github.com/google/neuroglancer/tree/f411a53f3ebc14172eb20614da2ccf6d14cd4905/src/neuroglancer/sliceview/compressed_segmentation</ext-link>
      </p>
    </fn>
    <fn id="fn0041">
      <p>
        <sup>41</sup>
        <ext-link xlink:href="https://github.com/seung-lab/compressedseg" ext-link-type="uri">https://github.com/seung-lab/compressedseg</ext-link>
      </p>
    </fn>
    <fn id="fn0042">
      <p>
        <sup>42</sup>
        <ext-link xlink:href="https://github.com/seung-lab/compresso" ext-link-type="uri">https://github.com/seung-lab/compresso</ext-link>
      </p>
    </fn>
    <fn id="fn0043">
      <p>
        <sup>43</sup>
        <ext-link xlink:href="https://github.com/google/neuroglancer/blob/056a3548abffc3c76c93c7a906f1603ce02b5fa3/src/neuroglancer/datasource/precomputed/meshes.md" ext-link-type="uri">https://github.com/google/neuroglancer/blob/056a3548abffc3c76c93c7a906f1603ce02b5fa3/src/neuroglancer/datasource/precomputed/meshes.md</ext-link>
      </p>
    </fn>
    <fn id="fn0044">
      <p>
        <sup>44</sup>
        <ext-link xlink:href="https://google.github.io/draco/" ext-link-type="uri">https://google.github.io/draco/</ext-link>
      </p>
    </fn>
    <fn id="fn0045">
      <p>
        <sup>45</sup>
        <ext-link xlink:href="https://github.com/google/neuroglancer/blob/b6adee6703db3a7fc4fa35f09a9ace8318ee128b/src/neuroglancer/datasource/precomputed/skeletons.md" ext-link-type="uri">https://github.com/google/neuroglancer/blob/b6adee6703db3a7fc4fa35f09a9ace8318ee128b/src/neuroglancer/datasource/precomputed/skeletons.md</ext-link>
      </p>
    </fn>
    <fn id="fn0046">
      <p><sup>46</sup>Mip is short for the Latin phrase âmultum in parvoâ or âmuch in littleâ.</p>
    </fn>
    <fn id="fn0047">
      <p>
        <sup>47</sup>
        <ext-link xlink:href="https://github.com/seung-lab/tinybrain/" ext-link-type="uri">https://github.com/seung-lab/tinybrain/</ext-link>
      </p>
    </fn>
    <fn id="fn0048">
      <p>
        <sup>48</sup>
        <ext-link xlink:href="https://github.com/seung-lab/zmesh" ext-link-type="uri">https://github.com/seung-lab/zmesh</ext-link>
      </p>
    </fn>
    <fn id="fn0049">
      <p>
        <sup>49</sup>
        <ext-link xlink:href="https://github.com/zlateski/zi_lib" ext-link-type="uri">https://github.com/zlateski/zi_lib</ext-link>
      </p>
    </fn>
    <fn id="fn0050">
      <p>
        <sup>50</sup>
        <ext-link xlink:href="https://github.com/seung-lab/DracoPy" ext-link-type="uri">https://github.com/seung-lab/DracoPy</ext-link>
      </p>
    </fn>
    <fn id="fn0051">
      <p>
        <sup>51</sup>
        <ext-link xlink:href="https://github.com/Kramer84/pyfqmr-Fast-Quadric-Mesh-Reduction" ext-link-type="uri">https://github.com/Kramer84/pyfqmr-Fast-Quadric-Mesh-Reduction</ext-link>
      </p>
    </fn>
    <fn id="fn0052">
      <p>
        <sup>52</sup>
        <ext-link xlink:href="https://scikit-image.org/" ext-link-type="uri">https://scikit-image.org/</ext-link>
      </p>
    </fn>
    <fn id="fn0053">
      <p>
        <sup>53</sup>
        <ext-link xlink:href="https://trimsh.org/trimesh.html" ext-link-type="uri">https://trimsh.org/trimesh.html</ext-link>
      </p>
    </fn>
    <fn id="fn0054">
      <p>
        <sup>54</sup>
        <ext-link xlink:href="https://github.com/aplbrain/saber" ext-link-type="uri">https://github.com/aplbrain/saber</ext-link>
      </p>
    </fn>
    <fn id="fn0055">
      <p>
        <sup>55</sup>
        <ext-link xlink:href="https://github.com/JaneliaSciComp/SharkViewer" ext-link-type="uri">https://github.com/JaneliaSciComp/SharkViewer</ext-link>
      </p>
    </fn>
    <fn id="fn0056">
      <p>
        <sup>56</sup>
        <ext-link xlink:href="https://github.com/saalfeldlab/paintera" ext-link-type="uri">https://github.com/saalfeldlab/paintera</ext-link>
      </p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s10">
    <title>Supplementary material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fncir.2022.977700/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fncir.2022.977700/full#supplementary-material</ext-link> We include three supplements to this text.</p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Table_1.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM2" position="float" content-type="local-data">
      <media xlink:href="Video_1.MP4" mimetype="video" mime-subtype="mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM3" position="float" content-type="local-data">
      <media xlink:href="Video_2.MP4" mimetype="video" mime-subtype="mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM4" position="float" content-type="local-data">
      <media xlink:href="Video_3.MP4" mimetype="video" mime-subtype="mp4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM5" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <list list-type="order">
      <list-item>
        <p>A three part video tutorial on getting started with Igneous.</p>
      </list-item>
      <list-item>
        <p>A tabular non-comprehensive landscape overview of selected viewers, proofreading tools, and computational pipelines.</p>
      </list-item>
      <list-item>
        <p>Supplementary figures that provide additional quantitative information comparing Igneous meshes and skeletons to scikit-image and Fiji's Skeletonize3d respectively.</p>
      </list-item>
    </list>
    <sec>
      <title> Video tutorial</title>
      <p>We show how to take sample HDF5 electron microscopy images and segmentations from <ext-link xlink:href="https://cremi.org/data/" ext-link-type="uri">https://cremi.org/data/</ext-link> and fully process them into multi-resolution, meshed, and skeletonized Neuroglancer Precomputed layers. This tutorial covers the simplest processing pathway and does not cover producing sharded versions of these data types, parameter selection, nor distributed processing.</p>
      <p>The tutorial consists of three parts:</p>
      <list list-type="order">
        <list-item>
          <p>Creating a base Neuroglancer Precomputed image and installing Igneous.</p>
        </list-item>
        <list-item>
          <p>Creating multi-resolution image layers with Igneous (âdownsamplingâ).</p>
        </list-item>
        <list-item>
          <p>Creating meshes and skeletons.</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title> Selected visualization, proofreading, and computational landscape overview</title>
      <p>To better help the reader understand Igneous' relationship to other tools, we compiled a table of selected visualization, proofreading, and computational tools and evaluated them along dimensions mentioned in the Related Work section. Though it includes many entries, this table should not be considered comprehensive.</p>
      <p>A few tools mentioned in this table were not previously mentioned in this article, so we will give their citations here: SABER<xref rid="fn0054" ref-type="fn"><sup>54</sup></xref>, SharkViewer<xref rid="fn0055" ref-type="fn"><sup>55</sup></xref>, and Paintera.<xref rid="fn0056" ref-type="fn"><sup>56</sup></xref></p>
      <p>For more information about the visualization landscape, please consult Beyer et al. (<xref rid="B9" ref-type="bibr">2013</xref>, <xref rid="B10" ref-type="bibr">2022</xref>) and Berger et al. (<xref rid="B8" ref-type="bibr">2018</xref>) which contain additional descriptions.</p>
    </sec>
    <sec>
      <title>Supplementary figures</title>
      <p>We include additional figures comparing meshes and skeleton quality to established libraries.</p>
      <list list-type="order">
        <list-item>
          <p>Ratio of mesh volume to voxel volume for zmesh meshes (both simplified and unsimplified).</p>
        </list-item>
        <list-item>
          <p>Comparison of zmesh mesh centroids to label centroids.</p>
        </list-item>
        <list-item>
          <p>Comparison of voxel thinning and Igneous skeleton properties.</p>
        </list-item>
        <list-item>
          <p>Example of a âbeehiveâ skeleton.</p>
        </list-item>
      </list>
    </sec>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>L. F.</given-names></name><name><surname>Bock</surname><given-names>D. D.</given-names></name><name><surname>Callaway</surname><given-names>E. M.</given-names></name><name><surname>Denk</surname><given-names>W.</given-names></name><name><surname>Dulac</surname><given-names>C.</given-names></name><name><surname>Fairhall</surname><given-names>A. L.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>The mind of a mouse</article-title>. <source>Cell</source><volume>182</volume>, <fpage>1372</fpage>â<lpage>1376</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2020.08.010</pub-id><?supplied-pmid 32946777?><pub-id pub-id-type="pmid">32946777</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ai-Awami</surname><given-names>A. K.</given-names></name><name><surname>Beyer</surname><given-names>J.</given-names></name><name><surname>Haehn</surname><given-names>D.</given-names></name><name><surname>Kasthuri</surname><given-names>N.</given-names></name><name><surname>Lichtman</surname><given-names>J. W.</given-names></name><name><surname>Pfister</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>NeuroBlocksâvisual tracking of segmentation and proofreading for large connectomics projects</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. <volume>22</volume>, <fpage>738</fpage>â<lpage>746</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2015.2467441</pub-id><?supplied-pmid 26529725?><pub-id pub-id-type="pmid">26529725</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alakuijala</surname><given-names>J.</given-names></name><name><surname>Szabadka</surname><given-names>Z.</given-names></name></person-group> (<year>2016</year>). <source>Brotli Compressed Data Format</source>. Internet Engineering Task Force. <pub-id pub-id-type="doi">10.17487/rfc.7932</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>J.</given-names></name><name><surname>Mohammed</surname><given-names>S.</given-names></name><name><surname>Grimm</surname><given-names>B.</given-names></name><name><surname>Jones</surname><given-names>B.</given-names></name><name><surname>Koshevoy</surname><given-names>P.</given-names></name><name><surname>Tasdizen</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>The Viking viewer for connectomics: scalable multi-user annotation and summarization of large volume data sets</article-title>. <source>J. Microsc</source>. <volume>241</volume>, <fpage>13</fpage>â<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1111/j.1365-2818.2010.03402.x</pub-id><?supplied-pmid 21118201?><pub-id pub-id-type="pmid">21118201</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balaji</surname><given-names>S. B.</given-names></name><name><surname>Krishnan</surname><given-names>M. N.</given-names></name><name><surname>Vajha</surname><given-names>M.</given-names></name><name><surname>Ramkumar</surname><given-names>V.</given-names></name><name><surname>Sasidharan</surname><given-names>B.</given-names></name><name><surname>Kumar</surname><given-names>P. V.</given-names></name></person-group> (<year>2018</year>). <article-title>Erasure coding for distributed storage: an overview</article-title>. <source>Sci. China Inform. Sci</source>. 61, 100301. <pub-id pub-id-type="doi">10.1007/s11432-018-9482-6</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behnel</surname><given-names>S.</given-names></name><name><surname>Bradshaw</surname><given-names>R.</given-names></name><name><surname>Citro</surname><given-names>C.</given-names></name><name><surname>Dalcin</surname><given-names>L.</given-names></name><name><surname>Seljebotn</surname><given-names>D. S.</given-names></name><name><surname>Smith</surname><given-names>K.</given-names></name></person-group> (<year>2011</year>). <article-title>Cython: the best of both worlds</article-title>. <source>Comput. Sci. Eng</source>. <volume>13</volume>, <fpage>31</fpage>â<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1109/MCSE.2010.118</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>S.</given-names></name><name><surname>Kutra</surname><given-names>D.</given-names></name><name><surname>Kroeger</surname><given-names>T.</given-names></name><name><surname>Straehle</surname><given-names>C. N.</given-names></name><name><surname>Kausler</surname><given-names>B. X.</given-names></name><name><surname>Haubold</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Ilastik: Interactive machine learning for (bio)image analysis</article-title>. <source>Nat. Methods</source><volume>16</volume>, <fpage>1226</fpage>â<lpage>1232</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0582-9</pub-id><?supplied-pmid 31570887?><pub-id pub-id-type="pmid">31570887</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>D. R.</given-names></name><name><surname>Seung</surname><given-names>H. S.</given-names></name><name><surname>Lichtman</surname><given-names>J. W.</given-names></name></person-group> (<year>2018</year>). <article-title>VAST (volume annotation and segmentation tool): efficient manual and semi-automatic labeling of large 3D image stacks</article-title>. <source>Front. Neural Circ</source>
<volume>12</volume>, <fpage>88</fpage>. <pub-id pub-id-type="doi">10.3389/fncir.2018.00088</pub-id><?supplied-pmid 30386216?><pub-id pub-id-type="pmid">30386216</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beyer</surname><given-names>J.</given-names></name><name><surname>Al-Awami</surname><given-names>A.</given-names></name><name><surname>Kasthuri</surname><given-names>N.</given-names></name><name><surname>Lichtman</surname><given-names>J. W.</given-names></name><name><surname>Pfister</surname><given-names>H.</given-names></name><name><surname>Hadwiger</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>). <article-title>ConnectomeExplorer: query-guided visual analysis of large volumetric neuroscience data</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. <volume>19</volume>, <fpage>2868</fpage>â<lpage>2877</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2013.142</pub-id><?supplied-pmid 24051854?><pub-id pub-id-type="pmid">24051854</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beyer</surname><given-names>J.</given-names></name><name><surname>Troidl</surname><given-names>J.</given-names></name><name><surname>Boorboor</surname><given-names>S.</given-names></name><name><surname>Hadwiger</surname><given-names>M.</given-names></name><name><surname>Kaufman</surname><given-names>A.</given-names></name><name><surname>Pfister</surname><given-names>H.</given-names></name></person-group> (<year>2022</year>). <article-title>âA survey of visualization and analysis in high-resolution connectomics,â</article-title> in <source>Computer Graphics Forum, Vol. 41</source> (<publisher-loc>Rome</publisher-loc>). <pub-id pub-id-type="doi">10.1111/cgf.14574</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bitter</surname><given-names>I.</given-names></name><name><surname>Kaufman</surname><given-names>A.</given-names></name><name><surname>Sato</surname><given-names>M.</given-names></name></person-group> (<year>2001</year>). <article-title>Penalized-distance volumetric skeleton algorithm</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. <volume>7</volume>, <fpage>195</fpage>â<lpage>206</lpage>. <pub-id pub-id-type="doi">10.1109/2945.942688</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boergens</surname><given-names>K. M.</given-names></name><name><surname>Berning</surname><given-names>M.</given-names></name><name><surname>Bocklisch</surname><given-names>T.</given-names></name><name><surname>BrÃ¤unlein</surname><given-names>D.</given-names></name><name><surname>Drawitsch</surname><given-names>F.</given-names></name><name><surname>Frohnhofen</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>webKnossos: efficient online 3D data annotation for connectomics</article-title>. <source>Nat. Methods</source><volume>14</volume>, <fpage>691</fpage>â<lpage>694</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4331</pub-id><?supplied-pmid 28604722?><pub-id pub-id-type="pmid">28604722</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buchanan</surname><given-names>J.</given-names></name><name><surname>Elabbady</surname><given-names>L.</given-names></name><name><surname>Collman</surname><given-names>F.</given-names></name><name><surname>Jorstad</surname><given-names>N. L.</given-names></name><name><surname>Bakken</surname><given-names>T. E.</given-names></name><name><surname>Ott</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Oligodendrocyte precursor cells prune axons in the mouse neocortex</article-title>. <source>bioRxiv.</source><pub-id pub-id-type="doi">10.21203/rs.3.rs-581121/v1</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardona</surname><given-names>A.</given-names></name><name><surname>Saalfeld</surname><given-names>S.</given-names></name><name><surname>Schindelin</surname><given-names>J.</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I.</given-names></name><name><surname>Preibisch</surname><given-names>S.</given-names></name><name><surname>Longair</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>TrakEM2 software for neural circuit reconstruction</article-title>. <source>PLoS ONE</source><volume>7</volume>, <fpage>e38011</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0038011</pub-id><?supplied-pmid 22723842?><pub-id pub-id-type="pmid">22723842</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorkenwald</surname><given-names>S.</given-names></name><name><surname>McKellar</surname><given-names>C. E.</given-names></name><name><surname>Macrina</surname><given-names>T.</given-names></name><name><surname>Kemnitz</surname><given-names>N.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Lu</surname><given-names>R.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>FlyWire: Online community for whole-brain connectomics</article-title>. <source>Nat. Methods</source><volume>19</volume>, <fpage>1</fpage>â<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1101/2020.08.30.274225</pub-id><?supplied-pmid 34949809?><pub-id pub-id-type="pmid">34949809</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dorkenwald</surname><given-names>S.</given-names></name><name><surname>Schneider-Mizell</surname><given-names>C.</given-names></name><name><surname>Collman</surname><given-names>F.</given-names></name></person-group> (<year>2020</year>). <source>Sdorkenw/MeshParty: V1.9.0</source>. Zenodo.</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiala</surname><given-names>J. C.</given-names></name></person-group> (<year>2005</year>). <article-title><italic>Reconstruct</italic>: a free editor for serial section microscopy</article-title>. <source>J. Microsc</source>. <volume>218</volume>, <fpage>52</fpage>â<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1111/j.1365-2818.2005.01466.x</pub-id><?supplied-pmid 17204069?><pub-id pub-id-type="pmid">15817063</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garland</surname><given-names>M.</given-names></name><name><surname>Heckbert</surname><given-names>P. S.</given-names></name></person-group> (<year>1997</year>). <article-title>âSurface simplification using quadric error metrics,â</article-title> in <source>Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques</source> - <italic>SIGGRAPH '97</italic> (Los Angelas, CA: ACM Press), <fpage>209</fpage>â<lpage>216</lpage>. <pub-id pub-id-type="doi">10.1145/258734.258849</pub-id><?supplied-pmid 29058486?><pub-id pub-id-type="pmid">29058486</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haehn</surname><given-names>D.</given-names></name><name><surname>Knowles-Barley</surname><given-names>S.</given-names></name><name><surname>Roberts</surname><given-names>M.</given-names></name><name><surname>Beyer</surname><given-names>J.</given-names></name><name><surname>Kasthuri</surname><given-names>N.</given-names></name><name><surname>Lichtman</surname><given-names>J. W.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Design and evaluation of interactive proofreading tools for connectomics</article-title>. <source>IEEE Trans. Visual. Comput. Graph</source>. 20(12):2466â2475. <pub-id pub-id-type="doi">10.1109/TVCG.2014.2346371</pub-id><?supplied-pmid 26356960?><pub-id pub-id-type="pmid">26356960</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helmstaedter</surname><given-names>M.</given-names></name><name><surname>Briggman</surname><given-names>K. L.</given-names></name><name><surname>Denk</surname><given-names>W.</given-names></name></person-group> (<year>2011</year>). <article-title>High-accuracy neurite reconstruction for high-throughput neuroanatomy</article-title>. <source>Nat. Neurosci</source>. <volume>14</volume>, <fpage>1081</fpage>â<lpage>1088</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2868</pub-id><?supplied-pmid 21743472?><pub-id pub-id-type="pmid">21743472</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Helmstaedter</surname><given-names>M.</given-names></name><name><surname>Briggman</surname><given-names>K. L.</given-names></name><name><surname>Turaga</surname><given-names>S. C.</given-names></name><name><surname>Jain</surname><given-names>V.</given-names></name><name><surname>Seung</surname><given-names>H. S.</given-names></name><name><surname>Denk</surname><given-names>W.</given-names></name></person-group> (<year>2013</year>). <article-title>Connectomic reconstruction of the inner plexiform layer in the mouse retina</article-title>. <source>Nature</source>
<volume>500</volume>, <fpage>168</fpage>â<lpage>174</lpage>. <pub-id pub-id-type="doi">10.1038/nature12346</pub-id><?supplied-pmid 23925239?><pub-id pub-id-type="pmid">23925239</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hider</surname><given-names>R.</given-names></name><name><surname>Kleissas</surname><given-names>D. M.</given-names></name><name><surname>Pryor</surname><given-names>D.</given-names></name><name><surname>Gion</surname><given-names>T.</given-names></name><name><surname>Rodriguez</surname><given-names>L.</given-names></name><name><surname>Matelsky</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>The block object storage service (bossDB): a cloud-native approach for petascale neuroscience discovery</article-title>. <source>bioRxiv.</source></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hoppe</surname><given-names>H.</given-names></name></person-group> (<year>1999</year>). <article-title>âNew quadric metric for simplifying meshes with appearance attributes,â</article-title> in <source>Proceedings Visualization '99</source> (<publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>59</fpage>â<lpage>510</lpage>. <pub-id pub-id-type="doi">10.1109/VISUAL.1999.809869</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>W. K.</given-names></name><name><surname>Beyer</surname><given-names>J.</given-names></name><name><surname>Hadwiger</surname><given-names>M.</given-names></name><name><surname>Blue</surname><given-names>R.</given-names></name><name><surname>Law</surname><given-names>C.</given-names></name><name><surname>Vazquez-Reina</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Ssecrett and NeuroTrace: interactive visualization and analysis tools for large-scale neuroscience data sets</article-title>. <source>IEEE Comput. Graph. Appl</source>. <volume>30</volume>, <fpage>58</fpage>â<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1109/MCG.2010.56</pub-id><?supplied-pmid 20650718?><pub-id pub-id-type="pmid">20650718</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kasthuri</surname><given-names>N.</given-names></name><name><surname>Hayworth</surname><given-names>K. J.</given-names></name><name><surname>Berger</surname><given-names>D. R.</given-names></name><name><surname>Schalek</surname><given-names>R. L.</given-names></name><name><surname>Conchello</surname><given-names>J. A.</given-names></name><name><surname>Knowles-Barley</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Saturated reconstruction of a volume of neocortex</article-title>. <source>Cell</source><volume>162</volume>, <fpage>648</fpage>â<lpage>661</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2015.06.054</pub-id><?supplied-pmid 26232230?><pub-id pub-id-type="pmid">26232230</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katz</surname><given-names>W. T.</given-names></name><name><surname>Plaza</surname><given-names>S. M.</given-names></name></person-group> (<year>2019</year>). <article-title>DVID: distributed versioned image-oriented dataservice</article-title>. <source>Front. Neural Circ</source>. 13, 5. <pub-id pub-id-type="doi">10.3389/fncir.2019.00005</pub-id><?supplied-pmid 30804760?><pub-id pub-id-type="pmid">30804760</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J. S.</given-names></name><name><surname>Greene</surname><given-names>M. J.</given-names></name><name><surname>Zlateski</surname><given-names>A.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Richardson</surname><given-names>M.</given-names></name><name><surname>Turaga</surname><given-names>S. C.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Space-time wiring specificity supports direction selectivity in the retina</article-title>. <source>Nature</source><volume>509</volume>, <fpage>331</fpage>â<lpage>336</lpage>. <pub-id pub-id-type="doi">10.1038/nature13240</pub-id><?supplied-pmid 24805243?><pub-id pub-id-type="pmid">24805243</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kremer</surname><given-names>J. R.</given-names></name><name><surname>Mastronarde</surname><given-names>D. N.</given-names></name><name><surname>McIntosh</surname><given-names>J. R.</given-names></name></person-group> (<year>1996</year>). <article-title>Computer visualization of three-dimensional image data using IMOD</article-title>. <source>J. Struct. Biol</source>. <volume>116</volume>, <fpage>71</fpage>â<lpage>76</lpage>. <pub-id pub-id-type="doi">10.1006/jsbi.1996.0013</pub-id><?supplied-pmid 8742726?><pub-id pub-id-type="pmid">8742726</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>T.-C.</given-names></name><name><surname>Kashyap</surname><given-names>R. L.</given-names></name></person-group> (<year>1994</year>). <article-title>âBuilding skeleton models via 3-D medial surface/axis thinning algorithms,â</article-title> in <source>CVGIP: Graphical Models and Image Processing</source> (<publisher-loc>Orlando, FL</publisher-loc>: <publisher-name>Academic Press, Inc.</publisher-name>), <fpage>462</fpage>â<lpage>478</lpage>. <pub-id pub-id-type="doi">10.1006/cgip.1994.1042</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorensen</surname><given-names>W. E.</given-names></name><name><surname>Cline</surname><given-names>H. E.</given-names></name></person-group> (<year>1987</year>). <article-title>Marching cubes: a high-resolution 3D surface construction algorithm</article-title>. <source>ACM SIGGRAPH Comput. Graph</source>. 21, 7. <pub-id pub-id-type="doi">10.1145/37402.37422</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maitin-Shepard</surname><given-names>J.</given-names></name><name><surname>Baden</surname><given-names>A.</given-names></name><name><surname>Silversmith</surname><given-names>W.</given-names></name><name><surname>Perlman</surname><given-names>E.</given-names></name><name><surname>Collman</surname><given-names>F.</given-names></name><name><surname>Blakely</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2021</year>). <source>Google/Neuroglancer</source>. Zenodo.</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matejek</surname><given-names>B.</given-names></name><name><surname>Haehn</surname><given-names>D.</given-names></name><name><surname>Lekschas</surname><given-names>F.</given-names></name><name><surname>Mitzenmacher</surname><given-names>M.</given-names></name><name><surname>Pfister</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>âCompresso: efficient compression of segmentation data for connectomics,â</article-title> in <source>Medical Image Computing and Computer Assisted Intervention</source> â <italic>MICCAI 2017</italic>, eds M. Descoteaux, L. Maier-Hein, A. Franz, P. Jannin, D. L. Collins, and S. Duchesne (Cham: Springer International Publishing), <fpage>781</fpage>â<lpage>788</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-66182-7_89</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matejek</surname><given-names>B.</given-names></name><name><surname>Wei</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Zhao</surname><given-names>J.</given-names></name><name><surname>PalÃ¡gyi</surname><given-names>K.</given-names></name><name><surname>Pfister</surname><given-names>H.</given-names></name></person-group> (<year>2019</year>). <article-title>âSynapse-aware skeleton generation for neural circuits,â</article-title> in <source>Medical Image Computing and Computer Assisted Intervention?MICCAI 2019</source>, eds D. Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P. T. Yap, and A. Khan (Cham: Springer International Publishing), <fpage>227</fpage>â<lpage>235</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-32239-7_26</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MICrONS Consortium</surname><given-names>Bae, J. A.</given-names></name><name><surname>Baptiste</surname><given-names>M.</given-names></name><name><surname>Bodor</surname><given-names>A. L.</given-names></name><name><surname>Brittain</surname><given-names>D.</given-names></name><name><surname>Buchanan</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Functional connectomics spanning multiple areas of mouse visual cortex</article-title>. <source>bioRxiv.</source><pub-id pub-id-type="doi">10.1101/2021.07.28.454025</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miles</surname><given-names>A.</given-names></name><name><surname>Bussonnier</surname><given-names>M.</given-names></name><name><surname>Moore</surname><given-names>J.</given-names></name><name><surname>Fulton</surname><given-names>A.</given-names></name><name><surname>Bourbeau</surname><given-names>J.</given-names></name><name><surname>Onalan</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2022</year>). <source>Zarr-Developers/Zarr-Python: None</source>. Zenodo.</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Ruan</surname><given-names>Z.</given-names></name><name><surname>Long</surname><given-names>F.</given-names></name><name><surname>Simpson</surname><given-names>J. H.</given-names></name><name><surname>Myers</surname><given-names>E. W.</given-names></name></person-group> (<year>2010</year>). <article-title>V3D enables real-time 3D visualization and quantitative analysis of large-scale biological image data sets</article-title>. <source>Nat. Biotechnol</source>. <volume>28</volume>, <fpage>348</fpage>â<lpage>353</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.1612</pub-id><?supplied-pmid 20231818?><pub-id pub-id-type="pmid">20231818</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pfister</surname><given-names>H.</given-names></name><name><surname>Kaynig</surname><given-names>V.</given-names></name><name><surname>Botha</surname><given-names>C. P.</given-names></name><name><surname>Bruckner</surname><given-names>S.</given-names></name><name><surname>Dercksen</surname><given-names>V. J.</given-names></name><name><surname>Hege</surname><given-names>H.-C.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>âVisualization in connectomics,â</article-title> in <source>Scientific Visualization: Uncertainty, Multifield, Biomedical, and Scalable Visualization</source>, eds C. D. Hansen, M. Chen, C. R. Johnson, A. E. Kaufman, and H. Hagen (<publisher-loc>London</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>221</fpage>â<lpage>245</lpage>. <pub-id pub-id-type="doi">10.1007/978-1-4471-6497-5_21</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pietzsch</surname><given-names>T.</given-names></name><name><surname>Saalfeld</surname><given-names>S.</given-names></name><name><surname>Preibisch</surname><given-names>S.</given-names></name><name><surname>Tomancak</surname><given-names>P.</given-names></name></person-group> (<year>2015</year>). <article-title>BigDataViewer: visualization and processing for large image data sets</article-title>. <source>Nat. Methods</source>
<volume>12</volume>, <fpage>481</fpage>â<lpage>483</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3392</pub-id><?supplied-pmid 26020499?><pub-id pub-id-type="pmid">26020499</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reilly</surname><given-names>E. P.</given-names></name><name><surname>Garretson</surname><given-names>J. S.</given-names></name><name><surname>Gray Roncal</surname><given-names>W. R.</given-names></name><name><surname>Kleissas</surname><given-names>D. M.</given-names></name><name><surname>Wester</surname><given-names>B. A.</given-names></name><name><surname>Chevillet</surname><given-names>M. A.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Neural reconstruction integrity: a metric for assessing the connectivity accuracy of reconstructed neural networks</article-title>. <source>Front. Neuroinform</source>. 12, 74. <pub-id pub-id-type="doi">10.3389/fninf.2018.00074</pub-id><?supplied-pmid 30455638?><pub-id pub-id-type="pmid">30455638</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Rose Li and Associates Inc</collab></person-group>. (<year>2021</year>). <source>Brain Connectivity Workshop Series Report</source>. Technical report, USDOE Office of Science (SC) (United States).</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saalfeld</surname><given-names>S.</given-names></name><name><surname>Cardona</surname><given-names>A.</given-names></name><name><surname>Hartenstein</surname><given-names>V.</given-names></name><name><surname>TomanÄÃ¡k</surname><given-names>P.</given-names></name></person-group> (<year>2009</year>). <article-title>CATMAID: collaborative annotation toolkit for massive amounts of image data</article-title>. <source>Bioinformatics</source>
<volume>25</volume>, <fpage>1984</fpage>â<lpage>1986</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btp266</pub-id><?supplied-pmid 19376822?><pub-id pub-id-type="pmid">19376822</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>M.</given-names></name><name><surname>Bitter</surname><given-names>I.</given-names></name><name><surname>Bender</surname><given-names>M.</given-names></name><name><surname>Kaufman</surname><given-names>A.</given-names></name><name><surname>Nakajima</surname><given-names>M.</given-names></name></person-group> (<year>2000</year>). <article-title>âTEASAR: tree-structure extraction algorithm for accurate and robust skeletons,â</article-title> in <source>Proceedings the Eighth Pacific Conference on Computer Graphics and Applications</source> (<publisher-loc>Hong Kong</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>281</fpage>â<lpage>449</lpage>. <pub-id pub-id-type="doi">10.1109/PCCGA.2000.883951</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindelin</surname><given-names>J.</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I.</given-names></name><name><surname>Frise</surname><given-names>E.</given-names></name><name><surname>Kaynig</surname><given-names>V.</given-names></name><name><surname>Longair</surname><given-names>M.</given-names></name><name><surname>Pietzsch</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>Fiji: an open-source platform for biological-image analysis</article-title>. <source>Nat. Methods</source><volume>9</volume>, <fpage>676</fpage>â<lpage>682</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id><?supplied-pmid 22743772?><pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlegel</surname><given-names>P.</given-names></name><name><surname>Kazimiers</surname><given-names>T.</given-names></name></person-group> (<year>2021</year>). <source>Schlegelp/Skeletor: Version 1.1.0</source>. Zenodo.</mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider-Mizell</surname><given-names>C. M.</given-names></name><name><surname>Bodor</surname><given-names>A. L.</given-names></name><name><surname>Collman</surname><given-names>F.</given-names></name><name><surname>Brittain</surname><given-names>D.</given-names></name><name><surname>Bleckert</surname><given-names>A. A.</given-names></name><name><surname>Dorkenwald</surname><given-names>S.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Chandelier cell anatomy and function reveal a variably distributed but common signal</article-title>. <source>bioRxiv.</source><pub-id pub-id-type="doi">10.1101/2020.03.31.018952</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapson-Coe</surname><given-names>A.</given-names></name><name><surname>Januszewski</surname><given-names>M.</given-names></name><name><surname>Berger</surname><given-names>D. R.</given-names></name><name><surname>Pope</surname><given-names>A.</given-names></name><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Blakely</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>A connectomic study of a petascale fragment of human cerebral cortex</article-title>. <source>bioRxiv.</source><pub-id pub-id-type="doi">10.1101/2021.05.29.446289</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Shearer</surname><given-names>R. W.</given-names></name></person-group> (<year>2009</year>). <source>Omni: visualizing and editing large-scale volume segmentations of neuronal tissue</source> (thesis). Massachusetts Institute of Technology, Cambridge, MA, United States.</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silversmith</surname><given-names>W.</given-names></name></person-group> (<year>2021</year>). <source>Seung-Lab/Connected-Components-3D: Zenodo Release v1</source>. Zenodo.</mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Silversmith</surname><given-names>W.</given-names></name><name><surname>Bae</surname><given-names>J. A.</given-names></name><name><surname>Li</surname><given-names>P. H.</given-names></name><name><surname>Wilson</surname><given-names>A. M.</given-names></name></person-group> (<year>2021a</year>). <publisher-loc>Seung-Lab/Kimimaro</publisher-loc>: <publisher-name>Zenodo Release v1. Zenodo</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silversmith</surname><given-names>W.</given-names></name><name><surname>Collman</surname><given-names>F.</given-names></name><name><surname>Kemnitz</surname><given-names>N.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Castro</surname><given-names>M.</given-names></name><name><surname>Falk</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2021b</year>). <source>Seung-Lab/Cloud-Volume: Zenodo Release v1</source>. Zenodo.</mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sofroniew</surname><given-names>N.</given-names></name><name><surname>Lambert</surname><given-names>T.</given-names></name><name><surname>Evans</surname><given-names>K.</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J.</given-names></name><name><surname>Bokota</surname><given-names>G.</given-names></name><name><surname>Winston</surname><given-names>P.</given-names></name><etal/></person-group>. (<year>2022</year>). <publisher-loc>Napari</publisher-loc>: <publisher-name>A Multi-Dimensional Image Viewer for Python. Zenodo</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tagliasacchi</surname><given-names>A.</given-names></name><name><surname>Delame</surname><given-names>T.</given-names></name><name><surname>Spagnuolo</surname><given-names>M.</given-names></name><name><surname>Amenta</surname><given-names>N.</given-names></name><name><surname>Telea</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>). <article-title>3D skeletons: a state-of-the-art report</article-title>. <source>Comput. Graph. Forum</source>
<volume>35</volume>, <fpage>573</fpage>â<lpage>597</lpage>. <pub-id pub-id-type="doi">10.1111/cgf.12865</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>N. L.</given-names></name><name><surname>Macrina</surname><given-names>T.</given-names></name><name><surname>Bae</surname><given-names>J. A.</given-names></name><name><surname>Yang</surname><given-names>R.</given-names></name><name><surname>Wilson</surname><given-names>A. M.</given-names></name><name><surname>Schneider-Mizell</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>Reconstruction of neocortex: organelles, compartments, cells, circuits, and activity</article-title>. <source>Cell</source>. <volume>185</volume>, <fpage>1082</fpage>â<lpage>1100</lpage>.e24. <pub-id pub-id-type="doi">10.1016/j.cell.2022.01.023</pub-id><?supplied-pmid 35216674?><pub-id pub-id-type="pmid">35216674</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wanner</surname><given-names>A. A.</given-names></name><name><surname>Genoud</surname><given-names>C.</given-names></name><name><surname>Masudi</surname><given-names>T.</given-names></name><name><surname>Siksou</surname><given-names>L.</given-names></name><name><surname>Friedrich</surname><given-names>R. W.</given-names></name></person-group> (<year>2016</year>). <article-title>Dense EM-based reconstruction of the interglomerular projectome in the zebrafish olfactory bulb</article-title>. <source>Nat. Neurosci</source>. <volume>19</volume>, <fpage>816</fpage>â<lpage>825</lpage>. <pub-id pub-id-type="doi">10.1038/nn.4290</pub-id><?supplied-pmid 27089019?><pub-id pub-id-type="pmid">27089019</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>A. M.</given-names></name><name><surname>Schalek</surname><given-names>R.</given-names></name><name><surname>Suissa-Peleg</surname><given-names>A.</given-names></name><name><surname>Jones</surname><given-names>T. R.</given-names></name><name><surname>Knowles-Barley</surname><given-names>S.</given-names></name><name><surname>Pfister</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Developmental rewiring between cerebellar climbing fibers and Purkinje cells begins with positive feedback synapse addition</article-title>. <source>Cell Rep</source>. <volume>29</volume>, <fpage>2849</fpage>â<lpage>2861</lpage>. <pub-id pub-id-type="doi">10.1016/j.celrep.2019.10.081</pub-id><?supplied-pmid 31775050?><pub-id pub-id-type="pmid">31775050</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Silversmith</surname><given-names>W. M.</given-names></name><name><surname>Lee</surname><given-names>K.</given-names></name><name><surname>Seung</surname><given-names>H. S.</given-names></name></person-group> (<year>2021</year>). <article-title>Chunkflow: hybrid cloud processing of large 3D images by convolutional nets</article-title>. <source>Nat. Methods</source>
<volume>18</volume>, <fpage>328</fpage>â<lpage>330</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01088-5</pub-id><?supplied-pmid 33750934?><pub-id pub-id-type="pmid">33750934</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J.</given-names></name><name><surname>Turner</surname><given-names>N.</given-names></name><name><surname>Bae</surname><given-names>J. A.</given-names></name><name><surname>Vishwanathan</surname><given-names>A.</given-names></name><name><surname>Seung</surname><given-names>H. S.</given-names></name></person-group> (<year>2022</year>). <article-title>RealNeuralNetworks.jl: an integrated julia package for skeletonization, morphological analysis, and synaptic connectivity analysis of terabyte-scale 3D neural segmentations</article-title>. <source>Front. Neuroinform</source>. 16, 828169. <pub-id pub-id-type="doi">10.3389/fninf.2022.828169</pub-id><?supplied-pmid 35311003?><pub-id pub-id-type="pmid">35311003</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>C. S.</given-names></name><name><surname>Januszewski</surname><given-names>M.</given-names></name><name><surname>Lu</surname><given-names>Z.</given-names></name><name><surname>Takemura</surname><given-names>S.-Y.</given-names></name><name><surname>Hayworth</surname><given-names>K. J.</given-names></name><name><surname>Huang</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>A connectome of the adult <italic>Drosophila</italic> central brain</article-title>. <source>Elife</source><volume>9</volume>:<fpage>e57443</fpage>.<pub-id pub-id-type="doi">10.7554/eLife.57443</pub-id><?supplied-pmid 32880371?><pub-id pub-id-type="pmid">32880371</pub-id></mixed-citation>
    </ref>
    <ref id="B59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>A. B.</given-names></name><name><surname>Jette</surname><given-names>M. A.</given-names></name><name><surname>Grondona</surname><given-names>M.</given-names></name></person-group> (<year>2003</year>). <article-title>âSLURM: simple linux utility for resource management,â</article-title> in <source>Job Scheduling Strategies for Parallel Processing</source>, eds D. Feitelson, L. Rudolph, and U. Schwiegelshohn (Berlin; Heidelberg: Springer), <fpage>44</fpage>â<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1007/10968987_3</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yushkevich</surname><given-names>P. A.</given-names></name><name><surname>Piven</surname><given-names>J.</given-names></name><name><surname>Hazlett</surname><given-names>H. C.</given-names></name><name><surname>Smith</surname><given-names>R. G.</given-names></name><name><surname>Ho</surname><given-names>S.</given-names></name><name><surname>Gee</surname><given-names>J. C.</given-names></name><etal/></person-group>. (<year>2006</year>). <article-title>User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability</article-title>. <source>Neuroimage</source><volume>31</volume>, <fpage>1116</fpage>â<lpage>1128</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.015</pub-id><?supplied-pmid 16545965?><pub-id pub-id-type="pmid">16545965</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Olbris</surname><given-names>D. J.</given-names></name><name><surname>Yu</surname><given-names>Y.</given-names></name><name><surname>Plaza</surname><given-names>S. M.</given-names></name></person-group> (<year>2018</year>). <article-title>NeuTu: software for collaborative, large-scale, segmentation-based connectome reconstruction</article-title>. <source>Front. Neural Circ</source>. 12, 101. <pub-id pub-id-type="doi">10.3389/fncir.2018.00101</pub-id><?supplied-pmid 30483068?><pub-id pub-id-type="pmid">30483068</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Plaza</surname><given-names>S. M.</given-names></name></person-group> (<year>2014</year>). <article-title>Automatic neuron type identification by neurite localization in the Drosophila medulla</article-title>. <source>arXiv preprint arXiv:1409.1892</source>.</mixed-citation>
    </ref>
    <ref id="B63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Z.</given-names></name><name><surname>Lauritzen</surname><given-names>J. S.</given-names></name><name><surname>Perlman</surname><given-names>E.</given-names></name><name><surname>Robinson</surname><given-names>C. G.</given-names></name><name><surname>Nichols</surname><given-names>M.</given-names></name><name><surname>Milkie</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2018</year>). A complete electron microscopy volume of the brain of adult <source>Drosophila melanogaster. Cell</source><volume>174</volume>, <fpage>730</fpage>â<lpage>743</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2018.06.019</pub-id><?supplied-pmid 30033368?><pub-id pub-id-type="pmid">30033368</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
