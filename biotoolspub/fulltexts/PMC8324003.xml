<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr J Mach Learn Res?>
<?submitter-system nihms?>
<?submitter-userid 8458682?>
<?submitter-authority eRA?>
<?submitter-login nnoble?>
<?submitter-name Noelle Noble?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101262635</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">32661</journal-id>
    <journal-id journal-id-type="nlm-ta">J Mach Learn Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Mach Learn Res</journal-id>
    <journal-title-group>
      <journal-title>Journal of machine learning research : JMLR</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1532-4435</issn>
    <issn pub-type="epub">1533-7928</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8324003</article-id>
    <article-id pub-id-type="pmid">34335111</article-id>
    <article-id pub-id-type="manuscript">nihpa1680635</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Near-optimal Individualized Treatment Recommendations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Meng</surname>
          <given-names>Haomiao</given-names>
        </name>
        <aff id="A1">Department of Mathematical Sciences, Binghamton University, State University of New York, Binghamton, NY 13902, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhao</surname>
          <given-names>Ying-Qi</given-names>
        </name>
        <aff id="A2">Public Health Sciences Division, Fred Hutchinson Cancer Research Center, Seattle, WA 98109, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fu</surname>
          <given-names>Haoda</given-names>
        </name>
        <aff id="A3">Eli Lilly and Company, Indianapolis, IN 46285, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Qiao</surname>
          <given-names>Xingye</given-names>
        </name>
        <aff id="A4">Department of Mathematical Sciences, Binghamton University, State University of New York, Binghamton, NY 13902, USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="CR1">
        <email>MENG@MATH.BINGHAMTON.EDU</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>11</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>30</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>183</elocation-id>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>License: CC-BY 4.0, see <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">The individualized treatment recommendation (ITR) is an important analytic framework for precision medicine. The goal of ITR is to assign the best treatments to patients based on their individual characteristics. From the machine learning perspective, the solution to the ITR problem can be formulated as a weighted classification problem to maximize the mean benefit from the recommended treatments given patients’ characteristics. Several ITR methods have been proposed in both the binary setting and the multicategory setting. In practice, one may prefer a more flexible recommendation that includes multiple treatment options. This motivates us to develop methods to obtain a set of near-optimal individualized treatment recommendations alternative to each other, called alternative individualized treatment recommendations (A-ITR). We propose two methods to estimate the optimal A-ITR within the outcome weighted learning (OWL) framework. Simulation studies and a real data analysis for Type 2 diabetic patients with injectable antidiabetic treatments are conducted to show the usefulness of the proposed A-ITR framework. We also show the consistency of these methods and obtain an upper bound for the risk between the theoretically optimal recommendation and the estimated one. An R package aitr has been developed, found at <ext-link xlink:href="https://github.com/menghaomiao/aitr" ext-link-type="uri">https://github.com/menghaomiao/aitr</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>individualized treatment recommendation</kwd>
      <kwd>set-valued classification</kwd>
      <kwd>angle-based classification</kwd>
      <kwd>reproducing kernel Hilbert space</kwd>
      <kwd>statistical learning theory</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P2">The individualized treatment recommendation (ITR) has drawn increasing attention in recent years. Because patients may respond differently to the same treatment (<xref rid="R17" ref-type="bibr">Lesko, 2007</xref>; <xref rid="R12" ref-type="bibr">Insel, 2009</xref>), it is desirable to individualize the treatment according to patients’ characteristics. Mathematically, an ITR is a map from such characteristics (the covariates, or features) to a treatment. The goal is to find the optimal treatment so that the average benefit that patients will receive by following such a recommendation is maximized.</p>
    <p id="P3">In the literature, many statistical approaches have been proposed for solving the ITR problem. In indirect modeling-based methods, one first builds a parametric or semiparametric model to estimate the expected outcome conditional on the covariates, then recommends the single treatment that renders the optimal outcome to the given patient (<xref rid="R22" ref-type="bibr">Robins, 2004</xref>; <xref rid="R21" ref-type="bibr">Qian and Murphy, 2011</xref>; <xref rid="R23" ref-type="bibr">Schulte et al., 2014</xref>). However, these methods require a correct model specification and an accurate estimation to perform well in practice. One may also obtain the optimal ITR directly. <xref rid="R37" ref-type="bibr">Zhao et al. (2012)</xref> proposed a classification-based method, coined as the outcome weighted learning (OWL), to estimate the optimal ITR. They transformed the ITR problem into a weighted classification problem and used support vector machine (SVM), a classification method, to solve it. Built on top of the OWL framework, there has been a rapidly growing literature on different aspects of the ITR problem. <xref rid="R35" ref-type="bibr">Zhao et al. (2014)</xref> and <xref rid="R5" ref-type="bibr">Cui et al. (2017)</xref> extended the OWL framework to accommodate survival outcomes. <xref rid="R38" ref-type="bibr">Zhou et al. (2017)</xref> and <xref rid="R18" ref-type="bibr">Liu et al. (2018)</xref> proposed residual weighted learning (RWL) and augmented outcome-weighted learning (AOL) respectively to reduce the variability of the weights in OWL to enhance its performance. <xref rid="R3" ref-type="bibr">Chen et al. (2018)</xref> proposed generalized OWL (GOWL) to solve an ITR with ordinal treatments. <xref rid="R31" ref-type="bibr">Zhang et al. (2018a)</xref> proposed angle-based approach for the multicategory case (in which there are more than two treatments available to choose from). Recently, <xref rid="R36" ref-type="bibr">Zhao et al. (2019)</xref> and <xref rid="R11" ref-type="bibr">Huang et al. (2019)</xref> considered replacing the weights in OWL with a doubly robust estimator to further improve the robustness of OWL. Methods based on other learning algorithms such as trees (<xref rid="R15" ref-type="bibr">Laber and Zhao, 2015</xref>; <xref rid="R13" ref-type="bibr">Kallus, 2016</xref>; <xref rid="R6" ref-type="bibr">Doubleday et al., 2018</xref>; <xref rid="R40" ref-type="bibr">Zhu et al., 2017</xref>) and nearest neighbors (<xref rid="R38" ref-type="bibr">Zhou and Kosorok, 2017</xref>; <xref rid="R27" ref-type="bibr">Wu et al., 2019</xref>) have also been studied. Another example of direct methods is the work by <xref rid="R30" ref-type="bibr">Zhang et al. (2012)</xref>, which searched for the ITR among a pre-specified class of decision rules that optimized a doubly robust augmented inverse probability weighted estimator of the overall population mean outcome.</p>
    <p id="P4">Despite the success of these methods in recommending a <italic toggle="yes">single</italic> “optimal” treatment to patients, a method that can suggest multiple “near-optimal” treatment options to a patient is not fully studied. Such options could be desirable when several treatments have comparable effects. <xref rid="R16" ref-type="bibr">Laber et al. (2014)</xref> and <xref rid="R19" ref-type="bibr">Lizotte and Laber (2016)</xref> proposed a set-valued dynamic treatment regime. In particular, if there are two treatments available (1 and −1), their set-valued rule may report {1}, {−1}, or {1, −1}. However, this approach is applicable only to cases with two competing outcomes. They would recommend the set {1, −1} if any treatment cannot be proven to be inferior to the other based on both outcomes. On the other hand, they used a regression-based method to estimate the optimal set-valued rule, which may suffer an inconsistency issue if the model is misspecified. <xref rid="R28" ref-type="bibr">Yuan (2015)</xref> considered a framework to allow a reject option in ITR estimation based on OWL. However, the method is restricted to the binary case with only two possible treatments.</p>
    <p id="P5">In this paper, we propose to study the ITR problem in the setting with only one clinical outcome from a new perspective. Different from the previous ITR work, it provides a set of treatment options that are near the optimality and are alternative to each other, which we called alternative individualized treatment recommendations (A-ITR). Specifically, if multiple treatments are expected to result in similar and near-optimal clinical outcomes for the patient, then they are all recommended to the patient (or the physician), who can choose any one of them to use after incorporating other considerations. There are several reasons this approach may be more desirable than a single treatment recommendation. First, since multiple treatments may yield the same or similar outcomes for some patients, the ranking among the near-optimal treatment options may vary randomly. If only the top treatment is reported, such a seemingly “random” recommendation may severely undermine the trust of the physicians and the patients toward the treatment recommendation system. Second, when the expected outcomes for multiple treatments are indistinguishable, it may be both legally and morally inappropriate to withhold such important information from the patients. Third, A-ITRs allow physicians and patients to incorporate other factors into their decision-making process regarding the treatment. These other factors include the healthcare expense, the painfulness of the treatment, the side-effect of the treatment, the life quality and lifestyle, and so on. For example, when two treatments are expected to have similar outcomes, it is reasonable to choose an option that is covered by the insurance, that is less painful, that has less side-effect, and that does not significantly compromise the quality of life. In this sense, conventional ITR methods that only recommend one treatment may prevent patients from making informed decisions about their lives.</p>
    <p id="P6">We will propose two methods to estimate A-ITR. Parallel to the development of the conventional ITR methods, we first introduce a regression-based plug-in method to estimate the optimal A-ITR, which will serve as the baseline. Within the OWL framework, we propose two classification-based methods. The technical tool we will use is multicategory classification with reject and refine options (<xref rid="R34" ref-type="bibr">Zhang et al., 2018b</xref>).</p>
    <p id="P7">The rest of the paper is organized as follows. In <xref rid="S2" ref-type="sec">Section 2</xref>, we review the background of the ITR and the classification with reject and refine options problems. We then introduce the proposed A-ITR framework and discuss several estimation methods in <xref rid="S5" ref-type="sec">Section 3</xref>. Detailed algorithm and the tuning procedure can be found in <xref rid="S10" ref-type="sec">Section 4</xref>. We demonstrate the empirical performance through simulation studies in <xref rid="S13" ref-type="sec">Section 5</xref> and a real-world application using Type 2 diabetes mellitus data in <xref rid="S16" ref-type="sec">Section 6</xref>. Theoretical studies of the proposed method can be found in <xref rid="S17" ref-type="sec">Section 7</xref>. Some concluding remarks are given in <xref rid="S20" ref-type="sec">Section 8</xref>. All technical proofs are provided in the supplementary materials.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Background</title>
    <p id="P8">In this section, we briefly review the background information of both the ITR problem and the problem of classification with reject and refine options.</p>
    <sec id="S3">
      <label>2.1</label>
      <title>Individualized Treatment Recommendation</title>
      <p id="P9">Denote the covariates of a patient by <inline-formula><mml:math id="M34" display="inline"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula>. Each treatment is denoted by a random variable <italic toggle="yes">A</italic>, where <inline-formula><mml:math id="M35" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">A</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> (<italic toggle="yes">k</italic> treatments available.) After assigning treatment to a patient, we observe an outcome <inline-formula><mml:math id="M36" display="inline"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>. Here we assume <italic toggle="yes">Y</italic> is bounded. Unlike many other ITR methods, we assume <italic toggle="yes">smaller Y</italic> is preferred due to a small technicality that can allow some computational savings. An individualized treatment recommendation, previously often referred to as an individualized treatment rule, is a map <inline-formula><mml:math id="M37" display="inline"><mml:mrow><mml:mi>d</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      <p id="P10">Let <italic toggle="yes">Y</italic>*(<italic toggle="yes">j</italic>) denote the potential outcome that would have been observed when treatment <italic toggle="yes">j</italic> is assigned to the patient with covariates <italic toggle="yes"><bold>X</bold></italic>. The actual observed outcome <italic toggle="yes">Y</italic> is related to the potential outcomes by <inline-formula><mml:math id="M38" display="inline"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. Define <italic toggle="yes">p</italic>(<italic toggle="yes">A</italic> = <italic toggle="yes">j</italic> ∣ <italic toggle="yes"><bold>X</bold></italic>) as the conditional probability of treatment <italic toggle="yes">j</italic> given <italic toggle="yes"><bold>X</bold></italic>. We assume the following assumption.</p>
      <p id="P11"><bold>Assumption 1</bold><italic toggle="yes">For any j, Y</italic>*(<italic toggle="yes">j</italic>) <italic toggle="yes">is independent of A given</italic><italic toggle="yes"><bold>X</bold></italic><italic toggle="yes">and p</italic>(<italic toggle="yes">A</italic> = <italic toggle="yes">j</italic> ∣ <italic toggle="yes"><bold>X</bold></italic>) &gt; 0 <italic toggle="yes">almost everywhere</italic>.</p>
      <p id="P12">Under Assumption 1, it was shown by <xref rid="R21" ref-type="bibr">Qian and Murphy (2011)</xref> and <xref rid="R13" ref-type="bibr">Kallus (2016)</xref> that the expected outcome under ITR <italic toggle="yes">d</italic> is
<disp-formula id="FD1"><label>(1)</label><mml:math id="M1" display="block"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>Y</mml:mi></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="M39" display="inline"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the expectation under ITR <italic toggle="yes">d</italic>. Note that <italic toggle="yes">p</italic>(<italic toggle="yes">A</italic>∣<italic toggle="yes"><bold>X</bold></italic>) is usually known in a randomized trial, while in an observational study it is unknown and needs to be estimated.</p>
      <p id="P13">Denote <inline-formula><mml:math id="M40" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, for <italic toggle="yes">j</italic> = 1,…, <italic toggle="yes">k</italic>. Then the optimal ITR <italic toggle="yes">d</italic>* is
<disp-formula id="FD2"><label>(2)</label><mml:math id="M2" display="block"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mi>d</mml:mi></mml:munder><mml:msup><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
that is, the optimal treatment for a patient has the smallest (the best) expected outcome.</p>
      <p id="P14">Many methods have been proposed for estimating the optimal ITR. One method is often called “regression and comparison” or Q-learning (<xref rid="R22" ref-type="bibr">Robins, 2004</xref>; <xref rid="R21" ref-type="bibr">Qian and Murphy, 2011</xref>). One first estimates the conditional mean <inline-formula><mml:math id="M41" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for each treatment <italic toggle="yes">j</italic>, then the optimal treatment is obtained by plugging the estimators in <xref rid="FD2" ref-type="disp-formula">(2)</xref>. However, this method relies on the accuracy of the regression model. If the model is mis-specified, the error could be fairly substantial. Another group of methods treat the problem as a classification problem. One example is called outcome weighted learning (OWL) or O-learning (<xref rid="R37" ref-type="bibr">Zhao et al., 2012</xref>, <xref rid="R35" ref-type="bibr">2014</xref>, <xref rid="R36" ref-type="bibr">2019</xref>; <xref rid="R38" ref-type="bibr">Zhou et al., 2017</xref>; <xref rid="R31" ref-type="bibr">Zhang et al., 2018a</xref>). In the OWL framework, by noting <xref rid="FD1" ref-type="disp-formula">(1)</xref>, we rewrite the ITR solution as
<disp-formula id="FD3"><label>(3)</label><mml:math id="M3" display="block"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mi>d</mml:mi></mml:munder><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
which is closely related to a weighted classification problem with weight <italic toggle="yes">Y</italic>/<italic toggle="yes">p</italic>(<italic toggle="yes">A</italic>∣<italic toggle="yes"><bold>X</bold></italic>). To overcome the non-continuity and non-convexity of the 0-1 loss, we can replace <inline-formula><mml:math id="M42" display="inline"><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> by a convex surrogate loss <italic toggle="yes">L</italic>(<italic toggle="yes">A</italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>X</bold></italic>)) in the empirical counterpart of <xref rid="FD3" ref-type="disp-formula">(3)</xref> and solve instead
<disp-formula id="FD4"><label>(4)</label><mml:math id="M4" display="block"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mi mathvariant="bold-italic">f</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="M43" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the empirical expectation, and <italic toggle="yes"><bold>f</bold></italic> is a vector-valued function defined on <inline-formula><mml:math id="M44" display="inline"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>. The estimated ITR <inline-formula><mml:math id="M45" display="inline"><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> is then obtained from <inline-formula><mml:math id="M46" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>.</p>
      <p id="P15">The relationship between <inline-formula><mml:math id="M47" display="inline"><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="M48" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> depends on the choice of the loss function <italic toggle="yes">L</italic> and how <italic toggle="yes"><bold>f</bold></italic> is defined. <xref rid="R37" ref-type="bibr">Zhao et al. (2012)</xref> proposed to replace the 0-1 loss by hinge loss in the binary case (<italic toggle="yes">k</italic> = 2, <italic toggle="yes">A</italic> ∈ {1, −1}), that is, <italic toggle="yes">L</italic>(<italic toggle="yes">A</italic>, <italic toggle="yes">f</italic>) = (1 − <italic toggle="yes">Af</italic>)<sub>+</sub>, where <italic toggle="yes">x</italic><sub>+</sub> = max(<italic toggle="yes">x</italic>, 0), and <italic toggle="yes">f</italic> is a scalar-valued function. In the current setting that a smaller <italic toggle="yes">Y</italic> is preferred, they could have used <italic toggle="yes">L</italic>(<italic toggle="yes">A</italic>, <italic toggle="yes">f</italic>) = (1 + <italic toggle="yes">Af</italic>)<sub>+</sub>. They showed that the optimal ITR can be estimated by <inline-formula><mml:math id="M49" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mtext>sign</mml:mtext><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <xref rid="R31" ref-type="bibr">Zhang et al. (2018a)</xref> then extended to the multicategory case using a large-margin loss under the angle-based learning framework (<xref rid="R32" ref-type="bibr">Zhang and Liu, 2014</xref>). Specifically, define <inline-formula><mml:math id="M50" display="inline"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <italic toggle="yes"><bold>W</bold></italic><sub>1</sub>, …, <italic toggle="yes"><bold>W</bold><sub>k</sub></italic> are vertices of a (<italic toggle="yes">k</italic> − 1)-dimensional simplex with equal pair-wise distances, defined as
<disp-formula id="FD5"><mml:math id="M5" display="block"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mn>1</mml:mn></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mstyle mathvariant="bold"><mml:mn>1</mml:mn></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">∕</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mn>2</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mphantom><mml:mo stretchy="true">}</mml:mo></mml:mphantom></mml:mrow></mml:math></disp-formula>
where <bold>1</bold><sub><italic toggle="yes">k</italic>−1</sub> is a (<italic toggle="yes">k</italic>−1)-dimensional vector with all 1 and <inline-formula><mml:math id="M51" display="inline"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a vector with the (<italic toggle="yes">j</italic>−1)th element 1 and 0 elsewhere. They let <italic toggle="yes">L</italic>(<italic toggle="yes">A</italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)) = <italic toggle="yes">ℓ</italic>(⟨<italic toggle="yes"><bold>W</bold><sub>A</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)⟩), where <italic toggle="yes">ℓ</italic> is a typical large-margin surrogate loss for binary classification (except that it is increasing instead of decreasing), and ⟨<italic toggle="yes"><bold>W</bold><sub>A</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)⟩ denotes the inner product of the vectors <italic toggle="yes"><bold>W</bold><sub>A</sub></italic> and <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>). From the geometry point of view, treatment <italic toggle="yes">j</italic> is represented by vertex <italic toggle="yes">j</italic> of the simplex, and the angle between <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>) and <italic toggle="yes"><bold>W</bold><sub>j</sub></italic>, ∠(<italic toggle="yes"><bold>W</bold><sub>j</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)), indicates how far away <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>) is from each of these treatments. The resulting ITR was estimated by <inline-formula><mml:math id="M52" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmin</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mi>∠</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo></mml:mrow></mml:math></inline-formula>, that is, the best treatment is the one whose corresponding vertex is closest to <inline-formula><mml:math id="M53" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in terms of the angle.</p>
      <p id="P16"><bold>Remark 2</bold><italic toggle="yes">In the ITR literature, one typically assumes that larger values of the outcome Y are preferred, so that instead of minimization, d* maximizes the objective <xref rid="FD3" ref-type="disp-formula">(3)</xref>, or equivalently</italic>, <inline-formula><mml:math id="M54" display="inline"><mml:mrow><mml:msub><mml:mtext>argmin</mml:mtext><mml:mi>d</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>≠</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <italic toggle="yes">which was indeed a weighted classification problem. In this article, recall that we assume smaller values of Y are preferred. As a consequence,</italic><inline-formula><mml:math id="M55" display="inline"><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>≠</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula><italic toggle="yes">is replaced by</italic><inline-formula><mml:math id="M56" display="inline"><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula><italic toggle="yes">in</italic><xref rid="FD3" ref-type="disp-formula">(3)</xref><italic toggle="yes">; additionally, the surrogate loss function is flipped with respect to the origin so that it is an increasing function instead of a decreasing function</italic>.</p>
    </sec>
    <sec id="S4">
      <label>2.2</label>
      <title>Classification with Reject and Refine Options</title>
      <p id="P17">We aim to provide set-valued recommendations that are near the optimality and are alternative to each other. To this end, we borrow the idea of multicategory classification with reject and refine options as a technical tool. Classification with a reject option has been widely studied. <xref rid="R9" ref-type="bibr">Herbei and Wegkamp (2006)</xref> formulated the problem as a minimization problem under the 0-<italic toggle="yes">d</italic>-1 loss. That is, the loss of a misclassified instance is 1 and the loss of a rejected instance is <italic toggle="yes">d</italic>, where 0 ≤ <italic toggle="yes">d</italic> ≤ 1/2. <xref rid="R1" ref-type="bibr">Bartlett and Wegkamp (2008)</xref> proposed an estimation procedure under the hinge loss. <xref rid="R29" ref-type="bibr">Yuan and Wegkamp (2010)</xref> extended this framework to a broad class of surrogate loss functions. <xref rid="R34" ref-type="bibr">Zhang et al. (2018b)</xref> generalized it to the multicategory case.</p>
      <p id="P18">We first introduce binary classification with reject option. Let (<italic toggle="yes"><bold>X</bold></italic>, <italic toggle="yes">A</italic>) be a pair of random variable with <inline-formula><mml:math id="M57" display="inline"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula> and class label <italic toggle="yes">A</italic> ∈ {1, −1}<xref rid="FN1" ref-type="fn">*</xref>, and denote <italic toggle="yes">p<sub>j</sub></italic>(<italic toggle="yes"><bold>x</bold></italic>) = <italic toggle="yes">p</italic>(<italic toggle="yes">A</italic> = <italic toggle="yes">j</italic> ∣ <italic toggle="yes"><bold>X</bold></italic> = <italic toggle="yes"><bold>x</bold></italic>) as the conditional class probability given <italic toggle="yes"><bold>X</bold></italic>. The goal is to train a classifier <italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>x</bold></italic>) that produces three possible outputs: 1, −1, and 0. Here 0 stands for a “reject” option, meaning that the classifier refuses to make a prediction based on the information available. Note that the decision “0” can be viewed as a set-valued decision of {1, −1}. <xref rid="R4" ref-type="bibr">Chow (1970)</xref> proposed the 0-<italic toggle="yes">d</italic><sub>0</sub>-1 loss with corresponding risk function <italic toggle="yes">P</italic>(<italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>X</bold></italic>) ≠ <italic toggle="yes">A</italic>, <italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>X</bold></italic>) ≠ 0) + <italic toggle="yes">d</italic><sub>0</sub><italic toggle="yes">P</italic>(<italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>X</bold></italic>) = 0) and it was shown that the Bayes rule under this risk is
<disp-formula id="FD6"><mml:math id="M6" display="block"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="true">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mphantom><mml:mo stretchy="true">}</mml:mo></mml:mphantom></mml:mrow></mml:math></disp-formula></p>
      <p id="P19">Here <italic toggle="yes">d</italic><sub>0</sub> ∈ [0, 1/2] controls the cost for refusing to make a classification. Intuitively, we produce the reject option “0” only when both <italic toggle="yes">p</italic><sub>1</sub>(<italic toggle="yes"><bold>x</bold></italic>) and <italic toggle="yes">p</italic><sub>2</sub>(<italic toggle="yes"><bold>x</bold></italic>) are close to 1/2. <xref rid="R1" ref-type="bibr">Bartlett and Wegkamp (2008)</xref> proposed a bent hinge loss to estimate the optimal rule <italic toggle="yes">ϕ</italic>*. The bent hinge loss is defined as <italic toggle="yes">ℓ</italic>(<italic toggle="yes">u</italic>) = max(0, 1 − <italic toggle="yes">u</italic>, 1 − (1 − <italic toggle="yes">d</italic><sub>0</sub>)<italic toggle="yes">u</italic>/<italic toggle="yes">d</italic><sub>0</sub>), i.e., the common hinge loss with a bent slope at 0. The effect of such bent slope is to shrink <italic toggle="yes">f</italic>(<italic toggle="yes"><bold>x</bold></italic>) to 0 when <italic toggle="yes">p</italic><sub>1</sub>(<italic toggle="yes"><bold>x</bold></italic>) and <italic toggle="yes">p</italic><sub>2</sub>(<italic toggle="yes"><bold>x</bold></italic>) are close. For <inline-formula><mml:math id="M58" display="inline"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we have <italic toggle="yes">ϕ</italic>* = sign(<italic toggle="yes">f</italic>*).</p>
      <p id="P20">The situation is much more complicated for multicategory classification. Suppose there are 3 classes, that is, <inline-formula><mml:math id="M59" display="inline"><mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, then the possible values for the classifier <italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>x</bold></italic>) are {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, and {1, 2, 3}. In general, assuming there are <italic toggle="yes">k</italic> classes, <italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>x</bold></italic>) can be any element in the power set of {1,…, <italic toggle="yes">k</italic>} (except the empty set). In addition to the reject option, which can be understood as the full set {1,…, <italic toggle="yes">k</italic>}, <xref rid="R34" ref-type="bibr">Zhang et al. (2018b)</xref> introduced the so-called refine option, which is a set-valued decision with cardinality strictly greater than 1 and less than <italic toggle="yes">k</italic>. It contains all those class labels which are nearly as plausible as the most plausible class. <xref rid="R34" ref-type="bibr">Zhang et al. (2018b)</xref> proposed to use a class of loss functions in conjunction with the angle-based learning framework (<xref rid="R32" ref-type="bibr">Zhang and Liu, 2014</xref>) to train a set-valued classifier that can render these different options. We note that both the reject option and the refine option are set-valued decisions, and they are analogous to the set-valued recommendations in this work.</p>
    </sec>
  </sec>
  <sec id="S5">
    <label>3.</label>
    <title>Methodology</title>
    <p id="P21">In this section, we introduce the framework of alternative individualized treatment recommendations (A-ITR) and propose two methods to estimate the optimal A-ITR.</p>
    <sec id="S6">
      <label>3.1</label>
      <title>A-ITR Framework</title>
      <p id="P22">There are several situations in which ITRs with additional alternative options are desirable. Even with small errors, when several treatments are near the optimality, the ranking of these treatments based on their estimated outcomes may differ from their true ranking. In this case, reporting only one treatment based on the estimated value is problematic. Secondly, when the error in the learning problem is substantially large, the so-called optimal treatment reported by conventional ITRs may lead to an outcome that is much worse than some of the other treatment options. In these situations, recommending a single treatment only adds to the distrust that patients may already have towards such black-box algorithms that they know little about. On the other hand, A-ITR provides a safety net, preventing from committing to a single treatment that is only one out of multiple treatments with similar or indistinguishable outcomes. Morally, as patients are more mindful about their financial responsibility and their quality of life, it is more appropriate to present these alternative options and have the patients themselves to make an informed decision, especially when many of these decisions are life-changing.</p>
      <p id="P23">An A-ITR is a set-valued map <inline-formula><mml:math id="M60" display="inline"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi mathvariant="script">A</mml:mi></mml:msup><mml:mo>∖</mml:mo><mml:mi>∅</mml:mi></mml:mrow></mml:math></inline-formula>. Inspired by the idea of classification with reject and refine options, given a user-predefined number <italic toggle="yes">c</italic> ≥ 1 that defines the scope of the near-optimal treatments, we formally define the optimal A-ITR as,
<disp-formula id="FD7"><label>(5)</label><mml:math id="M7" display="block"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">μ</italic><sub>(1)</sub> is the smallest conditional mean outcomes. This optimal A-ITR set contains all the treatment options with <italic toggle="yes">μ<sub>j</sub></italic> close enough to that of the optimal one, up to a multiplicative constant <italic toggle="yes">c</italic>. Recall that we assume smaller <italic toggle="yes">Y</italic> is preferred. Note that for certain <italic toggle="yes">c</italic> and <italic toggle="yes"><bold>x</bold></italic>, <italic toggle="yes">ϕ</italic>*(<italic toggle="yes"><bold>x</bold></italic>) may contain only one element, that is, the treatment with the smallest mean outcome, which corresponds to the conventional ITR. If it includes all the treatments, it is a non-informative recommendation, analogous to the reject option in set-valued classification problem. Here we call <italic toggle="yes">c</italic> the near-optimality constant.</p>
      <p id="P24">When <italic toggle="yes">c</italic> = 1, the optimal A-ITR reduces to the optimal ITR defined in <xref rid="FD2" ref-type="disp-formula">(2)</xref> for all <italic toggle="yes"><bold>x</bold></italic> since <italic toggle="yes">ϕ</italic>* = {<italic toggle="yes">j</italic> ∣ <italic toggle="yes">μ<sub>j</sub></italic>/<italic toggle="yes">μ</italic><sub>(1)</sub> ≤ 1} = {<italic toggle="yes">j</italic> ∣ <italic toggle="yes">μ<sub>j</sub></italic> = <italic toggle="yes">μ</italic><sub>(1)</sub>} = argmin<sub><italic toggle="yes">j</italic>∈{1,…,<italic toggle="yes">k</italic>}</sub>
<italic toggle="yes">μ<sub>j</sub></italic>. This means that the proposed optimal A-ITR generalizes the conventional optimal ITR.</p>
      <p id="P25"><bold>Remark 3</bold><italic toggle="yes">The choice of the near-optimality constant c is made in consultation with the physicians by taking into account meaningful domain knowledge in the clinical context. Note c is not a tuning parameter and is not meant to be selected in a way to minimize some risk or obtain some optimal model (whatever it means). The value of c reflects the physician’s judgment about how close is “indistinguishable” and may vary a lot depending on the application. One value of c (say 10) may be appropriate for one clinical outcome but can be too big for another. In practice, c is chosen according to the physician’s experience, or by empirical data if available. A possible candidate for c is an estimate to</italic> exp[<italic toggle="yes">SD</italic>(log(<italic toggle="yes">Y</italic>*) ∣ <italic toggle="yes"><bold>x</bold></italic>)]. <italic toggle="yes">Moreover, the physician may also try two or three different c values which may lead to recommendations with varying cardinalities. These recommendations may then be presented to the patient in the order of increasing cardinality. Caution should be exercised when communicating about these new alternative options and the corresponding possible sacrifice to the outcome</italic>.</p>
      <p id="P26"><bold>Remark 4</bold><italic toggle="yes">The optimal A-ITR ϕ* defined in</italic><xref rid="FD7" ref-type="disp-formula">(5)</xref><italic toggle="yes">is not invariant to addition but is invariant to multiplication. If it were invariant to addition, then the assumption of non-negativity of Y would be moot. In practice, many clinical outcome are positive (i.e., year of survival, blood count, etc.). A negative clinical outcome</italic><inline-formula><mml:math id="M61" display="inline"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula><italic toggle="yes">(i.e., a decrease of blood pressure) may be transformed to be positive, for example, by</italic><inline-formula><mml:math id="M62" display="inline"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <italic toggle="yes">In any case, a transformation of the original clinical outcome may be needed to adapt the definition</italic><xref rid="FD7" ref-type="disp-formula">(5)</xref><italic toggle="yes">to the specific clinical context. For example, in certain clinical contexts, it could make more sense to define the optimal treatment options as those with expected outcomes less than</italic><inline-formula><mml:math id="M63" display="inline"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula><italic toggle="yes">where</italic><inline-formula><mml:math id="M64" display="inline"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula><italic toggle="yes">is the smallest mean outcome (in the original, untransformed scale). In this case, we may define</italic><inline-formula><mml:math id="M65" display="inline"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula><italic toggle="yes">so that the optimal treatment options are those with expected (transformed) outcome less than</italic><italic toggle="yes">μ</italic><sub>(1)</sub> × <italic toggle="yes">c with c</italic> = exp(<italic toggle="yes">b</italic>). <italic toggle="yes">See</italic>
<xref rid="S16" ref-type="sec">Section 6</xref>
<italic toggle="yes">for a real data example in which some data transformation is done</italic>.</p>
    </sec>
    <sec id="S7">
      <label>3.2</label>
      <title>Estimation</title>
      <p id="P27">We consider two types of methods to estimate the optimal A-ITR: the regression-based methods and the classification-based methods. For regression-based methods, we can use Q-learning to first estimate the conditional mean <inline-formula><mml:math id="M66" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for each treatment <italic toggle="yes">j</italic>, then plug into <xref rid="FD7" ref-type="disp-formula">(5)</xref>, i.e., <inline-formula><mml:math id="M67" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. The success of this regression-based plug-in method relies on accurate estimation of <italic toggle="yes">μ<sub>j</sub></italic>.</p>
      <p id="P28">In contrast, the classification-based method targets on estimating the true boundary between different decision regions, bypassing the need to estimate <italic toggle="yes">μ<sub>j</sub></italic> directly. In the rest of the section, we propose two classification-based methods within the OWL framework, both of which are based on the angle-based learning approach (<xref rid="R32" ref-type="bibr">Zhang and Liu, 2014</xref>).</p>
      <p id="P29"><xref rid="R31" ref-type="bibr">Zhang et al. (2018a)</xref> first made use of the angle-based learning approach to solve the ITR problem, in which they denoted <italic toggle="yes"><bold>W</bold></italic><sub>1</sub> …, <italic toggle="yes"><bold>W</bold><sub>k</sub></italic> as the vertices of a (k − 1)-dimensional simplex and they chose the loss <italic toggle="yes">L</italic>(<italic toggle="yes">A</italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)) in <xref rid="FD4" ref-type="disp-formula">(4)</xref> to be a function that only depends on the inner product ⟨<italic toggle="yes"><bold>W</bold><sub>A</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>), namely, <italic toggle="yes">L</italic>(<italic toggle="yes">A</italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)) = <italic toggle="yes">ℓ</italic>(⟨<italic toggle="yes"><bold>W</bold><sub>A</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)⟩). Define <italic toggle="yes"><bold>f</bold></italic>* to be the population minimizer under such loss, that is
<disp-formula id="FD8"><label>(6)</label><mml:math id="M8" display="block"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:munder><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P30">The end product of <xref rid="R31" ref-type="bibr">Zhang et al. (2018a)</xref> was a single-treatment ITR. In the ideal case that <italic toggle="yes"><bold>f</bold></italic>* can be obtained, their ITR was defined as <italic toggle="yes">d</italic><sub><italic toggle="yes"><bold>f</bold></italic>*</sub>(<italic toggle="yes"><bold>x</bold></italic>) = argmax<sub><italic toggle="yes">j</italic>∈{1,…,<italic toggle="yes">k</italic>}</sub>,⟨<italic toggle="yes"><bold>W</bold><sub>j</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>*(<italic toggle="yes"><bold>x</bold></italic>)⟩, and it can be shown that as long as <italic toggle="yes">ℓ</italic> is convex and strictly increasing, Fisher consistency holds, i.e., <italic toggle="yes">d</italic><italic toggle="yes"><bold><sub>f*</sub></bold></italic> = <italic toggle="yes">d</italic>*. In practice, given the training data set <inline-formula><mml:math id="M68" display="inline"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo stretchy="false">}</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M69" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>, the estimate of <italic toggle="yes"><bold>f</bold></italic>*, is obtained by,
<disp-formula id="FD9"><label>(7)</label><mml:math id="M9" display="block"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mtext>subject to</mml:mtext><mml:mspace width="thinmathspace"/><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="M70" display="inline"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo>⊆</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> is a class of functions, and <italic toggle="yes">J</italic>(<italic toggle="yes"><bold>f</bold></italic>) is a penalty term to prevent overfitting.</p>
      <p id="P31">Both our proposed A-ITR methods are derived from the empirical minimizer <inline-formula><mml:math id="M71" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> in <xref rid="FD9" ref-type="disp-formula">(7)</xref> with an aim to estimate the population minimizer <italic toggle="yes"><bold>f</bold></italic>* in <xref rid="FD8" ref-type="disp-formula">(6)</xref>. The difference lies in the loss function <italic toggle="yes">ℓ</italic> they use, and how they convert <inline-formula><mml:math id="M72" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> or <italic toggle="yes"><bold>f</bold></italic>* to the final set-valued recommendations.</p>
      <sec id="S8">
        <label>3.2.1</label>
        <title>Two-step OWL METHOD</title>
        <p id="P32">For the two-step method, we use a convex, differentiable, and increasing loss function <italic toggle="yes">ℓ<sub>D</sub></italic>. Given any <italic toggle="yes"><bold>f</bold></italic> (which may be <italic toggle="yes"><bold>f</bold></italic>* or <inline-formula><mml:math id="M73" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>), to obtain the A-ITR, it is instrumental to first order the vertices <italic toggle="yes"><bold>W</bold><sub>j</sub></italic>’s, <italic toggle="yes">j</italic> = 1,…, <italic toggle="yes">k</italic>, which represent the <italic toggle="yes">k</italic> treatments, in the manner of <italic toggle="yes">reversed</italic> order statistics, ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(1)</sub>, <italic toggle="yes"><bold>f</bold></italic>⟩ &gt; ⋯ &gt; ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(<italic toggle="yes">k</italic>)</sub>, <italic toggle="yes"><bold>f</bold></italic>⟩. It turns out (see Proposition 5 below) that when <italic toggle="yes"><bold>f</bold></italic> = <italic toggle="yes"><bold>f</bold></italic>*, the <italic toggle="yes">j</italic>th <italic toggle="yes">reversed</italic> order statistic (i.e., the <italic toggle="yes">j</italic>th largest) ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(<italic toggle="yes">j</italic>)</sub>, <italic toggle="yes"><bold>f</bold></italic>⟩ corresponds to the <italic toggle="yes">j</italic>th order statistic (the <italic toggle="yes">j</italic>th smallest) <italic toggle="yes">μ</italic><sub>(<italic toggle="yes">j</italic>)</sub> where <italic toggle="yes">μ</italic><sub>(1)</sub> &lt; ⋯ &lt; <italic toggle="yes">μ</italic><sub>(<italic toggle="yes">k</italic>)</sub>.</p>
        <p id="P33">The resultant two-step estimator of the optimal A-ITR is then defined as
<disp-formula id="FD10"><label>(8)</label><mml:math id="M10" display="block"><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>≤</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="P34">Here <inline-formula><mml:math id="M74" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is the first derivative of <italic toggle="yes">ℓ<sub>D</sub></italic>, and the superscript “<italic toggle="yes">D</italic>” indicates that <italic toggle="yes"><bold>f</bold></italic> is the solution based on a <italic toggle="yes">differentiable</italic> loss function. Our estimator is motivated by the following result.</p>
        <p id="P35"><bold>Proposition 5</bold> (<xref rid="R31" ref-type="bibr">Zhang et al., 2018a</xref>) <italic toggle="yes">Let</italic>
<italic toggle="yes"><bold>f</bold></italic>* <italic toggle="yes">be the population minimizer in</italic>
<xref rid="FD8" ref-type="disp-formula">(6)</xref>
<italic toggle="yes">in which ℓ is a convex and differentiable function ℓ<sub>D</sub> with</italic>
<inline-formula><mml:math id="M75" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>
<italic toggle="yes">for all u</italic>. <italic toggle="yes">For any i</italic> ≠ <italic toggle="yes">j</italic> ∈ {1,…, <italic toggle="yes">k</italic>}, <italic toggle="yes">we have</italic>
<disp-formula id="FD11"><mml:math id="M11" display="block"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="P36">Proposition 5 implies the following Fisher-consistent-like result for our proposed A-ITR estimator <xref rid="FD10" ref-type="disp-formula">(8)</xref>.</p>
        <p id="P37"><bold>Proposition 6</bold><italic toggle="yes">Let</italic><italic toggle="yes"><bold>f</bold></italic>* <italic toggle="yes">be the population minimizer in</italic><xref rid="FD8" ref-type="disp-formula">(6)</xref><italic toggle="yes">in which ℓ is a convex and differentiable function ℓ<sub>D</sub> with</italic><inline-formula><mml:math id="M76" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula><italic toggle="yes">for all u</italic>. <italic toggle="yes">The</italic><inline-formula><mml:math id="M77" display="inline"><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula><italic toggle="yes">defined in</italic><xref rid="FD10" ref-type="disp-formula">(8)</xref><italic toggle="yes">based on</italic><italic toggle="yes"><bold>f</bold></italic>* <italic toggle="yes">coincides with the optimal A-ITR ϕ</italic>* <italic toggle="yes">in</italic><xref rid="FD7" ref-type="disp-formula">(5)</xref>.</p>
        <p id="P38">This method is a two-step procedure because it first estimates <italic toggle="yes"><bold>f</bold></italic>* using <inline-formula><mml:math id="M78" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>, then estimates the ratios of conditional means <italic toggle="yes">μ<sub>j</sub>/μ<sub>i</sub></italic> using <inline-formula><mml:math id="M79" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> which is then plugged into <xref rid="FD7" ref-type="disp-formula">(5)</xref> to obtain the A-ITR. Note that it does not estimate each conditional mean individually, but their ratios. The issue remains that if <italic toggle="yes"><bold>f</bold></italic>* is not accurately estimated, then the ratio <italic toggle="yes">μ<sub>j</sub>/μ<sub>i</sub></italic> cannot be accurately estimated.</p>
      </sec>
      <sec id="S9">
        <label>3.2.2</label>
        <title>One-step OWL Method</title>
        <p id="P39">The one-step method aims to directly obtain a set-valued recommendation without calculating <inline-formula><mml:math id="M80" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The crucial difference here is the use of a bent loss function, defined as
<disp-formula id="FD12"><mml:math id="M12" display="block"><mml:mrow><mml:msub><mml:mi>ℓ</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">ℓ</italic><sub>1</sub> &gt; 0 is a convex and increasing function with <inline-formula><mml:math id="M81" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for all <italic toggle="yes">u</italic> ≥ 0, and <italic toggle="yes">ℓ</italic><sub>2</sub>(<italic toggle="yes">u</italic>) = (<italic toggle="yes">c</italic> − 1)<italic toggle="yes">u</italic><sub>+</sub> with <italic toggle="yes">c</italic> ≥ 1. Such a loss function is bent at 0, since <inline-formula><mml:math id="M82" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M83" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula>. Note that the slope <italic toggle="yes">c</italic> is the same as the near-optimality constant as defined in (<xref rid="FD7" ref-type="disp-formula">5</xref>). An example of bent loss is the bent hinge loss, <italic toggle="yes">ℓ<sub>B</sub></italic>(<italic toggle="yes">u</italic>) = (1 + <italic toggle="yes">u</italic>)<sub>+</sub> + (<italic toggle="yes">c</italic> − 1)<italic toggle="yes">u</italic><sub>+</sub> (see <xref rid="F1" ref-type="fig">Figure 1</xref>.) The bent loss has been a critical tool that helps to achieve reject (and refine) options in the classification literature (<xref rid="R1" ref-type="bibr">Bartlett and Wegkamp, 2008</xref>; <xref rid="R34" ref-type="bibr">Zhang et al., 2018b</xref>).</p>
        <p id="P40">The main effect of the bent loss is to shrink the angle margin for class <italic toggle="yes">j</italic> (or treatment <italic toggle="yes">j</italic> here), defined as ⟨<italic toggle="yes"><bold>W</bold><sub>j</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)⟩, towards 0, similar to the shrinkage effect of the lasso penalty. Likewise, the additional slope <italic toggle="yes">c</italic> − 1 for <italic toggle="yes">u</italic> &gt; 0 is analogous to a penalty parameter in lasso regression, which would encourage a sparse model. Note that here such a shrinkage effect is applied to the classes/treatments with positive angle margins only. Specifically, Proposition 7 below, derived from Proposition 1 in <xref rid="R34" ref-type="bibr">Zhang et al. (2018b)</xref>, gives the precise values of the angle margins ⟨<italic toggle="yes"><bold>W</bold><sub>j</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)⟩’s with respect to <italic toggle="yes"><bold>f</bold></italic>*, the population minimizer of <xref rid="FD8" ref-type="disp-formula">(6)</xref> with the bent loss <italic toggle="yes">ℓ<sub>B</sub></italic>.</p>
        <p id="P41"><bold>Proposition 7</bold><italic toggle="yes">For the sequence μ</italic><sub>(1)</sub> &lt; ⋯ &lt; <italic toggle="yes">μ</italic><sub>(<italic toggle="yes">k</italic>)</sub>, <italic toggle="yes">suppose there exists an integer r</italic> ∈ {1,…, <italic toggle="yes">k</italic> − 1} <italic toggle="yes">such that μ</italic><sub>(<italic toggle="yes">j</italic>)</sub>/<italic toggle="yes">μ</italic><sub>(1)</sub> &lt; <italic toggle="yes">c</italic>
<italic toggle="yes">for j</italic> = 1,…, <italic toggle="yes">r and μ</italic><sub>(<italic toggle="yes">j</italic>)</sub>/<italic toggle="yes">μ</italic><sub>(1)</sub> &gt; <italic toggle="yes">c for j</italic> = <italic toggle="yes">r</italic> + 1,…, <italic toggle="yes">k</italic>. <italic toggle="yes">Let</italic>
<italic toggle="yes"><bold>f</bold></italic>* <italic toggle="yes">be the population minimizer to</italic>
<xref rid="FD8" ref-type="disp-formula">(6)</xref>
<italic toggle="yes">in which ℓ is a convex and increasing function ℓ<sub>B</sub> with</italic>
<inline-formula><mml:math id="M84" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>
<italic toggle="yes">and</italic>
<inline-formula><mml:math id="M85" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Then we have ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(1)</sub>, <italic toggle="yes"><bold>f</bold></italic>*⟩ &gt; 0, ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(2)</sub>, <italic toggle="yes"><bold>f</bold></italic>*⟩ = ⋯ = ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(<italic toggle="yes">r</italic>)</sub>, <italic toggle="yes"><bold>f</bold></italic>*⟩ = 0, <italic toggle="yes">and</italic> ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(<italic toggle="yes">r</italic>+1)</sub>, <italic toggle="yes"><bold>f</bold></italic>*⟩ = ⋯ ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(<italic toggle="yes">k</italic>)</sub>, <italic toggle="yes"><bold>f</bold></italic>*⟩ &lt; 0. <italic toggle="yes">If such an integer r does not exist, then</italic> ⟨<italic toggle="yes"><bold>W</bold></italic><sub>(<italic toggle="yes">j</italic>)</sub>, <italic toggle="yes"><bold>f</bold></italic>*⟩ = 0 <italic toggle="yes">for j</italic> = 1,…, <italic toggle="yes">k</italic>.</p>
        <p id="P42">A direct consequence of Proposition 7 is that all the near-optimal treatments (defined as <italic toggle="yes">μ</italic><sub>(<italic toggle="yes">j</italic>)</sub>/<italic toggle="yes">μ</italic><sub>(1)</sub> ≤ <italic toggle="yes">c</italic>) have non-negative angle margins, while the rest have negative angle margins. This naturally leads to the following set-valued recommendation,
<disp-formula id="FD13"><label>(9)</label><mml:math id="M13" display="block"><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi>B</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="P43">Here <italic toggle="yes"><bold>f</bold></italic> can be the population minimizer <italic toggle="yes"><bold>f</bold></italic>* <xref rid="FD8" ref-type="disp-formula">(6)</xref> or the empirical minimizer <inline-formula><mml:math id="M86" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>
<xref rid="FD9" ref-type="disp-formula">(7)</xref> and the superscript “<italic toggle="yes">B</italic>” indicates that the loss <italic toggle="yes">ℓ</italic> is a <italic toggle="yes">bent</italic> loss <italic toggle="yes">ℓ<sub>B</sub></italic>, as opposed to a differentiable loss function in the two-step method.</p>
        <p id="P44">Proposition 7 implies that <inline-formula><mml:math id="M87" display="inline"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>⊆</mml:mo><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⊆</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. The following assumption is necessary to resolve the identifiable issue of <xref rid="FD13" ref-type="disp-formula">(9)</xref> and to show its optimality.</p>
        <p id="P45"><bold>Assumption 8</bold><italic toggle="yes">For any positive c</italic><sub>0</sub>, <italic toggle="yes">p</italic>(<italic toggle="yes">μ<sub>j</sub></italic>(<italic toggle="yes"><bold>X</bold></italic>) = <italic toggle="yes">c</italic><sub>0</sub><italic toggle="yes">μ<sub>i</sub></italic>(<italic toggle="yes"><bold>X</bold></italic>)) = 0 <italic toggle="yes">for</italic> ∀<italic toggle="yes">i</italic> ≠ <italic toggle="yes">j</italic> ∈ {1,…, <italic toggle="yes">k</italic>} <italic toggle="yes">in which</italic>
<inline-formula><mml:math id="M88" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>
<italic toggle="yes">is the conditional mean outcome for treatment j</italic>.</p>
        <p id="P46">Assumption 8 guarantees the two sets, <inline-formula><mml:math id="M89" display="inline"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M90" display="inline"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">c</italic> is the near-optimality constant in <xref rid="FD7" ref-type="disp-formula">(5)</xref>, have measure 0 for any <italic toggle="yes">i</italic> ≠ <italic toggle="yes">j</italic> so that <italic toggle="yes"><bold>f</bold></italic>* is identifiable almost everywhere. Under Assumption 8, we have the following proposition, analogous to Fisher consistency in classification.</p>
        <p id="P47"><bold>Proposition 9</bold><italic toggle="yes">Suppose Assumption 8 holds. Let</italic><italic toggle="yes"><bold>f</bold></italic>* <italic toggle="yes">be the population minimizer in</italic><xref rid="FD8" ref-type="disp-formula">(6)</xref><italic toggle="yes">in which ℓ is a convex and increasing function ℓ<sub>B</sub> with</italic><inline-formula><mml:math id="M91" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula><italic toggle="yes">and</italic><inline-formula><mml:math id="M92" display="inline"><mml:mrow><mml:msubsup><mml:mi>ℓ</mml:mi><mml:mi>B</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. <italic toggle="yes">The A-ITR</italic><inline-formula><mml:math id="M93" display="inline"><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula><italic toggle="yes">defined in</italic><xref rid="FD13" ref-type="disp-formula">(9)</xref><italic toggle="yes">based on</italic><italic toggle="yes"><bold>f</bold></italic>* <italic toggle="yes">coincides with the optimal A-ITR ϕ</italic>* <xref rid="FD7" ref-type="disp-formula">(5)</xref><italic toggle="yes">with the near-optimality constant c</italic>.</p>
        <p id="P48">While Assumption 8 is useful as a technical assumption, it may not hold in certain practical situations. For example, when two treatments have the same conditional mean outcomes for a group of patients, Assumption 8 does not hold for <italic toggle="yes">c</italic> = 1. Another case that it is more likely to fail is when the outcome <italic toggle="yes">Y</italic> can only take finite and discrete values. Even if it does not hold, the proposed methods could still be useful. See Example 3 in <xref rid="S13" ref-type="sec">Section 5</xref> in which Assumption 8 is violated.</p>
        <p id="P49">Note that for both classification-based methods, a single-valued ITR can be easily defined by recommending the treatment option with the largest angle margin, that is, argmax<sub><italic toggle="yes">j</italic>∈{1,…,<italic toggle="yes">k</italic>}</sub>⟨<italic toggle="yes"><bold>W</bold><sub>j</sub></italic>, <italic toggle="yes"><bold>f</bold></italic>(<italic toggle="yes"><bold>x</bold></italic>)⟩.</p>
        <p id="P50">Unlike the regression-based method, the two classification-based methods do not estimate the conditional mean outcome. The success of the regression-based method relies on an accurate estimation of <italic toggle="yes">μ<sub>j</sub></italic> at every <italic toggle="yes"><bold>x</bold></italic> of interest, while reasonable performance is expected for the classification-based methods as long as the estimation is accurate around the “boundaries”. However, the two-step method and the one-step method seem to have different focuses. Both methods start with finding a discriminant function to minimize the outcome-weighted misclassification rate for the purpose of minimizing the expected outcome. As a consequence, both methods have “good” performances near boundaries that distinguish the optimal treatment from the non-optimal treatments for each patient. The one-step method, additionally, uses a bent loss with a shrinkage effect that is capable of determining whether a treatment is <italic toggle="yes">close enough to</italic>, not whether it is <italic toggle="yes">equal to</italic>, the optimal treatment. More precisely speaking, the one-step method calibrates the boundary defined by <italic toggle="yes">μ<sub>j</sub></italic>(<italic toggle="yes"><bold>x</bold></italic>)/<italic toggle="yes">μ</italic><sub>(1)</sub>(<italic toggle="yes"><bold>x</bold></italic>) = <italic toggle="yes">c</italic>. This is theoretically justified by Proposition 7. Hence, the one-step method also has “good” performance near such new notions of boundaries.</p>
        <p id="P51">To illustrate the additional strength of the one-step method, we show the boundaries between recommendations for a toy example (the details of which will be revisited in the numerical studies) in <xref rid="F2" ref-type="fig">Figure 2</xref>, in which the top row shows the single-valued ITR and the second row the set-valued A-ITR, by the Bayes rule, the two-step method, and the one-step method respectively. Both classification-based methods give good approximations to the Bayes ITR boundaries, shown in the top row. However, the two-step method seems to include more treatments into the near-optimal set when compared to the Bayes rule (shown in the bottom row), than the one-step method does. For example, the two-step estimator displays much more recommendations with 2 or 3 treatment options. This is probably due to the fact that the optimization for the two-step method is not designed to capture this subtle pattern, at least not with a finite sample.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S10">
    <label>4.</label>
    <title>Implementations</title>
    <p id="P52">In this section, we discuss various aspects of the implementations for the proposed methods, including the optimization, the normalization of the predictive function, and the parameter tuning.</p>
    <sec id="S11">
      <label>4.1</label>
      <title>Algorithm</title>
      <p id="P53">In this section, we introduce the optimization procedure to estimate <italic toggle="yes"><bold>f</bold></italic>* defined in <xref rid="FD8" ref-type="disp-formula">(6)</xref>. Instead of the constrained problem <xref rid="FD9" ref-type="disp-formula">(7)</xref>, we solve the regularized problem:
<disp-formula id="FD14"><label>(10)</label><mml:math id="M14" display="block"><mml:mrow><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">λ</italic> is a tuning parameter. It is a weighted classification problem with weight <italic toggle="yes">w<sub>i</sub></italic> = <italic toggle="yes">y<sub>i</sub></italic>/<italic toggle="yes">p</italic>(<italic toggle="yes">a<sub>i</sub></italic>∣<italic toggle="yes"><bold>x</bold></italic><sub><italic toggle="yes">i</italic></sub>).</p>
      <p id="P54">In terms of the function class <inline-formula><mml:math id="M94" display="inline"><mml:mi mathvariant="script">F</mml:mi></mml:math></inline-formula>, there are linear learning and kernel learning (<xref rid="R24" ref-type="bibr">Steinwart et al., 2007</xref>; <xref rid="R10" ref-type="bibr">Hofmann et al., 2008</xref>; <xref rid="R8" ref-type="bibr">Hastie et al., 2009</xref>). Let <inline-formula><mml:math id="M95" display="inline"><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi></mml:mrow></mml:math></inline-formula>, and for simplicity, we add a constant term to <italic toggle="yes"><bold>x</bold></italic>. Then for linear learning, we have <italic toggle="yes">f<sub>j</sub></italic>(<italic toggle="yes"><bold>x</bold></italic>) = <italic toggle="yes"><bold>x</bold></italic><sup><italic toggle="yes">T</italic></sup><italic toggle="yes">β<sub>j</sub></italic>, and the corresponding penalty <inline-formula><mml:math id="M96" display="inline"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. For kernel learning, <inline-formula><mml:math id="M97" display="inline"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">K</italic>(·, ·) is a kernel function. The penalty term becomes <inline-formula><mml:math id="M98" display="inline"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>α</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi mathvariant="bold">K</mml:mi><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>α</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <bold>K</bold> is the gram matrix. Note that we include the intercept term into <italic toggle="yes">J</italic>(<italic toggle="yes"><bold>f</bold></italic>) and a benefit by doing this is the reduction of the complexity of the algorithm. <xref rid="R33" ref-type="bibr">Zhang et al. (2016)</xref> shows theoretically that it can achieve the same convergence rate as the case without the intercept term.</p>
      <p id="P55">We proposed the two-step method and the one-step method. The two-step method is based on a differentiable loss <italic toggle="yes">ℓ<sub>D</sub></italic>, while the one-step method is based on a bent loss <italic toggle="yes">ℓ<sub>B</sub></italic>(<italic toggle="yes">u</italic>) = <italic toggle="yes">ℓ</italic><sub>1</sub>(<italic toggle="yes">u</italic>) + <italic toggle="yes">ℓ</italic><sub>2</sub>(<italic toggle="yes">u</italic>), where <italic toggle="yes">ℓ</italic><sub>1</sub> is convex and <italic toggle="yes">ℓ</italic><sub>2</sub>(<italic toggle="yes">u</italic>) = (<italic toggle="yes">c</italic> − 1)<italic toggle="yes">u</italic><sub>+</sub>. Since <italic toggle="yes">ℓ<sub>D</sub></italic> is similar to a special case of <italic toggle="yes">ℓ<sub>B</sub></italic> with <italic toggle="yes">c</italic> = 1, here we only need to focus on the algorithm for the bent loss <italic toggle="yes">ℓ<sub>B</sub></italic>. In the rest of this section, we use linear learning to demonstrate our algorithm and have deferred the details about kernel learning to the supplementary materials.</p>
      <p id="P56">We first consider the case when <italic toggle="yes">ℓ</italic><sub>1</sub> is differentiable. In this case, we use the ADMM (<xref rid="R2" ref-type="bibr">Boyd et al., 2011</xref>) algorithm to solve <xref rid="FD14" ref-type="disp-formula">(10)</xref>. The ADMM algorithm is used when the objective function can be written as a sum of two convex functions, which, in our case, are <italic toggle="yes">ℓ</italic><sub>1</sub> and <italic toggle="yes">ℓ</italic><sub>2</sub>.</p>
      <p id="P57">We denote the coefficient matrix as <italic toggle="yes">B</italic><sub><italic toggle="yes">p</italic>×(<italic toggle="yes">k</italic>−1)</sub> = [<italic toggle="yes"><bold>β</bold></italic><sub>1</sub>,…, <italic toggle="yes"><bold>β</bold></italic><sub><italic toggle="yes">k</italic>−1</sub>]. Then we create another copy of the coefficients <italic toggle="yes">G</italic><sub><italic toggle="yes">p</italic>×(<italic toggle="yes">k</italic>−1)</sub> = [<italic toggle="yes"><bold>γ</bold></italic><sub>1</sub>,…, <italic toggle="yes"><bold>γ</bold></italic><sub><italic toggle="yes">k</italic>−1</sub>], and let <italic toggle="yes">Z</italic><sub><italic toggle="yes">p</italic>×(<italic toggle="yes">k</italic>−1)</sub> = [<italic toggle="yes"><bold>z</bold></italic><sub>1</sub>,…, <italic toggle="yes"><bold>z</bold></italic><sub><italic toggle="yes">k</italic>−1</sub>]. Recall that <italic toggle="yes">w<sub>i</sub></italic> = <italic toggle="yes">y<sub>i</sub></italic>/<italic toggle="yes">p</italic>(<italic toggle="yes">a<sub>i</sub></italic>∣<italic toggle="yes"><bold>x</bold></italic><sub><italic toggle="yes">i</italic></sub>), then we minimize the augmented Lagrangian
<disp-formula id="FD15"><mml:math id="M15" display="block"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mi>λ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mspace linebreak="newline"/><mml:mspace width="6em"/><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">ρ</italic> &gt; 0 controls the step size.</p>
      <p id="P58">At step <italic toggle="yes">t</italic>, for each <italic toggle="yes">j</italic> = 1,…, <italic toggle="yes">k</italic> − 1 we can update <italic toggle="yes">B<sup>t</sup></italic>, <italic toggle="yes">G<sup>t</sup></italic> and <italic toggle="yes">Z<sup>t</sup></italic> as
<disp-formula id="FD16"><mml:math id="M16" display="block"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>L</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace linebreak="newline"/><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>L</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>B</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mn>1</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace linebreak="newline"/><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>γ</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
until matrix <italic toggle="yes">B</italic> converges. Note that in the two-step method where <italic toggle="yes">c</italic> = 1, we have <italic toggle="yes">ℓ</italic><sub>2</sub>(<italic toggle="yes">u</italic>) = 0. In this case, we can force <italic toggle="yes">B</italic> = <italic toggle="yes">G</italic> and only update <inline-formula><mml:math id="M99" display="inline"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>’s until they converge.</p>
      <p id="P59">Next we consider the case when <italic toggle="yes">ℓ</italic><sub>1</sub> is not differentiable. In the literature of classification, a non-differentiable loss that has been commonly used is hinge loss. Note that in our case, since we prefer smaller outcomes, we define the hinge loss as <italic toggle="yes">ℓ</italic><sub>1</sub>(<italic toggle="yes">u</italic>) = (1 + <italic toggle="yes">u</italic>)<sub>+</sub> (see <xref rid="F1" ref-type="fig">Figure 1</xref>). That is, we flip the traditional hinge loss with respect to the y-axis to make it an increasing function. A typical approach to an optimization problem with the hinge loss is to transform it into a quadratic programming (QP) problem in its duality (<xref rid="R7" ref-type="bibr">Fung and Mangasarian, 2005</xref>; <xref rid="R8" ref-type="bibr">Hastie et al., 2009</xref>; <xref rid="R34" ref-type="bibr">Zhang et al., 2018b</xref>). Specifically, the dual problem of (<xref rid="FD14" ref-type="disp-formula">10</xref>) can be written as
<disp-formula id="FD17"><mml:math id="M17" display="block"><mml:mrow><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mi>λ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace linebreak="newline"/><mml:mspace width="0.5em"/><mml:mtext>s.t.</mml:mtext><mml:mspace width="thickmathspace"/><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="thickmathspace"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="M100" display="inline"><mml:mrow><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">W</italic><sub><italic toggle="yes">a</italic><sub><italic toggle="yes">i</italic></sub>,<italic toggle="yes">j</italic></sub> is the <italic toggle="yes">j</italic>th component of <italic toggle="yes"><bold>W</bold></italic><sub><italic toggle="yes">a</italic><sub><italic toggle="yes">i</italic></sub></sub>. Note that the weight <italic toggle="yes">w<sub>i</sub></italic> = <italic toggle="yes">y<sub>i</sub></italic>/<italic toggle="yes">p</italic>(<italic toggle="yes">a<sub>i</sub></italic>∣<italic toggle="yes"><bold>x</bold></italic><sub><italic toggle="yes">i</italic></sub>) serves as the upper bound of the box constraints. Because the objective function is quadratic in <italic toggle="yes">α<sub>i</sub></italic> and <italic toggle="yes">γ<sub>i</sub></italic>, it has explicit solution at each iteration. Thus it converges very fast by using algorithms such as coordinate decent (<xref rid="R34" ref-type="bibr">Zhang et al., 2018b</xref>).</p>
      <p id="P60">In practice, there may be numerical errors to the solution. Moreover, due to different choices of the tuning parameter <italic toggle="yes">λ</italic>, the scale of the resulting angle margins may vary much between different tuning trials. We propose the following normalization procedure for the one-step A-ITR <inline-formula><mml:math id="M101" display="inline"><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>
<xref rid="FD13" ref-type="disp-formula">(9)</xref> to boost the empirical performance. The idea is that instead of recommending all treatments with angle margins greater than or equal to 0, we change the threshold to a small number varying around 0. Such a threshold is a fixed constant <italic toggle="yes">δ</italic> multiplied by a measure of the scale, chosen to be the magnitude of the smallest angle margin. The normalized one-step A-ITR is then
<disp-formula id="FD18"><label>(11)</label><mml:math id="M18" display="block"><mml:mrow><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>B</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo>≥</mml:mo><mml:mi>δ</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic toggle="yes">δ</italic> is a tuning parameter around 0 and <inline-formula><mml:math id="M102" display="inline"><mml:mrow><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">∣</mml:mo></mml:mrow></mml:math></inline-formula> is the magnitude of the smallest angle margin (note that <inline-formula><mml:math id="M103" display="inline"><mml:mrow><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo></mml:mrow></mml:math></inline-formula> is negative).</p>
    </sec>
    <sec id="S12">
      <label>4.2</label>
      <title>Tuning Procedure</title>
      <p id="P61">In this paper, the estimation procedure involves two tuning parameters. The first one is the regularization parameter <italic toggle="yes">λ</italic> in <xref rid="FD14" ref-type="disp-formula">(10)</xref> which appears in both the two-step and one-step methods. The second one is the normalization parameter <italic toggle="yes">δ</italic> in <xref rid="FD18" ref-type="disp-formula">(11)</xref> for the one-step method only. We will tune these two parameters differently in two steps.</p>
      <p id="P62">The first step is to tune <italic toggle="yes">λ</italic>. For each <italic toggle="yes">λ</italic>, the estimated solution is <inline-formula><mml:math id="M104" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>. Then we define the corresponding single-treatment ITR as <inline-formula><mml:math id="M105" display="inline"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mtext>argmax</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">〉</mml:mo></mml:mrow></mml:math></inline-formula> and calculate its empirical average of the expected outcome <xref rid="FD1" ref-type="disp-formula">(1)</xref>, which is given by
<disp-formula id="FD19"><mml:math id="M19" display="block"><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo stretchy="false">∕</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></disp-formula>
(<xref rid="R37" ref-type="bibr">Zhao et al., 2012</xref>; <xref rid="R31" ref-type="bibr">Zhang et al., 2018a</xref>). We choose the <italic toggle="yes">λ</italic> that yields the smallest empirical risk for the resulting ITR, even if our ultimate goal is to obtain a set-valued A-ITR. This can substantially simplify the tuning process. We found that other more complicated tuning procedures have led to a similar performance.</p>
      <p id="P63">For the one-step method, we need to continue to tune <italic toggle="yes">δ</italic>. For the same <italic toggle="yes">λ</italic> (same resulting ITR), because different <italic toggle="yes">δ</italic>’s may lead to slightly different set-valued A-ITRs and recommendations with different carnalities, we must actually compare the resulting A-ITRs to choose the best <italic toggle="yes">δ</italic>, instead of using the ITR as a proxy. However, there are some difficulties in evaluating the performance of the estimated A-ITR. Compared to the conventional ITR, the challenge here is that when the recommendation includes two or more treatment options, there are multiple potential outcomes and it is difficult to quantify the “overall” benefit for such a recommendation.</p>
      <p id="P64">Although the proposed optimal A-ITR <italic toggle="yes">ϕ</italic>* defined in <xref rid="FD7" ref-type="disp-formula">(5)</xref> is not a Bayes rule under any loss function, we can consider a closely related loss function, whose risk function is given by
<disp-formula id="FD20"><label>(12)</label><mml:math id="M20" display="block"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mfrac><mml:mrow><mml:mi>Y</mml:mi><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mo stretchy="false">[</mml:mo><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="M106" display="inline"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mspace width="thinmathspace"/><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi mathvariant="script">A</mml:mi></mml:msup><mml:mo>∖</mml:mo><mml:mi>∅</mml:mi></mml:mrow></mml:math></inline-formula> is a set-valued predictor and ∣ · ∣ denotes the cardinality of a set. Compared to the expected outcome <inline-formula><mml:math id="M107" display="inline"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> defined in <xref rid="FD1" ref-type="disp-formula">(1)</xref>, this quantity is a weighted outcome with weight 1/(1 + (∣<italic toggle="yes">ϕ</italic>∣ − 1)<italic toggle="yes">c</italic>) under <italic toggle="yes">ϕ</italic>. If we force ∣<italic toggle="yes">ϕ</italic>∣ ≡ 1, it reduces to <inline-formula><mml:math id="M108" display="inline"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. More importantly, it can be shown that the minimizer of <xref rid="FD20" ref-type="disp-formula">(12)</xref>, denoted by <italic toggle="yes">ϕ</italic><sup>+</sup>, is
<disp-formula id="FD21"><mml:math id="M21" display="block"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi mathvariant="script">A</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mi>μ</mml:mi><mml:mi>ϕ</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="1em"/><mml:mspace width="thinmathspace"/><mml:mtext>where</mml:mtext><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>μ</mml:mi><mml:mi>ϕ</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≜</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P65">Here <italic toggle="yes">μ<sup>ϕ</sup></italic> defines a new criterion that generalizes the expected outcomes under a set-valued treatment recommendation <italic toggle="yes">ϕ</italic>. To see that, note that for <italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>x</bold></italic>) = {1}, <italic toggle="yes">μ</italic><italic toggle="yes"><sup>ϕ</sup></italic>(<italic toggle="yes"><bold>x</bold></italic>) = <italic toggle="yes">μ</italic><sub>1</sub>(<italic toggle="yes"><bold>x</bold></italic>), while for <italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>x</bold></italic>) = {1, 2}, <italic toggle="yes">μ<sup>ϕ</sup></italic>(<italic toggle="yes"><bold>x</bold></italic>) = (<italic toggle="yes">μ</italic><sub>1</sub>(<italic toggle="yes"><bold>x</bold></italic>) + <italic toggle="yes">μ</italic><sub>2</sub>(<italic toggle="yes"><bold>x</bold></italic>))/(1 + <italic toggle="yes">c</italic>), which is smaller than the simple average (<italic toggle="yes">μ</italic><sub>1</sub>(<italic toggle="yes"><bold>x</bold></italic>) + <italic toggle="yes">μ</italic><sub>2</sub>(<italic toggle="yes"><bold>x</bold></italic>))/2 when <italic toggle="yes">c</italic> &gt; 1. Suppose treatment 1 is better than treatment 2 (<italic toggle="yes">μ</italic><sub>1</sub>(<italic toggle="yes"><bold>x</bold></italic>) &lt; <italic toggle="yes">μ</italic><sub>2</sub>(<italic toggle="yes"><bold>x</bold></italic>)). We can show that <italic toggle="yes">ϕ</italic><sub>2</sub>(<italic toggle="yes"><bold>x</bold></italic>) ≜ {1, 2} is as good as <italic toggle="yes">ϕ</italic><sub>1</sub>(<italic toggle="yes"><bold>x</bold></italic>) ≜ {1} under this new criterion if and only if <italic toggle="yes">μ</italic><sub>2</sub>(<italic toggle="yes"><bold>x</bold></italic>)/<italic toggle="yes">μ</italic><sub>1</sub>(<italic toggle="yes"><bold>x</bold></italic>) ≤ <italic toggle="yes">c</italic>, which is exactly the near-optimal recommendation set defined in <xref rid="FD7" ref-type="disp-formula">(5)</xref>.</p>
      <p id="P66">Intuitively, <italic toggle="yes">ϕ</italic><sup>+</sup>(<italic toggle="yes"><bold>x</bold></italic>) is an optimal set of treatments selected to minimize the “average” clinical outcome with a penalty on the cardinality of the recommendation set. Note that when <italic toggle="yes">c</italic> = 1, <italic toggle="yes">ϕ</italic><sup>+</sup> is the same as the optimal ITR <italic toggle="yes">d</italic>*. Moreover, when <italic toggle="yes">k</italic> = 2, <italic toggle="yes">ϕ</italic><sup>+</sup> is the same as the optimal A-ITR <italic toggle="yes">ϕ</italic>*, as shown above. When <italic toggle="yes">k</italic> ≥ 3, <italic toggle="yes">ϕ</italic><sup>+</sup> and <italic toggle="yes">ϕ</italic>* are different but are nested within each other in the following way: if we let <inline-formula><mml:math id="M109" display="inline"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M110" display="inline"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, then we have <inline-formula><mml:math id="M111" display="inline"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mo>∗</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M112" display="inline"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo>⊆</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> for <italic toggle="yes">t</italic> = 2,…, <italic toggle="yes">k</italic> − 1. <xref rid="F3" ref-type="fig">Figure 3</xref> demonstrates their relationship when <italic toggle="yes">k</italic> = 3.</p>
      <p id="P67">From <xref rid="F3" ref-type="fig">Figure 3</xref>, we observe that the regions with only one treatment are the same <inline-formula><mml:math id="M113" display="inline"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mo>∗</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, while the regions containing two or three treatments are slightly different. In general, the boundaries between the size-1 decisions and their complements are the same for the two rules <italic toggle="yes">ϕ</italic><sup>+</sup> and <italic toggle="yes">ϕ</italic>*. They only differ in the boundaries between recommendations with different cardinalities (for example, the boundary between size-2 decisions and size-3 decisions). Although <italic toggle="yes">ϕ</italic>* does not directly minimize the weighted outcome defined in (<xref rid="FD20" ref-type="disp-formula">12</xref>), the similarity between <italic toggle="yes">ϕ</italic><sup>+</sup> and <italic toggle="yes">ϕ</italic>* justifies the use of the weighted outcome (<xref rid="FD20" ref-type="disp-formula">12</xref>) as a new criterion for the tuning parameter selection. Specifically, we choose the <italic toggle="yes">δ</italic> value that can yield the smallest value of the following empirical counterpart of (<xref rid="FD20" ref-type="disp-formula">12</xref>),
<disp-formula id="FD22"><label>(13)</label><mml:math id="M22" display="block"><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mstyle mathvariant="double-struck"><mml:mn>1</mml:mn></mml:mstyle><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo stretchy="false">∕</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P68">In addition to the tuning parameter selection, we may also use this criterion to select different methods for conducting A-ITRs. In the real data analysis, we will use this criterion to select between the two proposed classification-based methods.</p>
    </sec>
  </sec>
  <sec id="S13">
    <label>5.</label>
    <title>Simulation Studies</title>
    <p id="P69">In this section, we study the numerical performance of the proposed methods.</p>
    <sec id="S14">
      <label>5.1</label>
      <title>Comparing Set-valued Recommendations</title>
      <p id="P70">For two ITRs <italic toggle="yes">d</italic><sub>1</sub> and <italic toggle="yes">d</italic><sub>2</sub>, we can compare them by evaluating the expected outcome defined in <xref rid="FD1" ref-type="disp-formula">(1)</xref>. However, for two A-ITRs <italic toggle="yes">ϕ</italic><sub>1</sub> and <italic toggle="yes">ϕ</italic><sub>2</sub>, it is difficult to quantify which one is better due to the fact that a measure for the overall benefit is not well defined when multiple treatments are recommended. Although in <xref rid="S12" ref-type="sec">Section 4.2</xref> we have proposed the weighted expected outcome <xref rid="FD20" ref-type="disp-formula">(12)</xref> for evaluating two A-ITRs, the optimal A-ITR <italic toggle="yes">ϕ</italic><sup>+</sup> under this new criterion is still different from the desired near-optimal recommendation set <italic toggle="yes">ϕ</italic>*. So in the simulation studies, in addition to the empirical weighted outcome <xref rid="FD22" ref-type="disp-formula">(13)</xref>, we consider another means to compare different A-ITRs, using the expected outcome of the best and the worst treatments among the treatments that are recommended, averaged over a set of observations. We conduct such an evaluation for different types of recommendations separately to see how the A-ITR performs differently on them. Based on the size of the true optimal A-ITR <italic toggle="yes">ϕ</italic>*, we split the covariate space <inline-formula><mml:math id="M114" display="inline"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula> into three regions corresponding to three kinds of recommendations:
<disp-formula id="FD23"><mml:math id="M23" display="block"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mtext>only one treatment is suggested</mml:mtext><mml:mo stretchy="false">}</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mspace linebreak="newline"/><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mtext>more than one treatment but not all of them are suggested</mml:mtext><mml:mo stretchy="false">}</mml:mo><mml:mspace linebreak="newline"/><mml:mspace width="1em"/><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mn>1</mml:mn><mml:mo>&lt;</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mspace linebreak="newline"/><mml:msub><mml:mi>R</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mtext>all treatments are suggested</mml:mtext><mml:mo stretchy="false">}</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P71">Note that <italic toggle="yes">R</italic><sub>1</sub>, <italic toggle="yes">R</italic><sub>2</sub> and <italic toggle="yes">R</italic><sub>3</sub> are disjoint and <inline-formula><mml:math id="M115" display="inline"><mml:mrow><mml:mi mathvariant="script">X</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. When <italic toggle="yes">c</italic> = 1, <italic toggle="yes">ϕ</italic>* is the optimal ITR <italic toggle="yes">d</italic>* and <inline-formula><mml:math id="M116" display="inline"><mml:mrow><mml:mi mathvariant="script">X</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. When <italic toggle="yes">c</italic> &gt; 1, we may have non-empty regions <italic toggle="yes">R</italic><sub>2</sub> and <italic toggle="yes">R</italic><sub>3</sub>.</p>
      <p id="P72">For two A-ITRs <italic toggle="yes">ϕ</italic><sub>1</sub> and <italic toggle="yes">ϕ</italic><sub>2</sub>, we will compare them separately on <italic toggle="yes">R</italic><sub>1</sub>, <italic toggle="yes">R</italic><sub>2</sub> and <italic toggle="yes">R</italic><sub>3</sub>. In each region, since multiple treatments may be suggested, we can compare the expected minimal outcome and the expected maximal outcome that they may lead to. Recall <italic toggle="yes">Y</italic>*(<italic toggle="yes">j</italic>) is the potential outcome by taking treatment <italic toggle="yes">j</italic>. Mathematically, we consider a performance interval,
<disp-formula id="FD24"><mml:math id="M24" display="block"><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mphantom><mml:mo stretchy="true">(</mml:mo></mml:mphantom><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msup><mml:mi>Y</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/></mml:mrow><mml:mo stretchy="true">∣</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mphantom><mml:mo stretchy="true">(</mml:mo></mml:mphantom><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:msup><mml:mi>Y</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/></mml:mrow><mml:mo stretchy="true">∣</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where the first quantity indicates the expected outcome if one can always use the best treatment within the recommended set <italic toggle="yes">ϕ</italic>(<italic toggle="yes"><bold>x</bold></italic>) and the second quantity represents the worst situation, i.e., how bad it can be if one always chooses the worst treatment among the recommended options. Note that on <italic toggle="yes">R</italic><sub>1</sub>, the two quantities are the same under <italic toggle="yes">ϕ</italic>* since only one treatment is recommended. As we increase <italic toggle="yes">c</italic>, we expect that this interval becomes wider on <italic toggle="yes">R</italic><sub>2</sub> and <italic toggle="yes">R</italic><sub>3</sub> since the diversity of the recommended options increases. From the definition of this interval, we claim that <italic toggle="yes">ϕ</italic><sub>1</sub> is better than <italic toggle="yes">ϕ</italic><sub>2</sub> if both the lower and the upper limits of this interval under <italic toggle="yes">ϕ</italic><sub>1</sub> are smaller than their counterparts under <italic toggle="yes">ϕ</italic><sub>2</sub>, on each region.</p>
    </sec>
    <sec id="S15">
      <label>5.2</label>
      <title>Results</title>
      <p id="P73">We consider three simulation examples. For each example, we let <italic toggle="yes"><bold>X</bold></italic> be uniformly sampled from <inline-formula><mml:math id="M117" display="inline"><mml:mrow><mml:mi mathvariant="script">X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M118" display="inline"><mml:mrow><mml:mi mathvariant="script">X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mn>10</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> respectively. For simplicity, we assume <italic toggle="yes">A</italic> ⊥ <italic toggle="yes"><bold>X</bold></italic> and <italic toggle="yes">p</italic>(<italic toggle="yes">A</italic>∣<italic toggle="yes"><bold>X</bold></italic>) = 1/<italic toggle="yes">k</italic>, and let <italic toggle="yes">Y</italic> = <italic toggle="yes">μ<sub>A</sub></italic>(<italic toggle="yes"><bold>X</bold></italic>)+<italic toggle="yes">ϵ</italic> where <italic toggle="yes">ϵ</italic> ~ <italic toggle="yes">N</italic>(0, 1/2). In each case, we let the training sample size to be <italic toggle="yes">n</italic> = 500, 1000, 2000, and use a test set with sample size 1000 to evaluate the performance. We compare three methods, namely, the regression-based method, the two-step classification-based method with squared loss, and the one-step classification-based method with the bent hinge loss. For each method, we output both ITR and A-ITR with <italic toggle="yes">c</italic> = 1.2. Finally, we repeat each simulation 100 times and report the averages.</p>
      <p id="P74"><bold>Example 1:</bold> This is an example with three treatments, where two conditional mean outcome functions are polynomial and the other is linear. Specifically, we have <inline-formula><mml:math id="M119" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M120" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <italic toggle="yes">μ</italic><sub>3</sub>(<italic toggle="yes"><bold>X</bold></italic>) = 3 + <italic toggle="yes">X</italic><sub>1</sub> + <italic toggle="yes">X</italic><sub>2</sub>. The upper panel in <xref rid="F4" ref-type="fig">Figure 4</xref> shows the true boundaries for the three treatments. We use polynomial kernel for both the two-step and one-step methods. The tuning parameter <italic toggle="yes">λ</italic> is chosen from 5<sup>−6</sup> to 5<sup>2</sup>.</p>
      <p id="P75"><bold>Example 2:</bold> This is an example with four treatments, where all the conditional mean outcome functions are non-linear, <italic toggle="yes">μ<sub>A</sub></italic>(<italic toggle="yes"><bold>X</bold></italic>) = 2 + sign(<italic toggle="yes">A</italic> − 2.5) cos (0.5<italic toggle="yes">π</italic>(<italic toggle="yes">X</italic><sub>1</sub> + (−1)<sup><italic toggle="yes">A</italic></sup><italic toggle="yes">X</italic><sub>2</sub>)). Specifically, treatment 2 and 4 are dominated by treatments 1 and 3 and the optimal ITR should only output either 1 or 3. However, in certain regions treatment 2 and 4 still produce fairly good outcomes which can only be captured by A-ITR. The lower panel in <xref rid="F4" ref-type="fig">Figure 4</xref> shows the true boundaries. For the two-step method, we report the results using Gaussian kernel. For the one-step method, we report the results with polynomial kernel. The tuning parameter <italic toggle="yes">λ</italic> is chosen from 5<sup>−9</sup> to 5<sup>−1</sup>.</p>
      <p id="P76"><bold>Example 3:</bold> This is an example where Assumption 8 is violated. Specifically, <inline-formula><mml:math id="M121" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2.5</mml:mn><mml:mo>,</mml:mo><mml:mn>2.3</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M122" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2.7</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>∗</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>3</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>4</mml:mn><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M123" display="inline"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>min</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>3.2</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Note that when <italic toggle="yes">c</italic> = 1.2, <italic toggle="yes">p</italic>(<italic toggle="yes">μ</italic><sub>3</sub>(<italic toggle="yes"><bold>X</bold></italic>) = <italic toggle="yes">cμ</italic><sub>1</sub>(<italic toggle="yes"><bold>X</bold></italic>)) &gt; 0 so Assumption 8 is violated. Similar to Example 1, we report the results using polynomial kernel for both the two-step and one-step methods. The tuning parameter <italic toggle="yes">λ</italic> is chosen from 5<sup>−7</sup> to 1.</p>
      <p id="P77"><xref rid="T1" ref-type="table">Table 1</xref> collects the results of the three examples with <italic toggle="yes">n</italic> = 1000 with dimension <italic toggle="yes">p</italic> = 5 and 10. The results with <italic toggle="yes">n</italic> = 500 and 2000 are provided in the supplementary material. In <xref rid="T1" ref-type="table">Table 1</xref>, the results of A-ITR are in the form of intervals while the results of ITR are single numbers. We also compute the empirical weighted outcome (“All” column in <xref rid="T1" ref-type="table">Table 1</xref>) defined in (<xref rid="FD22" ref-type="disp-formula">13</xref>) as an indicator for the overall performance for each method.</p>
      <p id="P78">We note that the performance intervals for A-ITR always cover the expected outcomes of the single-valued ITR. This implies that by applying our proposed A-ITR framework, patients will potentially get a much better outcome as long as they are willing to consider other equally effective options identified by the A-ITR. Even if the patient does not choose the best option within the recommendation set, the worst case is not too bad and the ratio of its outcome to that of the best option is about <italic toggle="yes">c</italic> if the A-ITR is accurately estimated.</p>
      <p id="P79">We compare different methods by inspecting the length and location of the A-ITR performance interval. Recall that the A-ITR with the shortest interval, the smallest lower limit, and the smallest upper limit on each region is the best A-ITR. However, since <italic toggle="yes">R</italic><sub>3</sub> is the region where all treatments are near the optimality, different recommendations are expected to perform similarly. Hence we focus on regions <italic toggle="yes">R</italic><sub>1</sub> and <italic toggle="yes">R</italic><sub>2</sub> for the purpose of comparison.</p>
      <p id="P80">From <xref rid="T1" ref-type="table">Table 1</xref>, we note that the regression-based A-ITR, though has the smallest lower limit in some cases, also has a longer interval in most cases, suggesting that the treatment could either go really well or really badly. This implies that the regression-based A-ITR method tends to include ineffective treatments into the near-optimal set. Part of the reason may be that the regression-based method has not accurately estimated each of the three or four potential outcome functions.</p>
      <p id="P81">For the classification-based A-ITRs, the lower limits are roughly the same between the one-step method and the two-step method; however, the one-step method has shorter intervals in most cases. This means that the one-step method is better at excluding ineffective treatment options from the recommendation than the two-step method. This is true even when Assumption 8 is violated (Example 3). In addition, the one-step method also has the smallest expected weighted outcome (shown in the “All” column). However, the performance of both the one-step method and the two-step method becomes worse as the sample size decreases (see supplementary material), or the number of covariates increases. This may be due to the inefficiency of using the inverse probability weighting in the OWL framework.</p>
    </sec>
  </sec>
  <sec id="S16">
    <label>6.</label>
    <title>Real Data Analysis</title>
    <p id="P82">In this section, we apply our proposed A-ITR framework to a Type 2 diabetes mellitus (T2DM) observational study. The data set contains 1139 patients. Every patient was assigned one out of four diabetes treatments, which are GLP-1 receptor agonists alone, long-acting insulin alone, intermediate-acting insulin alone, and insulin regimens including short-acting insulin. The endpoint is the change of hemoglobin A1c level before and after the treatment, which is denoted by Δ<italic toggle="yes">HbA</italic>1<italic toggle="yes">c</italic>. In practice, if the treatment works, this value is usually negative (meaning that the hemoglobin A1c level decreases). The smaller Δ<italic toggle="yes">HbA</italic>1<italic toggle="yes">c</italic> is, the more effective the treatment is.</p>
    <p id="P83">We first preprocess the original data set. Among the 19 covariates, we exclude those with a large proportion of missing values and with extremely imbalanced categories. We then impute the rest of them using the predictive mean matching method (<xref rid="R25" ref-type="bibr">Van Buuren, 2018</xref>). There are 10 covariates left after the preprocessing: gender, diabetic retinopathy, diabetic neuropathy, age, weight, body mass index (BMI), baseline hemoglobin A1c level, baseline high-density lipoprotein cholesterol (HDL), baseline low-density lipoprotein cholesterol (LDL), and heart disease.</p>
    <p id="P84">For the outcome variable Δ<italic toggle="yes">HbA</italic>1<italic toggle="yes">c</italic>, we can reduce its variability by subtracting an estimate of its conditional mean <inline-formula><mml:math id="M124" display="inline"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>Δ</mml:mi></mml:mstyle><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi>c</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to make the estimation of <inline-formula><mml:math id="M125" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> more robust (<xref rid="R18" ref-type="bibr">Liu et al., 2018</xref>; <xref rid="R38" ref-type="bibr">Zhou et al., 2017</xref>). Here we use the ordinary least square regression to estimate <inline-formula><mml:math id="M126" display="inline"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>Δ</mml:mi></mml:mstyle><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi>c</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Denote the estimated mean function fitted by regression as <inline-formula><mml:math id="M127" display="inline"><mml:mrow><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we then observe that <inline-formula><mml:math id="M128" display="inline"><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>Δ</mml:mi></mml:mstyle><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi>c</mml:mi><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be positive or negative. We perform an exponential transformation to make it positive, which also justifies the use of ratio <italic toggle="yes">μ<sub>j</sub></italic>/<italic toggle="yes">μ</italic><sub>(1)</sub> to determine the near-optimal recommendation set. Specifically, we let <inline-formula><mml:math id="M129" display="inline"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>Δ</mml:mi></mml:mstyle><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi>c</mml:mi><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>m</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. If we further assume conditional normality for Δ<italic toggle="yes">HbA</italic>1<italic toggle="yes">c</italic> given <italic toggle="yes"><bold>X</bold></italic> and treatment <italic toggle="yes">j</italic>, with mean <inline-formula><mml:math id="M130" display="inline"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>Δ</mml:mi></mml:mstyle><mml:mi>H</mml:mi><mml:mi>b</mml:mi><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi>c</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and equal variance across treatments, then <italic toggle="yes">Y</italic> ∣ (<italic toggle="yes"><bold>x</bold></italic>, <italic toggle="yes">j</italic>) follows a log-normal distribution with mean proportional to exp(<italic toggle="yes">v<sub>j</sub></italic>(<italic toggle="yes"><bold>x</bold></italic>)/5). Then the optimal A-ITR is,
<disp-formula id="FD25"><mml:math id="M25" display="block"><mml:mrow><mml:msup><mml:mi>ϕ</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mfrac><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mtext>min</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∕</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>≤</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="true">}</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mspace width="2.5em"/><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi>j</mml:mi><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>ν</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mn>5</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>c</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
    <p id="P85">In this study, we choose the near-optimality constant <italic toggle="yes">c</italic> = 1.2, so that 5 log <italic toggle="yes">c</italic> ≈ 0.9. This implies that the near-optimal recommendation set is constructed by including all treatments with conditional means Δ<italic toggle="yes">HbA</italic>1<italic toggle="yes">c</italic> within 0.9 of the optimal treatment.</p>
    <p id="P86">We compare the performance of the regression-based method, the two-step method, and the one-step method. For both classification-based methods, we estimate the propensity score <italic toggle="yes">p</italic>(<italic toggle="yes">A</italic>∣<italic toggle="yes"><bold>X</bold></italic>) using logistic regression. Each method leads to a single-valued ITR and a set-valued A-ITR. In <xref rid="T2" ref-type="table">Table 2</xref>, we compare the different recommendations using the 5-fold cross-validated empirical weighted outcome defined in (<xref rid="FD22" ref-type="disp-formula">13</xref>).</p>
    <p id="P87">From <xref rid="T2" ref-type="table">Table 2</xref>, we observe that the one-step method with Gaussian kernel has the best weighted outcome. To illustrate the resultant A-ITR, we split the data into a training set (70%) and a test set (30%). We fit the training set using the one-step method with Gaussian kernel and then construct the recommendation set for patients in the test set. In our analysis, no patient is recommended to take the intermediate-acting insulin and the majority of patients are recommended to choose between the short-acting insulin and GLP-1. Specifically, 55% of patients are recommended the short-acting insulin only, 8% are recommended GLP-1 only, and 24% are recommended to take either one of the two. For the remaining 13% of patients, they are all recommended to take the long-acting insulin, including 1% who are suggested to take either the long-acting insulin or GLP-1, 5% who are suggested to take either the long-acting insulin or the short-acting insulin, and 7% whose only option is the long-acting insulin. We visualize the predicted treatments in <xref rid="F5" ref-type="fig">Figure 5</xref>.</p>
    <p id="P88">From <xref rid="F5" ref-type="fig">Figure 5</xref>, we can see that age and BMI are two useful biomarkers in constructing the near-optimal recommendation set. In fact, by comparing the left panel and the right panel of <xref rid="F5" ref-type="fig">Figure 5</xref>, we observe that BMI behaves like the first principal component (PC1) while age behaves like the second principal component (PC2). <xref rid="F5" ref-type="fig">Figure 5</xref> suggests that for patients without obesity (BMI less than 30), younger patients should take the long-acting insulin while older patients should take GLP-1. The short-acting insulin, on the other hand, serves as an “universal” treatment that many patients can take as an alternative, and is especially effective for overweighted patients.</p>
  </sec>
  <sec id="S17">
    <label>7.</label>
    <title>Statistical Learning Theory</title>
    <p id="P89">In this section, we study the convergence rate of the excess <italic toggle="yes">ℓ</italic>-risk in both linear learning and kernel learning settings. We assume the random vector <italic toggle="yes"><bold>Z</bold></italic> = (<italic toggle="yes"><bold>X</bold></italic>, <italic toggle="yes">A</italic>, <italic toggle="yes">Y</italic>) follows a certain distribution <italic toggle="yes">P</italic> that satisfies Assumption 1. Furthermore, we make an additional assumption.</p>
    <p id="P90"><bold>Assumption 10</bold><italic toggle="yes">There is a constant C</italic> &gt; 0 <italic toggle="yes">such that</italic> ∣<italic toggle="yes">Y</italic>/<italic toggle="yes">p</italic>(<italic toggle="yes">A</italic>∣<italic toggle="yes"><bold>X</bold></italic>)∣ ≤ <italic toggle="yes">C holds. For simplicity, we set C</italic> = 1 <italic toggle="yes">through out this section</italic>.</p>
    <p id="P91">For <italic toggle="yes"><bold>f</bold></italic> and <italic toggle="yes"><bold>f′</bold></italic>, two (<italic toggle="yes">k</italic> − 1)-dimensional functions, and <italic toggle="yes">ℓ</italic>, an increasing, convex and Lipchitz loss function, denote
<disp-formula id="FD26"><mml:math id="M26" display="block"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
    <p id="P92">We call <italic toggle="yes">e<sub>ℓ</sub></italic>(<italic toggle="yes"><bold>f</bold></italic>, <italic toggle="yes"><bold>f</bold></italic>*) the excess <italic toggle="yes">ℓ</italic>-risk of <italic toggle="yes"><bold>f</bold></italic> if <italic toggle="yes"><bold>f</bold></italic>* is optimal within a certain function space <inline-formula><mml:math id="M131" display="inline"><mml:mi mathvariant="script">F</mml:mi></mml:math></inline-formula>.</p>
    <sec id="S18">
      <label>7.1</label>
      <title>Linear Learning</title>
      <p id="P93">We first consider the linear function space, that is, we assume <italic toggle="yes"><bold>f</bold></italic> = (<italic toggle="yes">f</italic><sub>1</sub>,…, <italic toggle="yes">f</italic><sub><italic toggle="yes">k</italic>−1</sub>)<sup><italic toggle="yes">T</italic></sup> with <italic toggle="yes">f<sub>j</sub></italic>(<italic toggle="yes"><bold>x</bold></italic>) = <italic toggle="yes"><bold>x</bold></italic><sup><italic toggle="yes">T</italic></sup><italic toggle="yes"><bold>β</bold><sub>j</sub></italic> for <italic toggle="yes">j</italic> = 1,…, <italic toggle="yes">k</italic> − 1. For simplicity, we assume each covariate is bounded by [0, 1].</p>
      <p id="P94"><bold>Assumption 11</bold><inline-formula><mml:math id="M132" display="inline"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      <p id="P95">Now consider the following function space,
<disp-formula id="FD27"><mml:math id="M27" display="block"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="M133" display="inline"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:msubsup><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. Let <inline-formula><mml:math id="M134" display="inline"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mo>∪</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Define
<disp-formula id="FD28"><mml:math id="M28" display="block"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace linebreak="newline"/><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thickmathspace"/><mml:mtext>and</mml:mtext><mml:mspace linebreak="newline"/><mml:mspace width="1.5em"/><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P96">Theorem 12 gives the convergence rate for the excess <italic toggle="yes">ℓ</italic>-risk <inline-formula><mml:math id="M135" display="inline"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">p</italic> = <italic toggle="yes">p</italic><sub><italic toggle="yes">n</italic></sub>, <italic toggle="yes">s</italic> = <italic toggle="yes">s</italic><sub><italic toggle="yes">n</italic></sub> can grow with <italic toggle="yes">n</italic> as <italic toggle="yes">n</italic> → ∞.</p>
      <p id="P97"><bold>Theorem 12</bold><italic toggle="yes">Let</italic><inline-formula><mml:math id="M136" display="inline"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula><italic toggle="yes">as n</italic> → ∞. <italic toggle="yes">For linear learning, suppose Assumptions 1, 8, 10, and 11 hold. We have</italic>
<disp-formula id="FD29"><mml:math id="M29" display="block"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<italic toggle="yes">almost surely under P, where δ<sub>n</sub></italic> = <italic toggle="yes">e<sub>ℓ</sub></italic>(<italic toggle="yes"><bold>f</bold></italic><sup>(<italic toggle="yes">p<sub>n</sub></italic>,<italic toggle="yes">s<sub>n</sub></italic>)</sup>, <italic toggle="yes"><bold>f</bold></italic><sup>(<italic toggle="yes">p<sub>n</sub></italic>)</sup>).</p>
      <p id="P98">In Theorem 12, <italic toggle="yes">δ<sub>n</sub></italic> stands for the approximation error between the optimal <italic toggle="yes"><bold>f</bold></italic> in <inline-formula><mml:math id="M137" display="inline"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the optimal <italic toggle="yes"><bold>f</bold></italic> in <inline-formula><mml:math id="M138" display="inline"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. So if <italic toggle="yes">s<sub>n</sub></italic> → ∞, <italic toggle="yes">δ<sub>n</sub></italic> converges to 0. On the other hand, the first term <inline-formula><mml:math id="M139" display="inline"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the estimation error between <inline-formula><mml:math id="M140" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> and <italic toggle="yes"><bold>f</bold></italic><sup>(<italic toggle="yes">p<sub>n</sub></italic>,<italic toggle="yes">s<sub>n</sub></italic>)</sup>, and as we increase <italic toggle="yes">s<sub>n</sub></italic>, it becomes larger. The optimal tuning parameter <italic toggle="yes">s<sub>n</sub></italic> is then chosen such that <inline-formula><mml:math id="M141" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>τ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
      <p id="P99">In Theorem 12 we may allow <italic toggle="yes">s<sub>n</sub></italic> → ∞ with an appropriately chosen rate. The reason is that when we include diverging number of covariates, i.e., <italic toggle="yes">p</italic> → ∞, <italic toggle="yes">f</italic><sup>(<italic toggle="yes">p</italic>)</sup> can become more complicated, and thus we need a larger <italic toggle="yes">s<sub>n</sub></italic> to accommodate this change. However, in practice it may not be necessary since the true model usually depends on a finite number of covariates. So we could simplify Theorem 12 if we make the assumption that there is a finite <italic toggle="yes">s</italic>* such that <inline-formula><mml:math id="M142" display="inline"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for all <italic toggle="yes">p</italic>. For example, suppose <inline-formula><mml:math id="M143" display="inline"><mml:mrow><mml:msubsup><mml:mi>f</mml:mi><mml:mi>j</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>f</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>∗</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> for <italic toggle="yes">j</italic> = 1,…, <italic toggle="yes">k</italic> − 1 and all <italic toggle="yes">p</italic> = <italic toggle="yes">m</italic>,…, ∞. Then we can choose <inline-formula><mml:math id="M144" display="inline"><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mo stretchy="false">∣</mml:mo><mml:msubsup><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>∗</mml:mo></mml:msubsup><mml:msup><mml:mo stretchy="false">∣</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      <p id="P100"><bold>Corollary 13</bold><italic toggle="yes">Suppose</italic><italic toggle="yes"><bold>f</bold></italic>* <italic toggle="yes">defined in</italic><xref rid="FD8" ref-type="disp-formula">(6)</xref><italic toggle="yes">only depends on finite many covariates, and that Assumptions 1, 8, 10 and 11 hold. We have</italic><disp-formula id="FD30"><mml:math id="M30" display="block"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>τ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><italic toggle="yes">almost surely under P</italic>.</p>
      <p id="P101">The convergence of excess <italic toggle="yes">ℓ</italic>-risk <inline-formula><mml:math id="M145" display="inline"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in Corollary 13 requires that <italic toggle="yes">p<sub>n</sub></italic> = <italic toggle="yes">o</italic>(<italic toggle="yes">n</italic>). Particularly, when <italic toggle="yes">p<sub>n</sub></italic> grows no faster than <italic toggle="yes">n</italic><sup>1−<italic toggle="yes">r</italic></sup>, where 0 <italic toggle="yes">&lt; <italic toggle="yes">r</italic> &lt;</italic> 1, it can be verified that the error rate is at an order of no greater than <inline-formula><mml:math id="M146" display="inline"><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. This result is consistent with most of the classical asymptotic theory that the dimension of covariates should not be greater than the number of observations. Furthermore, we observe that if <italic toggle="yes">p<sub>n</sub></italic> = <italic toggle="yes">O</italic>(1), then <inline-formula><mml:math id="M147" display="inline"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which is almost <italic toggle="yes">O</italic>(<italic toggle="yes">n</italic><sup>−1/2</sup>).</p>
    </sec>
    <sec id="S19">
      <label>7.2</label>
      <title>Kernel Learning</title>
      <p id="P102">Next we discuss the convergence rate of excess <italic toggle="yes">ℓ</italic>-risk for kernel learning. We denote <italic toggle="yes"><bold>f</bold></italic> = (<italic toggle="yes">f</italic><sub>1</sub>,…, <italic toggle="yes">f</italic><sub><italic toggle="yes">k</italic>−1</sub>)<sup><italic toggle="yes">T</italic></sup> to be a function in a reproducing kernel Hilbert space (RKHS) <italic toggle="yes">H</italic> with kernel function <italic toggle="yes">K</italic>(·, ·). Then by the RKHS theory, we can write <inline-formula><mml:math id="M148" display="inline"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for <italic toggle="yes">j</italic> = 1,…, <italic toggle="yes">k</italic> − 1. To develop the theory for the proposed methods, we still need one more assumption.</p>
      <p id="P103"><bold>Assumption 14</bold><italic toggle="yes">Suppose H is a separable RKHS equipped with kernel function K</italic>(·, ·). <italic toggle="yes">There exists a positive number B, such that K</italic>(<italic toggle="yes"><bold>x</bold></italic>, <italic toggle="yes"><bold>x′</bold></italic>) ≤ <italic toggle="yes">B</italic>
<italic toggle="yes">for any</italic>
<italic toggle="yes"><bold>x</bold></italic>, <inline-formula><mml:math id="M149" display="inline"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      <p id="P104">Assumption 14 states that the RKHS is separable and the kernel function is bounded. This is true for many commonly used kernel functions. For example, for the Gaussian kernel, we may take <italic toggle="yes">B</italic> = 1. We define the function space as
<disp-formula id="FD31"><mml:math id="M31" display="block"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:msup><mml:mspace width="thinmathspace"/><mml:mo stretchy="false">∣</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="M150" display="inline"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>α</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mi mathvariant="bold">K</mml:mi><mml:msub><mml:mstyle mathvariant="italic"><mml:mstyle mathvariant="bold"><mml:mi>α</mml:mi></mml:mstyle></mml:mstyle><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and <bold>K</bold> is the gram matrix. Recall we have included intercepts in the penalty for simplicity. Let <inline-formula><mml:math id="M151" display="inline"><mml:mrow><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mtext>lim</mml:mtext><mml:mrow><mml:mi>n</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>∪</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and define
<disp-formula id="FD32"><mml:math id="M32" display="block"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace linebreak="newline"/><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mfrac><mml:mi>Y</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thickmathspace"/><mml:mtext>and</mml:mtext><mml:mspace linebreak="newline"/><mml:mspace width="1.5em"/><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">〈</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">〉</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p id="P105">The following theorem gives the convergence rate of <inline-formula><mml:math id="M152" display="inline"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> when <italic toggle="yes">s</italic> = <italic toggle="yes">s<sub>n</sub></italic> grows with <italic toggle="yes">n</italic>.</p>
      <p id="P106"><bold>Theorem 15</bold><italic toggle="yes">For RKHS learning, suppose Assumptions 1, 8, 10 and 14 hold. We have</italic><disp-formula id="FD33"><mml:math id="M33" display="block"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><italic toggle="yes">almost surely under P, where δ<sub>n</sub></italic> = <italic toggle="yes">e<sub>ℓ</sub></italic>(<italic toggle="yes"><bold>f</bold></italic><sup>(<italic toggle="yes">n</italic>,<italic toggle="yes">s<sub>n</sub></italic>)</sup>, <italic toggle="yes"><bold>f</bold></italic><sup>(∞)</sup>).</p>
      <p id="P107">Similar to the linear case, there is a trade-off between the approximation error <italic toggle="yes">δ<sub>n</sub></italic> and the estimation error <inline-formula><mml:math id="M153" display="inline"><mml:mrow><mml:mi>O</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in Theorem 15, and the optimal tuning parameter <italic toggle="yes">s<sub>n</sub></italic> is determined roughly when <inline-formula><mml:math id="M154" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
      <p id="P108">Compared to Theorem 12, the excess <italic toggle="yes">ℓ</italic>-risk for RKHS learning seems to yield a faster rate. However, this is not always truly the case due to Assumption 14, which requires a bounded kernel function, and implies a restriction on the number of covariates <italic toggle="yes">p</italic>. For example, for linear kernel we have <italic toggle="yes">K</italic>(<italic toggle="yes"><bold>x</bold></italic>, <italic toggle="yes"><bold>x′</bold></italic>) = <italic toggle="yes"><bold>x</bold></italic><sup><italic toggle="yes">T</italic></sup><italic toggle="yes"><bold>x′</bold></italic> ≤ <italic toggle="yes">p</italic> under Assumption 11. For Assumption 14 to be true, we have to let <italic toggle="yes">p</italic> = <italic toggle="yes">O</italic>(1). In this case both convergence rates are <inline-formula><mml:math id="M155" display="inline"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">∕</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; that of the kernel learning is no faster than that of the linear learning. In general, to obtain a faster rate than that of the linear learning, we need a kernel function that does not increase in <italic toggle="yes">p</italic>, such as the Gaussian kernel.</p>
      <p id="P109">Note that the approximation error <italic toggle="yes">δ<sub>n</sub></italic> converges to 0 as <italic toggle="yes">n</italic> increases, and both the convergence rate of <italic toggle="yes">δ<sub>n</sub></italic> and that of the resulting <inline-formula><mml:math id="M156" display="inline"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> depend on the choice of the kernel. To illustrate the magnitude of <italic toggle="yes">δ<sub>n</sub></italic> and its impact on the excess risk, consider a binary example where <italic toggle="yes">X</italic> ~ Unif(0, 1) and <italic toggle="yes">f</italic>*(<italic toggle="yes">x</italic>) = (1 + <italic toggle="yes">x</italic>)<sup>2</sup>. With the polynomial kernel of degree 2 we have <italic toggle="yes">f</italic><sup>(∞)</sup> = <italic toggle="yes">f</italic>* and <italic toggle="yes">B</italic> = max<sub><italic toggle="yes">x,x′</italic></sub>(1 + <italic toggle="yes">x,x′</italic>)<sup>2</sup> = 2. Given a training set {<italic toggle="yes">x</italic><sub>1</sub>,…, <italic toggle="yes">x<sub>n</sub></italic>}, let <italic toggle="yes">x</italic><sub>(<italic toggle="yes">n</italic>)</sub> be the largest order statistic and define <italic toggle="yes">f</italic><sub>(<italic toggle="yes">n</italic>)</sub>(<italic toggle="yes">x</italic>) = (1 + <italic toggle="yes">xx</italic><sub>(<italic toggle="yes">n</italic>)</sub>)<sup>2</sup>. It can be shown that for any <italic toggle="yes">s<sub>n</sub></italic> ≥ 1, <inline-formula><mml:math id="M157" display="inline"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>⊆</mml:mo><mml:mi mathvariant="script">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> thus <italic toggle="yes">δ<sub>n</sub></italic> = <italic toggle="yes">e<sub>ℓ</sub></italic>(<italic toggle="yes">f</italic><sup>(<italic toggle="yes">n</italic>,<italic toggle="yes">s<sub>n</sub></italic>)</sup>, <italic toggle="yes">f</italic><sup>(∞)</sup>) ≤ <italic toggle="yes">e<sub>ℓ</sub></italic>(<italic toggle="yes">f</italic><sub>(<italic toggle="yes">n</italic>)</sub>, <inline-formula><mml:math id="M158" display="inline"><mml:mrow><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. Note that the difference between <italic toggle="yes">f</italic><sub>(<italic toggle="yes">n</italic>)</sub> and <italic toggle="yes">f</italic><sup>(∞)</sup> is maximized at 1, so <inline-formula><mml:math id="M159" display="inline"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">∣</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∣</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Because the density function of <italic toggle="yes">x</italic><sub>(<italic toggle="yes">n</italic>)</sub> is <italic toggle="yes">nx</italic><sup><italic toggle="yes">n</italic>−1</sup><bold>1</bold><sub>(0,1</sub>), we have <inline-formula><mml:math id="M160" display="inline"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Hence in this example, the order of <italic toggle="yes">δ<sub>n</sub></italic> is at most <italic toggle="yes">O</italic>(<italic toggle="yes">n</italic><sup>−1</sup>), thus <inline-formula><mml:math id="M161" display="inline"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>ℓ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>∞</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">∕</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thinmathspace"/><mml:mi>log</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
  </sec>
  <sec id="S20">
    <label>8.</label>
    <title>Conclusion and Discussions</title>
    <p id="P110">In this work, we propose a new individualized treatment recommendation framework, named A-ITR, that has the capacity to recommend to patients near-optimal treatment options in terms of their clinical outcomes. By adopting the A-ITR, patients have the opportunity to choose the treatment options tailored for their different financial situations, personal preferences, and lifestyle choices. To estimate the optimal A-ITR, we proposed two classification-based methods based on the OWL framework. We also provide a new evaluation criterion suitable for A-ITRs, namely the weighted expected outcome, defined in <xref rid="FD20" ref-type="disp-formula">(12)</xref>. The simulation study shows the usefulness of this new criterion in parameter tuning and model selection.</p>
    <p id="P111">The proposed methods may be subject to misuse when caution is not exercised with regard to the choice of the near-optimality constant <italic toggle="yes">c</italic>. Typically, <italic toggle="yes">c</italic> is chosen based on the physicians’ experience on what defines near optimality for a particular outcome. When possible, the choice could be made more objective by using a measure of the variability of the outcomes for the top treatments. Lastly, recommendations from a variety of <italic toggle="yes">c</italic> values could be presented to the patients for a final selection, as long as any possible sacrifice of the outcome can be clearly explained to the patients.</p>
    <p id="P112">There are several possible directions for future works. Firstly, the current A-ITR estimation is based on the OWL framework, which may be sensitive to the estimated propensity score. In particular, the OWL estimator is known to suffer a large variance in practice (<xref rid="R38" ref-type="bibr">Zhou et al., 2017</xref>). To address this issue, one may consider applying the recently proposed augmented OWL framework (<xref rid="R36" ref-type="bibr">Zhao et al., 2019</xref>; <xref rid="R11" ref-type="bibr">Huang et al., 2019</xref>) to improve the finite sample performance. These methods typically have a double robustness property so that the efficiency of the estimator can be further improved. Secondly, when applying the A-ITR framework in practice, it may be desirable to adjust <italic toggle="yes">c</italic> for different needs or preferences. A natural generalization of the method is to incorporate the patients’ preferences on multiple outcomes into the framework. For example, we can consider A-ITRs with an additional competing outcome as a secondary endpoint (<xref rid="R16" ref-type="bibr">Laber et al., 2014</xref>), or A-ITR with additional safety endpoints formulated as constraints (<xref rid="R26" ref-type="bibr">Wang et al., 2018</xref>). Thirdly, besides the kernel method, we can consider other learning algorithms to estimate <inline-formula><mml:math id="M162" display="inline"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> within the A-ITR framework. Finally, the current method assumes the outcome <italic toggle="yes">Y</italic> is continuous. We can consider nontrivial extensions to other types of outcome such as count outcomes, survival outcomes (<xref rid="R35" ref-type="bibr">Zhao et al., 2014</xref>; <xref rid="R20" ref-type="bibr">Qi et al., 2019</xref>) or dichotomous outcomes (<xref rid="R20" ref-type="bibr">Qi et al., 2019</xref>; <xref rid="R14" ref-type="bibr">Klausch et al., 2018</xref>).</p>
  </sec>
</body>
<back>
  <ack id="S21">
    <title>Acknowledgments</title>
    <p id="P113">The authors would like to thank the action editor Professor Xiaotong Shen and two reviewers for their thoughtful and constructive comments, which greatly improve the quality and the readability of this paper. Ying-Qi Zhao’s work was partially supported by National Institutes of Health (R01DK108073).</p>
  </ack>
  <fn-group>
    <fn id="FN1">
      <label>*.</label>
      <p id="P114">Although class labels are traditionally denoted as <italic toggle="yes">Y</italic> in the classification literature, the class labels are analogous to the treatment options in the ITR problem. Hence we denote the class label as <italic toggle="yes">A</italic> here.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <mixed-citation publication-type="journal"><name><surname>Bartlett</surname><given-names>Peter L</given-names></name> and <name><surname>Wegkamp</surname><given-names>Marten H</given-names></name>. <article-title>Classification with a reject option using a hinge loss</article-title>. <source>Journal of Machine Learning Research</source>, <volume>9</volume>(<month>Aug</month>):<fpage>1823</fpage>–<lpage>1840</lpage>, <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="R2">
      <mixed-citation publication-type="journal"><name><surname>Boyd</surname><given-names>Stephen</given-names></name>, <name><surname>Parikh</surname><given-names>Neal</given-names></name>, <name><surname>Chu</surname><given-names>Eric</given-names></name>, <name><surname>Peleato</surname><given-names>Borja</given-names></name>, <name><surname>Eckstein</surname><given-names>Jonathan</given-names></name>, <etal/><article-title>Distributed optimization and statistical learning via the alternating direction method of multipliers</article-title>. <source>Foundations and Trends® in Machine learning</source>, <volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>122</lpage>, <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="R3">
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Jingxiang</given-names></name>, <name><surname>Fu</surname><given-names>Haoda</given-names></name>, <name><surname>He</surname><given-names>Xuanyao</given-names></name>, <name><surname>Kosorok</surname><given-names>Michael R</given-names></name>, and <name><surname>Liu</surname><given-names>Yufeng</given-names></name>. <article-title>Estimating individualized treatment rules for ordinal treatments</article-title>. <source>Biometrics</source>, <volume>74</volume>(<issue>3</issue>):<fpage>924</fpage>–<lpage>933</lpage>, <year>2018</year>.<pub-id pub-id-type="pmid">29534296</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <mixed-citation publication-type="journal"><name><surname>Chow</surname><given-names>C</given-names></name>. <article-title>On optimum recognition error and reject tradeoff</article-title>. <source>IEEE Transactions on information theory</source>, <volume>16</volume>(<issue>1</issue>):<fpage>41</fpage>–<lpage>46</lpage>, <year>1970</year>.</mixed-citation>
    </ref>
    <ref id="R5">
      <mixed-citation publication-type="journal"><name><surname>Cui</surname><given-names>Yifan</given-names></name>, <name><surname>Zhu</surname><given-names>Ruoqing</given-names></name>, <name><surname>Kosorok</surname><given-names>Michael</given-names></name>, <etal/><article-title>Tree based weighted learning for estimating individualized treatment rules with censored data</article-title>. <source>Electronic journal of statistics</source>, <volume>11</volume>(<issue>2</issue>):<fpage>3927</fpage>–<lpage>3953</lpage>, <year>2017</year>.<pub-id pub-id-type="pmid">29403568</pub-id></mixed-citation>
    </ref>
    <ref id="R6">
      <mixed-citation publication-type="journal"><name><surname>Doubleday</surname><given-names>Kevin</given-names></name>, <name><surname>Zhou</surname><given-names>Hua</given-names></name>, <name><surname>Fu</surname><given-names>Haoda</given-names></name>, and <name><surname>Zhou</surname><given-names>Jin</given-names></name>. <article-title>An algorithm for generating individualized treatment decision trees and random forests</article-title>. <source>Journal of Computational and Graphical Statistics</source>, <volume>27</volume>(<issue>4</issue>):<fpage>849</fpage>–<lpage>860</lpage>, <year>2018</year>.<pub-id pub-id-type="pmid">32523325</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <mixed-citation publication-type="journal"><name><surname>Fung</surname><given-names>Glenn M</given-names></name> and <name><surname>Mangasarian</surname><given-names>Olvi L</given-names></name>. <article-title>Multicategory proximal support vector machine classifiers</article-title>. <source>Machine learning</source>, <volume>59</volume>(<issue>1-2</issue>):<fpage>77</fpage>–<lpage>97</lpage>, <year>2005</year>.</mixed-citation>
    </ref>
    <ref id="R8">
      <mixed-citation publication-type="book"><name><surname>Hastie</surname><given-names>T</given-names></name>, <name><surname>Tibshirani</surname><given-names>R</given-names></name>, and <name><surname>Friedman</surname><given-names>JH</given-names></name>. <source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</source>. <publisher-name>Springer series in statistics. Springer</publisher-name>, <year>2009</year>. ISBN 9780387848846. URL <comment><ext-link xlink:href="https://books.google.com/books?id=eBSgoAEACAAJ" ext-link-type="uri">https://books.google.com/books?id=eBSgoAEACAAJ</ext-link>.</comment></mixed-citation>
    </ref>
    <ref id="R9">
      <mixed-citation publication-type="journal"><name><surname>Herbei</surname><given-names>Radu</given-names></name> and <name><surname>Wegkamp</surname><given-names>Marten H</given-names></name>. <article-title>Classification with reject option</article-title>. <source>Canadian Journal of Statistics</source>, <volume>34</volume>(<issue>4</issue>):<fpage>709</fpage>–<lpage>721</lpage>, <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="R10">
      <mixed-citation publication-type="journal"><name><surname>Hofmann</surname><given-names>Thomas</given-names></name>, <name><surname>Schölkopf</surname><given-names>Bernhard</given-names></name>, and <name><surname>Smola</surname><given-names>Alexander J</given-names></name>. <article-title>Kernel methods in machine learning</article-title>. <source>The annals of statistics</source>, pages <fpage>1171</fpage>–<lpage>1220</lpage>, <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="R11">
      <mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>Xinyang</given-names></name>, <name><surname>Goldberg</surname><given-names>Yair</given-names></name>, and <name><surname>Xu</surname><given-names>Jin</given-names></name>. <article-title>Multicategory individualized treatment regime using outcome weighted learning</article-title>. <source>Biometrics</source>, <volume>75</volume>(<issue>4</issue>):<fpage>1216</fpage>–<lpage>1227</lpage>, <year>2019</year>.<pub-id pub-id-type="pmid">31095722</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <mixed-citation publication-type="journal"><name><surname>Insel</surname><given-names>Thomas R</given-names></name>. <article-title>Translating scientific opportunity into public health impact: a strategic plan for research on mental illness</article-title>. <source>Archives of general psychiatry</source>, <volume>66</volume>(<issue>2</issue>):<fpage>128</fpage>–<lpage>133</lpage>, <year>2009</year>.<pub-id pub-id-type="pmid">19188534</pub-id></mixed-citation>
    </ref>
    <ref id="R13">
      <mixed-citation publication-type="journal"><name><surname>Kallus</surname><given-names>Nathan</given-names></name>. <article-title>Learning to personalize from observational data</article-title>. <source>arXiv preprint arXiv:1608.08925</source>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R14">
      <mixed-citation publication-type="journal"><name><surname>Klausch</surname><given-names>Thomas</given-names></name>, <name><surname>van de Ven</surname><given-names>Peter</given-names></name>, <name><surname>van de Brug</surname><given-names>Tim</given-names></name>, <name><surname>Brakenhoff</surname><given-names>Ruud H</given-names></name>, <name><surname>van de Wiel</surname><given-names>Mark A</given-names></name>, and <name><surname>Berkhof</surname><given-names>Johannes</given-names></name>. <article-title>Estimating bayesian optimal treatment regimes for dichotomous outcomes using observational data</article-title>. <source>arXiv preprint arXiv:1809.06679</source>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R15">
      <mixed-citation publication-type="journal"><name><surname>Laber</surname><given-names>EB</given-names></name> and <name><surname>Zhao</surname><given-names>YQ</given-names></name>. <article-title>Tree-based methods for individualized treatment regimes</article-title>. <source>Biometrika</source>, <volume>102</volume>(<issue>3</issue>):<fpage>501</fpage>–<lpage>514</lpage>, <year>2015</year>.<pub-id pub-id-type="pmid">26893526</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <mixed-citation publication-type="journal"><name><surname>Laber</surname><given-names>Eric B</given-names></name>, <name><surname>Lizotte</surname><given-names>Daniel J</given-names></name>, and <name><surname>Ferguson</surname><given-names>Bradley</given-names></name>. <article-title>Set-valued dynamic treatment regimes for competing outcomes</article-title>. <source>Biometrics</source>, <volume>70</volume>(<issue>1</issue>):<fpage>53</fpage>–<lpage>61</lpage>, <year>2014</year>.<pub-id pub-id-type="pmid">24400912</pub-id></mixed-citation>
    </ref>
    <ref id="R17">
      <mixed-citation publication-type="journal"><name><surname>Lesko</surname><given-names>LJ</given-names></name>. <article-title>Personalized medicine: elusive dream or imminent reality?</article-title><source>Clinical Pharmacology &amp; Therapeutics</source>, <volume>81</volume>(<issue>6</issue>):<fpage>807</fpage>–<lpage>816</lpage>, <year>2007</year>.<pub-id pub-id-type="pmid">17505496</pub-id></mixed-citation>
    </ref>
    <ref id="R18">
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Ying</given-names></name>, <name><surname>Wang</surname><given-names>Yuanjia</given-names></name>, <name><surname>Kosorok</surname><given-names>Michael R</given-names></name>, <name><surname>Zhao</surname><given-names>Yingqi</given-names></name>, and <name><surname>Zeng</surname><given-names>Donglin</given-names></name>. <article-title>Augmented outcome-weighted learning for estimating optimal dynamic treatment regimens</article-title>. <source>Statistics in medicine</source>, <volume>37</volume>(<issue>26</issue>):<fpage>3776</fpage>–<lpage>3788</lpage>, <year>2018</year>.<pub-id pub-id-type="pmid">29873099</pub-id></mixed-citation>
    </ref>
    <ref id="R19">
      <mixed-citation publication-type="journal"><name><surname>Lizotte</surname><given-names>Daniel J</given-names></name> and <name><surname>Laber</surname><given-names>Eric B</given-names></name>. <article-title>Multi-objective markov decision processes for datadriven decision support</article-title>. <source>The Journal of Machine Learning Research</source>, <volume>17</volume>(<issue>1</issue>):<fpage>7378</fpage>–<lpage>7405</lpage>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R20">
      <mixed-citation publication-type="journal"><name><surname>Qi</surname><given-names>Zhengling</given-names></name>, <name><surname>Liu</surname><given-names>Dacheng</given-names></name>, <name><surname>Fu</surname><given-names>Haoda</given-names></name>, and <name><surname>Liu</surname><given-names>Yufeng</given-names></name>. <article-title>Multi-armed angle-based direct learning for estimating optimal individualized treatment rules with various outcomes</article-title>. <source>Journal of the American Statistical Association</source>, pages <fpage>1</fpage>–<lpage>33</lpage>, <year>2019</year>.<pub-id pub-id-type="pmid">34012183</pub-id></mixed-citation>
    </ref>
    <ref id="R21">
      <mixed-citation publication-type="journal"><name><surname>Qian</surname><given-names>Min</given-names></name> and <name><surname>Murphy</surname><given-names>Susan A</given-names></name>. <article-title>Performance guarantees for individualized treatment rules</article-title>. <source>Annals of statistics</source>, <volume>39</volume>(<issue>2</issue>):<fpage>1180</fpage>, <year>2011</year>.<pub-id pub-id-type="pmid">21666835</pub-id></mixed-citation>
    </ref>
    <ref id="R22">
      <mixed-citation publication-type="book"><name><surname>Robins</surname><given-names>James M</given-names></name>. <part-title>Optimal structural nested models for optimal sequential decisions</part-title>. In <source>Proceedings of the second seattle Symposium in Biostatistics</source>, pages <fpage>189</fpage>–<lpage>326</lpage>. <publisher-name>Springer</publisher-name>, <year>2004</year>.</mixed-citation>
    </ref>
    <ref id="R23">
      <mixed-citation publication-type="journal"><name><surname>Schulte</surname><given-names>Phillip J</given-names></name>, <name><surname>Tsiatis</surname><given-names>Anastasios A</given-names></name>, <name><surname>Laber</surname><given-names>Eric B</given-names></name>, and <name><surname>Davidian</surname><given-names>Marie</given-names></name>. <article-title>Q-and a-learning methods for estimating optimal dynamic treatment regimes</article-title>. <source>Statistical science: a review journal of the Institute of Mathematical Statistics</source>, <volume>29</volume>(<issue>4</issue>):<fpage>640</fpage>, <year>2014</year>.<pub-id pub-id-type="pmid">25620840</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <mixed-citation publication-type="journal"><name><surname>Steinwart</surname><given-names>Ingo</given-names></name>, <name><surname>Scovel</surname><given-names>Clint</given-names></name>, <etal/><article-title>Fast rates for support vector machines using gaussian kernels</article-title>. <source>The Annals of Statistics</source>, <volume>35</volume>(<issue>2</issue>):<fpage>575</fpage>–<lpage>607</lpage>, <year>2007</year>.</mixed-citation>
    </ref>
    <ref id="R25">
      <mixed-citation publication-type="book"><name><surname>Van Buuren</surname><given-names>Stef</given-names></name>. <source>Flexible imputation of missing data</source>. <publisher-name>Chapman and Hall/CRC</publisher-name>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R26">
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Yuanjia</given-names></name>, <name><surname>Fu</surname><given-names>Haoda</given-names></name>, and <name><surname>Zeng</surname><given-names>Donglin</given-names></name>. <article-title>Learning optimal personalized treatment rules in consideration of benefit and risk: with an application to treating type 2 diabetes patients with insulin therapies</article-title>. <source>Journal of the American Statistical Association</source>, <volume>113</volume>(<issue>521</issue>):<fpage>1</fpage>–<lpage>13</lpage>, <year>2018</year>.<pub-id pub-id-type="pmid">30034060</pub-id></mixed-citation>
    </ref>
    <ref id="R27">
      <mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>Peng</given-names></name>, <name><surname>Zeng</surname><given-names>Donglin</given-names></name>, and <name><surname>Wang</surname><given-names>Yuanjia</given-names></name>. <article-title>Matched learning for optimizing individualized treatment strategies using electronic health records</article-title>. <source>Journal of the American Statistical Association</source>, pages <fpage>1</fpage>–<lpage>23</lpage>, <year>2019</year>.<pub-id pub-id-type="pmid">34012183</pub-id></mixed-citation>
    </ref>
    <ref id="R28">
      <mixed-citation publication-type="book"><name><surname>Yuan</surname><given-names>Ming</given-names></name>. <part-title>Outcome weighted learning with a reject option</part-title>. In <source>Adaptive Treatment Strategies in Practice: Planning Trials and Analyzing Data for Personalized Medicine</source>, chap-ter 14, pages <fpage>239</fpage>–<lpage>248</lpage>. <publisher-name>SIAM</publisher-name>, <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="R29">
      <mixed-citation publication-type="journal"><name><surname>Yuan</surname><given-names>Ming</given-names></name> and <name><surname>Wegkamp</surname><given-names>Marten</given-names></name>. <article-title>Classification methods with reject option based on convex risk minimization</article-title>. <source>Journal of Machine Learning Research</source>, <volume>11</volume>(<issue>Jan</issue>):<fpage>111</fpage>–<lpage>130</lpage>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R30">
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Baqun</given-names></name>, <name><surname>Tsiatis</surname><given-names>Anastasios A</given-names></name>, <name><surname>Laber</surname><given-names>Eric B</given-names></name>, and <name><surname>Davidian</surname><given-names>Marie</given-names></name>. <article-title>A robust method for estimating optimal treatment regimes</article-title>. <source>Biometrics</source>, <volume>68</volume>(<issue>4</issue>):<fpage>1010</fpage>–<lpage>1018</lpage>, <year>2012</year>.<pub-id pub-id-type="pmid">22550953</pub-id></mixed-citation>
    </ref>
    <ref id="R31">
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>C</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Fu</surname><given-names>H</given-names></name>, <name><surname>He</surname><given-names>X</given-names></name>, <name><surname>Zhao</surname><given-names>Y</given-names></name>, and <name><surname>Liu</surname><given-names>Y</given-names></name>. <article-title>Multicategory outcome weighted margin-based learning for estimating individualized treatment rules</article-title>. <source>Statistica sinica</source>, <year>2018a</year>.</mixed-citation>
    </ref>
    <ref id="R32">
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Chong</given-names></name> and <name><surname>Liu</surname><given-names>Yufeng</given-names></name>. <article-title>Multicategory angle-based large-margin classification</article-title>. <source>Biometrika</source>, <volume>101</volume>(<issue>3</issue>):<fpage>625</fpage>–<lpage>640</lpage>, <year>2014</year>.<pub-id pub-id-type="pmid">26538663</pub-id></mixed-citation>
    </ref>
    <ref id="R33">
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Chong</given-names></name>, <name><surname>Liu</surname><given-names>Yufeng</given-names></name>, and <name><surname>Wu</surname><given-names>Yichao</given-names></name>. <article-title>On quantile regression in reproducing kernel hilbert spaces with the data sparsity constraint</article-title>. <source>The Journal of Machine Learning Research</source>, <volume>17</volume>(<issue>1</issue>):<fpage>1374</fpage>–<lpage>1418</lpage>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R34">
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Chong</given-names></name>, <name><surname>Wang</surname><given-names>Wenbo</given-names></name>, and <name><surname>Qiao</surname><given-names>Xingye</given-names></name>. <article-title>On reject and refine options in multicategory classification</article-title>. <source>Journal of the American Statistical Association</source>, <volume>113</volume>(<issue>522</issue>):<fpage>730</fpage>–<lpage>745</lpage>, <year>2018b</year>.</mixed-citation>
    </ref>
    <ref id="R35">
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Ying-Qi</given-names></name>, <name><surname>Zeng</surname><given-names>Donglin</given-names></name>, <name><surname>Laber</surname><given-names>Eric B</given-names></name>, <name><surname>Song</surname><given-names>Rui</given-names></name>, <name><surname>Yuan</surname><given-names>Ming</given-names></name>, and <name><surname>Kosorok</surname><given-names>Michael Rene</given-names></name>. <article-title>Doubly robust learning for estimating individualized treatment with censored data</article-title>. <source>Biometrika</source>, <volume>102</volume>(<issue>1</issue>):<fpage>151</fpage>–<lpage>168</lpage>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="R36">
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Ying-Qi</given-names></name>, <name><surname>Laber</surname><given-names>Eric B</given-names></name>, <name><surname>Ning</surname><given-names>Yang</given-names></name>, <name><surname>Saha</surname><given-names>Sumona</given-names></name>, and <name><surname>Sands</surname><given-names>Bruce E</given-names></name>. <article-title>Efficient augmentation and relaxation learning for individualized treatment rules using observational data</article-title>. <source>Journal of Machine Learning Research</source>, <volume>20</volume>(<issue>48</issue>):<fpage>1</fpage>–<lpage>23</lpage>, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R37">
      <mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>Yingqi</given-names></name>, <name><surname>Zeng</surname><given-names>Donglin</given-names></name>, <name><surname>John Rush</surname><given-names>A</given-names></name>, and <name><surname>Kosorok</surname><given-names>Michael R</given-names></name>. <article-title>Estimating individualized treatment rules using outcome weighted learning</article-title>. <source>Journal of the American Statistical Association</source>, <volume>107</volume>(<issue>499</issue>):<fpage>1106</fpage>–<lpage>1118</lpage>, <year>2012</year>.<pub-id pub-id-type="pmid">23630406</pub-id></mixed-citation>
    </ref>
    <ref id="R38">
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>Xin</given-names></name> and <name><surname>Kosorok</surname><given-names>Michael R</given-names></name>. <article-title>Causal nearest neighbor rules for optimal treatment regimes</article-title>. <source>arXiv preprint arXiv:1711.08451</source>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R39">
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>Xin</given-names></name>, <name><surname>Mayer-Hamblett</surname><given-names>Nicole</given-names></name>, <name><surname>Khan</surname><given-names>Umer</given-names></name>, and <name><surname>Kosorok</surname><given-names>Michael R</given-names></name>. <article-title>Residual weighted learning for estimating individualized treatment rules</article-title>. <source>Journal of the American Statistical Association</source>, <volume>112</volume>(<issue>517</issue>):<fpage>169</fpage>–<lpage>187</lpage>, <year>2017</year>.<pub-id pub-id-type="pmid">28943682</pub-id></mixed-citation>
    </ref>
    <ref id="R40">
      <mixed-citation publication-type="journal"><name><surname>Zhu</surname><given-names>Ruoqing</given-names></name>, <name><surname>Zhao</surname><given-names>Ying-Qi</given-names></name>, <name><surname>Chen</surname><given-names>Guanhua</given-names></name>, <name><surname>Ma</surname><given-names>Shuangge</given-names></name>, and <name><surname>Zhao</surname><given-names>Hongyu</given-names></name>. <article-title>Greedy outcome weighted tree learning of optimal personalized treatment rules</article-title>. <source>Biometrics</source>, <volume>73</volume> (<issue>2</issue>):<fpage>391</fpage>–<lpage>400</lpage>, <year>2017</year>.<pub-id pub-id-type="pmid">27704531</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Figure 1:</label>
    <caption>
      <p id="P115">Hinge loss (left panel) and bent hinge loss with <italic toggle="yes">c</italic> = 1.5 (right panel). Note the additional slope at <italic toggle="yes">u</italic> = 0 for the bent hinge loss.</p>
    </caption>
    <graphic xlink:href="nihms-1680635-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Figure 2:</label>
    <caption>
      <p id="P116">ITR (top) and A-ITR (bottom) for a toy example given by Bayes rule, the two-step estimator, and one-step estimator. Yellow triangles indicate recommendations with two options, and black squares indicate three options. Both estimators give similar results to the Bayes rule in terms of the single-valued ITR. The two-step method does not provide as good A-ITR results as the one-step method.</p>
    </caption>
    <graphic xlink:href="nihms-1680635-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Figure 3:</label>
    <caption>
      <p id="P117">Comparison between <italic toggle="yes">ϕ</italic>* and <italic toggle="yes">ϕ</italic><sup>+</sup> (left: <italic toggle="yes">ϕ</italic>* or <italic toggle="yes">ϕ</italic><sup>+</sup> with <italic toggle="yes">c</italic> = 1; middle: <italic toggle="yes">ϕ</italic>* with <italic toggle="yes">c</italic> = 1.2; right: <italic toggle="yes">ϕ</italic><sup>+</sup> with <italic toggle="yes">c</italic> = 1.2). Any point in the plot represents (<italic toggle="yes">μ</italic><sub>1</sub>, <italic toggle="yes">μ</italic><sub>2</sub>, <italic toggle="yes">μ</italic><sub>3</sub>) (suppose <italic toggle="yes">Y</italic>*(<italic toggle="yes">j</italic>) ∈ (0, 1)) with the recommendation illustrated by colors. Points in the red, green, and blue regions contain only one treatment; the yellow region contains two treatments; and the purple region includes all three treatments. <inline-formula><mml:math id="M163" display="inline"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mo>∗</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> (unions of red, blue and green regions), and <inline-formula><mml:math id="M164" display="inline"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mo>∗</mml:mo></mml:msubsup><mml:mo>⊆</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> (all but the purple regions).</p>
    </caption>
    <graphic xlink:href="nihms-1680635-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Figure 4:</label>
    <caption>
      <p id="P118">The true ITR and A-ITR for observations in Examples 1 and 2 with recommendation types shown in colors. The orange, blue and green dots indicate that only one single treatment is suggested, which by definition is <italic toggle="yes">R</italic><sub>1</sub>, while the yellow triangles indicate <italic toggle="yes">R</italic><sub>2</sub>, and the black squares indicate <italic toggle="yes">R</italic><sub>3</sub>. The upper panel is Example 1, the lower panel Example 2. The left panel is ITR, and the right panel is A-ITR (<italic toggle="yes">c</italic> = 1.2).</p>
    </caption>
    <graphic xlink:href="nihms-1680635-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Figure 5:</label>
    <caption>
      <p id="P119">Predicted treatment(s) for patients with recommendations given by different colors. The data set is projected on the first two principal components (left panel), and two particular covariates, age and BMI (right panel). Concentric circles indicate multiple treatments recommended to the same patient.</p>
    </caption>
    <graphic xlink:href="nihms-1680635-f0005" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1:</label>
    <caption>
      <p id="P120">Results of the simulation studies with <italic toggle="yes">n</italic> = 1000. In each region, the expected outcome for ITR and the outcome interval for A-ITR (<italic toggle="yes">c</italic> = 1.2) are reported. The empirical weighted outcome defined in <xref rid="FD22" ref-type="disp-formula">(13)</xref> is shown in the “All” column. Each number is averaged over 100 replications. In each case, the best performing method is marked in bold.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th rowspan="2" colspan="2" align="center" valign="middle">Example 1</th>
          <th colspan="4" align="center" valign="middle" rowspan="1"><italic toggle="yes">p</italic> = 5</th>
          <th colspan="4" align="center" valign="middle" rowspan="1"><italic toggle="yes">p</italic> = 10</th>
        </tr>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>1</sub> (70.02%)</th>
          <th align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>2</sub> (24.71%)</th>
          <th align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>3</sub> (5.27%)</th>
          <th align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">All</th>
          <th align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>1</sub> (70.02%)</th>
          <th align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>2</sub> (24.71%)</th>
          <th align="center" valign="middle" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>3</sub> (5.27%)</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">All</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">Reg.</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.49</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.55</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.61</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.51</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.49</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.55</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.61</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.51</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.99, 3.07)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.42, 3.00)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.47, 2.95)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">2.37</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.03, 3.00)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.42, 2.97)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.48, 2.94)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">
            <bold>2.37</bold>
          </td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">2-step</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.34</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.70</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.69</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.45</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.74</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.86</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.71</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.77</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.17, 2.66)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.58, 2.87)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.6, 2.79)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">2.41</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.47, 3.11)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.69, 3.07)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.64, 2.79)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">2.69</td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">1-step</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.15</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.68</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.30</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.43</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.72</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.70</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.51</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.04, 2.36)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.53, 2.73)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.61, 2.75)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">
            <bold>2.26</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.21, 2.74)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.6, 2.88)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.62, 2.78)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">2.45</td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">Bayes</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.92</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.44</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.55</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.08</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.92</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(2.38, 2.75)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(2.45, 2.99)</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.05</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
        </tr>
      </tbody>
      <tbody>
        <tr>
          <th rowspan="2" colspan="2" align="center" valign="middle" style="border-bottom: solid 1px">Example 2</th>
          <th colspan="4" align="center" valign="middle" rowspan="1"><italic toggle="yes">p</italic> = 5</th>
          <th colspan="4" align="center" valign="middle" rowspan="1"><italic toggle="yes">p</italic> = 10</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>1</sub> (56.78%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>2</sub> (41:84%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>3</sub> (1.38%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">All</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>1</sub> (56.78%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>2</sub> (41.84%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>3</sub> (1.38%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">All</th>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">Reg.</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.56</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.61</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.93</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">1.59</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.57</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.94</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.59</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.15, 1.99)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.24, 2.02)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.80, 2.07)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">1.46</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.19, 1.95)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.27, 1.98)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.81, 2.08)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">1.47</td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">2-step</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.29</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.93</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">1.32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.47</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.45</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.95</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.47</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.18, 1.52)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.23, 1.50)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.85, 2.04)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">1.30</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.29, 1.79)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.31, 1.75)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.86, 2.06)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">1.46</td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">1-step</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.29</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.93</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">1.31</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.42</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.42</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.95</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.43</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.18, 1.48)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.24, 1.44)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.85, 2.00)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">
            <bold>1.28</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.25, 1.67)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.29, 1.62)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(1.86, 2.03)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">
            <bold>1.39</bold>
          </td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">Bayes</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.13</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.25</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.87</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">1.19</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.13</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(1.15, 1.45)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(1.71, 2.28)</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">1.16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
        </tr>
      </tbody>
      <tbody>
        <tr>
          <th rowspan="2" colspan="2" align="center" valign="middle" style="border-bottom: solid 1px">Example 3</th>
          <th colspan="4" align="center" valign="middle" rowspan="1"><italic toggle="yes">p</italic> = 5</th>
          <th colspan="4" align="center" valign="middle" rowspan="1"><italic toggle="yes">p</italic> = 10</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>1</sub> (38.63%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>2</sub> (54.13%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>3</sub> (7.24%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">All</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>1</sub> (38.63%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>2</sub> (54.13%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sub>3</sub> (7.24%)</th>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">All</th>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">Reg.</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.17</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.74</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.78</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.52</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.17</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.75</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.76</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.53</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.12, 2.92)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.54, 2.89)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.58, 2.92)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">2.46</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.12, 2.87)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.57, 2.89)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.59, 2.91)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">
            <bold>2.47</bold>
          </td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">2-step</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.37</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.72</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.75</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.59</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.70</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.87</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.76</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.80</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.21, 2.87)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.53, 3.12)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.55, 2.95)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">2.51</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.42, 3.23)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.66, 3.24)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.62, 2.90)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">2.71</td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom: solid 1px" colspan="1">1-step</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.31</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.67</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.73</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.54</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.46</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.74</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.74</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.63</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.18, 2.67)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.54, 2.84)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.62, 2.83)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px; border-right: solid 1px" rowspan="1" colspan="1">
            <bold>2.45</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.24, 2.93)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.58, 2.95)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">(2.64, 2.86)</td>
          <td align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">2.54</td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" colspan="1">Bayes</td>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.11</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.51</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.60</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.36</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">A-ITR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.11</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(2.46, 2.88)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">(2.50, 3.04)</td>
          <td align="center" valign="middle" style="border-right: solid 1px" rowspan="1" colspan="1">2.30</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
          <td align="center" valign="middle" rowspan="1" colspan="1"/>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>Table 2:</label>
    <caption>
      <p id="P121">The mean 5-fold cross validated weighted outcome and its standard error (in the parenthesis) over 100 replications for T2DM data. The method that yields the best result is marked in bold.</p>
    </caption>
    <table frame="hsides" rules="rows">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1"/>
          <th align="center" valign="middle" rowspan="1" colspan="1">ITR</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">A-ITR</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Regression</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.071<break/> (0.010)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.988<break/> (0.006)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Two-step: Linear</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.995<break/> (0.007)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.975<break/> (0.006)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Two-step: Gaussian</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.959<break/> (0.006)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.947<break/> (0.005)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">One-step: Linear</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.150<break/> (0.008)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.033<break/> (0.006)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">One-step: Gaussian</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.939</bold><break/> (0.006)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1"><bold>0.935</bold><break/> (0.007)</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
