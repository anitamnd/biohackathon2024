<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Med Inform Decis Mak</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Med Inform Decis Mak</journal-id>
    <journal-title-group>
      <journal-title>BMC Medical Informatics and Decision Making</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1472-6947</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8422666</article-id>
    <article-id pub-id-type="publisher-id">1621</article-id>
    <article-id pub-id-type="doi">10.1186/s12911-021-01621-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Database</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ParaMed: a parallel corpus for English–Chinese translation in the biomedical domain</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2595-4463</contrib-id>
        <name>
          <surname>Liu</surname>
          <given-names>Boxiang</given-names>
        </name>
        <address>
          <email>jollier.liu@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Liang</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Institute of Deep Learning, Baidu Research, Sunnyvale, USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.4391.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2112 1969</institution-id><institution>School of Electrical Engineering and Computer Science, </institution><institution>Oregon State University, </institution></institution-wrap>Corvallis, USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>6</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>6</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>258</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>10</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>8</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Biomedical language translation requires multi-lingual fluency as well as relevant domain knowledge. Such requirements make it challenging to train qualified translators and costly to generate high-quality translations. Machine translation represents an effective alternative, but accurate machine translation requires large amounts of in-domain data. While such datasets are abundant in general domains, they are less accessible in the biomedical domain. Chinese and English are two of the most widely spoken languages, yet to our knowledge, a parallel corpus does not exist for this language pair in the biomedical domain.</p>
      </sec>
      <sec>
        <title>Description</title>
        <p id="Par2">We developed an effective pipeline to acquire and process an English-Chinese parallel corpus from the New England Journal of Medicine (NEJM). This corpus consists of about 100,000 sentence pairs and 3,000,000 tokens on each side. We showed that training on out-of-domain data and fine-tuning with as few as 4000 NEJM sentence pairs improve translation quality by 25.3 (13.4) BLEU for en<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M2"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq1.gif"/></alternatives></inline-formula>zh (zh<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M4"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq2.gif"/></alternatives></inline-formula>en) directions. Translation quality continues to improve at a slower pace on larger in-domain data subsets, with a total increase of 33.0 (24.3) BLEU for en<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M6"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq3.gif"/></alternatives></inline-formula>zh (zh<inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M8"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq4.gif"/></alternatives></inline-formula>en) directions on the full dataset.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">The code and data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/boxiangliu/ParaMed">https://github.com/boxiangliu/ParaMed</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Machine translation</kwd>
      <kwd>Natural language processing</kwd>
      <kwd>Text mining</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par8">Biomedical translation is used across various life science disciplines. Example applications include translation of clinical trial consent forms, regulatory documents, and interpretation within point-of-care facilities [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. Biomedical translation requires up-to-date domain knowledge and fluency in the source and target languages. Such requirements make it challenging to train qualified translators and costly to generate high-quality translations.</p>
    <p id="Par9">Recent advances in machine translation have demonstrated translation quality arguably on par with professional human translators in select domains [<xref ref-type="bibr" rid="CR3">3</xref>]. Supervised training of machine translation models usually benefits from large amounts of parallel corpora and such effect is the most evident for neural machine translation models. However, the collection and alignment of parallel corpora requires significant time and labor, and such datasets are not available for all domains or language pairs.</p>
    <p id="Par10">Machine translation in the biomedical domain is characterized by a long tail of medical terminology. For example, the Unified Medical Language System (UMLS) developed by the National Institute of Health contains over 2 million names for over 900,000 concepts [<xref ref-type="bibr" rid="CR4">4</xref>], much larger than the set of common English words. Therefore, domain adaptation (training on out-of-domain data and testing on in-domain data) from the general domain to the biomedical domain is challenging.</p>
    <p id="Par11">Two prevailing challenges impact biomedical translation quality when training is done on general-domain data. Biomedical concepts unseen in the general-domain training set (covariate shift) are difficult to translate accurately. Most medical terminologies, such as the word “oncogenesis”, falls into this category. Additionally, concepts that appear in both biomedical domain and general domain but with different semantics present a second challenge. For example, “<italic>primary</italic> care” is translated to Chinese as “
<inline-graphic xlink:href="12911_2021_1621_Figa_HTML.gif" id="d32e347"/>
” whereas “<italic>primary</italic> element” is translated as “
<inline-graphic xlink:href="12911_2021_1621_Figb_HTML.gif" id="d32e353"/>
”.</p>
    <p id="Par12">Various domain adaptation techniques have been developed. Synthetic data generation such as forward and backward translation [<xref ref-type="bibr" rid="CR5">5</xref>] aims to augment out-of-domain parallel data with monolingual in-domain data. Data selection methods aim to select in-domain examples from general domain data [<xref ref-type="bibr" rid="CR6">6</xref>]. Fine-tuning with a small amount of in-domain data has been shown to substantially improve translation quality [<xref ref-type="bibr" rid="CR7">7</xref>].</p>
    <p id="Par13">While the need for biomedical parallel corpora is evident, they are not available for all language pairs. In a literature survey, we found that existing public biomedical parallel corpora are between European Languages (Table <xref rid="Tab1" ref-type="table">1</xref>). The UFAL Medical Corpus covers language pairs from English to Czech, German, Spanish, French, Hungarian, Polish, Romanian and Swedish. The ReBEC dataset [<xref ref-type="bibr" rid="CR8">8</xref>] contains Portuguese and English parallel texts obtained from 1,188 clinical trial documents in the Brazilian Clinical Trials Registry. The 2020 Conference on Machine Translation (WMT20) Biomedical Translation Workshop [<xref ref-type="bibr" rid="CR9">9</xref>] provides training sentence pairs from Medline abstracts between English and Spanish/German/Portuguese/French/Italian/Russian, but only test sentence pairs for English and Chinese. The Khresmoi dataset [<xref ref-type="bibr" rid="CR10">10</xref>] samples 1,500 English sentences from medical documents. These sentences are manually translated into Czech, French, German, Hungarian, Polish, Spanish, and Swedish. The MeSpEn dataset [<xref ref-type="bibr" rid="CR11">11</xref>] contains English and Spanish parallel text collected from IBECS (Spanish Bibliographical Index in Health Sciences), SciELO (Scientific Electronic Library Online), Pubmed and MedlinePlus. Furthermore, we found that existing public English-Chinese parallel corpora are outside of the biomedical domain. The OPUS corpora contain English-Chinese translation from numerous sources such news, speeches, and movie subtitles [<xref ref-type="bibr" rid="CR12">12</xref>]. Perhaps the most closely related is the UM-corpus. It contains parallel text from eight different domains, one of which is science and technology [<xref ref-type="bibr" rid="CR13">13</xref>].<table-wrap id="Tab1"><label>Table 1</label><caption><p>Existing parallel corpus in the biomedical domain contains only European languages</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Corpus</th><th align="left">Language Components</th></tr></thead><tbody><tr><td align="left">UFAL</td><td align="left">cs, de, en, es, fr, hu, pl, ro, sv</td></tr><tr><td align="left">ReBEC</td><td align="left">en, pt</td></tr><tr><td align="left">WMT19</td><td align="left">de, en, es, fr, pt</td></tr><tr><td align="left">Khresmoi</td><td align="left">cs, de, en, es, fr, hu, pl, sv</td></tr><tr><td align="left">MeSpEn</td><td align="left">en, es</td></tr></tbody></table><table-wrap-foot><p>cs: Czech, de: German, en: English, es: Spanish, fr: French, hu: Hungarian, pl: Polish, ro: Romanian, sv: Swedish</p></table-wrap-foot></table-wrap></p>
    <p id="Par14">The New England Journal of Medicine (NEJM) provides Chinese translations of its publications dating back to 2011 (<ext-link ext-link-type="uri" xlink:href="http://nejmqianyan.cn/">http://nejmqianyan.cn/</ext-link>). The website repository currently hosts nearly 2,000 articles, with new articles added weekly. These articles include original research articles, clinical case reports, review articles, commentaries, Journal Watch (viz. article highlights), etc. The articles are translated by professional translators and proofread by members of the NEJM editorial team. For research articles, translations on statistical analysis are proofread by statisticians who are native Chinese speakers.</p>
    <p id="Par15">In this study, we present an English–Chinese parallel corpus in the biomedical domain constructed from NEJM (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). We provide sentence-aligned bitext for 1966 article pairs, totaling 97,441 sentence pairs. Further, we show that training a baseline model with the 2018 Conference on Machine Translation (WMT18) newswire data [<xref ref-type="bibr" rid="CR14">14</xref>] and fine-tuning the model with the ParaMed dataset will significantly improve translation quality over the baseline model, suggesting that the ParaMed dataset will be useful in improving biomedical translation quality.<fig id="Fig1"><label>Fig. 1</label><caption><p>An overview of the ParaMed corpus construction. The input is the NEJM website and the output is a Chinese/English parallel corpus</p></caption><graphic xlink:href="12911_2021_1621_Fig1_HTML" id="MO3"/></fig></p>
    <p id="Par16">Our contributions are the following:<list list-type="bullet"><list-item><p id="Par17">We present the first English-Chinese parallel corpus in the biomedical domain. We only use the <italic>open-access</italic> portion of NEJM articles to comply with their editorial policy.</p></list-item><list-item><p id="Par18">We provide an end-to-end pipeline for constructing parallel corpus using web-crawled text. We compare several software packages for sentence boundary detection and alignment and provide guidelines on their performance in the biomedical domain.</p></list-item><list-item><p id="Par19">We show that fine-tuning on as few as 4,000 sentence pairs from ParaMed can improve translation quality by 25.3 (13.4) BLEU for en<inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M10"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq5.gif"/></alternatives></inline-formula>zh (zh<inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M12"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq6.gif"/></alternatives></inline-formula>en). Translation quality continues to improve at a slower pace on larger datasets, finishing at an increase of 33.0 (24.3) BLEU for en<inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M14"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq7.gif"/></alternatives></inline-formula>zh (zh<inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M16"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq8.gif"/></alternatives></inline-formula>en) on the full dataset.</p></list-item></list></p>
  </sec>
  <sec id="Sec2">
    <title>Construction and content</title>
    <sec id="Sec3">
      <title>Standard approaches to parallel corpus construction</title>
      <p id="Par20">Construction of a sentence-aligned parallel corpus from multilingual websites involves the following steps. <list list-type="order"><list-item><p id="Par21">Documents in desired languages are crawled from multi-lingual websites.</p></list-item><list-item><p id="Par22">Plain texts are extracted from crawled documents and normalized to remove special characters.</p></list-item><list-item><p id="Par23">Documents from both languages are matched according to their contents.</p></list-item><list-item><p id="Par24">Within each document, paragraphs are broken down into individual sentences.</p></list-item><list-item><p id="Par25">Sentences are subsequently aligned into sentence pairs.</p></list-item><list-item><p id="Par26">Aligned sentence pairs are filtered to remove duplicated and low-quality pairs.</p></list-item></list>While the first two steps are well-established engineering tasks, the last four are under active research. For step 3, the 2016 Conference on Machine Translation (WMT16) hosted a shared task for bilingual document alignment [<xref ref-type="bibr" rid="CR15">15</xref>], in which the best entry relied on matching distinct bilingual phrase pairs [<xref ref-type="bibr" rid="CR16">16</xref>]. For step 4, Read <italic>et al.</italic> [<xref ref-type="bibr" rid="CR17">17</xref>] systematically evaluated nine existing tools for sentence boundary detection, among which LingPipe [<xref ref-type="bibr" rid="CR18">18</xref>] and Punkt [<xref ref-type="bibr" rid="CR19">19</xref>] are the top performers in the biomedical domain. Sentence alignment (step 5) is arguably the most challenging step. Compared with document alignment, sentence alignment uses a smaller amount of text but has more permutations. Various methods have been proposed, among which are length-based algorithm [<xref ref-type="bibr" rid="CR20">20</xref>], lexicon-based algorithm [<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR23">23</xref>], and translation-based algorithm [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>], with no consensus on the best performer. For step 6, WMT18 hosted a shared task on parallel corpora filtering [<xref ref-type="bibr" rid="CR26">26</xref>], in which the best performer used dual conditional cross-entropy filtering [<xref ref-type="bibr" rid="CR27">27</xref>].</p>
      <p id="Par27">The NEJM website provides hyperlinks between Chinese and English article pairs, allowing us to skip document alignment (step 3). Otherwise, we followed the best practices outlined therein and adapt them to our project (Fig. <xref rid="Fig2" ref-type="fig">2</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><p>The overall pipeline to construct the ParaMed dataset. NEJM webpages were crawled using Selenium. Various preprocessing steps were carried out to standardize punctuations and remove boilerplate texts. We tested two methods for splitting paragraphs into sentences, and three methods to align English and Chinese sentence into translated pairs. Duplicated sentence pairs were removed at the end</p></caption><graphic xlink:href="12911_2021_1621_Fig2_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>The New England Journal of Medicine Dataset</title>
      <p id="Par28">The Chinese website of the New England Journal of Medicine (<ext-link ext-link-type="uri" xlink:href="https://www.nejmqianyan.cn/">https://www.nejmqianyan.cn/</ext-link>) provides open-access Chinese translations dating back to 2011. All articles were translated sentence for sentence by professional translators, with occasional sentence concatenation and division for fluency. In other words, one English sentence can be split into two or more Chinese sentences and vice versa. Translations were proofread by members of the editorial team and research articles were additionally proofread by statisticians. The Chinese translations are organized chronologically, making the content easy to crawl. Correspondent article pairs are connected via hyperlinks, eliminating the need for document alignment.</p>
    </sec>
    <sec id="Sec5">
      <title>Web crawling</title>
      <p id="Par29">We used Selenium [<xref ref-type="bibr" rid="CR28">28</xref>] to crawl all available Chinese and English articles. While paragraph orderings are maintained across languages, locations of display items—figures, tables, and associated captions—are shuffled. We removed display items to keep content orders identical across English and Chinese. The English NEJM website contains untranslated auxillary contents such as job boards and visual advertisements. We instructed Selenium to ignore auxillary contents as these interjections make sentence alignment challenging. Chinese NEJM translations are cleaner but contain boilerplate sentences such as names of translators. These boilerplate contents were removed during preprocessing.</p>
    </sec>
    <sec id="Sec6">
      <title>Preprocessing</title>
      <p id="Par30">We truecased letters and standardized punctuations for crawled articles with moses [<xref ref-type="bibr" rid="CR29">29</xref>], and subsequently performed stitching and filtering described below.</p>
      <sec id="Sec7">
        <title>Stitching incorrectly split sentences</title>
        <p id="Par31">A single sentence is occasionally split incorrectly due to inappropriate HTML tags. In Chinese articles, we found that sentence breaks can be inserted by mistake before citations and before punctuations. To stitch them, we assigned any text segment consisted only of citations and/or punctuations to its preceding sentence. For English, we noticed that the phrase “open in new tab” always incorrectly break a full sentence into two halves. We concatenated flanking sentences and remove the said phrase.</p>
      </sec>
      <sec id="Sec8">
        <title>Filtering</title>
        <p id="Par32">Because display items and references are untranslated, we filtered out the following content for both languages:<list list-type="bullet"><list-item><p id="Par33">Figures and figure captions</p></list-item><list-item><p id="Par34">Tables and table legends</p></list-item><list-item><p id="Par35">Reference sections</p></list-item></list>Further, we removed content specific for either language. For Chinese, we removed any information about translators. For English, we removed:<list list-type="bullet"><list-item><p id="Par36">Video</p></list-item><list-item><p id="Par37">Interactive graphic</p></list-item><list-item><p id="Par38">Audio interview</p></list-item><list-item><p id="Par39">Visual abstract</p></list-item><list-item><p id="Par40">Quick take (video summary)</p></list-item></list></p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Sentence boundary detection (SBD)</title>
      <p id="Par41">Chinese sentences are concluded by three full-stop punctuations {
<inline-graphic xlink:href="12911_2021_1621_Figc_HTML.gif" id="d32e705"/>
}. These punctuations are used exclusively for sentence separation. Unlike European languages, they do not double as decimal points or other linguistic markers. Further, Chinese quotation marks appear before sentence breaks, making it easy to detect sentence boundaries. Breaking English sentences is more challenging due to punctuation overloading.</p>
      <p id="Par42">Read et al. [<xref ref-type="bibr" rid="CR17">17</xref>] showed that punkt, an unsupervised sentence tokenizer, is a top performer on biomedical corpora. We trained punkt on our ParaMed corpus and used the learned parameters to break sentences. Since punkt does not support the Chinese language, we implemented a custom regex-based tokenizer to split Chinese paragraphs into sentences.</p>
      <p id="Par43">Further, we tested a rule-based system eserix [<xref ref-type="bibr" rid="CR30">30</xref>] designed to process the United Nations parallel corpus and has built-in support for both Chinese and English [<xref ref-type="bibr" rid="CR31">31</xref>]. However, the default rules do not include commonly used abbreviation in biomedical literature, such as the word “Vol.” as an abbreviation for “Volume”. We added rules into the eserix ruleset specifically for the ParaMed corpus.</p>
    </sec>
    <sec id="Sec10">
      <title>Sentence alignment</title>
      <p id="Par44">While many methods have been proposed for sentence alignment, there is no consensus on their performance in the biomedical domain. We focused on three main classes of methods: length-based, lexicon-based, and translation-based methods. We drew one method from each class: Gale-Church (length-based), Microsoft Aligner (lexicon-based), and Bleualign (translation-based). The Gale-Church algorithm finds sentence pairs based on the assumption that the lengths of source and target sentences should be similar [<xref ref-type="bibr" rid="CR20">20</xref>]. The Microsoft Aligner integrates word correspondence with sentence lengths to search for sentence pairs [<xref ref-type="bibr" rid="CR21">21</xref>]. Bleualign compares original and translated texts to search for anchor sentences and subsequently aligns the rest with the Gale-Church algorithm [<xref ref-type="bibr" rid="CR25">25</xref>]. To compare these methods, we established a test set by manually aligning 1,019 sentence pairs from 12 articles. Table <xref rid="Tab2" ref-type="table">2</xref> shows the distribution of alignment types. Nearly 95% of all alignments are one-to-one. An example of one-to-many alignment is shown in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Alignment counts in manually aligned sentence pairs, in which the majority are 1–1 alignments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">zh-en</th><th align="left">Count</th><th align="left">Percent</th></tr></thead><tbody><tr><td align="left">0–1</td><td char="." align="char">10</td><td char="." align="char">1.0%</td></tr><tr><td align="left">1–0</td><td char="." align="char">11</td><td char="." align="char">1.1%</td></tr><tr><td align="left">1–1</td><td char="." align="char">964</td><td char="." align="char">94.6%</td></tr><tr><td align="left">1–2</td><td char="." align="char">17</td><td char="." align="char">1.7%</td></tr><tr><td align="left">2–1</td><td char="." align="char">15</td><td char="." align="char">1.5%</td></tr><tr><td align="left">2–2</td><td char="." align="char">1</td><td char="." align="char">0.1%</td></tr><tr><td align="left">2–3</td><td char="." align="char">1</td><td char="." align="char">0.1%</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>An example 1-to-2 alignment for clause breaking. The red text denotes the English clause corresponding to the first Chinese sentence. Sotagliflozin is cited once in the English sentence, but repeated in two Chinese sentences</p></caption><graphic position="anchor" xlink:href="12911_2021_1621_Tab3_HTML" id="MO6"/></table-wrap></p>
    </sec>
    <sec id="Sec11">
      <title>Post-processing</title>
      <p id="Par45">Medical literature is highly structured. Certain sections such as the abstract, introduction, methods, results and discussion are almost universal across articles. We removed duplicated header and other repeated text with bifixer [<xref ref-type="bibr" rid="CR32">32</xref>].</p>
    </sec>
    <sec id="Sec12">
      <title>Training, development and test split</title>
      <p id="Par46">We selected 2102 sentence pairs from 39 latest articles as the test set and 2036 sentence pairs from the next latest 40 articles as the development set. The remaining 93,303 sentence pairs constitute the training set. To avoid data leakage, all sentences from each articles must be in one of either train, development, and test set.</p>
    </sec>
    <sec id="Sec13">
      <title>Model architecture</title>
      <p id="Par47">We used the transformer model [<xref ref-type="bibr" rid="CR33">33</xref>] in OpenNMT with 6 layers, each with an output size of 512 hidden units [<xref ref-type="bibr" rid="CR34">34</xref>]. We used 8 attention heads and sinusoidal positional embedding. The final hidden feed-forward layer is of size 2,048. In addition, we used an LSTM [<xref ref-type="bibr" rid="CR35">35</xref>] in OpenNMT with 512 hidden units.</p>
    </sec>
    <sec id="Sec14">
      <title>Hardware and training procedure</title>
      <p id="Par48">We trained baseline transformer and LSTM models on the English-Chinese parallel corpus from WMT18 [<xref ref-type="bibr" rid="CR36">36</xref>] consisting about 24.8 million sentence pairs. Sentences are encoded with Byte-Pair Encoding [<xref ref-type="bibr" rid="CR37">37</xref>] with vocabularies of 16,000 tokens for each language. Sentence lengths are capped at 999 tokens, enough to accommodate most sentences. We trained these models on 8 Nvidia TitanX GPUs. For the transformer model, we used the Adam optimizer [<xref ref-type="bibr" rid="CR38">38</xref>] with <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$lr = 2$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq9.gif"/></alternatives></inline-formula>, <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _1 = 0.9$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq10.gif"/></alternatives></inline-formula>, <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _2 = 0.997$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.997</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq11.gif"/></alternatives></inline-formula> and 10,000 warm-up steps. We applied dropout with <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_{d} = 0.1$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq12.gif"/></alternatives></inline-formula> and label smoothing with <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon _{ls} = 0.1$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi mathvariant="italic">ls</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq13.gif"/></alternatives></inline-formula>. The model was trained for 500,000 steps in total. The training procedure took 4.5 days. We fine-tuned the baseline model on ParaMed for 100,000 steps with identical parameters. To establish a second comparison, we trained a transformer model <italic>de novo</italic> on the ParaMed corpus. For the LSTM model, we used the Adam optimizer with <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$lr = 0.001$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi>l</mml:mi><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq14.gif"/></alternatives></inline-formula>, <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _1 = 0.9$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq15.gif"/></alternatives></inline-formula>, <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _2 = 0.999$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq16.gif"/></alternatives></inline-formula>, and label smoothing with <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon _{ls} = 0.1$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi mathvariant="italic">ls</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq17.gif"/></alternatives></inline-formula>.</p>
    </sec>
  </sec>
  <sec id="Sec15">
    <title>Utility and discussion</title>
    <sec id="Sec16">
      <title>Statistics on crawled articles</title>
      <p id="Par49">The earliest official translation by NEJM dates back to 2011, and the number of translated articles has been on the rise year over year. Journal Watch (article highlights) leads in the number of articles, followed by original research and review articles. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the distribution of articles by year and type.<fig id="Fig3"><label>Fig. 3</label><caption><p>Distribution of articles by year and by article type. <bold>A</bold> The number of translated articles has been on the rise since the year 2015. <bold>B</bold> Journal Watch, original articles, and review articles are the top three article types</p></caption><graphic xlink:href="12911_2021_1621_Fig3_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec17">
      <title>Comparing pre- and post-filtered corpora</title>
      <p id="Par50">To remove untranslated text and display items, we manually compared the corresponding Chinese and English articles, identified HTML divisions to be filtered, and implemented a rule-based system to automatically filtered out matching HTML divisions (“Filtering” section). Figure <xref rid="Fig4" ref-type="fig">4</xref> compares the number of Chinese and English paragraphs in each article pair before and after filtering. Before filtering, the number of Chinese paragraphs exceeds that of English for numerous articles, indicated by the grey sub-diagonal cloud. This is due to the various untranslated and boilerplate texts within the articles. The number of English and Chinese paragraphs in each article become closer after filtering.<fig id="Fig4"><label>Fig. 4</label><caption><p>Number of Chinese and English paragraphs are closer post-filtering compared to pre-filtering. Each point represents an English/Chinese article pair</p></caption><graphic xlink:href="12911_2021_1621_Fig4_HTML" id="MO8"/></fig></p>
    </sec>
    <sec id="Sec18">
      <title>Comparing sentence boundary detection algorithms</title>
      <p id="Par51">Because no systematic evaluation exists for sentence boundary detection in the biomedical domain, we tested two popular algorithms, punkt and eserix. To compare the two, we plotted the difference in the number of sentences. Because NEJM articles were translated sentence for sentence, the ideal SBD result should have a difference of zero. We found that difference is smaller for eserix (median difference = 0) than punkt (median difference = 1) and thus used it for downstream analysis (Fig. <xref rid="Fig5" ref-type="fig">5</xref>).<fig id="Fig5"><label>Fig. 5</label><caption><p>The difference in the number of Chinese and English sentences are smaller in eserix than in punkt. Note that the distributions are left-skewed and the medians overlap with the 1st quartiles</p></caption><graphic xlink:href="12911_2021_1621_Fig5_HTML" id="MO9"/></fig></p>
      <p id="Par52">The two most frequent errors made by punkt were the failure to break at citations (Table <xref rid="Tab4" ref-type="table">4</xref>) and erroneous breaks before open parentheses (Table <xref rid="Tab5" ref-type="table">5</xref>). The latter created difficulty for sentence alignment because the Chinese sentence breaks appear after the close parenthesis. Conversely, eserix did not make these mistakes.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Example of a failure to break two English sentences due to a citation (red text). Corresponding Chinese sentences do not suffer from this problem</p></caption><graphic position="anchor" xlink:href="12911_2021_1621_Tab4_HTML" id="MO10"/></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Example of an erroneous break before the blue text. Notice the additional period before the open parenthesis for the English text</p></caption><graphic position="anchor" xlink:href="12911_2021_1621_Tab5_HTML" id="MO11"/></table-wrap></p>
    </sec>
    <sec id="Sec19">
      <title>Comparing sentence alignment algorithms</title>
      <p id="Par53">To find correspondence between English and Chinese sentences, we tested three types of aligners, Gale-Church (length-based), Microsoft Aligner (lexicon-based), and Bleualign (translation-based), using a manually annotated set of 1,019 sentence pairs (“Sentence alignment” section). It should be noted that Bleualign was tested in both unidirectional (zh<inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M36"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq18.gif"/></alternatives></inline-formula>en) and bidirectional (zh<inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\leftrightarrow$$\end{document}</tex-math><mml:math id="M38"><mml:mo stretchy="false">↔</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq19.gif"/></alternatives></inline-formula>en) modes. The unidirectional mode has higher recall but lower precision, whereas the bidirectional mode has lower recall but higher precision. The majority of sentence pairs are one-to-one aligned (Table <xref rid="Tab2" ref-type="table">2</xref>) and the performance of all algorithms degrade significantly for one-to-many and many-to-many alignments. Therefore, we focused on one-to-one alignments for this study. The precision, recall, and F1 scores are shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. The Microsoft Aligner achieved the best F1 score and was used for downstream analysis.<fig id="Fig6"><label>Fig. 6</label><caption><p>Performance of three sentence aligners on the ParaMed corpus. Uni-directional Bleualign uses translations in zh<inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M40"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq20.gif"/></alternatives></inline-formula>en only, whereas bi-directional Bleualign uses translations in both directions, giving it higher precision but lower recall</p></caption><graphic xlink:href="12911_2021_1621_Fig6_HTML" id="MO12"/></fig></p>
    </sec>
    <sec id="Sec20">
      <title>Statistics of the ParaMed corpus</title>
      <p id="Par54">After the aligned sentences cleaned with bifixer [<xref ref-type="bibr" rid="CR39">39</xref>], the final corpus contains 1,966 article pairs with a total of 97,441 sentences. We tokenized English sentences with moses [<xref ref-type="bibr" rid="CR29">29</xref>] and Chinese sentences with Jieba. The English corpus contains 3,028,434 tokens and 55,673 unique tokens. The Chinese corpus contains 2,916,779 tokens and 46,700 unique tokens. All statistics are reported in Table <xref rid="Tab6" ref-type="table">6</xref>.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Statistics of the ParaMed corpus</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Language</th><th align="left">Articles</th><th align="left">Sentences</th><th align="left">Avg. Len.</th><th align="left">Tokens</th><th align="left">Unique Tokens</th></tr></thead><tbody><tr><td align="left">English</td><td align="left" rowspan="2">1,966</td><td align="left" rowspan="2">97,441</td><td align="left">31.08</td><td align="left">3,028,434</td><td align="left">55,673</td></tr><tr><td align="left">Chinese</td><td align="left">29.93</td><td align="left">2,916,779</td><td align="left">46,700</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec21">
      <title>Machine translation performance</title>
      <p id="Par55">To measure the effect that the ParaMed corpus has on medical translation, we compared the baseline transformer model trained on the WMT18 English-Chinese dataset and a fine-tuned model with the ParaMed corpus (“Hardware and training procedure” section). Although translations are evaluated bidirectionally, it should be emphasized that the ParaMed corpus is translated by human translators from English to Chinese and this bias will influence the machine translation quality [<xref ref-type="bibr" rid="CR40">40</xref>].<fig id="Fig7"><label>Fig. 7</label><caption><p>Translation quality improves as the size of the ParaMed corpus increases. The solid lines correspond to the fine-tuned model, whereas the dashed lines correspond to the <italic>de novo</italic> model. The BLUE score for solid line at x = 0 shows the baseline model performance without fine-tuning</p></caption><graphic xlink:href="12911_2021_1621_Fig7_HTML" id="MO13"/></fig></p>
      <p id="Par56">To understand the translation quality as a function of in-domain dataset size, we fine-tuned the transformer model on 4,000, 8,000, 16,000, 32,000, 64,000 and all 93,303 sentence pairs (Fig. <xref rid="Fig7" ref-type="fig">7</xref>). For both zh<inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M42"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq21.gif"/></alternatives></inline-formula>en and en<inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M44"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq22.gif"/></alternatives></inline-formula>zh models, we saw improvement as the number of in-domain sentence pairs increased. The most significant improvement occurred at 4,000 sentence pairs (en<inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M46"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq23.gif"/></alternatives></inline-formula>zh: +25.3 BLEU; zh<inline-formula id="IEq24"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M48"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq24.gif"/></alternatives></inline-formula>en: +13.4 BLEU). Translation quality continued to improve as the size of the dataset grows, albeit at a slower pace. Compared with baseline, the full dataset with 93,303 sentence pairs increased the BLEU score by 33.0 (24.3) points in en<inline-formula id="IEq25"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M50"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq25.gif"/></alternatives></inline-formula>zh (zh<inline-formula id="IEq26"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M52"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq26.gif"/></alternatives></inline-formula>en) directions. We observed similar effects on the LSTM model. The most significant improvement occurred at 4,000 sentence pairs (en<inline-formula id="IEq27"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M54"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq27.gif"/></alternatives></inline-formula>zh: +19.9 BLEU; zh<inline-formula id="IEq28"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M56"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq28.gif"/></alternatives></inline-formula>en: +13.7 BLEU). Compared with the baseline, the full ParaMed dataset increased the BLEU score by 28.1 (23.0) in en<inline-formula id="IEq29"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M58"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq29.gif"/></alternatives></inline-formula>zh (zh<inline-formula id="IEq30"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M60"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq30.gif"/></alternatives></inline-formula>en) directions.</p>
      <p id="Par57">To determine whether the pre-training on WMT18 data is necessary, we trained a <italic>de novo</italic> model using only ParaMed data, which was significantly faster than training a baseline model followed by fine-tuning. Compared with <italic>de novo</italic> training with a transformer model, pre-training on WMT18 baseline plus fine-tuning provided a meaningful boost in translation quality. Such boosts were most evident on small in-domain datasets. With 4,000 sentence pairs, pre-training improved the BLEU score by 34.1 (28.8) points for en<inline-formula id="IEq31"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M62"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq31.gif"/></alternatives></inline-formula>zh (zh<inline-formula id="IEq32"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M64"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq32.gif"/></alternatives></inline-formula>en) directions. The difference decreased as in-domain dataset grew, dropping to 7.9 (6.8) BLEU points for en<inline-formula id="IEq33"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M66"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq33.gif"/></alternatives></inline-formula>zh (zh<inline-formula id="IEq34"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M68"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq34.gif"/></alternatives></inline-formula>en) at the full-set level. A larger in-domain dataset is needed to completely compensate for the gap in translation quality. We observed similar effects for the LSTM model. The fine-tuned model consistently outperformed the <italic>de novo</italic> model for the size of the ParaMed dataset. The gap will likely become smaller as the dataset grows.</p>
    </sec>
    <sec id="Sec22">
      <title>Machine translation error analysis</title>
      <p id="Par58">We showed two examples in this section to illustrate common mistakes made by our models. In the zh<inline-formula id="IEq35"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M70"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq35.gif"/></alternatives></inline-formula>en direction, the phrase “
<inline-graphic xlink:href="12911_2021_1621_Figd_HTML.gif" id="d32e1496"/>
” was correctly translated by the fine-tuned model to “platinum-taxane”, and mistranslated by the baseline model to “Pt-Pseudophyllus” Table <xref rid="Tab7" ref-type="table">7</xref>). The baseline model has not seen the phrase “
<inline-graphic xlink:href="12911_2021_1621_Fige_HTML.gif" id="d32e1502"/>
” during training and thus resulted in incorrect decoding. A similar situation occurred at the phrase “
<inline-graphic xlink:href="12911_2021_1621_Figf_HTML.gif" id="d32e1505"/>
”. The fine-tuned model was able to correctly translate the phrase into bevacizumab, a chemotherapy medication, whereas the baseline model incorrectly decoded the phrase as “Bavaris mono-repellent”.<table-wrap id="Tab7"><label>Table 7</label><caption><p> 
<inline-graphic xlink:href="12911_2021_1621_Figg_HTML.gif" id="d32e1516"/>
and
<inline-graphic xlink:href="12911_2021_1621_Figh_HTML.gif" id="d32e1519"/>
were never seen by the baseline model and were translated incorrectly (red text)</p></caption><graphic position="anchor" xlink:href="12911_2021_1621_Tab7_HTML" id="MO19"/></table-wrap><table-wrap id="Tab8"><label>Table 8</label><caption><p>Olaparib and bevacizumab were not seen by the baseline model and were translated incorrectly (red text)</p></caption><graphic position="anchor" xlink:href="12911_2021_1621_Tab8_HTML" id="MO20"/></table-wrap></p>
      <p id="Par59">Similar situations occurred for the en<inline-formula id="IEq36"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\rightarrow$$\end{document}</tex-math><mml:math id="M72"><mml:mo stretchy="false">→</mml:mo></mml:math><inline-graphic xlink:href="12911_2021_1621_Article_IEq36.gif"/></alternatives></inline-formula>zh direction (Table <xref rid="Tab8" ref-type="table">8</xref>). Two medications, “olaparib” and “bevacizumab”, were correctly translated by the fine-tuned model as “
<inline-graphic xlink:href="12911_2021_1621_Figi_HTML.gif" id="d32e1548"/>
” and “
<inline-graphic xlink:href="12911_2021_1621_Figj_HTML.gif" id="d32e1551"/>
”, but incorrectly translated by the baseline model as “
<inline-graphic xlink:href="12911_2021_1621_Figk_HTML.gif" id="d32e1554"/>
” and “
<inline-graphic xlink:href="12911_2021_1621_Figl_HTML.gif" id="d32e1558"/>
”. Fine-tuning on in-domain data extended the model vocabulary and made it more accurate to decode medical terminology.</p>
    </sec>
  </sec>
  <sec id="Sec23">
    <title>Conclusions</title>
    <p id="Par60">The popularity of neural machine translation models has boosted the need for large datasets. Public releases of many large-scale parallel corpora have significantly improved the quality of machine translation.</p>
    <p id="Par61">Machine translation in the biomedical domain has seen increasing attention in recent years [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR41">41</xref>, <xref ref-type="bibr" rid="CR42">42</xref>]. Biomedical literature is rich in terminology for describing various diseases and biological processes. To add to this challenge, biomedical translation mandates a high standard of translation accuracy because the consequence of misinterpretation in medical decisions can be severe. All these challenges call for the curation of large-scale biomedical parallel corpora.</p>
    <p id="Par62">Despite the need for biomedical parallel text, curation of large-scale corpora have been biased towards European language pairs. Biomedical parallel corpora have been made available across several pairs of European languages, including English, German, Spanish, France, Portuguese, and Polish, to name a few. To our knowledge, there is no English-Chinese parallel corpus in the public domain.</p>
    <p id="Par63">We have presented an English-Chinese parallel dataset in the biomedical domain. We have shown that a baseline model trained on out-of-domain data (WMT18) has limited generalizability to the biomedical domain and that as few as 4000 sentence pairs from the ParaMed dataset substantially improved translation quality. The translation quality continued to improve as the dataset grew. Further, pre-training with the out-of-domain data benefited translation quality, even at the full-set level.</p>
    <p id="Par64">We plan to expand our parallel corpus as New England Journal of Medicine continues to translate more articles. In the future, we would like to include bilingual PubMed abstracts as part of our parallel corpus.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>UMLS</term>
        <def>
          <p id="Par4">Unified Medical Language System</p>
        </def>
      </def-item>
      <def-item>
        <term>WMT</term>
        <def>
          <p id="Par5">Conference on Machine Translation</p>
        </def>
      </def-item>
      <def-item>
        <term>NEJM</term>
        <def>
          <p id="Par6">The New England Journal of Medicine</p>
        </def>
      </def-item>
      <def-item>
        <term>SBD</term>
        <def>
          <p id="Par7">Sentence boundary detection</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank Renjie Zheng, Baigong Zheng, Mingbo Ma, and Kenneth Church for their insights.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>BL conceived the study and performed the analyses, BL and LH wrote the paper. Both authors have read and approved the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>We have no funding source to disclose.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The code and data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/boxiangliu/ParaMed">https://github.com/boxiangliu/ParaMed</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar2" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par65">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bamforth</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Biomedical translation</article-title>
        <source>BMJ</source>
        <year>1998</year>
        <volume>316</volume>
        <issue>7124</issue>
        <fpage>2</fpage>
        <lpage>7124</lpage>
        <pub-id pub-id-type="doi">10.1136/bmj.316.7124.2a</pub-id>
        <pub-id pub-id-type="pmid">9451250</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Das</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Medical interpreters</article-title>
        <source>BMJ</source>
        <year>2009</year>
        <volume>338</volume>
        <fpage>2354</fpage>
        <pub-id pub-id-type="doi">10.1136/bmj.b2354</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Hassan H, Aue A, Chen C, Chowdhary V, Clark J, Federmann C, Huang X, Junczys-Dowmunt M, Lewis W, Li M, et al. Achieving human parity on automatic chinese to english news translation; 2018. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1803.05567">arXiv:1803.05567</ext-link></mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bodenreider</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>The unified medical language system (UMLs): integrating biomedical terminology</article-title>
        <source>Nucleic Acids Res</source>
        <year>2004</year>
        <volume>32</volume>
        <issue>suppl–1</issue>
        <fpage>267</fpage>
        <lpage>270</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkh061</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Sennrich R, Haddow B, Birch A. Improving neural machine translation models with monolingual data; 2015. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1511.06709">arXiv:1511.06709</ext-link></mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Duh K, Neubig G, Sudoh K, Tsukada H. Adaptation data selection using neural language models: experiments in machine translation. In: Proceedings of the 51st annual meeting of the Association for Computational Linguistics (Volume 2: Short Papers); 2013. p. 678–683.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Luong M-T, Manning CD. Stanford neural machine translation systems for spoken language domains. In: Proceedings of the international workshop on spoken language translation; 2015. p. 76–79.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Neves M. A parallel collection of clinical trials in Portuguese and English. In: Proceedings of the 10th workshop on building and using comparable corpora; 2017. p. 36–40</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Bawden R, Di Nunzio G, Grozea C, Unanue I, Yepes A, Mah N, Martinez D, Névéol A, Neves M, Oronoz M, et al. Findings of the WMT 2020 biomedical translation shared task: Basque, Italian and Russian as new additional languages. In: 5th conference on machine translation; 2020.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Dušek O, Hajič J, Hlaváčová J, Libovický J, Pecina P, Tamchyna A, Urešová Z. Khresmoi Summary Translation Test Data 2.0. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University; 2017. <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/11234/1-2122">http://hdl.handle.net/11234/1-2122</ext-link></mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Villegas M, Intxaurrondo A, Gonzalez-Agirre A, Marimon M, Krallinger M. The mespen resource for english-spanish medical machine translation and terminologies: census of parallel corpora, glossaries and term translations. In: Malero M, Krallinger M, Gonzalez-Agirre A, editors. LREC MultilingualBIO: Multilingual Biomedical Text Processing. 2018.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tiedemann</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Parallel data, tools and interfaces in opus</article-title>
        <source>LREC</source>
        <year>2012</year>
        <volume>2012</volume>
        <fpage>2214</fpage>
        <lpage>2218</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Tian L, Wong DF, Chao LS, Quaresma P, Oliveira F, Yi L. Um-corpus: A large english-chinese parallel corpus for statistical machine translation. In: LREC; 2014. p. 1837–1842</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Barrault L, Bojar O, Costa-jussà MR, Federmann C, Fishel M, Graham Y, Haddow B, Huck M, Koehn P, Malmasi S, et al. Findings of the 2019 conference on machine translation (wmt19). In: Proceedings of the fourth conference on machine translation (Volume 2: Shared Task Papers, Day 1); 2019. p. 1–61.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Buck C, Koehn P. Findings of the wmt 2016 bilingual document alignment shared task. In: Proceedings of the first conference on machine translation: Volume 2, Shared Task Papers; 2016. p. 554–563.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Gomes L, Lopes GP. First steps towards coverage-based document alignment. In: Proceedings of the first conference on machine translation: volume 2, Shared Task Papers; 2016. p. 697–702.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Read J, Dridan R, Oepen S, Solberg LJ. Sentence boundary detection: a long solved problem? In: Proceedings of COLING 2012: Posters; 2012. p. 985–994.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Alias-i: Alias-i. <ext-link ext-link-type="uri" xlink:href="http://alias-i.com/lingpipe.%20Accessed:2019-12-10">http://alias-i.com/lingpipe. Accessed:2019-12-10</ext-link> (2008)</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bird</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Loper</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <source>Natural language processing with Python</source>
        <year>2009</year>
        <publisher-loc>Newton</publisher-loc>
        <publisher-name>O’Reilly Media Inc.</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gale</surname>
            <given-names>WA</given-names>
          </name>
          <name>
            <surname>Church</surname>
            <given-names>KW</given-names>
          </name>
        </person-group>
        <article-title>A program for aligning sentences in bilingual corpora</article-title>
        <source>Comput Linguist</source>
        <year>1993</year>
        <volume>19</volume>
        <issue>1</issue>
        <fpage>75</fpage>
        <lpage>102</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Moore RC. Fast and accurate sentence alignment of bilingual corpora. In: Conference of the association for machine translation in the Americas, Springer; 2002 p. 135–144.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Varga D, Halácsy P, Kornai A, Nagy V, Németh L, Trón V. Parallel corpora for medium density languages. Amsterdam studies in the theory and history of linguistic science series 4. 2007;292:247.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Ma X. Champollion: a robust parallel text sentence aligner. In: LREC; 2006. p. 489–492.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Sennrich R, Volk M. Iterative, MT-based sentence alignment of parallel texts. In: Proceedings of the 18th Nordic conference of computational linguistics (NODALIDA 2011); 2011. p. 175–182.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Sennrich R, Volk M. Mt-based sentence alignment for ocr-generated parallel texts. In: The Ninth Conference of the Association for Machine Translation in the Americas (AMTA 2010); 2010.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Koehn P, Khayrallah H, Heafield K, Forcada ML. Findings of the WMT 2018 shared task on parallel corpus filtering. In: Proceedings of the third conference on machine translation: shared task papers; 2018. p. 726–739.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Junczys-Dowmunt M. Dual conditional cross-entropy filtering of noisy parallel corpora; 2018. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1809.00197">arXiv:1809.00197</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Muthukadan B. Selenium with Python. <ext-link ext-link-type="uri" xlink:href="https://selenium-python.readthedocs.io/">https://selenium-python.readthedocs.io/</ext-link>. Accessed 10 Dec 2019</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Koehn P, Hoang H, Birch A, Callison-Burch C, Federico M, Bertoldi N, Cowan B, Shen W, Moran C, Zens R, et al. Moses: open source toolkit for statistical machine translation. In: Proceedings of the 45th annual meeting of the Association for Computational Linguistics Companion Volume proceedings of the demo and poster sessions; 2007. p. 177–180.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Junczys-Dowmunt M. eserix. <ext-link ext-link-type="uri" xlink:href="https://github.com/emjotde/eserix">https://github.com/emjotde/eserix</ext-link>. Accessed 10 Dec 2019</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Ziemski M, Junczys-Dowmunt M, Pouliquen B. The united nations parallel corpus v1. 0. In: Proceedings of the tenth international conference on language resources and evaluation (LREC’16); 2016. p. 3530–3534.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Ramírez-Sánchez G, Zaragoza-Bernabeu J, Bañón M, Ortiz-Rojas S. Bifixer and bicleaner: two open-source tools to clean your parallel data. In: Proceedings of the 22nd annual conference of the European Association for Machine Translation; 2020. p. 291–298. European Association for Machine Translation, Lisboa, Portugal.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser, Ł, Polosukhin I. Attention is all you need. In: Advances in neural information processing systems; 2017. p. 5998–6008.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Klein G, Kim Y, Deng Y, Senellart J, Rush A. OpenNMT: Open-source toolkit for neural machine translation. In: Proceedings of ACL 2017, system demonstrations; 2017. p. 67–72. Association for Computational Linguistics, Vancouver, Canada. <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/P17-4012">https://www.aclweb.org/anthology/P17-4012</ext-link></mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Bojar O, Federmann C, Fishel M, Graham Y, Haddow B, Huck M, Koehn P, Monz C. Findings of the 2018 conference on machine translation (wmt18). In: Proceedings of the third conference on machine translation, Volume 2: Shared Task Papers; 2018. p. 272–307. Association for Computational Linguistics, Belgium, Brussels. <ext-link ext-link-type="uri" xlink:href="http://www.aclweb.org/anthology/W18-6401">http://www.aclweb.org/anthology/W18-6401</ext-link></mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Sennrich R, Haddow B, Birch A. Neural machine translation of rare words with subword units; 2015. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1508.07909">arXiv:1508.07909</ext-link></mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: a method for stochastic optimization; 2014. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Sánchez-Cartagena VM, Bañón M, Rojas SO, Ramírez-Sánchez G. Prompsit’s submission to WMT 2018 parallel corpus filtering shared task. In: Proceedings of the third conference on machine translation: shared task papers; 2018. p. 955–962.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Graham Y, Haddow B, Koehn P. Translationese in machine translation evaluation; 2019. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1906.09833">arXiv:1906.09833</ext-link></mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Bojar O, Chatterjee R, Federmann C, Graham Y, Haddow B, Huck M, Yepes AJ, Koehn P, Logacheva V, Monz C, et al. Findings of the 2016 conference on machine translation. In: Proceedings of the first conference on machine translation: Volume 2, Shared Task Papers; 2016. p. 131–198.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Bojar O, Chatterjee R, Christian F, Yvette G, Barry H, Matthias H, Philipp K, Qun L, Varvara L, Christof M, et al. Findings of the 2017 conference on machine translation (wmt17). In: Second Conference onMachine Translation; 2017. p. 169–214. The Association for Computational Linguistics</mixed-citation>
    </ref>
  </ref-list>
</back>
