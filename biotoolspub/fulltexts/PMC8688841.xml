<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1662-4548</issn>
    <issn pub-type="epub">1662-453X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8688841</article-id>
    <article-id pub-id-type="doi">10.3389/fnins.2021.778488</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MS-MDA: Multisource Marginal Distribution Adaptation for Cross-Subject and Cross-Session EEG Emotion Recognition</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Hao</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1471183/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jin</surname>
          <given-names>Ming</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1553058/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Zhunan</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1553060/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fan</surname>
          <given-names>Cunhang</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1552243/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Jinpeng</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1482965/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>He</surname>
          <given-names>Huiguang</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/10784/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>HwaMei Hospital, University of Chinese Academy</institution>, <addr-line>Ningbo</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Center for Pattern Recognition and Intelligent Medicine, Ningbo Institute of Life and Health Industry, University of Chinese Academy of Sciences</institution>, <addr-line>Ningbo</addr-line>, <country>China</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University</institution>, <addr-line>Hefei</addr-line>, <country>China</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Research Center for Brain-inspired Intelligence and National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Yudan Ren, Northwest University, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Huan Liu, Jiangsu University, China; Linling Li, Shenzhen University, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Jinpeng Li <email>lijinpeng@ucas.ac.cn</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Brain Imaging Methods, a section of the journal Frontiers in Neuroscience</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>07</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>778488</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>27</day>
        <month>10</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright Â© 2021 Chen, Jin, Li, Fan, Li and He.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Chen, Jin, Li, Fan, Li and He</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>As an essential element for the diagnosis and rehabilitation of psychiatric disorders, the electroencephalogram (EEG) based emotion recognition has achieved significant progress due to its high precision and reliability. However, one obstacle to practicality lies in the variability between subjects and sessions. Although several studies have adopted domain adaptation (DA) approaches to tackle this problem, most of them treat multiple EEG data from different subjects and sessions together as a single source domain for transfer, which either fails to satisfy the assumption of domain adaptation that the source has a certain marginal distribution, or increases the difficulty of adaptation. We therefore propose the multi-source marginal distribution adaptation (MS-MDA) for EEG emotion recognition, which takes both domain-invariant and domain-specific features into consideration. First, we assume that different EEG data share the same low-level features, then we construct independent branches for multiple EEG data source domains to adopt one-to-one domain adaptation and extract domain-specific features. Finally, the inference is made by multiple branches. We evaluate our method on SEED and SEED-IV for recognizing three and four emotions, respectively. Experimental results show that the MS-MDA outperforms the comparison methods and state-of-the-art models in cross-session and cross-subject transfer scenarios in our settings. Codes at <ext-link xlink:href="https://github.com/VoiceBeer/MS-MDA" ext-link-type="uri">https://github.com/VoiceBeer/MS-MDA</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>brain-computer interface</kwd>
      <kwd>EEG</kwd>
      <kwd>emotion recognition</kwd>
      <kwd>domain adaptation</kwd>
      <kwd>transfer learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="doi">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="9"/>
      <table-count count="5"/>
      <equation-count count="5"/>
      <ref-count count="51"/>
      <page-count count="14"/>
      <word-count count="9460"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>Emotion as physiological information, unlike widely studied logical intelligence, is central to the quality and range of daily human communications (Dolan, <xref rid="B10" ref-type="bibr">2002</xref>; Tyng et al., <xref rid="B40" ref-type="bibr">2017</xref>). In the human-computer interaction (HCI), emotion is crucial in influencing situation assessment and belief information, from cue identification to situation classification, with decision selection for building a friendly user interface (Jeon, <xref rid="B17" ref-type="bibr">2017</xref>). For example, affective brain-computer interfaces (aBCIs), acting as a bridge between the emotions extracted from the brain and the computer, which has shown potential for rehabilitation and communication (Birbaumer, <xref rid="B4" ref-type="bibr">2006</xref>; Frisoli et al., <xref rid="B14" ref-type="bibr">2012</xref>; Lee et al., <xref rid="B24" ref-type="bibr">2019</xref>). Besides, many studies have shown a strong correlation between emotions and mental illness. Barrett et al. (<xref rid="B3" ref-type="bibr">2001</xref>) studies the relation between emotion differentiation and emotion regulation. Joormann and Gotlib (<xref rid="B21" ref-type="bibr">2010</xref>) finds that depression is strongly associated with the use of emotion regulation strategies. Bucks and Radford (<xref rid="B6" ref-type="bibr">2004</xref>) investigates the identification of non-verbal communicative signals of emotion in people that are suffering from Alzheimer's disease. To quantify emotion, most researchers have focused on using conventional methods such as classifying emotions with facial expression or language (Ekman, <xref rid="B12" ref-type="bibr">1993</xref>). In recent years, with the advantage of reliability, easy accessibility, and high precision, non-invasive BCIs such as electroencephalogram (EEG) are widely used for brain signal acquisition, and analysis of psychological disorders (Sanei and Chambers, <xref rid="B36" ref-type="bibr">2013</xref>; Acharya et al., <xref rid="B1" ref-type="bibr">2015</xref>; Liu et al., <xref rid="B30" ref-type="bibr">2015</xref>; Ay et al., <xref rid="B2" ref-type="bibr">2019</xref>). With EEG signals, many works also investigate the rehabilitation methods for psychological disorders, such as (Jiang et al., <xref rid="B18" ref-type="bibr">2021</xref>) of using spatial information of EEG signals to classify depressions, and Zhang et al. (<xref rid="B46" ref-type="bibr">2020</xref>) proposes a brain functional network framework for major depressive disorder by using the EEG signals. Besides, Hosseinifard et al. (<xref rid="B16" ref-type="bibr">2013</xref>) investigates the non-linear features from EEG signals for classifying depression patients and normal subjects. The flow of an EEG-based affective BCI (aBCI) for emotion recognition is introduced in section 3.1.</p>
    <p>Due to the non-stationary between individual sessions and subjects of EEG signals (Sanei and Chambers, <xref rid="B36" ref-type="bibr">2013</xref>), it is still challenging to get a model that is shareable to different subjects and sessions in EEG-based emotion recognition scenarios, which elicits two scenarios: cross-subject and cross-session (i.e., data collected from the same subject at the same session can be very biased, detailed description is given in section 3.2). Besides, the analysis and classification of the collected signals are time-consuming and labor-intensive, so it is important to make use of the existing labeled data to analyze new signals in the EEG-based BCIs. With this purpose, domain adaptation is widely used in research works. As a sub-field of machine learning, domain adaptation (DA) improves the learning in the unlabeled target domain through the transfer of knowledge from the source domains, which can significantly reduce the number of labeled samples (Pan and Yang, <xref rid="B35" ref-type="bibr">2009</xref>). In practice, we often face the situation that contains multiple source domain data (i.e., data from different subjects or sessions). Due to the shift between domains, adopting DA for EEG data especially when facing multiple sources is difficult. In recent years, the researchers tend to merge all source domains into one single source and then use DA to align the distribution (Source-combine DA in <xref rid="F1" ref-type="fig">Figure 1</xref>) (Zheng and Lu, <xref rid="B50" ref-type="bibr">2016</xref>; Jin et al., <xref rid="B20" ref-type="bibr">2017</xref>; Li et al., <xref rid="B25" ref-type="bibr">2018</xref>, <xref rid="B27" ref-type="bibr">2019a</xref>,<xref rid="B28" ref-type="bibr">b</xref>, <xref rid="B26" ref-type="bibr">2020</xref>; Zheng et al., <xref rid="B48" ref-type="bibr">2018</xref>; Zhao et al., <xref rid="B47" ref-type="bibr">2021</xref>). This simple approach may improve the performance because it expands the training data for the model, but it ignores the non-stationary of each EEG source domain itself and disrupts it (i.e., EEG data of different people obey different marginal distributions), besides, directly merging into one new source domain cannot determine whether its new marginal distribution still obeys EEG-data distribution, thus brings a larger bias.</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p>Two strategies of multi-source domain adaptation. <bold>(A)</bold> is a single-branch strategy while <bold>(B)</bold> is a multi-branch strategy. In <bold>(A)</bold>, all source domains are combined into one new big source and then been used to align distribution with the target domain, while in <bold>(B)</bold>, multiple sources are being aligned at the same time, and are divided into multiple branches to adopt DA with the target domain. In short, <bold>(A)</bold> is one source, one branch with one-to-one DA; <bold>(B)</bold> is multiple sources, multiple branches with one-to-one DA. The figure is best viewed in color.</p>
      </caption>
      <graphic xlink:href="fnins-15-778488-g0001" position="float"/>
    </fig>
    <p>To solve the multi-source domain adaptation problems in EEG-based emotion recognition, we propose a <bold>Multi-Source Marginal Distribution Adaptation</bold> for cross-subject and cross-session EEG emotion recognition (MS-MDA, as illustrated in <xref rid="F1" ref-type="fig">Figure 1</xref>). First, we assume <italic toggle="yes">all the EEG data share low-level features, especially those taken from the same device, the same subject and the same session</italic>. Based on this, we construct a simple common feature extractor to extract domain-invariant features. Then for multiple sources, since <italic toggle="yes">each of them has some specific features</italic>, we pair every single source domain with the target domain to form a branch for one-to-one DA, and align the distribution and extract domain-specific features. After that, a classifier is trained for each branch, and <italic toggle="yes">the final inference is made by these multiple classifiers from multiple branches</italic>. The details of MS-MDA are given in section 4.</p>
    <p>In this study, we make two following contributions:</p>
    <list list-type="order">
      <list-item>
        <p>We proposed and evaluated MS-MDA for EEG-based emotion recognition in a new multi-source adaptation way to avoid disrupting the marginal distributions of EEG data. Extensive experiments demonstrate that our method outperforms the comparison methods on SEED and SEED-IV, and additional experiments also illustrate that our method generalizes well.</p>
      </list-item>
      <list-item>
        <p>Though many works have achieved considerable results, there is no systematic discussion of the normalization operation of EEG data. Thus we design and conduct extensive experiments to investigate the effects of three normalization types (i.e., electrode-wise, sample-wise, and global-wise, details are given in section 5.4), and the order of whether first concatenating multiple sources or normalizing each session individually. To our knowledge, we are the first to investigate the normalization methods for EEG data, which we believe can be taken as a guide for other future works, and be applied to all data in EEG-based datasets and EEG-related domains.</p>
      </list-item>
    </list>
    <p>In the remainder of this paper, we first review related works on domain adaptation in the field of EEG-based emotion recognition in section 2. Section 3 introduces the materials, including the diagram of EEG-based affective BCI with transfer scenarios, datasets and pre-processing methods. The details of MS-MDA are given in section 4, whereas section 5 demonstrates the settings, results, and additional experiments. Section 7 discusses the results of the experiment and our findings, as well as problems and solutions. Finally, section 7 concludes the work and outlines the future extension.</p>
  </sec>
  <sec id="s2">
    <title>2. Related Work</title>
    <p>In recent years, the research of affective computing has become one of the trends of machine learning, neural systems, and rehabilitation study. Among those works, emotions are usually characterized into two types of emotion model: discrete categories (basic emotional states, e.g., happy, sad, neutral; Zheng and Lu, <xref rid="B49" ref-type="bibr">2015</xref>) or continuous values (e.g., in 3D space of arousal, valence, and dominance; Koelstra et al., <xref rid="B23" ref-type="bibr">2011</xref>). With domain adaptation techniques, many works have achieved significant performance in the field of affective computing.</p>
    <p>Zheng and Lu (<xref rid="B50" ref-type="bibr">2016</xref>) first applies Transfer Component Analysis (Pan et al., <xref rid="B34" ref-type="bibr">2010</xref>) and Kernel Principle Analysis based methods on SEED dataset to personalize EEG-based affective models and demonstrates the feasibility of adopting DA in EEG-based aBCIs. Chai et al. proposes adaptive subspace feature matching (Chai et al., <xref rid="B8" ref-type="bibr">2017</xref>) to decrease the marginal distribution discrepancy between two domains, which requires no labeled samples in the target domain. To solve cross-day binary classification, Lin et al. (<xref rid="B29" ref-type="bibr">2017</xref>) extends robust principal component analysis (rPCA) (CandÃ¨s et al., <xref rid="B7" ref-type="bibr">2011</xref>) to their filtering strategy which can capture EEG oscillations of relatively consistent emotional responses. Li et al., different from the above, considering the multi-source scenario, and proposes a Multi-source Style Transfer Mapping (MS-STM) (Li et al., <xref rid="B28" ref-type="bibr">2019b</xref>) framework for cross-subject transfer. They first take a few labeled training data to learn multiple STMs, which are then being used to map the target domain distribution to the space of the sources. Though they consider adding prior information of source-specific features, they do not take the domain-invariant features into consideration, thus losing the low-level information.</p>
    <p>In recent years, with the development of deep learning techniques and its usability, many works of EEG-based decoding with neural networks have been proposed. Jin et al. (<xref rid="B20" ref-type="bibr">2017</xref>) and Li et al. (<xref rid="B25" ref-type="bibr">2018</xref>) adopts deep adaptation network (DAN) (Long et al., <xref rid="B31" ref-type="bibr">2015</xref>) to EEG-based emotion recognition, which takes maximum mean discrepancy (MMD) (Borgwardt et al., <xref rid="B5" ref-type="bibr">2006</xref>) as a measure of the distance between the source and the target domain, and training to reduce it on multiple layers. Extending the original method, Chai et al. proposes subspace alignment auto-encoder (SAAE) (Chai et al., <xref rid="B9" ref-type="bibr">2016</xref>) which first projects both source and target domains into a domain-invariant subspace using an auto-encoder, and then kernel PCA, graph regularization and MMD are used to align the feature distribution. To adapt the joint distribution, Li et al. (<xref rid="B27" ref-type="bibr">2019a</xref>) propose a domain adaptation method for EEG-based emotion recognition by simultaneously adapting marginal distributions and conditional distributions, they also present a fast online instance transfer (FOIT) for improved EEG emotion recognition (Li et al., <xref rid="B26" ref-type="bibr">2020</xref>). Zheng et al. extends SEED dataset to SEED-IV dataset and presents EmotionMeter (Zheng et al., <xref rid="B48" ref-type="bibr">2018</xref>), a multi-modal emotion recognition framework that combines two modalities of eye movements and EEG waves. With the concept of attention-based convolutional neural network (CNN) (Yin et al., <xref rid="B45" ref-type="bibr">2016</xref>), Fahimi et al. (<xref rid="B13" ref-type="bibr">2019</xref>) develops an end-to-end deep CNN for cross-subject transfer and fine-tunes it by using some calibration data from the target domain. To tackle the requirement of amassing extensive EEG data, Zhao et al. (<xref rid="B47" ref-type="bibr">2021</xref>) proposes a plug-and-play domain adaptation method for shortening the calibration time within a minute while maintaining the accuracy. Wang et al. (<xref rid="B43" ref-type="bibr">2021</xref>) present a domain adaptation SPD matrix network (daSPDnet) to help cut the demand of calibration data for BCIs.</p>
    <p>These aBCI works have gained significant improvement in their respective directions, transfer scenarios, and on multiple benchmark databases. However, many of them focus on combing multiple sources into one and adopt one-to-one DA, which ignores the differences of the marginal distribution of different EEG domains (source-combine DA in <xref rid="F1" ref-type="fig">Figure 1</xref>). This operation may compromise the effectiveness of downstream tasks, and although it somehow extends the training data, the trained models do not generalize well enough. Therefore, inspired by Zhu et al. (<xref rid="B51" ref-type="bibr">2019</xref>), a novel multi-source transfer framework, we propose <bold>MS-MDA</bold> (multi-source marginal distribution alignment for EEG-based emotion recognition), which transfers multiple source domains to the target domain separately, thus avoiding the destruction of the marginal distribution of the multiple EEG source domains; and also takes the domain-invariant features into consideration. Due to the sensitivity of the EEG data and intuition, we do not adopt complex networks, but just a combination of few multi-layer perceptrons (MLPs) (Gardner and Dorling, <xref rid="B15" ref-type="bibr">1998</xref>), and thus makes our method computationally efficient, and easy to expand.</p>
  </sec>
  <sec sec-type="materials" id="s3">
    <title>3. Materials</title>
    <sec>
      <title>3.1. Diagram</title>
      <p>The flow of one EEG-based aBCI for emotion recognition is shown in <xref rid="F2" ref-type="fig">Figure 2</xref>, which involves five steps:</p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Stimulating emotions</bold>. The subjects are first stimulated with stimuli that correspond to a target emotion. The most commonly used stimuli are movie clips with sound, which can better stimulate the desired emotion because they mix sound with images and actions. After each clip, self-assessment is also applied for the subject to ensure the consistency of the evoked emotion and the target emotion.</p>
        </list-item>
        <list-item>
          <p><bold>EEG signal acquisition and recording</bold>. The EEG data are collected using the dry electrodes on the BCI, and then be labeled with the target emotion.</p>
        </list-item>
        <list-item>
          <p><bold>Signal pre-processing</bold>. Since the EEG data is a mixture of various kinds of information containing much noise, it is required to pre-process the EEG signal to get cleaner data for subsequent recognition. This step often includes down-sampling, band-pass filtering, temporal filtering, and spatial filtering to improve the signal-to-noise ratio (SNR).</p>
        </list-item>
        <list-item>
          <p><bold>Feature extraction</bold>. In this step, features of the pre-processed signals are extracted in various ways. Most of the current research works are to extract features in the time or frequency domain.</p>
        </list-item>
        <list-item>
          <p><bold>Pattern recognition</bold>. The use of machine learning techniques to classify or regress data according to specific application scenarios.</p>
        </list-item>
      </list>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>The flowchart of EEG-based BCI for emotion recognition. The emotions are first evoked and encoded into EEG data, then the EEG data are pre-processed and extracted to various forms of features for subsequent pattern recognition.</p>
        </caption>
        <graphic xlink:href="fnins-15-778488-g0002" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. Scenarios</title>
      <p>Considering the sensitivity of the EEG, domain adaptation in emotion recognition can be divided into several cases: (1) <bold>Cross-subject transfer</bold>. In one session, new EEG data from a new subject is taken as the target domain, and the rest of existing EEG data from other subjects are taken as the source domains for DA. (2) <bold>Cross-session transfer</bold>. For one subject, data collected in the previous sessions can be used as the source domain for DA, and data collected in the new session are taken as the target domain.</p>
      <p>In our work, since the datasets we evaluate on contains 3 session and 15 subjects (refer to section 3.3 for details), we take the first 2 session data from one subject as the source domains for cross-session transfer, and take the first 14 subjects data from one session as the source domains for cross-subject transfer. The results of cross-session scenarios are averaged over 15 subjects, and the results of cross-subject are averaged over 3 sessions. Standard deviations are also calculated.</p>
    </sec>
    <sec>
      <title>3.3. Datasets</title>
      <p>The database we evaluate on are: <bold>SEED</bold> (Duan et al., <xref rid="B11" ref-type="bibr">2013</xref>; Zheng and Lu, <xref rid="B49" ref-type="bibr">2015</xref>) and <bold>SEED-IV</bold> (Zheng et al., <xref rid="B48" ref-type="bibr">2018</xref>), both are established by the BCMI laboratory led by Prof. Bao-Liang Lu from Shanghai Jiao Tong University.</p>
      <p>The SEED database contains emotion-related EEG signals that are evoked by 15 film clips (with positive, neutral, and negative emotions) from 15 subjects (7 males and 8 females, with average age of 23.27) with 3 sessions each. The signals are recorded by a 62-channel ESI neuroscan system.</p>
      <p>The SEED-IV is an evolution of SEED, which contains 3 sessions, each has 15 subjects and 24 film clips. Comparing to the SEED with EEG signals only, this database also includes eye movement features recorded by SMI eye-tracking glasses.</p>
    </sec>
    <sec>
      <title>3.4. Pre-processing</title>
      <p>After collecting EEG raw data, pre-processing on signals and feature extractions will be adopted. For both SEED and SEED-IV, to increase the SNR, the raw EEG signals are first down-sampled to a 200 Hz sampling rate, then been processed with a band-pass filter between 1 Hz to 75 Hz. After that, features are then being extracted.</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mtext>DE</mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>-</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:msub>
                    <mml:mrow>
                      <mml:mo>â«</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>X</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mstyle>
                <mml:mi>f</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo class="qopname">log</mml:mo>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mrow>
                    <mml:mi>f</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo>]</mml:mo>
                </mml:mrow>
                <mml:mtext>d</mml:mtext>
                <mml:mi>x</mml:mi>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>Recent works extract features from EEG data on the time domain, frequency domain, and time-frequency domain. Among them, Differential Entropy (DE) as in (1), has the ability to distinguish patterns from different bands (Soleymani et al., <xref rid="B37" ref-type="bibr">2015</xref>), thus we choose to take DE features as the input data of our model. For SEED and SEED-IV, extracted DE features at five frequency bands of delta (1â4 Hz), theta (4â8 Hz), alpha (8â14 Hz), and gamma (31â50 Hz) are provided.</p>
      <p>One data from one subject in one session for both databases is in the form of channel (62) Ã trial (15 for SEED, 24 for SEED-IV) Ã band (5), we then merge the channel with the band, and the form becomes trial Ã 310 (62 Ã 5). For SEED, 15 trials contain 3394 samples in total for each session. For SEED-IV, 24 trials contain 851/832/822 samples for three sessions, respectively. In the end, all data are formed into 3394 Ã 310 (SEED), or 851/832/822 Ã 310 (SEED-IV) with corresponding generated label vectors in the form of 3,394 Ã 1, or 851/832/822 Ã 1.</p>
    </sec>
  </sec>
  <sec id="s4">
    <title>4. Method</title>
    <p>For simplicity of demonstration, we list the symbols and their definition in <xref rid="T1" ref-type="table">Table 1</xref> that will be used in the following sections.</p>
    <table-wrap position="float" id="T1">
      <label>Table 1</label>
      <caption>
        <p>Notation table.</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th valign="top" align="left" rowspan="1" colspan="1">
              <bold>Symbol</bold>
            </th>
            <th valign="top" align="left" rowspan="1" colspan="1">
              <bold>Definition</bold>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">X</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Instance set (matrix)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">Y</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Label set (matrix)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">S</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Source domain</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">T</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Target domain</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">N</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">number of source domains</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">Q</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Common feature</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">R</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Domain-specific feature</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Å¶</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Predicted label (matrix)</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Ï, Î¦</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Mapping function</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <inline-formula>
                <mml:math id="M2" overflow="scroll">
                  <mml:mi mathvariant="script">H</mml:mi>
                </mml:math>
              </inline-formula>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Reproducing kernel Hilbert space</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">CFE</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Common feature extractor</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">DSFE</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Domain-specific feature extractor</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">DSC</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Domain-specific classifier</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">x</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Feature vector</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">y</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Label vector</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">q</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Feature vector after CFE</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <italic toggle="yes">r</italic>
            </td>
            <td valign="top" align="left" rowspan="1" colspan="1">Feature vector after DSFE</td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Å·</td>
            <td valign="top" align="left" rowspan="1" colspan="1">Predicted label vector</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>Given a set of pre-existing EEG data and a newly collected EEG data, our goal is to learn a model Ï that is trained on these multiple independent source domain data using DA, and thus has a better prediction on the newly collected data than simply combining the existed data into one source domain. The architecture of the proposed method is illustrated in <xref rid="F3" ref-type="fig">Figure 3</xref>.</p>
    <fig position="float" id="F3">
      <label>Figure 3</label>
      <caption>
        <p>The architecture of our proposed method. Our network consists of a common feature extractor, domain-specific feature extractor, and domain-specific classifier. For each source domain, a branch of DSFE and DSC is conducted for pair-wise domain adaptation. The model receives multiple source domains and leverages their knowledge to transfer to the target domain. Three loss in red squares stands for MMD loss, discrepancy loss, and classification loss which are described in section 4.</p>
      </caption>
      <graphic xlink:href="fnins-15-778488-g0003" position="float"/>
    </fig>
    <p>As shown in the <xref rid="F3" ref-type="fig">Figure 3</xref>, the input to the MS-MDA are <italic toggle="yes">N</italic> independent source domain data <inline-formula><mml:math id="M3" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">X</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">Y</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and a target domain data {<bold>X</bold><sup><italic toggle="yes">T</italic></sup>}, and then these data are fed into a common feature extractor module to get the domain-invariance features <inline-formula><mml:math id="M4" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">Q</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and {<bold>Q</bold><sup><italic toggle="yes">T</italic></sup>}. Then for each domain-specific feature extractor, extracted common features <inline-formula><mml:math id="M5" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">Q</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> will be fed into one branch with {<bold>Q</bold><sup><italic toggle="yes">T</italic></sup>} and get their domain-specific features: <inline-formula><mml:math id="M6" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M7" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, and on top of that, the MMD value is calculated, which is a measure of the distance of the current source and the target domain. Next, the target domain features <inline-formula><mml:math id="M8" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and all the source domain features <inline-formula><mml:math id="M9" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">R</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> extracted from the last step will get to the domain-specific classifiers to get the corresponding classification predictions: <inline-formula><mml:math id="M10" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">Y</mml:mtext></mml:mstyle></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="M11" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">Y</mml:mtext></mml:mstyle></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, then the results of the source domain are taken to calculate the classification loss. Since the target domain will be fed into all the source domain classifiers, multiple target domain predictions are generated. These predictions are taken to calculate the discrepancy loss. In the end, the average of these target-domain predictions is taken as the output of the model. Details of these modules are given below.</p>
    <sec>
      <title>4.1. Common Feature Extractor</title>
      <p>Common feature extractor in the MS-MDA is used to map the source and target domain data from the original feature spaces to a common sharing latent space, and then common representations of all domains are extracted. This module can help to extract some low-level domain-invariant features.</p>
    </sec>
    <sec>
      <title>4.2. Domain-Specific Feature Extractor</title>
      <p>Domain-specific feature extractor follows the Common Feature Extractor (CFE). After obtaining the features of all domains, we set up <italic toggle="yes">N</italic> single fully connected layers to correspond to <italic toggle="yes">N</italic> source domains. For each pair of source and target domain, we map the data to a unique latent space via the corresponding Domain-specific Feature Extractor (DSFE), respectively, and then obtain the domain-specific features in each branch. To apply DA and bring the two domains close in the latent space, we choose the MMD to estimate the distance between these two domains. MMD is widely used in the DA and can be formulated in (2). In the process of training, MMD loss is decreased to narrow the source domain and the target domain in the feature space, which helps make better predictions for the target domain. This module aims to learn multiple domain-specific features.</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M12" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mo class="qopname">MMD</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mo stretchy="true">(</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>X</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>S</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>,</mml:mo>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>X</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo stretchy="true">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mo>â</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>N</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>S</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mstyle displaystyle="true">
                      <mml:munderover>
                        <mml:mrow>
                          <mml:mo>â</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mi>N</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>S</mml:mi>
                            </mml:mrow>
                          </mml:msup>
                        </mml:mrow>
                      </mml:munderover>
                    </mml:mstyle>
                    <mml:mo>Î¦</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true">(</mml:mo>
                      <mml:mrow>
                        <mml:msubsup>
                          <mml:mrow>
                            <mml:mtext>x</mml:mtext>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>S</mml:mi>
                          </mml:mrow>
                        </mml:msubsup>
                      </mml:mrow>
                      <mml:mo stretchy="true">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>-</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>N</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mstyle displaystyle="true">
                      <mml:munderover>
                        <mml:mrow>
                          <mml:mo>â</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mi>N</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                            </mml:mrow>
                          </mml:msup>
                        </mml:mrow>
                      </mml:munderover>
                    </mml:mstyle>
                    <mml:mo>Î¦</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true">(</mml:mo>
                      <mml:mrow>
                        <mml:msubsup>
                          <mml:mrow>
                            <mml:mtext>x</mml:mtext>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>T</mml:mi>
                          </mml:mrow>
                        </mml:msubsup>
                      </mml:mrow>
                      <mml:mo stretchy="true">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>â</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi mathvariant="script">H</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:mrow>
                </mml:msubsup>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
    </sec>
    <sec>
      <title>4.3. Domain-Specific Classifier</title>
      <p>Domain-specific classifier uses the features extracted from the DSFE to predict the result. In Domain-specific Classifier (DSC), there are <italic toggle="yes">N</italic> single softmax classifiers that correspond to each source domain. For each classifier training, we choose cross-entropy to estimate the classification loss, as shown in (3). Besides, since there are <italic toggle="yes">N</italic> classifiers in this module, and these <italic toggle="yes">N</italic> classifiers are trained on <italic toggle="yes">N</italic> source domains, if their predictions are simply averaged as the final result, the variance will be high, especially when the target domain samples are at the decision boundary, which will have a significant negative impact on the results. To reduce this variance, a metric called discrepancy loss is introduced to make the predictions of the <italic toggle="yes">N</italic> classifiers converge, which is shown in (4). The average of the predictions of the <italic toggle="yes">N</italic> classifiers is taken as the final result.</p>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M13" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="script">L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mi>s</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover>
                    <mml:mrow>
                      <mml:mo>â</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>N</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="bold">
                      <mml:mtext>E</mml:mtext>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:mo>~</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>X</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mi>J</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="true">(</mml:mo>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mover accent="true">
                          <mml:mrow>
                            <mml:mstyle mathvariant="bold">
                              <mml:mtext>Y</mml:mtext>
                            </mml:mstyle>
                          </mml:mrow>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>,</mml:mo>
                    <mml:msubsup>
                      <mml:mrow>
                        <mml:mstyle mathvariant="bold">
                          <mml:mtext>Y</mml:mtext>
                        </mml:mstyle>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mo stretchy="true">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <disp-formula id="E4">
        <label>(4)</label>
        <mml:math id="M14" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="script">L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>d</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>s</mml:mi>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover>
                    <mml:mrow>
                      <mml:mo>â</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>â </mml:mo>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>N</mml:mi>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mstyle mathvariant="bold">
                      <mml:mtext>E</mml:mtext>
                    </mml:mstyle>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:mo>~</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>X</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:mstyle mathvariant="bold">
                            <mml:mtext>Y</mml:mtext>
                          </mml:mstyle>
                        </mml:mrow>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:mo>-</mml:mo>
                  <mml:msubsup>
                    <mml:mrow>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:mstyle mathvariant="bold">
                            <mml:mtext>Y</mml:mtext>
                          </mml:mstyle>
                        </mml:mrow>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>In summary, MS-MDA accepts <italic toggle="yes">N</italic> source domain EEG data and one target domain EEG data, and then includes a common feature extractor to get <italic toggle="yes">N</italic> source domain features and one target domain feature. Next, <italic toggle="yes">N</italic> domain-specific feature extractors are used to pairwise compute the MMD loss of one individual source with the target domain and extract their domain-specific features. Finally, a domain-specific classifier is used to do the classification task, which also calculates the classification loss of the <italic toggle="yes">N</italic> classifiers using the features, with the discrepancy loss of the <italic toggle="yes">N</italic> classifiers for the features of the target domain data after the previous <italic toggle="yes">N</italic> feature extractors.</p>
      <disp-formula id="E5">
        <label>(5)</label>
        <mml:math id="M15" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi mathvariant="script">L</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="script">L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mi>s</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:mi>Î±</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="script">L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>M</mml:mi>
                    <mml:mi>M</mml:mi>
                    <mml:mi>D</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:mi>Î²</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi mathvariant="script">L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>d</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>s</mml:mi>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                </mml:msub>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>The training is based on the (5) and following the algorithm as shown in Algorithm 1. For the three losses, minimizing MMD loss can get domain-invariant features for each pair of the source and target domains; minimizing classification loss will bring more accurate classifiers for predicting the source domain data; minimizing discrepancy loss will get more convergent multiple classifiers. The setting of Î± is illustrated in section 5.1 and we also investigate different settings of Î² in section 5.5.1.</p>
      <table-wrap position="float" id="TA1">
        <label>Algorithm 1</label>
        <caption>
          <p>Overview of MS-MDA.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <tbody>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>Input:</bold>
              </td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Â Â Â Iteration <inline-formula><mml:math id="M16" overflow="scroll"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula>, source domain data <inline-formula><mml:math id="M17" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">X</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mstyle class="text"><mml:mtext mathvariant="bold">Y</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and target domain data {<bold>X</bold><sup><italic toggle="yes">T</italic></sup>}</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">1: <bold>for</bold> t = 1,..., <inline-formula><mml:math id="M18" overflow="scroll"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula>
<bold>do</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">2: Â Â Â Take <italic toggle="yes">m</italic> samples <inline-formula><mml:math id="M19" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>from source domains and <inline-formula><mml:math id="M20" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> from target domain.</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">3: Â Â Â <inline-formula><mml:math id="M21" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>C</mml:mi><mml:mi>F</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">4: Â Â Â <inline-formula><mml:math id="M22" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>â</mml:mo><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5: Â Â Â <inline-formula><mml:math id="M23" overflow="scroll"><mml:mi mathvariant="script">L</mml:mi></mml:math></inline-formula><sub><italic toggle="yes">MMD</italic></sub> â (2) â <italic toggle="yes">DSFE</italic></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">6: Â Â Â <inline-formula><mml:math id="M24" overflow="scroll"><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Å·</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>Å·</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>â</mml:mo><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">7: Â Â Â <inline-formula><mml:math id="M25" overflow="scroll"><mml:mi mathvariant="script">L</mml:mi></mml:math></inline-formula><sub><italic toggle="yes">cls</italic></sub>, <inline-formula><mml:math id="M26" overflow="scroll"><mml:mi mathvariant="script">L</mml:mi></mml:math></inline-formula><sub><italic toggle="yes">disc</italic></sub> â (3)(4) â <italic toggle="yes">DSC</italic></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">8: Â Â Â Update model by minimizing the total loss </td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">9: <bold>end for</bold></td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">10: <bold>return</bold> {Å¶<sup><italic toggle="yes">T</italic></sup>};</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>Output:</bold>
              </td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Â Â Â Prediction of target domain data, {Å¶<sup><italic toggle="yes">T</italic></sup>};</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec id="s5">
    <title>5. Experiments</title>
    <p>In this section, we describe experiments settings and results in classifying of emotions on two datasets SEED and SEED-IV, with the normalization study with three types and two orders to the EEG data for domain adaptation. Besides, we also conduct some exploratory experiments in addition to the evaluation of our proposed methods and comparison methods.</p>
    <sec>
      <title>5.1. Implementation Details</title>
      <p>As mentioned in the section 4, there are many details in the three modules of MS-MDA. First, for the Common Feature Extractor (CFE), since we do not take raw data (<italic toggle="yes">i. e</italic>. EEG signals) but the extracted DE features as vectors, complex deep models such as deep convolutional neural networks are not suitable for this module, thus we choose 3-layer MLP for simplicity which reduces feature dimensions from 310-dimension (62 Ã 5, channel Ã band) to 64-D. In CFE, every linear layer is followed by a LeakyReLU (Xu et al., <xref rid="B44" ref-type="bibr">2015</xref>) layer. We also evaluate the effort of the ReLU (Nair and Hinton, <xref rid="B33" ref-type="bibr">2010</xref>) activation function, but due to the sensitivity of the EEG data, much information would be lost if using ReLU since the value less than zero would be dropped, so we choose LeakyReLU as a compromise. Next, for both domain-specific feature extractor (DSFE) and domain-specific classifier (DSC), there is a single linear which reduces 64-D to 32-D and 32-D to the corresponding number of categories (3 for SEED, 4 for SEED-IV), respectively. In DSFE, same as the settings in CFE, a LeakyReLU layer is followed after the linear layer, while in DSC, there is only one linear layer without any activation function. The network is trained using an Adam (Kingma and Ba, <xref rid="B22" ref-type="bibr">2014</xref>) optimizer with an initial learning rate of 0.01, and train for 200 epoch. The batch size we choose is 256, which means we take 256 samples from each domain in every iteration (we also evaluate different settings of batch size and epoch in section 5.5). The whole model is trained under the (5), for domain adaptation loss, we choose MMD as the metric of the distance between two domains in the feature space (CORAL loss has a similar effect). As for the discrepancy loss, L1 regularization is being used, we also evaluate this loss in section 5.5. Besides, we dynamically adjust the Î± coefficients to achieve the effect of focusing on the classification results first, and then start aligning MMD and the convergence between the classifiers (<inline-formula><mml:math id="M27" overflow="scroll"><mml:mi>Î±</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>10</mml:mn><mml:mo>*</mml:mo><mml:mi>i</mml:mi><mml:mo>/</mml:mo><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>). As for the training data, we take the DE features and reform one sample to a 310-D vector as illustrated in the section 3.4. Before feeding into the model, we normalize all the data in electrode-wise, refer to section 5.4 for details.</p>
    </sec>
    <sec>
      <title>5.2. Results</title>
      <p>Experiment results of comparison methods and our proposed method on SEED and SEED-IV are listed in <xref rid="T2" ref-type="table">Table 2</xref>, all the hyper-parameters are the same, except for those results taken directly from the original papers. It should be noticed that since many previous works do not make their codes public available, we then customize the comparison methods (in the deep learning domain adaptation field) that are described in their papers with our settings, and also including some typical deep learning domain adaptation models for better comparison (DDC Tzeng et al., <xref rid="B41" ref-type="bibr">2014</xref>, DCORAL Sun and Saenko, <xref rid="B38" ref-type="bibr">2016</xref>). The results indicate that our method largely outperforms the comparison methods in most transfer scenarios. For SEED dataset, our method has a minimum of 7 and 3% improvement in cross-session and cross-subject scenarios, respectively. While in SEED-IV dataset, our method has a minimum of 7 and 18% for two transfer scenarios. The results also show that our method outperforms comparison methods significantly in cross-subject, the reason for that may be that in the cross-subject scenario, the number of sources is 14, much bigger than the number of 2 in cross-session, and thus maximizes the effect of taking multiple sources as multiple individuals in domain adaptation rather than concatenating them.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Comparison results on SEED and SEED-IV of accuracy.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Dataset</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cross-session</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cross-subject</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="middle" align="left" rowspan="7" colspan="1">SEED</td>
              <td valign="top" align="left" rowspan="1" colspan="1">DDC<xref rid="TN1" ref-type="table-fn"><sup>â </sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.53 Â± 6.83</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.99 Â± 3.23</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DAN<xref rid="TN1" ref-type="table-fn"><sup>â </sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.93 Â± 7.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">65.84 Â± 2.25</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DAN (Li et al., <xref rid="B25" ref-type="bibr">2018</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">-</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.81 Â± 8.56</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DCORAL<xref rid="TN1" ref-type="table-fn"><sup>â </sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">76.86 Â± 7.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">66.29 Â± 4.53</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DANN (Li et al., <xref rid="B25" ref-type="bibr">2018</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">-</td>
              <td valign="top" align="center" rowspan="1" colspan="1">79.19 Â± 13.14</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">PPDA (Zhao et al., <xref rid="B47" ref-type="bibr">2021</xref>)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">-</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.70 Â± 7.10</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">MS-MDA (Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>88.56 Â± 7.80</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>89.63 Â± 6.79</bold>
              </td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="4" colspan="1">SEED-IV</td>
              <td valign="top" align="left" rowspan="1" colspan="1">DDC<xref rid="TN1" ref-type="table-fn"><sup>â </sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">57.63 Â± 11.28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">37.41 Â± 6.36</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DAN<xref rid="TN1" ref-type="table-fn"><sup>â </sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.14 Â± 12.79</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32.44 Â± 9.02</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DCORAL<xref rid="TN1" ref-type="table-fn"><sup>â </sup></xref></td>
              <td valign="top" align="center" rowspan="1" colspan="1">44.63 Â± 11.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">37.43 Â± 3.08</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MS-MDA (Ours)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.43 Â± 15.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">59.34 Â± 5.48</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="TN1">
            <label>â </label>
            <p><italic toggle="yes">stands for self-reproduced methods. The best results are shown in bold</italic>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>5.3. Ablation Study</title>
      <p>To understand the effect of each module in the MS-MDA, we remove them one at a time and evaluate the performance of the ablated model, the results are shown in <xref rid="T3" ref-type="table">Table 3</xref>. The first row of SEED and SEED-IV shows the performance of the full model (the same as in <xref rid="T2" ref-type="table">Table 2</xref>). The second row ablates the MMD loss in the training process, which makes the model focuses only on the classification loss and discrepancy loss. The significant drop compared to the full model indicates the important effect of domain adaptation. Notice that even the results without MMD loss are better than many comparison methods, showing the importance of taking multiple sources as multiple individuals during training. The third row of taking out the discrepancy loss shows that this loss will affect the performance but the impact is minimal, the reason is that we want this discrepancy loss to be the icing on the cake rather than having a dominant effect on the model. The fourth row only considers the classification loss, thus reduces losses (2) and (4).</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Ablation study of MS-MDA on SEED and SEED-IV of accuracy.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Dataset</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cross-session</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cross-subject</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="middle" align="left" rowspan="4" colspan="1">SEED</td>
              <td valign="top" align="left" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">Ours full</td>
              <td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">
                <bold>88.56 Â± 7.80</bold>
              </td>
              <td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">
                <bold>89.63 Â± 6.79</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">w/o MMD loss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">82.20 Â± 14.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">67.65 Â± 17.65</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">w/o disc. loss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">86.27 Â± 9.14</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.27 Â± 5.70</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">w/o MMD + disc. loss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">83.19 Â± 9.69</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.48 Â± 5.76</td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="4" colspan="1">SEED-IV</td>
              <td valign="top" align="left" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">Ours full</td>
              <td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">61.43 Â± 15.71</td>
              <td valign="top" align="center" style="border-bottom: thin solid #000000;" rowspan="1" colspan="1">
                <bold>59.34 Â± 5.48</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">w/o MMD loss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.51 Â± 18.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">49.71 Â± 5.82</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">w/o disc. loss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">61.63 Â± 17.62</td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.37 Â± 14.38</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">w/o MMD + disc. loss</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>62.66 Â± 16.07</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">55.81 Â± 4.17</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic toggle="yes">The best results are shown in bold</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Besides, we also conduct experiments to demonstrate how the performance measures change along with the number of source numbers. Since the amount of experiments is massive if following a full cross-validation rule, we simply take the first subject as the source domain to test one branch experiment, the first two subjects on two-branch, the first three subjects on three-branch, etc. The results are plotted in <xref rid="F4" ref-type="fig">Figure 4</xref>. From the figure, it is obvious that with the improvement of source number, our algorithm has a large improvement in the accuracy.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>The performance measures change along with the number of source numbers.</p>
        </caption>
        <graphic xlink:href="fnins-15-778488-g0004" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>5.4. Normalization</title>
      <p>During the experiments, we also find that different normalization to data can significantly impact the outcomes, and also the order of whether first concatenating multiple sources or first normalize each session individually. Thus we design diagrams and conduct extensive experiments to investigate the effects of different normalization strategies on the input data, <italic toggle="yes">i. e.</italic>, extracted feature vectors from two datasets. Since we have reformed the origin 4-D matrices (session Ã channel Ã trial Ã band) into 3-D matrices [session Ã trial Ã (channel*band)], for each session, there is a 2-D matrix of trial Ã 310. Following the common machine learning normalization approaches and the prior knowledge and intuition of EEG data (<italic toggle="yes">i. e.</italic>, the data acquired by the same electrode are more consistent with the same distribution), the normalization methods to these 2-D matrices can be categorized into three, as shown in <xref rid="F5" ref-type="fig">Figure 5</xref>. Besides, since we also take the multi-source situation into consideration, the order of normalization may also influence the performance.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>Evaluation of normalization methods and orders. <bold>(A)</bold> The dark blue box stands for the sample-wise normalization, while the light blue box stands for the electrode-wise normalization. The big gray box stands for the global-wise normalization. <bold>(B)</bold> There are two normalization orders (involving two operations for single source and all sources): concatenate all sources first, and normalize global-wise, or normalize every single source first, and concatenate them all. Small blue matrices are data from different subjects, (1) is an operation for one single source, and (2) is an operation for all sources. In order A, (1) in the figure stands for the normalization, and (2) stands for the concatenate (i.e., normalize every single source first, and then concatenate them all). In order B: (1) stands for concatenating while (2) is for normalization (i.e., concatenate every single source into one big source domain first, and then normalize this domain).</p>
        </caption>
        <graphic xlink:href="fnins-15-778488-g0005" position="float"/>
      </fig>
      <p>We evaluate three normalization methods and two normalization orders on SEED and SEED-IV with our proposed method MS-MDA and representative domain adaptation model DAN (Long et al., <xref rid="B31" ref-type="bibr">2015</xref>). The results are listed in <xref rid="T4" ref-type="table">Table 4</xref>. In all three sets, the normalization of electrode-wise outperforms the other three normalization types significantly. Comparing DAN<sup>1</sup> with DAN<sup>2</sup>, the results indicate that the first normalization order of normalizing the data first and then concatenating them is better. In the third set of MS-MDA, we find that all the results of four normalization types are better than those in the first and second sets, and the improvement is significant. Row w/o normalization in MS-MDA, for example, has a top of 47% improvement, which also indicates the generalization of our proposed method in different normalization types, and the positive effects of taking multiple sources as individual branches for DA.</p>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>Normalization study of MS-MDA and DAN of accuracy.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Model</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Normalization type</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>SEED</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>SEED-IV</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cross-session</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cross-subject</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cross-session</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Cross-subject</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="middle" align="left" rowspan="4" colspan="1">DAN<sup>1</sup></td>
              <td valign="top" align="left" rowspan="1" colspan="1">W/o normalization</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.96 Â± 0.23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.91 Â± 0.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.23 Â± 4.78</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.15 Â± 1.31</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Electrode-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>79.93 Â± 7.06</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>65.84 Â± 2.25</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>55.14 Â± 12.79</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>32.44 Â± 9.02</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sample-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">52.51 Â± 11.92</td>
              <td valign="top" align="center" rowspan="1" colspan="1">51.77 Â± 12.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.34 Â± 2.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32.03 Â± 4.24</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Global-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">54.02 Â± 9.29</td>
              <td valign="top" align="center" rowspan="1" colspan="1">49.12 Â± 12.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.72 Â± 6.46</td>
              <td valign="top" align="center" rowspan="1" colspan="1">29.31 Â± 2.40</td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="4" colspan="1">DAN<sup>2</sup></td>
              <td valign="top" align="left" rowspan="1" colspan="1">W/o normalization</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.96 Â± 0.23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.91 Â± 0.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.23 Â± 4.78</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.15 Â± 1.31</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Electrode-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>79.78 Â± 6.97</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>62.57 Â± 5.31</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>52.18 Â± 10.53</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>34.26 Â± 7.98</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sample-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">52.51 Â± 11.92</td>
              <td valign="top" align="center" rowspan="1" colspan="1">51.77 Â± 12.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">27.34 Â± 2.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32.03 Â± 4.24</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Global-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.07 Â± 10.50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50.22 Â± 3.66</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.01 Â± 7.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.77 Â± 2.08</td>
            </tr>
            <tr>
              <td valign="middle" align="left" rowspan="4" colspan="1">MS-MDA</td>
              <td valign="top" align="left" rowspan="1" colspan="1">W/o normalization</td>
              <td valign="top" align="center" rowspan="1" colspan="1">80.62 Â± 12.22</td>
              <td valign="top" align="center" rowspan="1" colspan="1">60.92 Â± 3.58</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30.11 Â± 6.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">29.64 Â± 7.26</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Electrode-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>88.56 Â± 7.80</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>89.63 Â± 6.79</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>62.66 Â± 16.07</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>59.34 Â± 5.48</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sample-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.84 Â± 13.72</td>
              <td valign="top" align="center" rowspan="1" colspan="1">74.09 Â± 5.79</td>
              <td valign="top" align="center" rowspan="1" colspan="1">34.71 Â± 10.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30.25 Â± 5.20</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Global-wise</td>
              <td valign="top" align="center" rowspan="1" colspan="1">81.80 Â± 12.75</td>
              <td valign="top" align="center" rowspan="1" colspan="1">78.89 Â± 10.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.87 Â± 10.99</td>
              <td valign="top" align="center" rowspan="1" colspan="1">31.88 Â± 7.73</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic toggle="yes">DAN<sup>1</sup> stands for the order A while DAN<sup>2</sup> stands for the order B. The best results are shown in bold</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>5.5. Additions</title>
      <sec>
        <title>5.5.1. Coefficient Study</title>
        <p>After multiple sets of experiments, we find that easy to control the MMD loss and it plays an influential role in the training as shown in <xref rid="T3" ref-type="table">Table 3</xref>. However, for the disc. loss, it remains many problems. Adding this loss to the model too early will affect the overall effect, and too late will lose the impact of learning convergence. Too large a weight would cause the training to focus on convergence, thus the few correct ones might follow the many incorrect ones; too small may not have enough influence on the model. Also, for better use and simplicity mentioned earlier, we do not make many tests on the Î², but simply compared the effects on only a few sets of Î², and the results are shown in <xref rid="T5" ref-type="table">Table 5</xref>. From which we can see that compared to row one (w/o disc. loss), introducing discrepancy loss increases the performance in most cases, especially when training for the whole process in cross-subject for SEED-IV. We then choose the weight of 0.01 and training discrepancy loss for the whole process according to the results.</p>
        <table-wrap position="float" id="T5">
          <label>Table 5</label>
          <caption>
            <p>Performance of MS-MDA on SEED and SEED-IV of accuracy with different settings of Î².</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="middle" align="left" rowspan="2" colspan="1">
                  <bold>Training percentage</bold>
                </th>
                <th valign="middle" align="left" rowspan="2" colspan="1">
                  <bold>Weight</bold>
                </th>
                <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                  <bold>SEED</bold>
                </th>
                <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                  <bold>SEED-IV</bold>
                </th>
              </tr>
              <tr>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Cross-session</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Cross-subject</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Cross-session</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>Cross-subject</bold>
                </td>
              </tr>
            </thead>
            <tbody>
              <tr style="border-bottom: thin solid #000000;">
                <td valign="top" align="center" colspan="2" rowspan="1">W/o disc. loss</td>
                <td valign="top" align="center" rowspan="1" colspan="1">86.27 Â± 9.14</td>
                <td valign="top" align="center" rowspan="1" colspan="1">87.27 Â± 5.70</td>
                <td valign="top" align="center" rowspan="1" colspan="1">61.63 Â± 17.62</td>
                <td valign="top" align="center" rowspan="1" colspan="1">55.37 Â± 14.38</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">0.2</td>
                <td valign="top" align="left" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">86.94 Â± 8.68</td>
                <td valign="top" align="center" rowspan="1" colspan="1">86.93 Â± 8.24</td>
                <td valign="top" align="center" rowspan="1" colspan="1">64.07 Â± 14.36</td>
                <td valign="top" align="center" rowspan="1" colspan="1">55.21 Â± 6.30</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">0.2</td>
                <td valign="top" align="left" rowspan="1" colspan="1">0.1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">86.99 Â± 9.23</td>
                <td valign="top" align="center" rowspan="1" colspan="1">87.37 Â± 7.64</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>64.75 Â± 13.36</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">53.15 Â± 11.88</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">0.2</td>
                <td valign="top" align="left" rowspan="1" colspan="1">0.01</td>
                <td valign="top" align="center" rowspan="1" colspan="1">86.87 Â± 9.30</td>
                <td valign="top" align="center" rowspan="1" colspan="1">87.09 Â± 8.02</td>
                <td valign="top" align="center" rowspan="1" colspan="1">64.04 Â± 13.72</td>
                <td valign="top" align="center" rowspan="1" colspan="1">50.54 Â± 15.59</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">0.2</td>
                <td valign="top" align="left" rowspan="1" colspan="1">0.001</td>
                <td valign="top" align="center" rowspan="1" colspan="1">86.91 Â± 9.35</td>
                <td valign="top" align="center" rowspan="1" colspan="1">86.93 Â± 8.24</td>
                <td valign="top" align="center" rowspan="1" colspan="1">64.14 Â± 13.88</td>
                <td valign="top" align="center" rowspan="1" colspan="1">53.25 Â± 9.55</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">1</td>
                <td valign="top" align="left" rowspan="1" colspan="1">1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">85.58 Â± 8.19</td>
                <td valign="top" align="center" rowspan="1" colspan="1">63.42 Â± 2.15</td>
                <td valign="top" align="center" rowspan="1" colspan="1">61.88 Â± 16.71</td>
                <td valign="top" align="center" rowspan="1" colspan="1">57.34 Â± 9.07</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">1</td>
                <td valign="top" align="left" rowspan="1" colspan="1">0.1</td>
                <td valign="top" align="center" rowspan="1" colspan="1">85.80 Â± 10.05</td>
                <td valign="top" align="center" rowspan="1" colspan="1">81.13 Â± 11.19</td>
                <td valign="top" align="center" rowspan="1" colspan="1">62.42 Â± 15.99</td>
                <td valign="top" align="center" rowspan="1" colspan="1">56.34 Â± 10.18</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">1</td>
                <td valign="top" align="left" rowspan="1" colspan="1">0.01</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>88.56 Â± 7.80</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>89.63 Â± 6.79</bold>
                </td>
                <td valign="top" align="center" rowspan="1" colspan="1">62.66 Â± 16.07</td>
                <td valign="top" align="center" rowspan="1" colspan="1">
                  <bold>59.34 Â± 5.48</bold>
                </td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">1</td>
                <td valign="top" align="left" rowspan="1" colspan="1">0.001</td>
                <td valign="top" align="center" rowspan="1" colspan="1">86.36 Â± 8.68</td>
                <td valign="top" align="center" rowspan="1" colspan="1">84.84 Â± 3.49</td>
                <td valign="top" align="center" rowspan="1" colspan="1">64.41 Â± 17.58</td>
                <td valign="top" align="center" rowspan="1" colspan="1">48.01 Â± 8.66</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <p><italic toggle="yes">Training percentage stands for when to add this loss into the training, 1 means whole training process while 0.2 stands for the last 20% of the training process. Weight of Î² represents the ratio compared to Î±. The best results are shown in bold</italic>.</p>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <title>5.5.2. Hyper-Parameters and Visualization</title>
        <p>To better investigating our proposed method, we evaluate it with different hyper-parameters, besides, we also take the representative method DAN as the comparison. The results are shown in <xref rid="F6" ref-type="fig">Figures 6</xref>, <xref rid="F7" ref-type="fig">7</xref>. From them we can see that, with the increase of batch size, both models show a drop in performance, especially when the batch size is 512, which has a significant decrease compared to 256 on SEED-IV. Besides, with the training epoch increases, neither model has a substantial improvement, especially MS-MDA, but our method achieves moderate accuracy and converges faster. Comparing cross-subject experiments on two datasets, it can be significantly seen that MS-MDA has a clear advantage over DAN, which indirectly shows that our approach has a more significant performance improvement for multiple source domain adaptation in EEG-based emotion recognition.</p>
        <fig position="float" id="F6">
          <label>Figure 6</label>
          <caption>
            <p>Evaluation of MS-MDA and DAN. The four color bars from left to right correspond to the four transfer scenarios: cross-session for SEED, cross-subject for SEED, cross-session for SEED-IV, cross-subject for SEED-IV. Batch size: {16, 32, 64, 128, 256, 512}, epoch: {100, 200, 300, 400, 500, 600, 700}. <bold>(A)</bold> Change of accuracy of MS-MDA under batch size. <bold>(B)</bold> Change of accuracy of DAN under batch size. <bold>(C)</bold> Change of accuracy of MS-MDA under epoch. <bold>(D)</bold> Change of accuracy of DAN under epoch.</p>
          </caption>
          <graphic xlink:href="fnins-15-778488-g0006" position="float"/>
        </fig>
        <fig position="float" id="F7">
          <label>Figure 7</label>
          <caption>
            <p>Evaluation of MS-MDA and DAN with different settings of batch size and epochs. The color lines from top to bottom stand for eight transfer scenarios: cross-session for SEED of MS-MDA, cross-subject for SEED of MS-MDA, cross-session for SEED-IV of MS-MDA, cross-subject for SEED-IV of MS-MDA; cross-session for SEED of DAN, cross-subject for SEED of DAN, cross-session for SEED-IV of DAN, cross-subject for SEED-IV of DAN. <bold>(A)</bold> Change of accuracy under different batch size. <bold>(B)</bold> Change of accuracy under different epoch.</p>
          </caption>
          <graphic xlink:href="fnins-15-778488-g0007" position="float"/>
        </fig>
        <p>We also visualize the four loss items (total loss, classification loss, MMD loss, and discrepancy loss) in <xref rid="F8" ref-type="fig">Figure 8</xref>. From the figure, we can see that the total loss, the classification loss, and the discrepancy loss decrease with the training step increases. However, the figure of MMD loss has a relatively significant rise at the 2k step. We assume that the alpha gets to value 1 at the 2k step, which makes the MMD loss the same weight as the classification loss, thus slightly impacting the model.</p>
        <fig position="float" id="F8">
          <label>Figure 8</label>
          <caption>
            <p>Visualization for four loss items. The upper left is the total loss, while the classification loss is in the upper middle. The lower left is for the MMD loss, and the discrepancy loss is in the lower middle. The figure in the right is the visualization of the Î± in (5).</p>
          </caption>
          <graphic xlink:href="fnins-15-778488-g0008" position="float"/>
        </fig>
        <p>For a better understanding of the effect of our proposed method, we randomly pick 100 EEG samples from each subject (domain) in the scenario of cross-subject to visualize with t-SNE (Van der Maaten and Hinton, <xref rid="B42" ref-type="bibr">2008</xref>), as displayed in <xref rid="F9" ref-type="fig">Figure 9</xref>. We only plot the cross-subject since this transfer scenario has more sources that will maximize visualization. In the <xref rid="F9" ref-type="fig">Figure 9</xref>, each color stands for a source domain, and the target domain are in black. To better plotting, we transparent the target sample to avoid overlap. It should be noticed that in the lower left figure, we pick 1400 samples since we concatenate all sources into one.</p>
        <fig position="float" id="F9">
          <label>Figure 9</label>
          <caption>
            <p>Visualization with t-SNE for raw data (upper left), normalization data (upper right), data using DAN (lower left), and data using MS-MDA (lower right). The input data of the last fully-connected (DSC) layer are used for the computation of the t-SNE. Target data are in the shape of X with black, all other 14 source data are in 14 colors. Notice that since we have concatenated all the source domains, the lower left figure has only one color for the source domain. All four figures are best viewed in color.</p>
          </caption>
          <graphic xlink:href="fnins-15-778488-g0009" position="float"/>
        </fig>
        <p>From the <xref rid="F9" ref-type="fig">Figure 9</xref>, it is apparently that the distribution of all EEG data from different subjects (with different colors) is close. Most samples are concentrated in one area, with a few outliers in individual subjects. These distribution confirms our hypothesis that all EEG data share some low-level features, i.e., their distribution on the feature space is slightly overlapping. This is especially noticeable after normalization (upper right in the <xref rid="F9" ref-type="fig">Figure 9</xref>), where the distribution of these EEG data is neater and around the center. In DAN, since all the source data are concatenated as one source domain, there is only one color for the source domain. Lower left figure of <xref rid="F9" ref-type="fig">Figure 9</xref> illustrates that domain adaptation process brings the source and target domains closer together, and resulting in a high degree of overlap between green and black samples, with a concentration of black samples in the more central region of the source domain. As for MS-MDA, since we adopt the distribution of each source domain separately with the target domain, it is intuitive that the black dots should have some closeness and overlap with each color of the source domain, and the lower right figure of <xref rid="F9" ref-type="fig">Figure 9</xref> does confirm our suspicion.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s6">
    <title>6. Discussion</title>
    <p>As can be seen from <xref rid="T2" ref-type="table">Table 2</xref>, comparing the results of selective methods and prior works, our proposed method has a significant improvement, especially for cross-subject DA in which the number of source domains is large. The ablation experiments from <xref rid="T3" ref-type="table">Table 3</xref> also show that our proposed method requires both MMD and discrepancy loss in most cases. Eliminating the MMD loss has a significant performance drop on both datasets, confirming the importance of DA, and eliminating disc. loss does not have as large an impact as MMD loss, but also verifies the help of multi-source convergence. Also, during the experiments, we find that the type of normalization of the data has a significant impact on the overall results, so we also design experiments and explore the normalization of EEG data in DA to help improve the performance of our model. As can be seen in <xref rid="T4" ref-type="table">Table 4</xref>, there is not much difference between the two normalization orders, and it is most appropriate to do data normalization on the electrode-wise, which has a crushing performance improvement compared to the other three methods; for our method, which does not concatenate data, electrode normalization is also the most effective. This conclusion is in line with our intuition that data collected from the same electrode are relatively more regular or conform to a certain distribution, while data collected from different electrodes are very different. In addition, during the experiments, we find that the disc. loss needs to be carefully adjusted, otherwise it is easy to cause harmful effects, which we guess is because this loss introduces a convergence effect on multiple classifiers in the model (in other words, smooth the inferences made from multiple classifiers), and if most of the classifiers are wrong, this convergence effect will cause the correct classifiers to error. Therefore, we also test and evaluate the impact of the disc. loss coefficients on the model at different settings, and from <xref rid="T5" ref-type="table">Table 5</xref>, we can see that the disc. loss achieves the best results if it is set to 0.01 times the MMD loss coefficient and is being used in the full model training.</p>
    <p>After exploring the internal details of the model, we also evaluated the performance of the model under different hyper-parameters. For better comparison, we chose a representative DAN as the comparison method. From <xref rid="F6" ref-type="fig">Figures 6</xref>, <xref rid="F7" ref-type="fig">7</xref>, we can see that both models have a significant decrease as the batch size is increasing. The reason for this we assume is that small batch size tends to fall into local optimal overfitting. Besides, the hyperparameter epoch has a minimal impact on both models, particularly the MS-MDA. From <xref rid="F6" ref-type="fig">Figures 6</xref>, <xref rid="F7" ref-type="fig">7</xref>, we can also clearly see that MS-MDA has a significant advantage over DAN in cross-subject DA where the number of multiple source domains is large, which also confirms the importance of constructing multiple branches for multiple source domains to adopt DA separately.</p>
    <p>Although it is clear from the results that our proposed method has a significant performance improvement, we also found that the training time consumed increases linearly with the number of source domains, i.e., the larger the number of source domains and the larger the model, the longer the training takes, unlike concatenating all source data into one, where there is only additional time due to the increase in the amount of data. For this problem, our current idea is to discard some less relevant source domains selectively and not build DA branches for them, allowing the disc. loss to play a more prominent role because there is less negative information. In addition, the encoders in the current model are the simplest MLP, and many literature and works have verified the usability of LSTM for EEG data (Ma et al., <xref rid="B32" ref-type="bibr">2019</xref>; Jiao et al., <xref rid="B19" ref-type="bibr">2020</xref>; Tao and Lu, <xref rid="B39" ref-type="bibr">2020</xref>), and we will consider switching to use LSTM as the encoders in future works.</p>
  </sec>
  <sec sec-type="conclusions" id="s7">
    <title>7. Conclusion</title>
    <p>In this paper, we propose MS-MDA, an EEG-based emotion recognition domain adaptation method, which is applicable to multiple source domain situations. Through experimental evaluation, we find that this method has a better ability to adapt to multiple source domains, which is validated by comparison with the selective approaches and the SOTA models, especially for cross-subject experiments where our proposed method consists of up to 20% improvement. In addition, we also explore the impact of different normalization methods for EEG data in domain adaptation, which we believe can serve as an inspiration for other EEG-based works while improving the effectiveness of the models. As for our future work, the current model for multiple source domains is to construct a DA branch for each of them without selection, which will increase the model size and training time exponentially, and also introduces information from the source domain that is not relevant to the target into the model. A more efficient approach may be to selectively build DA branches from a reservoir of source domains, allowing the model to be more efficient while only focusing on the source domain information that is relevant to the target domain.</p>
  </sec>
  <sec sec-type="data-availability" id="s8">
    <title>Data Availability Statement</title>
    <p>The datasets analyzed for this study can be found in the BCMI laboratory official website at <ext-link xlink:href="https://bcmi.sjtu.edu.cn/home/seed/index.html" ext-link-type="uri">https://bcmi.sjtu.edu.cn/home/seed/index.html</ext-link>.</p>
  </sec>
  <sec id="s9">
    <title>Author Contributions</title>
    <p>HC and JL: conceptualization and writing-original draft. HC, MJ, and ZL: investigations and data analysis and constructed the experiments. CF, JL, and HH: review the draft and editing. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s10">
    <title>Funding</title>
    <p>This work was supported by the National Natural Science Foundation of China (62020106015), the Strategic Priority Research Program of CAS (XDB32040000), the Zhejiang Provincial Natural Science Foundation of China (LQ20F030013), and the Ningbo Public Service Technology Foundation, China (202002N3181).</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s11">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acharya</surname><given-names>U. R.</given-names></name><name><surname>Sudarshan</surname><given-names>V. K.</given-names></name><name><surname>Adeli</surname><given-names>H.</given-names></name><name><surname>Santhosh</surname><given-names>J.</given-names></name><name><surname>Koh</surname><given-names>J. E.</given-names></name><name><surname>Adeli</surname><given-names>A.</given-names></name></person-group> (<year>2015</year>). <article-title>Computer-aided diagnosis of depression using eeg signals</article-title>. <source>Eur. Neurol</source>. <volume>73</volume>, <fpage>329</fpage>â<lpage>336</lpage>. <pub-id pub-id-type="doi">10.1159/000381950</pub-id><?supplied-pmid 25997732?><pub-id pub-id-type="pmid">25997732</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ay</surname><given-names>B.</given-names></name><name><surname>Yildirim</surname><given-names>O.</given-names></name><name><surname>Talo</surname><given-names>M.</given-names></name><name><surname>Baloglu</surname><given-names>U. B.</given-names></name><name><surname>Aydin</surname><given-names>G.</given-names></name><name><surname>Puthankattil</surname><given-names>S. D.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>Automated depression detection using deep representation and sequence learning with eeg signals</article-title>. <source>J. Med. Syst</source>. <volume>43</volume>, <fpage>1</fpage>â<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1007/s10916-019-1345-y</pub-id><?supplied-pmid 31139932?><pub-id pub-id-type="pmid">31139932</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>L. F.</given-names></name><name><surname>Gross</surname><given-names>J.</given-names></name><name><surname>Christensen</surname><given-names>T. C.</given-names></name><name><surname>Benvenuto</surname><given-names>M.</given-names></name></person-group> (<year>2001</year>). <article-title>Knowing what you're feeling and knowing what to do about it: mapping the relation between emotion differentiation and emotion regulation</article-title>. <source>Cogn. Emot</source>. <volume>15</volume>, <fpage>713</fpage>â<lpage>724</lpage>. <pub-id pub-id-type="doi">10.1080/02699930143000239</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Birbaumer</surname><given-names>N.</given-names></name></person-group> (<year>2006</year>). <article-title>Breaking the silence: brain-computer interfaces (bci) for communication and motor control</article-title>. <source>Psychophysiology</source>
<volume>43</volume>, <fpage>517</fpage>â<lpage>532</lpage>. <pub-id pub-id-type="doi">10.1111/j.1469-8986.2006.00456.x</pub-id><?supplied-pmid 17076808?><pub-id pub-id-type="pmid">17076808</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borgwardt</surname><given-names>K. M.</given-names></name><name><surname>Gretton</surname><given-names>A.</given-names></name><name><surname>Rasch</surname><given-names>M. J.</given-names></name><name><surname>Kriegel</surname><given-names>H.-P.</given-names></name><name><surname>SchÃ¶lkopf</surname><given-names>B.</given-names></name><name><surname>Smola</surname><given-names>A. J.</given-names></name></person-group> (<year>2006</year>). <article-title>Integrating structured biological data by kernel maximum mean discrepancy</article-title>. <source>Bioinformatics</source>
<volume>22</volume>, <fpage>e49</fpage>â<lpage>e57</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btl242</pub-id><?supplied-pmid 16873512?><pub-id pub-id-type="pmid">16873512</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bucks</surname><given-names>R. S.</given-names></name><name><surname>Radford</surname><given-names>S. A.</given-names></name></person-group> (<year>2004</year>). <article-title>Emotion processing in alzheimer's disease</article-title>. <source>Aging Mental Health</source>
<volume>8</volume>, <fpage>222</fpage>â<lpage>232</lpage>. <pub-id pub-id-type="doi">10.1080/13607860410001669750</pub-id><?supplied-pmid 15203403?><pub-id pub-id-type="pmid">15203403</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>CandÃ¨s</surname><given-names>E. J.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Ma</surname><given-names>Y.</given-names></name><name><surname>Wright</surname><given-names>J.</given-names></name></person-group> (<year>2011</year>). <article-title>Robust principal component analysis?</article-title>
<source>J. ACM</source>
<volume>58</volume>, <fpage>1</fpage>â<lpage>37</lpage>. <pub-id pub-id-type="doi">10.1145/1970392.1970395</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chai</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>A fast, efficient domain adaptation technique for cross-domain electroencephalography (eeg)-based emotion recognition</article-title>. <source>Sensors</source><volume>17</volume>, <fpage>1014</fpage>. <pub-id pub-id-type="doi">10.3390/s17051014</pub-id><?supplied-pmid 28467371?><pub-id pub-id-type="pmid">28467371</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chai</surname><given-names>X.</given-names></name><name><surname>Wang</surname><given-names>Q.</given-names></name><name><surname>Zhao</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name><name><surname>Bai</surname><given-names>O.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name></person-group> (<year>2016</year>). <article-title>Unsupervised domain adaptation techniques based on auto-encoder for non-stationary eeg-based emotion recognition</article-title>. <source>Comput. Biol. Med</source>. <volume>79</volume>, <fpage>205</fpage>â<lpage>214</lpage>. <pub-id pub-id-type="doi">10.1016/j.compbiomed.2016.10.019</pub-id><?supplied-pmid 27810626?><pub-id pub-id-type="pmid">27810626</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname><given-names>R. J.</given-names></name></person-group> (<year>2002</year>). <article-title>Emotion, cognition, and behavior</article-title>. <source>Science</source>
<volume>298</volume>, <fpage>1191</fpage>â<lpage>1194</lpage>. <pub-id pub-id-type="doi">10.1126/science.1076358</pub-id><?supplied-pmid 12424363?><pub-id pub-id-type="pmid">12424363</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>R.-N.</given-names></name><name><surname>Zhu</surname><given-names>J.-Y.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2013</year>). <article-title>Differential entropy feature for eeg-based emotion classification</article-title>, in <source>2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)</source> (<publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>81</fpage>â<lpage>84</lpage>.</mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ekman</surname><given-names>P.</given-names></name></person-group> (<year>1993</year>). <article-title>Facial expression and emotion</article-title>. <source>Am. Psychol</source>. <volume>48</volume>, <fpage>384</fpage>. <pub-id pub-id-type="doi">10.1037/0003-066X.48.4.384</pub-id><pub-id pub-id-type="pmid">8512154</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fahimi</surname><given-names>F.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Goh</surname><given-names>W. B.</given-names></name><name><surname>Lee</surname><given-names>T.-S.</given-names></name><name><surname>Ang</surname><given-names>K. K.</given-names></name><name><surname>Guan</surname><given-names>C.</given-names></name></person-group> (<year>2019</year>). <article-title>Inter-subject transfer learning with an end-to-end deep convolutional neural network for eeg-based bci</article-title>. <source>J. Neural Eng</source>. <volume>16</volume>, <fpage>026007</fpage>. <pub-id pub-id-type="doi">10.1088/1741-2552/aaf3f6</pub-id><?supplied-pmid 30524056?><pub-id pub-id-type="pmid">30524056</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frisoli</surname><given-names>A.</given-names></name><name><surname>Loconsole</surname><given-names>C.</given-names></name><name><surname>Leonardis</surname><given-names>D.</given-names></name><name><surname>Banno</surname><given-names>F.</given-names></name><name><surname>Barsotti</surname><given-names>M.</given-names></name><name><surname>Chisari</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>A new gaze-bci-driven control of an upper limb exoskeleton for rehabilitation in real-world tasks</article-title>. <source>IEEE Trans. Syst. Man Cybern. C</source><volume>42</volume>, <fpage>1169</fpage>â<lpage>1179</lpage>. <pub-id pub-id-type="doi">10.1109/TSMCC.2012.2226444</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>M. W.</given-names></name><name><surname>Dorling</surname><given-names>S.</given-names></name></person-group> (<year>1998</year>). <article-title>Artificial neural networks (the multilayer perceptron)âreview of applications in the atmospheric sciences</article-title>. <source>Atmos. Environ</source>. <volume>32</volume>, <fpage>2627</fpage>â<lpage>2636</lpage>. <pub-id pub-id-type="doi">10.1016/S1352-2310(97)00447-0</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosseinifard</surname><given-names>B.</given-names></name><name><surname>Moradi</surname><given-names>M. H.</given-names></name><name><surname>Rostami</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>). <article-title>Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from eeg signal</article-title>. <source>Comput. Methods Programs Biomed</source>. <volume>109</volume>, <fpage>339</fpage>â<lpage>345</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2012.10.008</pub-id><?supplied-pmid 23122719?><pub-id pub-id-type="pmid">23122719</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeon</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>Emotions and affect in human factors and human-computer interaction: taxonomy, theories, approaches, and methods</article-title>, in <source>Emotions and Affect in Human Factors and Human-Computer Interaction</source>, ed <person-group person-group-type="editor"><name><surname>Jeon</surname><given-names>M.</given-names></name></person-group> (<publisher-loc>Houghton, MI</publisher-loc>: <publisher-name>Elsevier</publisher-name>), <fpage>3</fpage>â<lpage>26</lpage>.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Tang</surname><given-names>Y.</given-names></name><name><surname>Guan</surname><given-names>C.</given-names></name></person-group> (<year>2021</year>). <article-title>Enhancing eeg-based classification of depression patients using spatial information</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng</source>. <volume>29</volume>, <fpage>566</fpage>â<lpage>575</lpage>. <pub-id pub-id-type="doi">10.1109/TNSRE.2021.3059429</pub-id><?supplied-pmid 33587703?><pub-id pub-id-type="pmid">33587703</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiao</surname><given-names>Y.</given-names></name><name><surname>Deng</surname><given-names>Y.</given-names></name><name><surname>Luo</surname><given-names>Y.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2020</year>). <article-title>Driver sleepiness detection from eeg and eog signals using gan and lstm networks</article-title>. <source>Neurocomputing</source>
<volume>408</volume>, <fpage>100</fpage>â<lpage>111</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2019.05.108</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>Y.-M.</given-names></name><name><surname>Luo</surname><given-names>Y.-D.</given-names></name><name><surname>Zheng</surname><given-names>W.-L.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2017</year>). <article-title>Eeg-based emotion recognition using domain adaptation network</article-title>, in <source>2017 International Conference on Orange Technologies (ICOT)</source> (<publisher-loc>Singapore</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>222</fpage>â<lpage>225</lpage>. <?supplied-pmid 33551775?></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joormann</surname><given-names>J.</given-names></name><name><surname>Gotlib</surname><given-names>I. H.</given-names></name></person-group> (<year>2010</year>). <article-title>Emotion regulation in depression: relation to cognitive inhibition</article-title>. <source>Cogn. Emot</source>. <volume>24</volume>, <fpage>281</fpage>â<lpage>298</lpage>. <pub-id pub-id-type="doi">10.1080/02699930903407948</pub-id><?supplied-pmid 20300538?><pub-id pub-id-type="pmid">20300538</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Adam: a method for stochastic optimization</article-title>. <source>arXiv preprint</source> arXiv:1412.6980.</mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelstra</surname><given-names>S.</given-names></name><name><surname>Muhl</surname><given-names>C.</given-names></name><name><surname>Soleymani</surname><given-names>M.</given-names></name><name><surname>Lee</surname><given-names>J.-S.</given-names></name><name><surname>Yazdani</surname><given-names>A.</given-names></name><name><surname>Ebrahimi</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Deap: a database for emotion analysis; using physiological signals</article-title>. <source>IEEE Trans. Affect. Comput</source>. <volume>3</volume>, <fpage>18</fpage>â<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1109/T-AFFC.2011.15</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>S.-H.</given-names></name><name><surname>Lee</surname><given-names>M.</given-names></name><name><surname>Jeong</surname><given-names>J.-H.</given-names></name><name><surname>Lee</surname><given-names>S.-W.</given-names></name></person-group> (<year>2019</year>). <article-title>Towards an eeg-based intuitive bci communication system using imagined speech and visual imagery</article-title>, in <source>2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</source> (<publisher-loc>Bari</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>4409</fpage>â<lpage>4414</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H.</given-names></name><name><surname>Jin</surname><given-names>Y.-M.</given-names></name><name><surname>Zheng</surname><given-names>W.-L.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2018</year>). <article-title>Cross-subject emotion recognition using deep adaptation networks</article-title>, in <source>International Conference on Neural Information Processing</source> (<publisher-loc>Siem Reap</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>403</fpage>â<lpage>413</lpage>.</mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>H.</given-names></name><name><surname>Cai</surname><given-names>T.</given-names></name></person-group> (<year>2020</year>). <article-title>Foit: fast online instance transfer for improved eeg emotion recognition</article-title>, in <source>2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</source> (<publisher-loc>Seoul</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>2618</fpage>â<lpage>2625</lpage>.</mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Qiu</surname><given-names>S.</given-names></name><name><surname>Du</surname><given-names>C.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>He</surname><given-names>H.</given-names></name></person-group> (<year>2019a</year>). <article-title>Domain adaptation for eeg emotion recognition based on latent representation similarity</article-title>. <source>IEEE Trans. Cogn. Dev. Syst</source>. <volume>12</volume>, <fpage>344</fpage>â<lpage>353</lpage>. <pub-id pub-id-type="doi">10.1109/TCDS.2019.2949306</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J.</given-names></name><name><surname>Qiu</surname><given-names>S.</given-names></name><name><surname>Shen</surname><given-names>Y.-Y.</given-names></name><name><surname>Liu</surname><given-names>C.-L.</given-names></name><name><surname>He</surname><given-names>H.</given-names></name></person-group> (<year>2019b</year>). <article-title>Multisource transfer learning for cross-subject eeg emotion recognition</article-title>. <source>IEEE Trans. Cybern</source>. <volume>50</volume>, <fpage>3281</fpage>â<lpage>3293</lpage>. <pub-id pub-id-type="doi">10.1109/TCYB.2019.2904052</pub-id><?supplied-pmid 30932860?><pub-id pub-id-type="pmid">30932860</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Y.-P.</given-names></name><name><surname>Jao</surname><given-names>P.-K.</given-names></name><name><surname>Yang</surname><given-names>Y.-H.</given-names></name></person-group> (<year>2017</year>). <article-title>Improving cross-day eeg-based emotion classification using robust principal component analysis</article-title>. <source>Front. Comput. Neurosci</source>. <volume>11</volume>:<fpage>64</fpage>. <pub-id pub-id-type="doi">10.3389/fncom.2017.00064</pub-id><?supplied-pmid 28769778?><pub-id pub-id-type="pmid">28769778</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Chen</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name></person-group> (<year>2015</year>). <article-title>A boosting-based spatial-spectral model for stroke patients' eeg analysis in rehabilitation training</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng</source>. <volume>24</volume>, <fpage>169</fpage>â<lpage>179</lpage>. <pub-id pub-id-type="doi">10.1109/TNSRE.2015.2466079</pub-id><?supplied-pmid 26302519?><pub-id pub-id-type="pmid">26302519</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Long</surname><given-names>M.</given-names></name><name><surname>Cao</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Jordan</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Learning transferable features with deep adaptation networks</article-title>, in <source>International Conference on Machine Learning</source> (<publisher-loc>Lile</publisher-loc>: <publisher-name>PMLR</publisher-name>), <fpage>97</fpage>â<lpage>105</lpage>. <?supplied-pmid 30188813?></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>J.</given-names></name><name><surname>Tang</surname><given-names>H.</given-names></name><name><surname>Zheng</surname><given-names>W.-L.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2019</year>). <article-title>Emotion recognition using multimodal residual lstm network</article-title>, in <source>Proceedings of the 27th ACM International Conference on Multimedia</source> (<publisher-loc>Nice</publisher-loc>), <fpage>176</fpage>â<lpage>183</lpage>.</mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nair</surname><given-names>V.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2010</year>). <article-title>Rectified linear units improve restricted boltzmann machines</article-title>, in <source>ICML</source> (<publisher-loc>Haifa</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>S. J.</given-names></name><name><surname>Tsang</surname><given-names>I. W.</given-names></name><name><surname>Kwok</surname><given-names>J. T.</given-names></name><name><surname>Yang</surname><given-names>Q.</given-names></name></person-group> (<year>2010</year>). <article-title>Domain adaptation via transfer component analysis</article-title>. <source>IEEE Trans. Neural Netw</source>. <volume>22</volume>, <fpage>199</fpage>â<lpage>210</lpage>. <pub-id pub-id-type="doi">10.1109/TNN.2010.2091281</pub-id><?supplied-pmid 21095864?><pub-id pub-id-type="pmid">21095864</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>S. J.</given-names></name><name><surname>Yang</surname><given-names>Q.</given-names></name></person-group> (<year>2009</year>). <article-title>A survey on transfer learning</article-title>. <source>IEEE Trans Knowl Data Eng</source>. <volume>22</volume>, <fpage>1345</fpage>â<lpage>1359</lpage>. <pub-id pub-id-type="doi">10.1109/TKDE.2009.191</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sanei</surname><given-names>S.</given-names></name><name><surname>Chambers</surname><given-names>J. A.</given-names></name></person-group> (<year>2013</year>). <source>EEG Signal Processing</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soleymani</surname><given-names>M.</given-names></name><name><surname>Asghari-Esfeden</surname><given-names>S.</given-names></name><name><surname>Fu</surname><given-names>Y.</given-names></name><name><surname>Pantic</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Analysis of eeg signals and facial expressions for continuous emotion detection</article-title>. <source>IEEE Trans. Affect. Comput</source>. <volume>7</volume>, <fpage>17</fpage>â<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1109/TAFFC.2015.2436926</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>B.</given-names></name><name><surname>Saenko</surname><given-names>K.</given-names></name></person-group> (<year>2016</year>). <article-title>Deep coral: correlation alignment for deep domain adaptation</article-title>, in <source>European Conference on Computer Vision</source> (<publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>443</fpage>â<lpage>450</lpage>.</mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tao</surname><given-names>L.-Y.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2020</year>). <article-title>Emotion recognition under sleep deprivation using a multimodal residual lstm network</article-title>, in <source>2020 International Joint Conference on Neural Networks (IJCNN)</source> (<publisher-loc>Glasgow</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1</fpage>â<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyng</surname><given-names>C. M.</given-names></name><name><surname>Amin</surname><given-names>H. U.</given-names></name><name><surname>Saad</surname><given-names>M. N.</given-names></name><name><surname>Malik</surname><given-names>A. S.</given-names></name></person-group> (<year>2017</year>). <article-title>The influences of emotion on learning and memory</article-title>. <source>Front. Psychol</source>. <volume>8</volume>:<fpage>1454</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2017.01454</pub-id><?supplied-pmid 28883804?><pub-id pub-id-type="pmid">28883804</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzeng</surname><given-names>E.</given-names></name><name><surname>Hoffman</surname><given-names>J.</given-names></name><name><surname>Zhang</surname><given-names>N.</given-names></name><name><surname>Saenko</surname><given-names>K.</given-names></name><name><surname>Darrell</surname><given-names>T.</given-names></name></person-group> (<year>2014</year>). <article-title>Deep domain confusion: maximizing for domain invariance</article-title>. <source>arXiv preprint</source> arXiv:1412.3474.</mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Maaten</surname><given-names>L.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2008</year>). <article-title>Visualizing data using t-sne</article-title>. <source>J. Mach. Learn. Res</source>. <volume>9</volume>, <fpage>2579</fpage>â<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Qiu</surname><given-names>S.</given-names></name><name><surname>Ma</surname><given-names>X.</given-names></name><name><surname>He</surname><given-names>H.</given-names></name></person-group> (<year>2021</year>). <article-title>A prototype-based spd matrix network for domain adaptation eeg emotion recognition</article-title>. <source>Pattern Recognit</source>. <volume>110</volume>:<fpage>107626</fpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2020.107626</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>B.</given-names></name><name><surname>Wang</surname><given-names>N.</given-names></name><name><surname>Chen</surname><given-names>T.</given-names></name><name><surname>Li</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>Empirical evaluation of rectified activations in convolutional network</article-title>. <source>arXiv preprint</source> arXiv:1505.00853.</mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>W.</given-names></name><name><surname>SchÃ¼tze</surname><given-names>H.</given-names></name><name><surname>Xiang</surname><given-names>B.</given-names></name><name><surname>Zhou</surname><given-names>B.</given-names></name></person-group> (<year>2016</year>). <article-title>Abcnn: attention-based convolutional neural network for modeling sentence pairs</article-title>. <source>Trans. Assoc. Comput. Linguist</source>. <volume>4</volume>, <fpage>259</fpage>â<lpage>272</lpage>. <pub-id pub-id-type="doi">10.1162/tacl_a_00097</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>B.</given-names></name><name><surname>Yan</surname><given-names>G.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Su</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Lei</surname><given-names>T.</given-names></name></person-group> (<year>2020</year>). <article-title>Brain functional networks based on resting-state eeg data for major depressive disorder analysis and classification</article-title>. <source>IEEE Trans. Neural Syst. Rehabil. Eng</source>. <volume>29</volume>, <fpage>215</fpage>â<lpage>229</lpage>. <pub-id pub-id-type="doi">10.1109/TNSRE.2020.3043426</pub-id><?supplied-pmid 33296307?><pub-id pub-id-type="pmid">33296307</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>L.-M.</given-names></name><name><surname>Yan</surname><given-names>X.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2021</year>). <article-title>Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition</article-title>, in <source>Proceedings of the 35th AAAI Conference on Artificial Intelligence</source>.</mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>W.-L.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Lu</surname><given-names>Y.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name><name><surname>Cichocki</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Emotionmeter: a multimodal framework for recognizing human emotions</article-title>. <source>IEEE Trans. Cybern</source>. <volume>49</volume>, <fpage>1110</fpage>â<lpage>1122</lpage>. <pub-id pub-id-type="doi">10.1109/TCYB.2018.2797176</pub-id><?supplied-pmid 29994384?><pub-id pub-id-type="pmid">29994384</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>W.-L.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2015</year>). <article-title>Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks</article-title>. <source>IEEE Trans. Auton. Ment. Dev</source>. <volume>7</volume>, <fpage>162</fpage>â<lpage>175</lpage>. <pub-id pub-id-type="doi">10.1109/TAMD.2015.2431497</pub-id><?supplied-pmid 27295638?><pub-id pub-id-type="pmid">27295638</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>W.-L.</given-names></name><name><surname>Lu</surname><given-names>B.-L.</given-names></name></person-group> (<year>2016</year>). <article-title>Personalizing eeg-based affective models with transfer learning</article-title>, in <source>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</source> (<publisher-loc>New York, NY</publisher-loc>), <fpage>2732</fpage>â<lpage>2738</lpage>.</mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Y.</given-names></name><name><surname>Zhuang</surname><given-names>F.</given-names></name><name><surname>Wang</surname><given-names>D.</given-names></name></person-group> (<year>2019</year>). <article-title>Aligning domain-specific distribution and classifier for cross-domain classification from multiple sources</article-title>, in <source>Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33</source> (<publisher-loc>Honolulu, HI</publisher-loc>), <fpage>5989</fpage>â<lpage>5996</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
