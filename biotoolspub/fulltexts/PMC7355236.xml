<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7355236</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa462</article-id>
    <article-id pub-id-type="publisher-id">btaa462</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Studies of Phenotypes and Clinical Applications</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Improved survival analysis by learning shared genomic information from pan-cancer data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Kim</surname>
          <given-names>Sunkyu</given-names>
        </name>
        <xref ref-type="aff" rid="btaa462-aff1">b1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kim</surname>
          <given-names>Keonwoo</given-names>
        </name>
        <xref ref-type="aff" rid="btaa462-aff1">b1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Choe</surname>
          <given-names>Junseok</given-names>
        </name>
        <xref ref-type="aff" rid="btaa462-aff1">b1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Inggeol</given-names>
        </name>
        <xref ref-type="aff" rid="btaa462-aff1">b1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6798-9106</contrib-id>
        <name>
          <surname>Kang</surname>
          <given-names>Jaewoo</given-names>
        </name>
        <xref ref-type="aff" rid="btaa462-aff1">b1</xref>
        <xref ref-type="aff" rid="btaa462-aff2">b2</xref>
        <xref ref-type="corresp" rid="btaa462-cor1"/>
        <!--<email>kangj@korea.ac.kr</email>-->
      </contrib>
    </contrib-group>
    <aff id="btaa462-aff1"><label>b1</label><institution>Department of Computer Science and Engineering, College of Informatics, Korea University</institution>, Seoul 02841, Republic of <country country="KR">Korea</country></aff>
    <aff id="btaa462-aff2"><label>b2</label><institution>Interdisciplinary Graduate Program in Bioinformatics, College of Informatics, Korea University</institution>, Seoul 02841, Republic of <country country="KR">Korea</country></aff>
    <author-notes>
      <corresp id="btaa462-cor1">To whom correspondence should be addressed. E-mail: <email>kangj@korea.ac.kr</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-07-13">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISMB 2020 Proceedings</issue-title>
    <fpage>i389</fpage>
    <lpage>i398</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa462.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Recent advances in deep learning have offered solutions to many biomedical tasks. However, there remains a challenge in applying deep learning to survival analysis using human cancer transcriptome data. As the number of genes, the input variables of survival model, is larger than the amount of available cancer patient samples, deep-learning models are prone to overfitting. To address the issue, we introduce a new deep-learning architecture called VAECox. VAECox uses transfer learning and fine tuning.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We pre-trained a variational autoencoder on all RNA-seq data in 20 TCGA datasets and transferred the trained weights to our survival prediction model. Then we fine-tuned the transferred weights during training the survival model on each dataset. Results show that our model outperformed other previous models such as Cox Proportional Hazard with LASSO and ridge penalty and Cox-nnet on the 7 of 10 TCGA datasets in terms of C-index. The results signify that the transferred information obtained from entire cancer transcriptome data helped our survival prediction model reduce overfitting and show robust performance in unseen cancer patient samples.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Our implementation of VAECox is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/SunkyuKim/VAECox">https://github.com/dmis-lab/VAECox</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Research Foundation of Korea</institution>
            <institution-id institution-id-type="DOI">10.13039/501100003725</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NRF-2017R1A2A1A17069645</award-id>
        <award-id>NRF-2016M3A9A7916996</award-id>
        <award-id>NRF-2014M3C9A3063541</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Cancer prognosis, including prediction of time to death of a cancer patient, remains one of the most challenging issues in the clinical domain even after decades of effort by cancer researchers (<xref rid="btaa462-B27" ref-type="bibr">Kourou <italic>et al.</italic>, 2015</xref>; <xref rid="btaa462-B38" ref-type="bibr">Nicholson <italic>et al.</italic>, 2001</xref>). One major factor for this difficulty is censored patient samples, i.e. the status of a patient may be unknown if a clinic is unable to monitor the patient. Traditional modelling approaches have difficulty in handling censored patient samples as they do not have a specific time point of death. Even the last follow-up time in the record of the censored patient sample cannot be used as its missing time of death. A model that can handle both censored and uncensored samples can be effective in time-to-death prediction. Survival analysis is a statistical method that handles censored samples (<xref rid="btaa462-B7" ref-type="bibr">Cox, 2018</xref>). As survival analysis methods focus on whether a patient survives at a certain time point rather than when the patient dies, any patient who survives at a certain time point can be used in modelling patient survivals.</p>
    <p>With the advent of the Human Genome Project (<xref rid="btaa462-B48" ref-type="bibr">Venter <italic>et al.</italic>, 2001</xref>), high-throughput transcriptomics data of cancer patients has become accessible and technologies for analyzing the large amount of transcriptomics data have been developed (<xref rid="btaa462-B14" ref-type="bibr">Hanahan and Weinberg, 2011</xref>; <xref rid="btaa462-B47" ref-type="bibr">Van’t Veer <italic>et al.</italic>, 2002</xref>). Researchers have found that transcriptomics data, especially gene expression data, can be useful for cancer analysis (<xref rid="btaa462-B34" ref-type="bibr">Lussier and Li, 2012</xref>; <xref rid="btaa462-B46" ref-type="bibr">Valdes Mora <italic>et al.</italic>, 2018</xref>). The Cox Proportional Hazard (Cox-PH) model treats predicting the survival time of patients as a regression task (<xref rid="btaa462-B2" ref-type="bibr">Bradburn <italic>et al.</italic>, 2003</xref>; <xref rid="btaa462-B6" ref-type="bibr">Cox, 1972</xref>). The Cox-PH model predicts the probability of patient death using a hazard function. However, since the Cox-PH model is based on a linear combination of given features, it cannot learn underlying non-linear biological processes from transcriptomics data for cancer prognosis.</p>
    <p>Deep learning has recently started to gain popularity in the bioinformatics domain due to its advancements in technology and flexibility in modelling (<xref rid="btaa462-B3" ref-type="bibr">Chaudhary <italic>et al.</italic>, 2018</xref>; <xref rid="btaa462-B4" ref-type="bibr">Ching <italic>et al.</italic>, 2018</xref>; <xref rid="btaa462-B25" ref-type="bibr">Katzman <italic>et al.</italic>, 2018</xref>). Although deep-learning approaches have been applied to most biomedical tasks, using deep learning on genomic data still remains a challenge. As the amount of cancer patient data available for deep-learning models is insufficient, using deep learning can lead to serious overfitting issues. Deep-learning models in the general domain are less likely to suffer from overfitting issues where the total number of data samples significantly exceeds the number of features. In the biomedical domain, the opposite is often the case. For example, the total number of breast cancer patient samples in the Cancer Genome Atlas, TCGA, (<xref rid="btaa462-B45" ref-type="bibr">Tomczak <italic>et al.</italic>, 2015</xref>) is approximately 1100, whereas the number of human genes for each patient is more than 20 000. From the perspective of machine learning, the lack of training samples can lead to overfitting models on the training set, and obtaining poor performance on other unseen samples.</p>
    <p>Model simplification is one solution to overfitting issues. Cox-nnet, which was proposed in 2018, has succeeded in predicting the survival of patients with various cancer types using simple two fully connected perceptron layers (<xref rid="btaa462-B4" ref-type="bibr">Ching <italic>et al.</italic>, 2018</xref>). <xref rid="btaa462-B21" ref-type="bibr">Huang <italic>et al.</italic> (2019b)</xref> evaluated various machine-learning models and deep-learning architectures, but found that the artificial neural network (ANN) with only one or two perceptron layers is the most effective architecture in analyzing omics data for disease classification. These results show that when all genes are considered as independent features, model simplification greatly helps avoid overfitting. However, simplifying models inevitably limit the models’ ability to learn complex non-linear relations among features.</p>
    <p>An alternative to model simplification is transfer learning. Transfer learning is a method which involves transferring knowledge from a source task to a target task and has been used in various deep-learning models (<xref rid="btaa462-B12" ref-type="bibr">Fernandes <italic>et al.</italic>, 2017</xref>; <xref rid="btaa462-B23" ref-type="bibr">Kandaswamy <italic>et al.</italic>, 2016</xref>; <xref rid="btaa462-B32" ref-type="bibr">Li <italic>et al.</italic>, 2016</xref>). Transfer learning can be diversified depending on its learning setting (inductive, transductive and unsupervised) and transfer approach (instance-based, feature-based, parameter-based and relational knowledge-based) (<xref rid="btaa462-B39" ref-type="bibr">Pan and Yang, 2010</xref>).</p>
    <p>Transfer learning is a method which involves transferring knowledge from a task with an abundant number of samples to another task with an insufficient number of samples. A model can be pre-trained on a large dataset of a task and the parameters of that model are transferred to another model of a different but similar task. Subsequently, the latter model can be fine-tuned on the smaller target dataset. The number of parameters used in deep-learning models far exceeds the number of parameters used in conventional machine-learning models. Deep-learning models often use transfer learning to avoid overfitting (<xref rid="btaa462-B12" ref-type="bibr">Fernandes <italic>et al.</italic>, 2017</xref>; <xref rid="btaa462-B23" ref-type="bibr">Kandaswamy <italic>et al.</italic>, 2016</xref>; <xref rid="btaa462-B32" ref-type="bibr">Li <italic>et al.</italic>, 2016</xref>).</p>
    <p>In this work, we introduce VAECox, a deep-learning model architecture that addresses the scarcity of data samples by exploiting transfer learning and fine-tuning. We pre-trained our variational autoencoder (VAE) on all the TCGA RNA-seq data of patients with 20 cancer types for extracting common characteristics of cancer. Then we initialized the weights of our VAECox model with the weights of our pre-trained VAE model, and fine-tuned our VAECox model on each cancer patient dataset. Our VAECox model outperformed other baseline models such as Cox-PH with LASSO and ridge penalty and Cox-nnet on the 7 of 10 different cancer-type datasets.</p>
  </sec>
  <sec>
    <title>2 Material and methods</title>
    <sec>
      <title>2.1 Dataset and pre-processing</title>
      <p>We first downloaded the cancer patient RNA-seq gene expression data from TCGA. These data are provided by International Cancer Genome Consortium (ICGC) data portal (<ext-link ext-link-type="uri" xlink:href="https://dcc.icgc.org/">https://dcc.icgc.org/</ext-link>). For evaluating our VAECox model predicting patient survivals, we used each of the gene expression data for 10 different cancer types used for one of the baselines (<xref rid="btaa462-B4" ref-type="bibr">Ching <italic>et al.</italic>, 2018</xref>). The 10 different cancer types are bladder carcinoma (BLCA), breast carcinoma (BRCA), head and neck squamous cell carcinoma (HNSC), kidney renal cell carcinoma (KIRC), brain lower-grade glioma (LGG), liver hepatocellular carcinoma (LIHC), lung adenocarcinoma (LUAD), lung squamous cell carcinoma (LUSC), ovarian carcinoma (OV) and stomach adenocarcinoma (STAD). The 10 different cancer types were selected if they had more than 50 uncensored samples. For each of the 10 different cancer types, we compared the performance of our VAECox model to those of other baseline survival prediction models.</p>
      <p>As our VAECox model uses transfer learning from another model trained on heterogeneous cancer sets, we used 24 TCGA cancer datasets from the ICGC data portal to train our VAE. 10 cancer types, BLCA, BRCA, HNSC, KIRC, LGG, LIHC, LUAD, LUSC, OV and STAD were first included as they are used in survival prediction models. For the remaining 14 cancer types, 3 of them which are lymphoid neoplasm diffuse large B-cell lymphoma, kidney chromophobe and sarcoma were excluded as they did not contain any RNA-seq data. In addition, we removed rectum adenocarcinoma as it has insufficient patient samples. As a result, we used 20 cancer gene expression datasets for training our VAE model.</p>
      <p>When analyzing the dataset, we found some patient samples with a high number of missing gene expression values. We assumed that these patient samples would not be helpful in training our model. Even if we impute the missing values using the existing values, the imputed values cannot be reliable due to the high number of missing values. Therefore, we filtered samples with a high number of missing values to reduce the noise from them. If the missing value rate of the patient samples was more than 15%, the patient and the gene were excluded from the dataset. Since we extracted common traits of pan-cancers and transfer the knowledge to each cancer model, we selected 20 502 genes commonly included in cancer gene expression datasets. To regularize the scale of gene values, then we applied feature-wise Z-normalization to each gene expression dataset for the 20 cancer types. <xref rid="btaa462-T1" ref-type="table">Table 1</xref> provides the number of patient samples of each cancer type used in our study. The first 10 cancer types are used in survival prediction models. We imputed the missing values of the remaining patients using a matrix factorization-based algorithm as other straightforward methods such as mean, median or zero-value imputation are known to have limitations such as reducing the variance of the imputed variables. We used the LibFM (<xref rid="btaa462-B42" ref-type="bibr">Rendle, 2012</xref>) package to impute the missing values and the parameters were optimized using a Monte Carlo Markov Chain algorithm.
</p>
      <table-wrap id="btaa462-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Statistics of transcriptomics data for 10 cancer types on which VAECox was trained</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Cancer type</th>
              <th colspan="2" align="center" rowspan="1">Before pre-processing<hr/></th>
              <th colspan="2" align="center" rowspan="1">After pre-processing<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"># All</th>
              <th rowspan="1" colspan="1"># Uncensored</th>
              <th rowspan="1" colspan="1"># All</th>
              <th rowspan="1" colspan="1"># Uncensored</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">BLCA</td>
              <td rowspan="1" colspan="1">398</td>
              <td rowspan="1" colspan="1">108</td>
              <td rowspan="1" colspan="1">286</td>
              <td rowspan="1" colspan="1">72</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BRCA</td>
              <td rowspan="1" colspan="1">1039</td>
              <td rowspan="1" colspan="1">104</td>
              <td rowspan="1" colspan="1">989</td>
              <td rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">HNSC</td>
              <td rowspan="1" colspan="1">522</td>
              <td rowspan="1" colspan="1">170</td>
              <td rowspan="1" colspan="1">477</td>
              <td rowspan="1" colspan="1">160</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KIRC</td>
              <td rowspan="1" colspan="1">528</td>
              <td rowspan="1" colspan="1">162</td>
              <td rowspan="1" colspan="1">512</td>
              <td rowspan="1" colspan="1">159</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LGG</td>
              <td rowspan="1" colspan="1">507</td>
              <td rowspan="1" colspan="1">91</td>
              <td rowspan="1" colspan="1">433</td>
              <td rowspan="1" colspan="1">69</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LIHC</td>
              <td rowspan="1" colspan="1">343</td>
              <td rowspan="1" colspan="1">91</td>
              <td rowspan="1" colspan="1">267</td>
              <td rowspan="1" colspan="1">72</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LUAD</td>
              <td rowspan="1" colspan="1">480</td>
              <td rowspan="1" colspan="1">122</td>
              <td rowspan="1" colspan="1">440</td>
              <td rowspan="1" colspan="1">113</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LUSC</td>
              <td rowspan="1" colspan="1">477</td>
              <td rowspan="1" colspan="1">158</td>
              <td rowspan="1" colspan="1">404</td>
              <td rowspan="1" colspan="1">134</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">OV</td>
              <td rowspan="1" colspan="1">578</td>
              <td rowspan="1" colspan="1">301</td>
              <td rowspan="1" colspan="1">260</td>
              <td rowspan="1" colspan="1">147</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">STAD</td>
              <td rowspan="1" colspan="1">396</td>
              <td rowspan="1" colspan="1">84</td>
              <td rowspan="1" colspan="1">374</td>
              <td rowspan="1" colspan="1">77</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">CESC</td>
              <td rowspan="1" colspan="1">288</td>
              <td rowspan="1" colspan="1">60</td>
              <td rowspan="1" colspan="1">251</td>
              <td rowspan="1" colspan="1">53</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">COAD</td>
              <td rowspan="1" colspan="1">347</td>
              <td rowspan="1" colspan="1">52</td>
              <td rowspan="1" colspan="1">324</td>
              <td rowspan="1" colspan="1">46</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GBM</td>
              <td rowspan="1" colspan="1">592</td>
              <td rowspan="1" colspan="1">446</td>
              <td rowspan="1" colspan="1">158</td>
              <td rowspan="1" colspan="1">106</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">KIRP</td>
              <td rowspan="1" colspan="1">264</td>
              <td rowspan="1" colspan="1">31</td>
              <td rowspan="1" colspan="1">209</td>
              <td rowspan="1" colspan="1">23</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">LAML</td>
              <td rowspan="1" colspan="1">173</td>
              <td rowspan="1" colspan="1">108</td>
              <td rowspan="1" colspan="1">149</td>
              <td rowspan="1" colspan="1">92</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PAAD</td>
              <td rowspan="1" colspan="1">181</td>
              <td rowspan="1" colspan="1">66</td>
              <td rowspan="1" colspan="1">138</td>
              <td rowspan="1" colspan="1">45</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PRAD</td>
              <td rowspan="1" colspan="1">500</td>
              <td rowspan="1" colspan="1">8</td>
              <td rowspan="1" colspan="1">375</td>
              <td rowspan="1" colspan="1">6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SKCM</td>
              <td rowspan="1" colspan="1">440</td>
              <td rowspan="1" colspan="1">155</td>
              <td rowspan="1" colspan="1">401</td>
              <td rowspan="1" colspan="1">149</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">THCA</td>
              <td rowspan="1" colspan="1">501</td>
              <td rowspan="1" colspan="1">14</td>
              <td rowspan="1" colspan="1">495</td>
              <td rowspan="1" colspan="1">14</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">UCEC</td>
              <td rowspan="1" colspan="1">540</td>
              <td rowspan="1" colspan="1">45</td>
              <td rowspan="1" colspan="1">505</td>
              <td rowspan="1" colspan="1">43</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>2.2 Dimension reduction using autoencoder and VAE</title>
      <p>An autoencoder (AE) is a neural network model where the encoder compresses an input vector to a latent vector and the decoder decompresses the latent vector to reconstruct the input vector (<xref rid="btaa462-B17" ref-type="bibr">Hinton and Salakhutdinov, 2006</xref>). The AE is trained to generate an output vector that is as similar as possible to its original one. During compression, the encoder learns salient features and achieves dimension reduction (<xref rid="btaa462-B49" ref-type="bibr">Wang <italic>et al.</italic>, 2016</xref>). As the number of our patient samples is much smaller than the number of features, we can achieve transfer learning using the pre-trained weights of the encoder. For input vector <italic>x</italic>, the encoded vector <italic>z</italic> and the reconstructed vector <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of the AE’s output are mathematically expressed as follows:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:msub><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>where <italic>W</italic><sub>e</sub> and <italic>b</italic><sub>e</sub> are parameters of the encoder, and <italic>W</italic><sub>d</sub> and <italic>b</italic><sub>d</sub> are parameters of the decoder. <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the tanh function which is expressed as follows:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p>For training the AE, we use a reconstruction error with root-mean-square error (RMSE) as its objective function which is mathematically expressed as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>recon</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula>where <italic>n</italic> is the number of samples in the training set.</p>
      <p>A VAE is a generative model that exploits the latent distribution of input data for reconstruction (<xref rid="btaa462-B11" ref-type="bibr">Doersch, 2016</xref>; <xref rid="btaa462-B26" ref-type="bibr">Kingma and Welling, 2013</xref>). The VAE does not only achieve dimension reduction but also learns to model a more generalized latent prior distribution (e.g. Gaussian distribution). Before input reconstruction, the latent variables are randomly sampled from the probability distribution made by the encoder. However, these randomly sampled variables are not differentiable, which makes it difficult to calculate gradients. As the VAE’s objective is to optimize both the encoding and decoding weights, a re-parameterization trick which involves using the mean and variances of the latent distribution as deterministic parameters. Therefore, the mean and variance encoders for modelling the latent distribution’s mean and variance, respectively, are used. This allows for the optimization of both the encoding and decoding weights in the VAE model. We believe that the pre-trained encoding weights can be generally used and be effective in various tasks as they can be used to model richer and more elaborate latent features of cancer patient data.</p>
      <p>In this study, we added a hidden layer to the encoder and decoder of our VAE model architecture. The architecture of our VAE model is shown in <xref ref-type="fig" rid="btaa462-F1">Figure 1A</xref>.The VAE model for input vector <italic>x</italic>, the outputs of encoder <italic>μ</italic>, <italic>ν</italic> and <italic>z</italic>, and the reconstructed vector <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> are mathematically expressed as follows:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mo>μ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>μ</mml:mo></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo>μ</mml:mo></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mo>ν</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mo>ν</mml:mo></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mo>ν</mml:mo></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:mo>σ</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>ν</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>ϵ</mml:mo><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mo>μ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>σ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mo>ϵ</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi mathvariant="normal">h</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>where <italic>μ</italic>, <italic>ν</italic> and <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> are the mean, log variance and the variance of a Gaussian distribution, respectively. <italic>ϵ</italic> is randomly sampled from the standard Gaussian distribution. <italic>W</italic> and <italic>b</italic> are trainable parameters of our VAE model.
</p>
      <fig id="btaa462-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>(<bold>A</bold>) The architecture of the VAE model used in this study. A hidden layer is added to both the encoder and decoder of the original VAE. We trained this VAE model on all the TCGA RNA-seq data of patients with 20 cancer types. (<bold>B</bold>) The architecture of the VAECox model which predicts a patient’s hazard ratio. The parameters of the first two layers are transferred from the encoder part of the pre-trained VAE model</p>
        </caption>
        <graphic xlink:href="btaa462f1"/>
      </fig>
      <p>We used a reconstruction error with RMSE as an objective function for our model. We used the Kullback–Leibler divergence (<xref rid="btaa462-B28" ref-type="bibr">Kullback and Leibler, 1951</xref>; <xref rid="btaa462-B41" ref-type="bibr">Press <italic>et al.</italic>, 2007</xref>) noted (<inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) as an additional objective function for measuring the distance between two distributions: (i) the true latent distribution of a given input; (ii) the variational latent distribution of a given input encoded by the VAE model.
<disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:mrow><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo>ν</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>μ</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi mathvariant="normal">k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The objective function of our VAE model is defined as follows.
<disp-formula id="E11"><label>(11)</label><mml:math id="M11"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>VAE</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mo>θ</mml:mo></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>recon</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>θ</italic><sub>VAE</sub> refers to the parameters our VAE model.</p>
    </sec>
    <sec>
      <title>2.3 Survival analysis</title>
      <p>The architecture of our VAECox model which can predict patient survival is shown in <xref ref-type="fig" rid="btaa462-F1">Figure 1B</xref>. We combined the encoder layers of the VAE model with the Cox-PH model (<xref rid="btaa462-B6" ref-type="bibr">Cox, 1972</xref>). The Cox-PH model predicts cancer patients’ hazard ratio after taking censored patient samples into consideration. Hazard ratio is a measure of how likely a patient is to die. A lower hazard ratio means the patient is more likely to survive. The Cox-PH model is defined as follows.
<disp-formula id="E12"><label>(12)</label><mml:math id="M12"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="E13"><label>(13)</label><mml:math id="M13"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msubsup><mml:mo>θ</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE6"><mml:math id="IM6"><mml:mo>ϕ</mml:mo></mml:math></inline-formula> is the log hazard ratio for patient i and <italic>θ</italic> is the trainable parameters of the model. The objective function of the Cox-PH model is a negative partial log likelihood defined as follows.
<disp-formula id="E14"><label>(14)</label><mml:math id="M14"><mml:mrow><mml:mover accent="true"><mml:mi>θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mo>θ</mml:mo></mml:munder><mml:mo>−</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>t</italic> is a sample’s survival time. The condition <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> means to select samples whose survival time is longer than the <italic>i</italic>th sample’s survival time. <italic>C</italic>(<italic>i</italic>) is an indicator which has a value of 1 when the death event of <italic>i</italic>th patient sample is observed.</p>
      <p>Our VAECox model architecture is a combination of the encoder layers of our VAE model and the Cox-PH model. Therefore, the objective function of VAECox is also the negative partial log likelihood defined at <xref ref-type="disp-formula" rid="E14">Equation 14</xref>, but the log hazard ratio of our VAECox using the pre-trained encoder defined at <xref ref-type="disp-formula" rid="E5">Equation 5</xref> is defined as follows:
<disp-formula id="E15"><label>(15)</label><mml:math id="M15"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mo>μ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></disp-formula>where <italic>W</italic> and <italic>b</italic> are the trainable parameters of the model.</p>
    </sec>
    <sec>
      <title>2.4 Transfer learning</title>
      <p>As explained above, transfer learning involves reusing a pre-trained model which was previously trained on a large dataset. We first trained our VAE on all TCGA RNA-seq data samples of 20 cancer types. We then transferred the pre-trained weights to the encoder layers attached to the Cox-PH layer in our VAECox model. To clarify, the weight values in the VAECox’s encoder layers were initialized with the pre-trained VAE encoder weights, whereas the remaining ones were randomly initialized. The encoder weights in VAECox were fine-tuned during training as the gradients were back-propagated.</p>
    </sec>
    <sec>
      <title>2.5 Experimental setting</title>
      <p>Prior to using transfer learning, we initially trained the VAE model on a combined dataset of 20 gene expression datasets of different cancer types. About 80% of this data was used for training the model, whereas the remaining 20% was used for evaluating model performance. The VAE model is trained with unsupervised learning as the model learns to copy input gene expression values to its output. Then we combined the encoder of the pre-trained VAE model with the two-layer Cox regression model and denote it as VAECox. We trained our VAECox with supervised learning. The input is a sample’s gene expression values and the output is the sample’s log hazard ratio. The model is trained to predict the log hazard ratio in the same order as the patients’ survival time based on the negative partial log-likelihood which is the objective function of VAECox.</p>
      <p>For a fair comparison, our VAECox was evaluated in the same way as Cox-nnet (<xref rid="btaa462-B4" ref-type="bibr">Ching <italic>et al.</italic>, 2018</xref>). We compared our VAECox with baseline models on 10 TCGA gene expression datasets of different cancer types. Each cancer-specific VAECox model was trained on a different cancer dataset. We used 80% of the gene expression data as training data and evaluated the performance of the trained models on the remaining 20% of the data. The optimal hyperparameters of each model were selected based on fivefold cross validation on the training data. To avoid a sampling bias caused by the random split of training and test data, we repeated this entire evaluation process 10 times and obtained the average performance of each cancer-specific VAECox model. We used the concordance index (<italic>C</italic>-index), which is one of the most commonly used metrics to evaluate the performance of survival prediction models (<xref rid="btaa462-B16" ref-type="bibr">Harrell Jr, 2015</xref>). <italic>C</italic>-index, which ranges from 0 to 1, measures the correlation between the ranked predicted hazard ratios of patients and the ranked survival times of patients.</p>
      <p>We implemented all the baseline models using the PyTorch framework, and have made the implementations available in our GitHub repository. In the case of Cox-nnet, we used the same model structure including the number of layers, the number of hidden nodes and the activation function. Since Cox-LASSO and Cox-ridge are simple extensions of the Cox-PH model, they do not have model hyperparameters. By fivefold cross validation, we found the optimal learning rate and the regularization factors for all the baseline models.</p>
      <p>The optimal hyperparameters of our VAECox model are shown in Supplementary Table S1. The third layer of VAECox has 12 hidden units. We used tanh as our activation function and optimized our model with Adam optimizer. We used PyTorch framework to implement our model. We used 80% of the gene expression data as training data, 10% of the data as validation data and the remaining 10% as test data, The hyperparameters were selected based on validation results. Training our VAECox including the VAE for a single cancer type takes around 4 h and 6 GB GPU memory. We used a NVIDIA Titan XP (12 GB) GPU to train and evaluate our VAECox.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Survival prediction results</title>
      <p>As shown in <xref ref-type="fig" rid="btaa462-F2">Figure 2</xref>, our VAECox model outperformed Cox-ridge, Cox-LASSO and Cox-nnet on most of the cancer types. Among the 10 cancer types selected for evaluation in this study, our VAECox model outperformed the other baseline models on seven cancer types (BRCA, HNSC, KIRC, LGG, LIHC, OV and STAD) in terms of average <italic>C</italic>-index. We also calculated the micro-average <italic>C</italic>-index on 10 cancer types. Micro-average considers the number of samples evaluated for each cancer type, when computing the average metric for all 10 cancer types and is mathematically expressed as,
<disp-formula id="E16"><label>(16)</label><mml:math id="M16"><mml:mrow><mml:mtext>avg</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <italic>n<sub>i</sub></italic> is the number of samples and <italic>c<sub>i</sub></italic> is the <italic>C</italic>-index for cancer-type index <italic>i</italic>.
</p>
      <fig id="btaa462-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>The box plot for the performance of the following survival prediction models on 10 cancer types: Cox-ridge, Cox-LASSO, Cox-nnet and our model VAECox. We randomly split the data into training (80%) and test sets (20%). We repeated this process 10 times and obtained 10 <italic>C</italic>-index scores. The white triangle of each box denotes the average of 10 <italic>C</italic>-index scores. The optimal hyperparameters were selected by fivefold cross validation on the training set</p>
        </caption>
        <graphic xlink:href="btaa462f2"/>
      </fig>
      <p>Our VAECox model outperformed Cox-ridge, Cox-LASSO and Cox-nnet by 0.046, 0.040 and 0.016 in terms of micro-average <italic>C</italic>-index, respectively. These results confirm that our transfer-learning method is a viable approach for improving the performance in predicting patient survival for most cancer types.</p>
      <p>The VAECox model was also utilized to perform further survival analysis. We divided the patient samples for each of the 10 cancer types into high- and low-risk groups based on their predicted hazard ratios. A patient sample is included in high-risk group when the hazard ratio of the sample is higher than the median hazard ratios of all patient samples. We also carried out same analysis with the baseline Cox-nnet. <xref ref-type="fig" rid="btaa462-F3">Figure 3</xref> shows the Kaplan–Meier plots and the log-rank test results of the high- and low-risk groups. The interesting observation is the <italic>P</italic>-value of our VAECox is lower than <italic>P</italic>-value of Cox-nnet in BLCA where our VAECox does not outperform Cox-nnet in terms of <italic>C</italic>-index. It means our VAECox shows a better performance to split samples into the high and low risk groups than the Cox-nnet.
</p>
      <fig id="btaa462-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Kaplan–Meier plots and results of the 10 cancer-types’ test patient samples from the log-rank test with VAECox and Cox-nnet. The patient samples are divided into high- and low-risk groups based on the predicted hazard ratios. A patient sample is included in high-risk group when the hazard ratio of the sample is higher than the median hazard ratios of all patient samples</p>
        </caption>
        <graphic xlink:href="btaa462f3"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Effectiveness of VAE</title>
      <p>We performed further analysis to investigate how the transferred weights of our pre-trained VAE model affects the performance of our VAECox model in predicting patient survivals. The results obtained after transferring and fine-tuning weights in three cases are provided below:
</p>
      <list list-type="bullet">
        <list-item>
          <p><italic>Weight transfer from VAE without fine-tuning</italic> yielded a micro-average <italic>C</italic>-index score of 0.569.</p>
        </list-item>
        <list-item>
          <p><italic>No weight transfer from VAE</italic> yielded a micro-average <italic>C</italic>-index score of 0.629.</p>
        </list-item>
        <list-item>
          <p><italic>Weight transfer from VAE with fine-tuning</italic> yielded a micro-average <italic>C</italic>-index score of 0.649, which is our best result.</p>
        </list-item>
      </list>
      <p>Interestingly, our pre-trained VAECox model achieved better <italic>C</italic>-index results than the randomly initialized VAECox model on the same seven cancer types where our pre-trained VAECox outperforms Cox-nnet. The results demonstrate the effectiveness of transfer learning. Both randomly initialized VAECox and Cox-nnet do not use transfer learning.</p>
      <p>Our VAE model may have difficulty in learning survival-related characteristics of cancer types as VAE is designed for dimension reduction and reconstruction of input data. However, considering the results obtained after transferring encoder weights to VAECox and fine-tuning them during training, we can assume that the learned parameters of the VAE model based on pan-cancer expression data can be beneficial to the Cox model in learning cancer survival-related features. This is further discussed in Section 4.</p>
      <p>We also conducted additional experiments to verify the validity of training the VAE with samples of heterogeneous cancer types. We trained the VAE with only samples of target cancer type and transferred the encoder part of the AE to our Cox regression layer. The micro-average of 10 concordance index scores of this experiment is 0.626, which is 0.023 lower than the performance of originally proposed VAECox. These results show that the VAE trained with heterogeneous cancer types are more beneficial to our VAECox model.</p>
      <p>Finally, we did experiments on the transfer learning and Cox regression approach setting with other AEs, a simple AE and a stacked de-noising AE. We obtained 0.623 of micro-averaged concordance indices when using the simple AE, and 0.638 of micro-averaged concordance indices when using the stacked de-noising AE. The result shows the model structure of simple AE is not sufficient to capture the common characteristics of heterogeneous cancer types, and the stacked de-noising AE is better than the simple AE because of the increased model complexity but does not outperform the VAE.</p>
    </sec>
    <sec>
      <title>3.3 Feature analysis of VAECox</title>
      <p>We further investigated the hidden nodes of the model to find which genes were significant and which pathways were important for patient survival. We assumed that hidden nodes with high variance have a crucial role to discriminate the patient samples. At first, we extracted the top nodes with the highest variance in each of the second and third hidden layers. Then we calculated Pearson’s correlation between the values of each hidden node and the expression of each gene across all patient samples in the BRCA dataset. <xref ref-type="fig" rid="btaa462-F4">Figure 4</xref> shows the correlation values of the top five genes which have the highest absolute correlation values for each hidden node of the third layer.
</p>
      <fig id="btaa462-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Pearson’s correlation values of the top five genes which have the highest absolute correlation values for each hidden node of the third layer in the BRCA dataset</p>
        </caption>
        <graphic xlink:href="btaa462f4"/>
      </fig>
      <p>To examine the association between breast cancer and the genes highly correlated with the hidden nodes of the Cox layer shown in <xref ref-type="fig" rid="btaa462-F4">Figure 4</xref>, we conducted a literature survey on the genes. Most of the genes are cancer-related genes, and some of them have an explicit association with breast cancer and breast cancer patients’ survival. The CDC20 gene is an essential component of cell division, and the high CDC20 expression is reported to be associated with the poor survival of breast cancer patients (<xref rid="btaa462-B22" ref-type="bibr">Jiang <italic>et al.</italic>, 2011</xref>; <xref rid="btaa462-B24" ref-type="bibr">Karra <italic>et al.</italic>, 2014</xref>). Overexpression of the C9orf86 gene, also known as RBEL1, is correlated with the survival of breast cancer patients (<xref rid="btaa462-B31" ref-type="bibr">Li <italic>et al.</italic>, 2013</xref>; <xref rid="btaa462-B50" ref-type="bibr">Yoshimura <italic>et al.</italic>, 2016</xref>). The MAMDC2 gene, whose function is unknown, is reported to be highly correlated with the disease-free survival of breast cancer patients (<xref rid="btaa462-B35" ref-type="bibr">Mannelqvist <italic>et al.</italic>, 2014</xref>; <xref rid="btaa462-B36" ref-type="bibr">Meng <italic>et al.</italic>, 2016</xref>). The high HJURP expression level of is reported to be a prognostic marker of breast cancer (<xref rid="btaa462-B8" ref-type="bibr">de Oca <italic>et al.</italic>, 2015</xref>; <xref rid="btaa462-B18" ref-type="bibr">Hu <italic>et al.</italic>, 2010</xref>). IKGKB is a type of NF-kappa B genes and reported to have a major driver in inflammatory breast cancer (<xref rid="btaa462-B30" ref-type="bibr">Lerebours <italic>et al.</italic>, 2008</xref>). The methylation of the KLHL17 gene is reported to be associated with early stage breast tumours and breast carcinogenesis (<xref rid="btaa462-B44" ref-type="bibr">Titus <italic>et al.</italic>, 2017</xref>). The DOT1L gene is reported to be highly associated with breast cancer and a new therapeutic target for aggressive breast cancer (<xref rid="btaa462-B5" ref-type="bibr">Cho <italic>et al.</italic>, 2015</xref>; <xref rid="btaa462-B29" ref-type="bibr">Lee and Kong, 2015</xref>; <xref rid="btaa462-B37" ref-type="bibr">Nassa <italic>et al.</italic>, 2019</xref>). The BCAP31 gene is reported to activate the downstream signalling of EGFR and drive triple-negative breast cancer (<xref rid="btaa462-B13" ref-type="bibr">Fu <italic>et al.</italic>, 2019</xref>). The SCN4B gene is reported to act as a metastasis-suppressor gene and the under-expression of the SCN4B gene is correlated with tumour progression in breast cancer (<xref rid="btaa462-B1" ref-type="bibr">Bon <italic>et al.</italic>, 2016</xref>). KIAA0247, also known as DRAGO, is reported to cooperate with p53 and KIAA0247’s overexpression is reported to result in cell death in breast cancer (<xref rid="btaa462-B19" ref-type="bibr">Huang <italic>et al.</italic>, 2011</xref>; <xref rid="btaa462-B40" ref-type="bibr">Polato <italic>et al.</italic>, 2014</xref>).</p>
      <p>We then explored the function of hidden nodes by pathway enrichment test with the correlation values of genes. As the correlation value between a gene and a hidden node means how much the gene contributes to the value of the hidden node, we can obtain a ranked gene list for each hidden node using the correlation values. Using the ranked gene list, we found enriched pathways in KEGG pathway database. <xref ref-type="fig" rid="btaa462-F5">Figure 5</xref> shows the enriched pathways for each hidden node of the third layer, and Supplementary Figure S1 shows that of the second layer.
</p>
      <fig id="btaa462-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Enriched KEGG pathways for each hidden node of the third layer in our VAECox model trained using the BRCA dataset. The pathway enrichment test is conducted using the correlation values between a vector of hidden node and a vector of gene expression value across all BRCA samples</p>
        </caption>
        <graphic xlink:href="btaa462f5"/>
      </fig>
      <p>We found an interesting difference when comparing the enriched pathways of the third layer that is the third layer of our VAECox, and the previous hidden layer that is the second layer of our VAECox. In case of the third layer, the pathways where many nodes are enriched are related to cancer, such as pathways in cancer, PI3K-Akt signalling pathway and Jak-STAT signalling pathway. But the enriched pathways in the second layer are related to only metabolism, such as fatty acid metabolism, propanoate metabolism and retinol metabolism. We can see that the third layer learns biologically basic and essential information and the second layer learns complex and disease-specific information. This observation is an interesting example in biological domain showing that a deep-learning model learns the basic signals in the front layer, and learns the high-level signals by abstracting the basic signals in the later layer.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <sec>
      <title>4.1 Effect of transfer learning for effective parameter initialization</title>
      <p>To use transfer learning, we pre-trained our VAE model on a combined set of 20 gene expression datasets for different cancer types. We then transferred the encoder weights of the pre-trained VAE model to our VAECox model by initializing the weights of the VAECox model with the transferred encoder weights of the VAE model. While we did not fine-tune the transferred weights when training our VAECox model, we found that our VAECox model did not show significant improvement. Our pre-trained VAE model is designed to learn compression and reconstruction-related signals, and not survival-related latent features.</p>
      <p>However, the encoder weights of the pre-trained VAE model contain information related to cancer, which were obtained when training our VAE model on the combined set of 20 gene expression datasets. As mentioned above, our objective is to optimize our VAECox model designed to predict the survival of cancer patients. We can use the encoder weights as the initial weights for our VAECox model and fine-tune the weights when training VAECox. The performance of our VAECox model improved more when the transferred encoder weights were fine-tuned compared to when the weights of VAECox were randomly initialized. The performance of VAECox was also higher when the transferred weights were fine-tuned than when transferring the encoder weights that were not fine-tuned. This indicates that the encoder weights of our pre-trained VAE model are effective when the transferred weights are used as the initial weights of our VAECox model and optimized to predict patient survival. The optimization of the transferred weights is required since the optimization enables the transferred weights to have survival-related information. In other words, the transfer-learning approach for customized weight initialization is effective in training our VAECox model on cancer data. Previous studies examined the importance of weight initialization (<xref rid="btaa462-B9" ref-type="bibr">Dewa <italic>et al.</italic>, 2018</xref>; <xref rid="btaa462-B15" ref-type="bibr">Hanin and Rolnick, 2018</xref>; <xref rid="btaa462-B33" ref-type="bibr">Li <italic>et al.</italic>, 2017</xref>; <xref rid="btaa462-B43" ref-type="bibr">Sutskever <italic>et al.</italic>, 2013</xref>). The well-initialized weights can help a model learn and improve its performance as it help escape the model trapped at local optima, which is one of the main reasons for model overfitting.</p>
    </sec>
    <sec>
      <title>4.2 VAE as a pre-trained model</title>
      <p>The main objective of the pre-training phase is to extract common cancer knowledge on multiple heterogeneous cancer types. A simple AE is prone to overfit on some features among various cancer characteristics as it aims to perfectly reconstruct the given input data. However, as a VAE is trained using the objective function based on the reconstruction loss between a given input and a decoded output based on a randomly sampled vector, it can learn robust features among various cancer characteristics.</p>
      <p>The main objective of the survival prediction phase is predicting deterministic patient survival, not generating probabilistic samples. The <italic>μ</italic> vector from the mean encoder in VAE can be treated as patient representation signals while the <italic>σ</italic> vector from the variance encoder in VAE can be treated as confidence signals aligned with the <italic>μ</italic> vector signals. We decided to transfer only the mean encoder weights to VAECox since the patient representation may change due to fine-tuning, depending on which type of cancer survival prediction the patient representation is used in.</p>
    </sec>
    <sec>
      <title>4.3 Cancer types where VAEcox did not outperform the baseline model</title>
      <p>Although our VAECox model did not outperform Cox-nnet on three cancer types (BLCA, LUAD and LUSC) in terms of <italic>C</italic>-index, our log-rank test results (<xref ref-type="fig" rid="btaa462-F3">Fig. 3</xref>) demonstrate that VAECox obtains higher <italic>P</italic>-value than Cox-nnet in BLCA survival prediction. We investigated why our VAECox model did not outperform Cox-nnet in LUAD and LUSC survival predictions. Both LUAD and LUSC are subtypes of non-small cell lung cancer (NSCLC). Among various types of cancer, NSCLCs are known to have distinct and heterogeneous characteristics. Therefore, we believe that common cancer characteristics extracted from samples of various cancer types affected the relatively poor survival of the heterogeneous LUAD and LUSC patients as the transferred knowledge acted as noise.</p>
      <p>The objective of this study is to demonstrate that our VAECox model’s approach of using a VAE of multiple heterogeneous cancer types and transfer learning is effective in improving the prediction of patient survival. Even though VAECox does not outperform the baseline model on all cancer types, we believe VAECox can be used to complement other state-of-the-art survival analysis models, instead of replacing them.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>In this work, we introduced VAECox which is a deep-learning-based survival prediction model. VAECox is a combination of a VAE and a Cox-PH model. We trained the VAE on transcriptomics data for 20 cancer types, and transferred the knowledge to cancer-specific survival prediction models. We showed that our VAECox model outperforms other baseline models on 7 cancer types among 10 cancer types, and extracted genes significant for patient survival based on predicted risks. In addition, we investigated the effectiveness of our VAE and discovered that the pre-trained encoder weights help train our VAECox model to learn features that can be used for patient survival prediction.</p>
    <p>We believe that our VAECox model which can be used for cancer patient survival analysis can benefit researchers in various fields. We also believe that this study which demonstrates the effectiveness of transfer learning will aid researchers in other fields.</p>
    <p>Despite of our efforts, overfitting still remains an obstacle. When it comes to high-dimensional transcriptomics data, the number of features still greatly exceeds the number of patient samples. One of the alternative approaches to address this issue is applying prior knowledge. Recent works have suggested taking biological interactions between different genes or proteins into consideration to develop more effective, robust models (<xref rid="btaa462-B10" ref-type="bibr">Dimitrakopoulos <italic>et al.</italic>, 2018</xref>; <xref rid="btaa462-B20" ref-type="bibr">Huang <italic>et al.</italic>, 2019a</xref>). In future works, we plan to utilize biological networks such as protein–protein interactions to better represent the omics information of cancer patients.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btaa462_Supplementary_Data</label>
      <media xlink:href="btaa462_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors thank Susan Kim for suggestions and editing of the manuscript.</p>
    <sec>
      <title>Funding</title>
      <p>This research was supported by the National Research Foundation of Korea (NRF-2017R1A2A1A17069645, NRF-2016M3A9A7916996 and NRF-2014M3C9A3063541).</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa462-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bon</surname><given-names>E.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>SCN4B acts as a metastasis-suppressor gene preventing hyperactivation of cell migration in breast cancer</article-title>. <source>Nat. Commun</source>., <volume>7</volume>, <fpage>1</fpage>–<lpage>18</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bradburn</surname><given-names>M.J.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>Survival analysis part ii: multivariate data analysis —an introduction to concepts and methods</article-title>. <source>Br. J. Cancer</source>, <volume>89</volume>, <fpage>431</fpage>–<lpage>436</lpage>.<pub-id pub-id-type="pmid">12888808</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chaudhary</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Deep learning-based multi-omics integration robustly predicts survival in liver cancer</article-title>. <source>Clin. Cancer Res</source>., <volume>24</volume>, <fpage>1248</fpage>–<lpage>1259</lpage>.<pub-id pub-id-type="pmid">28982688</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ching</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Cox-nnet: an artificial neural network method for prognosis prediction of high-throughput omics data</article-title>. <source>PLoS Comput. Biol</source>., <volume>14</volume>, <fpage>e1006076</fpage>.<pub-id pub-id-type="pmid">29634719</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cho</surname><given-names>M.-H.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>DOT1L cooperates with the c-Myc-p300 complex to epigenetically derepress CDH1 transcription factors in breast cancer progression</article-title>. <source>Nat. Commun</source>., <volume>6</volume>, <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cox</surname><given-names>D.R.</given-names></name></person-group> (<year>1972</year>) 
<article-title>Regression models and life-tables</article-title>. <source>J. R. Stat. Soc. Ser. B (Methodological)</source>, <volume>34</volume>, <fpage>187</fpage>–<lpage>202</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cox</surname><given-names>D.R.</given-names></name></person-group> (<year>2018</year>) <source>Analysis of Survival Data</source>. 
<publisher-name>Routledge</publisher-name>, England, UK.</mixed-citation>
    </ref>
    <ref id="btaa462-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Oca</surname><given-names>R.M.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>The histone chaperone HJURP is a new independent prognostic marker for luminal a breast carcinoma</article-title>. <source>Mol. Oncol</source>., <volume>9</volume>, <fpage>657</fpage>–<lpage>674</lpage>.<pub-id pub-id-type="pmid">25497280</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dewa</surname><given-names>C.K.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Suitable CNN weight initialization and activation function for Javanese vowels classification</article-title>. <source>Proc. Comput. Sci</source>., <volume>144</volume>, <fpage>124</fpage>–<lpage>132</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dimitrakopoulos</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Network-based integration of multi-omics data for prioritizing cancer genes</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>2441</fpage>–<lpage>2448</lpage>.<pub-id pub-id-type="pmid">29547932</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Doersch</surname><given-names>C.</given-names></name></person-group> (<year>2016</year>) Tutorial on variational autoencoders. CoRR, <italic>abs/1606.05908, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/">http://arxiv.org/</ext-link><italic>abs/1606.05908.</italic></italic></mixed-citation>
    </ref>
    <ref id="btaa462-B12">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Fernandes</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Transfer learning with partial observability applied to cervical cancer screening. In: <italic>Iberian Conference on Pattern Recognition and Image Analysis</italic>, pp. <fpage>243</fpage>–<lpage>250</lpage>. Springer, NY, US.</mixed-citation>
    </ref>
    <ref id="btaa462-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fu</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>BCAP31 drives TNBC development by modulating ligand-independent EGFR trafficking and spontaneous EGFR phosphorylation</article-title>. <source>Theranostics</source>, <volume>9</volume>, <fpage>6468</fpage>–<lpage>6484</lpage>.<pub-id pub-id-type="pmid">31588230</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hanahan</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Weinberg</surname><given-names>R.A.</given-names></name></person-group> (<year>2011</year>) 
<article-title>Hallmarks of cancer: the next generation</article-title>. <source>Cell</source>, <volume>144</volume>, <fpage>646</fpage>–<lpage>674</lpage>.<pub-id pub-id-type="pmid">21376230</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hanin</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Rolnick</surname><given-names>D.</given-names></name></person-group> (<year>2018</year>) 
<article-title>How to start training: the effect of initialization and architecture</article-title>. In: <source>Advances in Neural Information Processing Systems</source>, Neural Information Processing Systems Foundation, La Jolla, CA, pp. <fpage>571</fpage>–<lpage>581</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Harrell</surname><given-names>F.E.</given-names><suffix>Jr</suffix></name></person-group>, (<year>2015</year>) <source>Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis</source>. 
<publisher-name>Springer, NY, US</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa462-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>G.E.</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname><given-names>R.R.</given-names></name></person-group> (<year>2006</year>) 
<article-title>Reducing the dimensionality of data with neural networks</article-title>. <source>Science</source>, <volume>313</volume>, <fpage>504</fpage>–<lpage>507</lpage>.<pub-id pub-id-type="pmid">16873662</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>Z.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>The expression level of HJURP has an independent prognostic impact and predicts the sensitivity to radiotherapy in breast cancer</article-title>. <source>Breast Cancer Res</source>., <volume>12</volume>, <fpage>R18</fpage>.<pub-id pub-id-type="pmid">20211017</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>C.-J.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>A predicted protein, KIAA0247, is a cell cycle modulator in colorectal cancer cells under 5-FU treatment</article-title>. <source>J. Transl. Med</source>., <volume>9</volume>, <fpage>82</fpage>.<pub-id pub-id-type="pmid">21619678</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>a) 
<article-title>Driver network as a biomarker: systematic integration and network modeling of multi-omics data to derive driver signaling pathways for drug combination prediction</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>3709</fpage>–<lpage>3717</lpage>.<pub-id pub-id-type="pmid">30768150</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Z.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>b) 
<article-title>Salmon: survival analysis learning with multi-omics neural networks on breast cancer</article-title>. <source>Front. Genet</source>., <volume>10</volume>, 166.</mixed-citation>
    </ref>
    <ref id="btaa462-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Ganodermanontriol (GDNT) exerts its effect on growth and invasiveness of breast cancer cells through the down-regulation of CDC20 and uPA</article-title>. <source>Biochem. Biophys. Res. Commun</source>., <volume>415</volume>, <fpage>325</fpage>–<lpage>329</lpage>.<pub-id pub-id-type="pmid">22033405</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kandaswamy</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>High-content analysis of breast cancer using single-cell deep transfer learning</article-title>. <source>J. Biomol. Screen</source>., <volume>21</volume>, <fpage>252</fpage>–<lpage>259</lpage>.<pub-id pub-id-type="pmid">26746583</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karra</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Cdc20 and securin overexpression predict short-term breast cancer survival</article-title>. <source>Br. J. Cancer</source>, <volume>110</volume>, <fpage>2905</fpage>–<lpage>2913</lpage>.<pub-id pub-id-type="pmid">24853182</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Katzman</surname><given-names>J.L.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>DeepSurv: personalized treatment recommender system using a cox proportional hazards deep neural network</article-title>. <source>BMC Med. Res. Methodol</source>., <volume>18</volume>, <fpage>24</fpage>.<pub-id pub-id-type="pmid">29482517</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D.P.</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2013</year>) Auto-encoding variational Bayes. CoRR, abs/<italic>1312.6114, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/">http://arxiv.org/</ext-link><italic>1312.6114.</italic></italic></mixed-citation>
    </ref>
    <ref id="btaa462-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kourou</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Machine learning applications in cancer prognosis and prediction</article-title>. <source>Comput. Struct. Biotechnol. J</source>., <volume>13</volume>, <fpage>8</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">25750696</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kullback</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Leibler</surname><given-names>R.A.</given-names></name></person-group> (<year>1951</year>) 
<article-title>On information and sufficiency</article-title>. <source>Ann. Math. Stat</source>., <volume>22</volume>, <fpage>79</fpage>–<lpage>86</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>J.-Y.</given-names></name>, <name name-style="western"><surname>Kong</surname><given-names>G.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Dot1l: a new therapeutic target for aggressive breast cancer</article-title>. <source>Oncotarget</source>, <volume>6</volume>, <fpage>30451</fpage>–<lpage>30452</lpage>.<pub-id pub-id-type="pmid">26427043</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lerebours</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>NF-kappa B genes have a major role in inflammatory breast cancer</article-title>. <source>BMC Cancer</source>, <volume>8</volume>, <fpage>41</fpage>.<pub-id pub-id-type="pmid">18248671</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.-Y.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Down-regulation of C9orf86 in human breast cancer cells inhibits cell proliferation, invasion and tumor growth and correlates with survival of breast cancer patients</article-title>. <source>PLoS One</source>, <volume>8</volume>, <fpage>e71764</fpage>.<pub-id pub-id-type="pmid">23977139</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B32">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Transfer learning for survival analysis via efficient l2, 1-norm regularized cox regression. In: <italic>2016 IEEE 16th International Conference on Data Mining (ICDM)</italic>, pp. <fpage>231</fpage>–<lpage>240</lpage>. IEEE, Piscataway, New Jersey, US.</mixed-citation>
    </ref>
    <ref id="btaa462-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Initializing convolutional filters with semantic features for text classification. In <italic>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</italic>, pp. <fpage>1884</fpage>–<lpage>1889</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lussier</surname><given-names>Y.A.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group> (<year>2012</year>) 
<article-title>Breakthroughs in genomics data integration for predicting clinical outcome</article-title>. <source>J. Biomed. Inf</source>., <volume>45</volume>, <fpage>1199</fpage>–<lpage>1201</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mannelqvist</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>An 18-gene signature for vascular invasion is associated with aggressive features and reduced survival in breast cancer</article-title>. <source>PLoS One</source>, <volume>9</volume>, <fpage>e98787</fpage>.<pub-id pub-id-type="pmid">24905342</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Biomarker discovery to improve prediction of breast cancer survival: using gene expression profiling, meta-analysis, and tissue validation</article-title>. <source>OncoTargets Ther</source>., <volume>9</volume>, <fpage>6177</fpage>–<lpage>6185</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nassa</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>Inhibition of histone methyltransferase DOT1L silences ERα gene and blocks proliferation of antiestrogen-resistant breast cancer cells</article-title>. <source>Sci. Adv</source>., <volume>5</volume>, <fpage>eaav5590</fpage>.<pub-id pub-id-type="pmid">30775443</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nicholson</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2001</year>) 
<article-title>EGFR and cancer prognosis</article-title>. <source>Eur. J. Cancer</source>, <volume>37</volume>, <fpage>9</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">11165124</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>S.J.</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name></person-group> (<year>2010</year>) 
<article-title>A survey on transfer learning</article-title>. <source>IEEE Trans. Knowl. Data Eng</source>., <volume>22</volume>, <fpage>1345</fpage>–<lpage>1359</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Polato</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>DRAGO (KIAA0247), a new DNA damage–responsive, p53-inducible gene that cooperates with p53 as oncosuppressor</article-title>. <source>JNCI J. Natl. Cancer Inst</source>., <volume>106</volume>, <fpage>4</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B41">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Press</surname><given-names>W.H.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) <source>Numerical Recipes 3rd Edition: The Art of Scientific Computing</source>. 
<publisher-name>Cambridge University Press</publisher-name>, Cambridge, England.</mixed-citation>
    </ref>
    <ref id="btaa462-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rendle</surname><given-names>S.</given-names></name></person-group> (<year>2012</year>) 
<article-title>Factorization machines with libFM</article-title>. <source>ACM Trans. Intell. Syst. Technol</source>., <volume>3</volume>, <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B43">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Sutskever</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) On the importance of initialization and momentum in deep learning. In: <italic>International Conference on Machine Learning</italic>, ACM, NY, pp. <fpage>1139</fpage>–<lpage>1147</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Titus</surname><given-names>A.J.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Deconvolution of DNA methylation identifies differentially methylated gene regions on 1p36 across breast cancer subtypes</article-title>. <source>Sci. Rep</source>., <volume>7</volume>, <fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">28127051</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tomczak</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>The cancer genome atlas (TCGA): an immeasurable source of knowledge</article-title>. <source>Contemp. Oncol</source>., <volume>1A</volume>, <fpage>68</fpage>–<lpage>77</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Valdes Mora</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Single-cell transcriptomics in cancer immunobiology: the future of precision oncology</article-title>. <source>Front. Immunol</source>., <volume>9</volume>, <fpage>2582</fpage>.<pub-id pub-id-type="pmid">30483257</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van’t Veer</surname><given-names>L.J.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>Gene expression profiling predicts clinical outcome of breast cancer</article-title>. <source>Nature</source>, <volume>415</volume>, <fpage>530</fpage>–<lpage>536</lpage>.<pub-id pub-id-type="pmid">11823860</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Venter</surname><given-names>J.C.</given-names></name></person-group><etal>et al</etal> (<year>2001</year>) 
<article-title>The sequence of the human genome</article-title>. <source>Science</source>, <volume>291</volume>, <fpage>1304</fpage>–<lpage>1351</lpage>.<pub-id pub-id-type="pmid">11181995</pub-id></mixed-citation>
    </ref>
    <ref id="btaa462-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Auto-encoder based dimensionality reduction</article-title>. <source>Neurocomputing</source>, <volume>184</volume>, <fpage>232</fpage>–<lpage>242</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa462-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yoshimura</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>A novel prognostic marker of non-small cell lung cancer: chromosome 9 open reading frame 86 (C9orf86)</article-title>. <source>J. Thoracic Dis</source>., <volume>8</volume>, <fpage>2284</fpage>–<lpage>2286</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
