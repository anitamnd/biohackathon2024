<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Netw Neurosci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Netw Neurosci</journal-id>
    <journal-id journal-id-type="publisher-id">netn</journal-id>
    <journal-title-group>
      <journal-title>Network Neuroscience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2472-1751</issn>
    <publisher>
      <publisher-name>MIT Press</publisher-name>
      <publisher-loc>One Broadway, 12th Floor, Cambridge, Massachusetts 02142 USA journals-info@mit.edu</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9207992</article-id>
    <article-id pub-id-type="publisher-id">netn_a_00229</article-id>
    <article-id pub-id-type="doi">10.1162/netn_a_00229</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methods</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>NeuMapper: A scalable computational framework for multiscale exploration of the brain’s dynamical organization</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Geniesse</surname>
          <given-names>Caleb</given-names>
        </name>
        <xref rid="fn1" ref-type="author-notes">†</xref>
        <xref rid="aff1" ref-type="aff"/>
        <xref rid="aff2" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chowdhury</surname>
          <given-names>Samir</given-names>
        </name>
        <xref rid="fn1" ref-type="author-notes">†</xref>
        <xref rid="aff2" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Saggar</surname>
          <given-names>Manish</given-names>
        </name>
        <xref rid="cor1" ref-type="corresp">*</xref>
        <xref rid="aff2" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="aff1">Biophysics Program, Stanford University, Stanford, CA, USA</aff>
    <aff id="aff2">Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, CA, USA</aff>
    <author-notes>
      <fn fn-type="com">
        <p>Competing Interests: The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor1">* Corresponding Author: <email xlink:href="mailto:saggar@stanford.edu">saggar@stanford.edu</email></corresp>
      <fn id="fn1">
        <label>†</label>
        <p>Equal contribution.</p>
      </fn>
      <fn>
        <p>Handling Editor: Michael Breakspear</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>01</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>6</volume>
    <issue>2</issue>
    <fpage>467</fpage>
    <lpage>498</lpage>
    <history>
      <date date-type="received">
        <day>03</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>04</day>
        <month>1</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Massachusetts Institute of Technology</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Massachusetts Institute of Technology</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. For a full description of the license, please visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="netn-06-467.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>For better translational outcomes, researchers and clinicians alike demand novel tools to distill complex neuroimaging data into simple yet behaviorally relevant representations at the single-participant level. Recently, the Mapper approach from topological data analysis (TDA) has been successfully applied on noninvasive human neuroimaging data to characterize the entire dynamical landscape of whole-brain configurations at the individual level without requiring any spatiotemporal averaging at the outset. Despite promising results, initial applications of Mapper to neuroimaging data were constrained by (1) the need for dimensionality reduction and (2) lack of a biologically grounded heuristic for efficiently exploring the vast parameter space. Here, we present a novel computational framework for Mapper—designed specifically for neuroimaging data—that removes limitations and reduces computational costs associated with dimensionality reduction and parameter exploration. We also introduce new meta-analytic approaches to better anchor Mapper-generated representations to neuroanatomy and behavior. Our new NeuMapper framework was developed and validated using multiple fMRI datasets where participants engaged in continuous multitask experiments that mimic “ongoing” cognition. Looking forward, we hope our framework will help researchers push the boundaries of psychiatric neuroimaging toward generating insights at the single-participant level across consortium-size datasets.</p>
    </abstract>
    <abstract abstract-type="summary">
      <title>Author Summary</title>
      <p>Modern neuroimaging promises to transform how we understand human brain function, as well as how we diagnose and treat mental disorders. However, this promise hinges on the development of computational tools for distilling complex, high-dimensional neuroimaging data into simple representations that can be explored in research or clinical settings. The Mapper approach from topological data analysis (TDA) can be used to generate such representations. Here, we introduce several improvements to the underlying algorithm to aid scalability and parameter selection for high-dimensional neuroimaging data. We also provide new analytical tools for annotating and extracting neurobiological and behavioral insights from the generated representations. We hope this new framework will help facilitate translational applications of precision neuroimaging in clinical settings.</p>
    </abstract>
    <kwd-group>
      <kwd>TDA</kwd>
      <kwd>Mapper</kwd>
      <kwd>Optimal transport</kwd>
      <kwd>Multitask fMRI</kwd>
      <kwd>Ongoing cognition</kwd>
      <kwd>NeuroSynth</kwd>
    </kwd-group>
    <funding-group specific-use="FundRef">
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of Mental Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000025</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>MH-119735</award-id>
        <principal-award-recipient id="recipient1">
          <name>
            <surname>Saggar</surname>
            <given-names>Manish</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute of Mental Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000025</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>MH-104605</award-id>
        <principal-award-recipient id="recipient2">
          <name>
            <surname>Saggar</surname>
            <given-names>Manish</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Stanford University MCHRI Faculty Scholar</institution>
          </institution-wrap>
        </funding-source>
        <principal-award-recipient id="recipient3">
          <name>
            <surname>Saggar</surname>
            <given-names>Manish</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="32"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>citation</meta-name>
        <meta-value>Geniesse, C., Chowdhury, S., &amp; Saggar, M. (2022). NeuMapper: A scalable computational framework for multiscale exploration of the brain’s dynamical organization. <italic toggle="yes">Network Neuroscience</italic>, <italic toggle="yes">6</italic>(2), 467–498. <ext-link xlink:href="https://doi.org/10.1162/netn_a_00229" ext-link-type="uri">https://doi.org/10.1162/netn_a_00229</ext-link></meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>INTRODUCTION</title>
    <p>Modern noninvasive brain imaging technologies such as structural and functional magnetic resonance imaging promise to not only provide a better understanding of the neural basis of behavior but also to fundamentally transform how we diagnose and treat mental health disorders (<xref rid="bib92" ref-type="bibr">Saggar &amp; Uddin, 2019</xref>). However, unlike structural imaging, which has become standard in clinical practice, the clinical use of functional imaging (e.g., fMRI) has been limited to presurgical planning and functional mapping (<xref rid="bib66" ref-type="bibr">Mitchell et al., 2013</xref>; <xref rid="bib107" ref-type="bibr">Tie et al., 2014</xref>). One of the main reasons for the lack of fMRI-based clinical translation is that the traditional neuroimaging analyses (e.g., GLM or functional connectivity) tend to measure group-averaged (or central) tendencies, largely due to the low signal-to-noise ratio of the blood oxygenation level–dependent (BOLD) signal (<xref rid="bib115" ref-type="bibr">Welvaert &amp; Rosseel, 2013</xref>, <xref rid="bib116" ref-type="bibr">2014</xref>). Relatively newer functional connectome-based predictive modeling approaches have made some progress in generating insights at the single individual level (<xref rid="bib20" ref-type="bibr">Bzdok et al., 2020</xref>; <xref rid="bib95" ref-type="bibr">Shen et al., 2017</xref>), but several methodological issues need to be resolved before their clinical application becomes a reality (<xref rid="bib31" ref-type="bibr">Dadi et al., 2019</xref>).</p>
    <p>Recently, an approach called <xref rid="def1" ref-type="def">Mapper</xref> from the field of <xref rid="def2" ref-type="def">topological data analysis</xref> (TDA) has shown promise in generating data-driven insights from fMRI data at the single-participant level (<xref rid="bib44" ref-type="bibr">Geniesse et al., 2019</xref>; <xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>). TDA is a recently developed field of mathematics that combines ideas from algebraic topology and network science (<xref rid="bib22" ref-type="bibr">Carlsson, 2014</xref>), and TDA-based algorithms have gained recognition for their ability to generate robust, interpretable, and multiscale models of high-dimensional data (<xref rid="bib45" ref-type="bibr">Giusti et al., 2016</xref>; <xref rid="bib67" ref-type="bibr">Munch, 2017</xref>; <xref rid="bib74" ref-type="bibr">Petri et al., 2014</xref>). Among these techniques, Mapper is a particularly successful method that produces a <xref rid="def3" ref-type="def"><italic toggle="yes">shape graph</italic></xref>—a graphical representation of the underlying structure or shape of the high-dimensional data (<xref rid="bib61" ref-type="bibr">Lum et al., 2013</xref>; <xref rid="bib99" ref-type="bibr">Singh et al., 2007</xref>). Although Mapper bears some similarity to established <xref rid="def4" ref-type="def">dimensionality reduction</xref> methods (<xref rid="bib12" ref-type="bibr">Belkin &amp; Niyogi, 2003</xref>; <xref rid="bib28" ref-type="bibr">Coifman &amp; Lafon, 2006</xref>; <xref rid="bib105" ref-type="bibr">Tenenbaum et al., 2000</xref>; <xref rid="bib109" ref-type="bibr">Van Der Maaten &amp; Hinton, 2008</xref>), it extends and improves upon such methods by (1) reincorporating high-dimensional information in the low-dimensional projection and thereby putatively reducing information loss due to projection, and (2) producing a compressed (and putatively robust) graphical representation of the underlying structure that can be analyzed using network science tools. The revealed graphical representation can also be annotated using meta-information to extract further insights about the underlying structure of the data. Analogous to how a geographical map encodes large-scale topographical features such as mountains, valleys, and plains, a shape graph produced by Mapper encodes essential topological features such as connectivity, adjacency, and enclosure. In the context of functional neuroimaging data, the shape graph encodes the higher order spatiotemporal features of brain activity that underlie cognition.</p>
    <p>Mapper has been previously applied to generate insights from the underlying shape of data in oncology (<xref rid="bib70" ref-type="bibr">Nicolau et al., 2011</xref>), transcriptomics (<xref rid="bib84" ref-type="bibr">Rizvi et al., 2017</xref>), spinal cord and brain injury (<xref rid="bib71" ref-type="bibr">Nielson et al., 2015</xref>), fragile X syndrome (<xref rid="bib17" ref-type="bibr">Bruno et al., 2017</xref>; <xref rid="bib85" ref-type="bibr">Romano et al., 2014</xref>), gene expression (<xref rid="bib54" ref-type="bibr">Jeitziner et al., 2019</xref>), protein interaction (<xref rid="bib93" ref-type="bibr">Sardiu et al., 2019</xref>), and materials science (<xref rid="bib58" ref-type="bibr">Lee et al., 2017</xref>). In the field of neuroimaging, Mapper has been recently used to explore the whole-brain dynamics associated with different cognitive tasks and transitions during simulated “ongoing” cognition (<xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>); visualize the distributed and overlapping patterns of neural activity associated with different categories of visual stimuli via the DyNeuSR platform (<xref rid="bib44" ref-type="bibr">Geniesse et al., 2019</xref>); and relate gene co-expression to brain function (<xref rid="bib73" ref-type="bibr">Patania et al., 2019</xref>).</p>
    <p>While initial neuroimaging applications of Mapper have been promising, several key methodological improvements to the processing pipeline are still needed, especially before the approach can be scaled up to larger consortium-style datasets. First, Mapper requires embedding the data into a low-dimensional space via a user-chosen target dimension <italic toggle="yes">d</italic> and filter function <italic toggle="yes">f</italic> : ℝ<sup><italic toggle="yes">p</italic></sup> → ℝ<sup><italic toggle="yes">d</italic></sup>. Although the Mapper pipeline includes a <xref rid="def5" ref-type="def">partial clustering</xref> step to reincorporate some of the information loss due to initial projection (<xref rid="bib99" ref-type="bibr">Singh et al., 2007</xref>), low-dimensional embedding is by definition an inefficient step due to an invariable loss of information by going down 2–3 orders of magnitude in dimensions. Second, the Mapper approach traditionally rescales the low-dimensional embedding to be inside a grid with overlapping cells. The size of the grid and the level of overlap are controlled by the resolution (<italic toggle="yes">r</italic>) and gain (<italic toggle="yes">g</italic>) parameters, respectively. A caveat with this construction is that the number of cells in a grid with fixed <italic toggle="yes">r</italic>, <italic toggle="yes">g</italic> grows exponentially in dimension <italic toggle="yes">d</italic>, leading to inefficient computations. Given recent evidence (and growing consensus) that large-scale consortium-level sample sizes are essential for accurately and reproducibly linking brain function and behavior (<xref rid="bib63" ref-type="bibr">Marek et al., 2020</xref>), computational costs and scalability have thus become critical issues. Third, although Mapper results are stable over parameter perturbations, initial fine tuning of Mapper parameters is required due to their dependence on the data acquisition parameters (<xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>). Altogether, we argue that a systematic approach is required for exploring Mapper parameters, including <italic toggle="yes">f</italic>, <italic toggle="yes">d</italic>, <italic toggle="yes">r</italic>, and <italic toggle="yes">g</italic>, in order to select those that best capture the multiscale information putatively available in the neuroimaging data.</p>
    <p>In this work, we provide significant methodological advances for each step of the Mapper processing pipeline and introduce novel approaches to generate neurobiological insights from the shape graphs. Hereinafter, we refer to our neuroimaging-focused Mapper pipeline as <italic toggle="yes">NeuMapper</italic>. Our framework moves away from dimensionality reduction altogether in favor of working directly with distance metrics in the original acquisition space, leading to a significantly faster pipeline that simultaneously avoids information loss due to low-dimensional projection. Toward optimizing parameter space exploration, we provide a semiautomatic parameter selection scheme using neuroimaging-specific objectives to remove all but a few parameter choices. Apart from the methodological advancements, we also introduce methods to generate novel neurobiological insights. For example, we introduce quantitative tools from computational <xref rid="def6" ref-type="def">optimal transport</xref> (OT) (<xref rid="bib75" ref-type="bibr">Peyré &amp; Cuturi, 2019</xref>) for better handling of overlapping graphical annotations as they take into account both global and local properties of the graph. Further, to better anchor the Mapper representations into cognitive neuroscience, we present a novel approach for annotating shape graph nodes using the <xref rid="def7" ref-type="def">NeuroSynth</xref> meta-analytic cognitive decoding framework (<xref rid="bib119" ref-type="bibr">Yarkoni et al., 2011</xref>).</p>
    <p>Using NeuMapper, we not only reproduce and independently validate the results obtained by the traditional Mapper approach (<xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>) but also reveal several new neurobehavioral insights. We show that individual differences in the <xref rid="def8" ref-type="def">mesoscale structure</xref> (e.g., modularity) of the NeuMapper-generated shape graphs could reveal important neurobehavioral insights. For example, in line with the previous work, we found that recruiting task-specific brain circuits led to better performance on the task. Further, applying tools from OT on shape graphs, we provide an avenue to study relations and dependencies between cognitive tasks. For example, we found that higher degree of overlap between brain circuits engaged during working memory and math is required for better performance on the math task. Lastly, by linking the NeuroSynth meta-analytic database with NeuMapper-generated shape graphs, we provide a new avenue to study and decode cognitively anchored changes in mental states at the highest temporal resolution. Here, we showed that such decoding could be helpful in revealing the negative impact of overreflection or attention lapses on task performance.</p>
  </sec>
  <sec id="sec2">
    <title>RESULTS</title>
    <p>Our results are grouped into three parts. In the first part (see section <xref rid="sec3" ref-type="sec">Methodological Advances in the NeuMapper Framework</xref>), we start by presenting the standard Mapper approach and the methodological advances in NeuMapper. In the next part (section <xref rid="sec4" ref-type="sec">Mesoscale Structure of Shape Graphs Informs Behavior</xref>), we show the relevance of mesoscale network statistics for deriving brain-behavior insights from shape graphs. In the final part (section <xref rid="sec5" ref-type="sec">Anchoring Shape Graphs Into Known Cognitive Constructs</xref>), we anchor shape graphs into cognitive topic terms using the meta-analysis framework of NeuroSynth (<xref rid="bib119" ref-type="bibr">Yarkoni et al., 2011</xref>).</p>
    <p>To test the efficacy of our NeuMapper approach, we used two independent fMRI datasets, of which the first was used for method development and the second was used as <italic toggle="yes">held-out</italic> data to be used only for final quantitative evaluation. In both datasets, participants performed a continuous multitask experiment with known ground truth about the timing of transitions between mental states as dictated by the task blocks. These datasets also contained task performance scores for each participant and hence could be used to ground Mapper-generated insights into behavior. Dataset 1 (<italic toggle="yes">n</italic> = 18) was previously collected by <xref rid="bib47" ref-type="bibr">Gonzalez-Castillo et al. (2015)</xref> and comprised four task blocks (180 s each; repeated twice) consisting of resting state (R), working memory (M), math/arithmetic (A), and visual attention (V) tasks. We independently acquired Dataset 2 (<italic toggle="yes">n</italic> = 32) using the same paradigm as Dataset 1, but at a faster temporal resolution and shorter duration for task blocks (90 s; repeated twice). After discarding subjects due to excessive head motion and compliance issues resulting in very low behavioral scores, we retained 25 subjects in Dataset 2 (see <xref rid="sec7" ref-type="sec">Methods</xref> for details). We used Dataset 1 for methods development, leaving Dataset 2 aside for use only in the final quantitative analysis. Further, to demonstrate the robustness of our framework, we additionally performed reliability and validation checks via (1) extensive perturbation of Mapper parameters; and (2) comparison of results from real data with data generated from the phase-randomized null models.</p>
    <sec id="sec3">
      <title>Methodological Advances in the NeuMapper Framework</title>
      <p>We first begin by describing the traditional Mapper algorithm for generating a shape graph, followed by introducing two main improvements. Briefly, a shape graph is constructed from a dataset <italic toggle="yes">X</italic> ∈ ℝ<sup><italic toggle="yes">p</italic></sup> via a four-step recipe: <italic toggle="yes">filtering</italic>, <xref rid="def9" ref-type="def"><italic toggle="yes">binning</italic></xref>, <italic toggle="yes">partial clustering</italic>, and <italic toggle="yes">graph generation</italic>. In detail: (1) a dimension-reducing <italic toggle="yes">filter</italic> function <italic toggle="yes">f</italic> : ℝ<sup><italic toggle="yes">p</italic></sup> → ℝ<sup><italic toggle="yes">d</italic></sup> computes a low-dimensional embedding of <italic toggle="yes">X</italic>, (2) the embedding is covered by overlapping <italic toggle="yes">d</italic>-dimensional hypercubes; points in <italic toggle="yes">X</italic> are said to be in the same <xref rid="def10" ref-type="def"><italic toggle="yes">cover bin</italic></xref> if their projections land in the same hypercube, (3) points in the same cover bin are further clustered into smaller <xref rid="def11" ref-type="def"><italic toggle="yes">cluster bins</italic></xref> to account for faraway points (in <italic toggle="yes">p</italic>-dimensional space) erroneously landing in the same cover bin during projection, and (4) a graph is constructed with cluster bins as nodes, and edges between cluster bins that share points. Naively, the number of bins grows exponentially in dimension and becomes prohibitively expensive (<xref rid="bib51" ref-type="bibr">Hinneburg &amp; Keim, 1999</xref>; <xref rid="bib94" ref-type="bibr">Sheikholeslami et al., 1998</xref>) to compute for <italic toggle="yes">d</italic> ≫ 2, thus putatively requiring conventional Mapper applications to rely on an initial embedding into no more than one or two dimensions. This poses a problem for neuroimaging data, where meaningful dimension reduction requires embedding dimensions ranging from five (<xref rid="bib97" ref-type="bibr">Shine et al., 2019a</xref>) to 25 (<xref rid="bib102" ref-type="bibr">Stevner et al., 2019</xref>) to 50 (<xref rid="bib113" ref-type="bibr">Vidaurre et al., 2017</xref>). Additionally, exploring the large parameter space of Mapper is prohibitively expensive. While this issue could be mitigated by restricting the search space to fit within a preestablished computational budget, currently there are no neuroimaging-specific guidelines for obtaining this restricted search space.</p>
      <p>Our first methodological contribution (<xref rid="F1" ref-type="fig">Figure 1A</xref>) is to modify the core Mapper algorithm to avoid relying on an explicit low-dimensional embedding. Instead, we start with a matrix <italic toggle="yes">D</italic> of distances (between whole or parcellated brain volumes) in the native high-dimensional space and produce a transformed matrix <italic toggle="yes">D</italic>′ that approximates the geometry of temporal trajectories through brain activity space. Specifically, we obtain <italic toggle="yes">D</italic>′ as <xref rid="def12" ref-type="def">geodesic distances</xref> on a <italic toggle="yes">reciprocal</italic>
<xref rid="def13" ref-type="def"><italic toggle="yes">k</italic>-nearest neighbor (<italic toggle="yes">k</italic>NN) graph</xref>. This construction is reminiscent of a standard <italic toggle="yes">k</italic>NN graph, where data points are nodes and each point is connected to its <italic toggle="yes">k</italic>-closest neighbors, but the reciprocal variant adds an extra pruning step that reduces the effect of outliers (<xref rid="bib79" ref-type="bibr">Qin et al., 2011</xref>). <xref rid="sec28" ref-type="sec">Supporting Information Figure S6</xref> contains a toy example motivating our use of the <xref rid="def14" ref-type="def">reciprocal <italic toggle="yes">k</italic>NN</xref>. Next, we perform an <xref rid="def15" ref-type="def"><italic toggle="yes">intrinsic</italic> binning</xref> step that produces overlapping partitions of the data, but using only <italic toggle="yes">D</italic>′ and not any ambient space (for another application of this method, see <xref rid="bib34" ref-type="bibr">Dłotko, 2019</xref>). Note that this intrinsic binning is a general way of scaling up computations and does not rely on a particular method of generating <italic toggle="yes">D</italic>′, so one could just as easily apply a moderate- to high-dimensional projection and calculate distances (e.g., Euclidean distances) to obtain <italic toggle="yes">D</italic>′. Our use of geodesic distances as above is not necessary for the framework, but effective for producing useful neurobiological insights. Overall, intrinsic binning simultaneously avoids high runtimes (<xref rid="F1" ref-type="fig">Figure 1B</xref>) and projection-related information loss (<xref rid="F1" ref-type="fig">Figure 1C</xref>) as compared to existing Mapper implementations that perform binning by constructing grids in the <italic toggle="yes">d</italic>-dimensional embedding space. <xref rid="sec28" ref-type="sec">Supporting Information Figure S1</xref> provides a visual summary of the standard Mapper algorithm and adaptations for NeuMapper.</p>
      <fig position="float" id="F1">
        <label><bold>Figure 1.</bold> </label>
        <caption>
          <p>Methodological advances in the NeuMapper framework. (A) Given, a data matrix <italic toggle="yes">X</italic>, we first compute an initial distance matrix <italic toggle="yes">D</italic> and subsequently a reciprocal <italic toggle="yes">k</italic>NN graph. Next, we compute geodesic distances on this graph to obtain a transformed matrix <italic toggle="yes">D</italic>′ that captures nonlinear structure in the data. To access the hierarchical structure of this graph, we select landmarks <italic toggle="yes">L</italic><sub>1</sub>, …, <italic toggle="yes">L</italic><sub><italic toggle="yes">r</italic></sub> on the data for a resolution parameter <italic toggle="yes">r</italic> and produce an overlapping partition with bins <italic toggle="yes">B</italic><sub>1</sub>, …, <italic toggle="yes">B</italic><sub><italic toggle="yes">r</italic></sub> centered at these landmarks. Each bin is refined by a clustering step that uses the initial distances <italic toggle="yes">D</italic>, thus yielding a set of refined bins <italic toggle="yes">C</italic><sub>1</sub>, …, <italic toggle="yes">C</italic><sub><italic toggle="yes">m</italic></sub> where <italic toggle="yes">m</italic> is possibly larger than <italic toggle="yes">r</italic>. Note that this step injects information from the native space of the data. Finally, a graph is constructed with nodes <italic toggle="yes">N</italic><sub>1</sub>, …, <italic toggle="yes">N</italic><sub><italic toggle="yes">m</italic></sub> indexed by the refined bins and edges for overlapping bins. The nodes are further annotated by the labels of the data points, which may be user provided (e.g., different tasks in a blocked design, as shown here by Tasks 1–3) or generated by meta-analytic or otherwise data-driven approaches. (B) Open-source Mapper approaches (KeplerMapper 1.2.0 and Giotto-tda 0.2.2) explicitly construct a low-dimensional embedding and create hypercubes in the low-dimensional space. The complexity of this operation increases exponentially in dimension. NeuMapper avoids this step by performing implicit changes to the underlying distance matrix and can be much faster in practice. Here we standardized NeuMapper parameters to have the same y-intercept as KeplerMapper and Giotto-Mapper. Note also that KeplerMapper and Giotto-Mapper are written in Python and may obtain other benefits from interfacing with libraries such as scikit-learn. NeuMapper results reported here are from a version written in Matlab and meant to show only that the intrinsic binning method scales well with embedding dimension in practice. These results suggest that incorporating the NeuMapper methodology into KeplerMapper will allow the best of both pipelines. Differences in KeplerMapper and Giotto-Mapper runtimes are likely due to implementation differences, although we do not investigate these differences further. (C) Geodesic distances on the <italic toggle="yes">k</italic>NN graph can be embedded in Euclidean space using multidimensional scaling (MDS), but this projection step causes distortion. The amount of distortion goes down with increasing embedding dimension. However, using a high embedding dimension leads to costly computations in the standard Mapper approach. In contrast, our approach can work directly with the <italic toggle="yes">k</italic>NN distances and avoid this projection loss.</p>
        </caption>
        <graphic xlink:href="netn-06-467-g001" position="float"/>
      </fig>
      <p>Our second methodological contribution is a semiautomated parameter selection framework that we developed to guide parameter exploration and selection. Specifically, we provide a heuristic algorithm that leverages the autocorrelation structure naturally present in fMRI data (due to the slow hemodynamic response) and returns a parameter choice that presents a mesoscale view—that is, between views that are “too local” or “too global”—of the data. See <xref rid="sec7" ref-type="sec">Methods</xref> for more details of our methodological contributions.</p>
    </sec>
    <sec id="sec4">
      <title>Mesoscale Structure of Shape Graphs Informs Behavior</title>
      <p>To ensure that our methodological advancements, for example, doing away with dimensionality reduction and utilizing an intrinsic binning strategy, can still recover previously reported neurobehavioral insights obtained using the traditional Mapper approach (<xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>), we first replicated initial results pertaining to the mesoscale properties of the shape graphs. Using our NeuMapper approach we not only replicated results based on the original dataset used by <xref rid="bib91" ref-type="bibr">Saggar et al. (2018)</xref> (i.e., Dataset 1), but also reproduced the findings using an independent dataset (Dataset 2). In addition to replicating the mesoscale properties of the shape graphs, our NeuMapper approach can also capture temporal transitions in the brain activity patterns at the level of single time frames. In this section, we also show how our NeuMapper approach extends previous work using tools from OT theory to reveal pairwise mesoscale statistics.</p>
      <p>Complex networks are often characterized by their hierarchical structure (<xref rid="bib57" ref-type="bibr">Lancichinetti et al., 2009</xref>; <xref rid="bib83" ref-type="bibr">Ravasz &amp; Barabási, 2003</xref>), ranging from local descriptors at the node or edge level to global descriptors at the whole-graph level. At the mesoscale range are cohesive groups or clusters of nodes that are more densely connected to each other than to other nodes. In the most well-known model of these mesoscales, <italic toggle="yes">community structure</italic>, a group of nodes have higher density of within-group connections than a null model graph (<xref rid="bib68" ref-type="bibr">Newman, 2006</xref>). A second, increasingly popular model is the <italic toggle="yes">core-periphery structure</italic>, where the network contains a dense core with high within-group connectivity that also occupy central positions in the network, and a periphery of nodes that are sparsely connected to each other (<xref rid="bib16" ref-type="bibr">Borgatti &amp; Everett, 2000</xref>; <xref rid="bib86" ref-type="bibr">Rombach et al., 2014</xref>). Community and core-periphery structures have both been used extensively to gain insights into predictive components of functional brain networks, and new approaches into studying such mesoscale structures promise to deliver fundamentally new insights (<xref rid="bib8" ref-type="bibr">Bassett et al., 2013</xref>; <xref rid="bib101" ref-type="bibr">Sporns, 2013</xref>).</p>
      <p>Given a graph partition that yields communities, the modularity Q-score (<italic toggle="yes">Q</italic><sub><italic toggle="yes">mod</italic></sub>) measures the quality of modularity or community structure (<xref rid="bib68" ref-type="bibr">Newman, 2006</xref>). A higher <italic toggle="yes">Q</italic><sub><italic toggle="yes">mod</italic></sub> score implies better community structure. Previously, <xref rid="bib91" ref-type="bibr">Saggar et al. (2018)</xref> annotated the shape graphs based on task blocks and computed the modularity using node-level task-based annotation as community assignment. Using Dataset 1, they found that participants whose shape graph had higher task-based modularity performed better across the tasks in the continuous multitask paradigm (CMP) (in terms of both accuracy and response time). This suggested that participants with task-specific functional activations performed better on average across different tasks. We first replicated this finding on Dataset 1 using our NeuMapper framework to ensure that our strategy of moving away from dimension reduction still recovers previously reported brain-behavior relations. Specifically, we observed a significant correlation between the average task performance (accuracy) and the task-based modularity of the shape graphs (<italic toggle="yes">r</italic> = 0.561, <italic toggle="yes">p</italic> = 0.015). Additionally, we observed a significant correlation between response time and modularity (<italic toggle="yes">r</italic> = −0.488, <italic toggle="yes">p</italic> = 0.040). We further validated the task accuracy (<italic toggle="yes">r</italic> = 0.340, <italic toggle="yes">p</italic> = 0.026) and response time (<italic toggle="yes">r</italic> = −0.360, <italic toggle="yes">p</italic> = 0.018) findings in a larger dataset, <italic toggle="yes">n</italic> = 43, combining participants from Datasets 1 and 2 (<xref rid="F2" ref-type="fig">Figure 2B</xref>). <xref rid="sec28" ref-type="sec">Supporting Information Figure S2</xref> shows the modularity-behavior correlations for the individual datasets. Further, to verify that these results cannot be reproduced via null models that preserve linear properties of the data, we carried out the analysis pipeline on phase-randomized null surrogates and observed that these correlations disappeared (<xref rid="sec28" ref-type="sec">Supporting Information Figure S3</xref>). Finally, to show that the modularity-behavior correlations described above are largely stable to parameter perturbation, we performed a grid search over a moderate region of parameter space surrounding the optimal parameter values for each dataset, and then computed modularity-behavior correlations across Datasets 1 and 2. We report heat maps of these correlations and their significance (<italic toggle="yes">p</italic> values) in <xref rid="sec28" ref-type="sec">Supporting Information Figure S4</xref>.</p>
      <fig position="float" id="F2">
        <label><bold>Figure 2.</bold> </label>
        <caption>
          <p>Mesoscale structure of shape graphs. (A) Shape graphs from two representative participants from Dataset 1 (<italic toggle="yes">S</italic><sub>07</sub> and <italic toggle="yes">S</italic><sub>14</sub>) and two representative participants from Datset 2 (<italic toggle="yes">S</italic><sub>25</sub> and <italic toggle="yes">S</italic><sub>06</sub>) colored by experimental task annotations. On the one hand, <italic toggle="yes">S</italic><sub>07</sub> from Dataset 1 is an example of a good performer (i.e., high average percent correct) and their shape graph had a highly modular structure (i.e., nodes are preferentially connected to other nodes associated with the same task). On the other hand, <italic toggle="yes">S</italic><sub>14</sub> from Dataset 1 is an example of a relatively worse performer (i.e., lower average percent correct) and their shape graph had a much less modular structure (i.e., nodes associated with different tasks are connected to each other without any preference for those associated with the same task). (B) Significant correlations between the modularity score (<italic toggle="yes">Q</italic><sub><italic toggle="yes">mod</italic></sub>) and average task performance (e.g., percent correct, response time) were observed across both datasets (after <italic toggle="yes">z</italic>-scoring within each dataset, separately). These results replicate and strengthen the findings by <xref rid="bib91" ref-type="bibr">Saggar et al. (2018)</xref> that higher modularity in the shape graph was associated with better task performance. (C) Separation between tasks as measured by core-periphery scores was observed across both datasets. These results further reproduce and strenghten the finding by <xref rid="bib91" ref-type="bibr">Saggar et al. (2018)</xref> that nodes containing resting time frames mainly resided in the periphery (i.e., lower coreness scores), while nodes associated with the more cognitively demanding tasks (e.g., memory, math) tended to localize relatively deeper inside the shape graph (i.e., higher coreness scores).</p>
        </caption>
        <graphic xlink:href="netn-06-467-g002" position="float"/>
      </fig>
      <p>Next, we examined the core-periphery mesoscale structure of shape graphs. In the context of neural processes engaged during the CMP, core nodes could represent whole-brain configurations that consistently appear across a scan session, for example, due to task-switching in a CMP or due to high cognitive demands (<xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>; <xref rid="bib97" ref-type="bibr">Shine et al., 2019a</xref>). To replicate previous finding that configurations from resting state are better represented by peripheral excursions, whereas those from high cognitive load are better represented by the core nodes, we estimated <italic toggle="yes">coreness</italic> scores for nodes in the shape graphs. We recover the core-periphery structure observed by <xref rid="bib91" ref-type="bibr">Saggar et al. (2018)</xref>, that is, one-way ANOVA revealed a significant effect of task in Dataset 1 (<italic toggle="yes">F</italic>(3, 68) = 67.0, <italic toggle="yes">p</italic> = 2.9 · 10<sup>−20</sup>) such that tasks with high cognitive load such as working memory or math were associated with nodes found relatively deep inside the core of the shape graphs, whereas resting-state nodes were relatively more peripheral. We additionally carried out the nonparametric Kruskal–Wallis test and observed a significant effect of task (<italic toggle="yes">H</italic>(3) = 55.8, <italic toggle="yes">p</italic> = 4.8 · 10<sup>−12</sup>). We further validated these findings on Dataset 2 using both one-way ANOVA (<italic toggle="yes">F</italic>(3, 96) = 35.6, <italic toggle="yes">p</italic> = 1.5 · 10<sup>−15</sup>) and the Kruskal–Wallis test (<italic toggle="yes">H</italic>(3) = 54.2, <italic toggle="yes">p</italic> = 1.0 · 10<sup>−11</sup>) with the same observation regarding working memory nodes being in the core and resting-state nodes being in the periphery. The results obtained by combining participants from Datasets 1 and 2 are shown in <xref rid="F2" ref-type="fig">Figure 2C</xref>. <xref rid="sec28" ref-type="sec">Supporting Information Figure S2</xref> shows the core-periphery structure for the individual datasets. Additionally, we observed that this core-periphery structure disappeared in the phase-randomized null surrogates (<xref rid="sec28" ref-type="sec">Supporting Information Figure S3</xref>).</p>
      <p>As a final replication step, we also examined whether our NeuMapper approach can reveal transitions in task-evoked brain activity at the level of individual time frames. Given a shape graph, we follow <xref rid="bib91" ref-type="bibr">Saggar et al. (2018)</xref> in constructing a (#time frames × #time frames) temporal connectivity matrix (TCM) that shows how each time frame is connected (or similar) to all other time frames in the graph. Using the traditional Mapper approach on Dataset 1, <xref rid="bib91" ref-type="bibr">Saggar et al. (2018)</xref> found that time frames associated with tasks (e.g., working memory, video, math) typically had a higher degree of connectivity in the TCM, while those occurring between task blocks or during rest typically displayed a lower degree of connectivity. Further, they found that the temporal evolution of the degree connectivity (i.e., of each time frame, across the entire scan) recovered the task block structure (i.e., higher degrees evoked by and maintained during non-rest task blocks) and between-task transitions (i.e., lower degrees spanning the between-task instructional periods) of the CMP. We reproduced these previous findings on the same Dataset 1 using NeuMapper (<xref rid="sec28" ref-type="sec">Supporting Information Figure S5</xref>). Further validating our new framework, these results suggest that shape graphs produced by NeuMapper can capture similar temporal properties of the data compared to those produced by the traditional Mapper approach.</p>
      <p>While the two mesoscale properties of shape graphs present critical insights about neurobehavior, they can still be thought of as first-order insights. Thus, even though these mesoscale properties inform about how individual task blocks are represented on the graph, they miss any putative second-order structure, for example, how well individual task blocks are separated from each other on the shape graphs. To better account for such second-order structures, we use tools from <italic toggle="yes">optimal transport</italic> theory (<xref rid="bib75" ref-type="bibr">Peyré &amp; Cuturi, 2019</xref>). The pie chart–based proportional annotation of a shape graph node means that each task block contributes a fraction (possibly zero) of the time points making up the node. After normalizing, each task block thus yields a probability distribution over the nodes of the graph. We compare the dissimilarities between these distributions using an OT distance <italic toggle="yes">d</italic><sub><italic toggle="yes">OT</italic></sub>. Intuitively, task annotations correspond to different landforms making up the global landscape on which whole-brain dynamics occur during the CMP, and knowledge of pairwise distances between these landforms encodes the knowledge of the global structure of the landscape. <xref rid="F3" ref-type="fig">Figure 3</xref> further illustrates this construction on Datasets 1 and 2, where we additionally visualize interpolations between the shapes of the task landforms (<xref rid="F3" ref-type="fig">Figure 3D</xref>).</p>
      <fig position="float" id="F3">
        <label><bold>Figure 3.</bold> </label>
        <caption>
          <p>Quantifying pairwise similarity between annotations via optimal transport (OT). (A–C) Individual annotations can be isolated and normalized into probability distributions, which we can visualize as landscapes. (D) Annotations collapse data across time. Transitions in their spatial distribution can be quantified via OT distances. (E) These OT distances can be interpreted as follows: <italic toggle="yes">S</italic><sub>14</sub> has small <italic toggle="yes">d</italic><sub><italic toggle="yes">OT</italic></sub> because their memory and math annotations are close in the graph, whereas <italic toggle="yes">S</italic><sub>07</sub> has large <italic toggle="yes">d</italic><sub><italic toggle="yes">OT</italic></sub> because their memory and math annotations occupy different portions of the graph. We write <italic toggle="yes">μ</italic><sub><italic toggle="yes">TM</italic></sub>, <italic toggle="yes">μ</italic><sub><italic toggle="yes">TA</italic></sub> for the task annotations corresponding to memory and arithmetic/math, respectively. (F) Because <italic toggle="yes">d</italic><sub><italic toggle="yes">OT</italic></sub> provides distances, the separation between annotations can be visualized via MDS projection. MDS plots of separation between task annotations across the two datasets are shown in the two panels. For the task annotations, the projection reveals a subtle difference in the shape graphs from Dataset 1 and 2, showing higher separation between math and memory annotations for Dataset 2 than for Dataset 1. This lack of overlap is unexpected, due to the recruitment of working memory during arithmetic tasks and may suggest poorer performance in the math task. (G) Separation between task annotations may be related to the higher separation between memory and math performance in Dataset 2 than in Dataset 1, as reported in the boxplots. (H) We further compute and report Pearson correlations between two quantities: (1) <italic toggle="yes">d</italic><sub><italic toggle="yes">OT</italic></sub> (<italic toggle="yes">μ</italic><sub><italic toggle="yes">TM</italic></sub>, <italic toggle="yes">μ</italic><sub><italic toggle="yes">TA</italic></sub>) between memory task and math/arithmetic task annotations, and (2) math performance in terms of accuracy and response time. These plots show that lower overlap between memory task and math task annotations is related to poorer math performance.</p>
        </caption>
        <graphic xlink:href="netn-06-467-g003" position="float"/>
      </fig>
      <p>The OT distances between tasks in the shape graphs possess nontrivial structure (<xref rid="F3" ref-type="fig">Figure 3</xref>). For both Datasets 1 and 2, one-way ANOVA (Dataset 1: <italic toggle="yes">F</italic>(5,102) = 11.1, <italic toggle="yes">p</italic> = 1.4 · 10<sup>−8</sup>; Dataset 2: <italic toggle="yes">F</italic>(5,138) = 3.09, <italic toggle="yes">p</italic> = 0.011) and the Kruskal–Wallis test (Dataset 1: <italic toggle="yes">H</italic>(5) = 51.6, <italic toggle="yes">p</italic> = 6.5 · 10<sup>−10</sup>; Dataset 2: <italic toggle="yes">H</italic>(5) = 24.0, <italic toggle="yes">p</italic> &lt; 0.001) revealed significant effects of task such that the whole-graph distributions of different task blocks were separated from each other.</p>
      <p>To visualize between-task distances derived from OT, we used multidimensional scaling (<xref rid="F3" ref-type="fig">Figure 3F</xref>). Across both datasets, tasks with low cognitive load (e.g., resting state and video watching) were well separated from those with putatively higher cognitive load (e.g., working memory and math). However, only in Dataset 2, math and working memory task blocks were also separated. Given the previous work that suggests a significant role of working memory during arithmetic, higher overlap between the two tasks was expected (<xref rid="bib29" ref-type="bibr">Cragg &amp; Gilmore, 2014</xref>; <xref rid="bib82" ref-type="bibr">Raghubar et al., 2010</xref>). Lack of such overlap between working memory and math could indicate poor performance during arithmetic task. We verified this OT-generated observation by examining differences in behavioral performance during working memory and math tasks in both datasets. Although the performance during working memory was not different between datasets (<italic toggle="yes">t</italic> = −0.718, <italic toggle="yes">p</italic> = 0.477), there was a significant difference in math performance between datasets (<italic toggle="yes">t</italic> = 2.69, <italic toggle="yes">p</italic> = 0.010). Participants in Dataset 2 performed significantly worse in math than those in Dataset 1 (<xref rid="F3" ref-type="fig">Figure 3G</xref>). Further, within Dataset 2, we observed a negative relation trending toward significance (<xref rid="F3" ref-type="fig">Figure 3H</xref>) between OT-derived distance between memory and math and behavioral performance during math task as measured by accuracy (<italic toggle="yes">r</italic> = −0.264, <italic toggle="yes">p</italic> = 0.091) and response time (<italic toggle="yes">r</italic> = 0.269, <italic toggle="yes">p</italic> = 0.086). Thus, providing preliminary evidence in capturing behaviorally relevant interplay between working memory and arithmetic tasks using our NeuMapper framework.</p>
    </sec>
    <sec id="sec5">
      <title>Anchoring Shape Graphs Into Known Cognitive Constructs</title>
      <p>To anchor NeuMapper-generated shape graphs into known cognitive constructs and to potentially decode mental states revealed by the shape graphs, we annotated nodes using the NeuroSynth decoding database (<xref rid="bib119" ref-type="bibr">Yarkoni et al., 2011</xref>). Each node of the shape graph was annotated by the strength of spatial cross-correlation between the brain configuration represented by that node and configurations for related cognitive topics from the NeuroSynth decoding database (<xref rid="bib119" ref-type="bibr">Yarkoni et al., 2011</xref>). A similar decoding analysis using NeuroSynth topic terms has been previously performed for time-varying functional connectivity matrices (<xref rid="bib46" ref-type="bibr">Gonzalez-Castillo et al., 2019</xref>).</p>
      <p>Here, we first selected two topic terms from the NeuroSynth <italic toggle="yes">v4-topics-50</italic> database—task-positive (Topic 002) and task-negative (Topic 010)—to annotate shape graphs. We then compared “empirical” annotations of shape graph nodes based on NeuroSynth topic maps with annotations based on the expected task structure. We hypothesized that participants’ task performance would be higher whenever the empirical and expected annotations would match and that performance would be lower in case of a mismatch.</p>
      <p>Interestingly, we observed that eliciting brain configurations that are more similar to the task-positive topic model during the expected resting-state blocks (i.e., a mismatch) was significantly negatively correlated with the percentage of correct responses (averaged across tasks; <italic toggle="yes">r</italic> = −0.635, <italic toggle="yes">p</italic> = 4.78 · 10<sup>−6</sup>) and significantly positively correlated with average response time (averaged across tasks; <italic toggle="yes">r</italic> = 0.546, <italic toggle="yes">p</italic> = 0.00015) (<xref rid="F4" ref-type="fig">Figure 4A</xref>). These findings suggest that perhaps this negative relation is associated with putative <italic toggle="yes">overreflection</italic> about tasks during periods of rest (i.e., when the participants are instructed to let their minds wander). Note that we did not find any significant correlations in the reverse case, that is, eliciting brain configurations akin to task-negative topic model during nonresting-state task blocks.</p>
      <fig position="float" id="F4">
        <label><bold>Figure 4.</bold> </label>
        <caption>
          <p>Novel insights from NeuroSynth-generated annotations. (A) We first computed an <italic toggle="yes">overreflection</italic> score to quantify how often whole-brain configurations visited during the resting state were more similar to meta-analytic brain configurations generally associated with task-related cognition (i.e., Topic 002). Here we show the shape graph for a poor performer from Dataset 1, highlighting an example where the participant may have been thinking excessively of a task when the experimental instruction was to remain at rest. The nodes are colored by the experimental rest condition and the NeuroSynth-decoded task-positive topic annotations. The corresponding plots show the Pearson correlation between the (<italic toggle="yes">z</italic>-scored) overreflection score and average task accuracy/response time. (B) We then computed a “synchronization” score to quantify how often brain configurations visited during the working memory task were more similar to the meta-analytic brain configuration generally associated with working memory. Here we show the shape graph for a good performer from Dataset 1, highlighting the annotation overlap captured by the synchronization score. The nodes are colored by the experimental working memory task structure and NeuroSynth-decoded working memory topic annotations (i.e., Topic 022). The corresponding plots show the Pearson correlation between the (<italic toggle="yes">z</italic>-scored) synchronization score and accuracy/response time for the working memory task.</p>
        </caption>
        <graphic xlink:href="netn-06-467-g004" position="float"/>
      </fig>
      <p>Next, we computed a <italic toggle="yes">synchronization</italic> score by counting the proportion of nodes where the empirical task-positive annotation coincided with the expected task (M, V, A) blocks and the task-negative annotation coincided with the expected rest (R) block. As hypothesized, we observed a significant correlation between the synchronization score and the average accuracy (<italic toggle="yes">r</italic> = 0.422, <italic toggle="yes">p</italic> = 0.005) and response time (<italic toggle="yes">r</italic> = 0.474, <italic toggle="yes">p</italic> = 0.001). To go further beyond the task-positive topic model, we then asked ourselves if specific topic models corresponding to the tasks (M, V, A) could be related to performance in each individual task. Because the n-back working memory paradigm has been widely used in the literature and has been highly sampled in the NeuroSynth database (i.e., Topic 022 is associated with 933 studies), we limited ourselves to the working memory task block. Specifically, using a specific topic model for the working memory task (Topic 022), we examined whether performance in the working memory blocks is associated with eliciting brain configurations that are more similar to the meta-analytic brain configurations generally related with working memory. Here, we again found that having a higher match between empirical and expected task annotations—that is, memory (Topic 022) annotations predicted by NeuroSynth in nodes that were also annotated with the memory experimental task label—was significantly positively correlated with the percentage of correct responses for the memory task (<italic toggle="yes">r</italic> = 0.346, <italic toggle="yes">p</italic> = 0.023) and significantly negatively correlated with response time for the memory task (<italic toggle="yes">r</italic> = −0.440, <italic toggle="yes">p</italic> = 0.003) (<xref rid="F4" ref-type="fig">Figure 4B</xref>). In other words, this finding suggests that an individual’s performance tends to increase when the participants duly engage the task-specific brain circuits while performing the task. This results also amplifies the putative role that NeuMapper can play in decoding mental states.</p>
    </sec>
  </sec>
  <sec id="sec6">
    <title>DISCUSSION</title>
    <p>We present a fast, end-to-end computational framework that incorporates and extends the Mapper algorithm—a powerful manifold learning technique within the suite of methods provided by the field of TDA (<xref rid="bib61" ref-type="bibr">Lum et al., 2013</xref>; <xref rid="bib80" ref-type="bibr">Rabadán &amp; Blumberg, 2019</xref>; <xref rid="bib99" ref-type="bibr">Singh et al., 2007</xref>)—by integrating novel algorithmic contributions as well as downstream processing techniques for capturing second-order mesoscale structure and meta-analysis guided inference. We used our NeuMapper framework to study multiple functional neuroimaging datasets where participants engaged in a CMP simulating ongoing cognition. Our adaptations of methods for approximating nonlinear geometry revealed interesting topological structure in our datasets from the outset and our heuristics for parameter selection enabled efficient discovery of meaningful scales at which to observe various types of community structure in the data. We introduced tools from OT theory (<xref rid="bib75" ref-type="bibr">Peyré &amp; Cuturi, 2019</xref>) to further understand the second-order structure of groupings of data points as delineated by experiment design, and also introduced the use of NeuroSynth-based meta-analyses (<xref rid="bib119" ref-type="bibr">Yarkoni et al., 2011</xref>) to enrich our shape graphs with additional semantic meaning. Finally, we translated these computational methods into markers of individual differences in how the brain adapts to stimuli during ongoing cognition. In summary, we provide a validated computational pipeline for neuroimaging data that can be easily used by researchers and clinicians for interactive data representation with simultaneous access to quantitative insights.</p>
    <p>The success of approaches based on dynamic methods for translating patterns of ongoing cognition into meaningful cognitive states has led to a burgeoning landscape of computational methodologies for functional neuroimaging data (<xref rid="bib7" ref-type="bibr">Bassett et al., 2011</xref>; <xref rid="bib18" ref-type="bibr">Bullmore &amp; Sporns, 2009</xref>; <xref rid="bib21" ref-type="bibr">Calhoun et al., 2014</xref>; <xref rid="bib62" ref-type="bibr">Lurie et al., 2020</xref>; <xref rid="bib76" ref-type="bibr">Preti et al., 2017</xref>; <xref rid="bib100" ref-type="bibr">Sporns, 2011</xref>; <xref rid="bib103" ref-type="bibr">Tagliazucchi et al., 2012</xref>; <xref rid="bib108" ref-type="bibr">Turk-Browne, 2013</xref>; <xref rid="bib111" ref-type="bibr">van der Meer et al., n.d.</xref>; <xref rid="bib113" ref-type="bibr">Vidaurre et al., 2017</xref>). In contrast to the dynamic functional connectivity approaches that estimate interregional coactivity over time, topology/geometry processing techniques such as Mapper instead use whole-brain activation patterns to infer the shape of an underlying landscape of brain activity (<xref rid="bib44" ref-type="bibr">Geniesse et al., 2019</xref>; <xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>). By enriching this inferred landscape with annotations coming from experimental task block labels or meta-analysis based decoding (<xref rid="bib119" ref-type="bibr">Yarkoni et al., 2011</xref>) and subsequently capturing second-order interactions across groups of annotations—both experimentally based and decoded—we show that it is possible to use TDA methods to both infer dynamics and obtain a semantic segmentation of the underlying state space. This exploration of second-order interactions is concurrent with the development of related strategies for higher order dynamic connectivity (<xref rid="bib42" ref-type="bibr">Faskowitz et al., 2020</xref>; <xref rid="bib72" ref-type="bibr">Owen et al., 2019</xref>). Overall, we corroborate and extend prior findings (<xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>) on the relation between task performance and quantitative measures of shape graphs, and further contribute to the computational methodologies for understanding the brain’s dynamic adaptations to external stimuli.</p>
    <p>Our framework extends the traditional Mapper approach without discarding any of its key properties, and the modular organization of our pipeline suggests replacements based on user preference. For example, numerous works (<xref rid="bib30" ref-type="bibr">Cunningham &amp; Yu, 2014</xref>; <xref rid="bib69" ref-type="bibr">Nichols et al., 2017</xref>) have found evidence to suggest that brain activity is fundamentally low dimensional, and that a linear technique such as principal component analysis (PCA) potentially captures this activity along interpretable axes (<xref rid="bib97" ref-type="bibr">Shine et al., 2019a</xref>; <xref rid="bib98" ref-type="bibr">Shine et al., 2019b</xref>). Our framework could be freely used to this end, by simply replacing the <italic toggle="yes">k</italic>NN graph construction with a PCA projection. At the same time, depending on the amount of variance to be explained, one may need a moderate to high number of projection dimensions when carrying out PCA. More generally, dimension reduction techniques tend to preserve more information as the embedding dimension increases (see <xref rid="F1" ref-type="fig">Figure 1C</xref>, for an example). Runtime comparisons showed that our intrinsic binning approach tended to be much faster than existing open-source Mapper implementations such as Giotto-Mapper and KeplerMapper when higher embedding dimensions were used, suggesting that our methods could be incorporated as modules within such frameworks (<xref rid="bib104" ref-type="bibr">Tauzin et al., 2021</xref>; <xref rid="bib112" ref-type="bibr">van Veen et al., 2019</xref>). We note also that techniques for high-dimensional information retrieval and clustering (<xref rid="bib3" ref-type="bibr">Agrawal et al., 2005</xref>; <xref rid="bib13" ref-type="bibr">Beyer et al., 1998</xref>; <xref rid="bib51" ref-type="bibr">Hinneburg &amp; Keim, 1999</xref>; <xref rid="bib52" ref-type="bibr">Indyk &amp; Motwani, 1998</xref>; <xref rid="bib81" ref-type="bibr">Radovanović et al., 2010</xref>) could potentially be used to develop a unified computational framework for the binning stage of the Mapper algorithm that scales well with dimension, and use this work to invite future progress in developing Mapper applications that do not use low-dimensional embeddings at the outset. Recent works have fused graph neural network techniques with Mapper to consume graph-structured data as input and return meaningful embeddings (<xref rid="bib14" ref-type="bibr">Bodnar et al., 2020</xref>), and such a module could be easily inserted between the reciprocal <italic toggle="yes">k</italic>NN and intrinsic binning steps in our framework to take advantage of the representational power of neural networks. Along the lines of hardware-driven scalability, Mapper Interactive (<xref rid="bib120" ref-type="bibr">Zhou et al., n.d.</xref>) provides state-of-the-art GPU implementations of the Mapper algorithm for embedding dimensions 1 and 2, and our method could be adapted to fit into such a pipeline. In addition to such scalability improvements, semiautomated mesoscale network structure analysis is a fundamental aspect of our pipeline, and we have demonstrated multiple ways in which a user can supply annotations (i.e., based on task structure or NeuroSynth meta-analyses) to yield quantitative results from data. Combining these modules with the functionality of existing open-source Mapper implementations may be crucial in obtaining new insights via geometric and TDA on consortium-sized data (<xref rid="bib63" ref-type="bibr">Marek et al., 2020</xref>).</p>
    <p>Toward moving Mapper analysis from exploratory to quantitative, we introduced OT techniques that naturally delineate the overlapping categorical labels of shape graph nodes and quantify the dissimilarity between categories from the perspective of the shape graph. From an interesting dual perspective, the shape graphs themselves may thus be viewed as filters through which to quantify the overall landscape of different cognitive constructs, hence lending a <italic toggle="yes">Mapper of Mappers</italic> theme to our contributions. Our use of NeuroSynth-based decoding provided a new angle on obtaining <italic toggle="yes">learned</italic> categorical labels and introduced a novel study of <italic toggle="yes">semantically-aware TDA</italic> in analogy with the semantic segmentation approaches in modern deep-learning based computer vision (<xref rid="bib78" ref-type="bibr">Qi et al., 2017</xref>). Our developments suggest that it may be helpful to view Mapper as less of a fixed, immutable algorithm, and more of a <italic toggle="yes">philosophy</italic> that may be woven into alternative and diverse computational pipelines.</p>
    <p>With regards to neuroimaging studies, an important issue that is not directly addressed in this work and requires future effort is to estimate the minimum amount of data required per individual for stable estimation of Mapper-generated shape graphs. In this and previous works using Mapper, we used 20–30 min of task fMRI data per individual. While access to large quantities of artifact-free data (<xref rid="bib50" ref-type="bibr">Gordon et al., 2017</xref>) would be ideal for any computational method, for clinical populations it is often more practical to aggregate data across sites and studies (<xref rid="bib92" ref-type="bibr">Saggar &amp; Uddin, 2019</xref>). In the current work, we carried out two forms of aggregation: (1) Dataset 2 comprised two runs for each subject that we aggregated before computing shape graphs, and (2) shape graph properties such as modularity were aggregated across both datasets when reporting relations to behavior. Note that Datasets 1 and 2 were obtained at different sites and under different acquisition parameters, including scanner strengths and repetition times. Because our framework provided successful inference on the aggregated shape graph properties, we suggest that future work could consider further aggregations of shape graph properties for data collected under more variable acquisition parameters to fully test any limitations of our method. We also note that in related work, <xref rid="bib90" ref-type="bibr">Saggar et al. (2021)</xref> applied the traditional Mapper approach to data from 100 unrelated subjects in the Human Connectome Project combined over four 15-min runs. Altogether, these observations suggest that Mapper-based frameworks may be well suited for data aggregation across runs and that shape graph properties may be aggregated across datasets acquired under different scanning parameters.</p>
    <p>While we focused on some of the most salient aspects of neuroimaging-specific Mapper design, there were certain choices that we left for future work. In our NeuMapper filter design, the landmarks obtained via <xref rid="def16" ref-type="def">farthest point sampling</xref> do not have any particular biological relevance. However, it is possible to augment this step by choosing landmarks using some additional criterion, for example, by averaging over a proportion of frames to define a “baseline” state for each task (<xref rid="bib36" ref-type="bibr">Duman et al., 2019</xref>; <xref rid="bib55" ref-type="bibr">Khambhati et al., 2018</xref>). In particular, for resting-state studies, an interesting possibility in our NeuMapper design could be to choose a single base point in the <italic toggle="yes">k</italic>NN graph representing a baseline state, and then filter the remaining graph vertices by a one-dimensional number: the distance to the base point (<xref rid="bib24" ref-type="bibr">Chowdhury et al., 2020</xref>). This one-dimensional filter setting allows the import of the statistical guarantees provided by the seminal work of <xref rid="bib23" ref-type="bibr">Carriere et al. (2018)</xref> and should be carried out in future work. Further down the Mapper pipeline, the partial clustering step is itself a specialized clustering problem for which <xref rid="bib11" ref-type="bibr">Belchi et al. (2020)</xref> have provided theoretical guarantees as well as practical frameworks utilizing <italic toggle="yes">k</italic>-fold cross-validation. For expediency, we performed quick initial checks using hierarchical and nonhierarchical clustering techniques (<xref rid="bib53" ref-type="bibr">James et al., 2000</xref>) such as average linkage, complete linkage, DBSCAN, and HDBSCAN before settling on a simplistic choice of single linkage with a fixed cutoff parameter. Future work could look into invoking neurobiologically motivated clustering techniques along with additional cross-validation (<xref rid="bib11" ref-type="bibr">Belchi et al., 2020</xref>).</p>
    <p>As shown with CMP datasets, the shape graph representation of functional brain activity converts high-dimensional neural activity recordings into a landscape on which we can study whole-brain dynamics driven by the processes underlying cognitive states, and ultimately relate these analyses to multidimensional behavioral measures and clinical outcomes. A critical next step will be to demonstrate how our NeuMapper framework can be applied in unconstrained resting-state paradigms, where there is no known ground truth of the internal cognitive state of each participant. Our demonstration of NeuroSynth-based decoding of underlying cognitive states, following <xref rid="bib46" ref-type="bibr">Gonzalez-Castillo et al. (2019)</xref>, provided strong quantitative suggestion that such an approach could be viable and successful. While we did not test our approach on resting-state or naturalistic fMRI data, we note that the standard Mapper approach (extrinsic 2-D binning) has recently been applied (<xref rid="bib90" ref-type="bibr">Saggar et al., 2021</xref>) to 5 hours of resting-state fMRI data from the Midnight Scan Club dataset (<xref rid="bib50" ref-type="bibr">Gordon et al., 2017</xref>). There the authors observed highly subject-specific shape graphs with a central “basin of attraction” surrounded by peripheral areas having distinct network configurations. Based on these results, we expect that our approach will extend nicely to resting-state and naturalistic paradigms. Moreover, the scalability of our approach will allow us to explore some of the consortium-level naturalistic datasets that are increasingly becoming available.</p>
    <p>While designing our NeuMapper framework, we paid particular attention to potential scalability issues when working with the consortium-size datasets needed to reduce statistical error in brain-behavior association studies (<xref rid="bib63" ref-type="bibr">Marek et al., 2020</xref>). Given a fixed set of parameters and assuming the use of the nonlinear reciprocal <italic toggle="yes">k</italic>NN graph construction, the most expensive computation in our framework comprises a fixed number of calls to the <italic toggle="yes">O</italic>((|<italic toggle="yes">V</italic>| + |<italic toggle="yes">E</italic>|) log(|<italic toggle="yes">V</italic>|)) Dijkstra algorithm. For standard fMRI datasets of individual participants with up to several thousand frames, this computation can be carried out within seconds on standard hardware. For group-level analysis where datasets may be concatenated to contain up to several million frames (e.g., for datasets such as the Human Connectome Project), we may utilize GPU libraries such as cuGraph or nvGRAPH to carry out such computations in a few hours. The remaining steps of the NeuMapper pipeline—namely parameter selection and post hoc computations—are easily parallelized, for example, via the implementation that we used in this work. Future work should investigate large-scale application of NeuMapper to consortium-size datasets in order to generate a population-level “template” landscape on which we can map and study the dynamics of whole-brain activation during cognition. Because shape graphs may be of different sizes, this requires solving a node-correspondence problem. This can potentially be resolved using our recently developed extensions of OT theory for scalable generation of correspondences (<xref rid="bib25" ref-type="bibr">Chowdhury &amp; Mémoli, 2019</xref>; <xref rid="bib27" ref-type="bibr">Chowdhury &amp; Needham, 2020</xref>, <xref rid="bib26" ref-type="bibr">2021</xref>). Looking forward, our OT based approach could allow researchers to carefully parse population-level heterogeneity in the data, for example, by applying PCA on the space of shape graphs (<xref rid="bib27" ref-type="bibr">Chowdhury &amp; Needham, 2020</xref>).</p>
    <p>Lastly, we expect that a powerful use case of our NeuMapper framework will be in the setting of multimodal neuroimaging data analysis, where different types and scales of information that are uniquely captured by different neuroimaging modalities are combined into an overall representation that conveys more information than any individual modality. For example, analyses of the inherent temporal structure and dynamical properties measured by fMRI data could be augmented by high-resolution anatomical information provided by diffusion tract imaging. Combining these different information-rich sources of data into unified and individual-specific descriptions and signatures of brain activity could better capture individual differences in behaviorally relevant dynamics and in turn could improve the prospects of precision medicine.</p>
    <p>In summary, we provide a computationally scalable, biologically anchored, and downstream analysis-friendly Mapper framework for application in the empirical sciences in general and neurosciences in particular.</p>
  </sec>
  <sec id="sec7">
    <title>METHODS</title>
    <sec id="sec8">
      <title>Data Acquisition</title>
      <sec id="sec9">
        <title>Dataset 1: Continuous multitask paradigm.</title>
        <p>In this study, we utilized a previously collected continuous multitask fMRI dataset to develop our framework, and we acquired a second dataset using a similar paradigm but at a faster temporal resolution to validate our approach. The first dataset was originally collected by <xref rid="bib47" ref-type="bibr">Gonzalez-Castillo et al. (2015)</xref>, using a CMP. We retrieved the data from the XNAT Central public repository (<ext-link xlink:href="https://central.xnat.org" ext-link-type="uri">https://central.xnat.org</ext-link>; Project ID: FCStateClassif). The dataset contained de-identified fMRI and behavioral data from 18 participants who completed the CMP experiments as part of the original study (<xref rid="bib47" ref-type="bibr">Gonzalez-Castillo et al., 2015</xref>). Informed consent was obtained from all participants, and the local Institutional Review Board of the National Institute of Mental Health in Bethesda, MD, reviewed and approved the CMP data collection.</p>
        <p>Details about the experimental paradigm are described elsewhere (<xref rid="bib47" ref-type="bibr">Gonzalez-Castillo et al., 2015</xref>). Briefly, participants were scanned continuously for ∼25 min while performing four different cognitive tasks. Each task was presented for two separate 180-s blocks, with each task block being preceded by a 12-s instruction period. The order of task blocks was randomized such that each task was always preceded and followed by a different task. The same random ordering of tasks was used for all participants. The four cognitive tasks were (1) Rest (R), where participants were instructed to fixate on a crosshair in the center of the screen and let their mind wander; (2) Working Memory (M), where participants were presented with a continuous sequence of individual geometric shapes and were instructed to press a button when the current shape had also appeared two shapes prior (2-back design); (3) Math/Arithmetic (A), where participants were presented with simple arithmetic operations, involving three numbers between 1 and 10 and two operands (either addition or subtraction)—operations remained on the screen for 4 s, and successive trials were separated by a blank screen that appeared for 1 s, yielding a total of 36 operations per each 180-s block—and (4) Video (V), where participants watched a video of a fish tank from a single point of view with different types of fish swimming into an out of the frame. Participants were instructed to press a button when a red crosshair appeared on a clown fish and another when it appeared on any other type of fish. These targets appeared for 200 ms with a total of 16 targets during each of the 180-s blocks.</p>
        <p>The fMRI data were acquired on a Siemens 7 Tesla MRI scanner equipped with a 32-channel head coil using a whole-brain echo planar imaging (EPI) sequence (repetition time [TR] = 1.5 s, echo time [TE] = 25 ms, and voxel size = 2 mm isotropic). A total of 1,017 volumes were acquired while participants performed the CMP.</p>
        <p>Functional and anatomical MR images were preprocessed using the Configurable Pipeline for the Analysis of Connectomes (C-PAC version 0.3.4; <ext-link xlink:href="https://fcp-indi.github.io/docs/user/index.html" ext-link-type="uri">https://fcp-indi.github.io/docs/user/index.html</ext-link>). Details about this processing are provided elsewhere (<xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>). Briefly, the fMRI data preprocessing steps included ANTS registration into MNI152 space, slice timing correction, motion correction, skull stripping, grand mean scaling, spatial smoothing (FWHM of 4 mm), and temporal band-pass filtering (0.009 Hz &lt; f &lt; 0.08 Hz). For each voxel, nuisance signal correction was performed by regressing out linear and quadratic trends, physiological noise (white matter and cerebrospinal fluid), motion-related noise (three translational and three rotational head-motion parameters) using the Volterra expansion (<xref rid="bib43" ref-type="bibr">Friston et al., 1996</xref>) (i.e., six parameters, their temporal derivatives, and each of these values squared), and residual signal unrelated to neural activity extracted using the CompCor algorithm (<xref rid="bib10" ref-type="bibr">Behzadi et al., 2007</xref>) (i.e., five principal components derived from noise regions in which the time series data were unlikely to be modulated by neural activity). The resulting data were brought to 3-mm MNI space, and the mean time series was extracted from 375 predefined regions of interest (ROIs) using the <xref rid="bib96" ref-type="bibr">Shine et al. (2016)</xref> atlas. The atlas includes 333 cortical regions from the <xref rid="bib49" ref-type="bibr">Gordon et al. (2016)</xref> atlas, 14 subcortical regions from the Harvard-Oxford subcortical atlas, and 28 cerebellar regions from the SUIT atlas (<xref rid="bib33" ref-type="bibr">Diedrichsen et al., 2009</xref>). Before running Mapper, the preprocessed ROI time series data were converted to <italic toggle="yes">z</italic>-scores, and individual ROIs with zero variance were excluded.</p>
        <p>The behavioral data included both responses and reaction times for working memory, math, and video tasks. Participants were instructed to respond as quickly and accurately as possible with only one response per question. Behavior scores including the percent correct, percent missed, and response times for Working Memory (M), Math/Arithmetic (A), and Video (V) tasks were computed for each participant.</p>
      </sec>
      <sec id="sec10">
        <title>Dataset 2: Continuous multitask experiment.</title>
        <p>We collected the second fMRI dataset at a faster temporal resolution using a modified CMP (<xref rid="bib47" ref-type="bibr">Gonzalez-Castillo et al., 2015</xref>) as part of a simultaneous EEG and MRI functional neuroimaging (SEMFNI) study. The raw data includes fMRI scans and behavioral scores collected for 32 participants who completed two separate continuous multitask runs within a single scanning session, as part of the SEMFNI study. Informed consent was obtained from all participants, and the Stanford University Institutional Review Board reviewed and approved the SEMFNI data collection.</p>
        <p>Briefly, the paradigm used by the continuous multitask experiment was adapted from the original CMP (<xref rid="bib47" ref-type="bibr">Gonzalez-Castillo et al., 2015</xref>) so that data could be collected for two separate runs over the course of a single ∼30-min session. For each run, participants were scanned continuously for ∼15 min while performing four different cognitive tasks. Each task was presented for two separate 90-s blocks, with each task block being preceded by a 12-s instruction period. The order of task blocks presented during each run was randomized such that each task was always preceded and followed by a different task. The same random ordering of tasks was used for all participants and for both runs. The randomized order of trials presented during each task block was modified from the original paradigm (<xref rid="bib47" ref-type="bibr">Gonzalez-Castillo et al., 2015</xref>) such that the trials presented during the first run were different than the trials presented during the second run (i.e., trials from the first 90 s of each 180-s task block in the original paradigm were used for the first run of the modified paradigm, trials from the second half of each 180-s task block in the original paradigm were used for the second run of the modified paradigm).</p>
        <p>The fMRI data were acquired on a modified Siemens Skyra 3 Tesla MRI scanner equipped with a 32-channel head coil using a whole-brain EPI sequence (repetition time [TR] = 750 ms, echo time [TE] = 30 ms, and voxel size = 2 mm isotropic). A multiband acceleration factor of 8 was used to increase temporal resolution. A total of 2,160 volumes were acquired over two separate runs. The 1,080 volumes acquired during each run were preprocessed separately.</p>
        <p>Functional and anatomical MR images were preprocessed using fMRIPrep (version 1.5.4; <ext-link xlink:href="https://fmriprep.readthedocs.io/" ext-link-type="uri">https://fmriprep.readthedocs.io</ext-link>) (<xref rid="bib38" ref-type="bibr">Esteban et al., 2018a</xref>, <xref rid="bib39" ref-type="bibr">2018b</xref>). Before running fMRIPrep, bias correction was performed on individual anatomical MR images using FMRIB’s Automated Segmentation Tool. Individual functional MR images and the bias-corrected anatomical MR images were transformed into MNI152 space using FSL’s MNI ICBM 152 nonlinear 6th Generation Asymmetric Average Brain Stereotaxic Registration Model (<xref rid="bib40" ref-type="bibr">Evans et al., 2012</xref>). The fMRI data preprocessing steps included skull stripping, head motion and susceptibility distortion correction, boundary-based registration, and confound collection. After running fMRIPrep, motion censoring, demeaning, detrending, nuisance signal correction, and temporal filtering were performed in Matlab. Here, we only considered voxels within the gray matter. The brain masks provided by fMRIPrep were brought to 2-mm MNI space (in version 1.5.4 the provided brain masks retain the original 1-mm pixel dimensions), and these 3-D spatial masks were then used to extract the gray-matter voxel time series from the preprocessed 4-D fMRI data. The resulting 4-D fMRI signals were then transformed into a 2-D matrix. The first 10 frames (i.e., nonsteady state) of data were discarded, and individual time frames with high framewise displacement (FD &gt;0.5 mm) were annotated as motion outliers. The data corresponding to these frames were replaced with NaN values, such that these frames were ignored during subsequent processing steps. After motion censoring, the voxel-wise data were demeaned, and linear trends were removed. For each voxel, nuisance signal correction was then performed by regressing out physiological noise (white matter and cerebrospinal fluid) and motion-related noise (three translational and three rotational head-motion parameters) using the Volterra expansion (<xref rid="bib43" ref-type="bibr">Friston et al., 1996</xref>) of the two physiological signals and six motion parameters (i.e., eight parameters, their temporal derivatives, and each of these values squared). Linear interpolation was applied to the residual voxel signals to smooth over any missing data corresponding to high-motion frames, and temporal band-pass filtering (0.009 Hz &lt; f &lt; 0.08 Hz) was performed. The mean time series was extracted from 375 predefined ROIs using the <xref rid="bib96" ref-type="bibr">Shine et al. (2016)</xref> atlas. The atlas includes 333 cortical regions from the <xref rid="bib49" ref-type="bibr">Gordon et al. (2016)</xref> atlas, 14 subcortical regions from the Harvard-Oxford subcortical atlas, and 28 cerebellar regions from the SUIT atlas (<xref rid="bib33" ref-type="bibr">Diedrichsen et al., 2009</xref>). Before running Mapper, individual runs were concatenated together, and individual ROIs with zero variance were excluded.</p>
        <p>Behavioral data was collected during each run, including responses and reaction times for the different tasks. Participants were instructed to respond as quickly and accurately as possible with only one response per question. Behavior scores including the percent correct (%C), percent missed, and reaction times for Working Memory (M), Math/Arithmetic (A), and Video (V) tasks were computed (and averaged across runs) for each participant.</p>
        <p>Note that one participant’s data were excluded from preprocessing altogether due to inconsistent data acquisition (i.e., too many volumes were collected during the second run due to a technical issue), one participant was excluded from our final analysis due to excessive in-scanner head motion (i.e., more than 10% of the acquired datapoints had framewise displacement above 0.5 mm), and five participants were excluded from the final analysis because their average %Correct scores were substantially lower than those of the other participants. Specifically, after plotting all average %Correct scores on a histogram with bins of width 10, the scores of these five participants comprised a left tail (i.e., very poor performance) that was clearly separated from the scores of all other participants. At the final stage, Dataset 2 included 25 participants. Overall, this yielded <italic toggle="yes">n</italic> = 43 participants across both datasets.</p>
      </sec>
    </sec>
    <sec id="sec11">
      <title>The NeuMapper Framework</title>
      <sec id="sec12">
        <title>Geometric and TDA for neuroimaging data.</title>
        <p>Consider an <italic toggle="yes">n</italic> × <italic toggle="yes">p</italic> data matrix <italic toggle="yes">X</italic> where the columns correspond to voxels or brain ROIs and the rows correspond to acquisitions of whole-brain activation patterns. Standard neuroimaging techniques such as functional connectivity (FC) or dynamic functional connectivity (dFC) work on the columns of the data matrix to produce one or more <italic toggle="yes">p</italic> × <italic toggle="yes">p</italic> correlation matrices. While such techniques capture the coactivation of brain regions in response to external stimuli, an alternative approach is to study <italic toggle="yes">n</italic> × <italic toggle="yes">n</italic> matrices of distances between acquisitions to understand the overall geometry of the states traversed by the brain during the experiment. Examples such as Anscombe’s quartet (<xref rid="bib5" ref-type="bibr">Anscombe, 1973</xref>) and related generalizations (<xref rid="bib64" ref-type="bibr">Matejka &amp; Fitzmaurice, 2017</xref>) show that studying this underlying geometry may provide strictly complementary insights, and in our work we emphasize the importance of studying this geometry of brain states.</p>
        <p>Each of our data matrices had rows labeled by the tasks in the experiment design (Rest, R; Memory, M; Video, V; and Math/Arithmetic, A). To obtain a coarse understanding of the high-dimensional geometry of the data, we first applied a variety of linear and nonlinear dimension reduction methods (<xref rid="bib110" ref-type="bibr">Van Der Maaten et al., 2009</xref>) to data from a single subject (<italic toggle="yes">S</italic><sub>01</sub> from Dataset 1) and visualized the results in simple 3-D plots. The full list of methods and plots is provided in <xref rid="sec28" ref-type="sec">Supporting Information Figure S8</xref>. Of these methods, the linear approaches, for example, PCA, attempt to find linear combinations of features that best explain the data. Such approaches, while useful, do not consider the intrinsic geometry of data. In contrast, nonlinear approaches attempt to capture intrinsic geometry by considering the local neighborhood of each data point, as determined by a <italic toggle="yes">k</italic>-nearest neighbor (<italic toggle="yes">k</italic>NN) graph built on the data. While promising for capturing the geometry of brain states, such approaches work best when the data is sampled both uniformly and without noise from its underlying distribution, and are otherwise prone to a type of error called <italic toggle="yes">topological instability</italic> (<xref rid="bib6" ref-type="bibr">Balasubramanian &amp; Schwartz, 2002</xref>). Because fMRI data are inherently noisy and suffer from sampling variability, we expect that this error would be unavoidable for standard methods and thus resort to specialized techniques that capture intrinsic geometry while mitigating such errors due to noise.</p>
        <p>One plausible solution to the topological instability problem is to use techniques from TDA, namely, the Mapper algorithm. The standard Mapper approach initially applies a <italic toggle="yes">d</italic>-dimensional projection to the data matrix (<italic toggle="yes">filtering</italic>), and then constructs a <italic toggle="yes">d</italic>-dimensional grid with overlapping cells to cover the projected data points (<italic toggle="yes">binning</italic>). A clustering scheme is then applied to refine each cell (<italic toggle="yes">partial clustering</italic>), and finally a graph is constructed with the refined cells as nodes, and edges between nodes if the corresponding cells share one or more data points. Notably, the partial clustering scheme operates on the native high-dimensional embedding of the points in each cell, and thus avoids some of the information loss due to projection. Specifically, low-dimensional projection tends to erroneously collapse points close together, and partial clustering attempts to reverse this collapse. Thus, a way to address the topological instability of <italic toggle="yes">k</italic>NN-based approaches is to fuse together the partial clustering step from the Mapper algorithm.</p>
        <p>However, there is still a caveat that information loss is inevitable when representing data that is natively on the order of several hundred to thousands of dimensions in just two or three dimensions. Given that the final output of Mapper is a combinatorial graph, there is a surprising lack of methods in the existing literature that implicitly map the data matrix <italic toggle="yes">X</italic> into the output graph without incurring the information loss from low-dimensional projection. Here, we propose such an implicit method that moves away from low-dimensional projection and operates directly at the level of distance matrices. The key insight is that we can obtain the topological stabilization properties of Mapper by operating directly on the native high-dimensional metric <italic toggle="yes">D</italic> and any transformation <italic toggle="yes">D</italic>′ thereof without explicitly constructing a low-dimensional embedding.</p>
      </sec>
      <sec id="sec13">
        <title>Reciprocal kNN.</title>
        <p>To gain access to the intrinsic geometry of the high-dimensional data, our constructions leverage a particular type of <italic toggle="yes">k</italic>NN graph and the matrix of geodesic distances on this graph. First, we fix a choice of metric for our dataset <italic toggle="yes">X</italic> (in our case, the <italic toggle="yes">L</italic><sup>1</sup> metric, also known as the Manhattan distance) and use this to build an <italic toggle="yes">n</italic> × <italic toggle="yes">n</italic> matrix <italic toggle="yes">D</italic> of pairwise distances between the rows of <italic toggle="yes">X</italic>. We choose <italic toggle="yes">L</italic><sup>1</sup> over the more standard <italic toggle="yes">L</italic><sup>2</sup> (Euclidean) metric due to higher effectiveness for nearest neighbor searches in high dimensions (<xref rid="bib2" ref-type="bibr">Aggarwal et al., 2001</xref>). Next, for each row <italic toggle="yes">x</italic>, we select the top-<italic toggle="yes">k</italic> nearest neighbors and call this set <italic toggle="yes">NN</italic><sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">x</italic>). Then we build a graph <italic toggle="yes">G</italic> where the node set is indexed by <italic toggle="yes">X</italic>, and an edge (<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub>) is added whenever <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> ∈ <italic toggle="yes">NN</italic><sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub>) and <italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub> ∈ <italic toggle="yes">NN</italic><sub><italic toggle="yes">k</italic></sub>(<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>). This final condition of creating edges by a symmetric criterion is referred to as <italic toggle="yes">reciprocal kNN</italic>. Notice that these operations can be easily implemented using sort operations. We finally define a new metric matrix <italic toggle="yes">D</italic>′ by taking the shortest path distances (i.e., geodesic distances) on <italic toggle="yes">G</italic>. Note that this can be implemented in <italic toggle="yes">O</italic>(|<italic toggle="yes">V</italic>|<sup>3</sup>) time using a Floyd–Warshall algorithm, although the next section shows that we can alternatively utilize a small number of calls to the <italic toggle="yes">O</italic>((|<italic toggle="yes">V</italic>| + |<italic toggle="yes">E</italic>|) log(|<italic toggle="yes">V</italic>|)) Dijkstra algorithm.</p>
        <p>The choice of <italic toggle="yes">reciprocal k</italic>NN is a crucial aspect of our method, as it helps stabilize the noise levels of fMRI data by pruning connections across areas with different local data density (<xref rid="bib79" ref-type="bibr">Qin et al., 2011</xref>). <xref rid="sec28" ref-type="sec">Supporting Information Figure S6</xref> contains a toy example illustrating how reciprocal <italic toggle="yes">k</italic>NN reduces “false” connections compared to standard <italic toggle="yes">k</italic>NN. Additionally, in <xref rid="sec28" ref-type="sec">Supporting Information Figure S8</xref> we show for an exemplar subject in Dataset 1 that replacing the <italic toggle="yes">k</italic>NN construction in a standard nonlinear dimension reduction method with a reciprocal <italic toggle="yes">k</italic>NN construction yields a low-dimensional embedding where data points associated to different tasks form distinctive topological structures. We also note that <italic toggle="yes">k</italic>NN constructions have been used before in Mapper applications (<xref rid="bib37" ref-type="bibr">Duponchel, 2018</xref>), although in such applications the <italic toggle="yes">k</italic>NN graph construction has been followed by a low-dimensional projection via a graph drawing algorithm. In contrast, we use a tall, skinny submatrix of the matrix <italic toggle="yes">D</italic>′ with columns corresponding to landmarks chosen via the procedure described next.</p>
        <p>Finally, we note that the dimensionality of the data has an important effect on the computation of nearest neighbors. While techniques such as kd-trees can be very effective for nearest neighbor queries in dimensions below 10, for dimensions <italic toggle="yes">p</italic> &gt; 10 it may be preferable to compute nearest neighbors via a linear scan which runs in in <italic toggle="yes">O</italic>(<italic toggle="yes">np</italic>) time (<xref rid="bib114" ref-type="bibr">Weber et al., 1998</xref>). To obtain sublinear dependence on <italic toggle="yes">n</italic>, it is possible to use techniques such as locality sensitive hashing (<xref rid="bib52" ref-type="bibr">Indyk &amp; Motwani, 1998</xref>), although this approach is not addressed in the current work. In <xref rid="sec28" ref-type="sec">Supporting Information Figure S7</xref> we show that the runtime of our method initially scales linearly with data dimension before flattening and showing logarithmic dependence. We also note that when applying our method to voxel-level fMRI data, where <italic toggle="yes">p</italic> is in the order of hundreds of thousands, one may use scalable approximate nearest neighbor methods (<xref rid="bib35" ref-type="bibr">Dong et al., 2011</xref>).</p>
        <p>Thus far, we have replaced the filtering step, that is, the low-dimensional projection, of the standard Mapper algorithm by a technique for obtaining a pairwise <xref rid="def17" ref-type="def">distance matrix</xref>
<italic toggle="yes">D</italic>′ that encodes the intrinsic geometry of the data. Next, we carry out a sampling procedure that obtains landmarks on the data for use in a binning step while assuming access only to <italic toggle="yes">D</italic>′.</p>
      </sec>
      <sec id="sec14">
        <title>Farthest point sampling (FPS).</title>
        <p>FPS is a greedy algorithm for landmarking data (<xref rid="bib48" ref-type="bibr">Gonzalez, 1985</xref>). In this algorithm, one starts with a seed point <italic toggle="yes">x</italic><sub>0</sub> ∈ <italic toggle="yes">X</italic>. Then one chooses <italic toggle="yes">x</italic><sub>1</sub> to be a point that maximizes the <italic toggle="yes">D</italic>′ distance to <italic toggle="yes">x</italic><sub>0</sub>, possibly with some arbitrary tie breaking between equidistant points. Next one chooses <italic toggle="yes">x</italic><sub>2</sub> to be a point that maximizes the distance to {<italic toggle="yes">x</italic><sub>0</sub>, <italic toggle="yes">x</italic><sub>1</sub>}, where the latter is defined as min(<italic toggle="yes">D</italic>′(<italic toggle="yes">x</italic><sub>2</sub>, <italic toggle="yes">x</italic><sub>0</sub>), <italic toggle="yes">D</italic>′(<italic toggle="yes">x</italic><sub>2</sub>, <italic toggle="yes">x</italic><sub>1</sub>)). The algorithm proceeds in this manner until <italic toggle="yes">x</italic><sub><italic toggle="yes">r</italic>−1</sub> has been chosen, where <italic toggle="yes">r</italic> is a user-specified <italic toggle="yes">resolution</italic> parameter. We utilized the Matlab implementation provided in (<xref rid="bib32" ref-type="bibr">De Silva &amp; Perry, 2003</xref>). Note also that each step requires only a call to Dijkstra’s algorithm for a total of <italic toggle="yes">r</italic> calls, as well as some max and min operations. The final output of this step is a set of landmark points {<italic toggle="yes">x</italic><sub>0</sub>, <italic toggle="yes">x</italic><sub>1</sub>, …, <italic toggle="yes">x</italic><sub><italic toggle="yes">r</italic>−1</sub>} and a real number <italic toggle="yes">ϵ</italic> corresponding to the maximal distance from any point in <italic toggle="yes">X</italic> to its closest landmark point according to <italic toggle="yes">D</italic>′.</p>
        <p>Toward reproducibility, we attempted to minimize randomness during landmark selection in two ways. First, while FPS typically begins with the random selection of an initial seed point, we instead define the seed point to always be the first row of <italic toggle="yes">X</italic>. This ensures that landmark selection is at least reproducible given the same input matrix <italic toggle="yes">X</italic>. Second, while the sequence of landmarks visited during FPS depends heavily on the choice of initial seed point, the following argument shows that the output of applying FPS to <italic toggle="yes">D</italic>′ is minimally affected by randomness in the seed point. First, FPS achieves a 2-approximation of the optimal locations for placing landmarks (also known in the literature as the metric <italic toggle="yes">k</italic>-center problem) (<xref rid="bib48" ref-type="bibr">Gonzalez, 1985</xref>). Next, by appealing to this result and additional results from metric geometry (<xref rid="bib19" ref-type="bibr">Burago et al., 2001</xref>, Example 7.3.11 and Corollary 7.3.28), any two sets of samples obtained by FPS yield distance matrices that are guaranteed to approximate the input distance matrix <italic toggle="yes">D</italic>′ up to a small multiplicative factor. This approximation guarantee in turn ensures that landmark selection is minimally affected by randomness, and the approximated distance matrix converges to <italic toggle="yes">D</italic>′ as the number of landmarks approaches the number of data points.</p>
      </sec>
      <sec id="sec15">
        <title>Intrinsic binning.</title>
        <p>Having constructed landmarks, we now partition the data into <italic toggle="yes">r</italic> overlapping bins as follows: for each landmark <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, define<disp-formula><mml:math id="m1"><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≔</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mfenced open="(" close=")" separators=","><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>x</mml:mi></mml:mfenced><mml:mo>≤</mml:mo><mml:mn>4</mml:mn><mml:mi>ϵ</mml:mi><mml:mo>·</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi>g</mml:mi><mml:mn>100</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic toggle="yes">g</italic> is a <italic toggle="yes">gain</italic> parameter that controls the level of overlap. The choice of <italic toggle="yes">ϵg</italic>/25 is set up for the following scenario: suppose <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub> are two landmarks satisfying <italic toggle="yes">D</italic>′(<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub>) = 2<italic toggle="yes">ϵ</italic> and <italic toggle="yes">p</italic> is a point such that <italic toggle="yes">D</italic>′(<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">p</italic>) = <italic toggle="yes">ϵ</italic> = <italic toggle="yes">D</italic>′(<italic toggle="yes">p</italic>, <italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub>). Then a gain of 50 (interpreted as 50%) allows the inclusion <italic toggle="yes">x</italic><sub><italic toggle="yes">j</italic></sub> ∈ <italic toggle="yes">B</italic><sub><italic toggle="yes">i</italic></sub>. We set the minimum value of <italic toggle="yes">g</italic> to 25, which ensures that the collection of bins <italic toggle="yes">B</italic><sub><italic toggle="yes">i</italic></sub> fully covers <italic toggle="yes">X</italic>. We refer to this procedure of binning points using landmarks and the intrinsic metric <italic toggle="yes">D</italic>′ as <italic toggle="yes">intrinsic binning</italic>. In contrast, the standard Mapper algorithm uses a <italic toggle="yes">d</italic>-dimensional grid with overlapping cells that fully covers a <italic toggle="yes">d</italic>-dimensional projection of <italic toggle="yes">X</italic>, and we refer to this approach as <xref rid="def18" ref-type="def"><italic toggle="yes">extrinsic binning</italic></xref> due to its use of the ambient space ℝ<sup><italic toggle="yes">d</italic></sup>. Note that <italic toggle="yes">d</italic>-dimensional cubes tend to be mostly empty when <italic toggle="yes">d</italic> is large, and hence the extrinsic binning approach becomes increasingly wasteful and computationally expensive as <italic toggle="yes">d</italic> increases. Further analysis of this extrinsic binning procedure is carried out in <xref rid="bib34" ref-type="bibr">Dłotko (2019)</xref>, where additionally a linear variant of intrinsic binning was utilized.</p>
        <p>A property of this intrinsic binning procedure over binning using <italic toggle="yes">d</italic>-dimensional cubes is that it is data driven, and is grounded in techniques commonly used in TDA methods (<xref rid="bib45" ref-type="bibr">Giusti et al., 2016</xref>). The resolution and gain parameters used above are analogous to those used in the conventional Mapper algorithm. We note that controlling gain via a single parameter is not necessarily optimal, as our data is unlikely to be uniformly sampled. Techniques such as UMAP (<xref rid="bib65" ref-type="bibr">McInnes et al., 2018</xref>) suggest that one may instead carry out local computations around each landmark to set adaptive gain values <italic toggle="yes">g</italic><sub><italic toggle="yes">i</italic></sub> for each <italic toggle="yes">B</italic><sub><italic toggle="yes">i</italic></sub> that better reflect the data distribution. However, invoking such a method requires much more additional mathematical machinery, and we leave such extensions to future work.</p>
        <p>To complete the description of the intrinsic binning procedure, we consider the case where <italic toggle="yes">G</italic> is not connected. In some techniques utilizing <italic toggle="yes">k</italic>NN graphs, one often proceeds by dropping all but the largest connected component (<xref rid="bib110" ref-type="bibr">Van Der Maaten et al., 2009</xref>), which directly causes information loss. In our setting, however, we simply reallocate the number of landmark points to use for each connected component. Specifically, we allocate<disp-formula><mml:math id="m2"><mml:mtext>ceiling</mml:mtext><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>r</mml:mi><mml:mo>·</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mtext>#</mml:mtext><mml:mspace width="0.3em"/><mml:mtext>nodes</mml:mtext><mml:mspace width="0.3em"/><mml:mtext>in</mml:mtext><mml:mspace width="0.3em"/><mml:mtext>component</mml:mtext></mml:mrow><mml:mrow><mml:mtext>total</mml:mtext><mml:mspace width="0.3em"/><mml:mo>#</mml:mo><mml:mspace width="0.3em"/><mml:mtext>nodes</mml:mtext></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mfenced></mml:math></disp-formula>landmark points to each connected component and perform binning for each component individually as above.</p>
        <p>A different strategy that we also tried was to form a connected graph by augmenting edges with exponentially penalized weights, as described in (<xref rid="bib9" ref-type="bibr">Bayá &amp; Granitto, 2011</xref>). However, the binning strategy used above yielded greater signal and statistical power in downstream analysis.</p>
        <p>Toward scalability, we next consider computation times of intrinsic versus extrinsic binning. In our tests, standard open-source Mapper implementations (KeplerMapper; <xref rid="bib112" ref-type="bibr">van Veen et al., 2019</xref>; and Giotto-Mapper; <xref rid="bib104" ref-type="bibr">Tauzin et al., 2021</xref>) suffered from significant slowdowns (<xref rid="F1" ref-type="fig">Figure 1B</xref>) when carrying out the extrinsic binning procedure of the traditional Mapper approach after projecting data to spaces of four or more dimensions. A natural question then becomes: how many dimensions are needed to represent data? Intuitively, more dimensions are better for preserving information. In our case, to use existing open-source Mapper implementations after constructing a reciprocal <italic toggle="yes">k</italic>NN graph and computing graph geodesic distances, the easiest approach would be to apply an MDS projection in analogy with the Isomap algorithm for dimension reduction (<xref rid="bib105" ref-type="bibr">Tenenbaum et al., 2000</xref>). We observed (<xref rid="F1" ref-type="fig">Figure 1C</xref>) that the Pearson correlation between graph geodesic distances and distances after MDS projection increased with the dimension of the target space. Specifically, we needed 4- and 8-dimensional projections, respectively, to achieve correlation values exceeding 0.85 and 0.9. This suggests that information loss would be reduced by using a larger projection dimension, which unfortunately runs into the issue of slow runtime (<xref rid="F1" ref-type="fig">Figure 1B</xref>). In contrast, once a matrix of distances <italic toggle="yes">D</italic>′ has been provided, our projection-free intrinsic binning approach can be naively implemented in <italic toggle="yes">O</italic>(<italic toggle="yes">nr</italic>) operations, where <italic toggle="yes">n</italic> is the number of data points and <italic toggle="yes">r</italic> is the number of landmarks. In particular, there is no dependence on projection dimension, whereas the extrinsic binning approach requires computing a <italic toggle="yes">d</italic>-dimensional grid comprising a number of cells in the order <italic toggle="yes">O</italic>(<italic toggle="yes">r</italic><sup><italic toggle="yes">d</italic></sup>), that is, exponential dependence on dimension (<xref rid="F1" ref-type="fig">Figure 1B</xref>). Thus, in cases where we expect to need a moderate number of dimensions (&gt;10) to represent our data, intrinsic binning should be preferable to using extrinsic binning. Note that in standard neuroimaging literature where PCA is used for dimension reduction, standard choices for the embedding dimension range from 5–10 (<xref rid="bib97" ref-type="bibr">Shine et al., 2019a</xref>) to 50 (<xref rid="bib113" ref-type="bibr">Vidaurre et al., 2017</xref>).</p>
        <p>Finally, we remark that the procedure of computing a transformed metric <italic toggle="yes">D</italic>′, for example, via the reciprocal <italic toggle="yes">k</italic>NN construction, and subsequently an overlapping cover of <italic toggle="yes">X</italic> via intrinsic binning, has the same inputs and outputs as the filtering and extrinsic binning approach of the standard Mapper pipeline. Therefore, these aspects of our overall pipeline comprise a self-contained module that can be incorporated into existing Mapper packages such as KeplerMapper or Giotto-Mapper, thus enriching their diverse functionality with scalability improvements toward qualitative and quantitative exploration of consortium-sized datasets.</p>
      </sec>
      <sec id="sec16">
        <title>Partial clustering and graph generation.</title>
        <p>For partial clustering, we use the method described in the original Mapper literature (<xref rid="bib99" ref-type="bibr">Singh et al., 2007</xref>). For each bin <italic toggle="yes">B</italic><sub><italic toggle="yes">i</italic></sub>, we apply single linkage clustering using the native, high-dimensional metric <italic toggle="yes">D</italic>. Then we investigate the histogram of linkage values and set a cutoff threshold to be the first histogram bin edge at which the histogram bin count becomes zero. This threshold is then used to partition <italic toggle="yes">B</italic><sub><italic toggle="yes">i</italic></sub> into clusters.</p>
        <p>Intuitively, if <italic toggle="yes">B</italic><sub><italic toggle="yes">i</italic></sub> contains two well-separated clusters, then this cutoff value would separate the clusters. As noted in <xref rid="bib99" ref-type="bibr">Singh et al. (2007)</xref>, this method has its limitations, namely, that if a bin contains clusters of differing densities, it tends to recover only the high-density cluster. However, this simple histogram-based method has worked sufficiently well for our purposes.</p>
        <p>The output of the partial clustering step is an overlapping collection of bins <italic toggle="yes">C</italic><sub>0</sub>, <italic toggle="yes">C</italic><sub>1</sub>, …, <italic toggle="yes">C</italic><sub><italic toggle="yes">N</italic></sub>. The final NeuMapper shape graph is generated by taking the <italic toggle="yes">C</italic><sub><italic toggle="yes">i</italic></sub> as nodes and inserting edges (<italic toggle="yes">C</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">C</italic><sub><italic toggle="yes">j</italic></sub>) whenever <italic toggle="yes">C</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">C</italic><sub><italic toggle="yes">j</italic></sub> share one or more data points.</p>
      </sec>
      <sec id="sec17">
        <title>Annotations.</title>
        <p>Labels on data points are conveniently aggregated into annotations for each node of the shape graph. Specifically, given a shape graph on <italic toggle="yes">N</italic> nodes and <italic toggle="yes">T</italic> categorical labels, we construct an <italic toggle="yes">N</italic> × <italic toggle="yes">T</italic> annotation matrix where entry (<italic toggle="yes">v</italic>, <italic toggle="yes">t</italic>) counts the number of data points labeled <italic toggle="yes">t</italic> that belong to node <italic toggle="yes">v</italic>. The row vector (<italic toggle="yes">v</italic>, ·) comprises an annotation for node <italic toggle="yes">v</italic>. These annotations can be displayed as pie charts (<xref rid="bib44" ref-type="bibr">Geniesse et al., 2019</xref>; <xref rid="bib91" ref-type="bibr">Saggar et al., 2018</xref>) or used downstream in further analysis (see <xref rid="sec2" ref-type="sec">Results</xref>). While the labels themselves are typically supplied by the experiment design, we show below how these labels may be derived from meta-analysis to derive further insights into the data.</p>
      </sec>
      <sec id="sec18">
        <title>Parameter optimization.</title>
        <p>As is standard in machine learning pipelines, parameter optimization for NeuMapper may be carried out via cross-validation. However, it is always appealing to obtain heuristics for good parameter initializations, and here we outline one such heuristic for optimizing parameters (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) that uses the autocorrelation structure of fMRI data. Specifically, given a data matrix <italic toggle="yes">X</italic>, we plot the autocorrelation function for each column of <italic toggle="yes">X</italic> and visually determine the “elbow,” that is, the number of lags at which the autocorrelation function becomes level. We multiply this number by the sampling period of the dataset to obtain a <italic toggle="yes">critical lag</italic> τ in units of seconds. In the fMRI context, autocorrelation is naturally present due to the hemodynamic response function (HRF), and it is desirable to view data at a scale which incorporates signal that is not just driven by HRF. Toward this goal, we specify a percentage value <italic toggle="yes">α</italic> and set the criteria—denoted AutoCorrCrit—that an output shape graph should have at least <italic toggle="yes">α</italic>% nodes containing data points that were acquired at least τ seconds apart, that is, are less susceptible to the HRF. We set <italic toggle="yes">τ</italic> = 11<italic toggle="yes">s</italic> (corresponding to the HRF peak; <xref rid="bib60" ref-type="bibr">Lindquist et al., 2009</xref>) and <italic toggle="yes">α</italic> = 25% for both datasets.</p>
        <p>The procedure outlined above heuristically attempts to mitigate the dependency of NeuMapper shape graph nodes on autocorrelated samples. However, thus far we do not have any conditions guaranteeing that the output shape graphs will be sufficiently connected for carrying out downstream analysis by using network science tools. To this end, we introduce an additional percentage value <italic toggle="yes">β</italic> and require that for group-level analysis, each shape graph contains at least <italic toggle="yes">β</italic>% of its nodes in its largest connected component. We set <italic toggle="yes">β</italic> = 50% for both datasets. To ensure consistency in group-level analysis, we require a consensus (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) triplet that can be used to generate shape graphs for each dataset, that is, for data acquired under the same scanning parameters. In summary, we first obtain parameters for each shape graph in the dataset according to <italic toggle="yes">α</italic>, obtain a consensus (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) triplet for the full dataset, and finally perturb the consensus triple (if necessary) to satisfy the connectivity constraint <italic toggle="yes">β</italic>. While other strategies may also be invoked to satisfy our criteria, we found that this order of operations worked sufficiently well.</p>
        <p>In detail, the optimization at the level of a single subject is carried out as follows. We first specify a broad range of values for the <italic toggle="yes">r</italic> parameter, and choices of small initial <italic toggle="yes">k</italic> and <italic toggle="yes">g</italic> parameters. For each value of <italic toggle="yes">r</italic>, we carry out the following procedure:<list list-type="bullet"><list-item><p>Compute a shape graph with the initial values for <italic toggle="yes">k</italic> and <italic toggle="yes">g</italic>.</p></list-item><list-item><p>Verify that AutoCorrCrit is satisfied. If not, increment <italic toggle="yes">k</italic> ← <italic toggle="yes">k</italic> + 1 and <italic toggle="yes">g</italic> ← <italic toggle="yes">g</italic> + 3.</p></list-item><list-item><p>Iterate until AutoCorrCrit is satisfied.</p></list-item></list></p>
        <p>More specifically, for the <italic toggle="yes">r</italic> parameter, we explored 10 different values evenly spaced along the interval [floor(0.1 · <italic toggle="yes">n</italic>), floor(0.3 · <italic toggle="yes">n</italic>)], where <italic toggle="yes">n</italic> is the number of time points in the dataset. For the <italic toggle="yes">k</italic> and <italic toggle="yes">g</italic> parameters, we used small initial values of <italic toggle="yes">k</italic> = 3 and <italic toggle="yes">g</italic> = 25, respectively. The step sizes for incrementing <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic> were chosen to be the smallest integers such that perturbing the corresponding parameters produced observable changes to the shape graphs.</p>
        <p>Multiple <italic toggle="yes">r</italic> values may have the same optimal (<italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) parameters. To reduce these choices down to a manageable number, we cluster the different optimal <italic toggle="yes">k</italic> values (equivalently <italic toggle="yes">g</italic> values, as we increment <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic> together) using the classical DBSCAN density-based clustering algorithm. We then discard all but the top three largest clusters of optimal <italic toggle="yes">k</italic> values. Typically, each cluster will have a unique <italic toggle="yes">k</italic> value, but to ensure this programmatically, we select the minimum <italic toggle="yes">k</italic> value for each cluster. Finally, for each cluster we record the most frequently occurring <italic toggle="yes">r</italic> value. This yields a total of three optimal (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) triplets for a data matrix <italic toggle="yes">X</italic>.</p>
        <p>We repeat this procedure for the data matrix for each subject. To obtain consensus, we use a simple voting procedure to select three (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) triplets that occur most frequently among the optimal triples for each subject. This procedure returns three consensus (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) triplets at the group level. We observed that these three optimal parameter triples correspond to three different scales of shape graphs, and the triplet with intermediate values of (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) best corresponds to a mesoscale view with interesting structure.</p>
        <p>Having chosen a consensus (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) triplet, we then verified for each dataset that each of the shape graphs had over <italic toggle="yes">β</italic>% of the nodes in the largest connected component. If not, we increased <italic toggle="yes">k</italic> and decreased <italic toggle="yes">r</italic> (both steps incorporate more global information) in small steps of 3 and 5, respectively, until all graphs had a sufficient fraction of nodes in the largest connected component. Note that we could not perform this step before choosing the consensus triplet, as the procedure for obtaining consensus could result in some graphs no longer satisfying the connectivity criterion. For Dataset 1, the consensus triplet yielded <italic toggle="yes">r</italic> = 192, <italic toggle="yes">k</italic> = 8, <italic toggle="yes">g</italic> = 40. For these parameters, each of the shape graphs satisfied the connectivity criterion, and no further parameter tuning was needed. and we directly used these parameters for generating graphs via NeuMapper. For Dataset 2, the mesoscale triple yielded <italic toggle="yes">r</italic> = 260, <italic toggle="yes">k</italic> = 6, <italic toggle="yes">g</italic> = 34. However, visual inspection of the graphs generated using these parameters revealed highly disconnected graphs for multiple participants, suggesting that the data for these participants had different spatial characteristics from that of the rest of the group. After carrying out parameter tuning as described above, the final (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) triple that we used for Dataset 2 was (240, 18, 34).</p>
      </sec>
    </sec>
    <sec id="sec19">
      <title>Methods for Analyzing Shape Graph Annotations</title>
      <p>Toward qualitative and interactive exploration, we created visualizations of the shape graphs obtained by NeuMapper using the DyNeuSR platform (<xref rid="bib44" ref-type="bibr">Geniesse et al., 2019</xref>) and used its node annotation features to demonstrate insights into behavior (<xref rid="F2" ref-type="fig">Figure 2</xref>). On the quantitative side, we show that shape graphs are compressed graph representations of the data on which post hoc analysis via network science tools can be carried out efficiently.</p>
      <sec id="sec20">
        <title>Community detection tools.</title>
        <p>We used two standard community detection methods in our analysis: modularity (<xref rid="bib68" ref-type="bibr">Newman, 2006</xref>), which tracks the appearance of densely connected groups of vertices with sparse connections across groups, and core-periphery (<xref rid="bib16" ref-type="bibr">Borgatti &amp; Everett, 2000</xref>), which tracks the appearance of a centrally located group of vertices with dense connections to the whole graph that is surrounded by a periphery with sparse connections. Modularity of each Mapper-generated graph was computed as follows. Using the ground-truth labels, each node was assigned to a community (one of the four tasks) via winner-take-all voting. The modularity of this community assignment was then computed using the standard <italic toggle="yes">Q</italic>-score (<xref rid="bib68" ref-type="bibr">Newman, 2006</xref>). For core-periphery, we used the score derived in <xref rid="bib41" ref-type="bibr">Everett and Borgatti (2000)</xref>, <xref rid="bib68" ref-type="bibr">Newman (2006)</xref>, and <xref rid="bib88" ref-type="bibr">Rubinov et al. (2015)</xref> and validated in <xref rid="bib91" ref-type="bibr">Saggar et al. (2018)</xref>. Specifically we used the implementation core_periphery_dir provided in the Brain Connectivity Toolbox (<xref rid="bib87" ref-type="bibr">Rubinov &amp; Sporns, 2010</xref>). In this implementation, each node is given a binary core-periphery assignment. The coreness of each task was defined to be the fraction of time points for each task that belonged to core nodes.</p>
      </sec>
      <sec id="sec21">
        <title>Optimal transport.</title>
        <p>The main tool from OT that we use is the <italic toggle="yes">1-Wasserstein distance</italic>, also known as the Earth Mover’s Distance (<xref rid="bib89" ref-type="bibr">Rubner et al., 2000</xref>). The origin of OT is attributed to the work of Monge in the late 1700s and that of Kantorovich in the 1940s.</p>
        <p>The 1-Wasserstein distance is defined in our setting as follows. We start with a connected shape graph <italic toggle="yes">G</italic> = (<italic toggle="yes">X</italic>, <italic toggle="yes">E</italic>) (restricting to the largest connected component as necessary), matrix of graph geodesic distances <italic toggle="yes">D</italic>′, and probability distributions <italic toggle="yes">p</italic>, <italic toggle="yes">q</italic> defined on the vertices of <italic toggle="yes">G</italic>. For shape graphs, probability distributions arise naturally from task or NeuroSynth-derived annotations: for a given annotation <italic toggle="yes">S</italic> and any shape graph vertex <italic toggle="yes">v</italic>, we first compute the proportion of data points in <italic toggle="yes">v</italic> that are annotated by <italic toggle="yes">S</italic>, and then normalize these proportions over all of <italic toggle="yes">G</italic> to get a vector of length <italic toggle="yes">n</italic> × 1 (where <italic toggle="yes">n</italic> = |<italic toggle="yes">X</italic>|) with nonnegative numbers that sum to 1. Following this procedure for two separate annotations yields two annotation-derived probability distributions <italic toggle="yes">p</italic> and <italic toggle="yes">q</italic>. Intuitively <italic toggle="yes">p</italic>, <italic toggle="yes">q</italic> correspond to distributions of “mass” on the points of <italic toggle="yes">G</italic>. Next we consider the set of all joint probability distributions with marginals <italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>, denoted Π(<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>). Each <italic toggle="yes">μ</italic> ∈ Π(<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>) is an <italic toggle="yes">n</italic> × <italic toggle="yes">n</italic> matrix with nonnegative entries summing to 1 and row and column sums equal to <italic toggle="yes">p</italic> and <italic toggle="yes">q</italic>, respectively. Finally, the 1-Wasserstein distance is defined as:<disp-formula><mml:math id="m3"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>O</mml:mi><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mfenced open="(" close=")" separators=","><mml:mi>p</mml:mi><mml:mi>q</mml:mi></mml:mfenced><mml:mo>≔</mml:mo><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="normal">Π</mml:mi><mml:mfenced open="(" close=")" separators=","><mml:mi>p</mml:mi><mml:mi>q</mml:mi></mml:mfenced></mml:mrow></mml:munder><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></disp-formula>This is a linear program that can be solved using standard techniques in linear programming. We use the Matlab wrapper written by Antoine Rolet for the C++ implementation provided in <xref rid="bib15" ref-type="bibr">Bonneel et al. (2011)</xref>.</p>
        <p>This OT distance is a bona fide metric between probability distributions that calculates the cost of transforming one distribution into another while taking into account the ground distances between the points supporting the two distributions. In the language of network flows (<xref rid="bib4" ref-type="bibr">Ahuja et al., 1989</xref>), computing this metric amounts to solving a minimum cost flow from one distribution to another. For the NeuMapper setting, <italic toggle="yes">d</italic><sub><italic toggle="yes">OT</italic></sub> is a principled method for computing second-order information from different annotations. <xref rid="F3" ref-type="fig">Figure 3</xref> shows the use of pairwise <italic toggle="yes">d</italic><sub><italic toggle="yes">OT</italic></sub> followed by an MDS projection to get a 2-D summary of the separation between task annotations on shape graphs across our two datasets.</p>
      </sec>
      <sec id="sec22">
        <title>Generating annotations via meta-analysis.</title>
        <p>To better anchor the NeuMapper-generated graphs into neurobiology, we generated a second set of node-level annotations using topic association maps based on the NeuroSynth decoding framework (<xref rid="bib119" ref-type="bibr">Yarkoni et al., 2011</xref>). Topic association maps were downloaded from <ext-link xlink:href="http://neurosynth.org/" ext-link-type="uri">neurosynth.org</ext-link> using the NeuroSynth <italic toggle="yes">v4-topics-50</italic> database—a set of 50 topics extracted from all abstracts in the NeuroSynth database as of July 2015. For the purpose of this study, we limited our analysis to the topic association maps (i.e., Topics 010, 022, 011, 042) most relevant to the four tasks used in the CMP experiment (i.e., resting state, working memory, video, math), and a fifth topic association map corresponding to “task-positive” cognition (i.e., Topic 002).</p>
        <p>For each topic association map, we extracted the mean values from 333 predefined cortical ROIs based on the <xref rid="bib49" ref-type="bibr">Gordon et al. (2016)</xref> atlas, and transformed the resulting 3-D NIFTI image into a 2-D topic association matrix. These two steps were performed simultaneously, using the “NiftiLabelsMasker” object from the Nilearn software package (<xref rid="bib1" ref-type="bibr">Abraham et al., 2014</xref>). Note, the atlas used to prepare the fMRI data for input into NeuMapper was processed using the Shine 375 atlas (as described in <xref rid="sec8" ref-type="sec">Data Acquisition</xref> section), which includes the same 333 cortical regions from the <xref rid="bib49" ref-type="bibr">Gordon et al. (2016)</xref> atlas used here to process the topic association maps. Thus, before computing any correlations between the fMRI data and the NeuroSynth topic association maps, we extracted data corresponding to the same 333 cortical regions from each of the preprocessed fMRI data matrices. Further, the resulting data matrices were converted to <italic toggle="yes">z</italic>-scores (computed for each subject, separately), and individual ROIs with zero variance were excluded from further analysis.</p>
        <p>Next, for each subject, we computed “framewise” correlations between each topic association map and every time frame in the data matrix. This yielded a frame-by-topic correlation matrix that was then used to aggregate node-level annotations. For each node, the topic correlation scores corresponding to time frames associated with the node were extracted from the precomputed frame-by-topic correlation matrix, and then further restricted to include only those within the top 99th-percentile of strongest positive correlations associated with the node. Finally, to generate the node-level annotations, we counted the number of retained positive frame-by-topic correlations associated with each node, and then used these node-by-topic association counts as the annotations for each node. This yielded a final node-by-topic annotation matrix that could then be used to color the NeuMapper-generated graphs and perform further analysis (as described in <xref rid="sec5" ref-type="sec">Anchoring Shape Graphs Into Known Cognitive Constructs</xref> section).</p>
      </sec>
      <sec id="sec23">
        <title>Overreflection and synchronization measures.</title>
        <p>To better understand how the “decoded” cognitive topic annotations relate to “expected” task structure and experimental task performance, we developed two new measures to quantify the amount of overlap between NeuroSynth topic annotations and experimental task labels.</p>
        <p>Overreflection/mismatch was computed as follows. For each subject, the experimental task labels (R, M, V, A) were used to compute a node-by-task annotation matrix <inline-formula><mml:math id="m4"><mml:mover accent="true"><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="true">ˆ</mml:mo></mml:mover></mml:math></inline-formula>, from which we obtained a condensed two-column node-by-task annotation matrix Θ by summing the M, V, and A task columns. We also used NeuroSynth task-positive and task-negative topic annotations (Topic 002 and Topic 010) to compute a two-column node-by-topic association matrix Φ. We then computed the matrix product Φ<sup><italic toggle="yes">T</italic></sup>Θ and normalized this 2 × 2 matrix by its sum, formally written as Φ<italic toggle="yes"><sup>T</sup></italic>Θ/〈Φ<sup><italic toggle="yes">T</italic></sup>Θ, 1〉<sub><italic toggle="yes">F</italic></sub> where 〈·〉<sub><italic toggle="yes">F</italic></sub> denotes the matrix Frobenius product. Each entry of this matrix corresponds to the overlap between an empirical (NeuroSynth-decoded) annotation and an expected (experimental task-derived) annotation. The overreflection score was then defined to be the off-diagonal entry corresponding to the NeuroSynth-decoded task-positive annotation and the experimental rest annotation.</p>
        <p>Synchronization/match between the empirical and expected rest annotations and the empirical/expected task annotations was computed in two stages: once for aggregated tasks, and once for the individual memory task. First, synchronization at the task-aggregated level was taken to be the trace (i.e., the sum of the diagonal) of Φ<sup><italic toggle="yes">T</italic></sup>Θ/〈Φ<sup><italic toggle="yes">T</italic></sup>Θ, 1〉<sub><italic toggle="yes">F</italic></sub>. Second, for synchronization of the individual memory task, we used NeuroSynth rest and working memory topic annotations (Topic 010 and Topic 022) to compute a two-column node-by-topic annotation matrix <inline-formula><mml:math id="m5"><mml:mover accent="true"><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula>. We also extracted the R, M columns from <inline-formula><mml:math id="m6"><mml:mover accent="true"><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="true">ˆ</mml:mo></mml:mover></mml:math></inline-formula> to obtain a corresponding two-column node-by-task annotation matrix <inline-formula><mml:math id="m7"><mml:mover accent="true"><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula>. We then computed a synchronization score by taking the trace of the normalized matrix product <inline-formula><mml:math id="m8"><mml:mover accent="true"><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula><sup><italic toggle="yes">T</italic></sup><inline-formula><mml:math id="m9"><mml:mover accent="true"><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula>/〈<inline-formula><mml:math id="m10"><mml:mover accent="true"><mml:mi mathvariant="normal">Φ</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula><sup><italic toggle="yes">T</italic></sup><inline-formula><mml:math id="m11"><mml:mover accent="true"><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:math></inline-formula>, 1〉 as before.</p>
      </sec>
    </sec>
    <sec id="sec24">
      <title>Validation and Replicability Analysis</title>
      <sec id="sec25">
        <title>Phase-randomized surrogates.</title>
        <p>To ground NeuMapper in a randomization framework, we validated our pipeline against null datasets produced via Fourier phase randomization (<xref rid="bib106" ref-type="bibr">Theiler et al., 1992</xref>). Phase randomization takes a collection of time series data, applies a discrete Fourier transform to each time course, adds a uniformly distributed random phase to each frequency, and then inverts the discrete Fourier transform. Phase randomization produces stationary, linear, Gaussian null data that—by the Wiener-Khintchine theorem—preserves the autocorrelation structure of the original data (<xref rid="bib56" ref-type="bibr">Khintchine, 1934</xref>; <xref rid="bib77" ref-type="bibr">Prichard &amp; Theiler, 1994</xref>; <xref rid="bib117" ref-type="bibr">Wiener, 1930</xref>). Thus by testing against phase-randomized surrogates, we test against the possibility that our results are driven by stationarity, linearity, Gaussianity, or any combination of these properties (<xref rid="bib59" ref-type="bibr">Liégeois et al., 2017</xref>). We validated our framework on phase-randomized surrogates from both Datasets 1 and 2 (<xref rid="sec28" ref-type="sec">Supporting Information Figure S3</xref>), using 10 random surrogates for the data coming from each participant. Surrogate data were generated using the code accompanying (<xref rid="bib59" ref-type="bibr">Liégeois et al., 2017</xref>).</p>
      </sec>
      <sec id="sec26">
        <title>Parameter perturbation.</title>
        <p>To ensure that our results are stable to parameter perturbation, we repeated several of our analysis pipelines on a grid of (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>, <italic toggle="yes">g</italic>) parameters surrounding the chosen parameters for Datasets 1 and 2. Results for the correlations between modularity and task performance as well as the core-periphery structure are presented in <xref rid="sec28" ref-type="sec">Supporting Information Figure S4</xref>. Overall, we find that our results are stable to nearby parameter perturbations, with the gain parameter showing the most discrete changes. Future work should clarify the role of the gain parameter; some of our tests indicated that at least for methods based on <italic toggle="yes">k</italic>NN constructions, the gain parameter could be removed altogether, leaving only the (<italic toggle="yes">r</italic>, <italic toggle="yes">k</italic>) parameter pair to be tuned.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec27">
    <title>ACKNOWLEDGMENTS</title>
    <p>The authors thank the reviewers for their helpful comments. We also thank the research staff and students for their support in data collection, especially, Javier Gonzalez-Castillo for Dataset 1 and Amber Howell, Sahar Jahanikia, Rafi Ayub, and Hua Xie for Dataset 2.</p>
  </sec>
  <sec id="sec28">
    <title>SUPPORTING INFORMATION</title>
    <p>Supporting information for this article is available at <ext-link xlink:href="https://doi.org/10.1162/netn_a_00229" ext-link-type="uri">https://doi.org/10.1162/netn_a_00229</ext-link>. Dataset 1 was originally collected by <xref rid="bib47" ref-type="bibr">Gonzalez-Castillo et al. (2015)</xref> and is available for download from the XNAT Central public repository (<ext-link xlink:href="https://central.xnat.org" ext-link-type="uri">https://central.xnat.org</ext-link>; Project ID: FCStateClassif). The preprocessed data used for Dataset 2 is available upon reasonable request.</p>
    <p>Custom Matlab scripts used to generate and analyze the shape graphs can be found at <ext-link xlink:href="https://github.com/braindynamicslab/neumapper" ext-link-type="uri">https://github.com/braindynamicslab/neumapper</ext-link>. The DyNeuSR Python package used to visualize the shape graphs can be found at <ext-link xlink:href="https://github.com/braindynamicslab/dyneusr" ext-link-type="uri">https://github.com/braindynamicslab/dyneusr</ext-link>.</p>
  </sec>
  <sec id="sec29">
    <title>AUTHOR CONTRIBUTIONS</title>
    <p>Caleb Geniesse: Conceptualization; Investigation; Methodology; Software; Validation; Visualization; Writing – original draft; Writing – review &amp; editing. Samir Chowdhury: Conceptualization; Investigation; Methodology; Software; Validation; Visualization; Writing – original draft; Writing – review &amp; editing. Manish Saggar: Conceptualization; Data curation; Formal analysis; Funding acquisition; Investigation; Methodology; Project administration; Resources; Software; Supervision; Validation; Visualization; Writing – review &amp; editing.</p>
  </sec>
  <sec id="sec30">
    <title>FUNDING INFORMATION</title>
    <p>Manish Saggar, National Institute of Mental Health (<ext-link xlink:href="http://dx.doi.org/10.13039/100000025" ext-link-type="uri">https://dx.doi.org/10.13039/100000025</ext-link>), Award ID: MH-119735. Manish Saggar, National Institute of Mental Health (<ext-link xlink:href="http://dx.doi.org/10.13039/100000025" ext-link-type="uri">https://dx.doi.org/10.13039/100000025</ext-link>), Award ID: MH-104605. Manish Saggar, Stanford University MCHRI Faculty Scholar.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="SMS1" position="float" content-type="local-data">
      <media xlink:href="netn-06-467-s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <glossary>
    <title>TECHNICAL TERMS</title>
    <def-list>
      <def-item id="def1">
        <term>Mapper:</term>
        <def>
          <p>TDA-based technique for generating graph representations that identify meaningful subgroups of high-dimensional data.</p>
        </def>
      </def-item>
      <def-item id="def2">
        <term>Topological data analysis (TDA):</term>
        <def>
          <p>Applied mathematical approaches for analyzing datasets—using techniques from algebraic topology—that learn and leverage information about the shape of complex data.</p>
        </def>
      </def-item>
      <def-item id="def3">
        <term>Shape graph:</term>
        <def>
          <p>Compressed network representation generated by Mapper; nodes correspond to local clusters and edges connect clusters that share data points.</p>
        </def>
      </def-item>
      <def-item id="def4">
        <term>Dimensionality reduction:</term>
        <def>
          <p>Process of selecting and extracting features from data to reduce the number of variables under consideration.</p>
        </def>
      </def-item>
      <def-item id="def5">
        <term>Partial clustering:</term>
        <def>
          <p>Applying a clustering algorithm to a subset of data points. Used to obtain cluster bins from cover bins.</p>
        </def>
      </def-item>
      <def-item id="def6">
        <term>Optimal transport (OT) metric:</term>
        <def>
          <p>A measure of similarity (or dissimilarity) between two probability distributions (e.g., 1-Wasserstein distance). When comparing distributions on a graph, these metrics consider both global and local properties.</p>
        </def>
      </def-item>
      <def-item id="def7">
        <term>NeuroSynth:</term>
        <def>
          <p>A meta-analytic database of probabilistic mappings between cognitive (e.g., terms, topics) and neural states (i.e., activations) that can be used for a broad range of neuroimaging applications.</p>
        </def>
      </def-item>
      <def-item id="def8">
        <term>Mesoscale structure:</term>
        <def>
          <p>A description of network structure in terms of node density and within-group connectivity, for example, the clustering of nodes into different groups (i.e., modularity) or the partitioning of nodes into dense core and sparse peripheral regions (i.e., core-periphery).</p>
        </def>
      </def-item>
      <def-item id="def9">
        <term>Binning:</term>
        <def>
          <p>A step of Mapper-type algorithms that partitions a dataset. Neighboring partition blocks are encouraged to have overlaps.</p>
        </def>
      </def-item>
      <def-item id="def10">
        <term>Cover bins:</term>
        <def>
          <p>A collection of overlapping sets returned by the binning procedure.</p>
        </def>
      </def-item>
      <def-item id="def11">
        <term>Cluster bins:</term>
        <def>
          <p>A refined collection of overlapping sets produced by clustering each set in the collection of cover bins independently.</p>
        </def>
      </def-item>
      <def-item id="def12">
        <term>Geodesic distance matrix:</term>
        <def>
          <p>A distance matrix describing the pairwise shortest path lengths between nodes in a graph.</p>
        </def>
      </def-item>
      <def-item id="def13">
        <term><italic toggle="yes">k</italic>-nearest neighbor (<italic toggle="yes">k</italic>NN) graph:</term>
        <def>
          <p>A graph built on point cloud data where nodes correspond to data points, and edges connect a node to its <italic toggle="yes">k</italic> (a positive integer) closest data points.</p>
        </def>
      </def-item>
      <def-item id="def14">
        <term>Reciprocal <italic toggle="yes">k</italic>NN:</term>
        <def>
          <p>A <italic toggle="yes">k</italic>-nearest neighbors graph created by dropping nonsymmetric neighbors (i.e., two points are only considered neighbors if each is a <italic toggle="yes">k</italic>-closest neighbor of the other). Found to better respect local density when connecting neighbors than standard <italic toggle="yes">k</italic>NN.</p>
        </def>
      </def-item>
      <def-item id="def15">
        <term>Intrinsic binning:</term>
        <def>
          <p>A binning method that uses landmarks and distances from these landmarks to partition data points.</p>
        </def>
      </def-item>
      <def-item id="def16">
        <term>Farthest point sampling (FPS):</term>
        <def>
          <p>An algorithm for selecting a well-separated set of landmark points from a dataset.</p>
        </def>
      </def-item>
      <def-item id="def17">
        <term>Distance matrix:</term>
        <def>
          <p>A square matrix describing the pairwise distances (e.g., Euclidean) between points in a dataset.</p>
        </def>
      </def-item>
      <def-item id="def18">
        <term>Extrinsic binning:</term>
        <def>
          <p>A binning method that uses a grid to partition data points. Good for cases where the ambient space is low dimensional.</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ref-list>
    <title>REFERENCES</title>
    <ref id="bib1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abraham</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gervais</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mueller</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kossaifi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Machine learning for neuroimaging with scikit-learn</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>8</volume>. <pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id>, <?supplied-pmid 24600388?><pub-id pub-id-type="pmid">24600388</pub-id></mixed-citation>
    </ref>
    <ref id="bib2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Aggarwal</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Hinneburg</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Keim</surname>, <given-names>D. A.</given-names></string-name></person-group> (<year>2001</year>). <article-title>On the surprising behavior of distance metrics in high dimensional space</article-title>. In <person-group person-group-type="editor"><string-name><given-names>J.</given-names><surname>Van den Bussche</surname></string-name> &amp; <string-name><given-names>V.</given-names><surname>Vianu</surname></string-name></person-group> (Eds.), <source>Database theory — ICDT 2001. Lecture notes in computer science: Vol. 1973</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>. <pub-id pub-id-type="doi">10.1007/3-540-44503-X_27</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Agrawal</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gehrke</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gunopulos</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Raghavan</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2005</year>). <article-title>Automatic subspace clustering of high dimensional data</article-title>. <source>Data Mining and Knowledge Discovery</source>. <pub-id pub-id-type="doi">10.1007/s10618-005-1396-1</pub-id></mixed-citation>
    </ref>
    <ref id="bib4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ahuja</surname>, <given-names>R. K.</given-names></string-name>, <string-name><surname>Magnanti</surname>, <given-names>T. L.</given-names></string-name>, &amp; <string-name><surname>Orlin</surname>, <given-names>J. B.</given-names></string-name></person-group> (<year>1989</year>). <article-title>Network flows</article-title>. In <source>Handbooks in operations research and management science</source>. <publisher-name>Elsevier</publisher-name>. <pub-id pub-id-type="doi">10.1016/S0927-0507(89)01005-4</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Anscombe</surname>, <given-names>F. J.</given-names></string-name></person-group> (<year>1973</year>). <article-title>Graphs in statistical analysis</article-title>. <source>American Statistician</source>. <pub-id pub-id-type="doi">10.1080/00031305.1973.10478966</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Balasubramanian</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Schwartz</surname>, <given-names>E. L.</given-names></string-name></person-group> (<year>2002</year>). <article-title>The isomap algorithm and topological stability</article-title>. <source>Science</source>. <pub-id pub-id-type="doi">10.1126/science.295.5552.7a</pub-id>, <?supplied-pmid 11778013?><pub-id pub-id-type="pmid">11778013</pub-id></mixed-citation>
    </ref>
    <ref id="bib7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Wymbs</surname>, <given-names>N. F.</given-names></string-name>, <string-name><surname>Porter</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Mucha</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Carlson</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Grafton</surname>, <given-names>S. T.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Dynamic reconfiguration of human brain networks during learning</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <pub-id pub-id-type="doi">10.1073/pnas.1018985108</pub-id>, <?supplied-pmid 21502525?><pub-id pub-id-type="pmid">21502525</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Wymbs</surname>, <given-names>N. F.</given-names></string-name>, <string-name><surname>Rombach</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Porter</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Mucha</surname>, <given-names>P. J.</given-names></string-name>, &amp; <string-name><surname>Grafton</surname>, <given-names>S. T.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Task-based core-periphery organization of human brain dynamics</article-title>. <source>PLoS Computational Biology</source>, <volume>9</volume>(<issue>9</issue>), <fpage>1</fpage>–<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1003171</pub-id>, <?supplied-pmid 24086116?><pub-id pub-id-type="pmid">24086116</pub-id></mixed-citation>
    </ref>
    <ref id="bib9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bayá</surname>, <given-names>A. E.</given-names></string-name>, &amp; <string-name><surname>Granitto</surname>, <given-names>P. M.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Clustering gene expression data with a penalized graph-based metric</article-title>. <source>BMC Bioinformatics</source>, <volume>12</volume>(<issue>1</issue>), <fpage>2</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-12-2</pub-id>, <?supplied-pmid 21205299?><pub-id pub-id-type="pmid">21205299</pub-id></mixed-citation>
    </ref>
    <ref id="bib10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Behzadi</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Restom</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Liau</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Liu</surname>, <given-names>T. T.</given-names></string-name></person-group> (<year>2007</year>). <article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title>. <source>NeuroImage</source>, <volume>37</volume>(<issue>1</issue>), <fpage>90</fpage>–<lpage>101</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id>, <?supplied-pmid 17560126?><pub-id pub-id-type="pmid">17560126</pub-id></mixed-citation>
    </ref>
    <ref id="bib11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belchi</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Brodzki</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Burfitt</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Niranjan</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2020</year>). <article-title>A numerical measure of the instability of mapper-type algorithms</article-title>. <source>Journal of Machine Learning Research</source>, <volume>21</volume>, <fpage>1</fpage>–<lpage>45</lpage>.<pub-id pub-id-type="pmid">34305477</pub-id></mixed-citation>
    </ref>
    <ref id="bib12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Belkin</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Niyogi</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Laplacian eigenmaps for dimensionality reduction and data representation</article-title>. <source>Neural Computation</source>. <pub-id pub-id-type="doi">10.1162/089976603321780317</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Beyer</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Goldstein</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ramakrishnan</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Shaft</surname>, <given-names>U.</given-names></string-name></person-group> (<year>1998</year>). <article-title>When is “nearest neighbor” meaningful?</article-title>
<source>Lecture notes in computer science (Including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics)</source>. <pub-id pub-id-type="doi">10.1007/3-540-49257-7_15</pub-id></mixed-citation>
    </ref>
    <ref id="bib14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bodnar</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cangea</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Liò</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Deep Graph Mapper: Seeing graphs through the neural lens</article-title>. <source>arXiv Preprint arXiv:2002.03864</source>. <pub-id pub-id-type="doi">10.3389/fdata.2021.680535</pub-id>, <?supplied-pmid 34282408?><pub-id pub-id-type="pmid">34282408</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonneel</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Van De Panne</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Paris</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Heidrich</surname>, <given-names>W.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Displacement interpolation using Lagrangian mass transport</article-title>. <source>ACM Transactions on Graphics</source>. <pub-id pub-id-type="doi">10.1145/2024156.2024192</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Borgatti</surname>, <given-names>S. P.</given-names></string-name>, &amp; <string-name><surname>Everett</surname>, <given-names>M. G.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Models of core/periphery structures</article-title>. <source>Social Networks</source>, <volume>21</volume>(<issue>4</issue>), <fpage>375</fpage>–<lpage>395</lpage>. <pub-id pub-id-type="doi">10.1016/S0378-8733(99)00019-2</pub-id></mixed-citation>
    </ref>
    <ref id="bib17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bruno</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Romano</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mazaika</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lightbody</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Hazlett</surname>, <given-names>H. C.</given-names></string-name>, <string-name><surname>Piven</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Reiss</surname>, <given-names>A. L.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Longitudinal identification of clinically distinct neurophenotypes in young children with fragile X syndrome</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <pub-id pub-id-type="doi">10.1073/pnas.1620994114</pub-id>, <?supplied-pmid 28923933?><pub-id pub-id-type="pmid">28923933</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bullmore</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Complex brain networks: Graph theoretical analysis of structural and functional systems</article-title>. <source>Nature Reviews Neuroscience</source>. <pub-id pub-id-type="doi">10.1038/nrn2575</pub-id>, <?supplied-pmid 19190637?><pub-id pub-id-type="pmid">19190637</pub-id></mixed-citation>
    </ref>
    <ref id="bib19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Burago</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Burago</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Ivanov</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2001</year>). <source>Graduate studies in mathematics: Vol. 33. A course in metric geometry</source>. <publisher-loc>Providence, RI</publisher-loc>: <publisher-name>American Mathematical Society</publisher-name>. <pub-id pub-id-type="doi">10.1090/gsm/033</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bzdok</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Steyerberg</surname>, <given-names>E. W.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Prediction, not association, paves the road to precision medicine</article-title>. <source>JAMA Psychiatry</source>. <pub-id pub-id-type="doi">10.1001/jamapsychiatry.2020.2549</pub-id>, <?supplied-pmid 32804995?><pub-id pub-id-type="pmid">32804995</pub-id></mixed-citation>
    </ref>
    <ref id="bib21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Calhoun</surname>, <given-names>V. D.</given-names></string-name>, <string-name><surname>Miller</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Pearlson</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Adali</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2014</year>). <article-title>The chronnectome: Time-varying connectivity networks as the next frontier in fMRI data discovery</article-title>. <source>Neuron</source>, <volume>84</volume>(<issue>2</issue>), <fpage>262</fpage>–<lpage>274</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.10.015</pub-id>, <?supplied-pmid 25374354?><pub-id pub-id-type="pmid">25374354</pub-id></mixed-citation>
    </ref>
    <ref id="bib22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carlsson</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Topological pattern recognition for point cloud data</article-title>. <source>Acta Numerica</source>, <volume>23</volume>, <fpage>289</fpage>–<lpage>368</lpage>. <pub-id pub-id-type="doi">10.1017/S0962492914000051</pub-id></mixed-citation>
    </ref>
    <ref id="bib23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carriere</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Michel</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Oudot</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Statistical analysis and parameter selection for Mapper</article-title>. <source>The Journal of Machine Learning Research</source>, <volume>19</volume>(<issue>1</issue>), <fpage>478</fpage>–<lpage>516</lpage>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chowdhury</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Clause</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mémoli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Sánchez</surname>, <given-names>J. Á.</given-names></string-name>, &amp; <string-name><surname>Wellner</surname>, <given-names>Z.</given-names></string-name></person-group> (<year>2020</year>). <article-title>New families of stable simplicial filtration functors</article-title>. <source>Topology and Its Applications</source>. <pub-id pub-id-type="doi">10.1016/j.topol.2020.107254</pub-id></mixed-citation>
    </ref>
    <ref id="bib25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chowdhury</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Mémoli</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2019</year>). <article-title>The Gromov–Wasserstein distance between networks and stable network invariants</article-title>. <source>Information and Inference: A Journal of the IMA</source>, <volume>8</volume>(<issue>4</issue>), <fpage>757</fpage>–<lpage>787</lpage>. <pub-id pub-id-type="doi">10.1093/imaiai/iaz026</pub-id></mixed-citation>
    </ref>
    <ref id="bib27">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Chowdhury</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Needham</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Gromov-Wasserstein averaging in a Riemannian framework</article-title>. In <source>2020 IEEE/CVF conference on computer vision and pattern recognition workshops (CVPRW), Seattle, WA, USA, 2020</source> (pp. <fpage>3676</fpage>–<lpage>3684</lpage>). <pub-id pub-id-type="doi">10.1109/CVPRW50498.2020.00429</pub-id></mixed-citation>
    </ref>
    <ref id="bib26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chowdhury</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Needham</surname>, <given-names>T.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Generalized spectral clustering via Gromov-Wasserstein learning</article-title>. <source>AISTATS. arXiv:2006.04163</source></mixed-citation>
    </ref>
    <ref id="bib28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coifman</surname>, <given-names>R. R.</given-names></string-name>, &amp; <string-name><surname>Lafon</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Diffusion maps</article-title>. <source>Applied and Computational Harmonic Analysis</source>. <pub-id pub-id-type="doi">10.1016/j.acha.2006.04.006</pub-id></mixed-citation>
    </ref>
    <ref id="bib29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cragg</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Gilmore</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Skills underlying mathematics: The role of executive function in the development of mathematics proficiency</article-title>. <source>Trends in Neuroscience and Education</source>. <pub-id pub-id-type="doi">10.1016/j.tine.2013.12.001</pub-id></mixed-citation>
    </ref>
    <ref id="bib30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cunningham</surname>, <given-names>J. P.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>B. M.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Dimensionality reduction for large-scale neural recordings</article-title>. <source>Nature Neuroscience</source>. <pub-id pub-id-type="doi">10.1038/nn.3776</pub-id>, <?supplied-pmid 25151264?><pub-id pub-id-type="pmid">25151264</pub-id></mixed-citation>
    </ref>
    <ref id="bib31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dadi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Rahim</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Abraham</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Chyzhyk</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Milham</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Benchmarking functional connectome-based predictive models for resting-state fMRI</article-title>. <source>NeuroImage</source>, <volume>192</volume>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.062</pub-id>, <?supplied-pmid 30836146?><pub-id pub-id-type="pmid">30836146</pub-id></mixed-citation>
    </ref>
    <ref id="bib32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Silva</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Perry</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Plex: A MATLAB library for studying simplicial homology</article-title>. <source>TMSCSCS: Plex</source>, <volume>24</volume>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Balsters</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Flavell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cussans</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Ramnani</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2009</year>). <article-title>A probabilistic MR atlas of the human cerebellum</article-title>. <source>NeuroImage</source>, <volume>46</volume>(<issue>1</issue>), <fpage>39</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.01.045</pub-id>, <?supplied-pmid 19457380?><pub-id pub-id-type="pmid">19457380</pub-id></mixed-citation>
    </ref>
    <ref id="bib34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dłotko</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Ball mapper: A shape summary for topological data analysis</article-title>. <source>arXiv Preprint arXiv:1901.07410</source>.</mixed-citation>
    </ref>
    <ref id="bib35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dong</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Charikar</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Li</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Efficient K-nearest neighbor graph construction for generic similarity measures</article-title>. <source>Proceedings of the 20th International Conference on World Wide Web, WWW 2011</source>. <pub-id pub-id-type="doi">10.1145/1963405.1963487</pub-id></mixed-citation>
    </ref>
    <ref id="bib36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duman</surname>, <given-names>A. N.</given-names></string-name>, <string-name><surname>Tatar</surname>, <given-names>A. E.</given-names></string-name>, &amp; <string-name><surname>Pirim</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Uncovering dynamic brain reconfiguration in MEG working memory n-back task using topological data analysis</article-title>. <source>Brain Sciences</source>. <pub-id pub-id-type="doi">10.3390/brainsci9060144</pub-id>, <?supplied-pmid 31248185?><pub-id pub-id-type="pmid">31248185</pub-id></mixed-citation>
    </ref>
    <ref id="bib37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duponchel</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Exploring hyperspectral imaging data sets with topological data analysis</article-title>. <source>Analytica Chimica Acta</source>, <volume>1000</volume>, <fpage>123</fpage>–<lpage>131</lpage>. <pub-id pub-id-type="doi">10.1016/j.aca.2017.11.029</pub-id>, <?supplied-pmid 29289301?><pub-id pub-id-type="pmid">29289301</pub-id></mixed-citation>
    </ref>
    <ref id="bib38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteban</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Blair</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Markiewicz</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Berleant</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Moodie</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>F.</given-names></string-name>, … <string-name><surname>Gorgolewski</surname>, <given-names>K. J.</given-names></string-name></person-group> (<year>2018a</year>). <article-title>fMRIPrep</article-title>. <source>Software</source>. <pub-id pub-id-type="doi">10.5281/zenodo.852659</pub-id></mixed-citation>
    </ref>
    <ref id="bib39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Esteban</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Markiewicz</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Blair</surname>, <given-names>R. W.</given-names></string-name>, <string-name><surname>Moodie</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Isik</surname>, <given-names>A. I.</given-names></string-name>, <string-name><surname>Erramuzpe Aliaga</surname>, <given-names>A.</given-names></string-name>, … <string-name><surname>Gorgolewski</surname>, <given-names>K. J.</given-names></string-name></person-group> (<year>2018b</year>). <article-title>fMRIPrep: A robust preprocessing pipeline for functional MRI</article-title>. <source>Nature Methods</source>. <pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id>, <?supplied-pmid 30532080?><pub-id pub-id-type="pmid">30532080</pub-id></mixed-citation>
    </ref>
    <ref id="bib40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Evans</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Janke</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Collins</surname>, <given-names>D. L.</given-names></string-name>, &amp; <string-name><surname>Baillet</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Brain templates and atlases</article-title>. <source>NeuroImage</source>, <volume>62</volume>(<issue>2</issue>), <fpage>911</fpage>–<lpage>922</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.024</pub-id>, <?supplied-pmid 22248580?><pub-id pub-id-type="pmid">22248580</pub-id></mixed-citation>
    </ref>
    <ref id="bib41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Everett</surname>, <given-names>M. G.</given-names></string-name>, &amp; <string-name><surname>Borgatti</surname>, <given-names>S. P.</given-names></string-name></person-group> (<year>2000</year>). <article-title>Peripheries of cohesive subsets</article-title>. <source>Social Networks</source>, <volume>21</volume>(<issue>4</issue>), <fpage>397</fpage>–<lpage>407</lpage>. <pub-id pub-id-type="doi">10.1016/S0378-8733(99)00020-9</pub-id></mixed-citation>
    </ref>
    <ref id="bib42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Faskowitz</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Esfahlani</surname>, <given-names>F. Z.</given-names></string-name>, <string-name><surname>Jo</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Betzel</surname>, <given-names>R. F.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Edge-centric functional network representations of human cerebral cortex reveal overlapping system-level architecture</article-title>. <source>Nature Neuroscience</source>. <pub-id pub-id-type="doi">10.1038/s41593-020-00719-y</pub-id>, <?supplied-pmid 33077948?><pub-id pub-id-type="pmid">33077948</pub-id></mixed-citation>
    </ref>
    <ref id="bib43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Howard</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Frackowiak</surname>, <given-names>R. S. J.</given-names></string-name>, &amp; <string-name><surname>Turner</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1996</year>). <article-title>Movement-related effects in fMRI time-series</article-title>. <source>Magnetic Resonance in Medicine</source>, <volume>35</volume>(<issue>3</issue>), <fpage>346</fpage>–<lpage>355</lpage>. <pub-id pub-id-type="doi">10.1002/mrm.1910350312</pub-id>, <?supplied-pmid 8699946?><pub-id pub-id-type="pmid">8699946</pub-id></mixed-citation>
    </ref>
    <ref id="bib44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Geniesse</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Petri</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Saggar</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Generating dynamical neuroimaging spatiotemporal representations (DyNeuSR) using topological data analysis</article-title>. <source>Network Neuroscience</source>, <volume>3</volume>(<issue>3</issue>). <pub-id pub-id-type="doi">10.1162/netn_a_00093</pub-id>, <?supplied-pmid 31410378?><pub-id pub-id-type="pmid">31410378</pub-id></mixed-citation>
    </ref>
    <ref id="bib45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Giusti</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ghrist</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Two’s company, three (or more) is a simplex: Algebraic-topological tools for understanding higher-order structure in neural data</article-title>. <source>Journal of Computational Neuroscience</source>, <volume>41</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1007/s10827-016-0608-6</pub-id>, <?supplied-pmid 27287487?><pub-id pub-id-type="pmid">27287487</pub-id></mixed-citation>
    </ref>
    <ref id="bib46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gonzalez-Castillo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Caballero-Gaudes</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Topolski</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Handwerker</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Pereira</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Imaging the spontaneous flow of thought: Distinct periods of cognition contribute to dynamic functional connectivity during rest</article-title>. <source>NeuroImage</source>, <volume>202</volume>, <fpage>116129</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116129</pub-id>, <?supplied-pmid 31461679?><pub-id pub-id-type="pmid">31461679</pub-id></mixed-citation>
    </ref>
    <ref id="bib47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gonzalez-Castillo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hoy</surname>, <given-names>C. W.</given-names></string-name>, <string-name><surname>Handwerker</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Robinson</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Buchanan</surname>, <given-names>L. C.</given-names></string-name>, <string-name><surname>Saad</surname>, <given-names>Z. S.</given-names></string-name>, &amp; <string-name><surname>Bandettini</surname>, <given-names>P. A.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Tracking ongoing cognition in individuals using brief, whole-brain functional connectivity patterns</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>112</volume>(<issue>28</issue>), <fpage>8762</fpage>–<lpage>8767</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1501242112</pub-id>, <?supplied-pmid 26124112?><pub-id pub-id-type="pmid">26124112</pub-id></mixed-citation>
    </ref>
    <ref id="bib48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gonzalez</surname>, <given-names>T. F.</given-names></string-name></person-group> (<year>1985</year>). <article-title>Clustering to minimize the maximum intercluster distance</article-title>. <source>Theoretical Computer Science</source>. <pub-id pub-id-type="doi">10.1016/0304-3975(85)90224-5</pub-id></mixed-citation>
    </ref>
    <ref id="bib49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gordon</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T. O.</given-names></string-name>, <string-name><surname>Adeyemo</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Huckins</surname>, <given-names>J. F.</given-names></string-name>, <string-name><surname>Kelley</surname>, <given-names>W. M.</given-names></string-name>, &amp; <string-name><surname>Petersen</surname>, <given-names>S. E.</given-names></string-name></person-group> (<year>2016</year>). <article-title>Generation and evaluation of a cortical area parcellation from resting-state correlations</article-title>. <source>Cerebral Cortex</source>, <volume>26</volume>(<issue>1</issue>), <fpage>288</fpage>–<lpage>303</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhu239</pub-id>, <?supplied-pmid 25316338?><pub-id pub-id-type="pmid">25316338</pub-id></mixed-citation>
    </ref>
    <ref id="bib50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gordon</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T. O.</given-names></string-name>, <string-name><surname>Gilmore</surname>, <given-names>A. W.</given-names></string-name>, <string-name><surname>Newbold</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Greene</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Berg</surname>, <given-names>J. J.</given-names></string-name>, … <string-name><surname>Dosenbach</surname>, <given-names>N. U. F.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Precision functional mapping of individual human brains</article-title>. <source>Neuron</source>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.07.011</pub-id>, <?supplied-pmid 28757305?><pub-id pub-id-type="pmid">28757305</pub-id></mixed-citation>
    </ref>
    <ref id="bib51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hinneburg</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Keim</surname>, <given-names>D. A.</given-names></string-name></person-group> (<year>1999</year>). <article-title>Optimal grid-clustering: Towards breaking the curse of dimensionality in high-dimensional clustering</article-title>. In <source>Proceedings of the 25th international conference on very large data bases (VLDB ʼ99)</source> (pp. <fpage>506</fpage>–<lpage>517</lpage>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Indyk</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Motwani</surname>, <given-names>R.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Approximate nearest neighbors: Towards removing the curse of dimensionality</article-title>. <source>Conference Proceedings of the Annual ACM Symposium on Theory of Computing</source>. <pub-id pub-id-type="doi">10.4086/toc.2012.v008a014</pub-id></mixed-citation>
    </ref>
    <ref id="bib53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>James</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Witten</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hastie</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Tibshirani</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2000</year>). <article-title>An introduction to statistical learning</article-title>. <source>Current Medicinal Chemistry</source>. <pub-id pub-id-type="doi">10.1007/978-1-4614-7138-7</pub-id></mixed-citation>
    </ref>
    <ref id="bib54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jeitziner</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Carrière</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rougemont</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Oudot</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hess</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Brisken</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Two-Tier Mapper, an unbiased topology-based clustering method for enhanced global gene expression analysis</article-title>. <source>Bioinformatics</source>, <volume>35</volume>(<issue>18</issue>), <fpage>3339</fpage>–<lpage>3347</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz052</pub-id>, <?supplied-pmid 30753284?><pub-id pub-id-type="pmid">30753284</pub-id></mixed-citation>
    </ref>
    <ref id="bib55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khambhati</surname>, <given-names>A. N.</given-names></string-name>, <string-name><surname>Sizemore</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Betzel</surname>, <given-names>R. F.</given-names></string-name>, &amp; <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Modeling and interpreting mesoscale network dynamics</article-title>. <source>NeuroImage</source>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.029</pub-id>, <?supplied-pmid 28645844?><pub-id pub-id-type="pmid">28645844</pub-id></mixed-citation>
    </ref>
    <ref id="bib56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khintchine</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1934</year>). <article-title>Korrelationstheorie der stationären stochastischen Prozesse</article-title>. <source>Mathematische Annalen</source>. <pub-id pub-id-type="doi">10.1007/BF01449156</pub-id></mixed-citation>
    </ref>
    <ref id="bib57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lancichinetti</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Fortunato</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Kertész</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Detecting the overlapping and hierarchical community structure in complex networks</article-title>. <source>New Journal of Physics</source>. <pub-id pub-id-type="doi">10.1088/1367-2630/11/3/033015</pub-id></mixed-citation>
    </ref>
    <ref id="bib58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Barthel</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Dłotko</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Moosavi</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Hess</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Smit</surname>, <given-names>B.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Quantifying similarity of pore-geometry in nanoporous materials</article-title>. <source>Nature Communications</source>. <pub-id pub-id-type="doi">10.1038/ncomms15396</pub-id>, <?supplied-pmid 28534490?><pub-id pub-id-type="pmid">28534490</pub-id></mixed-citation>
    </ref>
    <ref id="bib59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liégeois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T. O.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Yeo</surname>, <given-names>B. T. T.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Interpreting temporal fluctuations in resting-state functional connectivity MRI</article-title>. <source>NeuroImage</source>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.012</pub-id>, <?supplied-pmid 28916180?><pub-id pub-id-type="pmid">28916180</pub-id></mixed-citation>
    </ref>
    <ref id="bib60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lindquist</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Meng Loh</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Atlas</surname>, <given-names>L. Y.</given-names></string-name>, &amp; <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Modeling the hemodynamic response function in fMRI: Efficiency, bias and mis-modeling</article-title>. <source>NeuroImage</source>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.10.065</pub-id>, <?supplied-pmid 19084070?><pub-id pub-id-type="pmid">19084070</pub-id></mixed-citation>
    </ref>
    <ref id="bib61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lum</surname>, <given-names>P. Y.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lehman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Ishkanov</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Vejdemo-Johansson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Alagappan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Carlsson</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Carlsson</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Extracting insights from the shape of complex data using topology</article-title>. <source>Scientific Reports</source>, <volume>3</volume>, <fpage>1</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1038/srep01236</pub-id>, <?supplied-pmid 23393618?><pub-id pub-id-type="pmid">23393618</pub-id></mixed-citation>
    </ref>
    <ref id="bib62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lurie</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Kessler</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bassett</surname>, <given-names>D. S.</given-names></string-name>, <string-name><surname>Betzel</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kheilholz</surname>, <given-names>S.</given-names></string-name>, … <string-name><surname>Calhoun</surname>, <given-names>V. D.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Questions and controversies in the study of time-varying functional connectivity in resting fMRI</article-title>. <source>Network Neuroscience</source>, <volume>4</volume>(<issue>1</issue>), <fpage>30</fpage>–<lpage>69</lpage>. <pub-id pub-id-type="doi">10.1162/netn_a_00116</pub-id>, <?supplied-pmid 32043043?><pub-id pub-id-type="pmid">32043043</pub-id></mixed-citation>
    </ref>
    <ref id="bib63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marek</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Tervo-Clemmens</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Calabro</surname>, <given-names>F. J.</given-names></string-name>, <string-name><surname>Montez</surname>, <given-names>D. F.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>B. P.</given-names></string-name>, <string-name><surname>Hatoum</surname>, <given-names>A. S.</given-names></string-name>, … <string-name><surname>Dosenbach</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2020</year>). <article-title>Towards reproducible brain-wide association studies</article-title>. <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2020.08.21.257758</pub-id></mixed-citation>
    </ref>
    <ref id="bib64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Matejka</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Fitzmaurice</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing</article-title>. <source>Conference on Human Factors in Computing Systems - Proceedings</source>. <pub-id pub-id-type="doi">10.1145/3025453.3025912</pub-id></mixed-citation>
    </ref>
    <ref id="bib65">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McInnes</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Healy</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Melville</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2018</year>). <article-title>UMAP: Uniform manifold approximation and projection for dimension reduction</article-title>. <source>arXiv:1802.03426</source></mixed-citation>
    </ref>
    <ref id="bib66">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mitchell</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Hacker</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Breshears</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Szrama</surname>, <given-names>N. P.</given-names></string-name>, <string-name><surname>Sharma</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bundy</surname>, <given-names>D. T.</given-names></string-name>, … <string-name><surname>Leuthardt</surname>, <given-names>E. C.</given-names></string-name></person-group> (<year>2013</year>). <article-title>A novel data-driven approach to preoperative mapping of functional cortex using resting-state functional magnetic resonance imaging</article-title>. <source>Neurosurgery</source>, <volume>73</volume>(<issue>6</issue>), <fpage>963</fpage>–<lpage>969</lpage>. <pub-id pub-id-type="doi">10.1227/NEU.0000000000000141</pub-id>, <?supplied-pmid 24264234?><pub-id pub-id-type="pmid">24264234</pub-id></mixed-citation>
    </ref>
    <ref id="bib67">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Munch</surname>, <given-names>E.</given-names></string-name></person-group> (<year>2017</year>). <article-title>A user’s guide to topological data analysis</article-title>. <source>Journal of Learning Analytics</source>. <pub-id pub-id-type="doi">10.18608/jla.2017.42.6</pub-id></mixed-citation>
    </ref>
    <ref id="bib68">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Newman</surname>, <given-names>M. E. J.</given-names></string-name></person-group> (<year>2006</year>). <article-title>Modularity and community structure in networks</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>103</volume>(<issue>23</issue>), <fpage>8577</fpage>–<lpage>8582</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0601602103</pub-id>, <?supplied-pmid 16723398?><pub-id pub-id-type="pmid">16723398</pub-id></mixed-citation>
    </ref>
    <ref id="bib69">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nichols</surname>, <given-names>A. L. A.</given-names></string-name>, <string-name><surname>Eichler</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Latham</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Zimmer</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2017</year>). <article-title>A global brain state underlies C. Elegans sleep behavior</article-title>. <source>Science</source>. <pub-id pub-id-type="doi">10.1126/science.aam6851</pub-id>, <?supplied-pmid 28642382?><pub-id pub-id-type="pmid">28642382</pub-id></mixed-citation>
    </ref>
    <ref id="bib70">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nicolau</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Levine</surname>, <given-names>A. J.</given-names></string-name>, &amp; <string-name><surname>Carlsson</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Topology based data analysis identifies a subgroup of breast cancers with a unique mutational profile and excellent survival</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>108</volume>(<issue>17</issue>), <fpage>7265</fpage>–<lpage>7270</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1102826108</pub-id>, <?supplied-pmid 21482760?><pub-id pub-id-type="pmid">21482760</pub-id></mixed-citation>
    </ref>
    <ref id="bib71">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Nielson</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Paquette</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>A. W.</given-names></string-name>, <string-name><surname>Guandique</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Tovar</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Inoue</surname>, <given-names>T.</given-names></string-name>, … <string-name><surname>Ferguson</surname>, <given-names>A. R.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Topological data analysis for discovery in preclinical spinal cord injury and traumatic brain injury</article-title>. <source>Nature Communications</source>, <volume>6</volume>, <fpage>8581</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms9581</pub-id>, <?supplied-pmid 26466022?><pub-id pub-id-type="pmid">26466022</pub-id></mixed-citation>
    </ref>
    <ref id="bib72">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Owen</surname>, <given-names>L. L. W.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>T. H.</given-names></string-name>, &amp; <string-name><surname>Manning</surname>, <given-names>J. R.</given-names></string-name></person-group> (<year>2019</year>). <article-title>High-level cognition during story listening is reflected in high-order dynamic correlations in neural activity patterns</article-title>. <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/763821</pub-id></mixed-citation>
    </ref>
    <ref id="bib73">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Patania</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Selvaggi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Veronese</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dipasquale</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Expert</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Petri</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Topological gene expression networks recapitulate brain anatomy and function</article-title>. <source>Network Neuroscience</source>. <pub-id pub-id-type="doi">10.1162/netn_a_00094</pub-id>, <?supplied-pmid 31410377?><pub-id pub-id-type="pmid">31410377</pub-id></mixed-citation>
    </ref>
    <ref id="bib74">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Petri</surname>, <given-names>G.</given-names></string-name>,<string-name><surname>Expert</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Turkheimer</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Carhart-Harris</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Nutt</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hellyer</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Vaccarino</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Homological scaffolds of brain functional networks</article-title>. <source>Journal of The Royal Society Interface</source>, <volume>11</volume>(<issue>101</issue>), <fpage>20140873</fpage>. <pub-id pub-id-type="doi">10.1098/rsif.2014.0873</pub-id>, <?supplied-pmid 25401177?><pub-id pub-id-type="pmid">25401177</pub-id></mixed-citation>
    </ref>
    <ref id="bib75">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peyré</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Cuturi</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Computational optimal transport</article-title>. <source>Foundations and Trends in Machine Learning</source>. <pub-id pub-id-type="doi">10.1561/2200000073</pub-id></mixed-citation>
    </ref>
    <ref id="bib76">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Preti</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Bolton</surname>, <given-names>T. A.</given-names></string-name>, &amp; <string-name><surname>Van De Ville</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2017</year>). <article-title>The dynamic functional connectome: State-of-the-art and perspectives</article-title>. <source>NeuroImage</source>, <volume>160</volume>, <fpage>41</fpage>–<lpage>54</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.12.061</pub-id>, <?supplied-pmid 28034766?><pub-id pub-id-type="pmid">28034766</pub-id></mixed-citation>
    </ref>
    <ref id="bib77">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prichard</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Theiler</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1994</year>). <article-title>Generating surrogate data for time series with several simultaneously measured variables</article-title>. <source>Physical Review Letters</source>. <pub-id pub-id-type="doi">10.1103/PhysRevLett.73.951</pub-id>, <?supplied-pmid 10057582?><pub-id pub-id-type="pmid">10057582</pub-id></mixed-citation>
    </ref>
    <ref id="bib78">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Qi</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Mo</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Guibas</surname>, <given-names>L. J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>PointNet: Deep learning on point sets for 3D classification and segmentation</article-title>. In <source>2017 IEEE conference on computer vision and pattern recognition (CVPR)</source> (pp. <fpage>77</fpage>–<lpage>85</lpage>). <pub-id pub-id-type="doi">10.1109/CVPR.2017.16</pub-id></mixed-citation>
    </ref>
    <ref id="bib79">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Qin</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Gammeter</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bossard</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Quack</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Van Gool</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Hello neighbor: Accurate object retrieval with <italic toggle="yes">k-</italic>reciprocal nearest neighbors</article-title>. <source>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source>. <pub-id pub-id-type="doi">10.1109/CVPR.2011.5995373</pub-id></mixed-citation>
    </ref>
    <ref id="bib80">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rabadán</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Blumberg</surname>, <given-names>A. J.</given-names></string-name></person-group> (<year>2019</year>). <source>Topological data analysis for genomics and evolution: Topology in biology</source>. <pub-id pub-id-type="doi">10.1017/9781316671665</pub-id></mixed-citation>
    </ref>
    <ref id="bib81">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Radovanović</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nanopoulos</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Ivanović</surname>, <given-names>M.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Hubs in space: Popular nearest neighbors in high-dimensional data</article-title>. <source>Journal of Machine Learning Research</source>, <volume>11</volume>(<issue>2010</issue>), <fpage>2487</fpage>–<lpage>2531</lpage>.</mixed-citation>
    </ref>
    <ref id="bib82">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raghubar</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Barnes</surname>, <given-names>M. A.</given-names></string-name>, &amp; <string-name><surname>Hecht</surname>, <given-names>S. A.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Working memory and mathematics: A review of developmental, individual difference, and cognitive approaches</article-title>. <source>Learning and Individual Differences</source>. <pub-id pub-id-type="doi">10.1016/j.lindif.2009.10.005</pub-id></mixed-citation>
    </ref>
    <ref id="bib83">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ravasz</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>Barabási</surname>, <given-names>A. L.</given-names></string-name></person-group> (<year>2003</year>). <article-title>Hierarchical organization in complex networks</article-title>. <source>Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics</source>. <pub-id pub-id-type="doi">10.1103/PhysRevE.67.026112</pub-id>, <?supplied-pmid 12636753?><pub-id pub-id-type="pmid">12636753</pub-id></mixed-citation>
    </ref>
    <ref id="bib84">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rizvi</surname>, <given-names>A. H.</given-names></string-name>, <string-name><surname>Camara</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Kandror</surname>, <given-names>E. K.</given-names></string-name>, <string-name><surname>Roberts</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Schieren</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Maniatis</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Rabadan</surname>, <given-names>R.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Single-cell topological RNA-seq analysis reveals insights into cellular differentiation and development</article-title>. <source>Nature Biotechnology</source>. <pub-id pub-id-type="doi">10.1038/nbt.3854</pub-id>, <?supplied-pmid 28459448?><pub-id pub-id-type="pmid">28459448</pub-id></mixed-citation>
    </ref>
    <ref id="bib85">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Romano</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Nicolau</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Quintin</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Mazaika</surname>, <given-names>P. K.</given-names></string-name>, <string-name><surname>Lightbody</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Cody Hazlett</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Piven</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Carlsson</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Reiss</surname>, <given-names>A. L.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Topological methods reveal high and low functioning neuro-phenotypes within fragile X syndrome</article-title>. <source>Human Brain Mapping</source>. <pub-id pub-id-type="doi">10.1002/hbm.22521</pub-id>, <?supplied-pmid 24737721?><pub-id pub-id-type="pmid">24737721</pub-id></mixed-citation>
    </ref>
    <ref id="bib86">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rombach</surname>, <given-names>M. P.</given-names></string-name>, <string-name><surname>Porter</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Fowler</surname>, <given-names>J. H.</given-names></string-name>, &amp; <string-name><surname>Mucha</surname>, <given-names>P. J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Core-periphery structure in networks</article-title>. <source>SIAM Journal on Applied Mathematics</source>. <pub-id pub-id-type="doi">10.1137/120881683</pub-id></mixed-citation>
    </ref>
    <ref id="bib87">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rubinov</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Complex network measures of brain connectivity: Uses and interpretations</article-title>. <source>NeuroImage</source>, <volume>52</volume>(<issue>3</issue>), <fpage>1059</fpage>–<lpage>1069</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.003</pub-id>, <?supplied-pmid 19819337?><pub-id pub-id-type="pmid">19819337</pub-id></mixed-citation>
    </ref>
    <ref id="bib88">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rubinov</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ypma</surname>, <given-names>R. J. F</given-names></string-name>, <string-name><surname>Watson</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Bullmore</surname>, <given-names>E. T.</given-names></string-name></person-group> (<year>2015</year>). <article-title>Wiring cost and topological participation of the mouse brain connectome</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>112</volume>(<issue>32</issue>), <fpage>10032</fpage>–<lpage>10037</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1420315112</pub-id>, <?supplied-pmid 26216962?><pub-id pub-id-type="pmid">26216962</pub-id></mixed-citation>
    </ref>
    <ref id="bib89">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rubner</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Tomasi</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Guibas</surname>, <given-names>L. J.</given-names></string-name></person-group> (<year>2000</year>). <article-title>The earth mover’s distance as a metric for image retrieval</article-title>. <source>International Journal of Computer Vision</source>, <volume>40</volume>(<issue>2</issue>), <fpage>99</fpage>–<lpage>121</lpage>. <pub-id pub-id-type="doi">10.1023/A:1026543900054</pub-id></mixed-citation>
    </ref>
    <ref id="bib90">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saggar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Shine</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Liégeois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Dosenbach</surname>, <given-names>N. U. F.</given-names></string-name>, &amp; <string-name><surname>Fair</surname>, <given-names>D.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Precision dynamical mapping using topological data analysis reveals a unique hub-like transition state at rest</article-title>. <source>BioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2021.08.05.455149</pub-id></mixed-citation>
    </ref>
    <ref id="bib91">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saggar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Gonzalez-Castillo</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bandettini</surname>, <given-names>P. A. P. A.</given-names></string-name>, <string-name><surname>Carlsson</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Glover</surname>, <given-names>G.</given-names></string-name>, &amp; <string-name><surname>Reiss</surname>, <given-names>A. L.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Towards a new approach to reveal dynamical organization of the brain using topological data analysis</article-title>. <source>Nature Communications</source>, <volume>9</volume>(<issue>1</issue>). <pub-id pub-id-type="doi">10.1038/s41467-018-03664-4</pub-id>, <?supplied-pmid 29643350?><pub-id pub-id-type="pmid">29643350</pub-id></mixed-citation>
    </ref>
    <ref id="bib92">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Saggar</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Uddin</surname>, <given-names>L. Q.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Pushing the boundaries of psychiatric neuroimaging to ground diagnosis in biology</article-title>. <source>ENeuro</source>. <pub-id pub-id-type="doi">10.1523/ENEURO.0384-19.2019</pub-id>, <?supplied-pmid 31685674?><pub-id pub-id-type="pmid">31685674</pub-id></mixed-citation>
    </ref>
    <ref id="bib93">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sardiu</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Gilmore</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Groppe</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Dutta</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Florens</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Washburn</surname>, <given-names>M. P.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Topological scoring of protein interaction networks</article-title>. <source>Nature Communications</source>. <pub-id pub-id-type="doi">10.1038/s41467-019-09123-y</pub-id>, <?supplied-pmid 30850613?><pub-id pub-id-type="pmid">30850613</pub-id></mixed-citation>
    </ref>
    <ref id="bib94">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sheikholeslami</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Chatterjee</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Zhang</surname>, <given-names>A.</given-names></string-name></person-group> (<year>1998</year>). <article-title>Wavecluster: A multi-resolution clustering approach for very large spatial databases</article-title>. In <source>Proceedings of the 24rd international conference on very large data bases (VLDB ʼ98)</source> (pp. <fpage>428</fpage>–<lpage>439</lpage>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib95">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Finn</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Scheinost</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Rosenberg</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Chun</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Papademetris</surname>, <given-names>X.</given-names></string-name>, &amp; <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Using connectome-based predictive modeling to predict individual behavior from brain connectivity</article-title>. <source>Nature Protocols</source>, <volume>12</volume>(<issue>3</issue>). <pub-id pub-id-type="doi">10.1038/nprot.2016.178</pub-id>, <?supplied-pmid 28182017?><pub-id pub-id-type="pmid">28182017</pub-id></mixed-citation>
    </ref>
    <ref id="bib96">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shine</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Bissett</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Bell</surname>, <given-names>P. T.</given-names></string-name>, <string-name><surname>Koyejo</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Balsters</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Gorgolewski</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Moodie</surname>, <given-names>C. A.</given-names></string-name>, &amp; <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name></person-group> (<year>2016</year>). <article-title>The dynamics of functional brain networks: Integrated network states during cognitive task performance</article-title>. <source>Neuron</source>. <pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.018</pub-id>, <?supplied-pmid 27693256?><pub-id pub-id-type="pmid">27693256</pub-id></mixed-citation>
    </ref>
    <ref id="bib97">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shine</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bell</surname>, <given-names>P. T.</given-names></string-name>, <string-name><surname>Martens</surname>, <given-names>K. A. E.</given-names></string-name>, <string-name><surname>Shine</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Koyejo</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name></person-group> (<year>2019a</year>). <article-title>Human cognition involves the dynamic integration of neural activity and neuromodulatory systems</article-title>. <source>Nature Neuroscience</source>, <volume>22</volume>(<issue>2</issue>), <fpage>289</fpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0312-0</pub-id>, <?supplied-pmid 30664771?><pub-id pub-id-type="pmid">30664771</pub-id></mixed-citation>
    </ref>
    <ref id="bib98">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shine</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Hearne</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hwang</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Müller</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Mattingley</surname>, <given-names>J. B.</given-names></string-name>, &amp; <string-name><surname>Cocchi</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2019b</year>). <article-title>The low-dimensional neural architecture of cognitive complexity is related to activity in medial thalamic nuclei</article-title>. <source>Neuron</source>. <pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.002</pub-id>, <?supplied-pmid 31653463?><pub-id pub-id-type="pmid">31653463</pub-id></mixed-citation>
    </ref>
    <ref id="bib99">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Singh</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Mémoli</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Carlsson</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2007</year>). <article-title>Topological methods for the analysis of high dimensional data sets and 3D object recognition</article-title>. <source>Symposium on point-based graphics</source> (pp. <fpage>91</fpage>–<lpage>100</lpage>). <publisher-loc>Geneva, Switzerland</publisher-loc>: <publisher-name>Eurographics Association</publisher-name>. <pub-id pub-id-type="doi">10.2312/SPBG/SPBG07/091-100</pub-id></mixed-citation>
    </ref>
    <ref id="bib100">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2011</year>). <source>Networks of the brain</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>. <pub-id pub-id-type="doi">10.7551/mitpress/8476.001.0001</pub-id></mixed-citation>
    </ref>
    <ref id="bib101">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sporns</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Making sense of brain network data</article-title>. <source>Nature Methods</source>. <pub-id pub-id-type="doi">10.1038/nmeth.2485</pub-id>, <?supplied-pmid 23722207?><pub-id pub-id-type="pmid">23722207</pub-id></mixed-citation>
    </ref>
    <ref id="bib102">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stevner</surname>, <given-names>A. B. A.</given-names></string-name>, <string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Cabral</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rapuano</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Nielsen</surname>, <given-names>S. F. V.</given-names></string-name>, <string-name><surname>Tagliazucchi</surname>, <given-names>E.</given-names></string-name>, … <string-name><surname>Kringelbach</surname>, <given-names>M. L.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Discovery of key whole-brain transitions and dynamics during human wakefulness and non-REM sleep</article-title>. <source>Nature Communications</source>. <pub-id pub-id-type="doi">10.1038/s41467-019-08934-3</pub-id>, <?supplied-pmid 30833560?><pub-id pub-id-type="pmid">30833560</pub-id></mixed-citation>
    </ref>
    <ref id="bib103">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tagliazucchi</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Balenzuela</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Fraiman</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Chialvo</surname>, <given-names>D. R.</given-names></string-name></person-group> (<year>2012</year>). <article-title>Criticality in large-scale brain fMRI dynamics unveiled by a novel point process analysis</article-title>. <source>Frontiers in Physiology</source>. <pub-id pub-id-type="doi">10.3389/fphys.2012.00015</pub-id>, <?supplied-pmid 22347863?><pub-id pub-id-type="pmid">22347863</pub-id></mixed-citation>
    </ref>
    <ref id="bib104">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tauzin</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Lupo</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Tunstall</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Prez</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Caorsi</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Medina-Mardones</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dassatti</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Hess</surname>, <given-names>K.</given-names></string-name></person-group> (<year>2021</year>). <article-title>giotto-tda: A topological data analysis toolkit for machine learning and data exploration</article-title>. <source>Journal of Machine Learning Research</source>, <volume>22</volume>(<issue>2021</issue>), <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="bib105">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>De Silva</surname>, <given-names>V.</given-names></string-name>, &amp; <string-name><surname>Langford</surname>, <given-names>J. C.</given-names></string-name></person-group> (<year>2000</year>). <article-title>A global geometric framework for nonlinear dimensionality reduction</article-title>. <source>Science</source>, <volume>290</volume>(<issue>5500</issue>), <fpage>2319</fpage>–<lpage>2323</lpage>. <pub-id pub-id-type="doi">10.1126/science.290.5500.2319</pub-id>, <?supplied-pmid 11125149?><pub-id pub-id-type="pmid">11125149</pub-id></mixed-citation>
    </ref>
    <ref id="bib106">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Theiler</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Eubank</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Longtin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Galdrikian</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Doyne Farmer</surname>, <given-names>J.</given-names></string-name></person-group> (<year>1992</year>). <article-title>Testing for nonlinearity in time series: The method of surrogate data</article-title>. <source>Physica D: Nonlinear Phenomena</source>. <pub-id pub-id-type="doi">10.1016/0167-2789(92)90102-S</pub-id></mixed-citation>
    </ref>
    <ref id="bib107">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tie</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Rigolo</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Norton</surname>, <given-names>I. H.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>R. Y.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Orringer</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mukundan</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Golby</surname>, <given-names>A. J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Defining language networks from resting-state fMRI for surgical planning—A feasibility study</article-title>. <source>Human Brain Mapping</source>, <volume>35</volume>(<issue>3</issue>), <fpage>1018</fpage>–<lpage>1030</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.22231</pub-id>, <?supplied-pmid 23288627?><pub-id pub-id-type="pmid">23288627</pub-id></mixed-citation>
    </ref>
    <ref id="bib108">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Turk-Browne</surname>, <given-names>N. B.</given-names></string-name></person-group> (<year>2013</year>). <article-title>Functional interactions as big data in the human brain</article-title>. <source>Science</source>. <pub-id pub-id-type="doi">10.1126/science.1238409</pub-id>, <?supplied-pmid 24179218?><pub-id pub-id-type="pmid">24179218</pub-id></mixed-citation>
    </ref>
    <ref id="bib109">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Der Maaten</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2008</year>). <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of Machine Learning Research</source>, <volume>9</volume>(<issue>2008</issue>), <fpage>2579</fpage>–<lpage>2605</lpage>.</mixed-citation>
    </ref>
    <ref id="bib110">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van Der Maaten</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Postma</surname>, <given-names>E.</given-names></string-name>, &amp; <string-name><surname>den Herik</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2009</year>). <article-title>Dimensionality reduction: A comparative review</article-title>. <source>Journal of Machine Learning Research</source>, <volume>10</volume>(<issue>66–71</issue>), <fpage>13</fpage>.</mixed-citation>
    </ref>
    <ref id="bib111">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>van der Meer</surname>, <given-names>J. N.</given-names></string-name>, <string-name><surname>Breakspear</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chang</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Sonkusare</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Cocchi</surname>, <given-names>L.</given-names></string-name></person-group> (<year>n.d.</year>). <source>Movie viewing elicits rich and reliable brain state dynamics</source>. <pub-id pub-id-type="doi">10.1038/s41467-020-18717-w</pub-id>, <?supplied-pmid 33020473?></mixed-citation>
    </ref>
    <ref id="bib112">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Veen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Saul</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Eargle</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Mangham</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Kepler Mapper: A flexible Python implementation of the Mapper algorithm</article-title>. <source>Journal of Open Source Software</source>. <pub-id pub-id-type="doi">10.21105/joss.01315</pub-id></mixed-citation>
    </ref>
    <ref id="bib113">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vidaurre</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name>, &amp; <string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Brain network dynamics are hierarchically organized in time</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>114</volume>(<issue>48</issue>), <fpage>12827</fpage>–<lpage>12832</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1705120114</pub-id>, <?supplied-pmid 29087305?><pub-id pub-id-type="pmid">29087305</pub-id></mixed-citation>
    </ref>
    <ref id="bib114">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Weber</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Schek</surname>, <given-names>H. J.</given-names></string-name>, &amp; <string-name><surname>Blott</surname>, <given-names>S.</given-names></string-name></person-group> (<year>1998</year>). <article-title>A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces</article-title>. In <source>Proceedings of the 24th international conference on very large data bases</source> (pp. <fpage>194</fpage>–<lpage>205</lpage>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib115">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Welvaert</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Rosseel</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2013</year>). <article-title>On the definition of signal-to-noise ratio and contrast-to-noise ratio for fMRI data</article-title>. <source>PLoS One</source>, <volume>8</volume>(<issue>11</issue>). <pub-id pub-id-type="doi">10.1371/journal.pone.0077089</pub-id>, <?supplied-pmid 24223118?><pub-id pub-id-type="pmid">24223118</pub-id></mixed-citation>
    </ref>
    <ref id="bib116">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Welvaert</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Rosseel</surname>, <given-names>Y.</given-names></string-name></person-group> (<year>2014</year>). <article-title>A review of fMRI simulation studies</article-title>. <source>PLoS One</source>, <volume>9</volume>(<issue>7</issue>), <fpage>e101953</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0101953</pub-id>, <?supplied-pmid 25048024?><pub-id pub-id-type="pmid">25048024</pub-id></mixed-citation>
    </ref>
    <ref id="bib117">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wiener</surname>, <given-names>N.</given-names></string-name></person-group> (<year>1930</year>). <article-title>Generalized harmonic analysis</article-title>. <source>Acta Mathematica</source>. <pub-id pub-id-type="doi">10.1007/BF02546511</pub-id></mixed-citation>
    </ref>
    <ref id="bib119">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yarkoni</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Van Essen</surname>, <given-names>D. C.</given-names></string-name>, &amp; <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name></person-group> (<year>2011</year>). <article-title>Large-scale automated synthesis of human functional neuroimaging data</article-title>. <source>Nature Methods</source>, <volume>8</volume>(<issue>8</issue>), <fpage>665</fpage>–<lpage>670</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.1635</pub-id>, <?supplied-pmid 21706013?><pub-id pub-id-type="pmid">21706013</pub-id></mixed-citation>
    </ref>
    <ref id="bib120">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Chalapathi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Rathore</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Wang</surname>, <given-names>B.</given-names></string-name></person-group> (<year>n.d.</year>). <article-title>Mapper Interactive: A scalable, extendable, and interactive toolbox for the visual exploration of high-dimensional data</article-title>. <source>IEEE Pacific Visualization (PacificVis)</source>. <ext-link xlink:href="https://mapperinteractive.github.io/" ext-link-type="uri">https://mapperinteractive.github.io/</ext-link></mixed-citation>
    </ref>
  </ref-list>
</back>
