<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7178389</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa003</article-id>
    <article-id pub-id-type="publisher-id">btaa003</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Sequence Analysis</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>UDSMProt: universal deep sequence models for protein classification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-4447-0162</contrib-id>
        <name>
          <surname>Strodthoff</surname>
          <given-names>Nils</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa003-cor1"/>
        <!--<email>nils.strodthoff@hhi.fraunhofer.de</email>-->
        <xref ref-type="aff" rid="btaa003-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wagner</surname>
          <given-names>Patrick</given-names>
        </name>
        <xref ref-type="aff" rid="btaa003-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wenzel</surname>
          <given-names>Markus</given-names>
        </name>
        <xref ref-type="aff" rid="btaa003-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Samek</surname>
          <given-names>Wojciech</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa003-cor1"/>
        <!--<email>wojciech.samek@hhi.fraunhofer.de</email>-->
        <xref ref-type="aff" rid="btaa003-aff1"/>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Ponty</surname>
          <given-names>Yann</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btaa003-aff1"><institution>Department of Video Coding &amp; Analytics, Fraunhofer Heinrich Hertz Institute</institution>, Berlin 10587, <country country="DE">Germany</country></aff>
    <author-notes>
      <corresp id="btaa003-cor1">To whom correspondence should be addressed. <email>nils.strodthoff@hhi.fraunhofer.de</email> or <email>wojciech.samek@hhi.fraunhofer.de</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>4</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-01-08">
      <day>08</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>36</volume>
    <issue>8</issue>
    <fpage>2401</fpage>
    <lpage>2409</lpage>
    <history>
      <date date-type="received">
        <day>02</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>13</day>
        <month>12</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>02</day>
        <month>1</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa003.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Inferring the properties of a protein from its amino acid sequence is one of the key problems in bioinformatics. Most state-of-the-art approaches for protein classification are tailored to single classification tasks and rely on handcrafted features, such as position-specific-scoring matrices from expensive database searches. We argue that this level of performance can be reached or even be surpassed by learning a task-agnostic representation once, using self-supervised language modeling, and transferring it to specific tasks by a simple fine-tuning step.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We put forward a universal deep sequence model that is pre-trained on unlabeled protein sequences from Swiss-Prot and fine-tuned on protein classification tasks. We apply it to three prototypical tasks, namely enzyme class prediction, gene ontology prediction and remote homology and fold detection. The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them. These results stress the possibility of inferring protein properties from the sequence alone and, on more general grounds, the prospects of modern natural language processing methods in omics. Moreover, we illustrate the prospects for explainable machine learning methods in this field by selected case studies.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Source code is available under <ext-link ext-link-type="uri" xlink:href="https://github.com/nstrodt/UDSMProt">https://github.com/nstrodt/UDSMProt</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Berlin Big Data Center</institution>
          </institution-wrap>
        </funding-source>
        <award-id>01IS14013A</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Berlin Center for Machine Learning</institution>
          </institution-wrap>
        </funding-source>
        <award-id>01IS18037I</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Inferring protein properties from the underlying sequence of amino acids (primary structure) is a long-standing theme in bioinformatics and is of particular importance in the light of advances in sequencing technology and the vast number of proteins with mostly unknown properties. A rough estimate for this number is given by the size of the sparsely annotated TrEMBL dataset (158M) and should be set into perspective by comparison to the size of well-curated Swiss-Prot (<xref rid="btaa003-B41" ref-type="bibr">The UniProt Consortium, 2018</xref>) dataset (560K) with a much more complete annotation of the protein properties.</p>
    <p>There is a large body of literature on methods to infer protein properties, most of which make use of additional handcrafted features in addition to the primary sequence alone (<xref rid="btaa003-B7" ref-type="bibr">Cozzetto <italic>et al.</italic>, 2016</xref>; <xref rid="btaa003-B8" ref-type="bibr">Dalkiran <italic>et al.</italic>, 2018</xref>; Gong <xref rid="btaa003-B11" ref-type="bibr"><italic>et al.</italic>, 2016</xref>; <xref rid="btaa003-B12" ref-type="bibr">Håndstad <italic>et al.</italic>, 2007</xref>; <xref rid="btaa003-B19" ref-type="bibr">Li <italic>et al.</italic>, 2017</xref>, <xref rid="btaa003-B20" ref-type="bibr">2018</xref>; <xref rid="btaa003-B35" ref-type="bibr">Shen and Chou, 2007</xref>). These features include experimentally determined functional annotations (such as Pfam; <xref rid="btaa003-B10" ref-type="bibr">El-Gebali <italic>et al.</italic>, 2019</xref>) and information from homologous, i.e. evolutionary-related proteins. The latter are typically inferred from well-motivated but still heuristic methods such as the basic local alignment search tool (BLAST; <xref rid="btaa003-B23" ref-type="bibr">Madden, 2013</xref>) that searches a database for proteins that are homologous to a given query protein, via multiple sequence alignment. Handcrafted features based on experimental results rely on a preferably complete functional annotation and are therefore likely to fail to generalize for incompletely annotated proteins (<xref rid="btaa003-B28" ref-type="bibr">Price <italic>et al.</italic>, 2018</xref>). Handcrafted features derived from multiple sequence alignments rely on alignment algorithms that typically scale at least linearly with query and database size. This time complexity is not able to keep up with the present size and the exponential growth rates of present protein databases.</p>
    <p>These bottlenecks urge for the development of methods that allow to directly predict protein properties from the sequence of amino acids alone, which is, therefore, a topic on the agenda of many research institutions (<xref rid="btaa003-B3" ref-type="bibr">Bileschi <italic>et al.</italic>, 2019</xref>; <xref rid="btaa003-B31" ref-type="bibr">Rao <italic>et al.</italic>, 2019</xref>; Rives <italic>et al.</italic>, 2019). Methods from deep learning, and, in particular, self-supervised algorithms from natural language processing (NLP), are promising approaches in this direction.</p>
    <p>The machine learning (ML) community recently gained interest in protein classification as possible application area for deep learning methods (see e.g. <xref rid="btaa003-B1" ref-type="bibr">AlQuraishi, 2019</xref>; <xref rid="btaa003-B3" ref-type="bibr">Bileschi <italic>et al.</italic>, 2019</xref>; <xref rid="btaa003-B31" ref-type="bibr">Rao <italic>et al.</italic>, 2019</xref>; Rives <italic>et al.</italic>, 2019; <xref rid="btaa003-B50" ref-type="bibr">Upmeier zu Belzen <italic>et al.</italic>, 2019</xref>). In NLP, self-supervised approaches have shown tremendous prospects across a wide variety of tasks (<xref rid="btaa003-B9" ref-type="bibr">Devlin <italic>et al.</italic>, 2019</xref>; <xref rid="btaa003-B14" ref-type="bibr">Howard and Ruder, 2018</xref>; <xref rid="btaa003-B21" ref-type="bibr">Liu <italic>et al.</italic>, 2019</xref>; <xref rid="btaa003-B27" ref-type="bibr">Peters <italic>et al.</italic>, 2018</xref>; <xref rid="btaa003-B29" ref-type="bibr">Radford <italic>et al.</italic>, 2018</xref>, <xref rid="btaa003-B30" ref-type="bibr">2019</xref>; <xref rid="btaa003-B38" ref-type="bibr">Song <italic>et al.</italic>, 2019</xref>; <xref rid="btaa003-B45" ref-type="bibr">Yang <italic>et al.</italic>, 2019</xref>), which rely on leveraging implicit knowledge from large unlabeled corpora by pre-training using autoregressive language modeling or autoencoding tasks. This approach goes significantly beyond the use of pre-trained word embeddings, where only the embedding layer is pre-trained, whereas the rest of the model is initialized randomly.</p>
    <p>Protein classification tasks represent a tempting application domain for such techniques exploiting the analogy of amino acids as words, protein domains as sentences and proteins as text paragraphs. In this setting, global protein classification tasks, such as enzyme class prediction, are analogous to text classification tasks (e.g. sentiment analysis). Protein annotation tasks, such as secondary structure or phosphorylation site prediction, map to text annotation tasks, such as part-of-speech tagging or named entity recognition. Although this general analogy has been recognized and exploited already earlier by <xref rid="btaa003-B2" ref-type="bibr">Asgari and Mofrad (2015)</xref>, self-supervised pre-training is a rather new technique in this field. Existing literature approaches in this direction (<xref rid="btaa003-B31" ref-type="bibr">Rao <italic>et al.</italic>, 2019</xref>; Rives <italic>et al.</italic>, 2019) show significant improvements for models that were pre-trained using self-supervision compared with their counterparts trained from scratch on a variety of tasks and demonstrate that models leverage biologically sensible information from pre-training. However, none of them explicitly demonstrated that pre-training can bridge the gap to state-of-the-art approaches that mostly rely on handcrafted features such as position-specific scoring matrices (PSSMs) derived via BLAST.</p>
    <p>Our main contributions in this article are the following: (i) we put forward a universal deep sequence model for protein classification (<italic>UDSMProt</italic>) that is pre-trained on Swiss-Prot and fine-tuned on specific classification tasks without any further task-specific modifications. (ii) We demonstrate that this model is able to reach or even surpass the performance level of state-of-the-art classification algorithms many of which make use of PSSM features. This indicates the feasibility of inferring protein properties from the sequence alone across a variety of different tasks. (iii) We demonstrate the particular effectiveness of our approach for small datasets.</p>
  </sec>
  <sec>
    <title>2 Algorithms and training procedures</title>
    <sec>
      <title>2.1 UDSMProt: universal deep sequence models for protein classification</title>
      <p>The idea of <italic>UDSMProt</italic> is to apply self-supervised pre-training to a state-of-the-art recurrent neural network (RNN) architecture using a language modeling task. In this way, the model learns implicit representations from unlabeled data that can be leveraged for downstream classification tasks. We aim to address a range of different classification problems within a single architecture that is universal in the sense that only the dimensionality of the output layer has to be adapted to the specific task. This facilitates the adaptation to classification tasks beyond the three exemplary tasks considered in this work. For fine-tuning on the downstream classification tasks, all embedding weights and long short-term memory (LSTM) weights are initialized using the same set of weights obtained from language model pre-training. As we will demonstrate, this is a particularly powerful choice for small datasets.</p>
      <p>Presently, there are two main objectives for self-supervised pre-training in NLP, autoregressive language modeling and autoencoding (see <xref rid="btaa003-B45" ref-type="bibr">Yang <italic>et al.</italic>, 2019</xref> for a concise introduction to both concepts). Autoregressive language modeling is an inherently unidirectional approach whereas autoencoding directly incorporated bidirectional context. One notable example of the autoregressive category is the RNN-based <italic>AWD-LSTM</italic> language model (<xref rid="btaa003-B24" ref-type="bibr">Merity <italic>et al.</italic>, 2018</xref>) compared with <italic>BERT</italic> (<xref rid="btaa003-B9" ref-type="bibr">Devlin <italic>et al.</italic>, 2019</xref>) as main proponent for the autoencoding category. Autoregressive approaches, such as <italic>ULMFit</italic> (<xref rid="btaa003-B14" ref-type="bibr">Howard and Ruder, 2018</xref>), can be trained with considerably smaller computational budget than autoencoding, transformer-based architectures, while still showing very competitive performance on NLP text classification tasks (see e.g. Xie <italic>et al.</italic>, 2019) and are, therefore, the method of choice for our intended task.</p>
      <p>Our proposed method relies on an <italic>AWD-LSTM</italic> language model (<xref rid="btaa003-B24" ref-type="bibr">Merity <italic>et al.</italic>, 2018</xref>), which is, at its heart, a three-layer LSTM regularized by different kinds of dropouts (embedding dropout, input dropout, weight dropout, hidden state dropout and output layer dropout). During language model training, gradients are backpropagated using backpropagation through time (BPTT) with variable length sequences as in (<xref rid="btaa003-B24" ref-type="bibr">Merity <italic>et al.</italic>, 2018</xref>) using batches of ∼70 tokens and the output layer remains tied to the weights of the embedding layer. For classifier training, we use BPTT for text classification (<xref rid="btaa003-B14" ref-type="bibr">Howard and Ruder, 2018</xref>), where gradients are accumulated potentially over multiple batches without resetting the LSTM’s hidden state and backpropagated explicitly up to a maximum context of 1024 tokens. Specific model parameters are listed in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S2</xref>. The training procedure for transfer learning is largely inspired by <italic>ULMFit</italic> and proceeds as follows: In a first step, we train a language model on the Swiss-Prot database. In a second step, the language model’s output layer is replaced by a concat-pooling layer (<xref rid="btaa003-B14" ref-type="bibr">Howard and Ruder, 2018</xref>) and two fully connected layers (see <xref ref-type="fig" rid="btaa003-F1">Fig. 1</xref> for a schematic illustration). When fine-tuning the classifier, we gradually unfreeze layer group by layer group (four in total) for optimization, where we reduce the learning rate by a factor of two compared with the respective previous layer group (<xref rid="btaa003-B14" ref-type="bibr">Howard and Ruder, 2018</xref>). A single model is by construction only able to capture the context in a unidirectional manner, i.e. processing the input in the forward or backward direction. As simplest approach to incorporate bidirectional context into the final prediction, we train separate forward and backward language models with corresponding fine-tuned classifiers. An ensemble model is obtained by averaging the output probabilities of both classifiers. We use a one-cycle learning rate schedule (<xref rid="btaa003-B37" ref-type="bibr">Smith, 2018</xref>) during training for 30 epochs in the final fine-tuning step. All hyperparameters were optimized based on the model performance on a separate validation set, while we report performance on a separate test set. Our way of addressing the specific challenges of the remote homology datasets are described in Section 3.4. In all cases, we use binary/categorical crossentropy as loss function and the AdamW optimizer (<xref rid="btaa003-B22" ref-type="bibr">Loshchilov and Hutter, 2019</xref>). Note that a potential intermediate step where one fine-tunes the generic language model on the corpus underlying the classification step, as proposed by <xref rid="btaa003-B14" ref-type="bibr">Howard and Ruder (2018)</xref>, did only show an improvement in terms of language model quality but did not result in an improved downstream classification performance. This step was therefore omitted for the results presented below.
</p>
      <fig id="btaa003-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>Schematic illustration of the training procedure, here for the amino acid sequence MSLR…RI. The <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi mathvariant="italic">BOS</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:math></inline-formula>-token marks the beginning of the sequence. The red arrows show the context for forward language model for predicting next character (S) given sequence <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mi mathvariant="italic">BOS</mml:mi><mml:mo>&gt;</mml:mo></mml:mrow></mml:math></inline-formula>M of length 2. For fine-tuning on the downstream classification tasks, all embeddings weights and LSTM weights are initialized using the same set of weights obtained from language model pre-training. This has to be contrasted with the use of pre-trained embeddings, where just the embedding weights are initialized in a structured way before the downstream fine-tuning step</p>
        </caption>
        <graphic xlink:href="btaa003f1"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 Baseline model</title>
      <p>In our experiments below, we mostly compare directly to reported results from approaches in the literature on predefined datasets. However, this does not allow for in-depth comparisons that modify for example details of the training procedure. To still allow to relate the results of the proposed method to state-of-the-art performance, we use a baseline model that reaches state-of-the-art performance on literature benchmarks and that can henceforth be used as proxy for models considered in the literature.</p>
      <p>The performance of literature approaches on many protein classification tasks has been driven to a large extend by the inclusion of different kinds of handcrafted features rather than sophisticated model architectures or training procedures. The most beneficial input features throughout a variety of different classification tasks are obviously the PSSMs based on a multiple sequence alignment computed via position-specific iterative BLAST (PSI-BLAST; <xref rid="btaa003-B23" ref-type="bibr">Madden, 2013</xref>). PSI-BLAST can compare query sequences with a given sequence database and returns a list of local alignments solved with heuristics instead of optimal local alignments solved with the more time-consuming Smith–Waterman algorithm. PSI-BLAST is then used to find more distant relatives of a query protein, where a list of closely related proteins is created to get an initial general profile sequence. This profile sequence is used as a new query for the next iteration where a larger list of proteins is found for which again a profile sequence is computed. This process is repeated to a desired number of iterations. In our experiments, we used the same parameters as reported in the literature (<xref rid="btaa003-B8" ref-type="bibr">Dalkiran <italic>et al.</italic>, 2018</xref>; <xref rid="btaa003-B20" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>; <xref rid="btaa003-B35" ref-type="bibr">Shen and Chou, 2007</xref>), namely three iterations with an <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="italic">value</mml:mi></mml:mrow></mml:math></inline-formula> of 0.001 as threshold for which an alignment is considered as significant. Although the raw sequences from Swiss-Prot contain 20 standard and six non-standard amino acids, PSSM features are computed only for the 20 standard amino acids. The raw sequence of length <italic>L</italic> was then one-hot encoded into an <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>26</mml:mn></mml:mrow></mml:math></inline-formula> matrix which is concatenated with the <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> PSSM feature matrix yielding an <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>46</mml:mn></mml:mrow></mml:math></inline-formula> input matrix overall. To make use of the full parallelization capabilities while retaining most information, we padded the sequences to a maximum length of 1024 residues.</p>
      <p>For all following experiments, we used a convolutional neural network (CNN) with seven layers. Each convolutional layer was followed by a rectified linear unit and max pooling by a factor of 2. The number of filters across layers is: 1024, 512, 512, 512, 256, 256 and 256 (with valid padding mode) each with a filter size of 3. The convolutional stage was followed by flattening layer and three dense layers (512, 256 and 128) each followed by dropout (with 25% dropout rate) and finally a softmax layer with nodes for each class (e.g. six nodes for Level 1 enzyme prediction). For all models, we minimized categorical crossentropy with AdaMax, a variant of adaptive moment estimation (Adam) based on the infinity norm (<xref rid="btaa003-B15" ref-type="bibr">Kingma and Ba, 2015</xref>), which lead to slightly better results in our CNN experiments compared with the original Adam optimizer. The hyperparameters follow those provided in this article.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results and discussion</title>
    <p>The results are organized as follows: we discuss language modeling as baseline task in Section 3.1 and then demonstrate the capabilities of <italic>UDSMProt</italic> on three prototypical protein classification tasks, namely enzyme class prediction in Section 3.2, gene ontology (GO) prediction in Section 3.3 and remote homology detection in Section 3.4. For enzyme class prediction, we provide an extensive evaluation highlighting several important aspects.</p>
    <sec>
      <title>3.1 Language modeling</title>
      <p>The language modeling task involves predicting the next token for a given sequences of tokens and is one of the key NLP tasks for demonstrating the general understanding of a language. In this particular case, it builds up an implicit knowledge about the structure of proteins, which can potentially be leveraged for downstream classification tasks.</p>
      <p>Our character-based language model operates on protein sequence data tokenized on the level of amino acids (see <xref ref-type="fig" rid="btaa003-F1">Fig. 1</xref>). Interestingly, the language model performance depends strongly on the way the similarity threshold is incorporated in the train–test split procedure. For this reason, we split the data into train, validation and test set with ratios 90:5:5 and compare two methods: (i) random splits without taking sequence similarity into account and (ii) splits according to UniRef50 cluster assignments. Although the model trained on a random split reaches a perplexity of 6.88 and a corresponding next character prediction accuracy of 0.409, the model trained on a cluster-based split only reaches a perplexity of 11.75 with 0.244 accuracy. However, these differences in language model performance do not lead to measurable differences in the downstream performance (see Supplementary Section S3 for a detailed discussion). In Supplementary Section S3, we also investigate the impact of the model architecture on the language performance within several ablation studies. Language model performance metrics inherently depend on the dataset and the vocabulary size and it is hard to estimate their significance. As simplest baseline, language model prediction accuracies can be put into perspective by comparison to random guessing corresponding to an accuracy of 0.04, which conveys that the language model acquired non-trivial knowledge about the underlying construction principles of proteins. We analyze the learned representations in two ways: first, we visualize the learned amino acid embeddings via t-SNE and find good agreement with their known physio-chemical properties (<xref rid="btaa003-B40" ref-type="bibr">Taylor, 1986</xref>). Second, we analyze the model’s outputs after the contact-pooling-layer which hints at the fact that language model pre-training compared with training from scratch leads to a more efficient encoder representation (see Supplementary Section S4).</p>
    </sec>
    <sec>
      <title>3.2 Enzyme class prediction</title>
      <p>We start our analysis on downstream classification tasks with enzyme classification for the reason that it is a conceptually simple task for which a large number of annotated examples are available. The main experiments in this section are organized in a two-step process: first, we analyze the proposed approach on custom datasets in a well-defined experimental environment comparing to a baseline model operating on PSSM features (Section 3.2.2). As a second step, we directly compare to literature results demonstrating the proposed method indeed reaches or even exceeds state-of-the-art performance for this task (Section 3.2.3).</p>
      <sec>
        <label>3.2.1</label>
        <title>Task and datasets</title>
        <p>Enzyme prediction is a functional prediction task targeted to predict the enzyme commission (EC) number from a hierarchical numerical classification scheme for enzymes based on the chemical reactions they catalyze. In particular, we consider discriminating enzyme versus non-enzyme (Level 0), predicting main enzyme class (Level 1) and enzyme subclass (Level 2). A powerful EC classification algorithm of the pre-deep-learning era was provided by <italic>EzyPred</italic> (<xref rid="btaa003-B35" ref-type="bibr">Shen and Chou, 2007</xref>), which owed its success to the design of a hierarchical approach and to appropriate input features, which are a combination of the functional (BLAST against a Pfam database) and evolutionary information (PSI-BLAST against the Swiss-Prot database). For hierarchical classification (Levels 0–2), a simple k nearest neighbor classifier (KNN) was trained in order to achieve convincing results. <italic>EzyPred</italic> was superseded by <italic>DEEPre</italic> (<xref rid="btaa003-B20" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>) where deep learning was applied to raw sequence and homology data as input and that was recently extended toward multi-functional enzyme classification (<xref rid="btaa003-B49" ref-type="bibr">Zou <italic>et al.</italic>, 2019</xref>). Instead of training simple classifiers on highly engineered features, they trained feature representation and classification in an end-to-end fashion with a hybrid CNN-LSTM-approach. Recently, <italic>ECPred</italic> (<xref rid="btaa003-B8" ref-type="bibr">Dalkiran <italic>et al.</italic>, 2018</xref>) also showed competitive results by building an ensemble of well-performing classifiers (Subsequence Profile Map with PSSM (<xref rid="btaa003-B34" ref-type="bibr">Sarac <italic>et al.</italic>, 2008</xref>), BLAST-KNN (<xref rid="btaa003-B23" ref-type="bibr">Madden, 2013</xref>) and Pepstats-SVM using peptides statistics (<xref rid="btaa003-B32" ref-type="bibr">Rice <italic>et al.</italic>, 2000</xref>). Nevertheless, the drawbacks described in Section 1 remain, i.e. requiring functional annotations of homologous proteins, which are not guaranteed for evolutionary distant or insufficient annotated proteins.</p>
        <p>In addition to the existing DEEPre (similarity threshold 40%) and ECPred (similarity threshold 50%) datasets (<xref rid="btaa003-B8" ref-type="bibr">Dalkiran <italic>et al.</italic>, 2018</xref>; <xref rid="btaa003-B20" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>), we also work with two custom EC40 and EC50 datasets, which provide all cluster members as opposed to only cluster representatives (with similarity threshold 40 and 50%) by combining best practices from the literature for the dataset construction (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Section S1</xref> for a detailed description).</p>
      </sec>
      <sec>
        <label>3.2.2</label>
        <title>Effect of similarity threshold and redundant sequences</title>
        <p>In order to investigate the benefits of the proposed approach in comparison to algorithms relying on alignment features, we based our initial analysis on the custom EC40 and EC50 datasets. This approach represents a very controlled experimental setup, where one can investigate the effect of the chosen similarity threshold, the impact of redundant sequences during training and potential sources of data leakage during pre-training in a reliable way.</p>
        <p>We base our detailed analysis of the proposed method <italic>UDSMProt</italic> (compared with a baseline algorithm operating on PSSM features) on EC prediction tasks at Level 0 (enzyme versus non-enzyme), Level 1 (main enzyme class) and Level 2 (enzyme subclass). It is a well-known effect that the difficulty of the classification problem scales inversely with the similarity threshold, as a higher similarity threshold leads to sequences in the test set that are potentially more similar to those seen during training. In the extreme case of a random split, i.e. by disregarding cluster information, the test set performance merely reflects the algorithm’s capability to approximate the training set rather than the generalization performance when applied to unseen data. The failure to correctly incorporate the similarity threshold is one of the major pitfalls for newcomers in the field. Here, we perform Levels 0, 1 and 2 prediction on two different datasets, namely EC40 (40%) and EC50 (50% similarity cutoff). Both datasets only differ in the similarity thresholds and the version of the underlying Swiss-Prot databases.</p>
        <p>If not noted otherwise, CNN models are trained on representative sequences as this considerably reduces the computational burden for determining PSSM features and is in line with the literature (see e.g. <xref rid="btaa003-B8" ref-type="bibr">Dalkiran <italic>et al.</italic>, 2018</xref>; <xref rid="btaa003-B20" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>). In contrast, <italic>UDSMProt</italic> is conventionally trained using the full training set including redundant sequences, whereas the corresponding test and validation sets always contain only non-redundant sequences. For the EC50 dataset, non-redundant sequences enlarge the size of the training set from 45 to 114k and from 86 to 170k sequences for Levels 1/2 and 0, respectively. For EC40, the size is enlarged from 20 to 100k and from 46 to 150k for Level 1/2 and 0, respectively.</p>
        <p>In <xref rid="btaa003-T1" ref-type="table">Table 1</xref>, we compare the two classification algorithms <italic>UDSMProt</italic> and the baseline CNN that were introduced in Section 2 in terms of classification accuracy, which is the default metric considered in the literature for this task. There is a noticeable gap in performance across all experiments between CNN(seq; non-red.) and CNN(seq + PSSM; non-red.) which is a strong indication for the power of PSSM features. This gap can be reduced by the use of redundant sequences from training clusters (CNN(seq)) but still remains sizable. Most importantly, the gap can be closed by the use of language model pre-training. Disregarding the case of the EC40 dataset at Level 0, the best-performing <italic>UDSMProt</italic> outperforms the baseline algorithms that make use of PSSM features. Combining information from both forward and backward context consistently improves over models with unidirectional context. As another observation, pre-training leads to a consistent advantage compared with models trained from scratch that cannot be compensated by increasing the number of training epochs for the models trained from scratch.
</p>
        <table-wrap id="btaa003-T1" orientation="portrait" position="float">
          <label>Table 1.</label>
          <caption>
            <p>EC classification accuracy on the custom EC40 and EC50 datasets </p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="2" colspan="1"/>
                <th rowspan="2" align="left" colspan="1">Level</th>
                <th colspan="3" rowspan="1">EC40<hr/></th>
                <th colspan="3" rowspan="1">EC50<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">0</th>
                <th rowspan="1" colspan="1">1</th>
                <th rowspan="1" colspan="1">2</th>
                <th rowspan="1" colspan="1">0</th>
                <th rowspan="1" colspan="1">1</th>
                <th rowspan="1" colspan="1">2</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="4" colspan="1">Baseline</td>
                <td rowspan="1" colspan="1">Seq; non-red.</td>
                <td rowspan="1" colspan="1">0.83</td>
                <td rowspan="1" colspan="1">0.38</td>
                <td rowspan="1" colspan="1">0.25</td>
                <td rowspan="1" colspan="1">0.88</td>
                <td rowspan="1" colspan="1">0.71</td>
                <td rowspan="1" colspan="1">0.70</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Seq</td>
                <td rowspan="1" colspan="1">0.84</td>
                <td rowspan="1" colspan="1">0.61</td>
                <td rowspan="1" colspan="1">0.47</td>
                <td rowspan="1" colspan="1">0.92</td>
                <td rowspan="1" colspan="1">0.80</td>
                <td rowspan="1" colspan="1">0.79</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Seq+PSSM; non-red.; clean</td>
                <td rowspan="1" colspan="1">0.91</td>
                <td rowspan="1" colspan="1">0.84</td>
                <td rowspan="1" colspan="1">0.72</td>
                <td rowspan="1" colspan="1">0.95</td>
                <td rowspan="1" colspan="1">0.94</td>
                <td rowspan="1" colspan="1">0.91</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Seq+PSSM; non-red.; leak.</td>
                <td rowspan="1" colspan="1">
                  <bold>0.92</bold>
                </td>
                <td rowspan="1" colspan="1">0.85</td>
                <td rowspan="1" colspan="1">0.71</td>
                <td rowspan="1" colspan="1">0.95</td>
                <td rowspan="1" colspan="1">0.95</td>
                <td rowspan="1" colspan="1">0.92</td>
              </tr>
              <tr>
                <td rowspan="5" colspan="1">
                  <italic>UDSMProt</italic>
                </td>
                <td rowspan="1" colspan="1">Fwd; pretr.; non-red.</td>
                <td rowspan="1" colspan="1">0.82</td>
                <td rowspan="1" colspan="1">0.79</td>
                <td rowspan="1" colspan="1">0.71</td>
                <td rowspan="1" colspan="1">0.93</td>
                <td rowspan="1" colspan="1">0.94</td>
                <td rowspan="1" colspan="1">0.92</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fwd; from scratch</td>
                <td rowspan="1" colspan="1">0.87</td>
                <td rowspan="1" colspan="1">0.79</td>
                <td rowspan="1" colspan="1">0.74</td>
                <td rowspan="1" colspan="1">0.94</td>
                <td rowspan="1" colspan="1">0.94</td>
                <td rowspan="1" colspan="1">0.92</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fwd; pretr.</td>
                <td rowspan="1" colspan="1">0.89</td>
                <td rowspan="1" colspan="1">0.84</td>
                <td rowspan="1" colspan="1">0.83</td>
                <td rowspan="1" colspan="1">0.95</td>
                <td rowspan="1" colspan="1">0.96</td>
                <td rowspan="1" colspan="1">0.94</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Bwd; pretr.</td>
                <td rowspan="1" colspan="1">0.90</td>
                <td rowspan="1" colspan="1">0.85</td>
                <td rowspan="1" colspan="1">0.81</td>
                <td rowspan="1" colspan="1">0.95</td>
                <td rowspan="1" colspan="1">0.96</td>
                <td rowspan="1" colspan="1">0.94</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fwd+bwd; pretr.</td>
                <td rowspan="1" colspan="1">0.91</td>
                <td rowspan="1" colspan="1">
                  <bold>0.87</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.84</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.96</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.97</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.95</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic>Note</italic>: The best-performing classifiers are marked in bold face.</p>
            </fn>
            <fn id="tblfn2">
              <p>Fwd/bwd, training in forward/backward direction; seq, raw sequence as input; non-red, training on non-redundant sequences, i.e. representatives only; pretr., using language model pre-training; leak., leakage PSSM features computed on the full dataset.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p><xref rid="btaa003-T1" ref-type="table">Table 1</xref> illustrates that the <italic>UDSMProt</italic> classification models benefit from redundant training sequences for the downstream classification task, where the benefit is greater as the similarity threshold decreases. Comparing corresponding results from different similarity thresholds, i.e. results from EC40 to those from EC50, reveals the expected pattern, in the sense that lowering the similarity threshold complicates the classification task as test sequences show smaller overlap with sequences from the training set.</p>
        <p>Finally, we wanted to use this task to raise the awareness for the issue of data leakage that has—to the best of our knowledge—not received much attention in the literature. Our point of concern is the common practice of pre-computing features such as PSSMs (or pre-training) on the full dataset disregarding the train–test splits for the downstream classification tasks, which inevitably leads to a systematic over-estimation of the model’s generalization performance by implicitly leveraging information about the test set during the training phase. In an attempt to quantify the size of this effect, we compute two sets of PSSM features, one set computed based on the whole Swiss-Prot database [corresponding classification model: CNN(seq+PSSM; non-red.; leakage)] and a separate set based only on cluster members from the training data [corresponding classification model: CNN(seq+PSSM; non-red.; clean)]. It turns out that the model with PSSM features computed on a consistent train–test split always performs slightly worse than its counterpart that relies on PSSM features computed on the whole dataset. However, from a practical perspective, the effect of test data leakage remains small (see Supplementary Section S3 for a corresponding discussion in the context of LM pre-training). In Supplementary Section S6, we provide a more extensive evaluation of the effect by varying the size of the training database that is used for calculating PSSM features.</p>
        <p>To reiterate the main findings of the experiments carried out in this section, the most crucial observation is that language model pre-training is capable of closing the gap in performance between models operating on PSSM features compared with models operating on the sequences alone. The second main observation is that redundant sequences rather than cluster representatives only have a positive impact on the downstream classification training. The most obvious explanations for this observation are inhomogeneous clusters that contain samples with different labels that carry more fine-grained information than a single label per cluster representative.</p>
        <p>Finally, data leakage arising from inconsistent train–test splits between pre-training and classification is a possible source of systematic over-estimation of the model’s generalization performance and arises e.g. by pre-computing features (such as PSSM or Pfam features) on the full Swiss-Prot database without excluding downstream test clusters. From our experiments on PSSM features in the context of EC prediction, its effect was found to be small in particular for large pre-training datasets such as Swiss-Prot, but it should be kept in mind for future investigations.</p>
      </sec>
      <sec>
        <label>3.2.3</label>
        <title>Comparison to literature benchmarks</title>
        <p>In order to relate our proposed approach to state-of-the-art methods in literature, we conducted an experiment on two datasets provided by <italic>ECPred</italic> (<xref rid="btaa003-B8" ref-type="bibr">Dalkiran <italic>et al.</italic>, 2018</xref>) and <italic>DEEPre</italic> (<xref rid="btaa003-B20" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>). One of the purposes of this analysis is to justify our choice of the CNN baseline algorithm by demonstrating that it performs on par with state-of-the-art algorithms that do not make use of additional side-information, e.g. in the form of Pfam features. When comparing to literature results on the <italic>DEEPre</italic> dataset, we exclude models relying on Pfam features from our comparison. Leaving aside the very unfavorable scaling with the dataset size (<xref rid="btaa003-B3" ref-type="bibr">Bileschi <italic>et al.</italic>, 2019</xref>) and possible issues with data leakage due to features computed on the full dataset, methods relying on these features will fail when applied to proteins without functional annotations (see also the discussion in <xref rid="btaa003-B8" ref-type="bibr">Dalkiran <italic>et al.</italic>, 2018</xref>). In fact, a recent study estimated that at least one-third of microbial proteins cannot be annotated through alignments on given sequences (<xref rid="btaa003-B28" ref-type="bibr">Price <italic>et al.</italic>, 2018</xref>). Most notably, this excludes the most elaborate <italic>DEEPre</italic> (<xref rid="btaa003-B20" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>) model (with 0.96 Level 0, 0.95 Level 1 and 0.94 Level 2 accuracy on the <italic>DEEPre</italic> dataset) and <italic>EzyPred</italic> (<xref rid="btaa003-B35" ref-type="bibr">Shen and Chou, 2007</xref>; with 0.91 Level 0 and 0.90 Level 1 accuracy) from the comparison.</p>
        <p><xref rid="btaa003-T2" ref-type="table">Table 2</xref> shows the results of this experiment (see Supplementary Section S5 for details on the evaluation procedure). Note, that a convolutional model (as our baseline) seemed sufficient when compared with the hybrid model of <italic>DEEPre</italic> [using convolutional layers followed by a recurrent layer (LSTM)] as can been seen in <xref rid="btaa003-T2" ref-type="table">Table 2</xref> where our baseline even surpassed the reported performances (91 versus 88% for Level 0 and 84 versus 82% for Level 1). Also for testing on <italic>ECPred</italic>, our baseline approach yielded competitive results indicating a well-chosen baseline model. These results justify a posteriori our design choices for the CNN baseline model.
</p>
        <table-wrap id="btaa003-T2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <p>EC classification accuracy on the published <italic>DEEPre</italic> and <italic>ECPred</italic> datasets compared with literature results from <italic>DEEPre</italic> (<xref rid="btaa003-B20" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>) and <italic>ECPred</italic> (<xref rid="btaa003-B8" ref-type="bibr">Dalkiran <italic>et al.</italic>, 2018</xref>) disregarding models relying on Pfam features</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="2" colspan="1"/>
                <th rowspan="2" align="left" colspan="1">Level</th>
                <th colspan="3" rowspan="1">DEEPre (acc.)<hr/></th>
                <th colspan="2" rowspan="1">ECPred (mean F<sub>1</sub>)<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">0</th>
                <th rowspan="1" colspan="1">1</th>
                <th rowspan="1" colspan="1">2</th>
                <th rowspan="1" colspan="1">0</th>
                <th rowspan="1" colspan="1">1</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">
                  <italic>ECPred</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td rowspan="1" colspan="1">0.96</td>
                <td rowspan="1" colspan="1">
                  <bold>0.96</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"><italic>DEEPre</italic> (seq+PSSM)</td>
                <td rowspan="1" colspan="1">0.88</td>
                <td rowspan="1" colspan="1">0.82</td>
                <td rowspan="1" colspan="1">0.43</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Baseline<xref ref-type="table-fn" rid="tblfn5"><sup>a</sup></xref> (seq+PSSM)</td>
                <td rowspan="1" colspan="1">
                  <bold>0.91</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.84</bold>
                </td>
                <td rowspan="1" colspan="1">0.59</td>
                <td rowspan="1" colspan="1">0.97</td>
                <td rowspan="1" colspan="1">0.94</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>UDSMProt</italic>
                  <xref ref-type="table-fn" rid="tblfn5">
                    <sup>a</sup>
                  </xref>
                </td>
                <td rowspan="1" colspan="1">Fwd; pretr.</td>
                <td rowspan="1" colspan="1">0.86</td>
                <td rowspan="1" colspan="1">0.81</td>
                <td rowspan="1" colspan="1">0.75</td>
                <td rowspan="1" colspan="1">0.95</td>
                <td rowspan="1" colspan="1">0.93</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Bwd; pretr.</td>
                <td rowspan="1" colspan="1">0.86</td>
                <td rowspan="1" colspan="1">0.83</td>
                <td rowspan="1" colspan="1">0.73</td>
                <td rowspan="1" colspan="1">0.97</td>
                <td rowspan="1" colspan="1">0.93</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Fwd+bwd; pretr.</td>
                <td rowspan="1" colspan="1">0.87</td>
                <td rowspan="1" colspan="1">
                  <bold>0.84</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.78</bold>
                </td>
                <td rowspan="1" colspan="1">0.97</td>
                <td rowspan="1" colspan="1">0.94</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Fwd; pretr.; red.</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td rowspan="1" colspan="1">0.97</td>
                <td rowspan="1" colspan="1">0.95</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Bwd; pretr.; red.</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td rowspan="1" colspan="1">0.97</td>
                <td rowspan="1" colspan="1">0.95</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">Fwd+bwd; pretr.; red.</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td rowspan="1" colspan="1">
                  <bold>0.98</bold>
                </td>
                <td rowspan="1" colspan="1">0.95</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn3">
              <p><italic>Note</italic>: Results on the <italic>DEEPre</italic> dataset were evaluated using 5-fold cross-validation.</p>
            </fn>
            <fn id="tblfn4">
              <p>Fwd/bwd, training in forward/backward direction; seq, raw sequence as input; pretr., using language model pre-training.</p>
            </fn>
            <fn id="tblfn5">
              <label>a</label>
              <p>Results established in this work.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>Turning to the performance of the proposed <italic>UDSMProt</italic>, we find a solid prediction performance reaching state-of-the-art performance reported in the literature for algorithms operating on PSSM features. Considering the results of the previous section, the results on the <italic>DEEPre</italic> dataset represent only the lower bound for the achievable performance as it profits considerably from redundant training sequences, which could, however, not be reconstructed from the given representatives without the underlying cluster assignments. Considering the sizable performance gaps between training on redundant and non-redundant datasets in <xref rid="btaa003-T1" ref-type="table">Table 1</xref>, it is even more remarkable that <italic>UDSMProt</italic> already reaches state-of-the-art performance when trained on non-redundant sequences. A notable observation is that our approach outperforms <italic>DEEPre</italic> by a large margin for Level 2 when excluding Pfam features. For <italic>ECPred</italic> we report both the performance for training on the original training set as well as the performance on a redundant training set comprising all corresponding Uniref50 cluster members as shown in the three bottom rows in <xref rid="btaa003-T2" ref-type="table">Table 2</xref>. In terms of Level 0 performance, the proposed approach outperforms <italic>ECPred</italic> and it shows competitive performance at Level 1.</p>
        <p>To summarize, our baseline model reaches state-of-the-art performance compared with literature approaches disregarding those that incorporate features from functional annotations (such as Pfam) and can therefore be used as proxy for state-of-the-art algorithms in the following investigations. This finding enhances a posteriori also the significance of the results established for the EC40 and EC50 datasets in Section 3.2.2. The proposed <italic>UDSMProt</italic> model is very competitive on both literature datasets.</p>
      </sec>
      <sec>
        <label>3.2.4</label>
        <title>Impact of dataset size</title>
        <p>In this section, we aim to demonstrate the particular advantages of the proposed <italic>UDSMProt</italic>-approach in the regime of small dataset sizes. To investigate this effect in a clean experimental setup, we conducted an experiment with consecutively decreasing training set sizes, while keeping test and validation sets fixed. The hyperparameters were kept fixed to those of the run with full training data.</p>
        <p>For this experiment, we used the EC50 dataset as described in Supplementary Section S1 with numbers per class as shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref> and trained a Level 1 classifier for each split. We compared our proposed approach (<italic>UDSMProt</italic> with pre-training and trained from scratch) with baseline models (CNN with PSSM features and CNN on redundant sequences only) for seven different training set sizes measured in terms of clusters compared with the number of clusters in the original training set.</p>
        <p>The results from <xref ref-type="fig" rid="btaa003-F2">Figure 2</xref> show an interesting pattern: the bidirectional <italic>UDSMProt</italic> model always outperforms the CNN baseline model and, most interestingly, the gap between the two models increases for small dataset sizes, which suggests the representations learned during language model fine-tuning represent a more effective baseline for fine-tuning than using PSSMs as fixed input features. As a second observation, also the gap to the models trained from scratch widens. Reducing the number of training clusters by 50% only leads to a decrease in model performance by 3%, whereas the performance of the model trained from scratch drops by 8%.
</p>
        <fig id="btaa003-F2" orientation="portrait" position="float">
          <label>Fig. 2.</label>
          <caption>
            <p>Dependence of the EC classification accuracy (Level 1; EC50 dataset) on the size of the training dataset. <italic>UDSMProt</italic> performs better than the baseline model also in the regime of small datasets that is particularly important for practical applications</p>
          </caption>
          <graphic xlink:href="btaa003f2"/>
        </fig>
        <p>To summarize, both observations represent strong arguments for applying <italic>UDSMProt</italic> in particular to small datasets. Our results suggest to make language model pre-training a standard procedure in these cases.</p>
      </sec>
    </sec>
    <sec>
      <title>3.3 GO prediction</title>
      <p>To stress the universality of the approach, we now present results for GO prediction, a functional multi-label classification task.</p>
      <sec>
        <label>3.3.1</label>
        <title>Task and dataset</title>
        <p>A more general although closely related problem to enzyme prediction is GO prediction. GO is an international bioinformatics initiative to unify a part of the vocabulary for the representation of proteins attributes. It covers three domains, namely cellular components (CCOs), molecular functions (MFOs) and biological processes (BPOs). The nomenclature is organized into hierarchies ranging from coarse to fine-grained attributes. Similar to enzyme class prediction, the first proposed approaches in this field relied on handcrafted features like functionally discriminating residues with PSSM (<xref rid="btaa003-B11" ref-type="bibr">Gong <italic>et al.</italic>, 2016</xref>) and classification models consisting of an array of support vector machines (<xref rid="btaa003-B7" ref-type="bibr">Cozzetto <italic>et al.</italic>, 2016</xref>). State-of-the-art methods based on neural networks are <italic>DeepGO</italic> (<xref rid="btaa003-B17" ref-type="bibr">Kulmanov <italic>et al.</italic>, 2018</xref>) and <italic>DeepGOPlus</italic> (<xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf, 2019</xref>), where the latter also leverages <italic>Diamond</italic> (BLAST; <xref rid="btaa003-B4" ref-type="bibr">Buchfink <italic>et al.</italic>, 2015</xref>) in an ensemble model. The best-performing method from the CAFA3 challenge (<xref rid="btaa003-B48" ref-type="bibr">Zhou <italic>et al.</italic>, 2019</xref>), <italic>GoLabeler</italic> (<xref rid="btaa003-B47" ref-type="bibr">You <italic>et al.</italic>, 2018b</xref>), is an ensemble method that was recently outperformed by <italic>DeepText2GO</italic> (<xref rid="btaa003-B46" ref-type="bibr">You <italic>et al.</italic>, 2018a</xref>), an ensemble method based on knowledge extraction.</p>
        <p>To allow for a direct comparison with the most recent state-of-the-art methods, we work with a dataset constructed using a time-based split (<xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf, 2019</xref>; <xref rid="btaa003-B47" ref-type="bibr">You <italic>et al.</italic>, 2018b</xref>), where the Swiss-Prot annotations before January 2016 are used as training set and those between January 2016 and October 2016 serve as test set (The underlying raw data are available from the data repository accompanying; <xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf, 2019</xref>.). The models are evaluated in each of the three categories MFO, BPO and CCO using the two main metrics of the CAFA3 challenge (<xref rid="btaa003-B48" ref-type="bibr">Zhou <italic>et al.</italic>, 2019</xref>), namely a protein-centric maximum <italic>F</italic>-measure, <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which quantifies the semantic distance between predicted and ground truth annotations (see <xref rid="btaa003-B6" ref-type="bibr">Clark and Radivojac, 2013</xref>, for details). These metrics are complemented by the area under the precision-recall-curve that was also reported by <xref rid="btaa003-B46" ref-type="bibr">You <italic>et al.</italic> (2018a</xref>,<xref rid="btaa003-B47" ref-type="bibr">b</xref>) and <xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf (2019)</xref>.</p>
      </sec>
      <sec>
        <label>3.3.2</label>
        <title>Experimental setup and results</title>
        <p>As the distribution of GO-labels is very long-tailed, we follow the approach by <xref rid="btaa003-B17" ref-type="bibr">Kulmanov <italic>et al.</italic> (2018)</xref> and <xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf (2019)</xref> and predict only GO-terms that occurred at least 50 times in the training dataset resulting in 5101 unique GO-labels across all three GO categories. As only modification compared with the EC prediction task, we enlarge the size of the hidden layer of the classifier to 1024. We train a single model for all three GO categories optimizing a binary crossentropy loss in this case, since we are dealing with a multi-label classification task. Similarly to (<xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf, 2019</xref>), due to the large size of output layer, we do not explicitly take into account the ontological nature of GO but use a flat output layer of dimension 5101. To illustrate the prospects of ensembling different classifiers, we also report results for ensembling our pre-trained (forward and backward) model with BLAST results from <italic>DiamondScore</italic> using the same relative weighting used by <xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf (2019)</xref>.</p>
        <p>The results in <xref rid="btaa003-T3" ref-type="table">Table 3</xref> demonstrate the strong performance of <italic>UDSMProt</italic> also in the domain of GO prediction. In particular, the forward–backward model outperforms state-of-the-art methods based on neural networks in terms of <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for all three GO categories and even reaches a new state-of-the-art result across all considered single-model approaches for <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> in the CCO category as well for all three categories in terms of area under the precision-recall curve (AUPR). Combining its predictions with BLAST-KNN features from <italic>DiamondScore</italic> following <xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf (2019)</xref> leads to very competitive results compared with state-of-the-art ensemble methods in terms of <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and AUPR even establishing new state-of-the-art results for both metrics in the BPO category. At this point, we would like to stress that the results presented here were obtained without any hyperparameter tuning, using the exact same parameters as for EC prediction apart from changing the dimensionality of the hidden layer, which is in strong contrast to most literature approaches. This suggests that further task-specific hyperparameter tuning might still further enhance the overall performance. The results presented in this section substantiate our claims regarding the universality of transferring implicit knowledge to task-specific requirements.
</p>
        <table-wrap id="btaa003-T3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <p>GO prediction performance on a dataset based on a time-based split as in (<xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf, 2019</xref>; <xref rid="btaa003-B47" ref-type="bibr">You <italic>et al.</italic>, 2018b</xref>) in comparison to literature results collected by <italic>DeepGOPlus</italic> (<xref rid="btaa003-B16" ref-type="bibr">Kulmanov and Hoehndorf, 2019</xref>)</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="2" colspan="1"/>
                <th rowspan="2" align="left" colspan="1">Methods</th>
                <th colspan="3" rowspan="1">
                  <inline-formula id="IE12">
                    <mml:math id="IM12">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>F</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mtext>max</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                  <hr/>
                </th>
                <th colspan="3" rowspan="1">
                  <inline-formula id="IE13">
                    <mml:math id="IM13">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>S</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mtext>min</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                  <hr/>
                </th>
                <th colspan="3" rowspan="1">AUPR<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">MFO</th>
                <th rowspan="1" colspan="1">BPO</th>
                <th rowspan="1" colspan="1">CCO</th>
                <th rowspan="1" colspan="1">MFO</th>
                <th rowspan="1" colspan="1">BPO</th>
                <th rowspan="1" colspan="1">CCO</th>
                <th rowspan="1" colspan="1">MFO</th>
                <th rowspan="1" colspan="1">BPO</th>
                <th rowspan="1" colspan="1">CCO</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="4" colspan="1">Single</td>
                <td rowspan="1" colspan="1">
                  <italic>Naive</italic>
                </td>
                <td rowspan="1" colspan="1">0.306</td>
                <td rowspan="1" colspan="1">0.318</td>
                <td rowspan="1" colspan="1">0.605</td>
                <td rowspan="1" colspan="1">12.105</td>
                <td rowspan="1" colspan="1">38.890</td>
                <td rowspan="1" colspan="1">9.646</td>
                <td rowspan="1" colspan="1">0.150</td>
                <td rowspan="1" colspan="1">0.219</td>
                <td rowspan="1" colspan="1">0.512</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>DiamondScore</italic>
                </td>
                <td rowspan="1" colspan="1">
                  <underline>0.548</underline>
                </td>
                <td rowspan="1" colspan="1">
                  <underline>0.439</underline>
                </td>
                <td rowspan="1" colspan="1">0.621</td>
                <td rowspan="1" colspan="1">
                  <underline>8.736</underline>
                </td>
                <td rowspan="1" colspan="1">
                  <underline>34.060</underline>
                </td>
                <td rowspan="1" colspan="1">7.997</td>
                <td rowspan="1" colspan="1">0.362</td>
                <td rowspan="1" colspan="1">0.240</td>
                <td rowspan="1" colspan="1">0.363</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>DeepGO</italic>
                </td>
                <td rowspan="1" colspan="1">0.449</td>
                <td rowspan="1" colspan="1">0.398</td>
                <td rowspan="1" colspan="1">0.667</td>
                <td rowspan="1" colspan="1">10.722</td>
                <td rowspan="1" colspan="1">35.085</td>
                <td rowspan="1" colspan="1">
                  <underline>7.861</underline>
                </td>
                <td rowspan="1" colspan="1">0.409</td>
                <td rowspan="1" colspan="1">0.328</td>
                <td rowspan="1" colspan="1">0.696</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>DeepGOCNN</italic>
                </td>
                <td rowspan="1" colspan="1">0.409</td>
                <td rowspan="1" colspan="1">0.383</td>
                <td rowspan="1" colspan="1">0.663</td>
                <td rowspan="1" colspan="1">11.296</td>
                <td rowspan="1" colspan="1">36.451</td>
                <td rowspan="1" colspan="1">8.642</td>
                <td rowspan="1" colspan="1">0.350</td>
                <td rowspan="1" colspan="1">0.316</td>
                <td rowspan="1" colspan="1">0.688</td>
              </tr>
              <tr>
                <td rowspan="3" colspan="1">Ensemble</td>
                <td rowspan="1" colspan="1">
                  <italic>DeepText2GO</italic>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.627</bold>
                </td>
                <td rowspan="1" colspan="1">0.441</td>
                <td rowspan="1" colspan="1">0.694</td>
                <td rowspan="1" colspan="1">5.240</td>
                <td rowspan="1" colspan="1">17.713</td>
                <td rowspan="1" colspan="1">
                  <bold>4.531</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.605</bold>
                </td>
                <td rowspan="1" colspan="1">0.336</td>
                <td rowspan="1" colspan="1">
                  <bold>0.729</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>GOLabeler</italic>
                </td>
                <td rowspan="1" colspan="1">0.580</td>
                <td rowspan="1" colspan="1">0.370</td>
                <td rowspan="1" colspan="1">0.687</td>
                <td rowspan="1" colspan="1">
                  <bold>5.077</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>15.177</bold>
                </td>
                <td rowspan="1" colspan="1">5.518</td>
                <td rowspan="1" colspan="1">0.546</td>
                <td rowspan="1" colspan="1">0.225</td>
                <td rowspan="1" colspan="1">0.700</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>DeepGOPlus</italic>
                </td>
                <td rowspan="1" colspan="1">0.585</td>
                <td rowspan="1" colspan="1">0.474</td>
                <td rowspan="1" colspan="1">
                  <bold>0.699</bold>
                </td>
                <td rowspan="1" colspan="1">8.824</td>
                <td rowspan="1" colspan="1">33.576</td>
                <td rowspan="1" colspan="1">7.693</td>
                <td rowspan="1" colspan="1">0.536</td>
                <td rowspan="1" colspan="1">0.407</td>
                <td rowspan="1" colspan="1">0.726</td>
              </tr>
              <tr>
                <td rowspan="5" colspan="1">
                  <italic>UDSMProt</italic>
                  <xref ref-type="table-fn" rid="tblfn8">
                    <sup>a</sup>
                  </xref>
                </td>
                <td rowspan="1" colspan="1">Fwd; from scratch</td>
                <td rowspan="1" colspan="1">0.418</td>
                <td rowspan="1" colspan="1">0.303</td>
                <td rowspan="1" colspan="1">0.655</td>
                <td rowspan="1" colspan="1">14.906</td>
                <td rowspan="1" colspan="1">47.208</td>
                <td rowspan="1" colspan="1">12.929</td>
                <td rowspan="1" colspan="1">0.304</td>
                <td rowspan="1" colspan="1">0.284</td>
                <td rowspan="1" colspan="1">0.612</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fwd; pretr.</td>
                <td rowspan="1" colspan="1">0.465</td>
                <td rowspan="1" colspan="1">0.404</td>
                <td rowspan="1" colspan="1">0.683</td>
                <td rowspan="1" colspan="1">10.578</td>
                <td rowspan="1" colspan="1">36.667</td>
                <td rowspan="1" colspan="1">8.210</td>
                <td rowspan="1" colspan="1">0.406</td>
                <td rowspan="1" colspan="1">0.345</td>
                <td rowspan="1" colspan="1">0.695</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Bwd; pretr.</td>
                <td rowspan="1" colspan="1">0.465</td>
                <td rowspan="1" colspan="1">0.403</td>
                <td rowspan="1" colspan="1">0.664</td>
                <td rowspan="1" colspan="1">10.802</td>
                <td rowspan="1" colspan="1">36.361</td>
                <td rowspan="1" colspan="1">8.210</td>
                <td rowspan="1" colspan="1">0.414</td>
                <td rowspan="1" colspan="1">0.348</td>
                <td rowspan="1" colspan="1">0.685</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fwd+bwd; pretr.</td>
                <td rowspan="1" colspan="1">0.481</td>
                <td rowspan="1" colspan="1">0.411</td>
                <td rowspan="1" colspan="1">
                  <underline>0.682</underline>
                </td>
                <td rowspan="1" colspan="1">10.505</td>
                <td rowspan="1" colspan="1">36.147</td>
                <td rowspan="1" colspan="1">8.244</td>
                <td rowspan="1" colspan="1">
                  <underline>0.472</underline>
                </td>
                <td rowspan="1" colspan="1">
                  <underline>0.356</underline>
                </td>
                <td rowspan="1" colspan="1">
                  <underline>0.704</underline>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Bwd+bwd; pretr. + <italic>DiamondScore</italic></td>
                <td rowspan="1" colspan="1">0.582</td>
                <td rowspan="1" colspan="1">
                  <bold>0.475</bold>
                </td>
                <td rowspan="1" colspan="1">0.697</td>
                <td rowspan="1" colspan="1">8.787</td>
                <td rowspan="1" colspan="1">33.615</td>
                <td rowspan="1" colspan="1">7.618</td>
                <td rowspan="1" colspan="1">0.548</td>
                <td rowspan="1" colspan="1">
                  <bold>0.422</bold>
                </td>
                <td rowspan="1" colspan="1">0.728</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn6">
              <p><italic>Note</italic>: Best overall results (highest <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and AUPR; lowest <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>) are marked in bold face and best single-model results are underlined.</p>
            </fn>
            <fn id="tblfn7">
              <p>Fwd/bwd, training in forward/backward direction; pretr., using language model pre-training.</p>
            </fn>
            <fn id="tblfn8">
              <label>a</label>
              <p>Results established in this work.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>On the architectural side, it remains to explore in detail if explicitly incorporating the label hierarchy represents an advantage, which one might suspect from <italic>DeepGO</italic> outperforming its successor <italic>DeepGOCNN</italic>. The rich literature on this subject is reviewed in Silla and Freitas (2010; see also <xref rid="btaa003-B43" ref-type="bibr">Wehrmann <italic>et al.</italic>, 2018</xref>, for a recent deep learning perspective on this subject). Possible solutions range from combining different classifiers via appropriate post-processing procedures, consistency-enforcing loss terms, custom output layers or hierarchical multi-label classification methods (<xref rid="btaa003-B26" ref-type="bibr">Nakano <italic>et al.</italic>, 2019</xref>; <xref rid="btaa003-B42" ref-type="bibr">Vens <italic>et al.</italic>, 2008</xref>). A more detailed analysis is beyond the scope of this article.</p>
      </sec>
    </sec>
    <sec>
      <title>3.4 Remote homology and fold detection</title>
      <p>As third demonstration of the universality of our approach, we consider remote homology detection tasks. The corresponding datasets consist of a few hundred training examples and are thus situated clearly in the small dataset regime investigated in Section 3.2. This substantiates the claims made in Section 3.2 in a real-world setting.</p>
      <sec>
        <label>3.4.1</label>
        <title>Task and datasets</title>
        <p>Remote homology detection is one of the key problems in computational biology and refers to the classification of proteins into structural and functional classes, which is considered to be a key step for further functional and structural classification tasks. Here, we consider remote homology detection in terms of the SCOP database (<xref rid="btaa003-B25" ref-type="bibr">Murzin <italic>et al.</italic>, 1995</xref>), where all proteins are organized in four levels: class, fold, superfamily and family. Proteins in the same superfamily are homologous and proteins in the same superfamily but in different families are considered to be remotely homologous. Remote homology detection has a rich history and the interested reader is referred to a review article on this topic by <xref rid="btaa003-B5" ref-type="bibr">Chen <italic>et al.</italic> (2018)</xref>. We will compare to <italic>ProDec-BLSTM</italic> (<xref rid="btaa003-B19" ref-type="bibr">Li <italic>et al.</italic>, 2017</xref>) with a bidirectional RNN operating on PSSM input features building on earlier work (<xref rid="btaa003-B13" ref-type="bibr">Hochreiter <italic>et al.</italic>, 2007</xref>). A classical baseline method is provided by <italic>GPkernel</italic> (<xref rid="btaa003-B12" ref-type="bibr">Håndstad <italic>et al.</italic>, 2007</xref>), who apply kernel-methods to sequence motifs.</p>
        <p>For remote homology detection, we make use of the SCOP 1.67 dataset as prepared by <xref rid="btaa003-B13" ref-type="bibr">Hochreiter <italic>et al.</italic> (2007</xref>), which has become a standard benchmark dataset in the field. Here, the problem is framed as a binary classification problem where one has to decide if a given protein is contained in the same superfamily or fold as a given reference protein. The superfamily/fold benchmark is composed of 102/85 separate datasets and we report the mean performance of all models across the whole set. The standard metrics considered in this context are AUC and AUC50, where the latter corresponds to the (normalized) partial area under the ROC curve integrated up to the first 50 false positives, which allows for a better characterization of the classifier in the domain of small false positive rates, which is most relevant for practical applications, than the overall discriminative power of the classifier as quantified by AUC.</p>
      </sec>
      <sec>
        <label>3.4.2</label>
        <title>Experimental setup and results</title>
        <p>The remote homology and fold detection tasks are challenging for two reasons. The datasets are rather small and the task comprises 102 or respectively 85 different datasets that would in principle require a separate set of hyperparameters. To keep the process as simple as possible, we decided to keep a global set of hyperparameters for all datasets of a given task. The procedure is as follows as no validation is provided for the original datasets, we split the training data into a training and a validation set based on CD-HIT clusters (threshold 0.5). We optimize hyperparameters using the mean AUC for all datasets of a given task measured on the validation set. Most importantly, this involves fixing a (in this case constant) learning rate that is appropriate across all datasets. Using these hyperparameter settings, we perform model selection based on the validation set AUC, i.e. for each individual dataset, we select the model at the epoch with the highest validation AUC. We evaluate the test set AUC for these models and report the mean test set metrics.</p>
        <p>The results of these experiments are shown in <xref rid="btaa003-T4" ref-type="table">Table 4</xref>. Both for homology and fold detection according to most metrics, the <italic>UDSMProt</italic> model trained from scratch performs worse than the original LSTM model (<xref rid="btaa003-B13" ref-type="bibr">Hochreiter <italic>et al.</italic>, 2007</xref>). This is most likely due to the fact that the <italic>UDSMProt</italic> model is considerably larger than the latter model and most datasets are fairly small with a few hundreds training examples per dataset. This deficiency is overcome with the use of language model pre-training, where both unidirectional models perform better than the LSTM baseline model. This observation is in line with the experiments in Section 3.2.4 that demonstrates the particular effectiveness of the proposed approach for small datasets. The best-performing model from the literature, <italic>ProDec-BLSTM</italic>, is a bidirectional LSTM operating on sequence as well as PSSM features. Interestingly, reaching its performance in terms of overall AUC required the inclusion of bidirectional context, i.e. the forward–backward ensemble model. The proposed method also clearly outperforms classical methods such as <italic>GPkernel</italic> (<xref rid="btaa003-B12" ref-type="bibr">Håndstad <italic>et al.</italic>, 2007</xref>) both on the fold and the superfamily level. The excellent results on remote homology and fold detection support our claims on the universality of the approach as well as the particular advantages in the regime of small dataset sizes.
</p>
        <table-wrap id="btaa003-T4" orientation="portrait" position="float">
          <label>Table 4.</label>
          <caption>
            <p>Remote homology and fold detection performance on the SCOP 1.67 benchmark dataset compared with literature results from <italic>GPkernel</italic> (<xref rid="btaa003-B12" ref-type="bibr">Håndstad <italic>et al.</italic>, 2007</xref>), <italic>LSTM_protein</italic> (<xref rid="btaa003-B13" ref-type="bibr">Hochreiter <italic>et al.</italic>, 2007</xref>) and <italic>ProDec-BLSTM</italic> (<xref rid="btaa003-B19" ref-type="bibr">Li <italic>et al.</italic>, 2017</xref>) </p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="2" colspan="1"/>
                <th rowspan="2" align="left" colspan="1">Methods</th>
                <th colspan="2" rowspan="1">Superfamily level<hr/></th>
                <th colspan="2" rowspan="1">Fold level<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">
                  <inline-formula id="IE16">
                    <mml:math id="IM16">
                      <mml:mrow>
                        <mml:mtext>AUC</mml:mtext>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                </th>
                <th rowspan="1" colspan="1">
                  <inline-formula id="IE17">
                    <mml:math id="IM17">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mrow>
                              <mml:mtext>AUC</mml:mtext>
                            </mml:mrow>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>50</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                </th>
                <th rowspan="1" colspan="1">
                  <inline-formula id="IE18">
                    <mml:math id="IM18">
                      <mml:mrow>
                        <mml:mtext>AUC</mml:mtext>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                </th>
                <th rowspan="1" colspan="1">
                  <inline-formula id="IE19">
                    <mml:math id="IM19">
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mrow>
                              <mml:mtext>AUC</mml:mtext>
                            </mml:mrow>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>50</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:math>
                  </inline-formula>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="7" colspan="1">
                  <italic>UDSMProt</italic>
                  <xref ref-type="table-fn" rid="tblfn10">
                    <sup>a</sup>
                  </xref>
                </td>
                <td rowspan="1" colspan="1">
                  <italic>GPkernel</italic>
                </td>
                <td rowspan="1" colspan="1">0.902</td>
                <td rowspan="1" colspan="1">0.591</td>
                <td rowspan="1" colspan="1">0.844</td>
                <td rowspan="1" colspan="1">0.514</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>LSTM_protein</italic>
                </td>
                <td rowspan="1" colspan="1">0.942</td>
                <td rowspan="1" colspan="1">0.773</td>
                <td rowspan="1" colspan="1">0.821</td>
                <td rowspan="1" colspan="1">0.571</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <italic>ProDec-BLSTM</italic>
                </td>
                <td rowspan="1" colspan="1">0.969</td>
                <td rowspan="1" colspan="1">0.849</td>
                <td align="left" rowspan="1" colspan="1">—</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fwd; from scratch</td>
                <td rowspan="1" colspan="1">0.706</td>
                <td rowspan="1" colspan="1">0.552</td>
                <td rowspan="1" colspan="1">0.734</td>
                <td rowspan="1" colspan="1">0.653</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fwd; pretr.</td>
                <td rowspan="1" colspan="1">0.957</td>
                <td rowspan="1" colspan="1">0.880</td>
                <td rowspan="1" colspan="1">0.834</td>
                <td rowspan="1" colspan="1">0.734</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Bwd; pretr.</td>
                <td rowspan="1" colspan="1">0.969</td>
                <td rowspan="1" colspan="1">0.912</td>
                <td rowspan="1" colspan="1">0.839</td>
                <td rowspan="1" colspan="1">0.757</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Fwd+bwd; pretr.</td>
                <td rowspan="1" colspan="1">
                  <bold>0.972</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.914</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.862</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.776</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn9">
              <p>Fwd/bwd, training in forward/backward direction; pretr., using language model pre-training. The best-performing classifiers are marked in bold face.</p>
            </fn>
            <fn id="tblfn10">
              <label>a</label>
              <p>Results established in this work.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Case studies with insights from explainable ML</title>
    <p>Even though deep neural networks are still often perceived as black-box models, there has been a paradigm shift in the past few years due to the advances in the field of explainable ML. In this section we outline possible applications of interpretability methods to gain deeper insights into the models and the structure of proteins itself (see also Upmeier zu Belzen <italic>et al.</italic>, 2019, for first applications in this direction). Here we focus on <italic>post</italic> <italic>hoc</italic> interpretability methods that return attribution heatmaps in the input space, i.e. the protein sequence itself, that relate to its impact on the classification decision. Attribution methods such as integrated gradients (<xref rid="btaa003-B39" ref-type="bibr">Sundararajan <italic>et al.</italic>, 2017</xref>) relate to parts of the input sequence that influenced the classifier toward/against a certain classification decision. These methods are therefore particularly suited to identify sequence motifs. The representative example in <xref ref-type="fig" rid="btaa003-F3">Figure 3</xref> shows this for the case of EC classification, where the classifier identifies a short sequence motif, known as the ‘DEAH’ box, as indicative for EC Class 3. Similarly, the classifier is strongly influenced toward EC Class 6 by regions surrounding the ‘HIGH’ and ‘KMKS’ motifs (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S4</xref>). A statistical analysis of these findings is beyond the scope of this manuscript but represents an interesting direction for future research.
</p>
    <fig id="btaa003-F3" orientation="portrait" position="float">
      <label>Fig. 3.</label>
      <caption>
        <p>Attribution map for the class EC3 for UniProt accession Q60452 based on integrated gradients. The heatmap shows high relevance on the ‘DEAH’ box (DEAH; Pos. 234–237)</p>
      </caption>
      <graphic xlink:href="btaa003f3"/>
    </fig>
    <p>Secondly, we revisit the glutaminase example put forward by <italic>DEEPre</italic> (<xref rid="btaa003-B20" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>). The first and the third isoforms of the amino acid sequence with the UniProt accession number O94925 are enzymes, whereas the second isoform shows no enzymatic activity. This behavior is correctly captured by a model trained on EC Level 1 and no-enzyme data. The attribution map for the first isoform in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S5</xref> highlights regions beyond position 170 that are not contained in the sequence of the second isoform. The attribution map based on integrated gradients for the second isoform (see <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S6</xref>), already shows high relevance toward the end of the sequence, where the sequence deviates from the canonical sequences. This is even more clearly visible in an occlusion-based attribution map (<xref rid="btaa003-B18" ref-type="bibr">Li <italic>et al.</italic>, 2016</xref>; see <xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S7</xref>), which shows the impact of sequentially exchanging amino acids by the unknown amino acid X and the corresponding change in the non-enzyme class prediction score. In any case, correlating known sequence properties from UniProt with interpretability methods represents a very promising direction for future work to gain deeper insights on the one hand into protein classification models and on the other hand into protein substructures.</p>
  </sec>
  <sec>
    <title>5 Summary and outlook</title>
    <p>In this work, we investigated the prospects of self-supervised pre-training for protein classification tasks leveraging the recent advances in NLP in this direction. Protein classification represents an ideal test bed for NLP methods. Most importantly, a single, universal model architecture with no task-specific modifications apart from a fine-tuning step that operates on the sequence of amino acids alone is able to reach or even exceed state-of-the-art performance on a number of protein classification tasks. This is achieved by powerful, implicitly learned representations from self-supervised pre-training, whereas most state-of-the-art algorithms make use of PSSM features obtained from BLAST database searches that scale unfavorably with dataset size. In addition, the proposed method shows particular advantages for small datasets. Differently from typical NLP tasks, the dataset creation and the evaluation procedure has to be carried out with particular care, as small differences in the procedure can have large impact on the difficulty of the classification problem and hence on the comparability of different approaches. This applies in particular to a well-defined way of handling the similarity threshold, i.e. dealing with homologous sequences that differ only by a few amino acids when splitting into train and test sets. These factors urge for the creation of appropriate benchmark datasets that convert raw data from an exemplary subset of the many existing protein classification tasks into benchmark datasets in a transparent manner that allow for a rigorous testing of ML algorithms in this setting.</p>
    <p>Given the insights gained from the three classification tasks, we can draw the following general conclusions for generic protein classification tasks:</p>
    <list list-type="order">
      <list-item>
        <p>Considering the fact that <italic>UDSMProt</italic> was able to reach or surpass state-of-the-art performance suggests that problem-specific architectures are less important than the training procedure, at least for models that are powerful enough. This allows to design task-independent, universal classification algorithms that can be applied without much manual intervention to unseen classification tasks.</p>
      </list-item>
      <list-item>
        <p>Redundant sequences are a valuable source of information also for downstream classification tasks. This fact is in tension with the standard practice in bioinformatics, where in many cases only representatives without the corresponding cluster assignments are presented. To ensure comparability, benchmarking datasets should always include full information to reproduce the cluster assignments used during dataset creation, which would allow at least retrospectively to reconstruct the complete dataset from a given set of representatives.</p>
      </list-item>
      <list-item>
        <p>Bidirectional context is important, which is reflected by the fact that in all cases forward-backward-ensembles reached the best performance and in most cases improved the performance of unidirectional models considerably. Ensembling forward and backward models is in fact the simplest—although at the same time a quite inefficient—way of capturing bidirectional context. From our perspective, this represents an opportunity for approaches such as <italic>BERT</italic> (<xref rid="btaa003-B9" ref-type="bibr">Devlin <italic>et al.</italic>, 2019</xref>; <xref rid="btaa003-B21" ref-type="bibr">Liu <italic>et al.</italic>, 2019</xref>) or <italic>XLNet</italic> (<xref rid="btaa003-B45" ref-type="bibr">Yang <italic>et al.</italic>, 2019</xref>), which are able to capture the bidirectional context directly. This might be particularly important for more complicated protein classification tasks such as sequence annotation tasks like secondary structure or phosphorylation site prediction that go beyond the prediction of a single global label.</p>
      </list-item>
    </list>
    <p>Leveraging large amounts of unlabeled data in the form of large, in parts very well-curated protein databases by the use of modern NLP methods, represents a new paradigm in the domain of proteomics. It will be interesting to see how this process continues with the rapidly evolving algorithmic advances in the field of NLP. Apart from the huge prospects in terms of quantitative prediction performance, the recent advances in the field of explainable ML research open exciting new avenues for deeper insights into the inner structure of proteins themselves. </p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btaa003_Supplementary_Data</label>
      <media xlink:href="btaa003_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We thank J. Vielhaben for discussions and work on related topics. <italic>UDSMProt</italic> was implemented using PyTorch and fast.ai and the CNN baseline models were implemented in Keras.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the Bundesministerium f<bold>ü</bold>r Bildung und Forschung (BMBF) through the Berlin Big Data Center under [grant 01IS14013A] and the Berlin Center for Machine Learning under [grant 01IS18037I]. </p>
      <p><italic>Conflict of Interest</italic>: none declared. </p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa003-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>AlQuraishi</surname><given-names>M.</given-names></name></person-group> (<year>2019</year>) 
<article-title>AlphaFold at CASP13</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>4862</fpage>–<lpage>4865</lpage>.<pub-id pub-id-type="pmid">31116374</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Asgari</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Mofrad</surname><given-names>M.R.K.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0141287</fpage>.<pub-id pub-id-type="pmid">26555596</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Bileschi</surname><given-names>M.L.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) Using deep learning to annotate the protein universe. <italic>bioRxiv</italic>, doi: 10.1101/626507.</mixed-citation>
    </ref>
    <ref id="btaa003-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Buchfink</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Fast and sensitive protein alignment using DIAMOND</article-title>. <source>Nat. Methods</source>, <volume>12</volume>, <fpage>59</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">25402007</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>A comprehensive review and comparison of different computational methods for protein remote homology detection</article-title>. <source>Brief. Bioinform</source>., <volume>19</volume>, <fpage>231</fpage>–<lpage>244</lpage>.<pub-id pub-id-type="pmid">27881430</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Clark</surname><given-names>W.T.</given-names></name>, <name name-style="western"><surname>Radivojac</surname><given-names>P.</given-names></name></person-group> (<year>2013</year>) 
<article-title>Information-theoretic evaluation of predicted ontological annotations</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>i53</fpage>–<lpage>i61</lpage>.<pub-id pub-id-type="pmid">23813009</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cozzetto</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>FFPred 3: feature-based function prediction for all gene ontology domains</article-title>. <source>Sci. Rep</source>., <volume>6</volume>, <fpage>31865</fpage>.<pub-id pub-id-type="pmid">27561554</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dalkiran</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>ECPred: a tool for the prediction of the enzymatic functions of protein sequences based on the EC nomenclature</article-title>. <source>BMC Bioinformatics</source>, <volume>19</volume>, <fpage>334</fpage>.<pub-id pub-id-type="pmid">30241466</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Devlin</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) {BERT}: pre-training of deep bidirectional transformers for language understanding. In: <italic>Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),</italic> pp. 4171–4186. Association for Computational Linguistics, Minneapolis, MN.</mixed-citation>
    </ref>
    <ref id="btaa003-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>El-Gebali</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>The Pfam protein families database in 2019</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D427</fpage>–<lpage>D432</lpage>.<pub-id pub-id-type="pmid">30357350</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>Q.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>GoFDR: a sequence alignment based method for predicting protein functions</article-title>. <source>Methods</source>, <volume>93</volume>, <fpage>3</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">26277418</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Håndstad</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Motif kernel generated by genetic programming improves remote homology and fold detection</article-title>. <source>BMC Bioinformatics</source>, <volume>8</volume>, <fpage>23</fpage>.<pub-id pub-id-type="pmid">17254344</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hochreiter</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Fast model-based protein homology detection without alignment</article-title>. <source>Bioinformatics</source>, <volume>23</volume>, <fpage>1728</fpage>–<lpage>1736</lpage>.<pub-id pub-id-type="pmid">17488755</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Howard</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Ruder</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>) Universal language model fine-tuning for text classification. In: <italic>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</italic>, pp. <fpage>328</fpage>–<lpage>339</lpage>. Association for Computational Linguistics, Melbourne, Australia.</mixed-citation>
    </ref>
    <ref id="btaa003-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D.P.</given-names></name>, <name name-style="western"><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>) Adam: a method for stochastic optimization. In: <italic>International Conference on Learning Representations (ICLR), May 7–9, 2015, San Diego, CA, USA</italic>.</mixed-citation>
    </ref>
    <ref id="btaa003-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulmanov</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Hoehndorf</surname><given-names>R.</given-names></name></person-group> (<year>2019</year>) 
<article-title>DeepGOPlus: improved protein function prediction from sequence</article-title>. <source>Bioinformatics</source>, doi:10.1093/bioinformatics/btz595.</mixed-citation>
    </ref>
    <ref id="btaa003-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulmanov</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>DeepGO: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>660</fpage>–<lpage>668</lpage>.<pub-id pub-id-type="pmid">29028931</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Understanding neural networks through representation erasure. <italic>arXiv preprint arXiv: 1612.08220</italic>.</mixed-citation>
    </ref>
    <ref id="btaa003-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Protein remote homology detection based on bidirectional long short-term memory</article-title>. <source>BMC Bioinformatics</source>, <volume>18</volume>, <fpage>443</fpage>.<pub-id pub-id-type="pmid">29017445</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>DEEPre: sequence-based enzyme EC number prediction by deep learning</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>760</fpage>–<lpage>769</lpage>.<pub-id pub-id-type="pmid">29069344</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) RoBERTa: a robustly optimized BERT pretraining approach. <italic>arXiv preprint arXiv: 1907.11692</italic>.</mixed-citation>
    </ref>
    <ref id="btaa003-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Loshchilov</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Hutter</surname><given-names>F.</given-names></name></person-group> (<year>2019</year>) Fixing weight decay regularization in Adam. In: <italic>International Conference on Learning Representations (ICLR), May 6–9, 2019, New Orleans, Louisiana, USA.</italic></mixed-citation>
    </ref>
    <ref id="btaa003-B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Madden</surname><given-names>T.</given-names></name></person-group> (<year>2013</year>) <chapter-title>The BLAST sequence analysis tool</chapter-title> In: <source>The NCBI Handbook</source>, <edition>2nd edn</edition>
<publisher-name>National Center for Biotechnology Information (US)</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa003-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Merity</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Regularizing and optimizing {LSTM} language models. In: <italic>International Conference on Learning Representations.</italic></mixed-citation>
    </ref>
    <ref id="btaa003-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Murzin</surname><given-names>A.G.</given-names></name></person-group><etal>et al</etal> (<year>1995</year>) 
<article-title>SCOP: a structural classification of proteins database for the investigation of sequences and structures</article-title>. <source>J. Mol. Biol</source>., <volume>247</volume>, <fpage>536</fpage>–<lpage>540</lpage>.<pub-id pub-id-type="pmid">7723011</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nakano</surname><given-names>F.K.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>Machine learning for discovering missing or wrong protein function annotations</article-title>. <source>BMC Bioinformatics</source>, <volume>20</volume>, <fpage>485</fpage>.<pub-id pub-id-type="pmid">31547800</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Peters</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Deep contextualized word representations</article-title>. In: <italic>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</italic>, pp. <fpage>2227</fpage>–<lpage>2237</lpage>. Association for Computational Linguistics New Orleans, Louisiana.</mixed-citation>
    </ref>
    <ref id="btaa003-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Price</surname><given-names>M.N.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Mutant phenotypes for thousands of bacterial genes of unknown function</article-title>. <source>Nature</source>, <volume>557</volume>, <fpage>503</fpage>–<lpage>509</lpage>.<pub-id pub-id-type="pmid">29769716</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Improving language understanding by generative pre-training. <italic>Technical report</italic>. OpenAI Blog.</mixed-citation>
    </ref>
    <ref id="btaa003-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Radford</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) Language models are unsupervised multitask learners. <italic>Technical report.</italic> OpenAI Blog.</mixed-citation>
    </ref>
    <ref id="btaa003-B31">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) Evaluating protein transfer learning with TAPE. In: Wallach, H. <italic>et al</italic> (eds) <italic>Advances in Neural Information Processing Systems 32</italic> Red Hook, NY, USA: Curran Associates, Inc., pp. <fpage>9686</fpage>–<lpage>9698</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa003-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rice</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2000</year>) 
<article-title>EMBOSS: the European molecular biology open software suite</article-title>. <source>Trends Genet</source>., <volume>16</volume>, <fpage>276</fpage>–<lpage>277</lpage>.<pub-id pub-id-type="pmid">10827456</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Rives</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. <italic>bioRxiv</italic>, doi:10.1101/622803.</mixed-citation>
    </ref>
    <ref id="btaa003-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sarac</surname><given-names>O.S.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Subsequence-based feature map for protein function classification</article-title>. <source>Comput. Biol. Chem</source>., <volume>32</volume>, <fpage>122</fpage>–<lpage>130</lpage>.<pub-id pub-id-type="pmid">18243801</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>H.-B.</given-names></name>, <name name-style="western"><surname>Chou</surname><given-names>K.-C.</given-names></name></person-group> (<year>2007</year>) 
<article-title>EzyPred: a top–down approach for predicting enzyme functional classes and subclasses</article-title>. <source>Biochem. Biophys. Res. Commun</source>., <volume>364</volume>, <fpage>53</fpage>–<lpage>59</lpage>.<pub-id pub-id-type="pmid">17931599</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Silla</surname><given-names>C.N.</given-names></name>, <name name-style="western"><surname>Freitas</surname><given-names>A.A.</given-names></name></person-group> (<year>2011</year>) 
<article-title>A survey of hierarchical classification across different application domains</article-title>. <source>Data Min. Knowl. Disc</source>., <volume>22</volume>, <fpage>31</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa003-B37">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>L.N.</given-names></name></person-group> (<year>2018</year>) A disciplined approach to neural network hyper-parameters: part 1—learning rate, batch size, momentum, and weight decay. <italic>US Naval Research Laboratory Technical Report 5510-026</italic>.</mixed-citation>
    </ref>
    <ref id="btaa003-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) MASS: masked sequence to sequence pre-training for language generation. In <italic>International Conference on Machine Learning</italic>, pp. <fpage>5926</fpage>–<lpage>5936</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa003-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Sundararajan</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Axiomatic attribution for deep networks. In: <italic>International Conference on Machine Learning (ICML)</italic>, <italic>Vol. 70, August 6–11, 2017, Sydney, Australia, JMLR</italic>, pp. <fpage>3319</fpage>–<lpage>3328</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa003-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Taylor</surname><given-names>W.R.</given-names></name></person-group> (<year>1986</year>) 
<article-title>The classification of amino acid conservation</article-title>. <source>J. Theor. Biol</source>., <volume>119</volume>, <fpage>205</fpage>–<lpage>218</lpage>.<pub-id pub-id-type="pmid">3461222</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B41">
      <mixed-citation publication-type="journal">The UniProt Consortium. (<year>2018</year>) 
<article-title>UniProt: a worldwide hub of protein knowledge</article-title>. <source>Nucleic Acids Res</source>., <volume>47</volume>, <fpage>D506</fpage>–<lpage>D515</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa003-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vens</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Decision trees for hierarchical multi-label classification</article-title>. <source>Mach. Learn</source>., <volume>73</volume>, <fpage>185</fpage>–<lpage>214</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa003-B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wehrmann</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) <chapter-title>Hierarchical multi-label classification networks</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Dy</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Krause</surname><given-names>A.</given-names></name></person-group> (eds), <source>Proceedings of the 35th International Conference on Machine Learning, Volume 80 of Proceedings of Machine Learning Research</source>, pp. <fpage>5075</fpage>–<lpage>5084</lpage>. 
<publisher-name>Stockholmsmässan</publisher-name>, 
<publisher-loc>Stockholm Sweden</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btaa003-B44">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Qizhe</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) Unsupervised data augmentation. <italic>arXiv preprint arXiv: 1904.12848.</italic></mixed-citation>
    </ref>
    <ref id="btaa003-B45">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>Z.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) XLNet: generalized autoregressive pretraining for language understanding. In: Wallach, H. <italic>et al</italic> (eds) <italic>Advances in Neural Information Processing Systems 32</italic> Curran Associates, Inc., pp. <fpage>5754</fpage>–<lpage>5764</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa003-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2018a</year>) 
<article-title>DeepText2go: improving large-scale protein function prediction with deep semantic text representation</article-title>. <source>Methods</source>, <volume>145</volume>, <fpage>82</fpage>–<lpage>90</lpage>.<pub-id pub-id-type="pmid">29883746</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2018b</year>) 
<article-title>GOLabeler: improving sequence-based large-scale protein function prediction by learning to rank</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>2465</fpage>–<lpage>2473</lpage>.<pub-id pub-id-type="pmid">29522145</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>The CAFA challenge reports improved protein function prediction and new functional annotations for hundreds of genes through experimental screens</article-title>. <source>Genome Biol</source>., <volume>20</volume>, <fpage>244</fpage>.<pub-id pub-id-type="pmid">31744546</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>Z.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>mlDEEPre: multi-functional enzyme function prediction with hierarchical multi-label deep learning</article-title>. <source>Front. Genet</source>., <volume>9</volume>, <fpage>714</fpage>.<pub-id pub-id-type="pmid">30723495</pub-id></mixed-citation>
    </ref>
    <ref id="btaa003-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Upmeier zu Belzen</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2019</year>) 
<article-title>Leveraging implicit knowledge in neural networks for functional dissection and engineering of proteins</article-title>. <source>Nat. Mach. Intell</source>., <volume>1</volume>, <fpage>225</fpage>–<lpage>235</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
