<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Med (Lausanne)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Med (Lausanne)</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Med.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Medicine</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2296-858X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9681494</article-id>
    <article-id pub-id-type="doi">10.3389/fmed.2022.1050436</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Medicine</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Combining transfer learning with retinal lesion features for accurate detection of diabetic retinopathy</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Hassan</surname>
          <given-names>Doaa</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gill</surname>
          <given-names>Hunter Mathias</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1433861/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Happe</surname>
          <given-names>Michael</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2019119/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bhatwadekar</surname>
          <given-names>Ashay D.</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/103292/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hajrasouliha</surname>
          <given-names>Amir R.</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/2061090/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Janga</surname>
          <given-names>Sarath Chandra</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/29084/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Department of BioHealth Informatics, School of Informatics and Computing, Indiana University Purdue University</institution>, <addr-line>Indianapolis, IN</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Computers and Systems Department, National Telecommunication Institute</institution>, <addr-line>Cairo</addr-line>, <country>Egypt</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Department of Ophthalmology, Glick Eye Institute, Indiana University School of Medicine</institution>, <addr-line>Indianapolis, IN</addr-line>, <country>United States</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Department of Medical and Molecular Genetics, Indiana University School of Medicine, Medical Research and Library Building</institution>, <addr-line>Indianapolis, IN</addr-line>, <country>United States</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Centre for Computational Biology and Bioinformatics, Indiana University School of Medicine, 5021 Health Information and Translational Sciences (HITS)</institution>, <addr-line>Indianapolis, IN</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Dario Rusciano, Sooft Italia SpA, Italy</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Chiou-Shann Fuh, National Taiwan University, Taiwan; Giuseppe Bernava, National Research Council (CNR), Italy</p>
      </fn>
      <corresp id="c001">*Correspondence: Sarath Chandra Janga <email>scjanga@iupui.edu</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Ophthalmology, a section of the journal Frontiers in Medicine</p>
      </fn>
      <fn fn-type="equal" id="fn002">
        <p>†These authors have contributed equally to this work</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>08</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>1050436</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Hassan, Gill, Happe, Bhatwadekar, Hajrasouliha and Janga.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Hassan, Gill, Happe, Bhatwadekar, Hajrasouliha and Janga</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Diabetic retinopathy (DR) is a late microvascular complication of Diabetes Mellitus (DM) that could lead to permanent blindness in patients, without early detection. Although adequate management of DM <italic>via</italic> regular eye examination can preserve vision in in 98% of the DR cases, DR screening and diagnoses based on clinical lesion features devised by expert clinicians; are costly, time-consuming and not sufficiently accurate. This raises the requirements for Artificial Intelligent (AI) systems which can accurately detect DR automatically and thus preventing DR before affecting vision. Hence, such systems can help clinician experts in certain cases and aid ophthalmologists in rapid diagnoses. To address such requirements, several approaches have been proposed in the literature that use Machine Learning (ML) and Deep Learning (DL) techniques to develop such systems. However, these approaches ignore the highly valuable clinical lesion features that could contribute significantly to the accurate detection of DR. Therefore, in this study we introduce a framework called DR-detector that employs the Extreme Gradient Boosting (XGBoost) ML model trained <italic>via</italic> the combination of the features extracted by the pretrained convolutional neural networks commonly known as transfer learning (TL) models and the clinical retinal lesion features for accurate detection of DR. The retinal lesion features are extracted <italic>via</italic> image segmentation technique using the UNET DL model and captures exudates (EXs), microaneurysms (MAs), and hemorrhages (HEMs) that are relevant lesions for DR detection. The feature combination approach implemented in DR-detector has been applied to two common TL models in the literature namely VGG-16 and ResNet-50. We trained the DR-detector model using a training dataset comprising of 1,840 color fundus images collected from e-ophtha, retinal lesions and APTOS 2019 Kaggle datasets of which 920 images are healthy. To validate the DR-detector model, we test the model on external dataset that consists of 81 healthy images collected from High-Resolution Fundus (HRF) dataset and MESSIDOR-2 datasets and 81 images with DR signs collected from Indian Diabetic Retinopathy Image Dataset (IDRID) dataset annotated for DR by expert. The experimental results show that the DR-detector model achieves a testing accuracy of 100% in detecting DR after training it with the combination of ResNet-50 and lesion features and 99.38% accuracy after training it with the combination of VGG-16 and lesion features. More importantly, the results also show a higher contribution of specific lesion features toward the performance of the DR-detector model. For instance, using only the hemorrhages feature to train the model, our model achieves an accuracy of 99.38 in detecting DR, which is higher than the accuracy when training the model with the combination of all lesion features (89%) and equal to the accuracy when training the model with the combination of all lesions and VGG-16 features together. This highlights the possibility of using only the clinical features, such as lesions that are clinically interpretable, to build the next generation of robust artificial intelligence (AI) systems with great clinical interpretability for DR detection. The code of the DR-detector framework is available on GitHub at <ext-link xlink:href="https://github.com/Janga-Lab/DR-detector" ext-link-type="uri">https://github.com/Janga-Lab/DR-detector</ext-link> and can be readily employed for detecting DR from retinal image datasets.</p>
    </abstract>
    <kwd-group>
      <kwd>retinal image</kwd>
      <kwd>Diabetic Retinopathy</kwd>
      <kwd>deep learning</kwd>
      <kwd>transfer learning</kwd>
      <kwd>lesion features</kwd>
    </kwd-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="5"/>
      <equation-count count="5"/>
      <ref-count count="60"/>
      <page-count count="14"/>
      <word-count count="9121"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Diabetic Retinopathy (DR) is a microvascular disorder associated with long-term diabetes mellitus and is one of the leading causes of preventable vision loss across the worldwide (<xref rid="B1" ref-type="bibr">1</xref>). DR manifests in individuals diagnosed with Type 1 Diabetes (T1D) or Type 2 Diabetes (T2D). Roughly one-third of diabetic patients are affected by DR (<xref rid="B2" ref-type="bibr">2</xref>, <xref rid="B3" ref-type="bibr">3</xref>), and the likelihood of developing DR scales with the length of diabetes duration (<xref rid="B4" ref-type="bibr">4</xref>).</p>
    <p>The progression of DR in T1D and T2D is characterized by damage to the retina. The retina is a multilayered network of rod and cone photoreceptor cells integrated with bipolar and ganglion cells that enable vision by encoding information gained from light as nerve impulses (<xref rid="B5" ref-type="bibr">5</xref>). The retina is supplied with oxygen and nutrients by an extensive vascular system. In T1D and T2D, high blood glucose levels contribute to pro-inflammatory changes that increase the permeability of the blood-retina barrier, leading to leakage of fluids and blood into the retina (<xref rid="B6" ref-type="bibr">6</xref>). High blood glucose can also block small retinal capillaries, impeding the delivery of nutrients and contributing to further damage (<xref rid="B7" ref-type="bibr">7</xref>).</p>
    <p>Although adequate management of DM <italic>via</italic> regular eye examination can preserve vision in DR in many cases, DR screening and diagnoses currently involve highly trained and qualified medical professionals at a high cost. Thus, there is a continuous need for the development of automatic approaches for DR detection as a cheaper alternative to the time-consuming manual DR diagnosis by trained clinicians. A promising application of these approaches is Computer Assisted Diagnosis (CAD) support for detection of DR. An advantage of such CAD applications is that they offset the burden on medical professionals like expert ophthalmologists and fill their absence in addition to preventing DR before affecting vision. This consideration is critical, considering that the global burden of DR is expected to expand to 700 million cases by the 2040s (<xref rid="B8" ref-type="bibr">8</xref>). Many DR detection techniques suitable for CAD utilize Machine Learning (ML), Deep Learning (DL) algorithms and various previously pretrained DL models commonly known as Transfer Learning (TL) models.</p>
    <p>The TL models have been successfully used for automated binary and multi-class classification of color fundus retinal images for DR detection (<xref rid="B9" ref-type="bibr">9</xref>–<xref rid="B12" ref-type="bibr">12</xref>). These algorithms have shown a great performance in the automatic detection of DR in non-clinical setups when the dataset is very small and might cause chances for underfitting or high generalization error. In such cases, TL is preferred over standard DL techniques. Recently, the focus has been shifted to TL feature-based models, where common TL algorithms are used for extracting many important local (textural) features from retinal images for detection of DR and predicting its severity level through convolving with a sliding window and forming a filter. For example, features extracted from AlexNet TL model (<xref rid="B13" ref-type="bibr">13</xref>) were passed to the Support Vector Machine (SVM) ML model to enhance the efficiency of the DR classification system, where SVM model achieved accuracies of 97.93 and 95.26% in five-class DR classification with linear discriminant analysis (LDA) feature selection and principle component analysis (PCA) dimensional reduction, respectively (<xref rid="B14" ref-type="bibr">14</xref>), when training the model and testing on Kaggle dataset<xref rid="fn0001" ref-type="fn"><sup>1</sup></xref>. As an extension to the same direction, features from the final layers of VGG-19 TL model (<xref rid="B15" ref-type="bibr">15</xref>) were collected and aggregated to get a deeper representation of retinal images, and these dense features were reduced by PCA and singular value decomposition (SVD) (<xref rid="B16" ref-type="bibr">16</xref>), where it was fed to a deep Neural network (DNN) model that achieved accuracies of 97.96, and 98.34% in DR severity classification with PCA and SVD, respectively when training the model and testing on Kaggle dataset. Yaqoob et al. (<xref rid="B17" ref-type="bibr">17</xref>) introduced a feature representation extracted by ResNet-50 TL model (<xref rid="B18" ref-type="bibr">18</xref>) that was fed to Random Forest (RF) classifier for binary and multiclassification of DR. This approach achieved an accuracy of 96% when it was applied on a dataset comprised of two DR categories for detecting DR and accuracy of 75.09% when it was applied as on five DR category dataset for predicting the severity of DR.</p>
    <p>Bodapati et al. (<xref rid="B19" ref-type="bibr">19</xref>) introduced a DR classification model that aggregates features extracted from multiple convolution blocks of TL models to enhance feature representation and hence improve DR detection. The model has compared various methodologies of pooling and feature aggregation and it was concluded that averaging pooling with simple fusion approaches using Deep Neural Networks (DNN) led to an improved performance. The authors of this work claimed that their approach for blending features from the convolution layers of the same TL model is simpler and better than the simple concatenation of features extracted from various TL models at a different scale that was presented early in (<xref rid="B20" ref-type="bibr">20</xref>). The latter approach introduced a multi-modal blended TL feature representation for extracting deep features from penultimate layers of multiple TL models and blending them using different pooling approaches to obtain the final DR image representation.</p>
    <p>However, besides the local features presented by various TL models or the fusion of those features, the global image features have been playing an important role in DR detection. Those features are represented by the contour and structural features that describe retinal lesions like exudates (EXs), microaneurysms (MAs), and hemorrhages (HEMs), where the presence of DR disease is characterized by detecting one or more of these lesions. Thus, those global features (lesion features) are considered as good signs of retinal image lesions and hence can be successfully applied to improve the final accuracy of a TL-feature based DR screening system. Therefore, there were some attempts that used image segmentation techniques for extracting/detecting retinal lesion features that can be used for DR detection and staging (<xref rid="B21" ref-type="bibr">21</xref>, <xref rid="B22" ref-type="bibr">22</xref>). In such image segmentation- based methods, a label is assigned to every pixel of an image based on pixel characteristics (<xref rid="B23" ref-type="bibr">23</xref>). The labels are encoded in a segmentation mask with equal dimensions to the image. In binary segmentation tasks, each mask pixel represents either the foreground (corresponds to an area-of-interest in the image; value = 1) or the background (corresponds to all non-area-of-interest; value = 0). Thus, binary segmentation tasks are useful for extracting notable areas from biomedical images.</p>
    <p>With the introduction of deep learning (DL), especially convolutional neural network (CNN), the DL based methods have resulted in an outstanding performance in medical image segmentation (<xref rid="B24" ref-type="bibr">24</xref>). UNET-architecture CNN models are one of DL models that achieve remarkable performance in medical image segmentation tasks (<xref rid="B25" ref-type="bibr">25</xref>). For the task of retinal image segmentation for DR detection, there were multiple research reports that demonstrate the use of UNET for segmentation of leakage-prone blood vessels (<xref rid="B26" ref-type="bibr">26</xref>, <xref rid="B27" ref-type="bibr">27</xref>). UNET models have also been successfully developed for the segmentation of MAs (<xref rid="B28" ref-type="bibr">28</xref>–<xref rid="B31" ref-type="bibr">31</xref>), EXs (<xref rid="B32" ref-type="bibr">32</xref>–<xref rid="B35" ref-type="bibr">35</xref>), and HEMs (<xref rid="B36" ref-type="bibr">36</xref>).</p>
    <p>Although TL and segmentation features provide robust information for DR detection, they were not used together in the literature for a two-class (binary) DR-detection. Only a few existing research attempts utilized the fusion of both types of features for improving the performance of predicting the severity levels of DR (<xref rid="B37" ref-type="bibr">37</xref>, <xref rid="B38" ref-type="bibr">38</xref>). For example, Harangi et al. (<xref rid="B37" ref-type="bibr">37</xref>) proposed a framework that combines AlexNet TL-based features with image-level features that reflect the intensity, shape, and texture of the structures of the image and lesion-specific features associated with MAs and EXs. This combination of features was passed through an additional fully connected layer followed by a softmax function that achieved an accuracy of 90.07% in predicting the class probabilities corresponding to 5 classes for DR that express its severity levels. In Bogacsovics et al. (<xref rid="B38" ref-type="bibr">38</xref>), the same idea was extended to several commonly used TL models for local image feature extraction other than AlexNet. Next, the results of concatenating the TL features of those models with the hand-crafted features (image level and lesion features) were objectively compared to demonstrate the best concatenation framework that improves the accuracy of predicting DR grades. Next, the best concatenation was passed through an additional fully connected layer then a softmax function to predict the five class probabilities of DR severity. However, both approaches in (<xref rid="B37" ref-type="bibr">37</xref>, <xref rid="B38" ref-type="bibr">38</xref>) were not tested for binary classification of DR to report the presence of the disease. Also, the performance of the lesion feature extraction was not explicitly investigated as well as the impact of those features on DR detection. Moreover, both approaches combined the image level features with TL and lesion features for training the DR severity levels predictors, increasing the curse of dimensionality.</p>
    <p>Therefore, in this study we propose a framework called DR-detector (<xref rid="F1" ref-type="fig">Figure 1</xref>) that employs the XGboost ML model (<xref rid="B39" ref-type="bibr">39</xref>) for an accurate detection of DR. The model is trained with a combination of the TL features, and three clinical lesion features that capture EXs, MAs, and HEMs. Such a combination is used to get a better representation of retinal image features that can be used to decide about the presence of DR disease. Thus, we seek the power of TL model to extract features that accurately capture the local textural retinal images while simultaneously taking advantage of the power of lesion features to represent the global features of the retinal images that would result in improving the performance of the DR-detector model and its interpretability for clinical use. We tested the DR-detector model on an external dataset of retinal images for detection of DR. We have applied our proposed framework to two common TL models in the literature namely VGG-16 and ResNet-50 models (<xref rid="F1" ref-type="fig">Figure 1A</xref>). The experimental results show that the DR-detector model achieves an accuracy of 100% in detecting DR when testing it on an external dataset after training it with the combination of Resnt-50 and lesion features and 99.38% accuracy after training it with the combination of VGG-16 and lesion features. The results also show a higher contribution of some lesion features toward the performance of the model over other lesion features. For instance, using the hemorrhages feature to train the model, our model achieves an accuracy of 99.38 in detecting DR which is higher than the accuracy of the model when training it with the combination of all lesion features (89%) and equal to the accuracy when training it with the combination of all lesions and VGG-16 features together. Thus, we arrived at two major conclusions. First, the extracted relevant lesion features can complement the textural features extracted by the TL model to improve the performance of DR-detector model and its interpretability for clinical use in detecting the presence of DR disease. Second, the contribution of lesion features to the performance of DR-detector model varies from one lesion feature to another. This highlights the possibility of using only the lesion features for training the next generation of robust and accurate AI models with clinical interpretability for DR detection.</p>
    <fig position="float" id="F1">
      <label>Figure 1</label>
      <caption>
        <p>The workflow of DR-detector framework for detection of DR using a combination of TL and lesion features. <bold>(A)</bold> Extracting TL features from VGG-16 and ResNet-50 models. <bold>(B)</bold> Extracting MAs, EXs, and HEMs lesion features from retinal fundus images using U-Net semantic segmentation models. <bold>(C)</bold> Flat combination of TL and lesion features in DR-detector framework.</p>
      </caption>
      <graphic xlink:href="fmed-09-1050436-g0001" position="float"/>
    </fig>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>Materials and methods</title>
    <sec>
      <title>Approach pipeline</title>
      <p>The main objective of this work is to develop a robust and efficient framework called DR-detector for automatic detection of DR. Thus, we employed ML model namely the XGBoost in this framework to achieve this objective. This model is trained with a combination of two types of extracted features. The first type is deep convolutional features extracted using a TL model (VGG16-model or Resnet50) pre-trained previously on ImageNet dataset (<xref rid="B40" ref-type="bibr">40</xref>) (<xref rid="F1" ref-type="fig">Figure 1A</xref>). Those deep features were known as the most descriptive and discriminate features that ultimately improve the performance of DR recognition (<xref rid="B16" ref-type="bibr">16</xref>). The second type of features are three clinical lesion features that capture the EXs, MAs, and HEMs and are extracted using image segmentation <italic>via</italic> U-Net DL model (<xref rid="F1" ref-type="fig">Figure 1B</xref>). Those lesion features were found to be the most common pathological signs of DR in the literature (<xref rid="B41" ref-type="bibr">41</xref>). Next the performance of DR-detector model is tested on external dataset of fundus retinal images after training it with the combination of TL and lesion features (<xref rid="F1" ref-type="fig">Figure 1C</xref>). In summary, the proposed pipeline of the DR-detector framework (<xref rid="F1" ref-type="fig">Figure 1</xref>) has five different modules including TL feature extraction (<xref rid="F1" ref-type="fig">Figure 1A</xref>), lesion feature extraction (<xref rid="F1" ref-type="fig">Figure 1B</xref>), and feature combination, model training and model evaluation (<xref rid="F1" ref-type="fig">Figure 1C</xref>).</p>
    </sec>
    <sec>
      <title>Datasets</title>
      <sec>
        <title>Training dataset</title>
        <p>We conduct our experiments on 1,840 color fundus images. 920 images of them have DR signs, and the remaining are healthy images. Those images were used for training the DR-detector model. The images with DR signs were collected from two public-available datasets, namely the e-ophtha<xref rid="fn0002" ref-type="fn"><sup>2</sup></xref>, and retinal lesions (<xref rid="B42" ref-type="bibr">42</xref>, <xref rid="B43" ref-type="bibr">43</xref>) with binary masks for extracting and quantifying the EXs, Mas, and HEMs lesion features. The healthy images were collected from APTOS 2019 Blindness Detection Kaggle competition training dataset (<xref rid="B44" ref-type="bibr">44</xref>). The Binary masks for healthy eye images are generated by creating all-black images with identical dimensions.</p>
        <p>The images in the training dataset have various levels of DR on a scale of 0 to 4 [0 - No DR (healthy), 1-Mild, 2-Moderate, 3-Severe, 4-Proliferative DR] to indicate different DR severity levels. However, the data is imbalanced as it consists of 920 healthy images of DR-level 0, 120 images of DR-level 1, 681 images of DR-level 2, 64 images of DR level 3, 55 images of DR level 4. Since we found that there is not enough data from DR classes of DR-levels (1–4) that we can include in the training dataset to balance it, we decided to go with the binary classification of DR for automatic detection of this disease. Therefore, we adopt the training data set for binary classification problem by merging all images of DR signs of 1–4 into a single positive class of 920 images labeled as DR and the remaining 920 images are labeled as healthy and assigned to the negative class as shown in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
        <table-wrap position="float" id="T1">
          <label>Table 1</label>
          <caption>
            <p>Number of healthy and DR images in the training dataset.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>DR severity level</bold>
                </th>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Number of healthy and DR images</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">0</td>
                <td valign="top" align="left" rowspan="1" colspan="1">920 (Healthy)</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">1</td>
                <td valign="top" align="left" rowspan="1" colspan="1">920 (DR)</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">2</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">3</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">4</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Total</td>
                <td valign="top" align="left" rowspan="1" colspan="1">1,840</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec>
        <title>Testing dataset</title>
        <p>For testing the DR-detector model, we have used a dataset of 162 color fundus images, where 81 of them are annotated as DR affected, and the remaining are from healthy individuals. Those images were collected from three publicly available datasets, namely High-Resolution Fundus (HRF) dataset (<xref rid="B45" ref-type="bibr">45</xref>), Indian Diabetic Retinopathy Image Dataset (IDRID) dataset (<xref rid="B46" ref-type="bibr">46</xref>), and MESSIDOR-2 datasets (<xref rid="B47" ref-type="bibr">47</xref>). The IDRID contained 81 color fundus images (4,288 x 2,848) with binary masks representing DR-affected eyes needed to extract and quantify the EXs, Mas, and HEMs lesion features. However, IDRID dataset does not contain any healthy eye images, so the healthy eye images in the testing dataset were randomly selected from HRF and MESSIDOR-2 datasets. Binary masks for healthy eye images are generated by creating all-black images with identical dimensions.</p>
        <p>Similar to the training data, the images in the testing dataset have 0–4 levels of DR to indicate different DR severity levels. However, the dataset is imbalanced as it consists of 81 healthy images of DR-level 0 and 81 images of DR affected images with 2 images of DR-level 1, 34 images of DR-level 2, 22 images of DR level 3, and 23 images of DR level 4. Since there is not enough data from DR classes of DR-levels (1–4) that we can include in the testing dataset to balance it, it has been more convenient to use such data for binary classification of DR for detection of the disease. To achieve this, we adopt the testing data set for binary classification problem by merging all images of DR levels of 1–4 into a single positive class of 81 images labeled as DR and the remaining 81 images are labeled as healthy and assigned to the negative class as shown in <xref rid="T2" ref-type="table">Table 2</xref>.</p>
        <table-wrap position="float" id="T2">
          <label>Table 2</label>
          <caption>
            <p>Number of healthy and DR images in the testing dataset.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>DR severity level</bold>
                </th>
                <th valign="top" align="left" rowspan="1" colspan="1">
                  <bold>Number of healthy and DR images</bold>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">0</td>
                <td valign="top" align="left" rowspan="1" colspan="1">81 (healthy)</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">1</td>
                <td valign="top" align="left" rowspan="1" colspan="1">81 (DR)</td>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">2</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">3</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">4</td>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td valign="top" align="left" rowspan="1" colspan="1">Total</td>
                <td valign="top" align="left" rowspan="1" colspan="1">162</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
    </sec>
    <sec>
      <title>Image feature extraction with transfer learning</title>
      <p>In this approach, local representations of the retinal image's features are obtained from the TL model (either the VGG16 or the Resent50 pretrained models) by extracting deep features from the final layers of the pre-trained models. When performing feature extraction with TL models, we treat the pre-trained network as an arbitrary feature extractor, allowing the input image to propagate forward, stopping at pre-specified layer, and taking the outputs of that layer as our features.</p>
      <p>As for extracting deep features using VGG-16 pretrained model, the original VGG-16 model (<xref rid="B15" ref-type="bibr">15</xref>) is adopted first to address the automatic detection of DR (top subfigure of <xref rid="F1" ref-type="fig">Figure 1A</xref>). For this task, the model expects input images of 224<sup>*</sup>224<sup>*</sup>3. Thus, images are reshaped to 224<sup>*</sup>224<sup>*</sup>3 before feeding them to this model. Next the soft-max layer and fully connected (FC) layers are removed from VGG-16 model (area after the solid vertical blue line in top subfigure of <xref rid="F1" ref-type="fig">Figure 1A</xref>) and the model utilizes the VGG-16 network (<xref rid="B15" ref-type="bibr">15</xref>) for feature extraction <italic>via</italic> the final layer prior to the FC layers—that outputs volume of size 7 x 7 x 512 dim (area with dashed border in top subfigure of <xref rid="F1" ref-type="fig">Figure 1A</xref>). This output will serve as VGG-16 extracted features which will be flattened later into a feature vector of 25,088-dim combined with the lesion features, as described later in section Combination of TL and lesion features.</p>
      <p>As for extracting deep features using the ResNet-50 pretrained model, the original ResNet-50 model (<xref rid="B17" ref-type="bibr">17</xref>) is adopted first to address the detection of DR task (bottom subfigure of <xref rid="F1" ref-type="fig">Figure 1A</xref>). For this task, the model expects input images of 224<sup>*</sup>224<sup>*</sup>3. Thus, images are reshaped to 224<sup>*</sup>224<sup>*</sup>3 before feeding them to this model. Next, the soft-max layer and fully connected (FC) layers are removed from ResNet-50 model (area after the solid vertical blue line in the bottom subfigure of <xref rid="F1" ref-type="fig">Figure 1A</xref>) and the model utilizes the ResNet-50 network (<xref rid="B18" ref-type="bibr">18</xref>) for feature extraction <italic>via</italic> the final layer before the fully connected (FC) layers—that outputs volume of size 7 x 7 x 2,048 dim (area with dash border in bottom subfigure of <xref rid="F1" ref-type="fig">Figure 1A</xref>). This output will serve as ResNet-50 extracted features which will be flattened later into a feature vector of 100, 352-dim combined with lesion features as described later in section Combination of TL and lesion features.</p>
    </sec>
    <sec>
      <title>Lesion feature extraction with image segmentation</title>
      <sec>
        <title>Retinal image lesions associated with DR</title>
        <p>Retinal lesions that develop early over the course of DR, including MAs, EXs, and HEMs (<xref rid="SM2" ref-type="supplementary-material">Supplementary Figure 1</xref>), are clinically important markers used to distinguish between healthy and DR-affected eyes. Below, we elaborate about each of these three lesions:</p>
        <sec>
          <title>Microaneurysms</title>
          <p>MA are the earliest symptoms of DR. These lesions are widened protrusions extending from capillary walls and are associated with abnormal fluid leakage through breakdown of the blood-retina barrier. MA can rupture to create hemorrhages, leading to greater leakage of capillary fluids and damage to surrounding retinal tissues. The number of microaneurysms can be used to gauge the progression of DR (<xref rid="B48" ref-type="bibr">48</xref>).</p>
        </sec>
        <sec>
          <title>Exudates</title>
          <p>EXs are lipids and proteins (fibrinogen, albumin) carried by filtrating fluids past the blood-retina barrier into the retinal tissue (<xref rid="B49" ref-type="bibr">49</xref>).</p>
        </sec>
        <sec>
          <title>Hemorrhages</title>
          <p>HEMs occur when MA burst, and leak blood and serum into the retina. Intraretinal bleeding is a sign of worsening DR. Blood can impair DR patient vision and increased intraretinal pressure can contribute to retinal damage (<xref rid="B50" ref-type="bibr">50</xref>).</p>
        </sec>
      </sec>
      <sec>
        <title>Framework for UNET- model-based lesion detection and quantification</title>
        <p>We have developed a framework to extract MAs, EXs, and HEMs lesions from retinal fundus images using U-Net semantic segmentation models (<xref rid="F1" ref-type="fig">Figure 1B</xref>) and deployed it in DR-detector framework for extracting lesion features. The steps for the UNET-based retinal lesion detection and quantification workflow are described below:</p>
        <sec>
          <title>Pre-processing</title>
          <p>Binary thresholding is applied to set all pixels corresponding to the image background (the non-eye margins) equal to zero. Multiple studies demonstrate the green channel encodes the greatest contrast between retinal structures (<xref rid="B32" ref-type="bibr">32</xref>–<xref rid="B34" ref-type="bibr">34</xref>). Input RGB retinal fundus images are split by channel and the green channel is extracted. Contrast Limited Adaptive Histogram Equalization (CLAHE) (8x8 tile size) (<xref rid="B51" ref-type="bibr">51</xref>) is applied to the green channel to correct the contrast over-amplification. A gamma correction is utilized to adjust luminescence of the CLAHE output (γ = 0.8). The contrast enhancement stages are shown in <xref rid="SM2" ref-type="supplementary-material">Supplementary Figure 2</xref>. After contrast enhancement, images are divided into patches.</p>
          <p>Each preprocessed retinal image and its corresponding ground truth mask is divided into overlapping square (<italic>n</italic> x <italic>n)</italic> patches. <italic>n</italic> is set to 128 pixels (px) for MAs (due to small lesion size) and is set to 256 px for EXs and HEMs. Created patches are randomly selected for augmentation operations.</p>
          <p>Augmentation for image and corresponding binary mask patches involves creating new training instances from existing ones by applying a spatial or color operation to represent them in a new orientation or perspective. The random flip (horizontal, vertical) and random rotation (360°) techniques from Keras ImageDataGenerator (<xref rid="B52" ref-type="bibr">52</xref>) are used to augment training patches. Any augmentation technique applied to a fundus image patch is likewise applied to its ground truth patch.</p>
        </sec>
        <sec>
          <title>Segmentation</title>
          <p>The input retinal fundus images are preprocessed and divided into augmented patches as described above. K-fold cross-validation (<italic>k</italic> = 5) is applied to the patches. The batch size for each fold is set to 32 and the number of epochs is set to 3. Epoch training and validation steps are set as the number of training or validation patches per fold divided by batch size, respectively.</p>
          <p>Patch probability maps output by the UNET DL model are merged to construct a probability map with equal dimensions to the input image. A threshold of 0.5 is applied to convert the reconstructed probability map into a binary image mask.</p>
        </sec>
        <sec>
          <title>Feature counts</title>
          <p>Canny edge detection (<xref rid="B53" ref-type="bibr">53</xref>) is applied to find the edges around mask foreground regions (white). Contour detection (<xref rid="B54" ref-type="bibr">54</xref>) is used to fill Canny edge gaps and fully close the foreground shapes. The number of lesions within the segmentation mask is defined as the number of distinct objects described by contours.</p>
        </sec>
      </sec>
      <sec>
        <title>U-net model implementation</title>
        <p>Keras (<xref rid="B55" ref-type="bibr">55</xref>), the free python deep learning API with TensorFlow (<xref rid="B56" ref-type="bibr">56</xref>) back end was used to construct a base UNET model for binary semantic segmentation. Input patches are supplied to the input layer as tensors with shape (32, <italic>n, n</italic>, 1). The UNET contracting path used for downsampling is defined by 5 convolution blocks, each with 2 (3 x 3) convolution layers (activation = “ReLU,” padding = “same”) and a (2 x 2) pooling layer. The UNET expansive path used for upsampling is defined by 5 convolution blocks, each with a (2 x 2) transpose convolution layer (activation = “ReLU,” padding = “same”), concatenation layer, and 2 (3 x 3) convolution layers (activation = “ReLU,” padding = “same”). A (1 x 1) output layer using a sigmoidal activation function returns the model output. Binary focal entropy is selected as the loss function due to large class imbalance between foreground and background pixels.</p>
        <p>The model output is a pixelwise probability map for each input patch. The probability map values range from 0 to 1; values closer to 0 represent pixels more likely to belong to the negative class and pixels closer to 1 represent pixels more likely to belong to the positive class.</p>
      </sec>
      <sec>
        <title>Metrics for performance evaluation of U-net model</title>
        <p>We evaluate the performance of U-Net DL model in extracting each of the three lesions in terms of accuracy, recall, precision, F1-score, and IoU. The mathematical equations that describe each of these metrics are shown below:</p>
        <disp-formula id="E1">
          <label>(1)</label>
          <mml:math id="M1" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext>Accuracy</mml:mtext>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mtext>TP </mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext> TN</mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext>TP </mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext> FP </mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext> FN </mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext> TN</mml:mtext>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E2">
          <label>(2)</label>
          <mml:math id="M2" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext>Recall</mml:mtext>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mtext>TP</mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext>TP </mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext> FN</mml:mtext>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E3">
          <label>(3)</label>
          <mml:math id="M3" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext>Precision</mml:mtext>
                  <mml:mo>=</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mtext>TP</mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext>TP </mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext> FP</mml:mtext>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E4">
          <label>(4)</label>
          <mml:math id="M4" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext>F</mml:mtext>
                  <mml:mn>1</mml:mn>
                  <mml:mo>-</mml:mo>
                  <mml:mtext>score</mml:mtext>
                  <mml:mo>=</mml:mo>
                  <mml:mn>2</mml:mn>
                  <mml:mo>*</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mtext>Precisio</mml:mtext>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mtext>n</mml:mtext>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>*</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mtext>Recall</mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext>Precision </mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mtext> Recall</mml:mtext>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <disp-formula id="E5">
          <label>(5)</label>
          <mml:math id="M5" overflow="scroll">
            <mml:mtable columnalign="left">
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext>Intersection over union </mml:mtext>
                  <mml:mrow>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mrow>
                      <mml:mtext>IoU</mml:mtext>
                    </mml:mrow>
                    <mml:mo stretchy="false">)</mml:mo>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>A</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>∩</mml:mo>
                      <mml:mtext> </mml:mtext>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mi>A</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>A</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>∪</mml:mo>
                      <mml:mtext> </mml:mtext>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mi>A</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
        <p>Where:</p>
        <p>TP = True Positives (the sum of positive (foreground) pixels classified by the model)</p>
        <p>TN = True Negatives (sum of correctly classified negative pixels)</p>
        <p>FP = False Positives (the sum of negative (background) pixels misclassified by the model)</p>
        <p>FN = False Negatives (the sum of misclassified positive pixels)</p>
        <p>A = Area of ground truth pixels and A′ = Area of predicted pixels.</p>
        <p>High recall values indicate that most of pixels belonging to the positive class (lesions) are predicted correctly by U-Net segmentation models. High precision values across the three lesion types also demonstrate the U-Net models successfully differentiate between lesion foreground and non-lesion background regions. High accuracy and F1 scores values reflect the excellent model performance and robustness. IoU is a useful metric for image segmentation by measuring the overlap between predicted and ground truth segmentation masks. This can be done for a class by dividing the intersection (overlap) of ground truth and predicted pixels belonging to the class by the total number of pixels in both masks belonging to the class. IoU score ranges from [0–1], where scores closer to 1 indicate greater agreement between predicted and ground truth masks.</p>
      </sec>
    </sec>
    <sec>
      <title>Combination of TL and lesion features</title>
      <p>The DR-detector framework performs a fusion of the TL features with lesion features to get a better representation of the features used for detection of DR. This is achieved by concatenating the flat representation of the features obtained by TL model (either VGG-16 or ResNet-50) with three lesion features that captures EXs, MAs and HEMs <italic>via</italic> image segmentation technique (<xref rid="F1" ref-type="fig">Figure 1C</xref>).</p>
      <p>By combining the TL features and the lesion features, the resulting feature vector for each image in the training dataset that will be used for training the XGboost DR-detector model would be of size = 25,088 + 3 = 25,091 dim for VGG-16 and 100,352 + 3 = 100,355 dim for ResNet-50.</p>
    </sec>
    <sec>
      <title>Metrics for performance evaluation of DR detection</title>
      <p>The accuracy, precision, recall, F1-score and the area under the ROC curve (AUC) (<xref rid="B57" ref-type="bibr">57</xref>) have been used to evaluate the Xgboost model's performance deployed in the DR detector framework. The mathematical equations that define each of the first four metrics were previously defined in Equations (1–4) for evaluating the performance of U-Net segmentation models for lesion types. However, TP,FP,FN and TN terms for DR-detection task have a different indication from their previous annotation for evaluting the image segmentation task using U-Net model and hence are described below:</p>
      <p>TP refers to the number of correctly classified DR images.</p>
      <p>FP refers to the number of healthy images misclassified as DR images</p>
      <p>FN refers to the number of DR images misclassified as healthy images</p>
      <p>TN refers to the number of correctly classified healthy images.</p>
      <p>As for the AUC metric, it measures the entire two-dimensional area under the ROC curve (<xref rid="B58" ref-type="bibr">58</xref>) which measures how accurately the model can distinguish between DR and healthy images.</p>
    </sec>
    <sec>
      <title>Experimental setup and model development</title>
      <p>All experiments in this study were executed on Ubuntu Linux server with 128 GB of RAM, 16 Intel Xeon E5-2609 1.7GHZ CPU cores, and 8 GPU cards. The optimized distributed gradient boosting python library (<xref rid="B59" ref-type="bibr">59</xref>) has been used for implementing the XGBoost model. The scikit-learn toolkit<xref rid="fn0003" ref-type="fn"><sup>3</sup></xref>, the free machine learning python library has been used for implementing other ML models that were developed as a proof of concept to show that XGBoost was chosen because it outperforms other ML models (see <bold>Tables 4</bold>, <bold>5</bold>). The Keras free python library (<xref rid="B55" ref-type="bibr">55</xref>) with tensorflow back end (<xref rid="B56" ref-type="bibr">56</xref>) was used to implement the TL models as well as the deep neural network (DNN) models to compare the performances in DR detection to the XGboost as an evidence to show the outperformance of later model.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <sec>
      <title>U-net model performance evaluation</title>
      <p>U-Net model performance evaluation results are shown in <xref rid="T3" ref-type="table">Table 3</xref>. The table shows an outperformance of the U-Net model for predicting MAs and EXs lesions over HEMs lesions. In other words, the general trend we observed was that the performance evaluation results for the HEMs segmentation model were consistently lower than those for the MAs and EXs models. We attribute this to the variation in the appearance of retinal hemorrhages, which can range from small, concentrated regions (dot hemorrhages) to larger and more irregular shapes. Examples of U-Net model predictions are shown in <xref rid="SM2" ref-type="supplementary-material">Supplementary Figure 3</xref>.</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Performance evaluation results of UNET model on extracting each type of lesions feature with the best 5-fold cross validation accuracy from all trials.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Lesion type</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Recall</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Precision</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>F1-score</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Accuracy</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>IoU</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MAs</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.93</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.84</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EXs</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.94</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.98</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.83</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">HEMs</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.86</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.81</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>DR detection using VGG-16 and lesion features</title>
      <p>As a proof of concept, we developed different ML models and trained them on the combination of VGG-16 and lesion features including the support vector machine (SVM), K-Nearest Neighbors (KNN), Xgboost, Logistic Regression (LR), Multi-Layered Perceptron (MLP), Decision Tree (DT), and Random Forest (RF) with default settings in addition to a DNN model with different structures including one input layer with 128 nodes, one input layer with 256 nodes and one hidden layer with 128 nodes, and one input layer with 512 nodes and 2 hidden layers with 256 and 128 nodes, respectively. As shown in <xref rid="T4" ref-type="table">Table 4</xref>, XGBoost outperformed all other ML and DNN models achieving 99.38% accuracy in detecting DR.</p>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>The performance of all Ml models and DNN for detection of DR using a combination of <bold>VGG-16</bold> and retinal lesion features.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Classifier</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Accuracy %</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Precision</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Recall</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>F1-score</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Xgboost</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>99.38</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>1</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.99</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.99</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.994</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">KNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">47.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.25</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.475</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">LR</td>
              <td valign="top" align="center" rowspan="1" colspan="1">51.85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.57</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.54</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.519</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SVM</td>
              <td valign="top" align="center" rowspan="1" colspan="1">43.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.45</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.62</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.432</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MLP</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.93</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.66</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.531</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DT</td>
              <td valign="top" align="center" rowspan="1" colspan="1">88.27</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.00</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.77</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.87</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.883</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">RF</td>
              <td valign="top" align="center" rowspan="1" colspan="1">68.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.63</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.67</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.685</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DNN (1 layer)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.58</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.481</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DNN (2 layers)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.38</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.43</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.5</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DNN (3 layers)</td>
              <td valign="top" align="center" rowspan="1" colspan="1">52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.54</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.519</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The bold values highlight the best accuracy, precision, recall, F1-score, and AUC of ML and DNN learning models for detection of DR using a combination of VGG-16 and retinal lesion features.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>DR detection using ResNet-50 and lesion features</title>
      <p><xref rid="T5" ref-type="table">Table 5</xref> shows the performance evaluation results of all DR detection models using the combination of ResNet-50 and lesion features. As can be observed from the table, XGBoost continued to outperform all other ML and DNN models achieving 100% accuracy for detection of DR.</p>
      <table-wrap position="float" id="T5">
        <label>Table 5</label>
        <caption>
          <p>The performance of all Ml models and DNN for detection of DR using a combination of <bold>ResNet-50</bold> and retinal lesion features.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Classifier</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Accuracy %</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Precision</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Recall</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>F1-score</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>AUC</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Xgboost</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>100</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>1.00</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>1.00</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>1.00</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>1.0</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">KNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">42.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.37</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.27</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.426</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">LR</td>
              <td valign="top" align="center" rowspan="1" colspan="1">53.09</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.531</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SVM</td>
              <td valign="top" align="center" rowspan="1" colspan="1">45.68</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.84</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.457</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">MLP</td>
              <td valign="top" align="center" rowspan="1" colspan="1">47.53</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.95</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.64</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.475</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DT</td>
              <td valign="top" align="center" rowspan="1" colspan="1">96.30</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.93</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.96</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.963</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">RF</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.79</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.55</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.70</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.62</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.568</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">1 layer-DNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">43.8</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.42</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.37</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.44</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">2 layer-DNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">48</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.75</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.59</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.481</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">3 layer-DNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.46</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.47</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.488</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The bold values highlight the best accuracy, precision, recall, F1-score and AUC of ML and DNN learning models for detection of DR using a combination of Resnet-50 and retinal lesion features.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>Performance of DR-detector with a single type of feature to understand feature importance and contribution</title>
      <p>We decided to deploy XGBoost model in DR-detector framework for automatic detection of DR since it outperforms all other ML and DNN models as highlighted in the previous sections and documented in <xref rid="T4" ref-type="table">Tables 4</xref>, <xref rid="T5" ref-type="table">5</xref>. For deep analysis of the features that highly contribute to the performance of XGBoost model in the detection of DR, we have analyzed the importance of each feature by evaluating the XGBoost model performance with each type of extracted features including the TL and lesion features. This has been achieved by building three versions of the XGBoost model, where each version of the model is trained with only one type of feature. <xref rid="F2" ref-type="fig">Figure 2</xref> shows a bar chart that represents the performance of XGBoost with VGG-16, ResNet-50 and lesion features (<xref rid="SM1" ref-type="supplementary-material">Supplementary Table 1</xref>). Clearly, we see a significant outperformance of the lesion features over the TL features that have been either extracted by VGG-16 or Resnt-50 models. This highlights the importance of the clinically manifested symptoms reflected in the form of lesion features in the detection of DR and how they can complement the textural features extracted by the TL model to improve the XGBoost performance and its interpretability for clinical use in detecting the presence of DR disease.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>The performance of XGBoost with VGG-16, ResNet-50 and lesions features.</p>
        </caption>
        <graphic xlink:href="fmed-09-1050436-g0002" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Performance of DR-detector with all possible feature combinations</title>
      <p><xref rid="F3" ref-type="fig">Figure 3</xref> shows a bar chart that represents the performance of XGBoost (DR-detector model) on the testing dataset with all possible combinations of TL and lesion features (<xref rid="SM1" ref-type="supplementary-material">Supplementary Table 2</xref>). Clearly, the figure shows that (Resent-50 and lesion) feature combination leads to the best performance of XGBoost model among all combinations and is slightly better than the performance of the model using (VGG-16 and lesions), and (VGG-16, ResNet-50, and lesions) combinations that lead to equal model performances. The figure also shows poor performance of XGBoost with VGG-16 and ResNet-50 combination (i.e., when the lesion features are specifically excluded) which also highlights the importance of lesion features in the detection of DR.</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>The performance of XGBoost with all possible combinations of TL and lesion features.</p>
        </caption>
        <graphic xlink:href="fmed-09-1050436-g0003" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Lesion feature importance and their effect on XGBoost performance</title>
      <p>For deep analysis of the contribution of each lesion feature to the performance of DR-detector model in DR detection, we have reported the importance of each lesion feature through training and testing the XGBoost model with each type of lesion feature individually. This has been achieved by building three versions of the XGBoost model, where each version of the model is trained with one lesion feature. <xref rid="F4" ref-type="fig">Figure 4</xref> shows the performance of XGBoost with EXs, MAs and HEMs lesion features as well as with their combinations (<xref rid="SM1" ref-type="supplementary-material">Supplementary Table 3</xref>). Clearly, the table shows a significant outperformance of XGBoost model using HEMs lesion feature over its performance using either the MAs or EXs lesion features which equally contribute to the performance of the model. More importantly, the model also achieves better performance using HEMs alone than its performance using the combination of the three lesion features altogether. Thus, our results highlight the importance of HEMs as a feature in the detection of DR and how it can be used to allow the interpretability of the model for clinical use in DR detection.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>The performance of XGBoost with each type of lesion features as well as their combinations.</p>
        </caption>
        <graphic xlink:href="fmed-09-1050436-g0004" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>There are several observations that can be summarized from our experimental results. First, using our proposed approach we found that the integration of lesion features with the TL features significantly improves the performance of the DR-detector model and adds a clear importance to its clinical interpretability. However, currently it is not possible to only work with the lesion features for training the DR-detector model as there are few retinal imaging datasets in the literature that provide the image masks corresponding to the retinal images in the dataset that are needed to extract and quantify the lesion features using the U-Net segmentation models and obtaining those masks need the involvement of trained ophthalmologists which is costly and time-consuming.</p>
    <p>It was also observed that deploying the XGBoost model as an ensemble of ML classifiers in the DR-detector framework led to a better binary classification of DR and error detection than deploying dense classifiers of DL models as introduced in few previous studies (<xref rid="B20" ref-type="bibr">20</xref>, <xref rid="B37" ref-type="bibr">37</xref>, <xref rid="B38" ref-type="bibr">38</xref>). This motivated us to use the flat combination of TL and lesion features (<xref rid="F1" ref-type="fig">Figure 1C</xref>) which is simpler, straight forward and more convenient to be applied to the XGBoost ML model than combining both types of features using different pooling approaches (<xref rid="B20" ref-type="bibr">20</xref>), or by extending the last fully connected (FC) layer of TL model with additional number of neurons equal to the number of lesion features and using softmax function to obtain the predictive DR class probabilities (<xref rid="B37" ref-type="bibr">37</xref>).</p>
    <p>Regarding the contribution of lesion features to the DR-detector (XGBoost) model performance, our approach shows that lesion features have different contributions to the model performance. Particularly, it shows a significant contribution of the hemorrhages over the other two lesion features for binary identification of DR (<xref rid="SM1" ref-type="supplementary-material">Supplementary Table 3</xref>). This is likely because hemorrhages may arise early during the progression of DR and are associated with the worsening of vision and the development of other vision-threatening lesions. Our observations highlight the importance of such a feature to train the binary classifier of DR-detector framework and adds value for its clinical usage in DR diagnosis. However, it remains to be seen how our presented results will hold when testing our lesion feature-based models on different fundus retinal testing datasets with more complex demographics and varying quality.</p>
    <p>It is also noteworthy to mention that although we have applied the DR-detector framework to a relatively small training and testing datasets in comparison with the large datasets that have been used in the literature for DR detection [e.g.,(<xref rid="B17" ref-type="bibr">17</xref>)] our datasets are from heterogenous sources i.e., have different variety of retinal images that were imported from multiple publicly available datasets with different settings of capturing the fundus retinal images. This, of course, highlights the efficiency of our proposed framework in detecting the initial signs of DR, even when applied to a set of retinal fundus images that were not imported from the same resource. Thus, we expect a better performance of DR-detector framework with larger training and testing datasets in the future.</p>
  </sec>
  <sec id="s5">
    <title>Conclusions and future work</title>
    <p>In this study, we have proposed a framework called DR-detector that combines the features extracted from fundus retinal image by transfer learning model and the lesion features extracted using semantic image segmentation <italic>via</italic> U-Net DL model for accurate detection of DR using the XGboost ML model deployed in this framework. The model was trained using the combination of both features on a training dataset collected from various publicly available datasets and was tested on an external dataset that consists of DR images from IDRID dataset and healthy images from HRF and Messior-2 dataset. Our experimental results show that our proposed framework for DR detection achieves a testing accuracy of 100% in detecting DR using the combination of Resnt-50 and lesion features and 99% testing accuracy using the combination of VGG-16 and lesion features. Based on these results, we arrived at the conclusion that the extracted clinically relevant lesion features have a significant impact on the performance of the DR-detector model and would be an excellent complement to the textural features extracted by the TL model to improve the performance of DR-detector model and its interpretability for clinical use for detecting the presence of DR disease.</p>
    <p>We anticipate a natural extension of our current work is to first extend to other TL models that are commonly used in the literature to study how they perform for DR classification task and then expand our general framework to be applied for the prediction of different severity levels of DR. Finally, we are looking forward to applying our approach to combine TL features with other types of DR lesion features such as cotton wool spots, foveal avascular zone, optic disc, and retinal blood vessels since these features were known in the literature to be associated with greater severity of DR (<xref rid="B60" ref-type="bibr">60</xref>). This might lead to better performance of the DR-detector model and its interpretability for clinical use on much larger and clinically diverse datasets, especially if it will be extended to predict the various grades of DR.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are included in the article/<xref rid="SM1" ref-type="supplementary-material">Supplementary material</xref>, further inquiries can be directed to the corresponding author/s.</p>
  </sec>
  <sec id="s7">
    <title>Author contributions</title>
    <p>DH, HG, MH, AB, AH, and SJ conceived and designed the study. DH did the TL feature extraction using VGG-16 and ResNet-50 models and combined TL and lesion features for training and evaluating the XGboost model of DR-detector framework. HG did the lesion feature extraction using image segmentation <italic>via</italic> U-Net model. DH and HG wrote the manuscript and contributed to the DR-detector GitHub repository. AB, AH, and SJ revised the manuscript and provided valuable comments. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s8">
    <title>Funding</title>
    <p>This research was funded by the National Eye Institute of the NIH under Award Number R01EY032080 (AB, AH, and SJ) and a pilot grant IUPUI Institute of Integrative Artificial Intelligence (iAI) (SJ and AH). This work was supported by the IUPUI Institute of Integrative Artificial Intelligence (iAI). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>AB is an <italic>ad hoc</italic> Staff Pharmacist at CVS Health/Aetna. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher's note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec id="s10">
    <title>Author disclaimer</title>
    <p>The content of this study does not reflect those of CVS Health/Aetna.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="fn0001">
      <p>
        <sup>1</sup>
        <ext-link xlink:href="https://www.kaggle.com/c/aptos2019-blindness-detection/data" ext-link-type="uri">https://www.kaggle.com/c/aptos2019-blindness-detection/data</ext-link>
      </p>
    </fn>
    <fn id="fn0002">
      <p>
        <sup>2</sup>
        <ext-link xlink:href="https://www.adcis.net/en/third-party/e-ophtha/" ext-link-type="uri">https://www.adcis.net/en/third-party/e-ophtha/</ext-link>
      </p>
    </fn>
    <fn id="fn0003">
      <p>
        <sup>3</sup>
        <ext-link xlink:href="https://scikit-learn.org/stable/" ext-link-type="uri">https://scikit-learn.org/stable/</ext-link>
      </p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s11">
    <title>Supplementary material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fmed.2022.1050436/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fmed.2022.1050436/full#supplementary-material</ext-link></p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_1.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="SM2" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_2.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartnett</surname><given-names>ME</given-names></name><name><surname>Baehr</surname><given-names>W</given-names></name><name><surname>Le</surname><given-names>YZ</given-names></name></person-group>. <article-title>Diabetic retinopathy, an overview</article-title>. <source>Vision Res.</source> (<year>2017</year>) <volume>139</volume>:<fpage>1</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1016/j.visres.2017.07.006</pub-id><?supplied-pmid 28757399?><pub-id pub-id-type="pmid">28757399</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nentwich</surname><given-names>MM</given-names></name><name><surname>Ulbig</surname><given-names>MW</given-names></name></person-group>. <article-title>Diabetic retinopathy - ocular complications of diabetes mellitus</article-title>. <source>World J Diabetes.</source> (<year>2015</year>) <volume>6</volume>:<fpage>489</fpage>–<lpage>99</lpage>. <pub-id pub-id-type="doi">10.4239/wjd.v6.i3.489</pub-id><?supplied-pmid 34032037?><pub-id pub-id-type="pmid">25897358</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matuszewski</surname><given-names>W</given-names></name><name><surname>Baranowska-Jurkun</surname><given-names>A</given-names></name><name><surname>Stefanowicz-Rutkowska</surname><given-names>MM</given-names></name><name><surname>Modzelewski</surname><given-names>R</given-names></name><name><surname>Pieczyński</surname><given-names>J</given-names></name><name><surname>Bandurska-Stankiewicz</surname><given-names>E</given-names></name></person-group>. <article-title>Prevalence of diabetic retinopathy in type 1 and type 2 diabetes mellitus patients in North-East Poland</article-title>. <source>Medicina</source>. (<year>2020</year>) <volume>56</volume>:<fpage>164</fpage>. <pub-id pub-id-type="doi">10.3390/medicina56040164</pub-id><?supplied-pmid 32268561?><pub-id pub-id-type="pmid">32268561</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hietala</surname><given-names>K</given-names></name><name><surname>Harjutsalo</surname><given-names>V</given-names></name><name><surname>Forsblom</surname><given-names>C</given-names></name><name><surname>Summanen</surname><given-names>P</given-names></name><name><surname>Groop</surname><given-names>PH</given-names></name><collab>FinnDiane Study Group</collab></person-group>. <article-title>Age at onset and the risk of proliferative retinopathy in type 1 diabetes</article-title>. <source>Diabetes Care.</source> (<year>2010</year>) <volume>33</volume>:<fpage>1315</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.2337/dc09-2278</pub-id><?supplied-pmid 20185730?><pub-id pub-id-type="pmid">20185730</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossniklaus</surname><given-names>HE</given-names></name><name><surname>Geisert</surname><given-names>EE</given-names></name><name><surname>Nickerson</surname><given-names>JM</given-names></name></person-group>. <article-title>Introduction to the retina</article-title>. <source>Prog Mol Biol Transl Sci.</source> (<year>2015</year>) <volume>134</volume>:<fpage>383</fpage>–<lpage>96</lpage>. <pub-id pub-id-type="doi">10.1016/bs.pmbts.2015.06.001</pub-id><?supplied-pmid 26310166?><pub-id pub-id-type="pmid">26310166</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovoor</surname><given-names>E</given-names></name><name><surname>Chauhan</surname><given-names>SK</given-names></name><name><surname>Hajrasouliha</surname><given-names>A</given-names></name></person-group>. <article-title>Role of inflammatory cells in pathophysiology and management of diabetic retinopathy</article-title>. <source>Surv Ophthalmol</source>. (<year>2022</year>) <volume>67</volume>:<fpage>1563</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1016/j.survophthal.2022.07.008</pub-id><?supplied-pmid 35914582?><pub-id pub-id-type="pmid">35914582</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eisma</surname><given-names>JH</given-names></name><name><surname>Dulle</surname><given-names>JE</given-names></name><name><surname>Fort</surname><given-names>PE</given-names></name></person-group>. <article-title>Current knowledge on diabetic retinopathy from human donor tissues</article-title>. <source>World J Diabetes.</source> (<year>2015</year>) <volume>6</volume>:<fpage>312</fpage>–<lpage>20</lpage>. <pub-id pub-id-type="doi">10.4239/wjd.v6.i2.312</pub-id><?supplied-pmid 25789112?><pub-id pub-id-type="pmid">25789112</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>A</given-names></name></person-group>. <article-title>Diabetic retinopathy: battling the global epidemic</article-title>. <source>Invest Ophthalmol Vis Sci</source>. (<year>2016</year>) <volume>57</volume>:<fpage>6669</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1167/iovs.16-21031</pub-id><?supplied-pmid 27936469?><pub-id pub-id-type="pmid">27936469</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alban</surname><given-names>M</given-names></name><name><surname>Gilligan</surname><given-names>T</given-names></name></person-group>. <source>Automated Detection of Diabetic Retinopathy Using Fluorescein Angiography Photographs</source>. Standford Education (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kermany</surname><given-names>DS</given-names></name><name><surname>Goldbaum</surname><given-names>M</given-names></name><name><surname>Cai</surname><given-names>W</given-names></name><name><surname>Valentim</surname><given-names>CCS</given-names></name><name><surname>Liang</surname><given-names>H</given-names></name><name><surname>Baxter</surname><given-names>SL</given-names></name><etal/></person-group>. <article-title>Identifying medical diagnoses and treatable diseases by image-based deep learning</article-title>. <source>Cell.</source> (<year>2018</year>) <volume>172</volume>:<fpage>1122</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2018.02.010</pub-id><?supplied-pmid 29474911?><pub-id pub-id-type="pmid">29474911</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lam</surname><given-names>C</given-names></name><name><surname>Yi</surname><given-names>D</given-names></name><name><surname>Guo</surname><given-names>M</given-names></name><name><surname>Lindsey</surname><given-names>T</given-names></name></person-group>. <article-title>Automated detection of diabetic retinopathy using deep learning</article-title>. <source>AMIA Jt Summits Transl Sci Proc</source>. (<year>2018</year>) <volume>2018</volume>:<fpage>147</fpage>–<lpage>55</lpage>. Available online at: <ext-link xlink:href="https://pubmed.ncbi.nlm.nih.gov/29888061/" ext-link-type="uri">https://pubmed.ncbi.nlm.nih.gov/29888061/</ext-link></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aswathi</surname><given-names>T</given-names></name><name><surname>Swapna</surname><given-names>TR</given-names></name><name><surname>Padmavathi</surname><given-names>S</given-names></name></person-group>. <article-title>Transfer learning approach for grading of Diabetic Retinopathy</article-title>. <source>J Phys</source>. (<year>2021</year>) <volume>1767</volume>:<fpage>012033</fpage>. <pub-id pub-id-type="doi">10.1088/1742-6596/1767/1/012033</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname></name><name><surname>A.</surname></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2017</year>). <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>Commun. ACM.</source>
<volume>60</volume>, <fpage>84</fpage>–<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1145/3065386</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansour</surname><given-names>RF</given-names></name></person-group>. <article-title>Deep-learning-based automatic computer-aided diagnosis system for diabetic retinopathy</article-title>. <source>Biomed Eng Lett</source>. (<year>2018</year>) <volume>8</volume>:<fpage>41</fpage>–<lpage>57</lpage>. <pub-id pub-id-type="doi">10.1007/s13534-017-0047-y</pub-id><?supplied-pmid 30603189?><pub-id pub-id-type="pmid">30603189</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group>. <article-title>Very deep convolutional networks for large scale image recognition</article-title>. In: <source>Proceedings of ICLR</source>. San Diego, CA (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mateen</surname><given-names>M</given-names></name><name><surname>Wen</surname><given-names>J</given-names></name><name><surname>Nasrullah</surname></name><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name></person-group>. <article-title>Fundus image classification using VGG-19 architecture with PCA and SVD</article-title>. <source>Symmetry</source>. (<year>2019</year>) <volume>11</volume>:<fpage>1</fpage>. <pub-id pub-id-type="doi">10.3390/sym11010001</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yaqoob</surname><given-names>MK</given-names></name><name><surname>Ali</surname><given-names>SF</given-names></name><name><surname>Bilal</surname><given-names>M</given-names></name><name><surname>Hanif</surname><given-names>MS</given-names></name><name><surname>Al-Saggaf</surname><given-names>UM</given-names></name></person-group>. <article-title>ResNet based deep features and random forest classifier for diabetic retinopathy detection</article-title>. <source>Sensors.</source> (<year>2021</year>) <volume>21</volume>:<fpage>3883</fpage>. <pub-id pub-id-type="doi">10.3390/s21113883</pub-id><?supplied-pmid 34199873?><pub-id pub-id-type="pmid">34199873</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group>. <article-title>Deep residual learning for image recognition</article-title>. In: <source>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Las Vegas, NV</publisher-loc> (<year>2016</year>). <?supplied-pmid 32166560?></mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bodapati</surname><given-names>JD</given-names></name><name><surname>Shaik</surname><given-names>NS</given-names></name><name><surname>Naralasetti</surname><given-names>V</given-names></name></person-group>. <article-title>Deep convolution feature aggregation: an application to diabetic retinopathy severity level prediction</article-title>. <source>Signal Image Video Process</source>. (<year>2021</year>) <volume>15</volume>:<fpage>923</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1007/s11760-020-01816-y</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bodapati</surname><given-names>JD</given-names></name><name><surname>Veeranjaneyulu</surname><given-names>N</given-names></name><name><surname>Shareef</surname><given-names>SN</given-names></name><name><surname>Hakak</surname><given-names>S</given-names></name><name><surname>Bilal</surname><given-names>M</given-names></name><name><surname>Maddikunta</surname><given-names>PKR</given-names></name><etal/></person-group>. <article-title>Blended multi-modal deep convnet features for diabetic retinopathy severity prediction</article-title>. <source>Electronics.</source> (<year>2020</year>) <volume>9</volume>:<fpage>914</fpage>. <pub-id pub-id-type="doi">10.3390/electronics9060914</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liskowski</surname><given-names>P</given-names></name></person-group>. <article-title>Krawiec K. Segmenting retinal blood vessels with deep neural networks</article-title>. <source>IEEE Trans Med Imaging.</source> (<year>2016</year>) <volume>35</volume>:<fpage>2369</fpage>–<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2546227</pub-id><?supplied-pmid 27046869?><pub-id pub-id-type="pmid">27046869</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>B</given-names></name><etal/></person-group>. <article-title>Learn to segment retinal lesions and beyond</article-title>. In: <source>Proceedings of 2020 25th International Conference on Pattern Recognition (ICPR)</source>. <publisher-loc>Milan, Italy</publisher-loc> (<year>2021</year>). p. <fpage>7403</fpage>–<lpage>10</lpage>. <?supplied-pmid 32789526?></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>N</given-names></name><name><surname>Aggarwal</surname><given-names>LM</given-names></name></person-group>. <article-title>Automated medical image segmentation techniques</article-title>. <source>J Med Phys.</source> (<year>2010</year>) <volume>35</volume>:<fpage>3</fpage>–<lpage>14</lpage>. <pub-id pub-id-type="doi">10.4103/0971-6203.58777</pub-id><?supplied-pmid 20177565?><pub-id pub-id-type="pmid">20177565</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Song</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name></person-group>. <article-title>A review of deep-learning-based medical image segmentation methods</article-title>. <source>Sustainability.</source> (<year>2021</year>) <volume>13</volume>:<fpage>1224</fpage>. <pub-id pub-id-type="doi">10.3390/su13031224</pub-id><?supplied-pmid 33992856?><pub-id pub-id-type="pmid">33992856</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group>. <article-title>U-net: convolutional networks for biomedical image segmentation</article-title>. In: <person-group person-group-type="editor"><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Hornegger</surname><given-names>J</given-names></name><name><surname>Wells</surname><given-names>W</given-names></name><name><surname>Frangi</surname><given-names>A</given-names></name></person-group> editors. <source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer</publisher-name> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>MCS</given-names></name><name><surname>Teoh</surname><given-names>SS</given-names></name><name><surname>Ibrahim</surname><given-names>H</given-names></name><name><surname>Embong</surname><given-names>Z</given-names></name></person-group>. <article-title>Neovascularization detection and localization in fundus images using deep learning</article-title>. <source>Sensors</source>. (<year>2021</year>) <volume>21</volume>:<fpage>5327</fpage>. <pub-id pub-id-type="doi">10.3390/s21165327</pub-id><?supplied-pmid 34450766?><pub-id pub-id-type="pmid">34450766</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Burewar</surname><given-names>S</given-names></name><name><surname>Gonde</surname><given-names>AB</given-names></name><name><surname>Vipparthi</surname><given-names>SK</given-names></name></person-group>. <article-title>Diabetic retinopathy detection by retinal segmentation with region merging using CNN</article-title>. In: <source>2018 IEEE 13th International Conference on Industrial and Information Systems (ICIIS)</source>. <publisher-loc>Rupnagar</publisher-loc> (<year>2018</year>). p. <fpage>136</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kou</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Liang</surname><given-names>W</given-names></name><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Hao</surname><given-names>J</given-names></name></person-group>. <article-title>Microaneurysms segmentation with a U-Net based on recurrent residual convolutional neural network</article-title>. <source>J Med Imaging.</source> (<year>2019</year>) <volume>6</volume>:<fpage>025008</fpage>. <pub-id pub-id-type="doi">10.1117/1.JMI.6.2.025008</pub-id><?supplied-pmid 31259200?><pub-id pub-id-type="pmid">31259200</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albahli</surname><given-names>S</given-names></name><name><surname>Ahmad Hassan Yar</surname><given-names>GN</given-names></name></person-group>. <article-title>Automated detection of diabetic retinopathy using custom convolutional neural network</article-title>. <source>J Xray Sci Technol</source>. (<year>2022</year>) <volume>30</volume>:<fpage>275</fpage>–<lpage>91</lpage>. <pub-id pub-id-type="doi">10.3233/XST-211073</pub-id><?supplied-pmid 35001904?><pub-id pub-id-type="pmid">35001904</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Fan</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name></person-group>. <article-title>An intelligent segmentation and diagnosis method for diabetic retinopathy based on improved U-NET network</article-title>. <source>J Med Syst.</source> (<year>2019</year>) <volume>43</volume>:<fpage>304</fpage>. <pub-id pub-id-type="doi">10.1007/s10916-019-1432-0</pub-id><?supplied-pmid 31407110?><pub-id pub-id-type="pmid">31407110</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Cao</surname><given-names>S</given-names></name></person-group>. <article-title>Multi-path recurrent U-Net segmentation of retinal fundus image</article-title>. <source>Appl Sci</source>. (<year>2020</year>) <volume>10</volume>:<fpage>3777</fpage>. <pub-id pub-id-type="doi">10.3390/app10113777</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kou</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Yu</surname><given-names>Z</given-names></name><name><surname>Yuan</surname><given-names>L</given-names></name></person-group>. <article-title>An enhanced residual U-Net for microaneurysms and exudates segmentation in fundus images</article-title>. <source>IEEE Access</source>. (<year>2020</year>) <volume>8</volume>:<fpage>185514</fpage>–<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3029117</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahebi</surname><given-names>J</given-names></name><name><surname>Hardalaç</surname><given-names>F</given-names></name></person-group>. <article-title>Retinal blood vessel segmentation with neural network by using gray-level co-occurrence matrix-based features</article-title>. <source>J Med Syst.</source> (<year>2014</year>) <volume>38</volume>:<fpage>85</fpage>. <pub-id pub-id-type="doi">10.1007/s10916-014-0085-2</pub-id><?supplied-pmid 24957399?><pub-id pub-id-type="pmid">24957399</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>N</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Wei</surname><given-names>P</given-names></name></person-group>. <article-title>FFU-Net: Feature Fusion U-Net for lesion segmentation of diabetic retinopathy</article-title>. <source>BioMed Res Int</source>. (<year>2021</year>) <volume>2021</volume>:<fpage>6644071</fpage>. <pub-id pub-id-type="doi">10.1155/2021/6644071</pub-id><?supplied-pmid 33490274?><pub-id pub-id-type="pmid">33490274</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zong</surname><given-names>Y</given-names></name></person-group>. <article-title>U-net based method for automatic hard exudates segmentation in fundus images using inception module and residual connection</article-title>. <source>IEEE Access</source>. (<year>2020</year>) <volume>8</volume>:<fpage>167225</fpage>–<lpage>35</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3023273</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erwin</surname><given-names>S</given-names></name><name><surname>Desiani</surname><given-names>A</given-names></name><name><surname>Suprihatin</surname><given-names>BF</given-names></name></person-group>. <article-title>The augmentation data of retina image for blood vessel segmentation using U-Net convolutional neural network method</article-title>. <source>Int J Comput Intell Appl</source>. (<year>2022</year>) <volume>21</volume>:<fpage>2250004</fpage>. <pub-id pub-id-type="doi">10.1142/S1469026822500043</pub-id><?supplied-pmid 35279601?><pub-id pub-id-type="pmid">35279601</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harangi</surname><given-names>B</given-names></name><name><surname>Toth</surname><given-names>J</given-names></name><name><surname>Baran</surname><given-names>A</given-names></name><name><surname>Hajdu</surname><given-names>A</given-names></name></person-group>. <article-title>Automatic screening of fundus images using a combination of convolutional neural network and hand-crafted features</article-title>. In: <source>2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), IEEE</source>. <publisher-loc>Berlin</publisher-loc> (<year>2019</year>). p. <fpage>2699</fpage>–<lpage>702</lpage>. <?supplied-pmid 31946452?></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacsovics</surname><given-names>G</given-names></name><name><surname>Toth</surname><given-names>J</given-names></name><name><surname>Hajdu</surname><given-names>A</given-names></name><name><surname>Harangi</surname><given-names>B</given-names></name></person-group>. <article-title>Enhancing CNNs through the use of hand-crafted features in automated fundus image classification</article-title>. <source>Biomed Signal Process Control.</source> (<year>2022</year>) <volume>76</volume>:<fpage>103685</fpage>. <pub-id pub-id-type="doi">10.1016/j.bspc.2022.103685</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Guestrin</surname><given-names>C</given-names></name></person-group>. <article-title>XGBoost: a scalable tree boosting system</article-title>. In: <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16)</source>. <publisher-loc>San Francisco, CA</publisher-loc> (<year>2016</year>). <?supplied-pmid 32561836?></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>L-J</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group>. (<year>2009</year>). <article-title>ImageNet: A large-scale hierarchical image database</article-title>, in <source>Proceedings of 2009 IEEE Conference on Computer Vision and Pattern Recognition (IEEE)</source>, <fpage>248</fpage>–<lpage>255</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khojasteh</surname><given-names>P</given-names></name><name><surname>Aliahmad</surname><given-names>B</given-names></name><name><surname>Kumar</surname><given-names>DK</given-names></name></person-group>. <article-title>Fundus images analysis using deep features for detection of exudates, hemorrhages and microaneurysms</article-title>. <source>BMC Ophthalmol</source>. (<year>2018</year>) 18: 288. <pub-id pub-id-type="doi">10.1186/s12886-018-0954-4</pub-id><?supplied-pmid 30400869?><pub-id pub-id-type="pmid">30400869</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>B</given-names></name><etal/></person-group>. <article-title>Learn to segment retinal lesions and beyond</article-title>. In: <source>Proceedings of International Conference on Pattern Recognition (ICPR)</source>. (<year>2013</year>) <volume>34</volume>:<fpage>196</fpage>–<lpage>203</lpage>. <?supplied-pmid 32789526?><pub-id pub-id-type="pmid">32789526</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="webpage"><source>Retinal Lesions</source>. Available online at: <ext-link xlink:href="https://github.com/WeiQijie/retinal-lesions" ext-link-type="uri">https://github.com/WeiQijie/retinal-lesions</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="webpage"><source>Aptos 2019- Blindness Detection</source>. Available online at: <ext-link xlink:href="https://www.kaggle.com/c/aptos2019-blindness-detection/data" ext-link-type="uri">https://www.kaggle.com/c/aptos2019-blindness-detection/data</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="webpage"><source>High Resolution Fundus Image Database</source>. Available online at: <ext-link xlink:href="https://www5.cs.fau.de/research/data/fundus-images/" ext-link-type="uri">https://www5.cs.fau.de/research/data/fundus-images/</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B46">
      <label>46.</label>
      <mixed-citation publication-type="webpage"><source>Diabetic Retinopathy – Segmentation and Grading Challenge</source>. Available online at: <ext-link xlink:href="https://idrid.grand-challenge.org" ext-link-type="uri">https://idrid.grand-challenge.org</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B47">
      <label>47.</label>
      <mixed-citation publication-type="webpage"><source>Messidor – 2</source>. Available online at: <ext-link xlink:href="https://www.adcis.net/en/third-party/messidor2/" ext-link-type="uri">https://www.adcis.net/en/third-party/messidor2/</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjølie</surname><given-names>AK</given-names></name><name><surname>Klein</surname><given-names>R</given-names></name><name><surname>Porta</surname><given-names>M</given-names></name><name><surname>Orchard</surname><given-names>T</given-names></name><name><surname>Fuller</surname><given-names>J</given-names></name><name><surname>Parving</surname><given-names>HH</given-names></name><etal/></person-group>. <article-title>Retinal microaneurysm count predicts progression and regression of diabetic retinopathy. Post-hoc results from the DIRECT Programme</article-title>. <source>Diabet Med.</source> (<year>2011</year>) <volume>28</volume>:<fpage>345</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1111/j.1464-5491.2010.03210.x</pub-id><?supplied-pmid 21309844?><pub-id pub-id-type="pmid">21309844</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Idiculla</surname><given-names>J</given-names></name><name><surname>Nithyanandam</surname><given-names>S</given-names></name><name><surname>Joseph</surname><given-names>M</given-names></name><name><surname>Mohan</surname><given-names>VA</given-names></name><name><surname>Vasu</surname><given-names>U</given-names></name><name><surname>Sadiq</surname><given-names>M</given-names></name></person-group>. <article-title>Serum lipids and diabetic retinopathy: a cross-sectional study</article-title>. <source>Indian J Endocrinol Metab.</source> (<year>2012</year>) <volume>16</volume>:<fpage>S492</fpage>–<lpage>4</lpage>. <pub-id pub-id-type="doi">10.4103/2230-8210.104142</pub-id><?supplied-pmid 23565476?><pub-id pub-id-type="pmid">23565476</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murugesan</surname><given-names>N</given-names></name><name><surname>Üstunkaya</surname><given-names>T</given-names></name><name><surname>Feener</surname><given-names>EP</given-names></name></person-group>. <article-title>Thrombosis and hemorrhage in diabetic retinopathy: a perspective from an inflammatory standpoint</article-title>. <source>Semin Thromb Hemost.</source> (<year>2015</year>) <volume>41</volume>:<fpage>659</fpage>–<lpage>64</lpage>. <pub-id pub-id-type="doi">10.1055/s-0035-1556731</pub-id><?supplied-pmid 26305236?><pub-id pub-id-type="pmid">26305236</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <label>51.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zuiderveld</surname><given-names>K</given-names></name></person-group>. <article-title>Contrast limited adaptive histogram equalization</article-title>. In: <person-group person-group-type="editor"><name><surname>Heckbert</surname><given-names>PS</given-names></name></person-group> editor. <source>Graphics Gems IV</source>. <publisher-name>Academic Press Professional, Inc</publisher-name> (<year>1994</year>). p. <fpage>474</fpage>–<lpage>85</lpage>.</mixed-citation>
    </ref>
    <ref id="B52">
      <label>52.</label>
      <mixed-citation publication-type="webpage"><source>tf.keras.preprocessing.image.ImageDateGenerator</source>. Available online at: <ext-link xlink:href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator" ext-link-type="uri">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B53">
      <label>53.</label>
      <mixed-citation publication-type="webpage"><source>Canny Edge Detection- OpenCV</source>. Available online at: <ext-link xlink:href="https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html" ext-link-type="uri">https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B54">
      <label>54.</label>
      <mixed-citation publication-type="webpage"><source>Contour Detection Using OpenCV (Python/C++)</source>. Available online at: <ext-link xlink:href="https://learnopencv.com/contour-detection-using-opencv-python-c/" ext-link-type="uri">https://learnopencv.com/contour-detection-using-opencv-python-c/</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B55">
      <label>55.</label>
      <mixed-citation publication-type="webpage"><source>Keras: The Python Deep Learning API</source>. Available online at: <ext-link xlink:href="https://keras.io" ext-link-type="uri">https://keras.io</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B56">
      <label>56.</label>
      <mixed-citation publication-type="webpage"><source>Tensorflow</source>. Available online at: <ext-link xlink:href="https://www.tensorflow.org/" ext-link-type="uri">https://www.tensorflow.org/</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrew</surname><given-names>E</given-names></name></person-group>. <article-title>Bradley. The use of the area under the roc curve in the evaluation of machine learning algorithms</article-title>. <source>Pattern Recogn</source>. (<year>1997</year>) <volume>30</volume>:<fpage>1145</fpage>–<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1016/S0031-3203(96)00142-2</pub-id><?supplied-pmid 25979036?><pub-id pub-id-type="pmid">25979036</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <label>58.</label>
      <mixed-citation publication-type="webpage"><source>Receiver Operating Characteristic</source>. Available online at: <ext-link xlink:href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" ext-link-type="uri">https://en.wikipedia.org/wiki/Receiver_operating_characteristic</ext-link> (accessed October 30, 2022).</mixed-citation>
    </ref>
    <ref id="B59">
      <label>59.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A</given-names></name></person-group>. <source>Complete Guide to Parameter Tuning in XGBoost With Codes in Python</source>. (<year>2016</year>). Available online at: <ext-link xlink:href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" ext-link-type="uri">https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</ext-link></mixed-citation>
    </ref>
    <ref id="B60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>D</given-names></name><name><surname>Biswas</surname><given-names>SK</given-names></name><name><surname>Bandyopadhyay</surname><given-names>S</given-names></name></person-group>. <article-title>A critical review on diagnosis of diabetic retinopathy using machine learning and deep learning</article-title>. <source>Multimed Tools Appl.</source> (<year>2022</year>) <volume>23</volume>:<fpage>1</fpage>–<lpage>43</lpage>. <pub-id pub-id-type="doi">10.1007/s11042-022-12642-4</pub-id><?supplied-pmid 35342328?><pub-id pub-id-type="pmid">35342328</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
