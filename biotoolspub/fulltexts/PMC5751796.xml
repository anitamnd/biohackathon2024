<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5751796</article-id>
    <article-id pub-id-type="publisher-id">1972</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-017-1972-6</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CNN-BLPred: a Convolutional neural network based predictor for β-Lactamases (BL) and their classes</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>White</surname>
          <given-names>Clarence</given-names>
        </name>
        <address>
          <email>crwhite1@aggies.ncat.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ismail</surname>
          <given-names>Hamid D.</given-names>
        </name>
        <address>
          <email>hdismail@ncat.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Saigo</surname>
          <given-names>Hiroto</given-names>
        </name>
        <address>
          <email>saigo@inf.kyushu-u.ac.jp</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>KC</surname>
          <given-names>Dukka B.</given-names>
        </name>
        <address>
          <email>dbkc@ncat.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0287 4439</institution-id><institution-id institution-id-type="GRID">grid.261037.1</institution-id><institution>Department of Computational Science and Engineering, </institution><institution>North Carolina A&amp;T State University, </institution></institution-wrap>Greensboro, NC 27411 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2242 4849</institution-id><institution-id institution-id-type="GRID">grid.177174.3</institution-id><institution>Faculty of Information Science and Electrical Engineering, </institution><institution>Kyushu University, </institution></institution-wrap>744 Motooka, Nishi-ku, Fukuoka, 819-0395 Japan </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>12</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>12</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>18</volume>
    <issue>Suppl 16</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>577</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s). 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The β-Lactamase (BL) enzyme family is an important class of enzymes that plays a key role in bacterial resistance to antibiotics. As the newly identified number of BL enzymes is increasing daily, it is imperative to develop a computational tool to classify the newly identified BL enzymes into one of its classes. There are two types of classification of BL enzymes: Molecular Classification and Functional Classification. Existing computational methods only address Molecular Classification and the performance of these existing methods is unsatisfactory.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We addressed the unsatisfactory performance of the existing methods by implementing a Deep Learning approach called Convolutional Neural Network (CNN). We developed CNN-BLPred, an approach for the classification of BL proteins. The CNN-BLPred uses Gradient Boosted Feature Selection (GBFS) in order to select the ideal feature set for each BL classification. Based on the rigorous benchmarking of CCN-BLPred using both leave-one-out cross-validation and independent test sets, CCN-BLPred performed better than the other existing algorithms.</p>
        <p id="Par3">Compared with other architectures of CNN, Recurrent Neural Network, and Random Forest, the simple CNN architecture with only one convolutional layer performs the best. After feature extraction, we were able to remove ~95% of the 10,912 features using Gradient Boosted Trees. During 10-fold cross validation, we increased the accuracy of the classic BL predictions by 7%. We also increased the accuracy of Class A, Class B, Class C, and Class D performance by an average of 25.64%. The independent test results followed a similar trend.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par4">We implemented a deep learning algorithm known as Convolutional Neural Network (CNN) to develop a classifier for BL classification. Combined with feature selection on an exhaustive feature set and using balancing method such as Random Oversampling (ROS), Random Undersampling (RUS) and Synthetic Minority Oversampling Technique (SMOTE), CNN-BLPred performs significantly better than existing algorithms for BL classification.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.1186/s12859-017-1972-6) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Beta lactamase protein classification</kwd>
      <kwd>Feature selection</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
    <conference>
      <conf-name>16th International Conference on Bioinformatics (InCoB 2017)</conf-name>
      <conf-acronym>InCoB 2017</conf-acronym>
      <conf-loc>Shenzhen, China</conf-loc>
      <conf-date>20-22 September 2017</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <sec id="Sec2">
      <title>β<italic>-lactamases family</italic></title>
      <p id="Par13">β-lactam antibiotics are an important class of drugs that are used to treat various pathogenic bacteria to treat bacterial infections. However, over the course of time, bacteria naturally develop resistance against antibiotics. Antibiotic resistance continues to threaten our ability to cope with the pace of development of new antibiotic drugs [<xref ref-type="bibr" rid="CR1">1</xref>].</p>
      <p id="Par14">One of the major bacterial enzymes that hinders the effort to produce new antibiotic drugs of the β-lactam family is the β-lactamase (BL) enzyme. The BL enzyme family has a chemically diverse set of substrates. BL develops resistance to penicillin and related antibiotics by hydrolyzing their conserved 4-atom β-lactam moiety, thus destroying their antibiotic activity [<xref ref-type="bibr" rid="CR2">2</xref>]. β-lactam antibiotics effectively inhibit bacterial transpeptidases, hence, they are also referred to as penicillin binding proteins (PBP). Bacteria have evolved BL enzymes to defend themselves against B-lactam antibiotics. This transformation causes the BL enzyme family to have varying degrees of antibiotic resistance activity. Once a BL enzyme is identified, it can be inhibited by a drug known as clavulanic acid. Clavulanic acid is a naturally produced BL inhibitor discovered in 1976, and when combined with β-lactams, it prevents hydrolysis of the Beta-Lactams. Pathogens develop resistance by modifying or replacing the target proteins and acquiring new BLs. This results in an increasing number of BLs, BL variants, and a widening gap between newly discovered BL protein sequences and their annotations.</p>
      <p id="Par15">The current classification schemes for BL enzymes are molecular classification and functional grouping. The molecular classes are A, B, C, and D. Class A, C, and D act by serine-based mechanism, while Class B requires zinc as a precursor for activation. Bush et al. originally proposed three functional groups in 1995: Group 1, Group 2 and Group 3. More recently [<xref ref-type="bibr" rid="CR3">3</xref>], the functional grouping scheme has been updated to correlate them with their phenotype in clinical isolates. The updated classification Group 1 (Cephalosporinases) contains molecular Class C which is not inhibited by clavulanic acid and contains a subgroup called 1e. Group 2 (Serine BLs) contains molecular Classes A and D, which are inhibited by clavulanic acid and contain subgroups 2a, 2b, 2be, 2br, 2ber, 2c, 2ce, 2d, 2de, 2df, 2e, and 2 f. Group 3 (Metallo-b-lactamases [MBLs]) contains molecular Class B, which is not inhibited by clavulanic acid and contains subclasses B1, B2, and B3 and subgroups 3a, 3b and 3c. A simple Venn diagram showing the relationship between molecular class and functional groups is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Fig. 1</label><caption><p>Venn diagram showing the relationship between molecular class and Functional group of Beta Lactamase</p></caption><graphic xlink:href="12859_2017_1972_Fig1_HTML" id="MO1"/></fig>
</p>
      <p id="Par16">Numerous studies have been performed to categorize all the classes of BL and their associated variants, along with their epidemiology and resistance pattern information [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref>]. One of these resources is the β-Lactamase Database (BLAD) [<xref ref-type="bibr" rid="CR5">5</xref>], which contains BL sequences linked with structural data, phenotypic data, and literature references to experimental studies. BLAD contains more than 1154 BL enzymes identified as of July 2015 [<xref ref-type="bibr" rid="CR7">7</xref>], which are classified into 4 classes [A, B, C and D] based on sequence similarity [<xref ref-type="bibr" rid="CR8">8</xref>]. Similarly, these proteins have also been divided into classes based on functional characteristics [<xref ref-type="bibr" rid="CR9">9</xref>]. BL belonging to classes A, C, and D have similar folds and a mechanism that involves a catalytic serine residue whereas class B of BL has a distinct fold [<xref ref-type="bibr" rid="CR7">7</xref>]. It is possible to detect the presence of BL enzymes by conducting various biological experiments; however, it is both time-consuming and costly. Hence, the development of computational methods to predict the identification and classification of BLs is a strong alternative approach to aid in the annotation of BL.</p>
      <p id="Par17">Few computational studies have been conducted in order to predict the BL proteins classes. Srivastava et al. proposed a fingerprint (unique family specific motif) based method to predict the family of BLs [<xref ref-type="bibr" rid="CR10">10</xref>]. As this method relies on extracting motifs in the sequences, there is inherent limitations when looking specifically for conserved motifs. Subsequently, Kumar et al. proposed a support vector machine based approach for prediction of BL classes [<xref ref-type="bibr" rid="CR11">11</xref>]. This method uses Chou’s pseudo-amino acid composition [<xref ref-type="bibr" rid="CR12">12</xref>] and is a two-level BL prediction method. The first level predicts whether or not a given sequence is a BL and if so, the second level classifies the BL into different classes. This method identifies BL with sufficient accuracy, but underperforms in classification accuracy.</p>
    </sec>
    <sec id="Sec3">
      <title>Feature extraction</title>
      <p id="Par18">We recently developed a comprehensive Feature Extraction from Protein Sequences (FEPS) web server [<xref ref-type="bibr" rid="CR13">13</xref>]. FEPS uses published feature extraction methods of proteins from single or multiple-FASTA formatted files. In addition, FEPS also provides users the ability to redefine some of the features by choosing one of the 544 physicochemical properties or to enter any user-defined amino acid indices, thereby increasing feature choices. The FEPS server includes 48 published feature extraction methods, six of which can use any of the 544 physicochemical properties. The total number of features calculated by FEPS is 2765, which exceeds the number of features computed by any other peer application. This exhaustive list of feature extraction methods enables us to develop machine learning based approaches for various classification problems in bioinformatics. FEPS has been successfully applied for the prediction and classification of nuclear receptors [<xref ref-type="bibr" rid="CR13">13</xref>], prediction of phosphorylation sites [<xref ref-type="bibr" rid="CR14">14</xref>], and prediction of hydroxylation sites [<xref ref-type="bibr" rid="CR15">15</xref>].</p>
    </sec>
    <sec id="Sec4">
      <title>Convolutional neural network (CNN)</title>
      <p id="Par19">To improve identification and classification of BL enzymes, we implemented a Convolutional Neural Network (CNN) based two-level approach called CNN-BLPred. CNN is a specific type of deep neural network that uses a translation-invariant convolution kernel that can be used to extract local contextual features and has proven to be quite successful in various domains [<xref ref-type="bibr" rid="CR16">16</xref>] including but not limited to computer vision and image classification, spam topic categorization, sentiment analysis, spam detection, and others [<xref ref-type="bibr" rid="CR17">17</xref>]. The basic structure of CNNs consists of convolution layers, nonlinear layers, and pooling layers. Recently, CNN has been applied to several bioinformatics problems [<xref ref-type="bibr" rid="CR18">18</xref>].</p>
      <p id="Par20">Moreover, there exist various balancing techniques like Synthetic Minority Oversampling Technique (SMOTE) [<xref ref-type="bibr" rid="CR19">19</xref>], random oversampling (ROS), and random undersampling (RUS) to balance the dataset when the number of positive and negative examples is not balanced. It has also been observed in several studies that a balanced dataset provides an improvement in the overall performance for classifiers. In the field of bioinformatics, Wei and Dunbrack [<xref ref-type="bibr" rid="CR19">19</xref>] studied the effect of unbalanced data and found that balanced training data results in the highest balanced performance.</p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Methods</title>
    <sec id="Sec6">
      <title>Beta lactamase family classification</title>
      <p id="Par21">Since BL have two types of classification, molecular classes and functional groups, we designed an algorithm to identify both types of classification. To our knowledge, this is the first computational work dealing with the classification of BL into functional groups.</p>
    </sec>
    <sec id="Sec7">
      <title>Benchmark dataset 1: Molecular class/functional group</title>
      <p id="Par22">BL have been classified into four molecular classes: Class A, Class B, Class C, and Class D. BL have also been classified into three functional groups: 1, 2, and 3.</p>
      <p id="Par23">We used one training dataset for cross-validation and two independent datasets for our testing purposes.</p>
      <p id="Par24">For the first benchmark dataset, the positive BL enzyme sequences were obtained from the NCBI website by using ‘Beta-Lactamase’ as a keyword search term to obtain BL enzyme sequences. In total 1,022,470 sequences were retrieved (as of Feb 2017) and sequences that contained keyword ‘partial’ in the sequence header were removed. Then, the sequences were split into molecular classes using keywords ‘Class A, Class B, Class C, and Class D’. This resulted in 11,987, 120,465, 12,350, and 4583 sequences for Class A, Class B, Class C, and Class D respectively (Table <xref rid="Tab1" ref-type="table">1</xref>). This is summarized in Table <xref rid="Tab1" ref-type="table">1</xref>. For the non-BL enzyme sequences, the same sequences used in PredLactamase [<xref ref-type="bibr" rid="CR11">11</xref>] were used. These sequences were used as a negative set for our general (Level 1) BL classifier.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Molecular Class/Functional Group Benchmark Dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>#</th><th>Class/Group</th><th># of Sequences Before /After CD-hit</th></tr></thead><tbody><tr><td>1</td><td>Class A</td><td>11,987/278</td></tr><tr><td>2</td><td>Class B/Group 3</td><td>120,465/2184</td></tr><tr><td>3</td><td>Class C/Group 1</td><td>12,350/744</td></tr><tr><td>4</td><td>Class D</td><td>4853/62</td></tr><tr><td>5</td><td>Group 2</td><td>16,840/340</td></tr><tr><td>6</td><td>Non BL</td><td>497</td></tr></tbody></table></table-wrap>
</p>
      <p id="Par25">Redundant sequences from each class were removed using CD-HIT (40%) [<xref ref-type="bibr" rid="CR20">20</xref>]. This resulted in 278 Class A, 2184 Class B (Group 3), 744 Class C (Group 1), and 62 Class D sequences. The 340 Group 2 sequences were derived by combining Class A and D sequences. From these sequences, 95% were used for training and the remaining 5% of the dataset was left out for independent testing (Table <xref rid="Tab2" ref-type="table">2</xref>).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Molecular Class/Functional Group Datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>#</th><th>Class/Group</th><th>Training</th><th>Independent 1</th><th>Independent 2</th></tr></thead><tbody><tr><td>1</td><td>Class A</td><td>268</td><td>10</td><td>4</td></tr><tr><td>2</td><td>Class B/Group 3</td><td>2069</td><td>115</td><td>6</td></tr><tr><td>3</td><td>Class C/Group 1</td><td>701</td><td>43</td><td>6</td></tr><tr><td>4</td><td>Class D</td><td>59</td><td>3</td><td>4</td></tr><tr><td>5</td><td>Group 2</td><td>318</td><td>22</td><td>8</td></tr><tr><td>6</td><td>Non BL</td><td>478</td><td>19</td><td>–</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec8">
      <title>Independent datasets</title>
      <p id="Par26">An independent dataset is required to assess the blind performance of the method. Our experiment incorporated two independent datasets. The number of sequences in the Independent Dataset 1 (Additional file <xref rid="MOESM1" ref-type="media">1</xref>) is shown in Table <xref rid="Tab2" ref-type="table">2</xref> (created with the remaining 5% of the left out dataset) and we used the independent dataset from PredLactamase [<xref ref-type="bibr" rid="CR11">11</xref>] as our Independent Dataset 2 (Additional file <xref rid="MOESM2" ref-type="media">2</xref>). Using Additional file <xref rid="MOESM2" ref-type="media">2</xref>: Independent Dataset 2 allows us to compare our method to the previously published PredLactamase method.</p>
      <p id="Par27">As discussed earlier, our method consists of two steps: identification and classification. The identification step uses the Level 1 predictor and will determine whether a protein is a BL or not. If the protein is not predicted as a BL enzyme during the identification step, the process will stop; otherwise the protein is passed to the next step, which is classification step. During classification, predictors for Classes A and B (aka Group 3), C (aka Group 1), D, and Group 2 are used. This step returns predictions and probabilities for each predictor and we take the prediction with the highest probability for each classification scheme (molecular and functional). Our method returns multiple predictions in the instance of multiple predictors returning the same maximum probabilities. The schematic of the one-vs.-rest classification is depicted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. a set of binary classifiers using a one-vs.-rest strategy, and each resulting molecular class dataset includes data from the other three classes as a negative set. For example, Class A has 278 positive examples and 2990 (total of classes B, C and D) negative examples. Our Group 2 predictor has 318 positive examples and 2770 (total of groups 1 and 2) as negative examples. Our Level 1 predictor has 3268 (total BL sequences) positive examples and 497 negative examples.<fig id="Fig2"><label>Fig. 2</label><caption><p>Schematic of our multi-class classification approach for Beta Lactamase</p></caption><graphic xlink:href="12859_2017_1972_Fig2_HTML" id="MO2"/></fig>
</p>
    </sec>
    <sec id="Sec9">
      <title>Balanced training data set</title>
      <p id="Par28">Due to the different number of positive and negative training examples (BL enzymes as well as respective BL enzymes belonging to each class), we must resolve class imbalance before moving to classifier training. We balanced our resulting dataset to obtain the optimal accuracy. Some of the techniques that we used to solve this imbalanced dataset problem are random undersampling (RUS), random oversampling (ROS), and Synthetic Minority Oversampling Technique SMOTE [<xref ref-type="bibr" rid="CR21">21</xref>]. RUS is the procedure of randomly eliminating examples from the majority class until the number of examples matches that of the minority class. RUS does not suffer from the problem of overfitting but can suffer from the loss of potentially useful data. ROS is the opposite of RUS in that it randomly replicates examples of the minority class until it matches that of the majority class. Using ROS, we will not lose potentially useful data; however, the act of randomly replicating data can cause a model to fit too closely to the training data and subsequently overfit. SMOTE is a variation of ROS that solves the overfitting problem by creating synthetic instances instead of making random copies. This method is also useful in that it can extract more information from data that is very helpful when our dataset is small.</p>
      <p id="Par29">For the molecular classes, we utilize ROS for Level 1, Class A, Class C/Group 1 and Group 2 so that we do not discard any potentially useful data. Because we have a significant number of examples of the majority class, we use RUS for Class B/Group 3 to reduce the potential of overfitting. The dataset for Class D is small, so we use SMOTE to maximize the data practicality. The resulting Dataset is shown in Table <xref rid="Tab3" ref-type="table">3</xref> and is used for training the model.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Molecular Class/Functional Group Benchmark Dataset after Balancing</p></caption><table frame="hsides" rules="groups"><thead><tr><th>#</th><th>Class</th><th>Method</th><th>Positive</th><th>Negative</th></tr></thead><tbody><tr><td>1</td><td>Level 1</td><td>ROS</td><td>3268</td><td>3268</td></tr><tr><td>2</td><td>Class A</td><td>ROS</td><td>2990</td><td>2990</td></tr><tr><td>3</td><td>Class B/Group 3</td><td>RUS</td><td>1084</td><td>1084</td></tr><tr><td>4</td><td>Class C/Group 1</td><td>ROS</td><td>2524</td><td>2524</td></tr><tr><td>5</td><td>Class D</td><td>SMOTE</td><td>3200</td><td>3200</td></tr><tr><td>6</td><td>Group 2</td><td>ROS</td><td>2770</td><td>2770</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec10">
      <title>Protein sequence features</title>
      <p id="Par30">Machine learning algorithms, like CNN, work on vectors of numerical values. To classify protein sequences using CNN, we transformed the protein sequences into vectors of numerical values using FEPS. The features we used in our study were: k-Spaced Amino Acid Pairs (CKSAAP), Conjoint Triad (CT), and Tri-peptide Amino Acid Composition (TAAC). CNNs have superior predictive power and are well-equipped to learn “simple” features, however they have limited capabilities for data of mixed types (complex features). Also, feature embedding is typically implemented on continuous vector space with low dimensions. To alleviate these issues, we only evaluate features that contain whole numbers, i.e. CKSAAP, CT, and TAAC. The total number of features considered in the study was 10,912 (Table <xref rid="Tab4" ref-type="table">4</xref>). We describe the features used in this study below.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Feature set and Feature Selection Results. CSKAAP [<xref ref-type="bibr" rid="CR22">22</xref>] refers to the K-spaced amino acid Pairs, CT [<xref ref-type="bibr" rid="CR20">20</xref>] refers to Conjoint Triad and TAAC is the Tri-peptide Amino acid composition</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Feature Set</th><th>Total Features</th><th/><th colspan="4">Molecular Class / Functional Group – Total Features after Feature Selection</th><th/></tr><tr><th/><th>Level 1</th><th>Class A</th><th>Class B / Group 3</th><th>Class C / Group 1</th><th>Class D</th><th>Group 2</th></tr></thead><tbody><tr><td>CKSAAP [<xref ref-type="bibr" rid="CR22">22</xref>]</td><td>2400</td><td>367</td><td>270</td><td>240</td><td>230</td><td>197</td><td>266</td></tr><tr><td>CT [<xref ref-type="bibr" rid="CR20">20</xref>]</td><td>512</td><td>208</td><td>151</td><td>149</td><td>145</td><td>147</td><td>160</td></tr><tr><td>TAAC</td><td>8000</td><td>325</td><td>227</td><td>262</td><td>249</td><td>120</td><td>219</td></tr><tr><td>ALL</td><td>10,912</td><td>363</td><td>288</td><td>243</td><td>257</td><td>195</td><td>270</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec11">
      <title>Tri-peptide amino acid composition (TAAC)</title>
      <p id="Par31">Tri-peptide Amino-Acid Composition (3-mer spectrum) of a sequence represents the frequency of three contiguous amino acids in a protein sequence. In other words, TAAC is the total count of each possible 3-mer of amino acids in the protein sequence. TAAC is defined as below where N is length of the sequence.</p>
      <p id="Par32"><disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {f}_j=\frac{\#\mathrm{of}\  \mathrm{tripeptide}\ j}{N-2}\times 100 $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>#</mml:mo><mml:mtext mathvariant="bold-italic"> of tripeptide</mml:mtext><mml:mspace width="0.25em"/><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mo>−</mml:mo><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mn mathvariant="bold">100</mml:mn></mml:math><graphic xlink:href="12859_2017_1972_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>tripeptide</italic><sub><italic>j</italic></sub> represents any possible tripeptide. The total number of 3-mers is 20<sup>3</sup> = 8000, <italic>i</italic> = 1,2,3, …8000.</p>
    </sec>
    <sec id="Sec12">
      <title>Conjoint triad</title>
      <p id="Par33">Conjoint triad descriptors (CT) were first described by Shen et al. [<xref ref-type="bibr" rid="CR22">22</xref>] to predict protein-protein interactions. The conjoint triad descriptors represent the features of protein pairs based on the classification of amino acids. In CTD the properties of one amino acid and its vicinal amino acids and regards any three continuous amino acids as a unit.</p>
      <p id="Par34">To calculate the conjoint triad, originally the amino acids are clustered into seven classes based on their dipole and the volume of the side chain. The newer Conjoint Triad Feature (CTF2) proposed by Yin and Tan [<xref ref-type="bibr" rid="CR23">23</xref>] includes the dummy amino acid that is used to ensure the identical of the window size of the amino acid sequence. Therefore, the dummy amino acid gets assigned an extra class, which is noted as O. The whole 21 amino acids are thus classified into eight classes: {A, G, V}, {I, L, F, P}, {Y, M, T, S}, {H, N, Q, W}, {R, K}, {D, E}, {C}, {O}. The rest of the encoding method is the same as the CT encoding [<xref ref-type="bibr" rid="CR22">22</xref>]. The amino acids in the same group are likely to substitute one another because of the physiochemical similarity. One class is added to account for possible ‘dummy’ amino acids that are placed into a sequence. We will refer to this newer Conjoint Triad features as CT in the rest of the paper. For CT, the amino acids are catalogued into eight classes; hence the size of the feature vector for CT is 8x8x8 = 512.</p>
    </sec>
    <sec id="Sec13">
      <title>K-spaced amino-acid pairs (CKSAAP)</title>
      <p id="Par35">k-spaced amino-acid pairs features were originally developed by Chen et al. [<xref ref-type="bibr" rid="CR24">24</xref>]. Essentially, for a given protein sequence all the adjacent pairs of Amino Acids (AAs) (dipeptides) in the sequence are counted. Since there are 400 possible AA pairs (<italic>AA</italic>, <italic>AC</italic>, <italic>AD</italic>, ..., <italic>YY</italic>), a feature vector of that size is used to represent occurrence of these pairs in the window. In order to accommodate for the short-range interactions between AAs, rather than only interactions between immediately adjacent AAs, CKSAAP also considers k-spaced pairs of AAs, i.e. pairs that are separated by <italic>k</italic> other AAs. For our purpose we use <italic>k</italic> = 0, 1... 5, where for <italic>k</italic> = 0 the pairs reduce to dipeptides. For each value of <italic>k</italic>, there are 400 corresponding features. In total we have 2400 features for CKSAAP. The feature type and number of features in each type is summarized in Table <xref rid="Tab4" ref-type="table">4</xref>. As discussed in the results section, we obtain best results using CKSAAP as the only type of feature. Hence, in CNN-BLPred we represent each protein sequence using CKSAAP only.</p>
    </sec>
    <sec id="Sec14">
      <title>Feature importance and feature selection</title>
      <p id="Par36">Feature importance for our purpose refers to determining the correlation between individual features in our feature set and the class labels. Highly correlated features are very important to our problem and features with low to no correlation are deemed unimportant to our problem. There are generally three method types to determine such importance. The first set of methods is linear methods, such as Lasso. These are easy to implement and scale readily to large dataset. However, as their name implies, linear methods are only able to determine linear correlations between features and provide no insight into non-linear correlations. The next set of methods is kernel methods, such as HSIC Lasso, which are able to determine non-linear correlations. These methods, however, do not scale well to large datasets and will quickly become intractable as the dataset grows. The last method, which is what we have chosen is called tree based methods, such as Gradient Boosted Trees, solves the issues of both previous methods by allowing us to detect non-linear correlations in a scalable way.</p>
      <p id="Par37">Once the features are extracted, we remove the unimportant features from our dataset to improve the overall quality of our model. We use XGBOOST in Python to construct the gradient boosted trees [<xref ref-type="bibr" rid="CR25">25</xref>]. Since our feature selection method is a tree based method, the feature importance is calculated based on a common metric known as impurity. Impurity is generally used to describe the ability of the feature to cleanly split the input data into the correct class. The equation used in our method is Gini Impurity that is denoted as:</p>
      <p id="Par38">
        <disp-formula id="Equ2">
          <label>2</label>
          <alternatives>
            <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ G=\sum \limits_{i=1}^{n_c}{p}_i\left(1-{p}_i\right) $$\end{document}</tex-math>
            <mml:math id="M4" display="block">
              <mml:mi mathvariant="italic">G</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:munderover>
                <mml:mo>∑</mml:mo>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn mathvariant="normal">1</mml:mn>
                </mml:mrow>
                <mml:msub>
                  <mml:mi mathvariant="italic">n</mml:mi>
                  <mml:mi mathvariant="italic">c</mml:mi>
                </mml:msub>
              </mml:munderover>
              <mml:msub>
                <mml:mi mathvariant="italic">p</mml:mi>
                <mml:mi mathvariant="italic">i</mml:mi>
              </mml:msub>
              <mml:mfenced close=")" open="(">
                <mml:mrow>
                  <mml:mn mathvariant="normal">1</mml:mn>
                  <mml:mo>−</mml:mo>
                  <mml:msub>
                    <mml:mi mathvariant="italic">p</mml:mi>
                    <mml:mi mathvariant="italic">i</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2017_1972_Article_Equ2.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par39">Where n<sub>c</sub> is the number class and p<sub>i</sub> is the probability value of i. Each node in the gradient boosted trees is given a Gini impurity index and this is used to calculate what is called the Gini Importance measure which is calculated as:</p>
      <p id="Par40">
        <disp-formula id="Equ3">
          <label>3</label>
          <alternatives>
            <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ I={G}_{\mathrm{parent}}-{G}_{\mathrm{split}1}-{G}_{\mathrm{split}2} $$\end{document}</tex-math>
            <mml:math id="M6" display="block">
              <mml:mi mathvariant="italic">I</mml:mi>
              <mml:mo>=</mml:mo>
              <mml:msub>
                <mml:mi mathvariant="italic">G</mml:mi>
                <mml:mtext mathvariant="italic">parent</mml:mtext>
              </mml:msub>
              <mml:mo>−</mml:mo>
              <mml:msub>
                <mml:mi mathvariant="italic">G</mml:mi>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">split</mml:mtext>
                  <mml:mn mathvariant="normal">1</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo>−</mml:mo>
              <mml:msub>
                <mml:mi mathvariant="italic">G</mml:mi>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">split</mml:mtext>
                  <mml:mn mathvariant="normal">2</mml:mn>
                </mml:mrow>
              </mml:msub>
            </mml:math>
            <graphic xlink:href="12859_2017_1972_Article_Equ3.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par41">Any feature with a relative importance value of &lt;0.001 is considered unimportant. Based on this, we were able to classify ~97.5% of the total features (for the combination of all the features) as unimportant and subsequently remove them. Table <xref rid="Tab4" ref-type="table">4</xref> shows the remaining features after calculating the feature importance and performing feature selection.</p>
    </sec>
    <sec id="Sec15">
      <title>Convolutional neural network (CNN)</title>
      <p id="Par42">For our CNN, we input a training data set and a corresponding label set (BL or not, class, etc.) and proceeded with the following steps. First, we used the schemes described in earlier section to construct features for each proteins. For each protein, there are 10,912 features. Next, we described the chosen architecture of the CNN for our purpose. The schematic of the architecture is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. The first layer of our network is the <italic>input layer</italic>. Our benchmark dataset, which includes the selected features in Table <xref rid="Tab4" ref-type="table">4</xref>, is fed into the input layer of the network which used a stochastic optimization method called Adam (Adaptive Moment Estimation), categorical cross entropy as the loss function, and a learning rate of 0.001.<fig id="Fig3"><label>Fig. 3</label><caption><p>Convolutional Neural Network (CNN) architecture used in our approach</p></caption><graphic xlink:href="12859_2017_1972_Fig3_HTML" id="MO3"/></fig>
</p>
      <p id="Par43">The next layer of our network is the <italic>embedding layer</italic> [<xref ref-type="bibr" rid="CR26">26</xref>]. This layer is used to identify semantic similarities between features. Typically, embedding is implemented on a space with one dimension per word or a continuous vector space with low dimensions. The input dimensions of this layer are the length of the feature vector space and the output of 128 embeddings that will be passed into the next layer.</p>
      <p id="Par44">The third layer of our network is the <italic>convolutional layer</italic>, which functions as a motif scanner. CNN-BLPred uses 256 convolutional filters, each scanning the input sequence with a step size of 1 and window size of 4. The output of each neuron on a convolutional layer is the convolution of the kernel matrix and the part of the input within the neuron’s window size. We used tanh activations along with L2-Regularization.</p>
      <p id="Par45">The fourth layer is the <italic>max-pooling layer</italic>. Since convolution output can vary in length, we performed max pooling to extract 2 × 2 (i.e. the kernel size) feature maps of the maximum activations of each filter. The max-pooling layer only outputs the maximum value of its respective convolutional layer outputs. The function of this max-pooling process can be thought of as determining whether the motif modelled by the respective convolutional layer exists in the input sequence or not.</p>
      <p id="Par46">The <italic>dropout layer</italic> [<xref ref-type="bibr" rid="CR10">10</xref>] is then used to randomly mask portions of its output to avoid overfitting. This is achieved by eliminating a random fraction of p (the probability that an element is dropped) hidden neurons while multiplying the remaining neurons by 1/p. For our implementation p was set to 0.5.</p>
      <p id="Par47">The final <italic>output layer</italic> consists of two neurons corresponding to the two classification results with softmax activation. The two neurons are fully connected to the previous layer. The deep learning CNN architecture was implemented using Tensorflow [<xref ref-type="bibr" rid="CR27">27</xref>] and TF.learn [<xref ref-type="bibr" rid="CR28">28</xref>].</p>
    </sec>
    <sec id="Sec16">
      <title>Model validation</title>
      <p id="Par48">The goal of the model validation is to assess the models thoroughly for prediction accuracy. In this study two evaluation strategies were adopted: 10-fold cross validation and independent test samples.</p>
    </sec>
    <sec id="Sec17">
      <title>10- fold cross validation</title>
      <p id="Par49">10-fold cross validation is a model validation technique to assess how the results of a model will be generalized to an independent data set. In 10-fold cross validation, the data is first partitioned into 10 equal segments (or folds). Then, 10 iterations of training and validation are performed where in each iteration, 9 folds are used for training and a different fold of data is held out for validation. The benchmark dataset is used for this purpose.</p>
    </sec>
    <sec id="Sec18">
      <title>Independent test samples</title>
      <p id="Par50">An independent test sample is a set of data that is independent of the data used in training the model. In addition to the k-fold cross-validation, independent test samples with known BL were used to evaluate the classification model as well. Independent Datasets 1 and 2 (Additional files <xref rid="MOESM1" ref-type="media">1</xref> and <xref rid="MOESM2" ref-type="media">2</xref>) were used for this purpose.</p>
    </sec>
    <sec id="Sec19">
      <title>Overfitting</title>
      <p id="Par51">One problem with using deep learning models is that they are prone to overfitting. Overfitting occurs when a model fits too well to the training data and is unable to generalize well. In this research, we incorporated several techniques to combat this problem. First, we used a simple convolutional neural network architecture with only one convolutional layer. This lowers the complexity of our model by minimizing the possible training parameters, giving our model fewer opportunities to overfit. Next, we employed sampling, feature selection, and embedding techniques to augment our data set. Then, we used L2 regularization and dropout with the probability 0.5. Also, our model was tuned using 10-fold cross validation during training to determine how well our model performed at predicting independent samples. Lastly, our method performs very well when evaluating our independent dataset; this further demonstrates that our model is not overfitting. Additional file <xref rid="MOESM3" ref-type="media">3</xref>: Figures S1-S6 show the validation loss curves for each classifier.</p>
    </sec>
    <sec id="Sec20">
      <title>Evaluation metrics</title>
      <p id="Par52">As discussed earlier, the BL classification is presented as a 2-level predictor. In the first level given a protein sequence, we predict whether that sequence is a BL or not and in the next level we predict to which class the BL belongs. The novelty of the approach is that we have implemented both the molecular classes and the functional groups. As both molecular class and functional groups contain more than two classes, CNN-BLPred uses the one-vs.-rest strategy to solve this multi-class classification problem. By doing so, the CNN-BLPred assigns for each class either positive or negative to the test sequence, giving rise to four frequencies: true positive (TP), false positive (FP), true negative (TN), and false negative (FN).</p>
      <p id="Par53">The above four frequencies are then used to calculate various evaluation metrics. The metrics include accuracy, sensitivity, specificity, and Matthew’s correlation coefficient (MCC) and are defined below.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Accuracy}=\frac{\mathrm{TP}+\mathrm{TN}}{\mathrm{TP}+\mathrm{TN}+\mathrm{FP}+\mathrm{FN}}\kern0.5em \times 100 $$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mtext mathvariant="italic">Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="0.5em"/><mml:mo>×</mml:mo><mml:mn mathvariant="normal">100</mml:mn></mml:math><graphic xlink:href="12859_2017_1972_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Sensitivity}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}\times 100 $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mtext mathvariant="italic">Sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TP</mml:mi><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mn mathvariant="normal">100</mml:mn></mml:math><graphic xlink:href="12859_2017_1972_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Specificity}=\frac{\mathrm{TN}}{\mathrm{TN}+\mathrm{FP}}\times 100 $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mtext mathvariant="italic">Specificity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TN</mml:mi><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mn mathvariant="normal">100</mml:mn></mml:math><graphic xlink:href="12859_2017_1972_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{MCC}=\frac{\left(\mathrm{TP}\right)\left(\mathrm{TN}\right)-\left(\mathrm{FP}\right)\left(\mathrm{FN}\right)}{\surd \left(\mathrm{TP}+\mathrm{FP}\right)\left(\mathrm{TP}+\mathrm{FN}\right)\left(\mathrm{TN}+\mathrm{FP}\right)\left(\mathrm{TN}+\mathrm{FN}\right)} $$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mi mathvariant="italic">MCC</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced close=")" open="("><mml:mi mathvariant="italic">TP</mml:mi></mml:mfenced><mml:mfenced close=")" open="("><mml:mi mathvariant="italic">TN</mml:mi></mml:mfenced><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mi mathvariant="italic">FP</mml:mi></mml:mfenced><mml:mfenced close=")" open="("><mml:mi mathvariant="italic">FN</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mo>√</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfenced><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfenced><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2017_1972_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>
</p>
      <p id="Par54">The area under the ROC (Receiver Operating Characteristic) curve (AUC) is also used as one of the metrics. We also compared our CNN-BLPred method with the existing PredLactamase [<xref ref-type="bibr" rid="CR11">11</xref>]. The results of cross-validation were adopted from the PredLactamase paper and the results for the independent datasets were obtained using their web-server [<xref ref-type="bibr" rid="CR11">11</xref>].</p>
    </sec>
  </sec>
  <sec id="Sec21">
    <title>Results</title>
    <sec id="Sec22">
      <title>Feature importance and feature selection</title>
      <p id="Par55">As discussed in the methods section, feature importance is calculated based on a relative importance measure created by constructing gradient boosted trees. Any feature with a relative importance value of &lt;0.001 is considered unimportant. Based upon this value, we were able to classify ~97.5% of the total features as unimportant and subsequently remove them. Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the top 10 features calculated from our classification models. Upon further analysis, we observed that features related to the Histidine (H) residue were heavily represented among the top features, which agrees with a previously published study [<xref ref-type="bibr" rid="CR29">29</xref>]. This study reported a signalling system in which membrane-associated histidine kinase directly binds β-lactams, triggering the expression of a β-lactamase and resistance to β-lactam antibiotics. It is also interesting to note that features like WY, WXXXW, WXXG, WXV, WF and others were deemed important for Class D β-lactamase. This is in agreement with the observation that tryptophan plays a critical role for the activity and stability of class D β-lactamase [<xref ref-type="bibr" rid="CR25">25</xref>].<fig id="Fig4"><label>Fig. 4</label><caption><p>Top 10 Features from CKSAAP (<italic>k-spaced Amino Acid Pairs</italic>). The features and their relative importance after feature selection for Level 1 and Classes A, B, C and D using XGBOOST</p></caption><graphic xlink:href="12859_2017_1972_Fig4_HTML" id="MO4"/></fig>
</p>
    </sec>
    <sec id="Sec23">
      <title>Performance of the individual feature type</title>
      <p id="Par56">In order to find the best combination of feature types, we compared the performance of individual features (i.e. CKSAAP, CT and TAAC) with the performance of the combination of all features (CKSAAP + CT + TAAC), which is represented in Table <xref rid="Tab5" ref-type="table">5</xref> as ALL. The performance of 10-fold cross validation for each of the features is shown in Table <xref rid="Tab5" ref-type="table">5</xref> for Level 1 prediction. A similar trend was observed for other class predictions. The performance of 10-fold cross validation and Independent Datasets 1 and 2 (Additional files <xref rid="MOESM1" ref-type="media">1</xref> and <xref rid="MOESM2" ref-type="media">2</xref>) for other classes is shown in Additional file <xref rid="MOESM3" ref-type="media">3</xref>: Tables S4a-e. It can be observed from Table <xref rid="Tab5" ref-type="table">5</xref> that CKSAAP and the collective set have the best performance for 10-fold cross validation. CKSAAP also outperformed all other features for the independent test as indicated in Additional file <xref rid="MOESM3" ref-type="media">3</xref>: Tables S4a-e. The ROC curves for each of the features are shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. From this evaluation, we determined that CKSAAP is the best feature set. Hence, only CKSAAP is used as the feature set for CNN-BLPred. The comparison of MCC scores for the 10-fold cross validation are presented in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. We also show the performance of CKSAAP using 10-fold cross validation in Table <xref rid="Tab6" ref-type="table">6</xref>. The performance of the independent test set of CKSAAP is shown in Table <xref rid="Tab7" ref-type="table">7</xref>. In addition, other evaluation metrics like Sensitivity, Specificity, Accuracy, F1 score, MCC and AUC of the CKSAAP are shown in Table <xref rid="Tab8" ref-type="table">8</xref>. The complete results of CNN-BLPred training are shown in Additional file <xref rid="MOESM3" ref-type="media">3</xref>: Table S7.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performance of CKSAAP, TAAC, CT and ALL for Level 1 using 10-Fold CV (ALL refers to CKSAAP + CT + TAAC)</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Methods</th><th colspan="4">Level 1</th></tr><tr><th>AUC</th><th>Sen (%)</th><th>Sp (%)</th><th>MCC</th></tr></thead><tbody><tr><td>CKSAAP</td><td>1.00</td><td>99.90</td><td>95.73</td><td>0.96</td></tr><tr><td>CT</td><td>0.98</td><td>98.30</td><td>93.81</td><td>0.92</td></tr><tr><td>TAAC</td><td>0.98</td><td>97.27</td><td>92.29</td><td>0.89</td></tr><tr><td>ALL</td><td>1.00</td><td>99.77</td><td>96.47</td><td>0.96</td></tr></tbody></table></table-wrap>
<fig id="Fig5"><label>Fig. 5</label><caption><p>ROC Curve for 10-fold cross validation (CKSAAP). All curves follow closely to the left and top border, with AUC above 90%, indicating the classifiers have a high accuracy</p></caption><graphic xlink:href="12859_2017_1972_Fig5_HTML" id="MO5"/></fig>
<fig id="Fig6"><label>Fig. 6</label><caption><p>Comparison of MCC Scores based on 10-fold cross validation</p></caption><graphic xlink:href="12859_2017_1972_Fig6_HTML" id="MO6"/></fig>
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Performance of CKSAAP using 10-Fold Cross Validation</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Class/Group</th><th>AUC</th><th>Sen (%)</th><th>Sp (%)</th><th>MCC</th></tr></thead><tbody><tr><td>Level 1</td><td>1.00</td><td>99.90</td><td>95.73</td><td>0.96</td></tr><tr><td>Class A</td><td>1.00</td><td>98.03</td><td>100.00</td><td>0.98</td></tr><tr><td>Class B/Group 3</td><td>1.00</td><td>97.94</td><td>97.94</td><td>0.96</td></tr><tr><td>Class C/Group 1</td><td>1.00</td><td>98.02</td><td>99.15</td><td>0.97</td></tr><tr><td>Class D</td><td>1.00</td><td>99.58</td><td>99.97</td><td>1.00</td></tr><tr><td>Group 2</td><td>1.00</td><td>97.44</td><td>99.93</td><td>0.97</td></tr></tbody></table></table-wrap>
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Independent Test Set Performance of CKSAAP</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Class/Group</th><th>AUC</th><th>Sen (%)</th><th>Sp (%)</th><th>MCC</th></tr></thead><tbody><tr><td>Level 1</td><td>0.96</td><td>97.60</td><td>68.18</td><td>0.70</td></tr><tr><td>Class A</td><td>0.99</td><td>76.92</td><td>98.68</td><td>0.78</td></tr><tr><td>Class B/Group 3</td><td>1.00</td><td>100.00</td><td>98.48</td><td>0.99</td></tr><tr><td>Class C/Group 1</td><td>0.99</td><td>86.49</td><td>99.21</td><td>0.89</td></tr><tr><td>Class D</td><td>1.00</td><td>83.33</td><td>100.00</td><td>0.91</td></tr><tr><td>Group 2</td><td>0.99</td><td>89.47</td><td>96.55</td><td>0.81</td></tr></tbody></table></table-wrap>
<table-wrap id="Tab8"><label>Table 8</label><caption><p>Complete Results of CNN-BLPred Independent Testing</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Class</th><th>Sensitivity</th><th>Specificity</th><th>Accuracy</th><th>F1 Score</th><th>MCC</th></tr></thead><tbody><tr><td>Level 1</td><td>97.60</td><td>68.18</td><td>94.18</td><td>0.97</td><td>0.70</td></tr><tr><td>Class A</td><td>76.92</td><td>98.68</td><td>96.95</td><td>0.80</td><td>0.78</td></tr><tr><td>Class B/Group 3</td><td>100.00</td><td>98.48</td><td>99.39</td><td>0.99</td><td>0.99</td></tr><tr><td>Class C/Group 1</td><td>86.49</td><td>99.21</td><td>96.34</td><td>0.91</td><td>0.89</td></tr><tr><td>Class D</td><td>83.33</td><td>100.00</td><td>99.39</td><td>0.91</td><td>0.91</td></tr><tr><td>Group 2</td><td>77.27</td><td>98.59</td><td>95.73</td><td>0.83</td><td>0.81</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec24">
      <title>Performance of the CNN-BLPred</title>
      <p id="Par57">We compared CNN (using CKSAAP as the feature based on the results in previous section) to other popular machine learning algorithms. Essentially, we compared the performance of CNN using our simple architecture with other machine learning methods like Random Forest and other Deep Learning architectures like RNN (Recurrent Neural Networks). In addition, we changed the architecture of our original Convolutional Neural Network (CNN) by adding another convolutional layer and max pooling layer after the original max pooling layer. We call this approach CNN-Ext. We also compared CNN-BLPred with PredLactamase. For the comparison of machine learning algorithms, we show results of both 10-fold cross validation as well as the independent test results (using Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Independent Dataset 1) in Table <xref rid="Tab9" ref-type="table">9</xref>. It was observed that CNN performed slightly higher than RF and significantly outperformed RNN and to some extent CNN-Ext. It must be noted that although CNN-Ext performs better in training (likely due to overfitting), it does not perform similarly in the independent set. In essence, with the comparison to other various ML algorithms and architecture, the one we used which is a simple architecture (with only one convolutional layer and max pooling layer) performs the best which supports the superior performance of CNN.<table-wrap id="Tab9"><label>Table 9</label><caption><p>Comparative Results using Benchmark Dataset 1 for RF, RNN, CNN-ext. and CNN. RF refers to Random Forest. RNN refers to Recurrent Neural Network. CNN-ext. refers to extended CNN where we use our original architecture with another convolutional layer and max pooling layer adding after the original max pooling layer. CNN refers to the Convolutional Neural Network described in the paper</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">#</th><th rowspan="2">Class/Group</th><th colspan="4">Training</th><th colspan="4">Independent Test</th></tr><tr><th>RF</th><th>RNN</th><th>CNN-ext</th><th>CNN</th><th>RF</th><th>RNN</th><th>CNN-ext</th><th>CNN</th></tr></thead><tbody><tr><td>1</td><td>Level 1</td><td>0.97</td><td>0.43</td><td>0.95</td><td>0.96</td><td>0.95</td><td>0.70</td><td>0.69</td><td>0.70</td></tr><tr><td>2</td><td>Class A</td><td>0.97</td><td>0.16</td><td>0.97</td><td>0.98</td><td>0.75</td><td>0.70</td><td>0.78</td><td>0.78</td></tr><tr><td>3</td><td>Class B/Group 3</td><td>0.94</td><td>−0.04</td><td>0.96</td><td>0.96</td><td>0.94</td><td>0.34</td><td>1.00</td><td>0.99</td></tr><tr><td>4</td><td>Class C/Group 1</td><td>0.92</td><td>0.20</td><td>0.96</td><td>0.97</td><td>0.90</td><td>0.54</td><td>0.89</td><td>0.89</td></tr><tr><td>5</td><td>Class D</td><td>1.00</td><td>0.66</td><td>0.99</td><td>1.00</td><td>0.44</td><td>0.06</td><td>1.00</td><td>0.91</td></tr><tr><td>6</td><td>Group 2</td><td>0.96</td><td>0.42</td><td>0.97</td><td>0.97</td><td>0.75</td><td>0.34</td><td>0.81</td><td>0.81</td></tr></tbody></table></table-wrap>
</p>
      <p id="Par58">We only present the results of the independent test (the results of 10-fold cross validation showed similar trends). It was observed that for each class, a predictive MCC of at least 0.78 and overall MCC were obtained (overall MCC of 0.81 obtained for CNN-BLPred* and 0.89 obtained for CNN-BLPred). Interestingly, our prediction accuracy and MCC for non-BL was 94.18% and 0.70 respectively for CNN-BLPred. Fig. <xref rid="Fig7" ref-type="fig">7</xref> shows the comparison between PredLactamase and our CNN-BLPred based on MCC scores.<fig id="Fig7"><label>Fig. 7</label><caption><p>Comparison of PredLactamase vs. CNN-BLPred (Independent Test) using MCC on an independent test set. MCC score was higher using CNN-BLPred than PredLactamase. CNN-BLPred is testing using Independent Dataset 1 (Additional file <xref rid="MOESM1" ref-type="media">1</xref>). CNN-BLPred* is testing using Independent Dataset 2 (Additional file <xref rid="MOESM2" ref-type="media">2</xref>)</p></caption><graphic xlink:href="12859_2017_1972_Fig7_HTML" id="MO7"/></fig>
</p>
      <sec id="Sec25">
        <title>Comparing PredLactamase with CNN-BLPred</title>
        <p id="Par59">The results of the independent test samples for our method CNN-BLPred and PredLactamase using Additional file <xref rid="MOESM2" ref-type="media">2</xref>: Independent Dataset 2 are summarized in Table <xref rid="Tab10" ref-type="table">10</xref>, in the form of a confusion matrix. The column labelled ‘correct’ for both predictors show the number of sequences that were correctly identified while the one that is labelled ‘incorrect’ shows the number of sequences that were incorrectly predicted and the incorrectly predicted subfamily. The column ACC denotes the accuracy of each method in percentages. It was observed can be seen that for all the BL Classes and non-BL proteins, the accuracy of CNN-BLPred was higher than PredLactamase method for all the BL classes and non-BL proteins.<table-wrap id="Tab10"><label>Table 10</label><caption><p>Comparative Results using Independent Dataset 1 for PredLactamase and CNN-BLPred</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Class</th><th colspan="3">PredLactamase</th><th colspan="3">CNN-BLPred*</th></tr><tr><th>Correct</th><th>Incorrect</th><th>ACC</th><th>Correct</th><th>Incorrect</th><th>ACC</th></tr></thead><tbody><tr><td>A</td><td>15</td><td>5</td><td>75.00</td><td>18</td><td>2</td><td>90.00</td></tr><tr><td>B</td><td>15</td><td>5</td><td>75.00</td><td>19</td><td>1</td><td>95.00</td></tr><tr><td>C</td><td>15</td><td>5</td><td>75.00</td><td>18</td><td>2</td><td>90.00</td></tr><tr><td>D</td><td>15</td><td>5</td><td>75.00</td><td>19</td><td>1</td><td>95.00</td></tr><tr><td>Overall</td><td/><td/><td>75.00</td><td/><td/><td>92.50</td></tr></tbody></table><table-wrap-foot><p>*CNN-BLPred is testing using Independent Dataset 2</p></table-wrap-foot></table-wrap>
</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec26">
    <title>Conclusions</title>
    <p id="Par60">We developed a Deep Learning based method (CNN-BLPred) to identify BL and subsequently classify them into respective BL classes. For the first time, in addition to molecular classes, we also implemented the functional classification. The BL classification problem is posed as a multi-class classification problem and solved using the one-vs.-rest strategy.</p>
    <p id="Par61">The number of embeddings were set to 128 based on the improved prediction accuracy. CNN-BLPred was able to predict with near optimal accuracy whether a query protein sequence belongs to one of the four molecular classes and/or one of the three functional groups. In order to use embedding technique effectively, this method uses CKSAAP features. This feature set was chosen, in part, because it can be represented as a small, continuous vector space and also because it outperformed other features that fit the same criteria (i.e. CT and TAAC).</p>
    <p id="Par62">To combat the class imbalance problem we used techniques such as ROS, RUS, and SMOTE. The number of features is considerably high compared to the number of sequences, which makes our classifier subject to the ‘curse of dimensionality’. To solve this issue we employ a feature selection method known as gradient boosted features selection.</p>
    <p id="Par63">Another concern we address is overfitting. Most overfitting problems are due to the fact that the dataset used for testing is used for the training as well. The training datasets used in this study were filtered from the closely similar and redundant sequences as explained in the dataset section. The test sequences, which were used for evaluation, are the sequences that were not included in the training dataset. A dataset with a redundancy reduction cut-off of 40% was utilized to ensure that our high prediction performance was not due to the sequence similarity of the dataset.</p>
    <p id="Par64">The method was systematically validated with cross validation and independent test samples using two sets of datasets that have varying sequence redundancy reduction criteria. Moreover, CNN-BLPred was compared with other machine learning algorithms like Random Forest, Recurrent Neural Network and other architectures of CNN and it was observed that a simple architecture of CNN works well for our purpose. Performance on the independent datasets and the comparative study between the CNN-BLPred and PredLactamase demonstrated that CNN-BLPred outperforms other well-established predictors. Deep Learning algorithms are considered to be better at learning abstract features from simple features, and one of the advantages of using Deep Learning is to get rid of hand-crafted features. The overall better performance of k-spaced amino acid features (a simple type of feature) also validates this point for this problem. Additionally, BL is a multi-domain protein and being able to identify a protein sequence as a BL will also help in prediction of its structure.</p>
    <p id="Par65">In conclusion, we were able to develop an improved BL classification method compared to existing methods based on Convolution Neural Network. A web site implementing the methodology will be developed soon to serve the scientific community. In the meantime, the software for the work is available upon request to academic researchers from the authors.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Additional files</title>
    <sec id="Sec27">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2017_1972_MOESM1_ESM.zip">
            <label>Additional file 1:</label>
            <caption>
              <p>Independent Dataset 1 (ZIP 5801 kb)</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="12859_2017_1972_MOESM2_ESM.zip">
            <label>Additional file 2:</label>
            <caption>
              <p>Independent Dataset 2 (ZIP 604 kb)</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="12859_2017_1972_MOESM3_ESM.pdf">
            <label>Additional file 3: Figure S1.</label>
            <caption>
              <p>Validation/Loss curve for Level 1. <bold>Figure S2.</bold> Validation/Loss curve for Class A. <bold>Figure S3.</bold> Validation/Loss curve for Class B (Group 3). <bold>Figure S4.</bold> Validation/Loss curve for Class C (Group 1). <bold>Figure S5.</bold> Validation/Loss curve for Class D. <bold>Figure S6.</bold> Validation/Loss curve for Group 2. <bold>Figure S7.</bold> FEPS top 10 features for level 1 and Classes A, B, C, and D. <bold>Table S1.</bold> Molecular Class/Functional Group training dataset. <bold>Table S2.</bold> Molecular Class/Functional Group Independent Dataset 1. <bold>Table S3.</bold> Molecular Class/Functional Group Independent Dataset 2. <bold>Table S4.</bold> a-f Performance of CKSAAP, TAAC, CT and ALL for level 1 and classes A, B, C, and D using 10-fold CV. <bold>Table S5.</bold> a-f. Independent test set performance of CKSAAP, TAAC, CT and ALL for level 1 and classes A, B, C, and D. <bold>Table S6.</bold> a-d. Performance of CNN-BLPred with PredLactamase using independent test sets. <bold>Table S7.</bold> Complete results of CNN-BLPred training. (PDF 201 kb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>BL</term>
        <def>
          <p id="Par5">Beta Lactamase</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par6">Convolutional Neural Network</p>
        </def>
      </def-item>
      <def-item>
        <term>NN</term>
        <def>
          <p id="Par7">Neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p id="Par8">Random Forest</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p id="Par9">Recurrent Neural Network</p>
        </def>
      </def-item>
      <def-item>
        <term>ROS</term>
        <def>
          <p id="Par10">Random over sampling</p>
        </def>
      </def-item>
      <def-item>
        <term>RUS</term>
        <def>
          <p id="Par11">Random under sampling</p>
        </def>
      </def-item>
      <def-item>
        <term>SMOTE</term>
        <def>
          <p id="Par12">Synthetic Minority Oversampling Technique</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Electronic supplementary material</bold>
      </p>
      <p>The online version of this article (10.1186/s12859-017-1972-6) contains supplementary material, which is available to authorized users.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to thank Dr. Robert Newman for his helpful discussions. The authors would also like to thank Amber Hoenig and Dr. Tiffani White for proof-reading the manuscript. The computation was performed in part using the high-performance computing resources at North Carolina A&amp;T State University and XSEDE resources. DBKC is partly supported by a start-up grant from the Department of Computational Science &amp; Engineering at North Carolina A&amp;T State University. DBKC is also partly supported by the National Science Foundation under grant no. 1647884. HS is supported by JSPS KAKENHI Grant Number JP25700004.</p>
    <sec id="FPar1">
      <title>Funding</title>
      <p id="Par66">The publication charges were funded by a start-up grant from the Department of Computational Science &amp; Engineering at North Carolina A&amp;T State University and the National Science Foundation grant no. 1647884.</p>
    </sec>
    <sec id="FPar2">
      <title>Availability of data and materials</title>
      <p id="Par67">The data used in the study is available in the additional files. The code is freely available from GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/whiteclarence/CNN-BLPred">https://github.com/whiteclarence/CNN-BLPred</ext-link>).</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>Conceived and designed the experiments: CW, SH, DK. Performed the experiments: CW, DK. Analyzed the data: CW, DK. Contributed reagents/materials/analysis tools: CW, HI, SH, DK. Wrote the manuscript: CW, SH, DK. All authors have read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="FPar4">
      <title>Ethics approval and consent to participate</title>
      <p id="Par69">Not applicable</p>
    </sec>
    <sec id="FPar3">
      <title>About this supplement</title>
      <p id="Par68">This article has been published as part of <italic>BMC Bioinformatics</italic> Volume 18 Supplement 16, 2017: 16th International Conference on Bioinformatics (InCoB 2017): Bioinformatics. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement-16">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement-16</ext-link>.</p>
    </sec>
    <sec id="FPar5">
      <title>Consent for publication</title>
      <p id="Par70">Not applicable</p>
    </sec>
    <sec id="FPar6">
      <title>Competing interests</title>
      <p id="Par71">The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="FPar7">
      <title>Publisher’s Note</title>
      <p id="Par72">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Donadio</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Antibiotic discovery in the twenty-first century: current trends and future perspectives</article-title>
        <source>J Antibiot (Tokyo)</source>
        <year>2010</year>
        <volume>63</volume>
        <issue>8</issue>
        <fpage>423</fpage>
        <lpage>430</lpage>
        <pub-id pub-id-type="doi">10.1038/ja.2010.62</pub-id>
        <pub-id pub-id-type="pmid">20551985</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Majiduddin</surname>
            <given-names>FK</given-names>
          </name>
          <name>
            <surname>Materon</surname>
            <given-names>IC</given-names>
          </name>
          <name>
            <surname>Palzkill</surname>
            <given-names>TG</given-names>
          </name>
        </person-group>
        <article-title>Molecular analysis of beta-lactamase structure and function</article-title>
        <source>Int J Med Microbiol</source>
        <year>2002</year>
        <volume>292</volume>
        <issue>2</issue>
        <fpage>127</fpage>
        <lpage>137</lpage>
        <pub-id pub-id-type="doi">10.1078/1438-4221-00198</pub-id>
        <?supplied-pmid 12195735?>
        <pub-id pub-id-type="pmid">12195735</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bush</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Jacoby</surname>
            <given-names>GA</given-names>
          </name>
        </person-group>
        <article-title>Updated functional classification of beta-lactamases</article-title>
        <source>Antimicrob Agents Chemother</source>
        <year>2010</year>
        <volume>54</volume>
        <issue>3</issue>
        <fpage>969</fpage>
        <lpage>976</lpage>
        <pub-id pub-id-type="doi">10.1128/AAC.01009-09</pub-id>
        <?supplied-pmid 19995920?>
        <pub-id pub-id-type="pmid">19995920</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pop</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>ARDB--antibiotic resistance genes database</article-title>
        <source>Nucleic Acids Res</source>
        <year>2009</year>
        <volume>37</volume>
        <issue>Database issue</issue>
        <fpage>D443</fpage>
        <lpage>D447</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkn656</pub-id>
        <?supplied-pmid 18832362?>
        <pub-id pub-id-type="pmid">18832362</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Danishuddin</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BLAD: a comprehensive database of widely circulated beta-lactamases</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>19</issue>
        <fpage>2515</fpage>
        <lpage>2516</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt417</pub-id>
        <?supplied-pmid 23943635?>
        <pub-id pub-id-type="pmid">23943635</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thai</surname>
            <given-names>QK</given-names>
          </name>
          <name>
            <surname>Pleiss</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>SHV Lactamase engineering database: a reconciliation tool for SHV beta-lactamases in public databases</article-title>
        <source>BMC Genomics</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>563</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2164-11-563</pub-id>
        <?supplied-pmid 20942904?>
        <pub-id pub-id-type="pmid">20942904</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jacoby</surname>
            <given-names>GA</given-names>
          </name>
        </person-group>
        <article-title>Beta-lactamase nomenclature</article-title>
        <source>Antimicrob Agents Chemother</source>
        <year>2006</year>
        <volume>50</volume>
        <issue>4</issue>
        <fpage>1123</fpage>
        <lpage>1129</lpage>
        <pub-id pub-id-type="doi">10.1128/AAC.50.4.1123-1129.2006</pub-id>
        <?supplied-pmid 16569819?>
        <pub-id pub-id-type="pmid">16569819</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fisher</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Meroueh</surname>
            <given-names>SO</given-names>
          </name>
          <name>
            <surname>Mobashery</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Bacterial resistance to beta-lactam antibiotics: compelling opportunism, compelling opportunity</article-title>
        <source>Chem Rev</source>
        <year>2005</year>
        <volume>105</volume>
        <issue>2</issue>
        <fpage>395</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="doi">10.1021/cr030102i</pub-id>
        <?supplied-pmid 15700950?>
        <pub-id pub-id-type="pmid">15700950</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bush</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Jacoby</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Medeiros</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <article-title>A functional classification scheme for beta-lactamases and its correlation with molecular structure</article-title>
        <source>Antimicrob Agents Chemother</source>
        <year>1995</year>
        <volume>39</volume>
        <issue>6</issue>
        <fpage>1211</fpage>
        <lpage>1233</lpage>
        <pub-id pub-id-type="doi">10.1128/AAC.39.6.1211</pub-id>
        <?supplied-pmid 7574506?>
        <pub-id pub-id-type="pmid">7574506</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Srivastava</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of family specific fingerprints in beta-lactamase families</article-title>
        <source>ScientificWorldJournal</source>
        <year>2014</year>
        <volume>2014</volume>
        <fpage>980572</fpage>
        <?supplied-pmid 24678282?>
        <pub-id pub-id-type="pmid">24678282</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prediction of beta-lactamase and its class by Chou's pseudo-amino acid composition and support vector machine</article-title>
        <source>J Theor Biol</source>
        <year>2015</year>
        <volume>365</volume>
        <fpage>96</fpage>
        <lpage>103</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jtbi.2014.10.008</pub-id>
        <?supplied-pmid 25454009?>
        <pub-id pub-id-type="pmid">25454009</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>KC</given-names>
          </name>
        </person-group>
        <article-title>Prediction of protein subcellular locations by incorporating quasi-sequence-order effect</article-title>
        <source>Biochem Biophys Res Commun</source>
        <year>2000</year>
        <volume>278</volume>
        <issue>2</issue>
        <fpage>477</fpage>
        <lpage>483</lpage>
        <pub-id pub-id-type="doi">10.1006/bbrc.2000.3815</pub-id>
        <?supplied-pmid 11097861?>
        <pub-id pub-id-type="pmid">11097861</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Ismail H, Saigo H, KC DB. RF-NR: Random forest based approach for improved classification of Nuclear Receptors. IEEE/ACM Trans Comput BIol Bioinfom, pp. 1–1, 2017. (Also appeared in GIW/INCoB 2015, Tokyo, Japan).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ismail</surname>
            <given-names>HD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>RF-Phos: a novel general Phosphorylation site prediction tool based on random Forest</article-title>
        <source>Biomed Res Int</source>
        <year>2016</year>
        <volume>2016</volume>
        <fpage>3281590</fpage>
        <pub-id pub-id-type="doi">10.1155/2016/3281590</pub-id>
        <?supplied-pmid 27066500?>
        <pub-id pub-id-type="pmid">27066500</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Ismail HD, Newman RH, Kc DB. RF-Hydroxysite: a random forest based predictor for hydroxylation sites. Mol BioSyst. 2016;12:–2427.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lecun</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gradient-based learning applied to document recognition</article-title>
        <source>Proc IEEE</source>
        <year>1998</year>
        <volume>86</volume>
        <issue>11</issue>
        <fpage>2278</fpage>
        <lpage>2324</lpage>
        <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Min S, Lee B, Yoon S.Deep learning in bioinformatics.Brief Bioinform. 2017;18(5):851–69. doi: 10.1093/bib/bbw068.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Dunbrack</surname>
            <given-names>RL</given-names>
            <suffix>Jr</suffix>
          </name>
        </person-group>
        <article-title>The role of balanced training and testing data sets for binary classifiers in bioinformatics</article-title>
        <source>PLoS One</source>
        <year>2013</year>
        <volume>8</volume>
        <issue>7</issue>
        <fpage>e67863</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0067863</pub-id>
        <?supplied-pmid 23874456?>
        <pub-id pub-id-type="pmid">23874456</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <issue>13</issue>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id>
        <?supplied-pmid 16731699?>
        <pub-id pub-id-type="pmid">16731699</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chawla</surname>
            <given-names>NV</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SMOTE: synthetic minority over-sampling technique</article-title>
        <source>J Artif Intell Res</source>
        <year>2002</year>
        <volume>16</volume>
        <fpage>321</fpage>
        <lpage>357</lpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting protein-protein interactions based only on sequences information</article-title>
        <source>Proc Natl Acad Sci U S A</source>
        <year>2007</year>
        <volume>104</volume>
        <issue>11</issue>
        <fpage>4337</fpage>
        <lpage>4341</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0607879104</pub-id>
        <?supplied-pmid 17360525?>
        <pub-id pub-id-type="pmid">17360525</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Yin Z, T.J. New encoding schemes for prediction of protein phosphorylation sites. in 2012 IEEE 6th International Conference on Systems Biology (ISB). Xi'an: IEEE; 2012.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kurgan</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Prediction of flexible/rigid regions from protein sequences using k-spaced amino acid pairs</article-title>
        <source>BMC Struct Biol</source>
        <year>2007</year>
        <volume>7</volume>
        <fpage>25</fpage>
        <pub-id pub-id-type="doi">10.1186/1472-6807-7-25</pub-id>
        <?supplied-pmid 17437643?>
        <pub-id pub-id-type="pmid">17437643</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Guestrin</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <source>XGBoost: A Scalable Tree Boosting System. in KDD’16</source>
        <year>2016</year>
        <publisher-loc>San Francisco</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source><italic>Distributed representations of words and phrases and their compositionality</italic>. In <italic>Advances in Neural Information Processing Systems</italic></source>
        <year>2013</year>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abadi</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>TensorFlow: learning functions at scale</article-title>
        <source>ACM SIGPLAN Not</source>
        <year>2016</year>
        <volume>51</volume>
        <issue>9</issue>
        <fpage>1</fpage>
        <lpage>1</lpage>
        <pub-id pub-id-type="doi">10.1145/3022670.2976746</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Tang Y. TF.Learn: TensorFlow's high-level module for distributed machine learning. CoRR, 2016;vol. abs/1612.04251.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>PL</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Characterisation of fosfomycin resistance mechanisms and molecular epidemiology in extended-spectrum beta-lactamase-producing Klebsiella Pneumoniae isolates</article-title>
        <source>Int J Antimicrob Agents</source>
        <year>2016</year>
        <volume>48</volume>
        <issue>5</issue>
        <fpage>564</fpage>
        <lpage>568</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ijantimicag.2016.08.013</pub-id>
        <?supplied-pmid 27765412?>
        <pub-id pub-id-type="pmid">27765412</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
