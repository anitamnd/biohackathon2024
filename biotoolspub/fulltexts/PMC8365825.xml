<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Chem Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Chem Sci</journal-id>
    <journal-id journal-id-type="publisher-id">SC</journal-id>
    <journal-id journal-id-type="coden">CSHCBM</journal-id>
    <journal-title-group>
      <journal-title>Chemical Science</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2041-6520</issn>
    <issn pub-type="epub">2041-6539</issn>
    <publisher>
      <publisher-name>The Royal Society of Chemistry</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8365825</article-id>
    <article-id pub-id-type="publisher-id">d1sc02957f</article-id>
    <article-id pub-id-type="doi">10.1039/d1sc02957f</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Chemistry</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ChemPix: automated recognition of hand-drawn hydrocarbon structures using deep learning<xref ref-type="fn" rid="fn1">†</xref></article-title>
      <fn-group>
        <fn id="fn1">
          <label>†</label>
          <p>Electronic supplementary information (ESI) available: Details of image processing, neural network training, and example image predictions. Link to code to generate data and run training experiments. See DOI: <uri>10.1039/d1sc02957f</uri></p>
        </fn>
      </fn-group>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1039-327X</contrib-id>
        <name>
          <surname>Weir</surname>
          <given-names>Hayley</given-names>
        </name>
        <xref ref-type="aff" rid="affa"/>
        <xref ref-type="aff" rid="affb"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3531-5857</contrib-id>
        <name>
          <surname>Thompson</surname>
          <given-names>Keiran</given-names>
        </name>
        <xref ref-type="aff" rid="affa"/>
        <xref ref-type="aff" rid="affb"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Woodward</surname>
          <given-names>Amelia</given-names>
        </name>
        <xref ref-type="aff" rid="affa"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7965-3199</contrib-id>
        <name>
          <surname>Choi</surname>
          <given-names>Benjamin</given-names>
        </name>
        <xref ref-type="aff" rid="affc"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Braun</surname>
          <given-names>Augustin</given-names>
        </name>
        <xref ref-type="aff" rid="affa"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4798-8947</contrib-id>
        <name>
          <surname>Martínez</surname>
          <given-names>Todd J.</given-names>
        </name>
        <xref ref-type="aff" rid="affa"/>
        <xref ref-type="aff" rid="affb"/>
      </contrib>
      <aff id="affa">
        <institution>Department of Chemistry, Stanford University</institution>
        <addr-line>Stanford</addr-line>
        <state>CA 94305</state>
        <country>USA</country>
        <email>toddjmartinez@gmail.com</email>
      </aff>
      <aff id="affb">
        <institution>SLAC National Accelerator Laboratory</institution>
        <addr-line>2575 Sand Hill Road</addr-line>
        <city>Menlo Park</city>
        <state>CA 94025</state>
        <country>USA</country>
      </aff>
      <aff id="affc">
        <institution>Department of Electrical Engineering, Stanford University</institution>
        <addr-line>Stanford</addr-line>
        <state>CA 94305</state>
        <country>USA</country>
      </aff>
    </contrib-group>
    <pub-date publication-format="electronic" date-type="pub" iso-8601-date="2021-07-03">
      <day>3</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date publication-format="electronic" date-type="collection" iso-8601-date="2021-08-11">
      <day>11</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>3</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>12</volume>
    <issue>31</issue>
    <fpage>10622</fpage>
    <lpage>10633</lpage>
    <history>
      <date date-type="received" iso-8601-date="2021-06-01">
        <day>1</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted" iso-8601-date="2021-06-28">
        <day>28</day>
        <month>6</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>This journal is © The Royal Society of Chemistry</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>The Royal Society of Chemistry</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense" start_date="2021-07-03">https://creativecommons.org/licenses/by-nc/3.0/</ali:license_ref>
      </license>
      <?release-delay 0|0?>
    </permissions>
    <abstract>
      <p>Inputting molecules into chemistry software, such as quantum chemistry packages, currently requires domain expertise, expensive software and/or cumbersome procedures. Leveraging recent breakthroughs in machine learning, we develop ChemPix: an offline, hand-drawn hydrocarbon structure recognition tool designed to remove these barriers. A neural image captioning approach consisting of a convolutional neural network (CNN) encoder and a long short-term memory (LSTM) decoder learned a mapping from photographs of hand-drawn hydrocarbon structures to machine-readable SMILES representations. We generated a large auxiliary training dataset, based on RDKit molecular images, by combining image augmentation, image degradation and background addition. Additionally, a small dataset of ∼600 hand-drawn hydrocarbon chemical structures was crowd-sourced using a phone web application. These datasets were used to train the image-to-SMILES neural network with the goal of maximizing the hand-drawn hydrocarbon recognition accuracy. By forming a committee of the trained neural networks where each network casts one vote for the predicted molecule, we achieved a nearly 10 percentage point improvement of the molecule recognition accuracy and were able to assign a confidence value for the prediction based on the number of agreeing votes. The ensemble model achieved an accuracy of 76% on hand-drawn hydrocarbons, increasing to 86% if the top 3 predictions were considered.</p>
    </abstract>
    <abstract abstract-type="toc">
      <p>Offline recognition of hand-drawn hydrocarbon structures is learned using an image-to-SMILES neural network through the application of synthetic data generation and ensemble learning.<graphic xlink:href="d1sc02957f-ga.jpg" id="ga" position="float" orientation="portrait"/></p>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Office of Naval Research</institution>
            <institution-id institution-id-type="doi">10.13039/100000006</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>N00014-18-1-2659</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>pubstatus</meta-name>
        <meta-value>Paginated Article</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec>
    <title>Introduction</title>
    <p>Artificial intelligence (AI) refers to the introduction of “human intelligence” into artificial machines. Machine learning is a subfield of AI that focuses specifically on the “learning” aspect of the machine's intelligence, removing the need for manually coding rules. Although Rosenblatt proposed the perceptron in the 1950s,<sup><xref rid="cit1" ref-type="bibr">1</xref></sup> it wasn't until the 1990s that machine learning shifted from a knowledge-based to a data-driven approach. A decade later, “deep learning” emerged as subclass of machine learning that employed multilayer neural networks (NNs). The boom of big-data and increasingly powerful computational hardware allowed deep learning algorithms to achieve unprecedented accuracy on a variety of problems. This resulted in much of the AI software used today, such as music/movie recommenders, speech recognition, language translation and email spam filters.</p>
    <p>Deep learning algorithms have been adopted by almost every academic field in the hope of solving both novel and age-old problems.<sup><xref rid="cit2" ref-type="bibr">2</xref></sup> The natural sciences have historically relied on the development of theoretical models derived from physically-grounded fundamental equations to explain and/or predict experimental observations. This makes data-driven models an interesting, and often novel, approach. In quantum chemistry, for example, to calculate the energy of a molecule one would traditionally solve an approximation to the electronic Schrodinger equation. A machine learning approach to this problem, however, might involve inputting a dataset of molecules and their respective energies into a NN, which would learn a mapping between the two.<sup><xref rid="cit3" ref-type="bibr">3–5</xref></sup> The ability to generate accurate models by extracting features directly from data without human input makes machine learning techniques an exciting avenue to explore in all areas of chemistry – from drug discovery and material design to analytical tools and synthesis planning.</p>
    <p>Easy-to-use machine learning based tools have the potential to accelerate research and enrich education. Here, we develop a hand-drawn molecule recognition tool to extract a digital representation of the molecule from an image of a hand-drawn hydrocarbon structure. Drawing skeletal chemical structures by hand is a routine task for students and researchers in the chemistry community. Therefore, photographing a hand-drawn chemical structure offers a low-barrier method of entering molecules into software that would normally require time-consuming workflows and domain expertise. Moreover, for the vast majority of the chemistry community, drawing a chemical structure by hand is far less cumbersome than building it with a mouse. The recognition tool could be integrated into a phone application that performs tasks such as quantum chemistry calculations, database lookups and AI synthesis planning directly from the hand-drawn molecule, extending the ChemVox voice-recognition system we recently developed.<sup><xref rid="cit6" ref-type="bibr">6</xref></sup></p>
    <p>In addition to its potential as a chemical research and education widget, hand-drawn hydrocarbon recognition is an interesting problem from a fundamental science perspective: it serves as a prototypical example of how deep learning can be applied to a well-suited chemical problem. Sourcing a large training dataset for this task is time and resource intensive – a common obstacle encountered in machine learning applications. To address this, we discuss strategies for synthetic data generation and their generalizability to scenarios where there is access to limited real-world data, but abundant similar data.</p>
    <p>Hand-drawn chemical structure recognition is, in many ways, similar to the task of handwriting recognition. Large variation in writing styles, poor image quality, lack of labelled data and cursive letters make hand-written text recognition a challenging task.<sup><xref rid="cit7" ref-type="bibr">7–10</xref></sup> Hand-writing recognition falls into two camps: online recognition, in which a user writes text on a tablet or phone and it is recognized in real-time, and offline recognition, which refers to static images of hand-written text. Offline recognition poses considerably more challenges than online recognition due largely to the latter's ability to use time dependent strokes in combination with the final image to distinguish between characters.<sup><xref rid="cit11" ref-type="bibr">11</xref></sup> In this work, we focus on offline hand-drawn hydrocarbon structure recognition, extending the potential use cases to digitization of lab notebooks.</p>
    <p>Automatic extraction of a molecule from an image of its 2D chemical structure to a machine-readable format, termed optical chemical structure recognition, first emerged in the 1990s.<sup><xref rid="cit12" ref-type="bibr">12–17</xref></sup> These systems were developed with the intent of mining ChemDraw type diagrams in the chemical literature to utilize the wealth of largely untapped chemical information that lies within publications.<sup><xref rid="cit17" ref-type="bibr">17–28</xref></sup> The majority of optical chemical structure recognition packages, including Kekulé,<sup><xref rid="cit14" ref-type="bibr">14</xref></sup> IBM's OROCS,<sup><xref rid="cit15" ref-type="bibr">15</xref></sup> CLiDE<sup><xref rid="cit16" ref-type="bibr">16</xref></sup> and CLiDEPro,<sup><xref rid="cit21" ref-type="bibr">21</xref></sup> ChemOCR,<sup><xref rid="cit20" ref-type="bibr">20</xref></sup> OSRA,<sup><xref rid="cit22" ref-type="bibr">22</xref></sup> ChemReader,<sup><xref rid="cit23" ref-type="bibr">23</xref></sup> MolRec,<sup><xref rid="cit25" ref-type="bibr">25</xref></sup> ChemEx,<sup><xref rid="cit26" ref-type="bibr">26</xref></sup> MLOCSR,<sup><xref rid="cit27" ref-type="bibr">27</xref></sup> and ChemSchematicResolver<sup><xref rid="cit28" ref-type="bibr">28</xref></sup> rely on a rule-based workflow rather than a data-driven approach. These systems achieve various degrees of accuracy, with the recently developed ChemSchematicResolver reaching 83–100% precision on a range of datasets.</p>
    <p>Rule-based systems often involve complex, interdependent workflows, which can make them brittle, and challenging to revise and extend. Therefore, several optical chemical structure recognition packages have been recently proposed based on data-driven, deep learning techniques.<sup><xref rid="cit29" ref-type="bibr">29–31</xref></sup> Notably, Staker <italic>et al.</italic><sup><xref rid="cit29" ref-type="bibr">29</xref></sup> employed end-to-end segmentation and image to molecule neural networks, and ChemGrapher<sup><xref rid="cit30" ref-type="bibr">30</xref></sup> used a series of deep neural networks to extract molecules from the chemical literature. These data-driven systems offer a promising alternative to rule-based systems for this task, provided one can obtain an appropriate training dataset.</p>
    <p>The optical chemical structure recognition systems mentioned thus far focus on recognition of computer generated, ChemDraw-type structures. A handful of promising online hand-drawn chemical structure recognition programs have recently been developed.<sup><xref rid="cit32" ref-type="bibr">32–34</xref></sup> Our goal of offline extraction of molecules from photographs of hand-drawn chemical structures adds a further level of complexity, and is well-suited for data-driven, machine learning models.</p>
    <p>In this article, we begin by discussing our chosen deep learning approach for hand-drawn chemical structure recognition and demonstrate proof-of-concept on ChemDraw type images of molecules produced with the RDKit. Next, we describe the generation of two datasets: a small set of real-world photographs of hand-drawn hydrocarbon structures and a large synthetic dataset. We perform a series of experiments with these datasets, aiming to optimize the recognition accuracy on out-of-sample real-world hand-drawn hydrocarbons. We end by forming an ensemble model consisting of a committee of NNs, which leads to a significant boost in recognition accuracy and introduces a confidence value for the prediction. The work serves as a prototypical case study for approaching a chemical problem with machine learning methods, focusing on the explanation of deep learning, synthetic data generation, and ensemble learning techniques.</p>
  </sec>
  <sec>
    <title>Methods</title>
    <sec>
      <title>Neural network architecture</title>
      <p>In this work, we represent molecules as simplified molecular-input line-entry system (SMILES)<sup><xref rid="cit35" ref-type="bibr">35</xref></sup> strings in order to leverage recent advances in natural language processing (NLP).<sup><xref rid="cit36" ref-type="bibr">36</xref></sup> We employ neural image captioning, in which an image is input into a NN and a caption for the image is produced.<sup><xref rid="cit37" ref-type="bibr">37,38</xref></sup> Here, an image of a hydrocarbon molecule is input and the predicted SMILES string is output, as shown in <xref ref-type="fig" rid="fig1">Fig. 1</xref>. The NN architecture consists of a convolutional neural network (CNN)<sup><xref rid="cit39" ref-type="bibr">39,40</xref></sup> encoder and a long short term memory (LSTM)<sup><xref rid="cit41" ref-type="bibr">41</xref></sup> decoder with beam search and attention. CNNs contain ‘convolutional layers’ that apply a convolutional filter over the image and pass the result to the next hidden layer; they are used primarily for encoding images since they conserve the spatial relationship of the pixels. LSTMs are a type of stable recurrent neural network (RNN) popularly used in language models. This is a useful feature in the case of decoding SMILES strings since there are often relations between characters at the start and end of the string, such as closing of a parentheses pair to indicate the end of a branching group. Our image-to-SMILES approach is inspired by the work of Deng <italic>et al.</italic>, who trained a NN to convert images of mathematical formulas to LaTeX code.<sup><xref rid="cit42" ref-type="bibr">42</xref></sup> A similar image-to-SMILES approach was also used by Staker <italic>et al.</italic>, which achieved test set recognition accuracies ranging from 41 to 83% on ChemDraw type structures extracted from the chemical literature.<sup><xref rid="cit29" ref-type="bibr">29</xref></sup> Details of the NN architecture used in this work are discussed in the ESI.<xref ref-type="fn" rid="fn1">†</xref></p>
      <fig id="fig1" orientation="portrait" position="float">
        <label>Fig. 1</label>
        <caption>
          <title>Image-to-SMILES neural network used for hand-drawn hydrocarbon recognition based on neural image captioning. The network consists of a convolutional neural network (CNN) encoder and long short-term memory (LSTM) decoder network.</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f1"/>
      </fig>
      <p>We define the NN accuracy as the proportion of molecules predicted exactly correctly, <italic>i.e.</italic>, the predicted SMILES matches the target SMILES character-by-character. Error bars were calculated by bootstrapping the accuracy of 1000 sets of 200 data points sampled from the test set with replacement and computing the range that contains the statistical mean with 95% likelihood based on the resampled sets.</p>
    </sec>
    <sec>
      <title>Datasets</title>
      <p>We extracted a dataset of 500 000 SMILES strings with a ring size of less than eight carbon atoms from the GDB-13 and GDB-11 databases.<sup><xref rid="cit43" ref-type="bibr">43–45</xref></sup> The vocabulary was restricted to “Cc=#()1”, where = and # indicate double and triple bonds, parentheses indicate the start and end of a branching group, lower case letters represent aromaticity, and numbers are found at the start and end of rings. To remove ambiguous skeletal structures from our dataset that confuse the NN during training, we only include the number ‘1’, meaning that molecules with multiple conjoined rings are not considered. The SMILES labels were canonicalized using RDKit to give a consistent target output. After canonicalization, molecules outside of the vocabulary were removed, resulting in a ∼10% reduction in size for all datasets used in the experiments presented. RDKit was used to generate images of molecules from the SMILES dataset, by first generating SVG files and then converting to PNG format. The result is a labelled dataset of image and SMILES pairs; representative examples are shown in the <xref ref-type="fig" rid="fig2">Fig. 2</xref> inset. We used this clean RDKit dataset to perform proof-of-concept for the image-to-SMILES network. A synthetic dataset based on RDKit images designed to mimic hand-drawn data was curated for the purpose of this study. We discuss the auxiliary data generation workflow, and experiments performed on this dataset in the coming sections.</p>
      <fig id="fig2" orientation="portrait" position="float">
        <label>Fig. 2</label>
        <caption>
          <title>Out-of-sample accuracy of the image-to-SMILES network trained with an increasing number of clean RDKit hydrocarbon structures and their corresponding SMILES label. Representative examples of labelled RDKit training images and SMILES are shown in the inset.</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f2"/>
      </fig>
      <p>The computer-generated datasets were first split into a 90% training/validation set, and a 10% test set. The test set serves as out-of-sample data used to evaluate the accuracy of the network after finishing the training process. The training/validation set, used during training, was then split further into a training set (90%) and a validation set (10%). The real-world photographs of hand-drawn hydrocarbons consisted of a total of 613 images. We set aside a 200-image test set, with the remaining 413 images being either used entirely as a validation set or split into validation (200 images) and training (213 images) datasets, depending on the experiment. All images were resized to 256 × 256 pixels and converted to PNG format using OpenCV.<sup><xref rid="cit46" ref-type="bibr">46</xref></sup></p>
    </sec>
  </sec>
  <sec>
    <title>Results and discussion</title>
    <sec>
      <title>Synthetic data generation</title>
      <p>To test the suitability of our image-to-SMILES network for hand-drawn molecule recognition, we begin by training with clean images of hydrocarbon skeletal structures generated with RDKit and their respective SMILES labels (<xref ref-type="fig" rid="fig2">Fig. 2</xref>). In order to determine the dataset size required to achieve a given recognition accuracy, the NN was trained with datasets of size 10<sup>4</sup>, 5 × 10<sup>4</sup>, 10<sup>5</sup>, 2 × 10<sup>5</sup> and 5 × 10<sup>5</sup> images (split between training, validation and test sets as described in the methods section). The results of the proof-of-concept training are shown in <xref ref-type="fig" rid="fig2">Fig. 2</xref>, illustrating the increasing NN recognition accuracy with dataset size. A dataset of 50 000 labelled RDKit images achieves an out-of-sample (test set) accuracy of over 90%, and a maximum accuracy of 98% is achieved with a dataset of 500 000 images. This demonstrates that the chosen NN architecture is capable of learning SMILES strings from machine-generated images of hydrocarbons.</p>
      <p>Although the results from training with synthetic RDKit images suggest that a dataset of 50 000 images obtains 90% out-of-sample accuracy, in reality a much greater number of hand-drawn hydrocarbon molecules are likely needed to achieve this same accuracy. As with handwritten text recognition, variation in drawing style, backgrounds and image quality provide significant challenges. There is noise associated with (i) the chemical structure, such as varying line widths, lengths, angles and distortion, (ii) the background, such as different textures, lighting, colors and surrounding text, and (iii) the photograph, such as blurring, pixel count and image format (<xref ref-type="fig" rid="fig3">Fig. 3</xref>). A further challenge of chemical structure recognition is the ability for a molecule to be drawn in any orientation, in contrast to text recognition of languages written in one direction, <italic>e.g.</italic>, left-to-right.</p>
      <fig id="fig3" orientation="portrait" position="float">
        <label>Fig. 3</label>
        <caption>
          <title>Comparison between a computer-generated (RDKit) image of a hydrocarbon structure (left) and a photographed hand-drawn hydrocarbon structure (right). The differences between the two images are highlighted, demonstrating the increased complexity of hand-drawn structure recognition.</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f3"/>
      </fig>
      <p>Since end-to-end NNs learn a model solely from the data presented during training, access to high-quality data is imperative to achieve an accurate model. Unfortunately, a large labelled dataset of real-world hand-drawn molecules does not exist and cannot be easily generated. Therefore, unlike in the case of RDKit images, it is not possible to achieve high recognition accuracy by simply training with hundreds of thousands of hand-drawn structures. Lack of training data is a common hurdle when attempting to apply end-to-end deep learning models to real-world problems, particularly in fields where data generation is time and energy intensive such as the chemical domain. In cases such as these, generating synthetic data can prove more efficient than spending excessive time and resources collecting large amounts of real-world data.</p>
      <p>We developed a data collection web app to source a small dataset of hand-drawn chemical structures. In order to capture the large noise in drawing style, photograph quality and background types that are prevalent in real-world data, we collected data from many different drawers by promoting the app to a range of groups in the Stanford University Chemistry Department. Over 100 unique users of the app generated over 5800 photographs of hand-drawn chemical structures, 613 of which were hydrocarbons. Details of the data collection app are shown in Fig. S1<xref ref-type="fn" rid="fn1">†</xref> and the collected dataset is released with this paper.<sup><xref rid="cit47" ref-type="bibr">47</xref></sup> Based on our earlier RDKit image results (<xref ref-type="fig" rid="fig2">Fig. 2</xref>), ∼600 images is several orders of magnitude less data than necessary to train to any reasonable recognition accuracy. As a result, in addition to sourcing real-world data, we also developed a workflow to generate a large synthetic dataset to be used in conjunction with the limited real-world dataset for training. We go on to show that our strategy is able to successfully train an accurate NN with this limited amount of real-world data. This is an encouraging result for machine learning approaches in the chemical sciences, where the availability of accurate data is often problematic.</p>
      <p>An ideal synthetic dataset is exactly equivalent to the target data but can be readily generated on large scales (unlike the target data). The desired datatype (of which there is insufficient data for training) could therefore be substituted with synthetic data during training and the weights would be directly transferable to the target data. To discuss how to generate such an auxiliary dataset, we consider a subspace that spans from the desired datatype to a similar machine-scalable datatype. In our case, this is the subspace between photographs of hand-drawn molecules and RDKit images. The aim is to find a mapping that moves both datatypes to the same point in the subspace such that they are indistinguishable. <xref ref-type="fig" rid="fig4">Fig. 4</xref> depicts such a subspace, highlighting possible convergence routes. Perhaps the most obvious pathway transforms raw RDKit data (bottom right) into images that resemble raw hand-drawn data (top left) as closely as possible (or <italic>visa versa</italic>). This might involve adding in backgrounds, distorting the lines and blurring the image. However, it is also possible to modify both datatypes such that they reach a common point in the subspace that lies away from both of the original data points. As long as the two datatypes are uniquely mapped to the same point, they are equivalent. For example, applying edge detection (or background removal) to both the hand-drawn and computer-generated data would result in movement away from their respective raw datatypes, but closer to one another. In this illustrative example, a model would be trained with an edge-detected synthetic dataset, and later applied to hand-drawn hydrocarbon molecule images that have been pre-processed with edge detection.</p>
      <fig id="fig4" orientation="portrait" position="float">
        <label>Fig. 4</label>
        <caption>
          <title>Data subspace that spans from target data (photographs of hand-drawn hydrocarbon chemical structures, top left) to readily available machine-generated data (raw RDKit images, bottom right). Paths to reach similar points in the subspace for the target data and synthetic data are demonstrated. Blue outline: hand-drawn images, red outline: computer-generated images.</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f4"/>
      </fig>
      <p>Mapping two datatypes to a common point in a subspace is commonly used in deep learning applications since there is often a limited amount of the exact data needed, but a similar readily accessible datatype that can form the basis of a synthetic dataset.<sup><xref rid="cit10" ref-type="bibr">10,48,49</xref></sup> It is important to note that a one-to-one mapping between the two datatypes and the output label must exist, <italic>i.e.</italic>, one image should only correspond to exactly one molecule.</p>
      <p>Although we did explore auxiliary datasets based on background removal and edge detection algorithms, we abandoned these image processing techniques because they were found to be brittle when applied to real-world hand-drawn data. For example, dark shadows, lined paper and thin pencils made it hard to clearly identify the molecule after applying such algorithms (Fig. S2<xref ref-type="fn" rid="fn1">†</xref>). To ensure the recognition software is robust to a wide range of potential images, for the remainder of this study we focus on generating a synthetic dataset that resembles hand-drawn molecules as closely as possible.</p>
      <p><xref ref-type="fig" rid="fig5">Fig. 5a</xref> outlines the synthetic data generation workflow developed to transform RDKit images into synthetic photographs of hand-drawn hydrocarbon structures. First, we introduce randomness to bond angles, lengths and widths <italic>via</italic> modification of the RDKit source code (RDKit'). The image is then passed through an augmentation pipeline which applies a series of random image transformations (RDKit'-aug). The augmented molecule image is then combined with a randomly augmented background image using OpenCV (RDKit'-aug-bkg). Next, the image is passed through a degradation pipeline to form the final synthetic data (RDKit'-aug-bkg-deg). The molecule augmentation, background augmentation and image degradation workflows are outlined in <xref ref-type="fig" rid="fig5">Fig. 5b</xref> (the transformations applied in these pipelines are detailed in Table S2<xref ref-type="fn" rid="fn1">†</xref>). A comparison of examples from the synthetic dataset and the real-world dataset can be found in Fig. S5.<xref ref-type="fn" rid="fn1">†</xref></p>
      <fig id="fig5" orientation="portrait" position="float">
        <label>Fig. 5</label>
        <caption>
          <title>(a) The synthetic data generation workflow with the datatype's assigned name for each stage of the pipeline. (b) The augment molecule, augment background and degradation pipelines used for the synthetic data generation. Each box corresponds to a function that is applied with probability <italic>p</italic>. A complete list of the image transforms associated with each function is given in the ESI.<xref ref-type="fn" rid="fn1">†</xref> (c) Schematic depiction of how the steps in the synthetic data workflow move the synthetic data distribution towards the hand-drawn data distribution by representing the datasets as two-dimensional Gaussians (not to scale).</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f5"/>
      </fig>
      <p>Generating a synthetic datapoint from a SMILES string takes ∼1 s, hence, over 85 000 labelled images of hydrocarbons can be produced in 24 hours of compute time. For comparison, it takes ∼1 minute for a human to draw, photograph, and label a hydrocarbon chemical structure, meaning that ∼2 months of continuous human effort would be needed to collect a dataset of this size.</p>
      <p>The molecule and background image augmentation pipelines (<xref ref-type="fig" rid="fig5">Fig. 5b</xref>) introduce noise into the data through rotations, translations, distortion and other image transformations. This acts as a form of regularization during training to reduce overfitting (where the NN reaches high accuracies during training but much lower accuracies on out-of-sample data). The importance of broadening the data distribution can be exemplified with background augmentation: without augmenting backgrounds the NN may become overly familiar with the structure of the background images used during training and learn to remove them from the image. The result is bad generalization when presented with images that have different backgrounds to those seen during training. We also randomly degrade the data to further increase the regularization. This accounts for features like variation in image quality and type. The degradation pipeline was adapted from work by Ingle <italic>et al.</italic>,<sup><xref rid="cit10" ref-type="bibr">10</xref></sup> which leveraged a large dataset of online data for offline hand-written text recognition by applying aggressive degradation. The augmentation and degradation are deliberately more aggressive than what would be found in real-world images to span the maximum dataset subspace, <italic>i.e.</italic>, make the distribution as wide as possible.</p>
      <p>As described previously, the stages of the synthetic data generation pipeline are designed to map the synthetic distribution onto the distribution of real-world hand-drawn chemical structures. A simplified schematic of how each step effects the data distribution is shown in <xref ref-type="fig" rid="fig5">Fig. 5c</xref>. The datasets are represented as two-dimensional Gaussians, with their amplitude proportional to the quantity of data and their width proportional to the data variation within the distribution. As the data proceeds through the augmentation, background addition and degradation steps, the synthetic distribution approaches the hand-drawn data distribution in the subspace.</p>
    </sec>
    <sec>
      <title>Neural network experiments</title>
      <p>In the following section we describe a series of experiments designed to understand how our real-world and synthetic datasets can be best utilized to achieve the highest out-of-sample hand-drawn hydrocarbon recognition accuracy. We form an ensemble model from the trained NNs, which allows us to assign a confidence value to the prediction, as well as improve the recognition accuracy. We finish by analysing the success of the model on specific examples from the test set and its performance on chemical subsets.</p>
      <p>First, we investigate how the NN performs when exposed only to synthetic data during training. To determine the effect of moving through the synthetic data generation pipeline (<xref ref-type="fig" rid="fig5">Fig. 5</xref>), we train the model on data from each stage of the workflow. Fig. S7<xref ref-type="fn" rid="fn1">†</xref> shows that image augmentation and degradation result in large increases in recognition of hydrocarbons in the hand-drawn test set, and somewhat surprisingly, the addition of backgrounds has an insignificant effect on the accuracy. By training with 500 000 synthetic images (RDKit'-aug-bkg-deg), we are able to correctly recognise an out-of-sample photograph of a hand-drawn hydrocarbon structure with over 50% accuracy. Although this accuracy is insufficient, at this stage the neural network has never seen a real-life hand-drawn image. We improve the accuracy by introducing our limited hand-drawn dataset to the training process as discussed below.</p>
      <p>In situations where there is limited access to data, a common strategy, is to use a real-world data validation set so the NN weights are saved according to the correct target distribution. We examine the effect of replacing the synthetic validation set with a 413-image hand-drawn validation set, varying the size of the synthetic training set from 50 000 to 500 000 (Fig. S8<xref ref-type="fn" rid="fn1">†</xref>). Using a hand-drawn validation set has little impact on the hand-drawn recognition accuracy in comparison to using a synthetic validation set since the number of images available is so limited.</p>
      <p>We now incorporate hand-drawn data into the training set so that it can directly impact the weight optimization during training, allowing the NN to learn from the target data, rather than only determine if the weights should be saved. The number of remaining images of hand-drawn hydrocarbon structures in our dataset after the removal of the test set is 413, which must be distributed between the training set and validation set. We assign 213 images to the training set and 200 images to the validation set. A dataset of 500 000 images is chosen since it reached the highest accuracies in our synthetic data experiments.</p>
      <p>We trained the image-to-SMILES network with varying ratios of augmented and degraded real-world hand-drawn and synthetic data, and tested the weights on the 200 image hand-drawn test set. Due to the very limited hand-drawn hydrocarbon data, we augmented and degraded the images to produce the number needed in the training set to satisfy each given ratio. For example, to generate a training set of 50% hand-drawn and 50% synthetic images (250 000 images each), each hand-drawn image was augmented ∼1173 times using the augment molecule pipeline (<xref ref-type="fig" rid="fig5">Fig. 5b</xref>, excluding the final translation step). Although this introduces a large number of repeated SMILES and similar images, the small amount of hand-drawn data makes this necessary to ensure that the information is not overridden by the large amount of synthetic data. Once the molecules have been augmented and degraded, the synthetic and hand-drawn data are randomly shuffled together for training.</p>
      <p>We investigate ratios of 0 : 100, 10 : 90, 50 : 50, 90 : 10 and 100 : 0 synthetic : hand-drawn data. From <xref ref-type="fig" rid="fig6">Fig. 6a</xref>, it can be seen that using entirely hand-drawn data results in an out-of-sample accuracy of 0% due to the network overfitting to the very narrow distribution of hand-drawn training data. Adding synthetic data allows the NN to be exposed to many more molecules and image types, and hence leads to a rapid increase in test set accuracy up to 90 : 10 synthetic : hand-drawn data. Removing the final 10% of hand-drawn hydrocarbon molecules from the training set (equivalent to the 500 000 image training run presented in Fig. S8<xref ref-type="fn" rid="fn1">†</xref>), however, leads to a decrease in the hydrocarbon recognition accuracy from 62% to 56%. Therefore, the results suggest that two opposing effects are at play: (i) including target data in the training set allows the weights to be optimized for the target application and (ii) including only a narrow or sparse distribution of target data leads to overfitting. As a result, including a small portion of target data, specifically 10% hand-drawn molecules, yields the highest recognition accuracy.</p>
      <fig id="fig6" orientation="portrait" position="float">
        <label>Fig. 6</label>
        <caption>
          <title>Recognition accuracy of the hand-drawn hydrocarbon test set of trained neural network with different training/validation datasets. (a) Results of training with varying ratios of augmented and degraded hand-drawn hydrocarbon to synthetic data training sets (500 000 image total) and hand-drawn hydrocarbons validation set. (b) The effect of fine tuning is investigated by restarting the weights from training with a 500 000-image synthetic dataset used for both training and validation, and a 500 000-image synthetic dataset used for training with a hand-drawn validation set. The weights are restarted with a training set consisting of 90% synthetic data and 10% augmented and degraded hand-drawn data, and a validation set of hand-drawn hydrocarbons.</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f6"/>
      </fig>
      <p>In all the experiments discussed so far, the image-to-SMILES network has been trained from scratch, <italic>i.e.</italic>, the weights are randomly initialized. When applying deep learning to tasks with limited available data, training the network with a large dataset before restarting the weights with a similar dataset has been shown to increase NN accuracy.<sup><xref rid="cit50" ref-type="bibr">50</xref></sup> This approach is termed fine-tuning due to the NN weights being tuned from a related task to better suit the desired datatype. We apply fine-tuning to our problem by first training with synthetic data and then restarting the NN weights with training data that includes real-life images of hand-drawn hydrocarbon structures. We fine-tune two trained NNs, both of which use 500 000 image synthetic training datasets but that differ in their validation data: the first uses a synthetic validation set (pre-training results shown in Fig. S7b<xref ref-type="fn" rid="fn1">†</xref>) and the second uses a hand-drawn validation set (pre-training results shown in Fig. S8<xref ref-type="fn" rid="fn1">†</xref>). The two trained NNs are restarted with a training set made up of 90% synthetic data and 10% hand-drawn data – the optimal ratio according to <xref ref-type="fig" rid="fig6">Fig. 6a</xref>. The results from the two fine-tuning runs (<xref ref-type="fig" rid="fig6">Fig. 6b</xref>) show that pre-training with synthetic data before incorporating hand-drawn data into the training set improves the molecule recognition accuracy. The network reaches 67.5% accuracy after pre-training with a hand-drawn validation set, in comparison to the best NN trained from scratch which was 61.5% accurate.</p>
    </sec>
    <sec>
      <title>Ensemble learning</title>
      <p>Instead of relying on a single model to predict a desired output, combining several models can result in improved performance, a process called ensemble learning.<sup><xref rid="cit51" ref-type="bibr">51</xref></sup> There are several ways in which ensemble models can operate, such as boosting, bagging and random forests.<sup><xref rid="cit52" ref-type="bibr">52</xref></sup> Here, we form an ensemble model comprised of a committee of trained NNs, where each NN casts a single vote according to their prediction. The predictions can then be ordered from most to least votes and the prediction that has the most votes is output. The number of agreeing votes for a prediction can give insight into the confidence of the ensemble model. If all of the NNs predict the same output, there is a high probability the prediction is accurate. However, if the NNs disagree, there is higher uncertainty in the prediction.</p>
      <p>We build an ensemble model comprised of trained NNs from previous experiments that achieve at least 50% accuracy on the hand-drawn test set (5 out of the 17 trained NNs). The out-of-sample hand-drawn hydrocarbon recognition accuracy for the ensemble model is shown in <xref ref-type="fig" rid="fig7">Fig. 7a</xref>, comparing the three predictions that have the most votes with the reference SMILES label. The ensemble model achieves an accuracy of 76% on the hand-drawn test set for the top prediction and 85.5% if the top three predictions are considered. By forming a committee of NNs, we see a significant improvement in accuracy in comparison to the constituent NNs (the highest of which obtained 67.5% on out-of-sample hand-drawn data).</p>
      <fig id="fig7" orientation="portrait" position="float">
        <label>Fig. 7</label>
        <caption>
          <title>(a) The out-of-sample hand-drawn hydrocarbon recognition accuracy of the highest <italic>N</italic> ranked predictions of an ensemble model made up of trained NNs with over 50% recognition accuracy on out-of-sample images of hand-drawn hydrocarbon molecules. (b) The out-of-sample hand-drawn hydrocarbon recognition accuracy of the ensemble model when the top prediction has a given number of agreeing votes, <italic>V</italic> (blue) and the percentage occurrence of a given number of agreeing votes for the top prediction (red). The accuracy is attributed to the confidence of the model when there are <italic>V</italic> votes for the top SMILES prediction.</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f7"/>
      </fig>
      <p>The agreement between the models that make up the committee offers insight into the certainty of the prediction. <xref ref-type="fig" rid="fig7">Fig. 7b</xref> shows the increase of recognition accuracy as the number of votes for the top-ranked prediction, <italic>V</italic>, rises. Here, we assign the accuracy of the ensemble model when there are <italic>V</italic> agreeing votes to its confidence value. When all the models disagree (<italic>V</italic> = 1) the model has low out-of-sample accuracy, equating to a low confidence value of the model. When more models agree, the prediction tends to have a higher accuracy. All of the models agreeing (<italic>V</italic> = 5) translates to a confidence value of 98% in the predicted hydrocarbon.</p>
      <p>In addition to knowing the confidence of the model's prediction, it is useful to know how often it achieves this confidence: if the model was 100% confident when all the votes agreed but this only occurred 1% of the time its use would be limited. We therefore investigate the portion of times that a confidence value occurs in the ensemble model's test set predictions (<xref ref-type="fig" rid="fig7">Fig. 7b</xref>). It can be seen that the percentage of times that <italic>V</italic> votes occurs increases with the number of votes – there are few instances where all the NNs disagree (<italic>V</italic> = 1), and by far the most common occurrence is all NNs agreeing (<italic>V</italic> = 5).</p>
      <p>The importance of knowing the uncertainty of a model's prediction should not be underestimated. In many cases, it is more important to achieve a lower accuracy but be able to predict when the model will fail, than to achieve a higher accuracy without insight into when it will fail. For example, in the case of autonomous vehicles, a model that is able to determine when it will fail and prompt a human to take over controls would be far safer than a model that failed less but was unable to forecast failure. In the case of hand-drawn molecule recognition, the software could, for example, prompt the user to take a second photograph if the uncertainty of the model was high. It may also offer insight into if an erroneous molecule was input by the user, as this would likely cause confusion and result in disagreement between committee members. A potential feature for the ChemPix app is to show the top three predictions if the uncertainty of the first prediction is high; the user could then select the correct molecule from the three if it appears. This data could be continuously collected and fed back into the NN to iteratively re-train and improve its performance as more data is collected.</p>
      <p>Of course, both the accuracy and confidence of the NN output should be optimized. Here, our ensemble model recognizes the correct molecule with 89% confidence in over 70% of cases and with near 100% confidence in over 50% of cases. This is a promising result for deploying this technology to real-world applications.</p>
      <p>A selection of examples from the hand-drawn test set and their respective predictions from the ensemble are highlighted in <xref ref-type="fig" rid="fig8">Fig. 8</xref>. The model is able to recognise a wide variety of hydrocarbons with different sized rings and chain lengths. The network confidently recognizes hydrocarbons drawn on a variety of textured materials, including a napkin, whiteboard and paper. The network is able to determine the molecular structure despite dark shadows and bright spots in the photograph, as well as molecules drawn with a range of pen and pencil types. Wavy lines and “unnatural” bond angles are generally handled well.</p>
      <fig id="fig8" orientation="portrait" position="float">
        <label>Fig. 8</label>
        <caption>
          <title>Representative examples of cyclic (top) and acyclic (bottom) molecules from the hand-drawn hydrocarbon test set and their corresponding predictions from the ensemble. The input image is presented next to the predictions; the number of votes for each predicted molecule, <italic>V</italic>, is shown, along with if the molecule was recognised correctly. Predictions of invalid SMILES strings are shown as N/A. Hydrocarbons that are recognised correctly by one of the models predictions are outlined in green, and those that fail to be predicted correctly are outlined in red.</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f8"/>
      </fig>
      <p>As far as we can determine, there is not a clear pattern between molecules that are predicted correctly and incorrectly, however we notice some features that make the recognition more challenging. Molecules drawn on lined and squared paper can increase the difficulty in comparison to those drawn on plain paper. The networks also struggle more when benzene rings are drawn in the resonance hybrid style (with a circle) in comparison to the Kekulé structure. This is likely due to the RDKit generated training imaged being exclusively Kekulé. As discussed previously, incorrectly predicted structures generally have disagreeing committee members. A rare case in which all the committee members agree on an incorrect prediction is shown in <xref ref-type="fig" rid="fig8">Fig. 8</xref>: α-Methyl-<italic>cis</italic>-stilbene is wrongly identified since two bonds are mistaken for one, resulting in a structure that is very close to correct. It is common for wrong predictions to contain only a minor mistake such as this. We also note that a large portion of the images in the hand-drawn dataset consists of molecules that are drawn on paper with chemical structures drawn on the opposite side of the page. In many cases, these structures bleed through the page, confusing the network. Lastly, we note that the model currently does not handle conjoined rings due to limitations of RDKit's image generation, which depicts bridges differently from the standard chemistry drawing style. This could be addressed by applying a different chemical structure renderer and/or collecting more hand-drawn structure data. The full test set with the corresponding reference and predicted SMILES can be found in the ESI.<xref ref-type="fn" rid="fn1">†</xref></p>
      <p><xref ref-type="fig" rid="fig9">Fig. 9a</xref> highlights the SMILES error for predictions of invalid molecules. The largest portion of errors corresponds to unclosed or unopened parenthesis, with the next most prominent error being rings left unclosed or the closure duplicated. This gives insight into the somewhat lower accuracy of branched molecules and rings. Lastly, a small portion of errors correspond to carbons with a valence greater than four, syntactical errors (<italic>e.g.</italic> a SMILES string ending in “=”), and aromatic carbons outside of a ring. Invalid SMILES predictions are quite rare (6.5% of the total predictions), and tend to be correlated with challenging images where the model has low confidence of the prediction.</p>
      <fig id="fig9" orientation="portrait" position="float">
        <label>Fig. 9</label>
        <caption>
          <title>(a) Proportion of errors associated with invalid SMILES predictions. (b) Recognition accuracy of highest ranked ensemble prediction for subsets of the hand-drawn hydrocarbon test set. From left-to-right: all hydrocarbons (vocab: “Cc=#()1”), acyclic hydrocarbons only (vocab: “C=#()”), cyclic hydrocarbons only (vocab: “Cc=#()1”, must contain “1”), unbranched hydrocarbons only (vocab:“Cc=#1”), invalid predicted SMILES removed from predictions.</title>
        </caption>
        <graphic xlink:href="d1sc02957f-f9"/>
      </fig>
      <p>To gain insight into if the model more or less accurately recognizes certain types of molecules, we compute the accuracy of the ensemble's first prediction for subsets of the test set, including acyclic, cyclic and unbranched hydrocarbons (<xref ref-type="fig" rid="fig9">Fig. 9b</xref>). The recognition accuracy is seen to be relatively consistent between the different groups of molecules, however, molecules without rings are correctly recognised slightly more often than those with rings, and un-branched molecules (those without “()” in their SMILES string) are more accurate still. We also investigate the effect of removing all invalid SMILES from the predictions, which leads to an insignificant change in accuracy.</p>
    </sec>
  </sec>
  <sec>
    <title>Conclusions</title>
    <p>In this work, we demonstrate how deep learning can be used to develop an offline hand-drawn hydrocarbon structure recognition tool. We curated a large synthetic dataset and a small hand-drawn dataset and explored how to best leverage the two to maximize molecule recognition accuracy. The datasets were used to train an image-to-SMILES neural network to extract the molecule from a photographed hand-drawn hydrocarbon structure. Training with synthetic data only leads to only 50% recognition accuracy on real-life hand-drawn hydrocarbons. Replacing a small fraction of the training set with augmented hand-drawn images and applying fine-tuning leads to an improvement of hand-drawn hydrocarbon recognition accuracy to nearly 70%. The trained data-driven models were combined with ensemble learning to achieve superior accuracy to the constituent models and gain information on when the model would fail. The final model achieved an accuracy of 76%, and the top three predictions included the exactly correct molecule over 85% of the time.</p>
    <p>Extending the hydrocarbon recognition results presented in this paper to the recognition of all molecules offers an obvious extension, however, variation in hand-drawn font style and letter location provides a significant challenge. A hybrid rule-based and data-driven workflow offers one strategy to overcome these barriers. For example, a functional group detector network and hydrocarbon backbone recognition network, such as that presented in this study, could be combined with a rule-based model to produce the complete molecular structure. We also plan to explore neural style transfer to enhance the quality of the synthetic data.<sup><xref rid="cit53" ref-type="bibr">53</xref></sup></p>
    <p>The chemical structure recognition software developed in this work has many interesting use cases, such as connecting it to a user interface to be used as a phone or tablet application. A wide range of chemistry software could then be connected to the backend such as theoretical chemistry packages, lab notebooks and analytical tools. It would be particularly useful for software that currently requires knowledge of coding, command line scripting, and specialized input file format and so is inaccessible to large sections of the chemistry community. Connection to ChemVox<sup><xref rid="cit6" ref-type="bibr">6</xref></sup> voice control and TeraChem Cloud<sup><xref rid="cit54" ref-type="bibr">54</xref></sup> electronic structure service offers one example of a potentially powerful integrated tool. Since drawing a chemical structure by hand is a familiar task for all chemists, this app would lower the barrier of accessing such software. As a result, these currently unattainable tools could be readily incorporated into laboratories and classrooms to catalyse advances in both chemical research and education.</p>
  </sec>
  <sec sec-type="data-availability">
    <title>Data availability</title>
    <p>Data and source code can be found at <uri xlink:href="https://github.com/mtzgroup/ChemPixCH">https://github.com/mtzgroup/ChemPixCH</uri>.</p>
  </sec>
  <sec>
    <title>Author contributions</title>
    <p>HW, KT, and TJM conceptualized the project and contributed to methodology. HW wrote the original draft. All authors contributed to reviewing and editing of the manuscript. HW led the investigation and implemented the models. AW, BC, and AB contributed to data generation and acquisition. TJM supervised the project and contributed to funding acquisition, resources, and project administration.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Conflicts of interest</title>
    <p>There are no conflicts to declare.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s001">
      <label>SC-012-D1SC02957F-s001</label>
      <media xlink:href="SC-012-D1SC02957F-s001.pdf" orientation="portrait" id="d31e678" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s002">
      <label>SC-012-D1SC02957F-s002</label>
      <media xlink:href="SC-012-D1SC02957F-s002.txt" orientation="portrait" id="d31e681" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s003">
      <label>SC-012-D1SC02957F-s003</label>
      <media xlink:href="SC-012-D1SC02957F-s003.png" orientation="portrait" id="d31e684" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s004">
      <label>SC-012-D1SC02957F-s004</label>
      <media xlink:href="SC-012-D1SC02957F-s004.png" orientation="portrait" id="d31e687" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s005">
      <label>SC-012-D1SC02957F-s005</label>
      <media xlink:href="SC-012-D1SC02957F-s005.png" orientation="portrait" id="d31e690" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s006">
      <label>SC-012-D1SC02957F-s006</label>
      <media xlink:href="SC-012-D1SC02957F-s006.png" orientation="portrait" id="d31e693" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s007">
      <label>SC-012-D1SC02957F-s007</label>
      <media xlink:href="SC-012-D1SC02957F-s007.png" orientation="portrait" id="d31e696" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s008">
      <label>SC-012-D1SC02957F-s008</label>
      <media xlink:href="SC-012-D1SC02957F-s008.png" orientation="portrait" id="d31e699" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s009">
      <label>SC-012-D1SC02957F-s009</label>
      <media xlink:href="SC-012-D1SC02957F-s009.png" orientation="portrait" id="d31e702" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s010">
      <label>SC-012-D1SC02957F-s010</label>
      <media xlink:href="SC-012-D1SC02957F-s010.png" orientation="portrait" id="d31e705" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s011">
      <label>SC-012-D1SC02957F-s011</label>
      <media xlink:href="SC-012-D1SC02957F-s011.png" orientation="portrait" id="d31e709" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s012">
      <label>SC-012-D1SC02957F-s012</label>
      <media xlink:href="SC-012-D1SC02957F-s012.png" orientation="portrait" id="d31e712" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s013">
      <label>SC-012-D1SC02957F-s013</label>
      <media xlink:href="SC-012-D1SC02957F-s013.png" orientation="portrait" id="d31e715" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s014">
      <label>SC-012-D1SC02957F-s014</label>
      <media xlink:href="SC-012-D1SC02957F-s014.png" orientation="portrait" id="d31e718" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s015">
      <label>SC-012-D1SC02957F-s015</label>
      <media xlink:href="SC-012-D1SC02957F-s015.png" orientation="portrait" id="d31e721" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s016">
      <label>SC-012-D1SC02957F-s016</label>
      <media xlink:href="SC-012-D1SC02957F-s016.png" orientation="portrait" id="d31e724" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s017">
      <label>SC-012-D1SC02957F-s017</label>
      <media xlink:href="SC-012-D1SC02957F-s017.png" orientation="portrait" id="d31e727" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s018">
      <label>SC-012-D1SC02957F-s018</label>
      <media xlink:href="SC-012-D1SC02957F-s018.png" orientation="portrait" id="d31e730" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s019">
      <label>SC-012-D1SC02957F-s019</label>
      <media xlink:href="SC-012-D1SC02957F-s019.png" orientation="portrait" id="d31e733" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s020">
      <label>SC-012-D1SC02957F-s020</label>
      <media xlink:href="SC-012-D1SC02957F-s020.png" orientation="portrait" id="d31e736" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s021">
      <label>SC-012-D1SC02957F-s021</label>
      <media xlink:href="SC-012-D1SC02957F-s021.png" orientation="portrait" id="d31e739" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s022">
      <label>SC-012-D1SC02957F-s022</label>
      <media xlink:href="SC-012-D1SC02957F-s022.png" orientation="portrait" id="d31e743" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s023">
      <label>SC-012-D1SC02957F-s023</label>
      <media xlink:href="SC-012-D1SC02957F-s023.png" orientation="portrait" id="d31e746" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s024">
      <label>SC-012-D1SC02957F-s024</label>
      <media xlink:href="SC-012-D1SC02957F-s024.png" orientation="portrait" id="d31e749" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s025">
      <label>SC-012-D1SC02957F-s025</label>
      <media xlink:href="SC-012-D1SC02957F-s025.png" orientation="portrait" id="d31e752" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s026">
      <label>SC-012-D1SC02957F-s026</label>
      <media xlink:href="SC-012-D1SC02957F-s026.png" orientation="portrait" id="d31e755" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s027">
      <label>SC-012-D1SC02957F-s027</label>
      <media xlink:href="SC-012-D1SC02957F-s027.png" orientation="portrait" id="d31e758" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s028">
      <label>SC-012-D1SC02957F-s028</label>
      <media xlink:href="SC-012-D1SC02957F-s028.png" orientation="portrait" id="d31e761" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s029">
      <label>SC-012-D1SC02957F-s029</label>
      <media xlink:href="SC-012-D1SC02957F-s029.png" orientation="portrait" id="d31e764" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s030">
      <label>SC-012-D1SC02957F-s030</label>
      <media xlink:href="SC-012-D1SC02957F-s030.png" orientation="portrait" id="d31e767" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s031">
      <label>SC-012-D1SC02957F-s031</label>
      <media xlink:href="SC-012-D1SC02957F-s031.png" orientation="portrait" id="d31e770" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s032">
      <label>SC-012-D1SC02957F-s032</label>
      <media xlink:href="SC-012-D1SC02957F-s032.png" orientation="portrait" id="d31e773" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s033">
      <label>SC-012-D1SC02957F-s033</label>
      <media xlink:href="SC-012-D1SC02957F-s033.png" orientation="portrait" id="d31e777" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s034">
      <label>SC-012-D1SC02957F-s034</label>
      <media xlink:href="SC-012-D1SC02957F-s034.png" orientation="portrait" id="d31e780" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s035">
      <label>SC-012-D1SC02957F-s035</label>
      <media xlink:href="SC-012-D1SC02957F-s035.png" orientation="portrait" id="d31e783" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s036">
      <label>SC-012-D1SC02957F-s036</label>
      <media xlink:href="SC-012-D1SC02957F-s036.png" orientation="portrait" id="d31e786" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s037">
      <label>SC-012-D1SC02957F-s037</label>
      <media xlink:href="SC-012-D1SC02957F-s037.png" orientation="portrait" id="d31e789" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s038">
      <label>SC-012-D1SC02957F-s038</label>
      <media xlink:href="SC-012-D1SC02957F-s038.png" orientation="portrait" id="d31e792" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s039">
      <label>SC-012-D1SC02957F-s039</label>
      <media xlink:href="SC-012-D1SC02957F-s039.png" orientation="portrait" id="d31e795" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s040">
      <label>SC-012-D1SC02957F-s040</label>
      <media xlink:href="SC-012-D1SC02957F-s040.png" orientation="portrait" id="d31e798" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s041">
      <label>SC-012-D1SC02957F-s041</label>
      <media xlink:href="SC-012-D1SC02957F-s041.png" orientation="portrait" id="d31e801" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s042">
      <label>SC-012-D1SC02957F-s042</label>
      <media xlink:href="SC-012-D1SC02957F-s042.png" orientation="portrait" id="d31e804" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s043">
      <label>SC-012-D1SC02957F-s043</label>
      <media xlink:href="SC-012-D1SC02957F-s043.png" orientation="portrait" id="d31e807" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s044">
      <label>SC-012-D1SC02957F-s044</label>
      <media xlink:href="SC-012-D1SC02957F-s044.png" orientation="portrait" id="d31e811" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s045">
      <label>SC-012-D1SC02957F-s045</label>
      <media xlink:href="SC-012-D1SC02957F-s045.png" orientation="portrait" id="d31e814" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s046">
      <label>SC-012-D1SC02957F-s046</label>
      <media xlink:href="SC-012-D1SC02957F-s046.png" orientation="portrait" id="d31e817" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s047">
      <label>SC-012-D1SC02957F-s047</label>
      <media xlink:href="SC-012-D1SC02957F-s047.png" orientation="portrait" id="d31e820" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s048">
      <label>SC-012-D1SC02957F-s048</label>
      <media xlink:href="SC-012-D1SC02957F-s048.png" orientation="portrait" id="d31e823" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s049">
      <label>SC-012-D1SC02957F-s049</label>
      <media xlink:href="SC-012-D1SC02957F-s049.png" orientation="portrait" id="d31e826" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s050">
      <label>SC-012-D1SC02957F-s050</label>
      <media xlink:href="SC-012-D1SC02957F-s050.png" orientation="portrait" id="d31e829" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s051">
      <label>SC-012-D1SC02957F-s051</label>
      <media xlink:href="SC-012-D1SC02957F-s051.png" orientation="portrait" id="d31e832" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s052">
      <label>SC-012-D1SC02957F-s052</label>
      <media xlink:href="SC-012-D1SC02957F-s052.png" orientation="portrait" id="d31e835" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s053">
      <label>SC-012-D1SC02957F-s053</label>
      <media xlink:href="SC-012-D1SC02957F-s053.png" orientation="portrait" id="d31e838" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s054">
      <label>SC-012-D1SC02957F-s054</label>
      <media xlink:href="SC-012-D1SC02957F-s054.png" orientation="portrait" id="d31e841" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s055">
      <label>SC-012-D1SC02957F-s055</label>
      <media xlink:href="SC-012-D1SC02957F-s055.png" orientation="portrait" id="d31e845" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s056">
      <label>SC-012-D1SC02957F-s056</label>
      <media xlink:href="SC-012-D1SC02957F-s056.png" orientation="portrait" id="d31e848" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s057">
      <label>SC-012-D1SC02957F-s057</label>
      <media xlink:href="SC-012-D1SC02957F-s057.png" orientation="portrait" id="d31e851" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s058">
      <label>SC-012-D1SC02957F-s058</label>
      <media xlink:href="SC-012-D1SC02957F-s058.png" orientation="portrait" id="d31e854" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s059">
      <label>SC-012-D1SC02957F-s059</label>
      <media xlink:href="SC-012-D1SC02957F-s059.png" orientation="portrait" id="d31e857" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s060">
      <label>SC-012-D1SC02957F-s060</label>
      <media xlink:href="SC-012-D1SC02957F-s060.png" orientation="portrait" id="d31e860" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s061">
      <label>SC-012-D1SC02957F-s061</label>
      <media xlink:href="SC-012-D1SC02957F-s061.png" orientation="portrait" id="d31e863" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s062">
      <label>SC-012-D1SC02957F-s062</label>
      <media xlink:href="SC-012-D1SC02957F-s062.png" orientation="portrait" id="d31e866" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s063">
      <label>SC-012-D1SC02957F-s063</label>
      <media xlink:href="SC-012-D1SC02957F-s063.png" orientation="portrait" id="d31e869" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s064">
      <label>SC-012-D1SC02957F-s064</label>
      <media xlink:href="SC-012-D1SC02957F-s064.png" orientation="portrait" id="d31e872" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s065">
      <label>SC-012-D1SC02957F-s065</label>
      <media xlink:href="SC-012-D1SC02957F-s065.png" orientation="portrait" id="d31e875" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s066">
      <label>SC-012-D1SC02957F-s066</label>
      <media xlink:href="SC-012-D1SC02957F-s066.png" orientation="portrait" id="d31e879" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s067">
      <label>SC-012-D1SC02957F-s067</label>
      <media xlink:href="SC-012-D1SC02957F-s067.png" orientation="portrait" id="d31e882" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s068">
      <label>SC-012-D1SC02957F-s068</label>
      <media xlink:href="SC-012-D1SC02957F-s068.png" orientation="portrait" id="d31e885" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s069">
      <label>SC-012-D1SC02957F-s069</label>
      <media xlink:href="SC-012-D1SC02957F-s069.png" orientation="portrait" id="d31e888" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s070">
      <label>SC-012-D1SC02957F-s070</label>
      <media xlink:href="SC-012-D1SC02957F-s070.png" orientation="portrait" id="d31e891" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s071">
      <label>SC-012-D1SC02957F-s071</label>
      <media xlink:href="SC-012-D1SC02957F-s071.png" orientation="portrait" id="d31e894" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s072">
      <label>SC-012-D1SC02957F-s072</label>
      <media xlink:href="SC-012-D1SC02957F-s072.png" orientation="portrait" id="d31e897" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s073">
      <label>SC-012-D1SC02957F-s073</label>
      <media xlink:href="SC-012-D1SC02957F-s073.png" orientation="portrait" id="d31e900" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s074">
      <label>SC-012-D1SC02957F-s074</label>
      <media xlink:href="SC-012-D1SC02957F-s074.png" orientation="portrait" id="d31e903" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s075">
      <label>SC-012-D1SC02957F-s075</label>
      <media xlink:href="SC-012-D1SC02957F-s075.png" orientation="portrait" id="d31e906" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s076">
      <label>SC-012-D1SC02957F-s076</label>
      <media xlink:href="SC-012-D1SC02957F-s076.png" orientation="portrait" id="d31e909" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s077">
      <label>SC-012-D1SC02957F-s077</label>
      <media xlink:href="SC-012-D1SC02957F-s077.png" orientation="portrait" id="d31e913" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s078">
      <label>SC-012-D1SC02957F-s078</label>
      <media xlink:href="SC-012-D1SC02957F-s078.png" orientation="portrait" id="d31e916" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s079">
      <label>SC-012-D1SC02957F-s079</label>
      <media xlink:href="SC-012-D1SC02957F-s079.png" orientation="portrait" id="d31e919" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s080">
      <label>SC-012-D1SC02957F-s080</label>
      <media xlink:href="SC-012-D1SC02957F-s080.png" orientation="portrait" id="d31e922" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s081">
      <label>SC-012-D1SC02957F-s081</label>
      <media xlink:href="SC-012-D1SC02957F-s081.png" orientation="portrait" id="d31e925" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s082">
      <label>SC-012-D1SC02957F-s082</label>
      <media xlink:href="SC-012-D1SC02957F-s082.png" orientation="portrait" id="d31e928" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s083">
      <label>SC-012-D1SC02957F-s083</label>
      <media xlink:href="SC-012-D1SC02957F-s083.png" orientation="portrait" id="d31e931" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s084">
      <label>SC-012-D1SC02957F-s084</label>
      <media xlink:href="SC-012-D1SC02957F-s084.png" orientation="portrait" id="d31e934" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s085">
      <label>SC-012-D1SC02957F-s085</label>
      <media xlink:href="SC-012-D1SC02957F-s085.png" orientation="portrait" id="d31e937" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s086">
      <label>SC-012-D1SC02957F-s086</label>
      <media xlink:href="SC-012-D1SC02957F-s086.png" orientation="portrait" id="d31e940" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s087">
      <label>SC-012-D1SC02957F-s087</label>
      <media xlink:href="SC-012-D1SC02957F-s087.png" orientation="portrait" id="d31e943" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s088">
      <label>SC-012-D1SC02957F-s088</label>
      <media xlink:href="SC-012-D1SC02957F-s088.png" orientation="portrait" id="d31e947" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s089">
      <label>SC-012-D1SC02957F-s089</label>
      <media xlink:href="SC-012-D1SC02957F-s089.png" orientation="portrait" id="d31e950" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s090">
      <label>SC-012-D1SC02957F-s090</label>
      <media xlink:href="SC-012-D1SC02957F-s090.png" orientation="portrait" id="d31e953" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s091">
      <label>SC-012-D1SC02957F-s091</label>
      <media xlink:href="SC-012-D1SC02957F-s091.png" orientation="portrait" id="d31e956" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s092">
      <label>SC-012-D1SC02957F-s092</label>
      <media xlink:href="SC-012-D1SC02957F-s092.png" orientation="portrait" id="d31e959" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s093">
      <label>SC-012-D1SC02957F-s093</label>
      <media xlink:href="SC-012-D1SC02957F-s093.png" orientation="portrait" id="d31e962" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s094">
      <label>SC-012-D1SC02957F-s094</label>
      <media xlink:href="SC-012-D1SC02957F-s094.png" orientation="portrait" id="d31e965" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s095">
      <label>SC-012-D1SC02957F-s095</label>
      <media xlink:href="SC-012-D1SC02957F-s095.png" orientation="portrait" id="d31e968" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s096">
      <label>SC-012-D1SC02957F-s096</label>
      <media xlink:href="SC-012-D1SC02957F-s096.png" orientation="portrait" id="d31e971" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s097">
      <label>SC-012-D1SC02957F-s097</label>
      <media xlink:href="SC-012-D1SC02957F-s097.png" orientation="portrait" id="d31e974" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s098">
      <label>SC-012-D1SC02957F-s098</label>
      <media xlink:href="SC-012-D1SC02957F-s098.png" orientation="portrait" id="d31e977" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s099">
      <label>SC-012-D1SC02957F-s099</label>
      <media xlink:href="SC-012-D1SC02957F-s099.png" orientation="portrait" id="d31e981" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s100">
      <label>SC-012-D1SC02957F-s100</label>
      <media xlink:href="SC-012-D1SC02957F-s100.png" orientation="portrait" id="d31e984" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s101">
      <label>SC-012-D1SC02957F-s101</label>
      <media xlink:href="SC-012-D1SC02957F-s101.png" orientation="portrait" id="d31e987" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s102">
      <label>SC-012-D1SC02957F-s102</label>
      <media xlink:href="SC-012-D1SC02957F-s102.png" orientation="portrait" id="d31e990" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s103">
      <label>SC-012-D1SC02957F-s103</label>
      <media xlink:href="SC-012-D1SC02957F-s103.png" orientation="portrait" id="d31e993" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s104">
      <label>SC-012-D1SC02957F-s104</label>
      <media xlink:href="SC-012-D1SC02957F-s104.png" orientation="portrait" id="d31e996" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s105">
      <label>SC-012-D1SC02957F-s105</label>
      <media xlink:href="SC-012-D1SC02957F-s105.png" orientation="portrait" id="d31e999" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s106">
      <label>SC-012-D1SC02957F-s106</label>
      <media xlink:href="SC-012-D1SC02957F-s106.png" orientation="portrait" id="d31e1002" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s107">
      <label>SC-012-D1SC02957F-s107</label>
      <media xlink:href="SC-012-D1SC02957F-s107.png" orientation="portrait" id="d31e1005" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s108">
      <label>SC-012-D1SC02957F-s108</label>
      <media xlink:href="SC-012-D1SC02957F-s108.png" orientation="portrait" id="d31e1008" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s109">
      <label>SC-012-D1SC02957F-s109</label>
      <media xlink:href="SC-012-D1SC02957F-s109.png" orientation="portrait" id="d31e1011" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s110">
      <label>SC-012-D1SC02957F-s110</label>
      <media xlink:href="SC-012-D1SC02957F-s110.png" orientation="portrait" id="d31e1015" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s111">
      <label>SC-012-D1SC02957F-s111</label>
      <media xlink:href="SC-012-D1SC02957F-s111.png" orientation="portrait" id="d31e1018" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s112">
      <label>SC-012-D1SC02957F-s112</label>
      <media xlink:href="SC-012-D1SC02957F-s112.png" orientation="portrait" id="d31e1021" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s113">
      <label>SC-012-D1SC02957F-s113</label>
      <media xlink:href="SC-012-D1SC02957F-s113.png" orientation="portrait" id="d31e1024" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s114">
      <label>SC-012-D1SC02957F-s114</label>
      <media xlink:href="SC-012-D1SC02957F-s114.png" orientation="portrait" id="d31e1027" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s115">
      <label>SC-012-D1SC02957F-s115</label>
      <media xlink:href="SC-012-D1SC02957F-s115.png" orientation="portrait" id="d31e1030" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s116">
      <label>SC-012-D1SC02957F-s116</label>
      <media xlink:href="SC-012-D1SC02957F-s116.png" orientation="portrait" id="d31e1033" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s117">
      <label>SC-012-D1SC02957F-s117</label>
      <media xlink:href="SC-012-D1SC02957F-s117.png" orientation="portrait" id="d31e1036" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s118">
      <label>SC-012-D1SC02957F-s118</label>
      <media xlink:href="SC-012-D1SC02957F-s118.png" orientation="portrait" id="d31e1039" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s119">
      <label>SC-012-D1SC02957F-s119</label>
      <media xlink:href="SC-012-D1SC02957F-s119.png" orientation="portrait" id="d31e1042" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s120">
      <label>SC-012-D1SC02957F-s120</label>
      <media xlink:href="SC-012-D1SC02957F-s120.png" orientation="portrait" id="d31e1045" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s121">
      <label>SC-012-D1SC02957F-s121</label>
      <media xlink:href="SC-012-D1SC02957F-s121.png" orientation="portrait" id="d31e1049" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s122">
      <label>SC-012-D1SC02957F-s122</label>
      <media xlink:href="SC-012-D1SC02957F-s122.png" orientation="portrait" id="d31e1052" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s123">
      <label>SC-012-D1SC02957F-s123</label>
      <media xlink:href="SC-012-D1SC02957F-s123.png" orientation="portrait" id="d31e1055" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s124">
      <label>SC-012-D1SC02957F-s124</label>
      <media xlink:href="SC-012-D1SC02957F-s124.png" orientation="portrait" id="d31e1058" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s125">
      <label>SC-012-D1SC02957F-s125</label>
      <media xlink:href="SC-012-D1SC02957F-s125.png" orientation="portrait" id="d31e1061" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s126">
      <label>SC-012-D1SC02957F-s126</label>
      <media xlink:href="SC-012-D1SC02957F-s126.png" orientation="portrait" id="d31e1064" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s127">
      <label>SC-012-D1SC02957F-s127</label>
      <media xlink:href="SC-012-D1SC02957F-s127.png" orientation="portrait" id="d31e1067" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s128">
      <label>SC-012-D1SC02957F-s128</label>
      <media xlink:href="SC-012-D1SC02957F-s128.png" orientation="portrait" id="d31e1070" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s129">
      <label>SC-012-D1SC02957F-s129</label>
      <media xlink:href="SC-012-D1SC02957F-s129.png" orientation="portrait" id="d31e1073" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s130">
      <label>SC-012-D1SC02957F-s130</label>
      <media xlink:href="SC-012-D1SC02957F-s130.png" orientation="portrait" id="d31e1076" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s131">
      <label>SC-012-D1SC02957F-s131</label>
      <media xlink:href="SC-012-D1SC02957F-s131.png" orientation="portrait" id="d31e1079" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s132">
      <label>SC-012-D1SC02957F-s132</label>
      <media xlink:href="SC-012-D1SC02957F-s132.png" orientation="portrait" id="d31e1083" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s133">
      <label>SC-012-D1SC02957F-s133</label>
      <media xlink:href="SC-012-D1SC02957F-s133.png" orientation="portrait" id="d31e1086" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s134">
      <label>SC-012-D1SC02957F-s134</label>
      <media xlink:href="SC-012-D1SC02957F-s134.png" orientation="portrait" id="d31e1089" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s135">
      <label>SC-012-D1SC02957F-s135</label>
      <media xlink:href="SC-012-D1SC02957F-s135.png" orientation="portrait" id="d31e1092" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s136">
      <label>SC-012-D1SC02957F-s136</label>
      <media xlink:href="SC-012-D1SC02957F-s136.png" orientation="portrait" id="d31e1095" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s137">
      <label>SC-012-D1SC02957F-s137</label>
      <media xlink:href="SC-012-D1SC02957F-s137.png" orientation="portrait" id="d31e1098" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s138">
      <label>SC-012-D1SC02957F-s138</label>
      <media xlink:href="SC-012-D1SC02957F-s138.png" orientation="portrait" id="d31e1101" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s139">
      <label>SC-012-D1SC02957F-s139</label>
      <media xlink:href="SC-012-D1SC02957F-s139.png" orientation="portrait" id="d31e1104" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s140">
      <label>SC-012-D1SC02957F-s140</label>
      <media xlink:href="SC-012-D1SC02957F-s140.png" orientation="portrait" id="d31e1107" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s141">
      <label>SC-012-D1SC02957F-s141</label>
      <media xlink:href="SC-012-D1SC02957F-s141.png" orientation="portrait" id="d31e1110" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s142">
      <label>SC-012-D1SC02957F-s142</label>
      <media xlink:href="SC-012-D1SC02957F-s142.png" orientation="portrait" id="d31e1113" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s143">
      <label>SC-012-D1SC02957F-s143</label>
      <media xlink:href="SC-012-D1SC02957F-s143.png" orientation="portrait" id="d31e1117" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s144">
      <label>SC-012-D1SC02957F-s144</label>
      <media xlink:href="SC-012-D1SC02957F-s144.png" orientation="portrait" id="d31e1120" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s145">
      <label>SC-012-D1SC02957F-s145</label>
      <media xlink:href="SC-012-D1SC02957F-s145.png" orientation="portrait" id="d31e1123" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s146">
      <label>SC-012-D1SC02957F-s146</label>
      <media xlink:href="SC-012-D1SC02957F-s146.png" orientation="portrait" id="d31e1126" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s147">
      <label>SC-012-D1SC02957F-s147</label>
      <media xlink:href="SC-012-D1SC02957F-s147.png" orientation="portrait" id="d31e1129" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s148">
      <label>SC-012-D1SC02957F-s148</label>
      <media xlink:href="SC-012-D1SC02957F-s148.png" orientation="portrait" id="d31e1132" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s149">
      <label>SC-012-D1SC02957F-s149</label>
      <media xlink:href="SC-012-D1SC02957F-s149.png" orientation="portrait" id="d31e1135" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s150">
      <label>SC-012-D1SC02957F-s150</label>
      <media xlink:href="SC-012-D1SC02957F-s150.png" orientation="portrait" id="d31e1138" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s151">
      <label>SC-012-D1SC02957F-s151</label>
      <media xlink:href="SC-012-D1SC02957F-s151.png" orientation="portrait" id="d31e1141" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s152">
      <label>SC-012-D1SC02957F-s152</label>
      <media xlink:href="SC-012-D1SC02957F-s152.png" orientation="portrait" id="d31e1144" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s153">
      <label>SC-012-D1SC02957F-s153</label>
      <media xlink:href="SC-012-D1SC02957F-s153.png" orientation="portrait" id="d31e1147" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s154">
      <label>SC-012-D1SC02957F-s154</label>
      <media xlink:href="SC-012-D1SC02957F-s154.png" orientation="portrait" id="d31e1151" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s155">
      <label>SC-012-D1SC02957F-s155</label>
      <media xlink:href="SC-012-D1SC02957F-s155.png" orientation="portrait" id="d31e1154" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s156">
      <label>SC-012-D1SC02957F-s156</label>
      <media xlink:href="SC-012-D1SC02957F-s156.png" orientation="portrait" id="d31e1157" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s157">
      <label>SC-012-D1SC02957F-s157</label>
      <media xlink:href="SC-012-D1SC02957F-s157.png" orientation="portrait" id="d31e1160" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s158">
      <label>SC-012-D1SC02957F-s158</label>
      <media xlink:href="SC-012-D1SC02957F-s158.png" orientation="portrait" id="d31e1163" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s159">
      <label>SC-012-D1SC02957F-s159</label>
      <media xlink:href="SC-012-D1SC02957F-s159.png" orientation="portrait" id="d31e1166" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s160">
      <label>SC-012-D1SC02957F-s160</label>
      <media xlink:href="SC-012-D1SC02957F-s160.png" orientation="portrait" id="d31e1169" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s161">
      <label>SC-012-D1SC02957F-s161</label>
      <media xlink:href="SC-012-D1SC02957F-s161.png" orientation="portrait" id="d31e1172" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s162">
      <label>SC-012-D1SC02957F-s162</label>
      <media xlink:href="SC-012-D1SC02957F-s162.png" orientation="portrait" id="d31e1175" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s163">
      <label>SC-012-D1SC02957F-s163</label>
      <media xlink:href="SC-012-D1SC02957F-s163.png" orientation="portrait" id="d31e1178" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s164">
      <label>SC-012-D1SC02957F-s164</label>
      <media xlink:href="SC-012-D1SC02957F-s164.png" orientation="portrait" id="d31e1181" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s165">
      <label>SC-012-D1SC02957F-s165</label>
      <media xlink:href="SC-012-D1SC02957F-s165.png" orientation="portrait" id="d31e1185" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s166">
      <label>SC-012-D1SC02957F-s166</label>
      <media xlink:href="SC-012-D1SC02957F-s166.png" orientation="portrait" id="d31e1188" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s167">
      <label>SC-012-D1SC02957F-s167</label>
      <media xlink:href="SC-012-D1SC02957F-s167.png" orientation="portrait" id="d31e1191" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s168">
      <label>SC-012-D1SC02957F-s168</label>
      <media xlink:href="SC-012-D1SC02957F-s168.png" orientation="portrait" id="d31e1194" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s169">
      <label>SC-012-D1SC02957F-s169</label>
      <media xlink:href="SC-012-D1SC02957F-s169.png" orientation="portrait" id="d31e1197" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s170">
      <label>SC-012-D1SC02957F-s170</label>
      <media xlink:href="SC-012-D1SC02957F-s170.png" orientation="portrait" id="d31e1200" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s171">
      <label>SC-012-D1SC02957F-s171</label>
      <media xlink:href="SC-012-D1SC02957F-s171.png" orientation="portrait" id="d31e1203" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s172">
      <label>SC-012-D1SC02957F-s172</label>
      <media xlink:href="SC-012-D1SC02957F-s172.png" orientation="portrait" id="d31e1206" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s173">
      <label>SC-012-D1SC02957F-s173</label>
      <media xlink:href="SC-012-D1SC02957F-s173.png" orientation="portrait" id="d31e1209" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s174">
      <label>SC-012-D1SC02957F-s174</label>
      <media xlink:href="SC-012-D1SC02957F-s174.png" orientation="portrait" id="d31e1212" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s175">
      <label>SC-012-D1SC02957F-s175</label>
      <media xlink:href="SC-012-D1SC02957F-s175.png" orientation="portrait" id="d31e1215" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s176">
      <label>SC-012-D1SC02957F-s176</label>
      <media xlink:href="SC-012-D1SC02957F-s176.png" orientation="portrait" id="d31e1219" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s177">
      <label>SC-012-D1SC02957F-s177</label>
      <media xlink:href="SC-012-D1SC02957F-s177.png" orientation="portrait" id="d31e1222" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s178">
      <label>SC-012-D1SC02957F-s178</label>
      <media xlink:href="SC-012-D1SC02957F-s178.png" orientation="portrait" id="d31e1225" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s179">
      <label>SC-012-D1SC02957F-s179</label>
      <media xlink:href="SC-012-D1SC02957F-s179.png" orientation="portrait" id="d31e1228" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s180">
      <label>SC-012-D1SC02957F-s180</label>
      <media xlink:href="SC-012-D1SC02957F-s180.png" orientation="portrait" id="d31e1231" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s181">
      <label>SC-012-D1SC02957F-s181</label>
      <media xlink:href="SC-012-D1SC02957F-s181.png" orientation="portrait" id="d31e1234" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s182">
      <label>SC-012-D1SC02957F-s182</label>
      <media xlink:href="SC-012-D1SC02957F-s182.png" orientation="portrait" id="d31e1237" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s183">
      <label>SC-012-D1SC02957F-s183</label>
      <media xlink:href="SC-012-D1SC02957F-s183.png" orientation="portrait" id="d31e1240" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s184">
      <label>SC-012-D1SC02957F-s184</label>
      <media xlink:href="SC-012-D1SC02957F-s184.png" orientation="portrait" id="d31e1243" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s185">
      <label>SC-012-D1SC02957F-s185</label>
      <media xlink:href="SC-012-D1SC02957F-s185.png" orientation="portrait" id="d31e1246" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s186">
      <label>SC-012-D1SC02957F-s186</label>
      <media xlink:href="SC-012-D1SC02957F-s186.png" orientation="portrait" id="d31e1249" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s187">
      <label>SC-012-D1SC02957F-s187</label>
      <media xlink:href="SC-012-D1SC02957F-s187.png" orientation="portrait" id="d31e1253" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s188">
      <label>SC-012-D1SC02957F-s188</label>
      <media xlink:href="SC-012-D1SC02957F-s188.png" orientation="portrait" id="d31e1256" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s189">
      <label>SC-012-D1SC02957F-s189</label>
      <media xlink:href="SC-012-D1SC02957F-s189.png" orientation="portrait" id="d31e1259" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s190">
      <label>SC-012-D1SC02957F-s190</label>
      <media xlink:href="SC-012-D1SC02957F-s190.png" orientation="portrait" id="d31e1262" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s191">
      <label>SC-012-D1SC02957F-s191</label>
      <media xlink:href="SC-012-D1SC02957F-s191.png" orientation="portrait" id="d31e1265" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s192">
      <label>SC-012-D1SC02957F-s192</label>
      <media xlink:href="SC-012-D1SC02957F-s192.png" orientation="portrait" id="d31e1268" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s193">
      <label>SC-012-D1SC02957F-s193</label>
      <media xlink:href="SC-012-D1SC02957F-s193.png" orientation="portrait" id="d31e1271" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s194">
      <label>SC-012-D1SC02957F-s194</label>
      <media xlink:href="SC-012-D1SC02957F-s194.png" orientation="portrait" id="d31e1274" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s195">
      <label>SC-012-D1SC02957F-s195</label>
      <media xlink:href="SC-012-D1SC02957F-s195.png" orientation="portrait" id="d31e1277" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s196">
      <label>SC-012-D1SC02957F-s196</label>
      <media xlink:href="SC-012-D1SC02957F-s196.png" orientation="portrait" id="d31e1280" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s197">
      <label>SC-012-D1SC02957F-s197</label>
      <media xlink:href="SC-012-D1SC02957F-s197.png" orientation="portrait" id="d31e1283" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s198">
      <label>SC-012-D1SC02957F-s198</label>
      <media xlink:href="SC-012-D1SC02957F-s198.png" orientation="portrait" id="d31e1287" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s199">
      <label>SC-012-D1SC02957F-s199</label>
      <media xlink:href="SC-012-D1SC02957F-s199.png" orientation="portrait" id="d31e1290" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s200">
      <label>SC-012-D1SC02957F-s200</label>
      <media xlink:href="SC-012-D1SC02957F-s200.png" orientation="portrait" id="d31e1293" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s201">
      <label>SC-012-D1SC02957F-s201</label>
      <media xlink:href="SC-012-D1SC02957F-s201.png" orientation="portrait" id="d31e1296" position="anchor"/>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SC-012-D1SC02957F-s202">
      <label>SC-012-D1SC02957F-s202</label>
      <media xlink:href="SC-012-D1SC02957F-s202.png" orientation="portrait" id="d31e1299" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>This work was supported by the Office of Naval Research (N00014-18-1-2659).</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="cit1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rosenblatt</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>The perceptron: a probabilistic model for information storage and organization in the brain</article-title>
        <source>Psychol. Rev.</source>
        <year>1958</year>
        <volume>65</volume>
        <fpage>386</fpage>
        <pub-id pub-id-type="pmid">13602029</pub-id>
      </element-citation>
    </ref>
    <ref id="cit2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Courville</surname><given-names>A.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group>, <source>Deep learning</source>, <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge</publisher-loc>, <year>2016</year></mixed-citation>
    </ref>
    <ref id="cit3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Noé</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Tkatchenko</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Müller</surname>
            <given-names>K.-R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Clementi</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Machine learning for molecular simulation</article-title>
        <source>Annu. Rev. Phys. Chem.</source>
        <year>2020</year>
        <volume>71</volume>
        <fpage>361</fpage>
        <lpage>390</lpage>
        <pub-id pub-id-type="pmid">32092281</pub-id>
      </element-citation>
    </ref>
    <ref id="cit4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rupp</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Tkatchenko</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Müller</surname>
            <given-names>K.-R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Von Lilienfeld</surname>
            <given-names>O. A.</given-names>
          </name>
        </person-group>
        <article-title>Fast and accurate modeling of molecular atomization energies with machine learning</article-title>
        <source>Phys. Rev. Lett.</source>
        <year>2012</year>
        <volume>108</volume>
        <fpage>058301</fpage>
        <pub-id pub-id-type="pmid">22400967</pub-id>
      </element-citation>
    </ref>
    <ref id="cit5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Behler</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Perspective: Machine learning potentials for atomistic simulations</article-title>
        <source>J. Chem. Phys.</source>
        <year>2016</year>
        <volume>145</volume>
        <fpage>170901</fpage>
        <pub-id pub-id-type="pmid">27825224</pub-id>
      </element-citation>
    </ref>
    <ref id="cit6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raucci</surname>
            <given-names>U.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Valentini</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Pieri</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Weir</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Seritan</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Martinez</surname>
            <given-names>T. J.</given-names>
          </name>
        </person-group>
        <article-title>Voice-controlled quantum chemistry</article-title>
        <source>Nat. Comput. Sci.</source>
        <year>2021</year>
        <volume>1</volume>
        <fpage>42</fpage>
        <lpage>45</lpage>
      </element-citation>
    </ref>
    <ref id="cit7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bluche</surname><given-names>T.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Louradour</surname><given-names>J.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Messina</surname><given-names>R.</given-names></name></person-group>, <article-title>Scan, attend and read: End-to-end handwritten paragraph recognition with mdlstm attention</article-title>, <source>Proceedings of 14th IAPR International Conference on Document Analysis and Recognition, ICDAR</source>, <year>2017</year>, pp. 1050–1055</mixed-citation>
    </ref>
    <ref id="cit8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Michael</surname><given-names>J.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Labahn</surname><given-names>R.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Grüning</surname><given-names>T.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Zöllner</surname><given-names>J.</given-names></name></person-group>, <article-title>Evaluating sequence-to-sequence models for handwritten text recognition</article-title>, <source>Proceedings of International Conference on Document Analysis and Recognition (ICDAR)</source>, <year>2019</year>, pp. 1286–1293</mixed-citation>
    </ref>
    <ref id="cit9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>A.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group>, <article-title>Offline handwriting recognition with multidimensional recurrent neural networks</article-title>, <source>Proceedings of Advances in Neural Information Processing Systems</source>, <year>2009</year>, pp. 545–552</mixed-citation>
    </ref>
    <ref id="cit10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ingle</surname><given-names>R. R.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Fujii</surname><given-names>Y.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Deselaers</surname><given-names>T.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Baccash</surname><given-names>J.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Popat</surname><given-names>A. C.</given-names></name></person-group>, <article-title>A scalable handwritten text recognition system</article-title>, <source>Proceedings of 2019 International Conference on Document Analysis and Recognition ICDAR</source>, <year>2019</year>, pp. 17–24</mixed-citation>
    </ref>
    <ref id="cit11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Plamondon</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Srihari</surname>
            <given-names>S. N.</given-names>
          </name>
        </person-group>
        <article-title>Online and off-line handwriting recognition: a comprehensive survey</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell.</source>
        <year>2000</year>
        <volume>22</volume>
        <fpage>63</fpage>
        <lpage>84</lpage>
      </element-citation>
    </ref>
    <ref id="cit12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rozas</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Fernandez</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Automatic processing of graphics for image databases in science</article-title>
        <source>J. Chem. Inf. Comput. Sci.</source>
        <year>1990</year>
        <volume>30</volume>
        <fpage>7</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
    <ref id="cit13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Contreras</surname>
            <given-names>M. L.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Allendes</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Alvarez</surname>
            <given-names>L. T.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Rozas</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Computational perception and recognition of digitized molecular structures</article-title>
        <source>J. Chem. Inf. Comput. Sci.</source>
        <year>1990</year>
        <volume>30</volume>
        <fpage>302</fpage>
        <lpage>307</lpage>
      </element-citation>
    </ref>
    <ref id="cit14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McDaniel</surname>
            <given-names>J. R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Balmuth</surname>
            <given-names>J. R.</given-names>
          </name>
        </person-group>
        <article-title>Kekule: OCR-optical chemical (structure) recognition</article-title>
        <source>J. Chem. Inf. Comput. Sci.</source>
        <year>1992</year>
        <volume>32</volume>
        <fpage>373</fpage>
        <lpage>378</lpage>
      </element-citation>
    </ref>
    <ref id="cit15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Casey</surname><given-names>R.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Boyer</surname><given-names>S.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Healey</surname><given-names>P.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Miller</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Oudot</surname><given-names>B.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Zilles</surname><given-names>K.</given-names></name></person-group>, <article-title>Optical recognition of chemical graphics</article-title>, <source>Proceedings of 2nd International Conference on Document Analysis and Recognition (ICDAR'93)</source>, <year>1993</year>, pp. 627–631</mixed-citation>
    </ref>
    <ref id="cit16">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ibison</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Jacquot</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Kam</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Neville</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Simpson</surname>
            <given-names>R. W.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Tonnelier</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Venczel</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>A. P.</given-names>
          </name>
        </person-group>
        <article-title>Chemical literature data extraction: the CLiDE Project</article-title>
        <source>J. Chem. Inf. Comput. Sci.</source>
        <year>1993</year>
        <volume>33</volume>
        <fpage>338</fpage>
        <lpage>344</lpage>
      </element-citation>
    </ref>
    <ref id="cit17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rajan</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Brinkhaus</surname>
            <given-names>H. O.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Zielesny</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Steinbeck</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>A review of optical chemical structure recognition tools</article-title>
        <source>J. Cheminf.</source>
        <year>2020</year>
        <volume>12</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="cit18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gkoutos</surname>
            <given-names>G. V.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Rzepa</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Clark</surname>
            <given-names>R. M.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Adjei</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Johal</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Chemical machine vision: automated extraction of chemical metadata from raster images</article-title>
        <source>J. Chem. Inf. Comput. Sci.</source>
        <year>2003</year>
        <volume>43</volume>
        <fpage>1342</fpage>
        <lpage>1355</lpage>
        <pub-id pub-id-type="pmid">14502466</pub-id>
      </element-citation>
    </ref>
    <ref id="cit19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rosania</surname>
            <given-names>G. R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Crippen</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Woolf</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Shedden</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>A cheminformatic toolkit for mining biomedical knowledge</article-title>
        <source>Pharm. Res.</source>
        <year>2007</year>
        <volume>24</volume>
        <fpage>1791</fpage>
        <lpage>1802</lpage>
        <pub-id pub-id-type="pmid">17385012</pub-id>
      </element-citation>
    </ref>
    <ref id="cit20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Algorri</surname><given-names>M.-E.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Zimmermann</surname><given-names>M.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>C. M.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Akle</surname><given-names>S.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Hofmann-Apitius</surname><given-names>M.</given-names></name></person-group>, <article-title>Reconstruction of chemical molecules from images</article-title>, <source>Proceedings of 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source>, <year>2007</year>, pp. 4609–4612</mixed-citation>
    </ref>
    <ref id="cit21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Valko</surname>
            <given-names>A. T.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Johnson</surname>
            <given-names>A. P.</given-names>
          </name>
        </person-group>
        <article-title>CLiDE Pro: the latest generation of CLiDE, a tool for optical chemical structure recognition</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2009</year>
        <volume>49</volume>
        <fpage>780</fpage>
        <lpage>787</lpage>
        <pub-id pub-id-type="pmid">19298076</pub-id>
      </element-citation>
    </ref>
    <ref id="cit22">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Filippov</surname>
            <given-names>I. V.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Nicklaus</surname>
            <given-names>M. C.</given-names>
          </name>
        </person-group>
        <article-title>Optical structure recognition software to recover chemical information: OSRA, an open source solution</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2009</year>
        <volume>49</volume>
        <fpage>740</fpage>
        <lpage>743</lpage>
        <pub-id pub-id-type="pmid">19434905</pub-id>
      </element-citation>
    </ref>
    <ref id="cit23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Park</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Rosania</surname>
            <given-names>G. R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Shedden</surname>
            <given-names>K. A.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Lyu</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Saitou</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Automated extraction of chemical structure information from digital raster images</article-title>
        <source>Chem. Cent. J.</source>
        <year>2009</year>
        <volume>3</volume>
        <fpage>4</fpage>
        <pub-id pub-id-type="pmid">19196483</pub-id>
      </element-citation>
    </ref>
    <ref id="cit24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Park</surname><given-names>J.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Saitou</surname><given-names>K.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Rosania</surname><given-names>G.</given-names></name></person-group>, <article-title>Image-based automated chemical database annotation with ensemble of machine-vision classifiers</article-title>, <source>Proceedings of 2010 IEEE International Conference on Automation Science and Engineering</source>, <year>2010</year>, pp. 168–173</mixed-citation>
    </ref>
    <ref id="cit25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sadawi</surname><given-names>N. M.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Sexton</surname><given-names>A. P.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Sorge</surname><given-names>V.</given-names></name></person-group>, <article-title>Chemical structure recognition: a rule-based approach</article-title>, <source>Proceedings of Document Recognition and Retrieval XIX</source>, <year>2012</year>, p. 82970E</mixed-citation>
    </ref>
    <ref id="cit26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tharatipyakul</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Numnark</surname><given-names>S.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Wichadakul</surname><given-names>D.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Ingsriswang</surname><given-names>S.</given-names></name></person-group>, <article-title>ChemEx: information extraction system for chemical data curation</article-title>, <source>Proceedings of BMC Bioinformatics</source>, <year>2012</year>, <volume>vol. S9</volume></mixed-citation>
    </ref>
    <ref id="cit27">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frasconi</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Gabbrielli</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Lippi</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Marinai</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Markov logic networks for optical chemical structure recognition</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2014</year>
        <volume>54</volume>
        <fpage>2380</fpage>
        <lpage>2390</lpage>
        <pub-id pub-id-type="pmid">25068386</pub-id>
      </element-citation>
    </ref>
    <ref id="cit28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Beard</surname>
            <given-names>E. J.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Cole</surname>
            <given-names>J. M.</given-names>
          </name>
        </person-group>
        <article-title>ChemSchematicResolver: A Toolkit to Decode 2D Chemical Diagrams with Labels and R-Groups into Annotated Chemical Named Entities</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2020</year>
        <volume>60</volume>
        <fpage>2059</fpage>
        <lpage>2072</lpage>
        <pub-id pub-id-type="pmid">32212690</pub-id>
      </element-citation>
    </ref>
    <ref id="cit29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Staker</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Marshall</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Abel</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>McQuaw</surname>
            <given-names>C. M.</given-names>
          </name>
        </person-group>
        <article-title>Molecular Structure Extraction from Documents Using Deep Learning</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2019</year>
        <volume>59</volume>
        <fpage>1017</fpage>
        <lpage>1029</lpage>
        <pub-id pub-id-type="pmid">30758950</pub-id>
      </element-citation>
    </ref>
    <ref id="cit30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Oldenhof</surname><given-names>M.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Arany</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Moreau</surname><given-names>Y.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Simm</surname><given-names>J.</given-names></name></person-group>, <article-title>ChemGrapher: Optical Graph Recognition of Chemical Compounds by Deep Learning</article-title>, arXiv preprint arXiv:2002.09914, <year>2020</year></mixed-citation>
    </ref>
    <ref id="cit31">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rajan</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Zielesny</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Steinbeck</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>DECIMER: towards deep learning for chemical image recognition</article-title>
        <source>J. Cheminf.</source>
        <year>2020</year>
        <volume>12</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="cit32">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>T. Y.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Davis</surname><given-names>R.</given-names></name></person-group>, <article-title>Recognition of hand drawn chemical diagrams</article-title>, <source>Proceedings of AAAI</source>, <year>2007</year>, pp. 846–851</mixed-citation>
    </ref>
    <ref id="cit33">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ramel</surname><given-names>J.-Y.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Boissier</surname><given-names>G.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Emptoz</surname><given-names>H.</given-names></name></person-group>, <article-title>Automatic reading of handwritten chemical formulas from a structural representation of the image</article-title>, <source>Proceedings of Fifth International Conference on Document Analysis and Recognition. ICDAR'99</source>, <year>1999</year>, pp. 83–86</mixed-citation>
    </ref>
    <ref id="cit34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>VISIONARCANUM</surname></name></person-group>, <source>InkToMolecule online</source>, <uri xlink:href="https://visionarcanum.com/ink2mol/">https://visionarcanum.com/ink2mol/</uri>, accessed May 1, <year>2021</year></mixed-citation>
    </ref>
    <ref id="cit35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weininger</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</article-title>
        <source>J. Chem. Inf. Comput. Sci.</source>
        <year>1988</year>
        <volume>28</volume>
        <fpage>31</fpage>
        <lpage>36</lpage>
      </element-citation>
    </ref>
    <ref id="cit36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hirschberg</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Manning</surname>
            <given-names>C. D.</given-names>
          </name>
        </person-group>
        <article-title>Advances in natural language processing</article-title>
        <source>Science</source>
        <year>2015</year>
        <volume>349</volume>
        <fpage>261</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="pmid">26185244</pub-id>
      </element-citation>
    </ref>
    <ref id="cit37">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vinyals</surname><given-names>O.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Toshev</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Bengio</surname><given-names>S.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Erhan</surname><given-names>D.</given-names></name></person-group>, <article-title>Show and tell: A neural image caption generator</article-title>, <source>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</source>, <year>2015</year>, pp. 3156–3164</mixed-citation>
    </ref>
    <ref id="cit38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>K.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Ba</surname><given-names>J.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Kiros</surname><given-names>R.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Cho</surname><given-names>K.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Courville</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Salakhudinov</surname><given-names>R.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Zemel</surname><given-names>R.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group>, <article-title>Show, attend and tell: Neural image caption generation with visual attention</article-title>, <source>Proceedings of International conference on machine learning</source>, <year>2015</year>, pp. 2048–2057</mixed-citation>
    </ref>
    <ref id="cit39">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="cit40">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krizhevsky</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Sutskever</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>G. E.</given-names>
          </name>
        </person-group>
        <article-title>Imagenet classification with deep convolutional neural networks</article-title>
        <source>Commun. ACM</source>
        <year>2017</year>
        <volume>60</volume>
        <fpage>84</fpage>
        <lpage>90</lpage>
      </element-citation>
    </ref>
    <ref id="cit41">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput.</source>
        <year>1997</year>
        <volume>9</volume>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="cit42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>Y.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Kanervisto</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Ling</surname><given-names>J.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Rush</surname><given-names>A. M.</given-names></name></person-group>, <article-title>Image-to-markup generation with coarse-to-fine attention</article-title>, <source>Proceedings of International Conference on Machine Learning</source>, <year>2017</year>, pp. 980–989</mixed-citation>
    </ref>
    <ref id="cit43">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fink</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Bruggesser</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Reymond</surname>
            <given-names>J. L.</given-names>
          </name>
        </person-group>
        <article-title>Virtual exploration of the small-molecule chemical universe below 160 daltons</article-title>
        <source>Angew. Chem. Int. Ed.</source>
        <year>2005</year>
        <volume>44</volume>
        <fpage>1504</fpage>
        <lpage>1508</lpage>
      </element-citation>
    </ref>
    <ref id="cit44">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fink</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Reymond</surname>
            <given-names>J.-L.</given-names>
          </name>
        </person-group>
        <article-title>Virtual exploration of the chemical universe up to 11 atoms of C, N, O, F: assembly of 26.4 million structures (110.9 million stereoisomers) and analysis for new ring systems, stereochemistry, physicochemical properties, compound classes, and drug discovery</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2007</year>
        <volume>47</volume>
        <fpage>342</fpage>
        <lpage>353</lpage>
        <pub-id pub-id-type="pmid">17260980</pub-id>
      </element-citation>
    </ref>
    <ref id="cit45">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blum</surname>
            <given-names>L. C.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Reymond</surname>
            <given-names>J.-L.</given-names>
          </name>
        </person-group>
        <article-title>970 million druglike small molecules for virtual screening in the chemical universe database GDB-13</article-title>
        <source>J. Am. Chem. Soc.</source>
        <year>2009</year>
        <volume>131</volume>
        <fpage>8732</fpage>
        <lpage>8733</lpage>
        <pub-id pub-id-type="pmid">19505099</pub-id>
      </element-citation>
    </ref>
    <ref id="cit46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G.</given-names></name></person-group>, <article-title>The OpenCV Library</article-title>, <source>Dr Dobb's Journal of Software Tools</source>, <year>2000</year>, <volume>120</volume>, pp. 122–125</mixed-citation>
    </ref>
    <ref id="cit47">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Weir</surname><given-names>H.</given-names></name></person-group>, ChemPixCH, <year>2021</year>, <uri xlink:href="https://github.com/mtzgroup/ChemPixCH">https://github.com/mtzgroup/ChemPixCH</uri></mixed-citation>
    </ref>
    <ref id="cit48">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Gao</surname><given-names>J.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Lin</surname><given-names>W.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Yuan</surname><given-names>Y.</given-names></name></person-group>, <article-title>Learning from synthetic data for crowd counting in the wild</article-title>, <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>, <year>2019</year>, pp. 8198–8207</mixed-citation>
    </ref>
    <ref id="cit49">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kuznichov</surname><given-names>D.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Zvirin</surname><given-names>A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Honen</surname><given-names>Y.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Kimmel</surname><given-names>R.</given-names></name></person-group>, <article-title>Data augmentation for leaf segmentation and counting tasks in rosette plants</article-title>, <source>Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</source>, <year>2019</year></mixed-citation>
    </ref>
    <ref id="cit50">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tajbakhsh</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Shin</surname>
            <given-names>J. Y.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Gurudu</surname>
            <given-names>S. R.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Hurst</surname>
            <given-names>R. T.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Kendall</surname>
            <given-names>C. B.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Gotway</surname>
            <given-names>M. B.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Liang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Convolutional neural networks for medical image analysis: Full training or fine tuning?</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1299</fpage>
        <lpage>1312</lpage>
        <pub-id pub-id-type="pmid">26978662</pub-id>
      </element-citation>
    </ref>
    <ref id="cit51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>C. M.</given-names></name></person-group>, <source>Neural networks for pattern recognition</source>, <publisher-name>Oxford University Press</publisher-name>, <year>1995</year></mixed-citation>
    </ref>
    <ref id="cit52">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Polikar</surname><given-names>R.</given-names></name></person-group>, <source>Ensemble learning in Ensemble machine learning</source>, <publisher-name>Springer</publisher-name>, <year>2012</year>, pp. 1–34</mixed-citation>
    </ref>
    <ref id="cit53">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gatys</surname><given-names>L. A.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Ecker</surname><given-names>A. S.</given-names></name></person-group> and <person-group person-group-type="author"><name><surname>Bethge</surname><given-names>M.</given-names></name></person-group>, <article-title>Image style transfer using convolutional neural networks</article-title>, <source>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</source>, <year>2016</year>, pp. 2414–2423</mixed-citation>
    </ref>
    <ref id="cit54">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seritan</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Thompson</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <person-group person-group-type="author">
          <name>
            <surname>Martínez</surname>
            <given-names>T. J.</given-names>
          </name>
        </person-group>
        <article-title>TeraChem Cloud: A high-performance computing service for scalable distributed GPU-accelerated electronic structure calculations</article-title>
        <source>J. Chem. Inf. Model.</source>
        <year>2020</year>
        <volume>60</volume>
        <fpage>2126</fpage>
        <lpage>2137</lpage>
        <pub-id pub-id-type="pmid">32267693</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
