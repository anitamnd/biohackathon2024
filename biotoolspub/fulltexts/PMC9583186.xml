<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_PATTER100589 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEfx1 jpg ?>
<?FILEsi1 gif ?>
<?FILEsi2 gif ?>
<?FILEsi3 gif ?>
<?FILEsi4 gif ?>
<?FILEsi5 gif ?>
<?FILEsi6 gif ?>
<?FILEsi7 gif ?>
<?FILEsi8 gif ?>
<?FILEsi9 gif ?>
<?FILEsi10 gif ?>
<?FILEsi11 gif ?>
<?FILEsi12 gif ?>
<?FILEsi13 gif ?>
<?FILEsi14 gif ?>
<?FILEsi15 gif ?>
<?FILEsi16 gif ?>
<?FILEsi17 gif ?>
<?FILEsi18 gif ?>
<?FILEsi19 gif ?>
<?FILEsi20 gif ?>
<?FILEsi21 gif ?>
<?FILEsi22 gif ?>
<?FILEsi23 gif ?>
<?FILEsi24 gif ?>
<?FILEsi25 gif ?>
<?FILEsi26 gif ?>
<?FILEsi27 gif ?>
<?FILEsi28 gif ?>
<?FILEsi29 gif ?>
<?FILEsi30 gif ?>
<?FILEsi31 gif ?>
<?FILEsi32 gif ?>
<?FILEsi33 gif ?>
<?FILEsi34 gif ?>
<?FILEsi35 gif ?>
<?FILEsi36 gif ?>
<?FILEsi37 gif ?>
<?FILEsi38 gif ?>
<?FILEsi39 gif ?>
<?FILEsi40 gif ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Patterns (N Y)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Patterns (N Y)</journal-id>
    <journal-title-group>
      <journal-title>Patterns</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2666-3899</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9583186</article-id>
    <article-id pub-id-type="pii">S2666-3899(22)00207-0</article-id>
    <article-id pub-id-type="doi">10.1016/j.patter.2022.100589</article-id>
    <article-id pub-id-type="publisher-id">100589</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Descriptor</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DADApy: Distance-based analysis of data-manifolds in Python</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Glielmo</surname>
          <given-names>Aldo</given-names>
        </name>
        <email>aldo.glielmo@bancaditalia.it</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff2" ref-type="aff">2</xref>
        <xref rid="fn1" ref-type="fn">6</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Macocco</surname>
          <given-names>Iuri</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Doimo</surname>
          <given-names>Diego</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Carli</surname>
          <given-names>Matteo</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au5">
        <name>
          <surname>Zeni</surname>
          <given-names>Claudio</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au6">
        <name>
          <surname>Wild</surname>
          <given-names>Romina</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au7">
        <name>
          <surname>d’Errico</surname>
          <given-names>Maria</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">3</xref>
        <xref rid="aff4" ref-type="aff">4</xref>
      </contrib>
      <contrib contrib-type="author" id="au8">
        <name>
          <surname>Rodriguez</surname>
          <given-names>Alex</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">5</xref>
      </contrib>
      <contrib contrib-type="author" id="au9">
        <name>
          <surname>Laio</surname>
          <given-names>Alessandro</given-names>
        </name>
        <email>laio@sissa.it</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="aff5" ref-type="aff">5</xref>
        <xref rid="cor2" ref-type="corresp">∗∗</xref>
      </contrib>
      <aff id="aff1"><label>1</label>International School for Advanced Studies (SISSA), Via Bonomea 265, Trieste, Italy</aff>
      <aff id="aff2"><label>2</label>Banca d’Italia, Italy</aff>
      <aff id="aff3"><label>3</label>Functional Genomics Center, ETH Zurich/UZH, Winterthurerstrasse 190, Zurich, Switzerland</aff>
      <aff id="aff4"><label>4</label>Swiss Institute of Bioinformatics, Quartier Sorge – Batiment, Amphipole 1015, Lausanne, Switzerland</aff>
      <aff id="aff5"><label>5</label>The Abdus Salam International Centre for Theoretical Physics (ICTP), Strada Costiera 11, Trieste, Italy</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author <email>aldo.glielmo@bancaditalia.it</email></corresp>
      <corresp id="cor2"><label>∗∗</label>Corresponding author <email>laio@sissa.it</email></corresp>
      <fn id="fn1">
        <label>6</label>
        <p id="ntpara0010">Lead contact</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>19</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>14</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>19</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <volume>3</volume>
    <issue>10</issue>
    <elocation-id>100589</elocation-id>
    <history>
      <date date-type="received">
        <day>20</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>24</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>8</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Author(s)</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <title>Summary</title>
      <p>DADApy is a Python software package for analyzing and characterizing high-dimensional data manifolds. It provides methods for estimating the intrinsic dimension and the probability density, for performing density-based clustering, and for comparing different distance metrics. We review the main functionalities of the package and exemplify its usage in a synthetic dataset and in a real-world application. DADApy is freely available under the open-source Apache 2.0 license.</p>
    </abstract>
    <abstract abstract-type="graphical" id="abs0015">
      <title>Graphical abstract</title>
      <fig id="undfig1" position="anchor">
        <graphic xlink:href="fx1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0020">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">DADApy is a Python software library to characterize data manifolds</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">DADApy can compute intrinsic dimension, density, cluster structures, and optimal metrics</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">DADApy is not based on projections and can work also on topologically complex manifolds</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">DADApy has an easy-to-use Python interface and efficient C-compiled routines</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="editor-highlights" id="abs0025">
      <title>The bigger picture</title>
      <p>Data are often represented via many thousands of features. Fortunately, in most applications, such high-dimensional spaces are very sparsely populated, and data points effectively live on low-dimensional “data manifolds.” This is the key reason behind the success of dimensionality reduction schemes, which, however, cannot be easily deployed on data manifolds with nontrivial geometries and topologies, where a set of coordinates capable of describing the manifold globally cannot exist. In these scenarios, one can analyze the data manifold directly, without an explicit dimensional reduction step, and compute fundamental properties, such as the intrinsic dimension of the manifold and the density of the points lying on it. DADApy implements a set of methods recently developed to this aim. DADApy is easy-to-use as it is written entirely in Python, but also computationally efficient as time-consuming routines are C-compiled through Cython.</p>
    </abstract>
    <abstract abstract-type="teaser" id="abs0030">
      <p>Real-world data are typically represented by high-dimensional features, but live on low-dimensional data manifolds with a great deal of hidden structure. One can analyze such a structure, for instance, by estimating the intrinsic dimension of the manifold, as well as the density of the points lying on it. DADApy collects several algorithms for data manifolds characterization that have already proven effective in specific applications, aims to popularize them, and to make them available for data-science practitioners.</p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>manifold analysis</kwd>
      <kwd>intrinsic dimension</kwd>
      <kwd>density estimation</kwd>
      <kwd>density-based clustering</kwd>
      <kwd>metric learning</kwd>
      <kwd>feature selection</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc0010">Published: September 19, 2022</p>
  </notes>
</front>
<body>
  <sec id="sec1">
    <title>Introduction</title>
    <p id="p0035">The need to analyze large volumes of data is rapidly becoming ubiquitous in all branches of computational science, from quantum chemistry, biophysics, and materials science<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref><sup>,</sup><xref rid="bib2" ref-type="bibr"><sup>2</sup></xref> to astrophysics and particle physics.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref></p>
    <p id="p0040">In many practical applications, data come in the form of a large matrix of features, and one can think of a dataset as a cloud of points living in the very high-dimensional space defined by these features. The number of features for each data point can easily exceed the thousands, and if such a cloud of points were to occupy the entire space uniformly, there would be no hope of extracting any kind of usable information from data.<xref rid="bib4" ref-type="bibr"><sup>4</sup></xref><sup>,</sup><xref rid="bib5" ref-type="bibr"><sup>5</sup></xref> Luckily this never happens in practice, and real-world datasets possess a great deal of hidden intrinsic structure. The most important one is that the feature space, even if very high dimensional, is very sparsely populated. In fact, the points typically lie on a data manifold of much lower dimension than the number of features of the dataset (<xref rid="fig1" ref-type="fig">Figure 1</xref>A). A second important hidden structure, which is almost ubiquitous in real-world data, is that the density of points on such a manifold is far from uniform (<xref rid="fig1" ref-type="fig">Figure 1</xref>B). The data points are instead often grouped in density peaks (DPs) (<xref rid="fig1" ref-type="fig">Figures 1</xref>B and 1C), at times well separated from each other, at times organized hierarchically in “mountain chains.”<fig id="fig1"><label>Figure 1</label><caption><p>An illustration of the four main classes of tasks that DADApy can perform</p><p>From (A) to (D): Intrinsic dimension estimation, density estimation, density peaks estimation (i.e., density-based clustering), and comparison of distance measures.</p></caption><graphic xlink:href="gr1"/></fig></p>
    <p id="p0045">DADApy implements in a single and user-friendly software a set of state-of-the-art algorithms to characterize and analyze the intrinsic manifold of a dataset. In particular, DADApy implements algorithms aimed at estimating the intrinsic dimension (ID) of the manifold (<xref rid="fig1" ref-type="fig">Figure 1</xref>A) and the probability density of the data (<xref rid="fig1" ref-type="fig">Figure 1</xref>B), at inferring the topography and the relative position of the DPs by density-based clustering (<xref rid="fig1" ref-type="fig">Figure 1</xref>C) and, finally, at comparing different metrics, finding in this way the features that are better suited to describe the manifold (<xref rid="fig1" ref-type="fig">Figure 1</xref>D).</p>
    <p id="p0050">All these approaches belong to the class of unsupervised learning methods and are designed to work also in situations in which only the distances between data points are available instead of their features. Therefore, the same tools can be used for analyzing a molecular dynamics trajectory (where features are available) but also a metagenomics or a linguistic database, where one can only define a similarity or a distance between the data.</p>
    <p id="p0055">Another important feature of the methods included in the package is that they are specifically designed to work even when the ID of the data manifold is relatively high, of order ten or more, and if the manifold is topologically complex, and, in particular, not isomorphic to a hyperplane. Therefore, the package can be considered complementary to other packages, such as Scikit-learn,<xref rid="bib6" ref-type="bibr"><sup>6</sup></xref> which implement classical approaches for unsupervised manifold learning, which should be preferred in simpler cases, such as PCA,<xref rid="bib7" ref-type="bibr"><sup>7</sup></xref> kernel-PCA,<xref rid="bib8" ref-type="bibr"><sup>8</sup></xref> or Isomap.<xref rid="bib9" ref-type="bibr"><sup>9</sup></xref></p>
    <p id="p0060">In the following, we first briefly describe the four classes of algorithms implemented in DADApy. We then illustrate the structure of the package and demonstrate its usage for the analysis of both a synthetic and a realistic dataset. We will also discuss the computational efficiency of the implementations, demonstrating that the package can be used to analyze datasets of <inline-formula><mml:math id="M1" altimg="si1.gif"><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> points or more, even with moderate computational resources.</p>
  </sec>
  <sec id="sec2">
    <title>Results and discussion</title>
    <sec id="sec2.1">
      <title>Description of the methods</title>
      <sec id="sec2.1.1">
        <title>ID estimators</title>
        <p id="p0065">The ID of a dataset can be defined as the minimum number of coordinates that are needed to describe the data manifold without significant information loss.<xref rid="bib10" ref-type="bibr"><sup>10</sup></xref><sup>,</sup><xref rid="bib11" ref-type="bibr"><sup>11</sup></xref> In our package we provide the implementation of a class of approaches that are suitable to estimate the ID using only the distances between the points, and not the features. Most of these approaches are rooted in the observation that, in a uniform distribution of points, the ratio <inline-formula><mml:math id="M2" altimg="si2.gif"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the distances of two consecutive nearest neighbors of a point <italic>i</italic> are distributed with a Pareto distribution, which depends only on the ID. This allows defining a simple likelihood for the <italic>N</italic> observations of <inline-formula><mml:math id="M3" altimg="si2.gif"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, one for each point of the dataset:<disp-formula id="fd1"><label>(Equation 1)</label><mml:math id="M4" altimg="si3.gif"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mtext>ID</mml:mtext></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mtext>ID</mml:mtext><mml:mspace width="0.25em"/><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>ID</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0070">The ID is then estimated either by maximizing the likelihood,<xref rid="bib12" ref-type="bibr"><sup>12</sup></xref> by Bayesian inference,<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> or by linear regression after a suitable variable transformation.<xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> We refer to these estimators as two nearest neighbors (2NN) estimators.</p>
        <p id="p0075">It is possible that the data manifold possesses different IDs depending on the scale of variations considered. For example, a spiral dataset can be one-dimensional on a short scale, but two-dimensional on a larger scale. Hence, one might be interested in computing an ID estimate as a function of the scale. The package provides two routines to perform this task. The first method allows to probe the ID at increasing length scales by sub-sampling the original dataset. By virtue of the reduced number of points considered, the average distance between them will be larger; this can be then interpreted as the length scale at which the ID is computed. Obviously, subsampling the dataset also increases the variance of the ID estimate. The second method, an algorithm called "generalized ratios ID estimator (Gride),” circumvents this issue by generalizing the likelihood in <xref rid="fd1" ref-type="disp-formula">Equation 1</xref> to directly probe longer length scales without subsampling.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref></p>
        <p id="p0080">After using one of these algorithms, one can select the ID of the dataset as the estimate that is most consistently found across different scales. However, this choice is often not straightforward, and for a more in depth discussion on this topic we refer to Denti et al.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref><sup>,</sup><xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> and Facco et al.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref><sup>,</sup><xref rid="bib14" ref-type="bibr"><sup>14</sup></xref></p>
        <p id="p0085">ID estimation has been successfully deployed in a number of applications, ranging from the analysis of deep neural networks,<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref> to physical applications, such as phase transition detection<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref> and molecular force-field validation.<xref rid="bib17" ref-type="bibr"><sup>17</sup></xref></p>
      </sec>
      <sec id="sec2.1.2">
        <title>Density estimators</title>
        <p id="p0090">The goal of density estimation is to reconstruct the probability density <inline-formula><mml:math id="M5" altimg="si4.gif"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from which the dataset has been harvested. The package implements a non-parametric density estimator called point-adaptive <italic>kNN</italic> (PA<italic>k</italic>),<xref rid="bib18" ref-type="bibr"><sup>18</sup></xref> which uses as input only the distances between points and, importantly, is designed to work under the explicit assumption that the data are contained in an embedding manifold of relatively small dimension. This algorithm is an extension of the standard <italic>k</italic>NN estimator,<xref rid="bib19" ref-type="bibr"><sup>19</sup></xref> which estimates the density on a point as proportional to the empirical density sampled in its immediate surrounding. More precisely, the <italic>k</italic>NN estimates can be written as<disp-formula id="fd2"><label>(Equation 2)</label><mml:math id="M6" altimg="si5.gif"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mfrac><mml:mi>k</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>k</italic> is the number of nearest neighbors considered, and <inline-formula><mml:math id="M7" altimg="si6.gif"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the volume they occupy. The volume is typically computed as <inline-formula><mml:math id="M8" altimg="si7.gif"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mtext>ID</mml:mtext></mml:msub><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mtext>ID</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M9" altimg="si8.gif"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mtext>ID</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> is the volume of unit sphere in <inline-formula><mml:math id="M10" altimg="si9.gif"><mml:mrow><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mtext>ID</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M11" altimg="si10.gif"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the distance between point <italic>i</italic> and its <italic>k</italic>th nearest neighbor.</p>
        <p id="p0095">In PA<italic>k</italic> the number of neighbors <italic>k</italic> used for estimating the density around point <italic>i</italic> is chosen adaptively for each data point by an unsupervised statistical approach in such a way that the density, up to that neighbor, can be considered approximately constant. This trick dramatically improves the performance of the estimator in complex scenarios, where the density varies significantly at short distances.<xref rid="bib18" ref-type="bibr"><sup>18</sup></xref> Importantly, the volumes that enter the definition of the estimator are measured in the low-dimensional intrinsic manifold rather than in the full embedding space. This prevents the positional information of the data from being diluted on irrelevant directions orthogonal to the data manifold. Assuming that the data manifold is Riemannian, namely locally flat, it can be locally approximated by its tangent hyperplane and distances between neighbors, the only distances used in the estimator, can be measured in this low-dimensional Euclidean space. This allows to operate on the intrinsic manifold without any explicit parametrization. The only prerequisite is an estimate of the local ID, since this is needed to measure the volumes directly on the manifold.</p>
        <p id="p0100">Another key difference between <italic>k</italic>NN and PA<italic>k</italic> estimators is that <italic>k</italic>NN assumes the density to be exactly constant in the neighborhood of each point, while PA<italic>k</italic> possesses an additional free parameter that allows to describe small density variations. The PA<italic>k</italic> density estimator can be used to reconstruct free energy surfaces, especially in high-dimensional spaces,<xref rid="bib18" ref-type="bibr"><sup>18</sup></xref><sup>,</sup><xref rid="bib20" ref-type="bibr">20</xref>, <xref rid="bib21" ref-type="bibr">21</xref>, <xref rid="bib22" ref-type="bibr">22</xref> and it can also be used for a detailed analysis of the data, as in Offei-Danso et al.,<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> where a distinct analysis of the data points with different densities lead to some physical insight about the system under study.</p>
        <p id="p0105">The same estimator can be used also for estimating the density on points that do not belong to the dataset,<xref rid="bib24" ref-type="bibr"><sup>24</sup></xref> a procedure that has been recently used to quantify the degree to which test data are well represented by a training dataset.<xref rid="bib25" ref-type="bibr"><sup>25</sup></xref></p>
        <p id="p0110">Finally, PA<italic>k</italic> is commonly used within the density-based clustering algorithms discussed in the following section.</p>
      </sec>
      <sec id="sec2.1.3">
        <title>DP clustering</title>
        <p id="p0115">The different “peaks” of the probability density can be considered a natural partition of the dataset into separate groups or “clusters.” This is the key idea underlying density peak (DP) clustering,<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> implemented in DADApy. This algorithms works by first estimating the density <inline-formula><mml:math id="M12" altimg="si11.gif"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of all points <italic>i</italic>, for example using the PA<italic>k</italic> method described in the previous section. Then, the minimum distance <inline-formula><mml:math id="M13" altimg="si12.gif"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> between point <italic>i</italic> and any other point with higher density is computed as<disp-formula id="fd3"><label>(Equation 3)</label><mml:math id="M14" altimg="si13.gif"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="0.25em"/><mml:mo>∣</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>ρ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0120">The peaks of the density (and hence the cluster centers) are expected to have both a high density <inline-formula><mml:math id="M15" altimg="si11.gif"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and a large distance <inline-formula><mml:math id="M16" altimg="si12.gif"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from points with higher density, and are hence selected as the few points for which both <inline-formula><mml:math id="M17" altimg="si11.gif"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M18" altimg="si12.gif"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are very large. The selection is typically done by plotting <inline-formula><mml:math id="M19" altimg="si11.gif"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> against <inline-formula><mml:math id="M20" altimg="si12.gif"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and visually identifying the outliers of the distribution. Once the cluster centers are found, each remaining point is assigned to the same cluster as its nearest neighbor of higher density.</p>
        <p id="p0125">In DP clustering the DPs must be specified by the user, and this arbitrariness represents an obvious source of errors. The advanced DP (ADP) clustering approach,<xref rid="bib27" ref-type="bibr"><sup>27</sup></xref> also available in DADApy, proposes a solution to this problem. In ADP clustering, all local maxima of the density are initially considered DPs, and a statistical significance analysis of each peak is subsequently performed. A peak <italic>c</italic> is considered statistically significant only if the difference between the log density of the peak <inline-formula><mml:math id="M21" altimg="si14.gif"><mml:mrow><mml:mi>ln</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>ρ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and the log density of any neighboring saddle point <inline-formula><mml:math id="M22" altimg="si15.gif"><mml:mrow><mml:mi>ln</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:msup><mml:mi>c</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is sufficiently larger than the sum of the errors on the two estimated quantities<disp-formula id="fd4"><label>(Equation 4)</label><mml:math id="M23" altimg="si16.gif"><mml:mrow><mml:mi>ln</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>ρ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo linebreak="goodbreak">−</mml:mo><mml:mi>ln</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:msup><mml:mi>c</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">&gt;</mml:mo><mml:mi>Z</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:msup><mml:mi>c</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0130">If this is not the case, the two peaks <italic>c</italic> and <inline-formula><mml:math id="M24" altimg="si17.gif"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mtext>′</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> are merged into a single peak. This process is iterated until no peak that is not statistically significant is remaining. The parameter <italic>Z</italic> appearing in <xref rid="fd4" ref-type="disp-formula">Equation 4</xref> can be interpreted as the statistical significance threshold of the found peaks. A higher value of <italic>Z</italic> will give rise to a smaller number of peaks with a higher statistical significance. Typical values range from 1 to 5. ADP and DP are general clustering tools, and as such have been used in different fields, including single-cell transcriptomics,<xref rid="bib28" ref-type="bibr"><sup>28</sup></xref><sup>,</sup><xref rid="bib29" ref-type="bibr"><sup>29</sup></xref> spike-sorting,<xref rid="bib30" ref-type="bibr"><sup>30</sup></xref><sup>,</sup><xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> word embedding,<xref rid="bib32" ref-type="bibr"><sup>32</sup></xref> climate modelling,<xref rid="bib33" ref-type="bibr"><sup>33</sup></xref> Markov state modelling,<xref rid="bib34" ref-type="bibr"><sup>34</sup></xref> and the analysis of molecular dynamics simulations,<xref rid="bib35" ref-type="bibr"><sup>35</sup></xref><sup>,</sup><xref rid="bib36" ref-type="bibr"><sup>36</sup></xref> just to mention some of them.</p>
        <p id="p0135">Another clustering algorithm available in DADApy is <italic>k</italic>-peaks clustering.<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref> In short, this method is a variant of ADP that takes advantage of the observation that the optimal <inline-formula><mml:math id="M25" altimg="si18.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is high in two cases: (1) in high-density regions, due to the high concentration of points, and (2) in vast regions where the density is everywhere constant. Therefore, the peaks in <inline-formula><mml:math id="M26" altimg="si18.gif"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> correspond either to peaks in density or to the center of large regions with nearly constant density (e.g., metastable states stabilized by entropy). An example application of <italic>k-</italic>peaks clustering can be found in Sormani et al.,<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref> where it was used to describe the free-energy landscape of the folding/unfolding process of a protein.</p>
      </sec>
      <sec id="sec2.1.4">
        <title>Metric comparisons</title>
        <p id="p0140">In several applications, the similarity (or the distance) between different data points can be measured using very different metrics. For instance, a group of atoms or molecules in a physical system can be represented by their Cartesian coordinates, by the set of their inter-particle distances, or by a set of dihedral angles, and one can measure the distance between two configurations with any arbitrary subset of these coordinates. Similarly, the “distance” between two patients can be measured taking into account their clinical history, any subset of blood exams, radiomics features, genome expression measures, or a combination of those.</p>
        <p id="p0145">It might hence be useful to evaluate the relationships between all these different manners to measure the similarity between data points. DADApy implements two methods for doing this: the neighborhood overlap and the information imbalance. Both approaches use only the distances between the data points as input, making the approaches applicable also when the features are not explicitly defined (e.g., a social network, a set of protein sequences, a dataset of sentences).</p>
        <p id="p0150">The neighborhood overlap is a simple measure of equivalence between two representations.<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref> Given two representations <italic>a</italic> and <italic>b</italic>, one can define two <italic>k</italic>-adjacency matrices <inline-formula><mml:math id="M27" altimg="si19.gif"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M28" altimg="si20.gif"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as matrices of dimension <inline-formula><mml:math id="M29" altimg="si21.gif"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, which are all zero except when <italic>j</italic> is one of the <italic>k</italic> nearest neighbors of point <italic>i</italic>. The neighborhood overlap <inline-formula><mml:math id="M30" altimg="si22.gif"><mml:mrow><mml:mi>χ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mtext>,</mml:mtext><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is then defined as<disp-formula id="fd5"><label>(Equation 5)</label><mml:math id="M31" altimg="si23.gif"><mml:mrow><mml:mi>χ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mtext>,</mml:mtext><mml:mspace width="0.25em"/><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mfrac><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msubsup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="p0155">Note that the term <inline-formula><mml:math id="M32" altimg="si24.gif"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:msubsup><mml:mi>A</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is equal to one only if <italic>j</italic> is within the <italic>k</italic> nearest neighbors of <italic>i</italic> both in <italic>a</italic> and in <italic>b</italic>, otherwise it is zero. For this reason, the neighborhood overlap can also be given a very intuitive interpretation: it is the average fraction of common neighbors in the two representations. If <inline-formula><mml:math id="M33" altimg="si25.gif"><mml:mrow><mml:mi>χ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mtext>,</mml:mtext><mml:mspace width="0.25em"/><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> the two representations can be considered effectively equivalent, while if <inline-formula><mml:math id="M34" altimg="si26.gif"><mml:mrow><mml:mi>χ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mtext>,</mml:mtext><mml:mspace width="0.25em"/><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> they can be considered completely independent. The parameter <italic>k</italic> can be adjusted to improve the robustness of the estimate but in practice this does not significantly change the results obtained as long as <inline-formula><mml:math id="M35" altimg="si27.gif"><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≪</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>.<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref></p>
        <p id="p0160">In the original article,<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref> the neighborhood overlap was proposed to compare layer representations of deep neural networks and to analyze in this their inner workings.</p>
        <p id="p0165">The information imbalance is a recently introduced quantity capable of assessing the information that a distance measure <italic>a</italic> provides about a second distance measure <italic>b</italic>.<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref> It can be used to detect not only whether two distance measures are equivalent or not, but also whether one distance measure is more informative than the other. The information imbalance definition is closely linked to information theory and the theory of copula variables.<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref> However, for the scope of this article it can be empirically defined as<disp-formula id="fd6"><label>(Equation 6)</label><mml:math id="M36" altimg="si28.gif"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo linebreak="badbreak">→</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mi>b</mml:mi></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="1em"/><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>:</mml:mo><mml:mspace width="0.25em"/><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M37" altimg="si29.gif"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the rank matrix of the distance <italic>a</italic> between the points (namely <inline-formula><mml:math id="M38" altimg="si30.gif"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if <italic>j</italic> is the nearest neighbor of <italic>i</italic>, <inline-formula><mml:math id="M39" altimg="si31.gif"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> if <italic>j</italic> is the second neighbor, and so on). In words, the information imbalance from <italic>a</italic> to <italic>b</italic> is proportional to the empirical expectation of the distance ranks in <italic>b</italic> conditioned on the fact that the distance rank between the same two points in <italic>a</italic> is equal to one. If <inline-formula><mml:math id="M40" altimg="si32.gif"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo linebreak="badbreak">→</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> then <italic>a</italic> can be used to describe <italic>b</italic> with no loss of information.</p>
        <p id="p0170">When measuring the information imbalances between two representations we can have three scenarios. If <inline-formula><mml:math id="M41" altimg="si33.gif"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo linebreak="badbreak">→</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="badbreak">→</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> the two representations are equivalent, if <inline-formula><mml:math id="M42" altimg="si34.gif"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo linebreak="badbreak">→</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="badbreak">→</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> the two representations are independent, and, finally, if <inline-formula><mml:math id="M43" altimg="si32.gif"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo linebreak="badbreak">→</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M44" altimg="si35.gif"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="badbreak">→</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> we have that <italic>a</italic> is informative about <italic>b</italic> but not vice versa, therefore <italic>a</italic> is more informative than <italic>b</italic>. The information imbalance allows for effective dimensional reduction since a small subset of features that are the most relevant, either for the full set or for a target property, can be identified and selected.<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref> This feature selection operation is available in DADApy and can be performed as a pre-processing step before the tools described in the previous sections are deployed.</p>
        <p id="p0175">The information imbalance proved successful in dealing with atomistic and molecular descriptors, either to directly perform compression<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref> or to quantify the information loss incurred by competing compression schemes.<xref rid="bib40" ref-type="bibr"><sup>40</sup></xref> In the original article,<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref> the information imbalance was also proposed for detecting causality in time series—with illustrative results shown on COVID-19 time series—and to analyze or optimize the layer representations of deep neural networks.</p>
      </sec>
    </sec>
    <sec id="sec2.2">
      <title>Software structure and usage</title>
      <p id="p0180">DADApy is written entirely in Python, with the most computationally intensive methods being sped up through Cython. It is organized in six main classes: Base, IdEstimation, DensityEstimation, Clustering, MetricComparison, and Data. The relationships of inheritance between these classes, as well as the main methods and attributes available in each class are summarized in <xref rid="fig2" ref-type="fig">Figure 2</xref>. The Base class contains basic methods of data cleaning and manipulation that are inherited in all other classes. Attributes containing the coordinates and/or the distances defining the dataset are contained here. Then, in a train of inheritance: IdEstimation inherits from Base; DensityEstimation inherits from IdEstimation and Clustering inherits from DensityEstimation. Each of these classes contains as methods the algorithms described in the previous section, under the same name. The inheritance structure of these classes is well motivated by the fact that, to perform a density-based clustering one first needs to compute the density, and to perform a density estimation one first needs to know the ID, which can be estimated only if the distances are preliminarily computed. The MetricComparison class contains the algorithms described in the section titled “<xref rid="sec2.1.4" ref-type="sec">Metric comparisons</xref>” used to compare couples of representations using the distances between points.<fig id="fig2"><label>Figure 2</label><caption><p>The class structure of the package</p><p>Classes are highlighted in blue boxes, and the main methods and attributes of each class are reported in the yellow and red boxes, respectively. Relationships of inheritance are indicated as black arrows. The class Data inherits from all other classes, thus providing easy access to all available algorithms of the package.</p></caption><graphic xlink:href="gr2"/></fig></p>
      <p id="p0185">The class Data does not implement any extra attribute or method but, importantly, it inherits all methods and attributes from the other classes. As such, Data provides easy access to all available algorithms of the package and is the main class that is used in practice.</p>
      <p id="p0190">A typical usage of DADApy is reported in <xref rid="fig3" ref-type="fig">Figure 3</xref>. In this simple example a Data object is first initialized with the matrix containing the coordinates of the points shown in <xref rid="fig1" ref-type="fig">Figures 1</xref>B and 1C, and later a series of methods are called sequentially to compute the distances, the ID, the density (<xref rid="fig1" ref-type="fig">Figure 1</xref>B), and finally the DPs (clusters) of the dataset (<xref rid="fig1" ref-type="fig">Figure 1</xref>C). In the example given, Data is initialized with a matrix of coordinates, and the distances between points are later computed. Note that, however, the object could have been equivalently initialized directly with the distances between points, and all methods in the package would work equivalently. This is particularly important for those applications for which coordinates are not available, but distances can be computed, such as DNA or protein sequences, or networks.<fig id="fig3"><label>Figure 3</label><caption><p>A simple DADApy script</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0195">The main aim of the package is to provide user-friendly, fast, and light routines to extract some of the most common and fundamental characteristics of a data manifold through solid statistical and numerical techniques. DADApy offers high-speed code with reduced memory consumption. These features are achieved by exploiting locality. In particular, it is generally enough to compute the distances between each point and a small number of its neighbors (defined in DADApy by an attribute named maxk), and hence such distances can be computed and stored with close-to-linear time and memory requirements.</p>
      <p id="p0200">We believe that the Python interface of DADApy will encourage its rapid diffusion, as Python is by far the most used language in the computational science community nowadays. We are aware that Python is, however, a notoriously inefficient language for large-scale computation. In DADApy we circumvent this shortcoming by implementing all the heavy numerical routines using Cython extensions, which essentially generate C-compilable code that runs with very high efficiency (typically over two orders of magnitude faster in evaluation time than the pure Python implementation). In this manner we are able to maintain the user friendliness of Python without sacrificing the computational efficiency of a fully compiled language.</p>
      <p id="p0205">All of the mentioned properties allow to easily analyze up to a million points on an ordinary laptop within minutes. This can be seen in <xref rid="fig4" ref-type="fig">Figure 4</xref>, where we report the time spent by the code on many DADApy routines as a function of the number <italic>N</italic> of points of the dataset, using a neighborhood of maxk = 100 points. The plot shows that all methods scale linearly in computational time with <italic>N</italic>, with the exception of the ADP clustering, whose scaling becomes unfavorable for more than 50,000 points. This is a consequence of the neighborhood size maxk being much smaller than the number of points <italic>N</italic> of the dataset, a condition which forces the estimation of many fictitious DPs that take a long time to be merged together. The problem can be solved by appropriately increasing maxk when necessary.<fig id="fig4"><label>Figure 4</label><caption><p>Time complexity of DADApy</p><p>The time required by the various routines of DADApy grows linearly with the number of samples <italic>N</italic>, with the only exception of ADP (see text for details). The dataset used was two dimensional and we set maxk = 100. The benchmark was performed on an ordinary desktop using a single Intel Xeon(R) CPU E5-2650 v2 at 2.60 GHz.</p></caption><graphic xlink:href="gr4"/></fig></p>
      <p id="p0210">The runtime performance for the computation of the distances also scales linearly with the embedding dimension <italic>D</italic>, while the other routines take as input the computed distances, and are thus independent on <italic>D</italic>. Therefore, when <italic>D</italic> is very large, say <inline-formula><mml:math id="M45" altimg="si36.gif"><mml:mrow><mml:mi>D</mml:mi><mml:mo>⪆</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, the distance computation can represent the actual computational bottleneck of the package.</p>
      <p id="p0215">The code has been thoroughly commented and documented through a set of easy-to-run Jupyter notebooks, an online manual, and an extensive code reference. This can allow new users approaching DADApy to quickly learn to use it, as well as to modify or extend it.</p>
    </sec>
    <sec id="sec2.3">
      <title>Illustration on a topologically complex synthetic dataset</title>
      <p id="p0220">We now illustrate the use of some key DADApy methods on the synthetic dataset depicted in <xref rid="fig5" ref-type="fig">Figure 5</xref>A, and consisting of a 2D plane with eight clusters, twisted to form a 3D Möbius strip and finally embedded in a noisy 50D space. The reference 2D dataset is taken from d’Errico et al.,<xref rid="bib27" ref-type="bibr"><sup>27</sup></xref> and consists of data points sampled from an analytic density function, with points belonging to a single mode of this density assigned to the same cluster, and all other considered unassigned.<fig id="fig5"><label>Figure 5</label><caption><p>Example usage of DADApy for the analysis of a topologically complex synthetic dataset</p><p>(A) The dataset analyzed, consisting of clusters lying on a 2D sheet twisted to form a Möbius strip and immersed in a noisy 50D space.</p><p>(B) The accuracy of some common clustering methods on reconstructing the original clusters (in order: Kmeans, Spectral Clustering [SC], DBSCAN, HDBSCAN, and ADP), as well as two low-dimensional projections.</p><p>(C) Summary of the results obtained using 2NN ID estimation, PA<italic>k</italic> density estimation and ADP clustering. The top part shows the estimated density peaks, while the bottom part shows the dendrogram of the dataset. The y axis of the dendrogram reports the log density of the density peaks and of the saddle points. The x axis provides an indication on the relative cluster sizes, since each cluster is in the middle of a region proportional to its population. This region is delimited by the links in which these clusters are involved and, in the case of the first and last clusters, by the beginning and end of the graph.</p></caption><graphic xlink:href="gr5"/></fig></p>
      <p id="p0225">Despite the 2D inner structure of the dataset, common projection methods can easily fail as a consequence of the nontrivial topological properties of the data manifold. This is illustrated in <xref rid="fig5" ref-type="fig">Figure 5</xref>B, where PCA and ISOMAP projections are reported.</p>
      <p id="p0230">One key advantage of the methods implemented in DADApy is their ability to exploit the low-dimensional structure of the data without any explicit projection. In this case, for example, we compute the ID using the Gride method (see “intrinsic dimension estimators"), which is correctly identified around 2. We then use the ID to provide accurate density estimates using the PA<italic>k</italic> method from “density estimators,” and finally identify the clusters (or DPs) using the ADP algorithm from “density peak clustering.” The end result is a cluster assignment that is remarkably close to the ground truth, and often superior to other state-of-the-art clustering schemes that do not exploit the low-dimensional structure of the data (see <xref rid="fig5" ref-type="fig">Figure 5</xref>B).</p>
      <p id="p0235">Another unique feature of DADApy is the ability of compactly representing the cluster structure through a special kind of dendrogram reporting the log densities of the DPs and of the saddle points between them. The bottom part of <xref rid="fig5" ref-type="fig">Figure 5</xref>C depicts the dendrogram for the Möbius strip data, which can be seen to provide a remarkably accurate perspective of the relationship between the estimated DPs shown in the upper panel of the figure.</p>
      <p id="p0240">Note that the dendrogram can be generated independently of the ID of the manifold, unlike most graphical data representations which are practically limited to three dimensions, thus providing a robust way to visualize the cluster structure even for the common scenario of <inline-formula><mml:math id="M46" altimg="si37.gif"><mml:mrow><mml:mtext>ID</mml:mtext><mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> manifolds.</p>
      <p id="p0245">The Jupyter notebook used to perform the analysis described in this section can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/sissa-data-science/DADApy/blob/main/examples/notebook_mobius.ipynb" id="intref0030">https://github.com/sissa-data-science/DADApy/blob/main/examples/notebook_mobius.ipynb</ext-link>.</p>
    </sec>
    <sec id="sec2.4">
      <title>Usage for a realistic application</title>
      <p id="p0250">We now exemplify and showcase the usage of DADApy for the analysis of a biomolecular trajectory. The dataset is composed of 41,580 frames from a replica-exchange MD simulation (400 ns, 340 K replica, dt = 2 fs) of the 10-residue peptide CLN025, which folds into a beta hairpin.<xref rid="bib41" ref-type="bibr"><sup>41</sup></xref> Several numerical representations are possible for this trajectory. A very high-dimensional one is given by the set of all distances between the heavy atoms, which amounts to 4,278 features. Such a representation is possibly very redundant, and in fact typically more compact representations are used to describe systems of this type. For example, a compact representation for this system can be taken as the set of all its 32 dihedral angles.<xref rid="bib42" ref-type="bibr"><sup>42</sup></xref><sup>,</sup><xref rid="bib43" ref-type="bibr"><sup>43</sup></xref> In <xref rid="fig6" ref-type="fig">Figure 6</xref>A we use DADApy to compute the information imbalance from the space of heavy atom distances to the space of the dihedral angles for an increasing number of dihedral angles, and vice versa. Not surprisingly, the compact space of dihedral angles is seen to be almost equally informative to the very high-dimensional heavy atom distance space, with information imbalance <inline-formula><mml:math id="M47" altimg="si38.gif"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mtext>dihedrals</mml:mtext></mml:msub><mml:mo linebreak="badbreak">→</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mtext>full</mml:mtext></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> lower than 0.1 when considering around 15 angles (<xref rid="fig6" ref-type="fig">Figure 6</xref>A). We thus select the set of the 15 most informative dihedral angles as the collective variables to represent this dataset, since the information imbalance reaches a plateau around this number.<fig id="fig6"><label>Figure 6</label><caption><p>Example usage of DADApy for the analysis of a biomolecular trajectory</p><p>(A) The computation of the information imbalance between a compact molecular representation <inline-formula><mml:math id="M48" altimg="si39.gif"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mtext>dihedrals</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> (optimally selected sets of dihedral angles with increasing size) and a much higher dimensional one <inline-formula><mml:math id="M49" altimg="si40.gif"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mtext>full</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> (the full space of heavy atom distances). The inset shows the information imbalance between the space of heavy atom distances and the space of dihedral angles, and vice versa. For clarity, the depicted points are sparsed out.</p><p>(B) The computation of the intrinsic dimension across different scales using both 2NN and Gride. The main graph refers to the space of 15 dihedrals, while the inset refers to the space of 4,278 heavy atom distances.</p><p>(C) A dendrogram visualization of the peaks and the saddle points of the density, estimated using PA<italic>k</italic> and the ADP clustering algorithm. Peptide backbones of cluster center structures are drawn next to their corresponding peaks. The main graph refers to the space of dihedrals, while the inset refers to the space of heavy atom distances. In both cases, the central and rightmost peaks capture the main macro states of the peptide and are much more populated than the leftmost peak. The two cluster assignments are identical for roughly 90% of the data points.</p></caption><graphic xlink:href="gr6"/></fig></p>
      <p id="p0255">We then use DADApy to compute the ID of the dataset along different scales through both decimation and the Gride algorithm<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> (<xref rid="fig6" ref-type="fig">Figure 6</xref>B). The two procedures provide fairly overlapping estimates for the ID, which is comprised between 5 and 8 within short range distances, and thus much lower than the original feature space. We continue by estimating the density through the PA<italic>k</italic> algorithm, for which we set the ID to 7. This ID selection is motivated by the observation that the density is a local property computed at short scales but, importantly, selecting a lower ID consistent with <xref rid="fig6" ref-type="fig">Figure 6</xref>B (say, 5 or 6) does not significantly affect the results. Finally, we use DADApy to perform clustering using the ADP algorithm. The results are shown in <xref rid="fig6" ref-type="fig">Figure 6</xref>C.</p>
      <p id="p0260">ADP clustering (Z = 4.5) produces three clusters. The biggest cluster is the folded beta hairpin state of the protein, as depicted in <xref rid="fig6" ref-type="fig">Figure 6</xref>C (cluster 0). A cluster of roughly half the size is made of a collapsed twisted loop structure (<xref rid="fig6" ref-type="fig">Figure 6</xref>C, cluster 2). Since CLN025 is suspected to have two main metastable states, the folded hairpin and a denatured collapsed state,<xref rid="bib44" ref-type="bibr"><sup>44</sup></xref> we suggest that the twisted loop could be the dominant topology of the denatured collapsed ensemble. The high occurrence of the twisted loop might be due to the simulation temperature of 340 K, which is just below the experimental melting temperature of CLN025 of 343 K.<xref rid="bib45" ref-type="bibr"><sup>45</sup></xref> Less than 1% of the structures are in cluster 1, which is composed of denatured extended and less-structured topologies.</p>
      <p id="p0265">The 32-dimensional space of dihedrals used so far in our analysis is known to be well suited to differentiate meaningful protein structures but, to showcase the possibility of using DADApy to work in very-high-dimensional spaces, we performed a similar analysis also on the 4,278-dimensional space of all heavy atom distances. Using this alternative data description we performed ID estimation with the 2NN method, density estimation with the PA<italic>k</italic> estimator, and clustering with the ADP algorithm (ID = 9; Z = 3.5). The resulting dendrogram is shown as an inset of <xref rid="fig6" ref-type="fig">Figure 6</xref>C.</p>
      <p id="p0270">As clear from the figure, we find a remarkably similar cluster structure, defined by the two major macrostates of the molecule, the beta pin and the twisted loop, as well as the cluster with unstructured configurations.</p>
      <p id="p0275">The equivalence in the two cluster assignments is confirmed by the fact that 89% of the data points are assigned to the same cluster independently of the data representation.</p>
      <p id="p0280">A Jupyter notebook containing the analyses performed in this section is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sissa-data-science/DADApy/blob/main/examples/notebook_beta_hairpin.ipynb" id="intref0035">https://github.com/sissa-data-science/DADApy/blob/main/examples/notebook_beta_hairpin.ipynb</ext-link> along with the necessary datasets.</p>
    </sec>
    <sec id="sec2.5">
      <title>Conclusions</title>
      <p id="p0285">In this work we introduce DADApy, a software package for quickly extracting fundamental properties of data manifolds. DADApy is written entirely in Python, which makes it easy to use and to extend; and it exploits Cython extensions and algorithms for sparse computation and sparse memory handling, which make it computationally efficient and scalable to large datasets. The package is documented by a set of easy-to-run Jupyter notebooks and by a code-reference and manual available online.</p>
      <p id="p0290">DADApy includes state-of-the-art algorithms for ID estimation, density estimation, density-based clustering, and distance comparison, which found numerous applications in recent years, but have not yet found widespread usability. We believe this was, at least in part, precisely due to the lack of a fast and easy-to-use software like DADApy, and we hope that our work will allow a growing number of practitioners from different research domains to approach the field of manifold learning.</p>
      <p id="p0295">The algorithms included in DADApy do not rely on low-dimensional projections or on any strong assumptions on the structure of the data. This can be a great advantage, as it makes DADApy suited to analyze topologically complex data manifolds, but it also means that DADApy cannot be used to build low-dimensional maps for data visualization. Other shortcomings of the software are in its level of maturity for industrial-grade standards—DADApy is still a young software—and in the relatively small number of algorithms implemented in it.</p>
      <p id="p0300">We plan to improve DADApy by addressing both of these issues. On the one hand we are working on the development of algorithms that extend many of the methods discussed here, including ID estimators for discrete spaces,<xref rid="bib46" ref-type="bibr"><sup>46</sup></xref> density estimators that exploit data correlations, and more refined feature selection schemes based on the information imbalance, and intend to implement these as new DADApy methods. On the other hand we intend to improve code quality in a variety of directions, such as by increasing unit test coverage, expanding documentation and lint checks, and adding static type checking. Finally, we will greatly welcome open-source contributions to the project.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Experimental procedures</title>
    <sec id="sec3.1">
      <title>Resource availability</title>
      <sec id="sec3.1.1">
        <title>Lead contact</title>
        <p id="p0305">Further information and requests for resources should be directed to and will be fulfilled by the lead contact, Aldo Glielmo (<ext-link ext-link-type="uri" xlink:href="mailto:aldo.glielmo@bancaditalia.it" id="intref0040">aldo.glielmo@bancaditalia.it</ext-link>).</p>
      </sec>
      <sec id="sec3.1.2">
        <title>Materials availability</title>
        <p id="p0310">This study did not generate new materials.</p>
      </sec>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Schütt</surname>
            <given-names>K.T.</given-names>
          </name>
          <name>
            <surname>Chmiela</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>von Lilienfeld</surname>
            <given-names>O.A.</given-names>
          </name>
          <name>
            <surname>Tkatchenko</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tsuda</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>K.-R.</given-names>
          </name>
        </person-group>
        <article-title>Machine learning meets quantum physics</article-title>
        <source>Lect. Notes Phys.</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Glielmo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Husic</surname>
            <given-names>B.E.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Clementi</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Noé</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised learning methods for molecular simulation data</article-title>
        <source>Chem. Rev.</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Carleo</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Cirac</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Cranmer</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Daudet</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Schuld</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Tishby</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Vogt-Maranto</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zdeborová</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Machine learning and the physical sciences</article-title>
        <source>Rev. Mod. Phys.</source>
        <volume>91</volume>
        <year>2019</year>
        <fpage>045002</fpage>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="book" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Keogh</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Mueen</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Curse of Dimensionality</part-title>
        <year>2010</year>
        <publisher-name>Springer US</publisher-name>
        <fpage>257</fpage>
        <lpage>258</lpage>
        <pub-id pub-id-type="doi">10.1007/978-0-387-30164-8_192</pub-id>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="book" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Aggarwal</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>Hinneburg</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Keim</surname>
            <given-names>D.A.</given-names>
          </name>
        </person-group>
        <part-title>On the surprising behavior of distance metrics in high dimensional space</part-title>
        <source>International conference on database theory</source>
        <year>2001</year>
        <publisher-name>Springer</publisher-name>
        <fpage>420</fpage>
        <lpage>434</lpage>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: machine learning in Python</article-title>
        <source>J. Mach. Learn. Res.</source>
        <volume>12</volume>
        <year>2011</year>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Abdi</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>L.J.</given-names>
          </name>
        </person-group>
        <article-title>Principal component analysis</article-title>
        <source>WIREs. Comp. Stat.</source>
        <volume>2</volume>
        <year>2010</year>
        <fpage>433</fpage>
        <lpage>459</lpage>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="book" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Schölkopf</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Smola</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Müller</surname>
            <given-names>K.-R.</given-names>
          </name>
        </person-group>
        <part-title>Kernel principal component analysis</part-title>
        <source>International conference on artificial neural networks</source>
        <year>1997</year>
        <publisher-name>Springer</publisher-name>
        <fpage>583</fpage>
        <lpage>588</lpage>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Balasubramanian</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>E.L.</given-names>
          </name>
        </person-group>
        <article-title>The isomap algorithm and topological stability</article-title>
        <source>Science</source>
        <volume>295</volume>
        <year>2002</year>
        <fpage>7</fpage>
        <pub-id pub-id-type="pmid">11778013</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Campadelli</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Casiraghi</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Ceruti</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Rozza</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic dimension estimation: relevant techniques and a benchmark framework</article-title>
        <source>Math. Probl Eng.</source>
        <volume>2015</volume>
        <year>2015</year>
        <fpage>1</fpage>
        <lpage>21</lpage>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Camastra</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Staiano</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic dimension estimation: Advances and open problems</article-title>
        <source>Inf. Sci.</source>
        <volume>328</volume>
        <year>2016</year>
        <fpage>26</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="book" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Levina</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bickel</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <part-title>Maximum likelihood estimation of intrinsic dimension</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Saul</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bottou</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <series>Advances in Neural Information Processing Systems</series>
        <volume>17</volume>
        <year>2004</year>
        <publisher-name>MIT Press</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf" id="intref0045">https://proceedings.neurips.cc/paper/2004/file/74934548253bcab8490ebd74afed7031-Paper.pdf</ext-link>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Denti</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Doimo</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mira</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Distributional results for model-based intrinsic dimension estimators</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2021</year>
        <fpage>13832</fpage>
        <comment>preprint arXiv:2104</comment>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Facco</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>d’Errico</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Estimating the intrinsic dimension of datasets by a minimal neighborhood information</article-title>
        <source>Sci. Rep.</source>
        <volume>7</volume>
        <year>2017</year>
        <fpage>12140</fpage>
        <lpage>12148</lpage>
        <pub-id pub-id-type="pmid">28939866</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="book" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Ansuini</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Macke</surname>
            <given-names>J.H.</given-names>
          </name>
          <name>
            <surname>Zoccolan</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Intrinsic dimension of data representations in deep neural networks</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Wallach</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Larochelle</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Beygelzimer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>d’ Alché-Buc</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Fox</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Garnett</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <series>Advances in Neural Information Processing Systems</series>
        <volume>32</volume>
        <year>2019</year>
        <publisher-name>Curran Associates, Inc.</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2019/file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf" id="intref0050">https://proceedings.neurips.cc/paper/2019/file/cfcce0621b49c983991ead4c3d4d3b6b-Paper.pdf</ext-link>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Mendes-Santos</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Turkeshi</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Dalmonte</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised learning universal critical behavior via the intrinsic dimension</article-title>
        <source>Phys. Rev. X</source>
        <volume>11</volume>
        <year>2021</year>
        <fpage>011040</fpage>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Capelli</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Gardin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Empereur-Mot</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Doni</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Pavan</surname>
            <given-names>G.M.</given-names>
          </name>
        </person-group>
        <article-title>A data-driven dimensionality reduction approach to compare and classify lipid force fields</article-title>
        <source>J. Phys. Chem. B</source>
        <volume>125</volume>
        <year>2021</year>
        <fpage>7785</fpage>
        <lpage>7796</lpage>
        <pub-id pub-id-type="pmid">34254518</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>d’Errico</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Facco</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Computing the free energy without collective variables</article-title>
        <source>J. Chem. Theory Comput.</source>
        <volume>14</volume>
        <year>2018</year>
        <fpage>1206</fpage>
        <lpage>1215</lpage>
        <pub-id pub-id-type="pmid">29401379</pub-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Loftsgaarden</surname>
            <given-names>D.O.</given-names>
          </name>
          <name>
            <surname>Quesenberry</surname>
            <given-names>C.P.</given-names>
          </name>
        </person-group>
        <article-title>A nonparametric estimate of a multivariate density function</article-title>
        <source>Ann. Math. Statist.</source>
        <volume>36</volume>
        <year>1965</year>
        <fpage>1049</fpage>
        <lpage>1051</lpage>
        <pub-id pub-id-type="doi">10.1214/aoms/1177700079</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Unfolding hidden barriers by active enhanced sampling</article-title>
        <source>Phys. Rev. Lett.</source>
        <volume>121</volume>
        <year>2018</year>
        <fpage>010601</fpage>
        <pub-id pub-id-type="doi">10.1103/PhysRevLett.121.010601</pub-id>
        <ext-link ext-link-type="uri" xlink:href="https://link.aps.org/doi/10.1103/PhysRevLett.121.010601" id="intref0055">https://link.aps.org/doi/10.1103/PhysRevLett.121.010601</ext-link>
        <pub-id pub-id-type="pmid">30028174</pub-id>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Marinelli</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Faraldo-Gómez</surname>
            <given-names>J.D.</given-names>
          </name>
        </person-group>
        <article-title>Force-correction analysis method for derivation of multidimensional free-energy landscapes from adaptively biased replica simulations</article-title>
        <source>J. Chem. Theory Comput.</source>
        <volume>17</volume>
        <year>2021</year>
        <fpage>6775</fpage>
        <lpage>6788</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jctc.1c00586</pub-id>
        <comment>pMID: 34669402. arXiv:</comment>
        <pub-id pub-id-type="pmid">34669402</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Salahub</surname>
            <given-names>D.R.</given-names>
          </name>
        </person-group>
        <article-title>Multiscale molecular modelling: from electronic structure to dynamics of nanosystems and beyond</article-title>
        <source>Phys. Chem. Chem. Phys.</source>
        <volume>24</volume>
        <year>2022</year>
        <fpage>9051</fpage>
        <lpage>9081</lpage>
        <pub-id pub-id-type="doi">10.1039/D1CP05928A</pub-id>
        <pub-id pub-id-type="pmid">35389399</pub-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Offei-Danso</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hassanali</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>High-dimensional fluctuations in liquid water: Combining chemical intuition with unsupervised learning</article-title>
        <source>J. Chem. Theory Comput.</source>
        <volume>18</volume>
        <year>2022</year>
        <fpage>3136</fpage>
        <lpage>3150</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jctc.1c01292</pub-id>
        <comment>pMID: 35472272. arXiv:</comment>
        <pub-id pub-id-type="pmid">35472272</pub-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Carli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Statistically unbiased free energy estimates from biased simulations</article-title>
        <source>Mol. Phys.</source>
        <volume>119</volume>
        <year>2021</year>
        <fpage>e1899323</fpage>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Zeni</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Anelli</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Glielmo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rossi</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Exploring the robust extrapolation of high-dimensional machine learning potentials</article-title>
        <source>Phys. Rev. B</source>
        <volume>105</volume>
        <year>2022</year>
        <fpage>165141</fpage>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Clustering by fast search and find of density peaks</article-title>
        <source>science</source>
        <volume>344</volume>
        <year>2014</year>
        <fpage>1492</fpage>
        <lpage>1496</lpage>
        <pub-id pub-id-type="pmid">24970081</pub-id>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>d’Errico</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Facco</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Automatic topography of high-dimensional data sets by non-parametric density peak clustering</article-title>
        <source>Inf. Sci.</source>
        <volume>560</volume>
        <year>2021</year>
        <fpage>476</fpage>
        <lpage>492</lpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Ziegler</surname>
            <given-names>C.G.K.</given-names>
          </name>
          <name>
            <surname>Allon</surname>
            <given-names>S.J.</given-names>
          </name>
          <name>
            <surname>Nyquist</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Mbano</surname>
            <given-names>I.M.</given-names>
          </name>
          <name>
            <surname>Miao</surname>
            <given-names>V.N.</given-names>
          </name>
          <name>
            <surname>Tzouanas</surname>
            <given-names>C.N.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yousif</surname>
            <given-names>A.S.</given-names>
          </name>
          <name>
            <surname>Bals</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hauser</surname>
            <given-names>B.M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sars-cov-2 receptor ace2 is an interferon-stimulated gene in human airway epithelial cells and is detected in specific cell subsets across tissues</article-title>
        <source>Cell</source>
        <volume>181</volume>
        <year>2020</year>
        <fpage>1016</fpage>
        <lpage>1035.e19</lpage>
        <pub-id pub-id-type="pmid">32413319</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Habib</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Heidenreich</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Swiech</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Avraham-Davidi</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Trombetta</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Hession</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Regev</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Div-seq: single-nucleus rna-seq reveals dynamics of rare adult newborn neurons</article-title>
        <source>Science</source>
        <volume>353</volume>
        <year>2016</year>
        <fpage>925</fpage>
        <lpage>928</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aad7038</pub-id>
        <comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/pdf/10.1126/science.aad7038" id="intref0060">https://www.science.org/doi/pdf/10.1126/science.aad7038</ext-link></comment>
        <ext-link ext-link-type="uri" xlink:href="https://www.science.org/doi/abs/10.1126/science.aad7038" id="intref0065">https://www.science.org/doi/abs/10.1126/science.aad7038</ext-link>
        <pub-id pub-id-type="pmid">27471252</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="journal" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Yger</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Spampinato</surname>
            <given-names>G.L.</given-names>
          </name>
          <name>
            <surname>Esposito</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Lefebvre</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Deny</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Gardella</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Stimberg</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Jetter</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Zeck</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Picaud</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A spike sorting toolbox for up to thousands of electrodes validated with ground truth recordings in vitro and in vivo</article-title>
        <source>Elife</source>
        <volume>7</volume>
        <year>2018</year>
        <fpage>e34518</fpage>
        <pub-id pub-id-type="doi">10.7554/eLife.34518</pub-id>
        <pub-id pub-id-type="pmid">29557782</pub-id>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="journal" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Sperry</surname>
            <given-names>Z.J.</given-names>
          </name>
          <name>
            <surname>Na</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Jun</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>L.R.</given-names>
          </name>
          <name>
            <surname>Socha</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Seymour</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Bruns</surname>
            <given-names>T.M.</given-names>
          </name>
        </person-group>
        <article-title>High-density neural recordings from feline sacral dorsal root ganglia with thin-film array</article-title>
        <source>J. Neural. Eng.</source>
        <volume>18</volume>
        <year>2021</year>
        <fpage>046005</fpage>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="journal" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W.M.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>C.-L.</given-names>
          </name>
          <name>
            <surname>Hao</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification</article-title>
        <source>Asian Pac. J. Trop. Med.</source>
        <volume>9</volume>
        <year>2016</year>
        <fpage>806</fpage>
        <lpage>811</lpage>
        <pub-id pub-id-type="pmid">27569893</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>33</label>
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Margazoglou</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Grafke</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lucarini</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Dynamical landscape and multistability of a climate model</article-title>
        <source>Proc. Math. Phys. Eng. Sci.</source>
        <volume>477</volume>
        <year>2021</year>
        <fpage>20210019</fpage>
        <pub-id pub-id-type="pmid">35153562</pub-id>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>34</label>
      <element-citation publication-type="journal" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Pinamonti</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Noé</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bussi</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The mechanism of rna base fraying: molecular dynamics simulations analyzed with core-set Markov state models</article-title>
        <source>J. Chem. Phys.</source>
        <volume>150</volume>
        <year>2019</year>
        <fpage>154123</fpage>
        <pub-id pub-id-type="pmid">31005065</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>35</label>
      <element-citation publication-type="journal" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Jong</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Hassanali</surname>
            <given-names>A.A.</given-names>
          </name>
        </person-group>
        <article-title>A data science approach to understanding water networks around biomolecules: the case of tri-alanine in liquid water</article-title>
        <source>J. Phys. Chem. B</source>
        <volume>122</volume>
        <year>2018</year>
        <fpage>7895</fpage>
        <lpage>7906</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jpcb.8b03644</pub-id>
        <comment>pMID: 30019898</comment>
        <pub-id pub-id-type="pmid">30019898</pub-id>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>36</label>
      <element-citation publication-type="journal" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Carli</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Sormani</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Candidate binding sites for allosteric inhibition of the SARS-CoV-2 main protease from the analysis of large-scale molecular dynamics simulations</article-title>
        <source>J. Phys. Chem. Lett.</source>
        <volume>12</volume>
        <year>2020</year>
        <fpage>65</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jpclett.0c03182</pub-id>
        <pub-id pub-id-type="pmid">33306377</pub-id>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>Sormani</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Explicit characterization of the free-energy landscape of a protein in the space of all its cα carbons</article-title>
        <source>J. Chem. Theory Comput.</source>
        <volume>16</volume>
        <year>2020</year>
        <fpage>80</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="doi">10.1021/acs.jctc.9b00800</pub-id>
        <comment>80–87, pMID: 31809040. arXiv:</comment>
        <pub-id pub-id-type="pmid">31809040</pub-id>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="book" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Doimo</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Glielmo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ansuini</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Hierarchical nucleation in deep neural networks</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Larochelle</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Ranzato</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hadsell</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Balcan</surname>
            <given-names>M.F.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <source>Adv. Neural Inf. Process. Syst.</source>
        <volume>33</volume>
        <year>2020</year>
        <publisher-name>Curran Associates, Inc.</publisher-name>
        <fpage>7526</fpage>
        <lpage>7536</lpage>
        <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2020/file/54f3bc04830d762a3b56a789b6ff62df-Paper.pdf" id="intref0070">https://proceedings.neurips.cc/paper/2020/file/54f3bc04830d762a3b56a789b6ff62df-Paper.pdf</ext-link>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="journal" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Glielmo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zeni</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Csányi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Ranking the information content of distance measures</article-title>
        <source>PNAS Nexus</source>
        <volume>1</volume>
        <year>04 2022</year>
        <fpage>pgac039</fpage>
        <pub-id pub-id-type="doi">10.1093/pnasnexus/pgac039</pub-id>
        <comment>arXiv:<ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/pnasnexus/article-pdf/1/2/pgac039/44246399/pgac039.pdf" id="intref0075">https://academic.oup.com/pnasnexus/article-pdf/1/2/pgac039/44246399/pgac039.pdf</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="journal" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Darby</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Kermode</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Csányi</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Compressing local atomic neighbourhood descriptors</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2021</year>
        <comment>2112.13055</comment>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="journal" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Honda</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yamasaki</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Sawada</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Morii</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>10 residue folded peptide designed by segment statistics</article-title>
        <source>Structure</source>
        <volume>12</volume>
        <year>2004</year>
        <fpage>1507</fpage>
        <lpage>1518</lpage>
        <pub-id pub-id-type="doi">10.1016/j.str.2004.05.022</pub-id>
        <pub-id pub-id-type="pmid">15296744</pub-id>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Bonomi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Branduardi</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bussi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Camilloni</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Provasi</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Raiteri</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Donadio</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Marinelli</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Pietrucci</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Broglia</surname>
            <given-names>R.A.</given-names>
          </name>
          <name>
            <surname>Parrinello</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Plumed: a portable plugin for free-energy calculations with molecular dynamics</article-title>
        <source>Comput. Phys. Commun.</source>
        <volume>180</volume>
        <year>2009</year>
        <fpage>1961</fpage>
        <lpage>1972</lpage>
      </element-citation>
    </ref>
    <ref id="bib43">
      <label>43</label>
      <element-citation publication-type="journal" id="sref43">
        <person-group person-group-type="author">
          <name>
            <surname>Cossio</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pietrucci</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Which similarity measure is better for analyzing protein structures in a molecular dynamics trajectory?</article-title>
        <source>Phys. Chem. Chem. Phys.</source>
        <volume>13</volume>
        <year>2011</year>
        <fpage>10421</fpage>
        <lpage>10425</lpage>
        <pub-id pub-id-type="pmid">21465020</pub-id>
      </element-citation>
    </ref>
    <ref id="bib44">
      <label>44</label>
      <element-citation publication-type="journal" id="sref44">
        <person-group person-group-type="author">
          <name>
            <surname>McKiernan</surname>
            <given-names>K.A.</given-names>
          </name>
          <name>
            <surname>Husic</surname>
            <given-names>B.E.</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V.S.</given-names>
          </name>
        </person-group>
        <article-title>Modeling the mechanism of cln025 beta-hairpin formation</article-title>
        <source>J. Chem. Phys.</source>
        <volume>147</volume>
        <year>2017</year>
        <fpage>104107</fpage>
        <pub-id pub-id-type="doi">10.1063/1.4993207</pub-id>
        <pub-id pub-id-type="pmid">28915754</pub-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <label>45</label>
      <element-citation publication-type="journal" id="sref45">
        <person-group person-group-type="author">
          <name>
            <surname>Honda</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Akiba</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kato</surname>
            <given-names>Y.S.</given-names>
          </name>
          <name>
            <surname>Sawada</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Sekijima</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ishimura</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ooishi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Watanabe</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Odahara</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Harata</surname>
            <given-names>K.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Crystal structure of a ten-amino acid protein</article-title>
        <source>J. Am. Chem. Soc.</source>
        <volume>130</volume>
        <year>2008</year>
        <fpage>15327</fpage>
        <lpage>15331</lpage>
        <pub-id pub-id-type="doi">10.1021/ja8030533</pub-id>
        <pub-id pub-id-type="pmid">18950166</pub-id>
      </element-citation>
    </ref>
    <ref id="bib46">
      <label>46</label>
      <element-citation publication-type="journal" id="sref46">
        <person-group person-group-type="author">
          <name>
            <surname>Macocco</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Glielmo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Grilli</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Laio</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Intrinsic dimension estimation for discrete metrics</article-title>
        <comment>Preprint at</comment>
        <source>arXiv</source>
        <year>2022</year>
        <comment>2207.09688</comment>
      </element-citation>
    </ref>
  </ref-list>
  <sec sec-type="data-availability" id="da0010">
    <title>Data and code availability</title>
    <p id="p0030">DADApy is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sissa-data-science/DADApy" id="intref0010">https://github.com/sissa-data-science/DADApy</ext-link> (<ext-link ext-link-type="doi" xlink:href="10.5281/zenodo.6998360" id="intref0015">https://doi.org/10.5281/zenodo.6998360</ext-link>), and the notebooks to generate the key graphs of <xref rid="fig5" ref-type="fig">Figures 5</xref> and <xref rid="fig6" ref-type="fig">6</xref> are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/sissa-data-science/DADApy/blob/main/examples/notebook_mobius.ipynb" id="intref0020">https://github.com/sissa-data-science/DADApy/blob/main/examples/notebook_mobius.ipynb</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/sissa-data-science/DADApy/blob/main/examples/notebook_beta_hairpin.ipynb" id="intref0025">https://github.com/sissa-data-science/DADApy/blob/main/examples/notebook_beta_hairpin.ipynb</ext-link>, respectively. We strongly encourage the scientific community to fork the repository, submit pull requests, and open new issues through the GitHub interface.</p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0320">A.G. and A.L. acknowledge support from the <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100007601</institution-id><institution>European Union’s Horizon 2020</institution></institution-wrap></funding-source> research and innovation program (grant no. 824143, <funding-source id="gs2">MaX “Materials design at the eXascale”</funding-source> Centre of Excellence).</p>
    <sec id="sec5">
      <title>Author contributions</title>
      <p id="p0325">Conceptualization, A.G. and A.L.; software, all authors; writing – original draft, A.G., I.M., and A.L.; writing – review &amp; editing, all authors; visualization, C.Z., I.M., and R.W.; supervision, A.G. and A.L.</p>
    </sec>
    <sec sec-type="COI-statement" id="sec6">
      <title>Declaration of interests</title>
      <p id="p0330">The authors declare no competing interests.</p>
    </sec>
  </ack>
</back>
