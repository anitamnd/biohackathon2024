<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-3.dtd?>
<?SourceDTD.Version 1.3?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Int J Mol Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Int J Mol Sci</journal-id>
    <journal-id journal-id-type="publisher-id">ijms</journal-id>
    <journal-title-group>
      <journal-title>International Journal of Molecular Sciences</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1422-0067</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9653958</article-id>
    <article-id pub-id-type="doi">10.3390/ijms232113469</article-id>
    <article-id pub-id-type="publisher-id">ijms-23-13469</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PollenDetect: An Open-Source Pollen Viability Status Recognition System Based on Deep Learning Neural Networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Tan</surname>
          <given-names>Zhihao</given-names>
        </name>
        <xref rid="af1-ijms-23-13469" ref-type="aff">1</xref>
        <xref rid="fn1-ijms-23-13469" ref-type="author-notes">†</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Jing</given-names>
        </name>
        <xref rid="af1-ijms-23-13469" ref-type="aff">1</xref>
        <xref rid="af2-ijms-23-13469" ref-type="aff">2</xref>
        <xref rid="fn1-ijms-23-13469" ref-type="author-notes">†</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8579-3390</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Qingyuan</given-names>
        </name>
        <xref rid="af3-ijms-23-13469" ref-type="aff">3</xref>
        <xref rid="c1-ijms-23-13469" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Su</surname>
          <given-names>Fengxiang</given-names>
        </name>
        <xref rid="af1-ijms-23-13469" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Tianxu</given-names>
        </name>
        <xref rid="af1-ijms-23-13469" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Weiran</given-names>
        </name>
        <xref rid="af2-ijms-23-13469" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Aierxi</surname>
          <given-names>Alifu</given-names>
        </name>
        <xref rid="af2-ijms-23-13469" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Xianlong</given-names>
        </name>
        <xref rid="af1-ijms-23-13469" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1095-1355</contrib-id>
        <name>
          <surname>Yang</surname>
          <given-names>Wanneng</given-names>
        </name>
        <xref rid="af1-ijms-23-13469" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2456-9595</contrib-id>
        <name>
          <surname>Kong</surname>
          <given-names>Jie</given-names>
        </name>
        <xref rid="af2-ijms-23-13469" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4278-0626</contrib-id>
        <name>
          <surname>Min</surname>
          <given-names>Ling</given-names>
        </name>
        <xref rid="af1-ijms-23-13469" ref-type="aff">1</xref>
        <xref rid="c1-ijms-23-13469" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Chen</surname>
          <given-names>Jen-Tsung</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Cao</surname>
          <given-names>Yunpeng</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <aff id="af1-ijms-23-13469"><label>1</label>National Key Laboratory of Crop Genetic Improvement, Huazhong Agricultural University, Wuhan 430070, China</aff>
    <aff id="af2-ijms-23-13469"><label>2</label>Institute of Economic Crops, Xinjiang Academy of Agricultural Sciences, Urumchi 830091, China</aff>
    <aff id="af3-ijms-23-13469"><label>3</label>Forestry and Fruit Tree Research Institute, Wuhan Academy of Agricultural Sciences, Wuhan 430075, China</aff>
    <author-notes>
      <corresp id="c1-ijms-23-13469"><label>*</label>Correspondence: <email>liqingyuan@webmail.hzau.edu.cn</email> (Q.L.); <email>lingmin@mail.hzau.edu.cn</email> (L.M.)</corresp>
      <fn id="fn1-ijms-23-13469">
        <label>†</label>
        <p>These authors contributed equally to this work.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>03</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <issue>21</issue>
    <elocation-id>13469</elocation-id>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>31</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 by the authors.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Pollen grains, the male gametophytes for reproduction in higher plants, are vulnerable to various stresses that lead to loss of viability and eventually crop yield. A conventional method for assessing pollen viability is manual counting after staining, which is laborious and hinders high-throughput screening. We developed an automatic detection tool (PollenDetect) to distinguish viable and nonviable pollen based on the YOLOv5 neural network, which is adjusted to adapt to the small target detection task. Compared with manual work, PollenDetect significantly reduced detection time (from approximately 3 min to 1 s for each image). Meanwhile, PollenDetect can maintain high detection accuracy. When PollenDetect was tested on cotton pollen viability, 99% accuracy was achieved. Furthermore, the results obtained using PollenDetect show that high temperature weakened cotton pollen viability, which is highly similar to the pollen viability results obtained using 2,3,5-triphenyltetrazolium formazan quantification. PollenDetect is an open-source software that can be further trained to count different types of pollen for research purposes. Thus, PollenDetect is a rapid and accurate system for recognizing pollen viability status, and is important for screening stress-resistant crop varieties for the identification of pollen viability and stress resistance genes during genetic breeding research.</p>
    </abstract>
    <kwd-group>
      <kwd>computer vision</kwd>
      <kwd>deep learning</kwd>
      <kwd>high temperature stress</kwd>
      <kwd>open-source</kwd>
      <kwd>pollen viability</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>National Natural Science Foundation of China</funding-source>
        <award-id>32072024</award-id>
      </award-group>
      <award-group>
        <funding-source>Xinjiang Academy of Agricultural Sciences</funding-source>
        <award-id>xjnkq-2019008</award-id>
      </award-group>
      <award-group>
        <funding-source>Fundamental Research Funds for the Central Universities</funding-source>
        <award-id>2021ZKPY019</award-id>
      </award-group>
      <award-group>
        <funding-source>Xinjiang Major Science and Technology Projects</funding-source>
        <award-id>2021A02001-4</award-id>
      </award-group>
      <award-group>
        <funding-source>Xinjiang Joint Research on the Breeding of Long-staple Cotton</funding-source>
        <award-id>2022-2-2</award-id>
      </award-group>
      <funding-statement>This research was funded by the National Natural Science Foundation of China (32072024), Sci-Tech Talents Projects of Xinjiang Academy of Agricultural Sciences (xjnkq-2019008), the Fundamental Research Funds for the Central Universities (2021ZKPY019), the Xinjiang Major Science and Technology Projects (2021A02001-4), and the Xinjiang Joint Research on the Breeding of Long-staple Cotton (2022-2-2).</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-ijms-23-13469">
    <title>1. Introduction</title>
    <p>Pollen is an important research subject in botany, climatology, ecology, and other fields. As the male gametophyte in the sexual reproductive stage of plants, pollen plays an important role throughout reproductive development in plants. The development of pollen, the combination of male and female gametophytes, and the germination and growth of the pollen tube are important processes that determine the reproductive quality of plants. Pollen is subjected to a variety of environmental stresses during its development, including, but not limited to, low temperature, high temperature, radiation, and drought. Some of these factors act directly on the pollen itself, while some act on stamens and even on the whole plant. For example, the processes of stamen development, pollen germination on the stigma, and pollen tube growth in cotton are very sensitive to temperature, and high temperature will lead to a decrease in pollen viability and hinder pollen fertilization and the development of cotton bolls, resulting in a decline in cotton yield [<xref rid="B1-ijms-23-13469" ref-type="bibr">1</xref>,<xref rid="B2-ijms-23-13469" ref-type="bibr">2</xref>,<xref rid="B3-ijms-23-13469" ref-type="bibr">3</xref>]. Botanists have worked diligently to identify genes that affect pollen viability, and have combined gene editing technologies to improve the resistance of plant pollen to adverse stresses, expand planting areas, and increase crop yield [<xref rid="B4-ijms-23-13469" ref-type="bibr">4</xref>,<xref rid="B5-ijms-23-13469" ref-type="bibr">5</xref>].</p>
    <p>Researchers have also explored the best observation and statistical methods for assessing pollen development. The traditional method for identifying the viability of pollen is manual observation through microscopy after staining, including bright- and darkfield microscopy, which can be combined with different pollen preparation methods, such as fresh pollen staining or acetolysis [<xref rid="B6-ijms-23-13469" ref-type="bibr">6</xref>]; the latter can also be used for fossil pollen [<xref rid="B7-ijms-23-13469" ref-type="bibr">7</xref>]. Thus, botanists who need to obtain large-scale (e.g., population-scale) data on pollen phenotypic status would need to invest a great deal of effort in much repetitive work to determine the phenotype of pollen by manual observation. It is obvious that the traditional pollen viability detection method is easily subject to the influence of tedious operation steps and subjective factors of manual operation. To improve the efficiency of genetics and breeding research, a fast pollen viability identification method with high accuracy is needed by researchers. With technological and equipment development, researchers have made great progress in automatic analysis and computer-aided observation of pollen, and many new pollen analysis methods have emerged, such as molecular methods: metabarcoding [<xref rid="B8-ijms-23-13469" ref-type="bibr">8</xref>], genome skimming [<xref rid="B9-ijms-23-13469" ref-type="bibr">9</xref>], and chemotaxonomy [<xref rid="B10-ijms-23-13469" ref-type="bibr">10</xref>]. Image detection technology based on deep learning convolutional neural networks (CNNs) and related machine learning technology, which have the advantages of high efficiency, accuracy, and rapidness, has the potential to advance pollen analysis.</p>
    <p>To date, researchers have made some exciting achievements in agriculture through deep learning techniques combined with molecular biology methods [<xref rid="B11-ijms-23-13469" ref-type="bibr">11</xref>]. For example, technicians developed the MicroScan system, which has been used to detect the pollen developmental stages of different eggplant varieties, and the detection accuracy has reached 86.3% [<xref rid="B12-ijms-23-13469" ref-type="bibr">12</xref>]. By combining this system with the high-throughput genotyping platform of single primer enrichment technology, haploid and diploid plants were identified with a high recognition rate by flow cytometry. Moreover, to detect apple growth period in real time and estimate apple yield, the DenseNet method was used to improve the YOLOv3 model to overcome the limitation of detection being applicable at specific growth stages, thus realizing the high-throughput and rapid detection of apples at different growth stages with the same model [<xref rid="B13-ijms-23-13469" ref-type="bibr">13</xref>]. To improve tea picking efficiency, the researchers trained a model through the YOLOv3 model. It can automatically locate the picking point of tea leaves, and the successful picking rate of tea leaves was 80% [<xref rid="B14-ijms-23-13469" ref-type="bibr">14</xref>]. In order to accurately predict the orange yield of an orchard, researchers trained the YOLO model to detect oranges under different lighting fields and constructed a yield estimation method. The spatial distribution error of the final fruit yield was only 9.19% [<xref rid="B15-ijms-23-13469" ref-type="bibr">15</xref>]. At the same time, nondestructive testing of mango fruits is also emerging with the help of deep learning models [<xref rid="B16-ijms-23-13469" ref-type="bibr">16</xref>]. At present, pollen classification techniques based on deep learning in plant science are booming, and can distinguish the pollen of 30 different crops in the same data set [<xref rid="B17-ijms-23-13469" ref-type="bibr">17</xref>]. However, the detection and classification of pollen viability using deep learning technology have not been considered in detail.</p>
    <p>In this study, we combined the PyTorch deep-learning-framework-based YOLOv5 algorithm and computer vision to develop a high-throughput automatic pollen viability recognition tool (PollenDetect). This tool can be used to automatically detect a large number of pollen grains from images for classification, counting, and pollen viability recording. We obtained a recognition accuracy of 99% when applying PollenDetect to a cotton pollen data set. The robustness of this tool for pollen viability detection in different plants (<italic toggle="yes">Arabidopsis</italic>, corn, camellia, and azalea) was evaluated. Finally, a visual interface for PollenDetect was created and released on GitHub (<uri xlink:href="https://github.com/Tanzhihao1998/Identification-of-pollen-activity.git">https://github.com/Tanzhihao1998/Identification-of-pollen-activity.git</uri>, accessed on 1 April 2022), which can be used for pollen viability detection and counting on computers with only a CPU by botanists without a computer science background.</p>
  </sec>
  <sec sec-type="results" id="sec2-ijms-23-13469">
    <title>2. Results</title>
    <sec id="sec2dot1-ijms-23-13469">
      <title>2.1. Pollen Viability Detection Algorithm Design and Training</title>
      <p>We used YOLOv5, Faster R-CNN (PaddlePaddle 1.7), and YOLOv3 deep learning models to construct three pollen viability detection systems that can detect pollen viability by computer instead of manual work. The construction process of each system is shown in <xref rid="ijms-23-13469-f001" ref-type="fig">Figure 1</xref>.</p>
      <p>The viable pollen was dark red and reddish brown after 2,3,5-triphenyl tetrazolium chloride (TTC) staining, while the nonviable pollen was light yellow or gray. Thus, we were able to distinguish the viable and nonviable pollen by color characteristics. Some scholars have shown that transfer training can enable the underlying CNN of an algorithm to form memories of shape, color, and other traits [<xref rid="B18-ijms-23-13469" ref-type="bibr">18</xref>]. Thus, in this study, algorithm training was performed with transfer training, and the results of the previous training of the algorithm were used to optimize the subsequent training of the algorithm. In this study, the initial weights of YOLOv5, Faster R-CNN, and YOLOv3 networks were obtained from the pre-trained models on the COCO data set, and the obtained initial weights were used for transfer training on the cotton pollen data set (as described in Materials and Methods <xref rid="sec4dot1-ijms-23-13469" ref-type="sec">Section 4.1</xref>, <xref rid="sec4dot4-ijms-23-13469" ref-type="sec">Section 4.4</xref> and <xref rid="sec4dot5-ijms-23-13469" ref-type="sec">Section 4.5</xref>). Through this method, the training time was reduced, the computer hardware requirements were reduced, and the accuracy was improved. The model parameters were initialized and adjusted according to the characteristics of the pollen detection task. The model training weight was saved every 20 epochs. When the model learning rate and loss value tended to be stable, model training was considered to be over, and the optimal weights of the three models on the verification set were saved (<xref rid="ijms-23-13469-f001" ref-type="fig">Figure 1</xref>).</p>
      <p>Then, the YOLOv5, Faster R-CNN, and YOLOv3 models were tested on an untrained data set to verify the accuracy and counting performance of each model. We used two indicators to evaluate the performance of each model: (1) the mean average accuracy (mAP@0.5:0.95), i.e., the position relationship between the predicted and real values of the label box, and (2) the correlation coefficient (<italic toggle="yes">R</italic><sup>2</sup>) used to evaluate the quantitative difference between the predicted value and the real value. In this study, we hoped to establish a pollen viability recognition model with high mAP and <italic toggle="yes">R</italic><sup>2</sup> values as the final tool system for plant researchers.</p>
    </sec>
    <sec id="sec2dot2-ijms-23-13469">
      <title>2.2. Comparison of the Accuracy of the Three Algorithms</title>
      <p>mAP@0.5:0.95 is a commonly used index for evaluating the quality of deep learning models. We used the optimal weights of the three models for prediction on the same verification set and compared the accuracy of the three models for pollen viability detection tasks. The difference in the mAP@0.5:0.95 values of the three optimal weights was obvious, showing a ladder distribution (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>A). The performance of the YOLOv5 model was the best (mAP@0.5:0.95 was 0.774), the mAP@0.5:0.95 of the YOLOv3 model was medium (0.627), and the mAP@0.5:0.95 of the Faster R-CNN model was the worst, at 0.533; however, this value is relatively high according to our experience (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>A). In summary, the three models showed high levels of accuracy, and could be embedded within prediction systems, in which the performances of the YOLOv5 and YOLOv3 models were better than that of the Faster R-CNN model.</p>
    </sec>
    <sec id="sec2dot3-ijms-23-13469">
      <title>2.3. Comparison of the Counting Performance of the Three Algorithms</title>
      <p>The accuracy of the counting of viable and nonviable pollen is critical to modeling. We used the optimal weight of the three models to predict the same test set, and compared the output with the real value. The results show that YOLOv5 was the most reliable model, with an <italic toggle="yes">R</italic><sup>2</sup> of 0.99 for both the ‘alive’ tag and the ‘dead’ tag. On the other hand, the Faster R-CNN model and YOLOv3 model were not satisfactory for the detection of ‘dead’ tags, with <italic toggle="yes">R</italic><sup>2</sup> values of only 0.15 and 0.11, respectively, while for ‘alive’ tags, the performance of the Faster R-CNN model reached a high level (<italic toggle="yes">R</italic><sup>2</sup> = 0.77), but the <italic toggle="yes">R</italic><sup>2</sup> of the YOLOv3 model was only 0.49 (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>B). The results show that the optimal weights of the three models differed greatly for counting performance in pollen viability detection tasks (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>B). YOLOv5 and YOLOv3 were then used to detect the same pollen staining picture, and the results are shown in <xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>C,D. It is obvious that for pollen from the same area in the image, YOLOv5 accurately detected all the samples and marked the correct categories (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>E), while YOLOv3 ignored many pollen grains (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>F). In summary, both the YOLOv5 and Faster R-CNN models performed well in terms of accuracy for the recognition of ‘alive’ tags, but for the detection of ‘dead’ tags, the YOLOv5 model showed a great advantage over the other two models.</p>
    </sec>
    <sec id="sec2dot4-ijms-23-13469">
      <title>2.4. Transfer of the Detection Capability of PollenDetect to Pollen from Different Plants</title>
      <p>As discussed above, the PollenDetect tool built on the basis of YOLOv5 was the optimal model for detecting cotton pollen viability (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>). To examine the knowledge transfer capability of YOLOv5 and assess whether the developed algorithm can be used on pollen of other plants, the PollenDetect tool was used to detect the pollen viability of other plants. We chose corn (monocotyledon), <italic toggle="yes">Arabidopsis</italic> (dicotyledonous plant), camellia, and azalea (ornamental flowers), which are different from cotton in terms of pollen to some extent. Cotton pollen has spines, while the outer layer of <italic toggle="yes">Arabidopsis</italic> and corn pollen is smooth. In addition, cotton and corn pollen is globoid, while <italic toggle="yes">Arabidopsis</italic> pollen is spindle-shaped, and the pollen of ornamental flowers is tetrahedral in shape. The pollen of the four kinds of plants (corn, <italic toggle="yes">Arabidopsis</italic>, camellia, and azalea) was stained with TTC using the same experimental procedure used for the cotton pollen data set, and the pollen staining images were photographed with a camera-connected microscope. We used PollenDetect to detect the pollen in 10 images of each plant, and compared the results with the results of manual counting, to evaluate the performance of the model in terms of pollen viability detection for different plants. The detection results show that the accuracy of PollenDetect for cotton, <italic toggle="yes">Arabidopsis</italic>, corn, camellia, and azalea pollen viability was 99%, 83%, 85%, 77%, and 64%, respectively (<xref rid="ijms-23-13469-f003" ref-type="fig">Figure 3</xref>).</p>
      <p>To solve the problem of low accuracy when applied to pollen viability counting of other plants, we further fine-tuned the YOLOv5 algorithm using a set of eight annotated azalea images for additional training and training for 200 epochs, and then tested 10 images. The fine-tuning improved the accuracy from 64% to 93%. For example, in the same pollen staining image of azalea, pollen that could not be detected before fine-tuning were detected after fine-tuning and correctly classified as viable or nonviable (<xref rid="ijms-23-13469-f003" ref-type="fig">Figure 3</xref>H,I,K,L). The above fine-tuning process took only 1 h. Thus, the knowledge transfer from cotton to other plants was successful and straightforward, as expected.</p>
    </sec>
    <sec id="sec2dot5-ijms-23-13469">
      <title>2.5. Detection of Cotton Pollen High-Temperature Tolerance via PollenDetect</title>
      <p>In our previous study, we explored the TTF quantification analysis of pollen viability in anthers under normal and HT conditions [<xref rid="B19-ijms-23-13469" ref-type="bibr">19</xref>] (Chinese patent number: ZL201910010240.2). To confirm the accuracy of PollenDetect and the correlation between TTC staining and TTF quantification results, three cotton lines with different tolerances to HT were selected for analysis. The HT-tolerant (HTT) cotton line, intermediately HT-tolerant cotton line (HTI), and HT-sensitive cotton line (HTS) were treated under normal temperature and HT conditions. For each line, TTC staining recognized by PollenDetect and TTF quantification were used to calculate pollen viability (<xref rid="ijms-23-13469-f004" ref-type="fig">Figure 4</xref>). TTC pollen staining recognition by PollenDetect showed that the pollen viability of the three cotton lines decreased to varying degrees after HT treatment: the pollen viability of HTT slightly decreased (<xref rid="ijms-23-13469-f004" ref-type="fig">Figure 4</xref>A,D,G), that of HTI and HTS both decreased significantly, and HTS pollen viability decreased more dramatically than that of HTI (<xref rid="ijms-23-13469-f004" ref-type="fig">Figure 4</xref>B,C,E–G), consistent with the results of TTF quantification (<xref rid="ijms-23-13469-f004" ref-type="fig">Figure 4</xref>H). We believe that PollenDetect can be used to detect pollen viability and screen stress-resistant plants with high pollen viability.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec3-ijms-23-13469">
    <title>3. Discussion</title>
    <p>The development of a high-throughput tool is of great significance for quickly evaluating pollen viability status and then screening and locating target genes in combination with genome-wide association technology. Our study explored an efficient and accurate method based on deep learning to detect viable and nonviable pollen, which can reduce the workload of researchers and significantly increase the yield of reliable quantitative data. This shows the great advantage of using deep learning in agricultural image processing.</p>
    <p>In a previous study, machine learning was used to detect plant pollen quality according to the grain and shape characteristics of pollen [<xref rid="B20-ijms-23-13469" ref-type="bibr">20</xref>]. Furthermore, pollen recognition based on deep learning mostly focuses on crop pollen detection and counting [<xref rid="B21-ijms-23-13469" ref-type="bibr">21</xref>], while in this study, PollenDetect could not only detect the number of pollen grains, but also accurately identify whether each pollen grain was viable. Because pollen exhibits a high contrast with a pure-color background, the largest indicator of whether the pollen is viable or nonviable is the difference in colors after staining. In this study, the pretraining model had excellent generalization performance in the detection of cotton pollen, and was used in training. After training three different deep learning networks (YOLOv3, YOLOv5, and Faster R-CNN), the YOLOv5 model was chosen because it had the best mAP@0.5:0.95 value (0.774) and the best recognition accuracy (99%) (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>A,B), while the YOLOv3 and Faster R-CNN models had lower detection accuracies for nonviable cotton pollen (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>A). The identification of viable and nonviable pollen grains is equally important, as both involve in the calculation of pollen viability rates. In addition, for many cotton pollen aggregates, probably due to their unclear boundaries and mutual occlusion, the YOLOv3 and Faster R-CNN models could not accurately detect all cotton pollen grains, while the YOLOv5 model could accurately detect occluded and overlapping targets (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>C,F) due to the addition of the NMS algorithm. Therefore, special attention should be given to this phenomenon when imaging and using PollenDetect software.</p>
    <p>In this study, we gave priority to the classification counting accuracy of the model rather than the general performance index mAP of the model. For counting performance regarding viable and nonviable cotton pollen, the accuracy of the model reached 99% (<xref rid="ijms-23-13469-f002" ref-type="fig">Figure 2</xref>B). We speculate that higher recognition accuracy can be achieved using a more streamlined network structure (YOLOv5s) and increasing the number of samples included in the training process; furthermore, the reasoning speed of the model can be accelerated at the same time.</p>
    <p>When PollenDetect is directly applied to the pollen of corn and <italic toggle="yes">Arabidopsis thaliana</italic>, only some parameters needed to be fine-tuned, no retraining was needed, and the detection accuracy of the model exceeded 83% (<xref rid="ijms-23-13469-f003" ref-type="fig">Figure 3</xref>M). In addition, we also used pollen from ornamental flowers (camellia and azalea) to test the detection effect of PollenDetect (<xref rid="ijms-23-13469-f003" ref-type="fig">Figure 3</xref>G,H,J,K). The detection performance of the model did not sharply decline with respect to whether the pollen grains had spines or whether the plants were monocotyledons or dicotyledons. To achieve the same detection accuracy observed for cotton pollen, one simply needs to expand the data set, and the new data set needs to include more kinds of plant pollen and images under different light conditions. After migration training, PollenDetect can constantly learn the characteristics of pollen of different plants and improve their detection. The refinement of the pre-existing YOLOv5 models using azalea images for an extra training phase (also called fine-tuning) yielded better results in terms of count accuracy (<xref rid="ijms-23-13469-f003" ref-type="fig">Figure 3</xref>N). Notably, during this generalization, we only used 18 pictures for fine-tuning (training and validation), compared to 74 pictures that were used for cotton.</p>
    <p>The use of PollenDetect on pictures of pollen from different laboratories allowed us to identify some of the limitations of the counting algorithm: (1) the light source was not uniform when the images were taken, resulting in different tones within each group of images, which affected the model’s recognition of whether the pollen was viable, and (2) regarding the cleanliness of the image background, when more impurities were mixed with the stained pollen, it was possible to identify some impurities as pollen, which affected the detection accuracy. The first point depends on the experimental setup, which could be overcome by fine-tuning our model using images from different laboratories. We are able to provide some guidance for overcoming the second issue. We provided a detailed description of handling pollen prior to taking photographs (see <xref rid="sec4-ijms-23-13469" ref-type="sec">Section 4</xref>) to reduce the number of impurities in the pollen batch. Alternatively, a new batch of pollen training images containing impurities is recommended before the use of PollenDetect to detect unclean pollen batches [<xref rid="B22-ijms-23-13469" ref-type="bibr">22</xref>].</p>
    <p>In this study, we verified the effect of high temperature on cotton pollen viability [<xref rid="B19-ijms-23-13469" ref-type="bibr">19</xref>,<xref rid="B23-ijms-23-13469" ref-type="bibr">23</xref>,<xref rid="B24-ijms-23-13469" ref-type="bibr">24</xref>] through PollenDetect software. We believe that changes in other environmental conditions will also cause changes in pollen activity, and researchers can use the software we provide to perform high-throughput pollen screening.</p>
  </sec>
  <sec id="sec4-ijms-23-13469">
    <title>4. Materials and Methods</title>
    <sec id="sec4dot1-ijms-23-13469">
      <title>4.1. Plant Materials and Growth Conditions</title>
      <p>The cotton germplasms were planted at the Breeders Base of the Institute of Economic Crops, Xinjiang Academy of Agricultural Sciences, Arvati Township, Korla (86.08° E, 41.68° N), and the cotton experimental field and greenhouse of Huazhong Agricultural University (114.35° E, 30.47° N). For cotton growth in the field, 23 °C to 32 °C during the day and 20 °C to 27 °C at night were set as normal temperature conditions (NT). The high-temperature (HT) treatment included temperatures above 35 °C during the day and above 27 °C at night. In the greenhouse, the plants were grown at 28 °C to 35 °C during the day and 20 °C to 27 °C at night as normal conditions. During HT treatment in the greenhouse, the plants were cultivated at 35 °C to 39 °C during the day and 29 °C to 31 °C at night. Single plants growing in flowerpots were maintained in the greenhouse. The soil was mixed with gravel and substrate at a 1:2 ratio.</p>
      <p><italic toggle="yes">Arabidopsis thaliana</italic> ecotype Columbia (<italic toggle="yes">Col-0</italic>) was grown in an incubator under long-day conditions (16 h light/8 h dark) with white fluorescent light at 20 °C and a relative humidity of 60%. Corn plants were planted in the experimental field of Huazhong Agricultural University in Wuhan, Hubei Province, China (114.35° E, 30.47° N). The row spacing of the corn was 90 cm, and the initial distance between corn plants in each row was 30 cm. The temperature conditions were the same as those for cotton planted in the experimental field of Huazhong Agricultural University. Camellia and azalea were planted in the Germplasm Resource Garden of the Forestry and Fruit Tree Research Institute, Wuhan Academy of Agricultural Sciences (114.24° E, 30.34° N).</p>
    </sec>
    <sec id="sec4dot2-ijms-23-13469">
      <title>4.2. TTC Staining</title>
      <p>Under NT, anthers were collected when the pollen was fully dispersed on the day of anthesis. Anthers from five flowers of each line were collected as a biological replicate. The anthers were placed in 0.1 Mol L-1 2,3,5-triphenyl tetrazolium chloride (TTC) solution (8 g TTC was added to 1 L phosphate buffer at 0.1 M and pH = 7; China patent number: ZL201910010240.2) for staining for 60 min. HT samples were collected after 3 d of HT treatment with the same processes. After TTC staining, pollen viability was observed with a Nikon SM25 stereomicroscope (Japan), and images were collected by a charge-coupled device (CCD). The viable pollen became red after staining, while the nonviable pollen was gray or unstained. A pipette was used to absorb 1 mL of the stained pollen and place it on a glass slide to make a slice. Three slices were made for each flower, and three evenly distributed visual fields were randomly selected for photography. To generate enough pollen samples for deep learning model training at the same developmental stage, all stained pollen grains were imaged (<xref rid="app1-ijms-23-13469" ref-type="app">Figure S1</xref>). A total of 74 usable images of stained pollen were collected, and a total of 8232 stained pollen samples were obtained, including 6068 viable cotton pollen samples and 2164 nonviable pollen samples. Pollen grains from all plant species were processed the same way.</p>
    </sec>
    <sec id="sec4dot3-ijms-23-13469">
      <title>4.3. TTF Quantitation</title>
      <p>Anthers were collected in 2 mL tubes with TTC solution (less than 1 mL), weighed, stained for 60 min, and then ground for 70 s at 60 Hz using a TissueLyser-192 (Shanghai Jingxin, Shanghai). Next, 700 µL ethyl acetate was added to the tubes and mixed well by vortexing. After centrifugation for 5 min at 12,000 rpm/min, 200 µL supernatant was transferred into a microtiter plate. Absorbance at 485 nm was measured using a multimode plate reader (Perkinelmer EnSpire, Turku, Finland), and the concentration of 2,3,5-triphenyl tetrazolium formazan (TTF, ng/g fresh weight) was calculated. Each line had three biological replicates, each replicate included five anthers, and each anther sample had three technological replicates.</p>
    </sec>
    <sec id="sec4dot4-ijms-23-13469">
      <title>4.4. Image Annotation</title>
      <p>The purpose of our study was to determine whether we could directly examine cotton pollen staining images to identify whether each pollen grain is viable through a deep learning model without any other tools or systems. According to our observation of the samples, the color difference of stained and unstained pollen was obvious in the RGB channel, and could be used to distinguish whether pollen is viable or nonviable. The viable cotton pollen was dark red and reddish brown after TTC staining, while the nonviable cotton pollen was light yellow or gray (<xref rid="app1-ijms-23-13469" ref-type="app">Figure S2</xref>); thus, we distinguished the viable and nonviable cotton pollen by color characteristics. In this experiment, Labellmg (version 1.8.4) software was used to label the cotton pollen staining images, and a minimum outer rectangle was applied for each pollen grain visible in the image. According to the pollen color after staining, viable pollen was labeled ‘alive’, while nonviable pollen was labeled ‘dead’, so that each pollen grain had an independent tag as ground truth that could be used in the deep learning model (<xref rid="app1-ijms-23-13469" ref-type="app">Figure S3</xref>).</p>
    </sec>
    <sec id="sec4dot5-ijms-23-13469">
      <title>4.5. Data Preprocessing</title>
      <p>The labeled cotton pollen images were randomly divided into a training set and a verification set at a ratio of 8:2 (<xref rid="app1-ijms-23-13469" ref-type="app">Figure S4</xref>). To prevent overprediction in certain species due to sample imbalance, the ratio of viable to nonviable cotton pollen in the data set was set as 3:1 for the tested species, and the weights of categories with fewer samples were increased to avoid errors in model training caused by the dominant number of sample categories in the data set.</p>
    </sec>
    <sec id="sec4dot6-ijms-23-13469">
      <title>4.6. YOLOv5 Detection Algorithm</title>
      <p>YOLOv5, released in June 2020, is the latest target detection framework in the YOLO series. It runs on a Tesla P100 and has an inference time of 0.07 s per image, which is half the time of YOLOv4. YOLOv5 has higher recognition accuracy and recognition speed than the previous generations. The YOLOv5 algorithm is especially suitable for portable devices that cannot carry a large GPU. Since the pollen size is relatively fixed, there is no need to use a large detection frame. Thus, the YOLOv5 deep learning algorithm was used for pollen recognition training, and the structure contained four aspects, i.e., input, backbone, neck, and prediction, which are shown in <xref rid="ijms-23-13469-f005" ref-type="fig">Figure 5</xref>.</p>
      <sec id="sec4dot6dot1-ijms-23-13469">
        <title>4.6.1. Input Side</title>
        <p>The YOLOv5 input side mainly includes mosaic data enhancement, adaptive anchor frame calculation, and adaptive image scaling. Mosaic data enhancement is one of the algorithms unique to YOLOv5 [<xref rid="B26-ijms-23-13469" ref-type="bibr">26</xref>]. The proportion of small targets is usually slightly lower in daily visual recognition data sets. To meet the need for small target recognition, four images were stitched together with random scaling, cropping, and arrangement. This approach makes the algorithm more sensitive to small targets, such as cotton pollen, and enriches their data sets.</p>
        <p>An adaptive anchor frame calculation was performed by setting different initial anchor frame lengths and widths. The algorithm generates possible initial prediction frames based on the initial anchor frames, compares them with the calibrated ground truth, calculates the differences between them, and then updates them in reverse, iterating the network parameters. The initial anchor frames set by YOLOv5 in this study are (116, 90, 156, 198, 373, 326) (30, 61, 62, 45, 59, 119) (10, 13, 16, 30, 23).</p>
        <p>Adaptive image scaling fills differently sized images with differently sized black edges at both ends so that they become standard-sized images. For this algorithm, the Letterbox function was used to fill both sides of an image with black edges to reduce computation and improve speed.</p>
      </sec>
      <sec id="sec4dot6dot2-ijms-23-13469">
        <title>4.6.2. Backbone</title>
        <p>The backbone is a CNN that aggregates and forms image features from different image minutiae, including the focus structure and cross-stage partial (CSP) structure [<xref rid="B25-ijms-23-13469" ref-type="bibr">25</xref>].</p>
        <p>Focus structure is a major innovation in YOLOv5 that does not appear in other versions of the YOLO series. The focus structure mainly performs an image slicing operation, as shown in <xref rid="app1-ijms-23-13469" ref-type="app">Figure S5</xref>. The slicing operation preserves image features while reducing image size by extracting image feature values, which speeds up the inference of the algorithm. For example, in our deep learning algorithm, after a pollen image of 608 × 608 × 3 pixels was input into the focus structure, a feature mean average precision (mAP) of 304 × 304 × 12 pixels was output, and then after one convolution operation with 32 convolution kernels, a feature mAP of 304 × 304 × 32 pixels was output.</p>
        <p>The main function of the CSP structure is to reduce the computation and memory cost while ensuring accuracy. The convolution kernel size of each CSP module at the backbone side is 3 × 3 with a step size of 2, which serves as a downsampling function by convolving the original image. A total of five CSP modules were used in this experimental algorithm. The size of the feature mAP changed from 608 × 608 to 19 × 19 after five iterations. The CSP structure was incorporated into both the backbone and neck ends of this algorithm.</p>
      </sec>
      <sec id="sec4dot6dot3-ijms-23-13469">
        <title>4.6.3. Neck</title>
        <p>A neck is a series of network layers that mix image features and pass them to a prediction layer. The neck side in this experiment included a path aggregation network, feature pyramid network (FPN), and pixel aggregation network (PAN) structure [<xref rid="B27-ijms-23-13469" ref-type="bibr">27</xref>], in addition to the CSP structure mentioned above. As shown in <xref rid="app1-ijms-23-13469" ref-type="app">Figure S5</xref>, the FPN structure delivers strong semantic features by downsampling from top to bottom, followed by two PAN structures that deliver strong localization features by downsampling from bottom to top, aggregating features from different backbone layers for different detection layer information to extract the most feature information. This not only preserves the image features, but also reduces the size of the data set that needs to be processed, speeds up the algorithm’s reasoning, and increases its recognition accuracy.</p>
      </sec>
      <sec id="sec4dot6dot4-ijms-23-13469">
        <title>4.6.4. Prediction</title>
        <p>The prediction side extracts the image features to generate bounding boxes and predict their categories. In this experiment, GIoU_Loss was used as the bounding box regression loss function, and weighted non-maximum suppression (NMS) was used to exclude redundant candidate boxes [<xref rid="B28-ijms-23-13469" ref-type="bibr">28</xref>,<xref rid="B29-ijms-23-13469" ref-type="bibr">29</xref>]. Since cotton pollen grain overlap with and obscure each other, using only the NMS algorithm resulted in the inability to accurately identify adjacent pollen grains, causing a few pollen images with the highest confidence to be retained. Therefore, we used the soft-NMS algorithm [<xref rid="B30-ijms-23-13469" ref-type="bibr">30</xref>]. This method can effectively prevent the omission of a certain target or repeated detection of a certain target when a single target position overlaps.</p>
      </sec>
    </sec>
    <sec id="sec4dot7-ijms-23-13469">
      <title>4.7. YOLOv3 Detection Algorithm Design</title>
      <p>YOLOv3 is an end-to-end one-stage target detection model. The basic idea of YOLOv3 algorithm prediction is as follows: first, the feature of the input image is extracted through the feature extraction network, and the feature mAP of a specific size is obtained and output. Next, the input image is divided into 13 × 13 network elements, and if the central coordinates of a category in the real box fall within a network element, then the network element predicts the category. Each category has a fixed number of bounding boxes, and there are three bounding boxes within each network unit in YOLOv3. Logical regression is used to determine the regression boxes and then to predict a category. The YOLOv3 model of this experiment adopted the Darknet-53 feature extraction network, which is a 53-layer full convolution network structure that pursues the accuracy of detection while maintaining a certain detection speed.</p>
    </sec>
    <sec id="sec4dot8-ijms-23-13469">
      <title>4.8. Faster R-CNN Detection Algorithm Design</title>
      <p>Faster R-CNN is a two-stage target detection model. The specific idea of Faster R-CNN model detection is as follows: first, the CNN network is used to extract the features of the input image, and the feature mAP of the image is obtained. Next, the candidate box is obtained from the feature mAP through the RPN network, the candidate box is classified (to determine whether it belongs to the foreground or the background), and the position relationship between the candidate box and the labeled real box is adjusted by the regression algorithm in the foreground to make them similar. This step is the most important difference between the two-stage detection model and the single-stage detection model. The candidate box obtained in the previous step is sent to multiple classifiers for classification to determine whether the selected area of the candidate box belongs to a specific category.</p>
    </sec>
    <sec id="sec4dot9-ijms-23-13469">
      <title>4.9. YOLOv5 Model Refinement</title>
      <p>After the 17th layer of the network structure, the original network structure is modified to continue the upsampling operation to expand the feature map, and at the 20th layer, the acquired 160 × 160 and the 2nd layer of the backbone network are used to obtain a larger feature map for small target detection. In the detection layer, a small target detection layer is added to use a total of four layers of structure (22, 25, 28, 31) for detection.</p>
    </sec>
    <sec id="sec4dot10-ijms-23-13469">
      <title>4.10. Model Evaluation Metrics</title>
      <p>The main evaluation metrics applied in this study were mAP@0.5:0.95 and accuracy. The formula is expressed as follows:
<disp-formula id="FD1-ijms-23-13469"><mml:math id="mm1" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2-ijms-23-13469"><mml:math id="mm2" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mfenced><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3-ijms-23-13469"><mml:math id="mm3" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>accuracy</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4-ijms-23-13469"><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:msubsup><mml:mi>P</mml:mi><mml:mfenced><mml:mi>R</mml:mi></mml:mfenced><mml:mi>d</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>In the above formula, <italic toggle="yes">T<sub>P</sub></italic> is the number of instances that are actually positive instances and classified by the classifier as positive instances; <italic toggle="yes">F<sub>P</sub></italic> is the number of instances that are actually negative instances and classified by the classifier as positive instances; <italic toggle="yes">F<sub>N</sub></italic> is the number of instances that are actually positive instances and classified by the classifier as negative instances; <italic toggle="yes">T<sub>N</sub></italic> is the number of instances that are actually negative instances and classified by the classifier as negative instances; mAP is the average of the average precision (AP) of all categories. mAP@0.5:0.95 is the process of increasing intersection over union (IoU) from 0.5 to 0.95 with steps of 0.05. The mAP corresponding to each IoU is added to obtain the average value of mAP in this process.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec5-ijms-23-13469">
    <title>5. Conclusions</title>
    <p>In this study, we developed and validated software, PollenDetect, as a high-throughput and efficient tool for pollen viability detection (viable or nonviable) and counting. For the cotton pollen data set, the mAP@0.5:0.95 of the model was 0.774, the accuracy was 99%, and the correlation <italic toggle="yes">R</italic><sup>2</sup> with manual calibration results was 0.99. This is the first study to apply a deep learning target detection model to the high-throughput analysis of pollen viability. This model can replace manual counting or quantification to determine the viability of large quantities of pollen. The obtained pollen viability data can be used to identify the target genes that affect the response of pollen to high temperature and other stresses. For the convenience of other users, PollenDetect was embedded into a user-friendly interface (<xref rid="app1-ijms-23-13469" ref-type="app">Figure S6</xref>) and uploaded to GitHub (<uri xlink:href="https://github.com/Tanzhihao1998/Identification-of-pollen-activity.git">https://github.com/Tanzhihao1998/Identification-of-pollen-activity.git</uri>, accessed on 1 April 2022), where it can be downloaded and freely used by all internet users. The software allows users to integrate pollen staining images into the same folder for rapid batch detection. The test results are output for further statistical analysis. Our output contains not only the total number of pollen grains, but also the number of viable and nonviable pollen grains in the same image.</p>
    <sec>
      <title>Limitations and Future Work</title>
      <p>We demonstrated the possibility of applying the model to the pollen of other plants (<italic toggle="yes">Arabidopsis</italic>, corn, and ornamental flowers). Thus, fine-tuning some of the parameters and training the model can allow it to be applied in pollen viability detection modules for other plants. However, the current precision of pollen detection for other crops needs to be further improved. In view of the transferability of knowledge, we will continue to add pollen detection modules of other plants to this model in the future. In addition to this, other phenotypes of pollen should also be detected, e.g., volume size of pollen, circumference of pollen. We intend build an integrated model for the detection of multiple phenotypes of pollen in the future.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>Appreciations are given to the editors and reviewer.</p>
  </ack>
  <fn-group>
    <fn>
      <p><bold>Publisher’s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <app-group>
    <app id="app1-ijms-23-13469">
      <title>Supplementary Materials</title>
      <p>The following supporting information can be downloaded at: <uri xlink:href="https://www.mdpi.com/article/10.3390/ijms232113469/s1">https://www.mdpi.com/article/10.3390/ijms232113469/s1</uri>. Figure S1: Scheme of how the pollen image was obtained using a microscope. The images were taken, generating overlapping areas to obtain a macro image without blemishes in these areas. Figure S2: Close-up of cotton pollen. (A,B): Inviable cotton pollen grains. (C,D): Viable cotton pollen grains. Bars in (A–D): 10 μm. Figure S3: Graphic environment of the manual labeling software used for the classification of the pollen. In the central part, it can be seen how the pollen grains have been labeled by an expert and classified according to their status. Figure S4: Data set composition. The data set contained 74 pollen images, and the ratio of the training set to the validation set was 8:2. Figure S5: Focus schematic and FPN + PAN structure. (A): The original image inputted by the model was 4 ∗ 4 ∗ 3, the eigenvalues of the image were extracted by the focus structure, and the 2 ∗ 2 ∗ 12 image was output. This structure can effectively reduce the size of the image, speed up the reasoning of the model, and maintain the original features of the image. (B): Pollen images output by the FPN+PAN structure with 19 ∗ 19, 38 ∗ 38, and 76 ∗ 76 feature mAPs corresponding to different sizes of anchor boxes were used for prediction. FPN: feature pyramid network; mAP: mean average precision; PAN: path aggregation network. Figure S6: PollenDetect software interface. The software can select a single image or a group of images for pollen viability detection, and the test results will be counted and output to the text box for further statistics and analysis.</p>
      <supplementary-material id="ijms-23-13469-s001" position="float" content-type="local-data">
        <media xlink:href="ijms-23-13469-s001.zip">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
    </app>
  </app-group>
  <notes>
    <title>Author Contributions</title>
    <p>Z.T. made data sets and trained models, J.Y., Q.L., F.S. and T.Y. carried out the experiments, W.W., A.A., J.K., J.Y. and Q.L. planted the plants in the field, Z.T., Q.L. and L.M. wrote the main manuscript, L.M., X.Z., W.Y., J.K. and Q.L. designed and supervised the research and revised the manuscript, L.M. agrees to serve as the author responsible for contact and ensures communication. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Institutional Review Board Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Informed Consent Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability Statement</title>
    <p>Details and source code of the PollenDetect model used in this study are openly available at <uri xlink:href="https://github.com/Tanzhihao1998/Identification-of-pollen-activity.git/">https://github.com/Tanzhihao1998/Identification-of-pollen-activity.git/</uri> (accessed on 1 April 2022).</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <glossary>
    <title>Abbreviations</title>
    <array>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CNNs</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">convolutional neural networks</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">R-CNN</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">region-CNN</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">CSP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">cross-stage partial</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">mAP</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">mean average precision</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">FPN</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">feature pyramid network</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">PAN</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">pixel aggregation network</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">NMS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">non-maximum suppression</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">HT</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">high temperature</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">HTT</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HT-tolerant</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">HTI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">intermediately HT-tolerant</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">HTS</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">HT-sensitive</td>
        </tr>
      </tbody>
    </array>
  </glossary>
  <ref-list>
    <title>References</title>
    <ref id="B1-ijms-23-13469">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>The effects of high-temperature stress on the germination of pollen grains of upland cotton during square development</article-title>
        <source>Euphytica</source>
        <year>2014</year>
        <volume>200</volume>
        <fpage>175</fpage>
        <lpage>186</lpage>
        <pub-id pub-id-type="doi">10.1007/s10681-014-1141-1</pub-id>
      </element-citation>
    </ref>
    <ref id="B2-ijms-23-13469">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Min</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Sugar and Auxin Signaling Pathways Respond to High-Temperature Stress during Anther Development as Revealed by Transcript Profiling Analysis in Cotton</article-title>
        <source>Plant Physiol.</source>
        <year>2014</year>
        <volume>164</volume>
        <fpage>1293</fpage>
        <lpage>1308</lpage>
        <pub-id pub-id-type="doi">10.1104/pp.113.232314</pub-id>
        <?supplied-pmid 24481135?>
        <pub-id pub-id-type="pmid">24481135</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-ijms-23-13469">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>High Temperature Reduces the Viability of Pollen from Upland Cotton in China</article-title>
        <source>Agron. J.</source>
        <year>2019</year>
        <volume>111</volume>
        <fpage>3039</fpage>
        <lpage>3047</lpage>
        <pub-id pub-id-type="doi">10.2134/agronj2019.03.0150</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-ijms-23-13469">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J.K.</given-names>
          </name>
        </person-group>
        <article-title>Abiotic stress responses in plants</article-title>
        <source>Nat. Rev. Genet.</source>
        <year>2022</year>
        <volume>23</volume>
        <fpage>104</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="doi">10.1038/s41576-021-00413-0</pub-id>
        <?supplied-pmid 34561623?>
        <pub-id pub-id-type="pmid">34561623</pub-id>
      </element-citation>
    </ref>
    <ref id="B5-ijms-23-13469">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Molecular Regulation of Plant Responses to Environmental Temperatures</article-title>
        <source>Mol. Plant</source>
        <year>2020</year>
        <volume>13</volume>
        <fpage>544</fpage>
        <lpage>564</lpage>
        <pub-id pub-id-type="doi">10.1016/j.molp.2020.02.004</pub-id>
        <?supplied-pmid 32068158?>
        <pub-id pub-id-type="pmid">32068158</pub-id>
      </element-citation>
    </ref>
    <ref id="B6-ijms-23-13469">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jolls</surname>
            <given-names>C.L.</given-names>
          </name>
        </person-group>
        <article-title>Techniques for Pollination Biologists</article-title>
        <source>BioScience</source>
        <year>1994</year>
        <volume>44</volume>
        <fpage>366</fpage>
        <lpage>368</lpage>
        <pub-id pub-id-type="doi">10.2307/1312388</pub-id>
      </element-citation>
    </ref>
    <ref id="B7-ijms-23-13469">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bourel</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Marchant</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>de Garidel-Thoron</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tetard</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Barboni</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Gally</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Beaufort</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Automated recognition by multiple convolutional neural networks of modern, fossil, intact and damaged pollen grains</article-title>
        <source>Comput. Geosci.</source>
        <year>2020</year>
        <volume>140</volume>
        <fpage>104498</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cageo.2020.104498</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-ijms-23-13469">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keller</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Danner</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Grimmer</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Ankenbrand</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>von der Ohe</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>von der Ohe</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Härtel</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Steffan-Dewenter</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Evaluating multiplexed next-generation sequencing as a method in palynology for mixed pollen samples</article-title>
        <source>Plant Biol.</source>
        <year>2014</year>
        <volume>17</volume>
        <fpage>558</fpage>
        <lpage>566</lpage>
        <pub-id pub-id-type="doi">10.1111/plb.12251</pub-id>
        <pub-id pub-id-type="pmid">25270225</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-ijms-23-13469">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Genome-skimming provides accurate quantification for pollen mixtures</article-title>
        <source>Mol. Ecol. Resour.</source>
        <year>2019</year>
        <volume>19</volume>
        <fpage>1433</fpage>
        <lpage>1446</lpage>
        <pub-id pub-id-type="doi">10.1111/1755-0998.13061</pub-id>
        <pub-id pub-id-type="pmid">31325909</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-ijms-23-13469">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jardine</surname>
            <given-names>P.E.</given-names>
          </name>
          <name>
            <surname>Gosling</surname>
            <given-names>W.D.</given-names>
          </name>
          <name>
            <surname>Lomax</surname>
            <given-names>B.H.</given-names>
          </name>
          <name>
            <surname>Julier</surname>
            <given-names>A.C.M.</given-names>
          </name>
          <name>
            <surname>Fraser</surname>
            <given-names>W.T.</given-names>
          </name>
        </person-group>
        <article-title>Chemotaxonomy of domesticated grasses: A pathway to understanding the origins of agriculture</article-title>
        <source>J. Micropalaeontol.</source>
        <year>2019</year>
        <volume>38</volume>
        <fpage>83</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.5194/jm-38-83-2019</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-ijms-23-13469">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kamilaris</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Prenafeta-Boldu</surname>
            <given-names>F.X.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in agriculture: A survey</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2018</year>
        <volume>147</volume>
        <fpage>70</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2018.02.016</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-ijms-23-13469">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>García-Fortea</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>García-Pérez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gimeno-Páez</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Sánchez-Gimeno</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Vilanova</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Prohens</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Pastor-Calle</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>A Deep Learning-Based System (Microscan) for the Identification of Pollen Development Stages and Its Application to Obtaining Doubled Haploid Lines in Eggplant</article-title>
        <source>Biology</source>
        <year>2020</year>
        <volume>9</volume>
        <elocation-id>272</elocation-id>
        <pub-id pub-id-type="doi">10.3390/biology9090272</pub-id>
        <?supplied-pmid 32899465?>
        <pub-id pub-id-type="pmid">32899465</pub-id>
      </element-citation>
    </ref>
    <ref id="B13-ijms-23-13469">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tian</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Apple detection during different growth stages in orchards using the improved YOLO-V3 model</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2019</year>
        <volume>157</volume>
        <fpage>417</fpage>
        <lpage>426</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2019.01.012</pub-id>
      </element-citation>
    </ref>
    <ref id="B14-ijms-23-13469">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>A YOLOv3-based computer vision system for identification of tea buds and the picking point</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2022</year>
        <volume>198</volume>
        <fpage>107116</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2022.107116</pub-id>
      </element-citation>
    </ref>
    <ref id="B15-ijms-23-13469">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mirhaji</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Mirhaji</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Soleymani</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Soleymani</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Asakereh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Asakereh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mehdizadeh</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Mehdizadeh</surname>
            <given-names>S.A.</given-names>
          </name>
        </person-group>
        <article-title>Fruit detection and load estimation of an orange orchard using the YOLO models through simple approaches in different imaging and illumination conditions</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2021</year>
        <volume>191</volume>
        <fpage>106533</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2021.106533</pub-id>
      </element-citation>
    </ref>
    <ref id="B16-ijms-23-13469">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Yamaguchi</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>An attribution-based pruning method for real-time mango detection with YOLO network</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2020</year>
        <volume>169</volume>
        <fpage>106533</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2020.105214</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-ijms-23-13469">
      <label>17.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Daood</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ribeiro</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bush</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Pollen Grain Recognition Using Deep Learning</article-title>
        <source>Proceedings of the International Symposium on Visual Computing</source>
        <conf-loc>Virtual</conf-loc>
        <conf-date>12 December 2016</conf-date>
        <fpage>321</fpage>
        <lpage>330</lpage>
      </element-citation>
    </ref>
    <ref id="B18-ijms-23-13469">
      <label>18.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bodla</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Chellappa</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>L.S.</given-names>
          </name>
        </person-group>
        <article-title>Soft-NMS—Improving Object Detection With One Line of Code</article-title>
        <source>Proceedings of the International Conference on Computer Vision (Iccv)</source>
        <conf-loc>Venice, Italy</conf-loc>
        <conf-date>22–29 October 2017</conf-date>
        <fpage>5562</fpage>
        <lpage>5570</lpage>
        <pub-id pub-id-type="doi">10.1109/Iccv.2017.593</pub-id>
      </element-citation>
    </ref>
    <ref id="B19-ijms-23-13469">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Min</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>A combination of genome-wide and transcriptome-wide association studies reveals genetic elements leading to male sterility during high temperature stress in cotton</article-title>
        <source>New Phytol.</source>
        <year>2021</year>
        <volume>231</volume>
        <fpage>165</fpage>
        <lpage>181</lpage>
        <pub-id pub-id-type="doi">10.1111/nph.17325</pub-id>
        <pub-id pub-id-type="pmid">33665819</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-ijms-23-13469">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Schultz</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Burkhardt</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Automated pollen recognition using 3D volume images from fluorescence microscopy</article-title>
        <source>Aerobiologia</source>
        <year>2002</year>
        <volume>18</volume>
        <fpage>107</fpage>
        <lpage>115</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1020623724584</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-ijms-23-13469">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gallardo-Caballero</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>García-Orellana</surname>
            <given-names>C.J.</given-names>
          </name>
          <name>
            <surname>García-Manso</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>González-Velasco</surname>
            <given-names>H.M.</given-names>
          </name>
          <name>
            <surname>Tormo-Molina</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Macías-Macías</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Precise Pollen Grain Detection in Bright Field Microscopy Using Deep Learning Techniques</article-title>
        <source>Sensors</source>
        <year>2019</year>
        <volume>19</volume>
        <elocation-id>3583</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s19163583</pub-id>
      </element-citation>
    </ref>
    <ref id="B22-ijms-23-13469">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Masteling</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Voorhoeve</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ijsselmuiden</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Dini-Andreote</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Boer</surname>
            <given-names>W.D.</given-names>
          </name>
          <name>
            <surname>Raaijmakers</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>DiSCount: Computer vision for automated quantification of Striga seed germination</article-title>
        <source>Plant Methods</source>
        <year>2020</year>
        <volume>16</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1186/s13007-020-00602-8</pub-id>
        <pub-id pub-id-type="pmid">31911810</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-ijms-23-13469">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>High temperature induces male sterility via MYB66–MYB4–Casein kinase I signaling in cotton</article-title>
        <source>Plant Physiol.</source>
        <year>2022</year>
        <volume>189</volume>
        <fpage>2091</fpage>
        <lpage>2109</lpage>
        <pub-id pub-id-type="doi">10.1093/plphys/kiac213</pub-id>
        <?supplied-pmid 35522025?>
        <pub-id pub-id-type="pmid">35522025</pub-id>
      </element-citation>
    </ref>
    <ref id="B24-ijms-23-13469">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>Y.Z.</given-names>
          </name>
          <name>
            <surname>Min</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C.Z.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y.L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.Y.</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>Q.D.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Y.L.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>Y.H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Disrupted genome methylation in response to high temperature has distinct affects on microspore abortion and anther indehiscence</article-title>
        <source>Plant Cell</source>
        <year>2018</year>
        <volume>30</volume>
        <fpage>1387</fpage>
        <lpage>1403</lpage>
        <pub-id pub-id-type="doi">10.1105/tpc.18.00074</pub-id>
        <?supplied-pmid 29866646?>
        <pub-id pub-id-type="pmid">29866646</pub-id>
      </element-citation>
    </ref>
    <ref id="B25-ijms-23-13469">
      <label>25.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>C.Y.</given-names>
          </name>
          <name>
            <surname>Bochkovskiy</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>H.Y.M.</given-names>
          </name>
        </person-group>
        <article-title>Scaled-YOLOv4: Scaling Cross Stage Partial Network</article-title>
        <source>Proceedings of the IEEE/Cvf Conference on Computer Vision and Pattern Recognition</source>
        <conf-loc>Nashville, TN, USA</conf-loc>
        <conf-date>20–25 June 2021</conf-date>
        <fpage>13024</fpage>
        <lpage>13033</lpage>
        <pub-id pub-id-type="doi">10.1109/Cvpr46437.2021.01283</pub-id>
      </element-citation>
    </ref>
    <ref id="B26-ijms-23-13469">
      <label>26.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Fawzi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Samulowitz</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Turaga</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Frossard</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Adaptive data augmentation for image classification</article-title>
        <source>Proceedings of the IEEE International Conference on Image Processing</source>
        <conf-loc>Phoenix, AZ, USA</conf-loc>
        <conf-date>25–28 September 2016</conf-date>
        <fpage>3688</fpage>
        <lpage>3692</lpage>
        <pub-id pub-id-type="doi">10.1109/ICIP.2016.7533048</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-ijms-23-13469">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.J.</given-names>
          </name>
        </person-group>
        <article-title>YOLOV3 traffic sign recognition and detection based on FPN improvement</article-title>
        <source>Sci. J. Intell. Syst. Res.</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>11</fpage>
      </element-citation>
    </ref>
    <ref id="B28-ijms-23-13469">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>J.Q.</given-names>
          </name>
          <name>
            <surname>Shao</surname>
            <given-names>W.Y.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>Y.B.</given-names>
          </name>
          <name>
            <surname>Xue</surname>
            <given-names>X.Y.</given-names>
          </name>
        </person-group>
        <article-title>Arbitrary-Oriented Scene Text Detection via Rotation Proposals</article-title>
        <source>IEEE Trans. Multimed.</source>
        <year>2018</year>
        <volume>20</volume>
        <fpage>3111</fpage>
        <lpage>3122</lpage>
        <pub-id pub-id-type="doi">10.1109/TMM.2018.2818020</pub-id>
      </element-citation>
    </ref>
    <ref id="B29-ijms-23-13469">
      <label>29.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>Z.H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.Z.</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>D.W.</given-names>
          </name>
        </person-group>
        <article-title>Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression</article-title>
        <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
        <conf-loc>New York, NY, USA</conf-loc>
        <conf-date>7–12 February 2020</conf-date>
        <volume>Volume 34</volume>
        <fpage>12993</fpage>
        <lpage>13000</lpage>
      </element-citation>
    </ref>
    <ref id="B30-ijms-23-13469">
      <label>30.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sharif Razavian</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Azizpour</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Sullivan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Carlsson</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>CNN features off-the-shelf: An astounding baseline for recognition</article-title>
        <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</source>
        <conf-loc>Columbus, OH, USA</conf-loc>
        <conf-date>23–28 June 2014</conf-date>
        <fpage>806</fpage>
        <lpage>813</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="ijms-23-13469-f001">
    <label>Figure 1</label>
    <caption>
      <p>Pipeline for algorithm development. (1) The pollen was placed in TTC solution for 1 h. (2) The supernatant was absorbed and placed on a glass slide. (3) The cotton pollen image was taken under a binocular microscope with a camera. (4) The captured pollen image was cleaned and the unclear images were removed. (5) The cleaned pollen image was marked with image marking software to distinguish viable and nonviable pollen. (6) Pollen images and tagging files were fed into the deep learning model for training. (7) A set of new images was sent to the trained deep learning model for detection to evaluate model performance and select the best model. (8) Each pollen grain was marked with its category in the detected image.</p>
    </caption>
    <graphic xlink:href="ijms-23-13469-g001" position="float"/>
  </fig>
  <fig position="float" id="ijms-23-13469-f002">
    <label>Figure 2</label>
    <caption>
      <p>Performance evaluation of the pollen viability detection model based on deep learning. (<bold>A</bold>) The YOLOv5, Faster R-CNN, and YOLOv3 models were used to detect pollen in 16 images, and the position accuracy of the different models was judged by comparing the gap between the predicted frame position and the manually marked boundary box position. (<bold>B</bold>) Sixteen images were detected by the YOLOv5, Faster R-CNN, and YOLOv3 models, and the counting accuracy of the different models was judged by analyzing the correlation with the actual values (<italic toggle="yes">R</italic><sup>2</sup>). (<bold>C</bold>,<bold>D</bold>) The visualization results of the same cotton pollen image obtained by the YOLOv5 model and YOLOv3 model, respectively. Bars: 100 µm. (<bold>E</bold>,<bold>F</bold>) An enlarged image of the box in (<bold>C</bold>,<bold>D</bold>), respectively.</p>
    </caption>
    <graphic xlink:href="ijms-23-13469-g002" position="float"/>
  </fig>
  <fig position="float" id="ijms-23-13469-f003">
    <label>Figure 3</label>
    <caption>
      <p>Direct extension of the trained model to the pollen of other plants. (<bold>A</bold>–<bold>C</bold>,<bold>G</bold>–<bold>I</bold>) The input image of pollen stained with TTC solution for cotton (<bold>A</bold>), corn (<bold>B</bold>), <italic toggle="yes">Arabidopsis</italic> (<bold>C</bold>), camellia (<bold>G</bold>), azalea ((<bold>H</bold>) before fine-tuning), and azalea ((<bold>I</bold>) after fine-tuning). (<bold>D</bold>–<bold>F</bold>,<bold>J</bold>,<bold>K</bold>) The training algorithm was used to visualize the results of pollen staining image detection for cotton (<bold>D</bold>), corn (<bold>E</bold>), <italic toggle="yes">Arabidopsis</italic> (<bold>F</bold>), camellia (<bold>J</bold>), and azalea ((<bold>K</bold>), before fine-tuning). (<bold>L</bold>) The training algorithm after fine-tuning was used to visualize the results of azalea pollen staining image detection. (<bold>M</bold>) The results of model detection were compared with those of manual labeling, and the average accuracy of model detection was calculated. (<bold>N</bold>) Before and after fine-tuning, the detection results for azalea were compared with those of manual labeling, and the average accuracy of model detection was calculated. (<bold>A</bold>–<bold>L</bold>) Bars: 100 µm.</p>
    </caption>
    <graphic xlink:href="ijms-23-13469-g003" position="float"/>
  </fig>
  <fig position="float" id="ijms-23-13469-f004">
    <label>Figure 4</label>
    <caption>
      <p>Pollen viability analysis via PollenDetect recognition of TTC staining images corresponding with the TTF quantitative analysis results. (<bold>A</bold>–<bold>C</bold>) Pollen TTC staining images of HTT (<bold>A</bold>), HTI (<bold>B</bold>), and HTS (<bold>C</bold>) cotton lines under NT conditions. Bars: 100 µm. (<bold>D</bold>–<bold>F</bold>) Pollen TTC staining images of HTT (<bold>D</bold>), HTI (<bold>E</bold>), and HTS (<bold>F</bold>) cotton lines under HT conditions. Bars: 100 µm. (<bold>G</bold>,<bold>H</bold>) TTC staining and TTF quantitative methods were used to calculate the pollen viability of cotton varieties with different HT tolerances. Values are the mean ± s.d. for three biological replicates. Asterisks indicate statistically significant differences (* <italic toggle="yes">p</italic> &lt; 0.05; *** <italic toggle="yes">p</italic> &lt; 0.001, Student’s <italic toggle="yes">t</italic>-test). HTT, HT-tolerant; HTI, intermediately HT-tolerant; HTS, HT-sensitive; NT, normal temperature; HT, high temperature; TTC, 2,3,5-triphenyl tetrazolium chloride; TTF, 2,3,5-triphenyltetrazolium formazan.</p>
    </caption>
    <graphic xlink:href="ijms-23-13469-g004" position="float"/>
  </fig>
  <fig position="float" id="ijms-23-13469-f005">
    <label>Figure 5</label>
    <caption>
      <p>YOLOv5 network structure. The YOLOv5 network includes the input side, backbone side, neck side, and prediction side [<xref rid="B25-ijms-23-13469" ref-type="bibr">25</xref>]. The input side is mainly responsible for mosaic data enhancement and adaptive anchor frame calculation. The backbone side has a focus structure and CSP structure. The neck side is made up of the main part of the FPN + PAN structure. The prediction side uses GIOU_LOSS to output image detection results. CSP, cross-stage partial; SSP, spatial pyramid pooling; BN, batch normalization; CONV, convolution; GIOU_LOSS, generalized intersection over union loss.</p>
    </caption>
    <graphic xlink:href="ijms-23-13469-g005" position="float"/>
  </fig>
</floats-group>
