<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9909867</article-id>
    <article-id pub-id-type="pmid">36755242</article-id>
    <article-id pub-id-type="publisher-id">5164</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-023-05164-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>pLMSNOSite: an ensemble-based approach for predicting protein S-nitrosylation sites by integrating supervised word embedding and embedding from pre-trained protein language model</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Pratyush</surname>
          <given-names>Pawel</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pokharel</surname>
          <given-names>Suresh</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Saigo</surname>
          <given-names>Hiroto</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>KC</surname>
          <given-names>Dukka B.</given-names>
        </name>
        <address>
          <email>dbkc@mtu.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.259979.9</institution-id><institution-id institution-id-type="ISNI">0000 0001 0663 5937</institution-id><institution>Department of Computer Science, </institution><institution>Michigan Technological University, </institution></institution-wrap>Houghton, MI USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.177174.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2242 4849</institution-id><institution>Department of Electrical Engineering and Computer Science, </institution><institution>Kyushu University, </institution></institution-wrap>744, Motooka, Nishi-Ku, 819-0395 Japan </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>8</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>8</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <volume>24</volume>
    <elocation-id>41</elocation-id>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2023</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Protein S-nitrosylation (SNO) plays a key role in transferring nitric oxide-mediated signals in both animals and plants and has emerged as an important mechanism for regulating protein functions and cell signaling of all main classes of protein. It is involved in several biological processes including immune response, protein stability, transcription regulation, post translational regulation, DNA damage repair, redox regulation, and is an emerging paradigm of redox signaling for protection against oxidative stress. The development of robust computational tools to predict protein SNO sites would contribute to further interpretation of the pathological and physiological mechanisms of SNO.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">Using an intermediate fusion-based stacked generalization approach, we integrated embeddings from supervised embedding layer and contextualized protein language model (ProtT5) and developed a tool called pLMSNOSite (protein language model-based SNO site predictor). On an independent test set of experimentally identified SNO sites, pLMSNOSite achieved values of 0.340, 0.735 and 0.773 for MCC, sensitivity and specificity respectively. These results show that pLMSNOSite performs better than the compared approaches for the prediction of S-nitrosylation sites.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">Together, the experimental results suggest that pLMSNOSite achieves significant improvement in the prediction performance of S-nitrosylation sites and represents a robust computational approach for predicting protein S-nitrosylation sites. pLMSNOSite could be a useful resource for further elucidation of SNO and is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/KCLabMTU/pLMSNOSite">https://github.com/KCLabMTU/pLMSNOSite</ext-link>.</p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-023-05164-9.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>S-nitrosylation</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>Post-translational modification</kwd>
      <kwd>Word embedding</kwd>
      <kwd>Protein language model</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000076</institution-id>
            <institution>Directorate for Biological Sciences</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1901793</award-id>
        <award-id>1901793</award-id>
        <award-id>1901793</award-id>
        <principal-award-recipient>
          <name>
            <surname>Pratyush</surname>
            <given-names>Pawel</given-names>
          </name>
          <name>
            <surname>Pokharel</surname>
            <given-names>Suresh</given-names>
          </name>
          <name>
            <surname>KC</surname>
            <given-names>Dukka B.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par23">Nitric oxide (NO) is a highly reactive molecule, and abnormal NO levels in mammalian cells are associated with multiple human diseases, including cancer [<xref ref-type="bibr" rid="CR1">1</xref>]. The role of NO as a major regulator of physiological function has become increasingly evident. S-nitrosylation (SNO) is one of the most important regulatory mechanisms of this vital signaling molecule. In S-nitrosylation, the NO is covalently attached to the thiol side chain of cysteine residues to form S-nitrosothiol (SN), a critical mechanism of transferring NO-mediated signals [<xref ref-type="bibr" rid="CR2">2</xref>]. Additionally, S-nitrosylation has unfolded as an important mechanism for regulating protein functions and cell signaling of all main classes of protein and is involved in several biological processes including immune response [<xref ref-type="bibr" rid="CR1">1</xref>], protein stability, transcription regulation, post translational regulation, DNA damage repair, and redox regulation [<xref ref-type="bibr" rid="CR3">3</xref>], and is an emerging paradigm of redox signaling for protection against oxidative stress. Recently, it has also been shown that SNO also regulates diverse biological processes in plants [<xref ref-type="bibr" rid="CR4">4</xref>].</p>
    <p id="Par24">The experimental identification of S-nitrosylated sites is generally performed by combining the Biotin-switch technique (BST) [<xref ref-type="bibr" rid="CR5">5</xref>]with Mass Spectrometry (MS). With few exceptions, all methods for the identification of S-nitrosylation sites are based on the BST and differ only in the utilized MS equipment, ion sources, and the use of liquid chromatography. Please refer to the excellent review by Lamotte et al. [<xref ref-type="bibr" rid="CR4">4</xref>] for an in-depth description of experimental identification of S-nitrosylation.</p>
    <p id="Par25">Although some studies have suggested that the target cysteine residues often lie within an acid–base or hydrophobic motif [<xref ref-type="bibr" rid="CR6">6</xref>], recent studies have proven that the acid–base motif is located farther from the cysteine [<xref ref-type="bibr" rid="CR7">7</xref>]. Additionally, even though some studies have suggested that the target cysteine must be within a signature motif (I/L-X-C-X2-D/E) and be in a suitable environment [<xref ref-type="bibr" rid="CR1">1</xref>], there is not yet a consensus motif for SNO [<xref ref-type="bibr" rid="CR8">8</xref>]. In this regard, various mechanisms are involved in the formation of SNO.</p>
    <p id="Par26">Owing to this fact that high throughput experimental approaches do not yet exist for SNO, several complimentary computational approaches have been developed to predict protein SNO sites. These approaches are mostly based on machine learning models that use experimentally identified S-nitrosylation sites to train the model and use various features such as identity of the neighboring residues during training. Some of the existing SNO site prediction tools are: GPS-SNO [<xref ref-type="bibr" rid="CR9">9</xref>], SNOSite [<xref ref-type="bibr" rid="CR10">10</xref>], iSNOPSeAAC [<xref ref-type="bibr" rid="CR11">11</xref>], etc. SNOSID [<xref ref-type="bibr" rid="CR12">12</xref>], developed by Hao et al., is perhaps the first computational tool for predicting S-nitrosylation sites. GPS-SNO [<xref ref-type="bibr" rid="CR9">9</xref>] is another approach for prediction of S-nitrosylation sites and is based on the GPS 3.0 algorithm. Moreover, iSNO-PseAAC [<xref ref-type="bibr" rid="CR11">11</xref>] is another approach developed by Xu et al. that uses PseAAC to represent protein sequences for prediction of protein S-nitrosylation sites. Recently, various deep learning-based methods [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>] have been developed for prediction of various post-translation modification sites including SNO sites. In that regard, DeepNitro [<xref ref-type="bibr" rid="CR15">15</xref>], a deep learning-based approach, developed by Xie et al. for the prediction of protein S-nitrosylation sites uses four different types of features: one-hot encoding, Property Factor Representation (PFR), k-space spectrum, and PSSM encoding.</p>
    <p id="Par27">Additionally, Hasan et al. proposed PreSNO [<xref ref-type="bibr" rid="CR16">16</xref>] which integrates two classifiers: RF and SVM using Linear regression. The input to both the RF and SVM in PreSNO is based on four different encoding schemes: the composition of profile-based amino acid pair (CPA), the K-space spectral amino acid composition (SAC), tripeptide composition from PSSM (TCP), and physicochemical properties of amino acids (PPA). It must be noted here that the DeepNitro dataset is used for training and testing of the PreSNO model. For a thorough review of the existing computational approaches for predicting Protein S-nitrosylation sites, please refer to Zhao et al. [<xref ref-type="bibr" rid="CR17">17</xref>].</p>
    <p id="Par28">Lately, we have witnessed the development of exciting array of Natural Language Processing (NLP) algorithms and technologies including recent breakthroughs in the field of bioinformatics [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR20">20</xref>]. Among these developments, language models (LMs) have emerged as a powerful paradigm in NLP for learning embeddings directly from large, unlabeled natural language datasets. In contrast to uncontextualized word embeddings, which return the same embedding for a word irrespective of the surrounding words, embeddings from LMs are contextualized in a way that they render the embedding dependent on the surrounding words. These advances are now being explored in proteins through the development of various protein language models (pLMs) [<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR24">24</xref>]. The representations (embeddings) extracted from these transformer-based language models have been successful for various downstream bioinformatics prediction tasks [<xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR27">27</xref>], suggesting that the huge amount of information learned by these pLMs can be transferred to other tasks by extracting embeddings from these pLMs and using these embeddings as an input to predict other properties of protein.</p>
    <p id="Par29">As discussed above, though there exist various computational approaches for predicting SNO sites, the prediction performance of the existing approaches is not yet satisfactory. Additionally, the potential uses of deep learning methods including natural language processing and language models in predicting SNO sites is largely unexplored. Furthermore, the existing approaches do not leverage the distilled information from these pLMs. To the best of our knowledge, embedding from pLMS has not been previously used to predict SNO sites. In this regard, here we propose pLMSNOSite, a stacked generalization approach based on intermediate fusion of models that combines two different learned marginal amino acid sequence representations: per-residue contextual embedding learned on full sequences from a pre-trained protein language model and per-residue supervised word embedding learned on window sequences. Based on independent testing, pLMSNOSite performs better than other widely available approaches for SNO site prediction in proteins.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Benchmark dataset</title>
      <p id="Par30">The training and testing dataset for this work was adopted from PreSNO [<xref ref-type="bibr" rid="CR16">16</xref>]. PreSNO utilizes the original DeepNitro [<xref ref-type="bibr" rid="CR15">15</xref>] dataset which is curated through an extensive literature search for experimentally verified S-nitrosylation sites. This dataset consists of an experimentally confirmed 4762 sites from 3113 protein sequences. These sequences are first subjected to homology removal using the cd-hit algorithm [<xref ref-type="bibr" rid="CR28">28</xref>] with an identity cut-off of 0.3, resulting in 3734 positive sites. The remaining cysteine residues from the same protein sequences (ones that have the experimental SNO sites) are considered as the negative S-nitrosylation sites resulting in 20,548 negative sites. Furthermore, by eliminating the negative site if there is an identical window sequence in the set of positive sites, we obtained 20,333 negative sites. From these sites, the independent dataset is constructed by randomly sampling 20% of the sites, and the remaining sites are used to construct the training dataset. This resulted in 3383 SNO sites and 17,165 non-SNO sites in the training set and 351 SNO sites and 3168 non-SNO sites in the independent test set. Clearly, the training set is highly skewed in class distribution towards negative sites. This imbalance in the training dataset was resolved by randomly undersampling the negative sites. The balanced training set thus obtained was used for building the models whereas the independent test set was unaltered for assessing the generalization ability of the trained models on unseen data. Note that the main difference between DeepNitro [<xref ref-type="bibr" rid="CR15">15</xref>] and PreSNO [<xref ref-type="bibr" rid="CR16">16</xref>] datasets is the different cut-off used in cd-hit [<xref ref-type="bibr" rid="CR28">28</xref>]. The description of the training dataset and independent dataset used in the study is shown in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref> respectively.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Number of proteins, number of sites, and training set used in this study (adopted from PreSNO)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Sites</th><th align="left">Number of proteins</th><th align="left">Number of sites (before balancing)</th><th align="left">Number of sites (after balancing)</th></tr></thead><tbody><tr><td align="left">SNO sites</td><td char="." align="char">1962</td><td char="." align="char">3383</td><td char="." align="char">3383</td></tr><tr><td align="left">Non-SNO sites</td><td char="." align="char">340</td><td char="." align="char">17,165</td><td char="." align="char">3383</td></tr><tr><td align="left">Total</td><td char="." align="char">2302</td><td char="." align="char">20,548</td><td char="." align="char">6766</td></tr></tbody></table><table-wrap-foot><p>The balanced sites are used for training the model</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Number of proteins, positive, and negative sites of independent test set used in the experiments (adopted from PreSNO)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Sites</th><th align="left">Number of proteins</th><th align="left">Number of sites</th></tr></thead><tbody><tr><td align="left">SNO sites</td><td char="." align="char">267</td><td char="." align="char">351</td></tr><tr><td align="left">Non-SNO sites</td><td char="." align="char">231</td><td char="." align="char">3168</td></tr><tr><td align="left">Total</td><td char="." align="char">438</td><td char="." align="char">3519</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Sequence representation</title>
      <p id="Par31">A critical step before passing amino acid sequences to a machine learning model is the numerical encoding of each amino acid through an encoding scheme that assigns a numerical representation to the amino acid. Choosing informative, discriminating, and independent encoding (or features) is a crucial element of effective machine learning algorithms. Most of the existing SNO prediction tools rely on manual or hand-crafted features for the representation of amino acids [<xref ref-type="bibr" rid="CR17">17</xref>]. We aim to eliminate the reliance on hand-crafted features by leveraging two feature representation approaches for establishing a robust representation of S-nitrosylation sites: word embeddings from a supervised embedding layer and embeddings from ProtT5 (ProtT5-XL-UniRef50) [<xref ref-type="bibr" rid="CR21">21</xref>], a pre-trained protein language model based on Google’s T5 (Text-to-Text Transfer Transformer) [<xref ref-type="bibr" rid="CR29">29</xref>] architecture. Below, we describe these two types of embeddings in detail.</p>
    </sec>
    <sec id="Sec5">
      <title>Word embedding using supervised embedding layer</title>
      <p id="Par32">Word embedding is a class of approaches to represent words using a dense vector representation. Protein sequences can be seen as documents, and amino acids that make the protein sequence can be seen as words. In that regard, amino acids (words) can be represented by dense vectors using word embeddings where a vector represents the projection of the amino acid into a continuous vector space. We used Keras’s embedding layer [<xref ref-type="bibr" rid="CR30">30</xref>], as in LMSuccSite [<xref ref-type="bibr" rid="CR27">27</xref>], to implement supervised word embedding where the embedding is learned as a part of training a deep learning model. The process of parameter learning in this approach is supervised; the parameters are updated with subsequent layers during the learning process under the supervision of a label.  With subsequent epochs, the layer learns a feature-rich representation of sequences while still preserving the semantic relation between amino acids (each vectorized representation being orthogonal in some other dimension [<xref ref-type="bibr" rid="CR31">31</xref>]). The input for this representation is the window sequence centered around the site of interest flanked by an equal number of residues upstream and downstream. In cases where there are not enough residues to create the window sequence, we pad the window with virtual amino acids (‘−’). Initially, the amino acids are integer encoded, so that each amino acid can be represented by a unique integer which is provided as an input to the embedding layer. Then, the embedding layer is initialized with random weights, and the layer will learn better embedding for all the amino acids with subsequent epochs as the part of the training process. There are three salient parameters in word embedding (obtained through Keras’s embedding layer) that determines the quality of the feature representation of amino acid sequences. These parameters are <italic>input_dim</italic> denoting the size of the vocabulary, <italic>output_dim</italic> denoting the length of the feature vector for each word and <italic>input_length</italic> denoting the maximum length of input sequence (in our case, the length of window sequence). The vocabulary size is set to 23 to represent 20 canonical, two non-canonical, and one virtual amino acid (denoted by ‘−’.). Based on fivefold cross-validation on a wide range of values of embedding dimension, we obtained the best performance using a dimension of size four. Similarly, performing fivefold cross-validation on multiple window sizes, we obtained the best results using a window size of 37. Hence, the output of the embedding layer is 37 × 4 where 37 is the window size and four is the embedding dimension. The hyperparameter tuning of the window size (input_length) and the embedding dimension (output_dim) is explained in detail in the result section.</p>
    </sec>
    <sec id="Sec6">
      <title>Embedding from pre-trained protein language model ProtT5</title>
      <p id="Par33">Another representation that we use in our work is based on embeddings from ProtT5, a pre-trained protein language model (pLM). The advances in Natural Language Processing (NLP) gained by the development of newer language models have been transferred to protein sequences by learning to predict masked or missing amino acids using a large corpus of protein sequences [<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR23">23</xref>]. Processing/distilling the information learned by these pLMs yields a representation of protein sequences referred to as embeddings [<xref ref-type="bibr" rid="CR21">21</xref>]. Recently, these embeddings have been shown to be beneficial in various structural bioinformatics tasks including but not limited to secondary structure prediction and subcellular location, among others. In that regard, in this work, we use pLM ProtT5 [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR27">27</xref>] as a static feature encoders to extract per residue embeddings for protein sequences for which we are predicting S-nitrosylation sites. It is relevant to note that the input to ProtT5 is the overall protein sequence. ProtT5 is a pLM trained on BFD (Big Fantastic Database consisting of 2.5 billion sequences), fine-tuned on Uniref50 consisting of 45 million sequences, and developed at Rostlab using T5 [<xref ref-type="bibr" rid="CR29">29</xref>] architecture. Infact, postional encoding is learned specific to each attention head in the transfromer architecture which is shared across all the layers of attention stack. Using ProtT5, the per-residue embeddings were extracted from the last hidden layer of the encoder model with the size of <italic>L</italic>x1024, where <italic>L</italic> is the size of the protein using the overall protein sequence as the input. As suggested by ProtTrans [<xref ref-type="bibr" rid="CR26">26</xref>], LMSuccSite [<xref ref-type="bibr" rid="CR27">27</xref>], the encoder side of ProtT5 was used, and embeddings were extracted in half-precision. For our purpose, as the per-residue embeddings are a contextualized representation, we only used the 1024 length embeddings for the site of interrogation (aka cystine ‘C’). The schematic of the extraction of embedding from ProtT5 is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Fig. 1</label><caption><p>Extraction of Embeddings from ProtT5 language model, the site is the site of interrogation (C, represented in red)</p></caption><graphic xlink:href="12859_2023_5164_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Deep learning models</title>
      <p id="Par34">Given the input and output, we train several DL models to learn underlying patterns in the protein sequence.</p>
    </sec>
    <sec id="Sec8">
      <title>Sequence-based models</title>
      <p id="Par35">The input to the model is the sequence of amino acids which can thought of as sequence of words in the field of natural language processing (NLP). Hence, an obvious option is to employ models designed to train and process sequences, such as recurrent neural network (RNN), long-short term memory (LSTM) [<xref ref-type="bibr" rid="CR32">32</xref>], bidirectional long-short term memory (BiLSTM), and so forth. The main drawback of these sequence-oriented models is that they are computationally intense, requiring a large number of parameters for training.</p>
    </sec>
    <sec id="Sec9">
      <title>Convolution neural network (CNN) model</title>
      <p id="Par36">CNN models have demonstrated great success in various computer vision tasks, where convolution kernels or filters are used to learn and discern the spatial co-relation between pixels in images. In our SNO site prediction setting, CNNs can help learn the underlying relationship among the amino acids in the input protein sequence. CNNs are less computationally intensive models than sequence-oriented models and facilitate the training of deeper networks as significantly fewer parameters are needed to be learned. The usage of CNNs is prevalent in several PTM prediction tasks [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. In our case, we use CNN to process the feature representation of the protein sequence obtained from the word embedding layer as described in the previous section. The process of obtaining feature maps of input integer encoded window sequence from the convolution layer (or kernel) is given by the formula:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G\left[m,n\right]=\left(f\cdot h\right)\left[m,n\right]={\sum }_{ j}{\sum }_{k }h\left[j,k\right]f\left[m-j,n-k\right]$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>G</mml:mi><mml:mfenced close="]" open="["><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mi>f</mml:mi><mml:mo>·</mml:mo><mml:mi>h</mml:mi></mml:mfenced><mml:mfenced close="]" open="["><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mi>h</mml:mi><mml:mfenced close="]" open="["><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mfenced><mml:mi>f</mml:mi><mml:mfenced close="]" open="["><mml:mi>m</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2023_5164_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where the input sequence is denoted by <italic>f</italic> and the kernel by <italic>h</italic>. The index of rows and columns in the resultant matrix is denoted by <italic>m</italic> and <italic>n</italic> respectively. Typically, we use multiple convolutions over the input sequence which helps to extract diverse features from a single input map and the output maps are stacked forming a volume. The dimension of the obtained feature map from convolution over volume can be calculated using the following formula:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[n,n,{n}_{c}\right].\left[f,f,{n}_{c}\right]=\left[floor\left(\frac{n+2p-f}{s}+1\right),floor\left(\frac{n+2p-f}{s}+1\right),{n}_{f}\right]$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mfenced close="]" open="["><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced><mml:mo>.</mml:mo><mml:mfenced close="]" open="["><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2023_5164_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>n</italic> is the size input sequence, <italic>n</italic><sub><italic>c</italic></sub> is the number of channels, f is the kernel size, <italic>p</italic> is the used padding <italic>s</italic> is the used stride and <italic>n</italic><sub><italic>f</italic></sub> is the number of kernels. The convolution layer is followed by the max-pooling layer which selects the maximum value from regions of feature maps, creating a downsampled map. The downsampled feature map is then flattened and passed into a conventional fully connected network. All the weights in the network are updated using the backpropagation algorithm. It is to be mentioned that we use a non-linear activation function called ReLU (Rectified Linear Unit) in all layers of the architecture for capturing non-linear signals in the data. Among other activation functions, ReLU is widely adopted in deep learning applications due to its benefits such as representational sparsity and efficiency with respect to computation. The ReLU activation function for a domain value <italic>x</italic> is given by:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$RELU\left(x\right) = \mathrm{max}\left(0,x\right)$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mfenced close=")" open="("><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2023_5164_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec10">
      <title>pLMSNOSite architecture</title>
      <p id="Par37">The general framework of the stacked generalization consists of two base models (level-0 models) and a higher-level meta-model (level-1 model, meta-classifier). Our approach pLMSNOSite (protein Language Model-based S-Nitrosylation Site predictor) uses stacked generalization to combine the meta-features (marginal representations) learned from the base models to achieve better prediction. Specifically, the first base model (herein referred to as embedding layer module) learns the representation of local information of cysteine residue of interest captured by proximal residues within window sequences using supervised word embedding. The second base model (herein referred to as pLM ProtT5 module) learns the contextualized information of the same cysteine residue generated by unsupervised pLM using a full-length sequence as input. These learned features by the base models using different representations are fused together and a meta-model is learned adopting an ensemble approach known as stacked generalization. The overall architecture of pLMSNOSite is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. As shown in the figure, the architecture of pLMSNOSite consists of two base models: the supervised embedding layer module and the ProtT5 module, followed by a higher-level meta-model (meta-classifier) that performs the feature-level fusion of base models. We further describe the supervised embedding layer module and ProtT5 modules and higher-level meta-model in detail below.<fig id="Fig2"><label>Fig. 2</label><caption><p>The overall architecture of pLMSNOSite with the two base models and a meta-classifier model</p></caption><graphic xlink:href="12859_2023_5164_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
    <sec id="Sec11">
      <title>Supervised (word) embedding layer module</title>
      <p id="Par38">The input to this module is a protein window sequence (centered around the site of interest flanked by an equal number of residues on both sides) that captures the local interaction between amino acids surrounding the site of interrogation (in this case S-nitrosylation/non-S-nitrosylation sites) within the window sequence. We choose a deep two-dimensional (2D) Convolutional Neural Network (CNN) to extract feature maps from these localized interactions of proximal amino acids. The advantage of CNN over other sequence-oriented models has been explained in the previous section. Interestingly, the CNN model also showed promising performance in fivefold cross-validation (refer result section). The 2D-CNN architecture in this module first consists of a word embedding layer which takes integer encoded sequence as an input. The output from this layer (37 × 4, where 37 is the window size and 4 is the embedding dimension) is passed into a 2D convolutional layer to extract feature maps from a window sequence followed by a dropout layer to prevent overfitting, a max-pooling layer and a fully connected layer consisting of a flatten layer and a dense layer. The hyperparameters associated with the model architecture were determined by performing an extensive grid search based on fivefold cross-validation. The search space and optimal hyperparameter values of the model obtained from cross-validation are reported in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S2. Finally, the feature map of size 16 obtained from the final hidden layer from the optimized 2D-CNN model (hereafter dubbed Embedding2DCNN) is treated as the output of the first base model.</p>
    </sec>
    <sec id="Sec12">
      <title>pLM ProtT5 module</title>
      <p id="Par39">In this module, at first per-residue embeddings are extracted from the last hidden layer of the encoder models of ProtT5 of the size of <italic>L</italic>x1024, where <italic>L</italic> is the length of the protein using the overall protein sequence as the input. Subsequently, the 1024 features corresponding to the site of interest are extracted and fed as an input to this module. A dense neural network was used to learn the representation from the obtained features. The architecture of this model and its corresponding hyperparameter values in this module were also chosen based on grid search using fivefold cross-validation. The search space and selected hyperparameter values are reported in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1. Similar to Embedding2DCNN, we obtained a feature map of size 4 from this base model (hereafter dubbed as ProtT5ANN module).</p>
    </sec>
    <sec id="Sec13">
      <title>Stacked generalization</title>
      <p id="Par40">To integrate the capability of the representation learned by the base models (Embedding2DCNN and ProtT5ANN), we implemented stacked generalization of these modules. To this end, instead of stacking on a decision level or a score level, we performed an intermediate level feature fusion by concatenating the feature maps obtained from the final hidden layers of the base models (16 x 1 from the Embedding2DCNN and 4 x 1 from the ProtT5ANN) as explained in previous subsections. The fused features were then used to train the meta-model (meta-classifier) that acts as the final inference model. Since the datasets used to train base models and meta-classifier are similar, there is a likelihood of data leakage about target information from base models to meta-classifier [<xref ref-type="bibr" rid="CR33">33</xref>], which could result in overestimation of cross-validation performance leading to spuriousness in the model selection process. Considering this, we paid special attention to ensure that there is no data leakage from base models to meta-classifier. In this work, we performed the fivefold cross-validation algorithm called Stacking with K-fold cross-validation, developed by Wolpert [<xref ref-type="bibr" rid="CR34">34</xref>], to ensure no target information is leaked while training the meta-classifier. Initially, the overall training data are randomly split into K folds. Subsequently, base models are trained using K-1 folds, and the models are tested against the remaining onefold validation set. The predictions or features obtained from different base models for each fold are collected to train the next-level model (meta classifier). As a result, the meta classifier is trained on a non-overlapping dataset preventing any potential data leakage. Similar to other modules, we selected a single layer feed forward neural network as the architecture for the stacked generalization model using cross-validation.</p>
    </sec>
    <sec id="Sec14">
      <title>Model training</title>
      <p id="Par41">All the deep learning models were trained to minimize the binary cross-entropy loss or log loss function which is given by the following equation:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$-\frac{1}{N}{\sum }_{i=1}^{N}\left[{y}_{i}\mathrm{log}\left({y}_{i}^{^{\prime}}\right)+\left(1-{y}_{i}\right)\mathrm{log}\left(1-{y}_{i}^{^{\prime}}\right)\right]$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mfenced close="]" open="["><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi mathvariant="normal">log</mml:mi><mml:mfenced close=")" open="("><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mi mathvariant="normal">log</mml:mi><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2023_5164_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq1"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2023_5164_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i}^{^{\prime}}$$\end{document}</tex-math><mml:math id="M12"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2023_5164_Article_IEq2.gif"/></alternatives></inline-formula> are the ground truth and predicted probability for the <italic>i</italic>th instance of N points respectively.</p>
      <p id="Par42">The parameters in the model were optimized to minimize the above loss function using Adam stochastic optimization method (AMSGrad variant) with an adaptive learning rate of 0.001, the decay rate for the first moment as 0.9, and the decay rate for the second moment as 0.999. Prior to training, the number of epochs was set to 200 and the batch size was set to 128. Additionally, an early stopping strategy with patience equal to 5 was used which stops the training after 5 epochs if no improvement in loss is recorded. Any potential overfitting while training was averted by carefully monitoring accuracy/loss curves.</p>
    </sec>
    <sec id="Sec15">
      <title>Evaluation of models and performance metrics</title>
      <p id="Par43">We adopt a stratified fivefold cross-validation strategy for model selection. Subsequently, we perform independent testing to assess the generalization error of our approach as well as compare with it the existing approaches. Below, we define the performance metrics used for evaluating the models.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{Accuracy (ACC) }= \frac{TP+TN}{TP+TN+FP+FN}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Accuracy</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">ACC</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2023_5164_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{Sensitivity(SN) }= \frac{TP}{TP+FN}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Sensitivity</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">SN</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2023_5164_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{Specificity(SP) }= \frac{TN}{TN+FP}$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Specificity</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">SP</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2023_5164_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{MCC }= \frac{TP*TN-FP*FN}{\left(TP+FP\right)\left(TP+FN\right)\left(TN+FP\right)\left(TN+FN\right)}$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:mi mathvariant="normal">MCC</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mfenced><mml:mfenced close=")" open="("><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mfenced><mml:mfenced close=")" open="("><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mfenced><mml:mfenced close=")" open="("><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2023_5164_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where TP (True Positive) is the number of actual SNO sites predicted as positive, TN (True Negative) is the number of non-SNO sites predicted as negative, FP (False Positive) is the number of non-SNO sites predicted as positive and FN (False Negative) is the number of actual SNO sites predicted as negative.</p>
      <p id="Par44">We also use AUROC (Area Under Receiver Operating Characteristic curve) and AUPR (Area Under Precision-Recall curve) to further evaluate the discriminating performance of the models.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Results</title>
    <p id="Par45">As described above, pLMSNOSite uses stacked generalization to combine the supervised word embedding layer module (Embedding2DCNN) and the pLM ProtT5 module (ProT5ANN) using a meta-classifier. The meta-classifier in fact learns from the output of the base models and thus the base models were first optimized to robustly learn their corresponding representations. Successively, the meta-classifier was optimized to produce the classification inference accurately.</p>
    <p id="Par46">Initially, we analyze the comparative performance of various ML/DL architectures for the selection of the optimal base models using fivefold cross-validation. Subsequently, the comparative cross-validation performance of various models was analyzed for the selection of optimal meta-classifier. Finally, we compare the performance of the overall architecture pLMNOSite against existing SNO site prediction tools using the independent test set. The details of the results obtained from these experiments are presented in the following subsections.</p>
    <sec id="Sec17">
      <title>Selection of window size and embedding dimension for word embedding module</title>
      <p id="Par47">As described in the Methods section, the supervised embedding layer has three major parameters: vocabulary size (input_dim), window size (input_length), and embedding dimension (output_dim). The <italic>input_dim</italic> is fixed to 23 based on the number of canonical amino acids (= 20), non-canonical amino acids (= 2), and virtual amino acids (= 1). The window size (input_length) is important as too few residues might result in information loss while too many residues might result in loss of local contextual information of the site. To obtain the optimal <italic>input_length</italic>, fivefold cross-validation was performed by varying window sizes from 21 to 63. Similarly, a higher embedding dimension demands substantial computational cost and thus the optimal <italic>output_dim</italic> was determined by exhaustively searching the value of the embedding dimension in the search space ranging from 2 to 32.</p>
      <p id="Par48">The cross-validation experiments suggest that the output_dim (or, embedding dimension) of 4 and input_length (or window size) of 37 produced the highest MCC and these values were utilized for further analysis. The obtained value of output dimension is indeed a significant improvement over the traditional binarization encoding (or, one-hot encoding) where static and relatively higher dimensional features are generated. It is also worthwhile to note that the optimal window size for PreSNO is 41 (only 2 residue difference on each side of the central residue). The sensitivity analysis of MCC (mean) on fivefold cross-validation for different window sizes and embedding dimension for Embedding2DCNN is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, b respectively and the respective plots for other models are in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S1.<fig id="Fig3"><label>Fig. 3</label><caption><p>Sensitivity analysis of MCC on fivefold cross-validation when <bold>a</bold> window size is varied keeping the dimension and vocabulary size constant (dimension = 4, vocabulary size = 23), <bold>b</bold> dimension is varied keeping the window size and vocabulary size constant (window size = 37, vocabulary size = 4)</p></caption><graphic xlink:href="12859_2023_5164_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec18">
      <title>Selection of model architecture for the word embedding  module</title>
      <p id="Par49">To obtain the best architecture for the word embedding module, we performed a fivefold cross-validation of the model using various architectures: 2D-CNN [<xref ref-type="bibr" rid="CR29">29</xref>], ANN, LSTM [<xref ref-type="bibr" rid="CR30">30</xref>], ConvLSTM [<xref ref-type="bibr" rid="CR31">31</xref>], and BiLSTM using the value of window size (= 37), vocabulary size (= 23) and embedding dimension (= 4) obtained from the prior experiments. It must be noted here that the supervised word embedding is obtained as a part of the training process of the model, so we only experimented with DL-based architectures. These DL architectures were tuned using grid search with fivefold cross-validation over wide range of search space (provided in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S2). The results of the fivefold cross-validation of the optimized models are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. Similarly, the AUPR and AUC for cross-validation for these models are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. It can be observed from Table <xref rid="Tab3" ref-type="table">3</xref> as well as Fig. <xref rid="Fig4" ref-type="fig">4</xref> that the 2D-CNN architecture produces the best results (MCC in the table and AUC in the figures). Based on this, 2D-CNN architecture was chosen as the final architecture for the word embedding module.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Fivefold cross-validation results (mean ± one standard deviation) of embedding layer module on the training set</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">ACC</th><th align="left">SN</th><th align="left">SP</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left">2D-CNN</td><td char="." align="char"><bold>0.688</bold> ± <bold>0.018</bold></td><td char="." align="char">0.760 ± 0.063</td><td char="." align="char">0.615 ± 0.069</td><td char="." align="char"><bold>0.382</bold> ± <bold>0.034</bold></td></tr><tr><td align="left">ANN</td><td char="." align="char">0.658 ± 0.018</td><td char="." align="char">0.697 ± 0.0351</td><td char="." align="char">0.619 ± 0.010</td><td char="." align="char">0.318 ± 0.036</td></tr><tr><td align="left">LSTM</td><td char="." align="char">0.674 ± 0.011</td><td char="." align="char"><bold>0.816</bold> ± <bold>0.067</bold></td><td char="." align="char">0.533 ± 0.074</td><td char="." align="char">0.368 ± 0.024</td></tr><tr><td align="left">ConvLSTM</td><td char="." align="char">0.667 ± 0.006</td><td char="." align="char">0.836 ± 0.023</td><td char="." align="char">0.498 ± 0.017</td><td char="." align="char">0.355 ± 0.017</td></tr><tr><td align="left">BiLSTM</td><td char="." align="char">0.686 ± 0.009</td><td char="." align="char">0.747 ± 0.093</td><td char="." align="char"><bold>0.626</bold> ± <bold>0.083</bold></td><td char="." align="char">0.380 ± 0.022</td></tr></tbody></table><table-wrap-foot><p>The highest values in each category are bolded</p></table-wrap-foot></table-wrap><fig id="Fig4"><label>Fig. 4</label><caption><p><bold>a</bold> ROC curves and area under curve (AUC) values for different architectures for the supervised embedding layer module. <bold>b</bold> Precision-recall (PR) curves and area under curve (AUC) values for different architectures for the supervised embedding layer module</p></caption><graphic xlink:href="12859_2023_5164_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec19">
      <title>Selection of model architecture for the pLM module (protT5)</title>
      <p id="Par50">It has been observed in multiple studies encompassing various bioinformatics tasks that a simple machine learning model is enough to obtain a satisfactory performance for pLM based embeddings [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. Based on this knowledge, we experimented with ANN (Artificial Neural Network), SVM (Support Vector Machine) [<xref ref-type="bibr" rid="CR35">35</xref>], RF (Random Forest) [<xref ref-type="bibr" rid="CR36">36</xref>], XGBoost (Extreme Gradient Boosting), and AdaBoost (Adaptive Boosting) architectures for protT5 module using fivefold cross-validation. The scikit-learn’s GridsearchCV was used to optimize SVM, RF, XGBoost and AdaBoost with <italic>cv</italic> as 5 and <italic>param_grid</italic> (parameters grid) value as mentioned in the Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1. The results of the fivefold cross-validation of the optimal models are reported in Table <xref rid="Tab3" ref-type="table">3</xref> and ROC and PR curves for the same are shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. It can be observed that the ANN architecture produced the best results (MCC in Table <xref rid="Tab4" ref-type="table">4</xref> and AUC in Fig. <xref rid="Fig5" ref-type="fig">5</xref>). Based on this, ANN architecture was chosen as the final architecture for the pLM ProtT5 module.<fig id="Fig5"><label>Fig. 5</label><caption><p><bold>a</bold> Area under ROC curves (AUROC) values for different architectures for the ProtT5 module. <bold>b</bold> Area under precision-recall (AUPR) values for different architectures for the ProtT5 module</p></caption><graphic xlink:href="12859_2023_5164_Fig5_HTML" id="MO5"/></fig><table-wrap id="Tab4"><label>Table 4</label><caption><p>Fivefold cross validation results (mean ± one standard deviation) of different models based on ProtT5 features</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Architecture</th><th align="left">ACC</th><th align="left">SN</th><th align="left">SP</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left">ANN</td><td char="." align="char"><bold>0.710 ± 0.015</bold></td><td char="." align="char">0.745 ± 0.028</td><td char="." align="char">0.674 ± 0.015</td><td char="." align="char"><bold>0.421</bold> ± <bold>0.030</bold></td></tr><tr><td align="left">SVM</td><td char="." align="char">0.700 ± 0.012</td><td char="." align="char">0.702 ± 0.016</td><td char="." align="char"><bold>0.699 ± 0.020</bold></td><td char="." align="char">0.401 ± 0.024</td></tr><tr><td align="left">RF</td><td char="." align="char">0.682 ± 0.010</td><td char="." align="char"><bold>0.815 ± 0.815</bold></td><td char="." align="char">0.549 ± 0.815</td><td char="." align="char">0.379 ± 0.378</td></tr><tr><td align="left">XGBoost</td><td char="." align="char">0.699 ± 0.008</td><td char="." align="char">0.752 ± 0.019</td><td char="." align="char">0.645 ± 0.007</td><td char="." align="char">0.400 ± 0.016</td></tr><tr><td align="left">AdaBoost</td><td char="." align="char">0.672 ± 0143</td><td char="." align="char">0.695 ± 0.024</td><td char="." align="char">0.650 ± 0.022</td><td char="." align="char">0.345 ± 0.029</td></tr></tbody></table><table-wrap-foot><p>Highest values in each column are highlighted in bold</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec20">
      <title>Selection of model architecture for meta classifier</title>
      <p id="Par51">Additionally, the optimal architecture for the stacked generalization (aka meta classifier) was obtained using fivefold cross-validation on various ML models. Essentially, during the cross-validation of models for the meta-classifier, the intermediate features obtained from base models were used paying special attention to any potential leakage of target information in the training of the meta classifier as described in the methods section. The candidate models for the meta-classifier were optimized using this approach (data leakage mitigation) for fivefold cross-validation (over the search space reported in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S3). Table <xref rid="Tab5" ref-type="table">5</xref> and Fig. <xref rid="Fig6" ref-type="fig">6</xref> show the comparison of the optimized models based on fivefold cross-validation. These results indicate that Artificial Neural networks (ANN) achieves better validation performance compared to other classifiers in terms of MCC and competitive results in terms of AUPR and AUROC. The meta-classifier based on ANN was hence chosen for our work and we call the overall approach as pLMSNOSite.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performance comparison using different architectures for meta-classifier based on fivefold cross-validation results (mean ± one standard deviation)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left">ACC</th><th align="left">SN</th><th align="left">SP</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left">ANN</td><td char="." align="char"><bold>0.727 ± 0.017</bold></td><td char="." align="char">0.769 ± 0.016</td><td char="." align="char"><bold>0.685 ± 0.033</bold></td><td char="." align="char"><bold>0.4573 ± 0.032</bold></td></tr><tr><td align="left">LR</td><td char="." align="char">0.703 ± 0.014</td><td char="." align="char">0.740 ± 0.017</td><td char="." align="char">0.665 ± 0.028</td><td char="." align="char">0.407 ± 0.027</td></tr><tr><td align="left">SVM</td><td char="." align="char">0.719 ± 0.021</td><td char="." align="char"><bold>0.807 ± 0.029</bold></td><td char="." align="char">0.631 ± 0.017</td><td char="." align="char">0.445 ± 0.043</td></tr><tr><td align="left">RF</td><td char="." align="char">0.724 ± 0.010</td><td char="." align="char">0.771 ± 0.026</td><td char="." align="char">0.678 ± 0.022</td><td char="." align="char">0.451 ± 0.021</td></tr><tr><td align="left">XGBoost</td><td char="." align="char">0.697 ± 0.006</td><td char="." align="char">0.735 ± 0.014</td><td char="." align="char">0.660 ± 0.022</td><td char="." align="char">0.396 ± 0.011</td></tr></tbody></table><table-wrap-foot><p>The highest value in each column is highlighted in bold</p></table-wrap-foot></table-wrap><fig id="Fig6"><label>Fig. 6</label><caption><p>Results based on fivefold cross-validation <bold>a</bold> ROC curves and area under curve (AUC) values for different architectures for the meta classifier model. <bold>b</bold> Precision-recall (PR) curves and area under curve (AUC) values for different architectures for meta classifier model</p></caption><graphic xlink:href="12859_2023_5164_Fig6_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec21">
      <title>Performance of base models and pLMSNOSite on independent test set</title>
      <p id="Par52">To observe the relative performance of the base models (aka Embedding2DCNN and ProtT5ANN) and ensemble model on the independent test set, we compared the performance of these models using an independent test set. Note that this independent test set is imbalanced and that these results have no effect whatsoever on model selection (model selection was solely done based on the results of fivefold cross-validation on training data). The ROC and PR curves of the base models and ensemble model (pLMSNOSite) are shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref> and Table <xref rid="Tab6" ref-type="table">6</xref> shows other performance metrics for the base models and ensemble model. The results indicate that the ensemble model (pLMSNOSite) exhibits higher AUROC, AUPR and MCC compared to the base models. This demonstrates the better generalization ability of the ensemble model (pLMSNOSite) compared to the base models. From the figure, the AUPR values are quite low which is to be expected because precision and recall are focused on minority class (minority class size: 351, majority class size: 3168). Nevertheless, pLMSNOSite still has better precision compared to other existing approaches (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S4).<fig id="Fig7"><label>Fig. 7</label><caption><p>Results based on independent test set (imbalanced): <bold>a</bold> ROC curve and <bold>b</bold> AUPR curve for the base models and pLMSNOSite</p></caption><graphic xlink:href="12859_2023_5164_Fig7_HTML" id="MO7"/></fig><table-wrap id="Tab6"><label>Table 6</label><caption><p>Performance comparison of base models (aka Embedding2DCNN and ProtT5ANN models) and ensemble model (pLMSNOSite)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Models</th><th align="left">ACC</th><th align="left">SN</th><th align="left">SP</th><th align="left">MCC</th></tr></thead><tbody><tr><td align="left">Embedding2DCNN</td><td char="." align="char">0.706</td><td char="." align="char"><bold>0.798</bold></td><td char="." align="char">0.696</td><td char="." align="char">0.310</td></tr><tr><td align="left">ProtT5ANN</td><td char="." align="char"><bold>0.791</bold></td><td char="." align="char">0.598</td><td char="." align="char"><bold>0.812</bold></td><td char="." align="char">0.293</td></tr><tr><td align="left">pLMSNOSite</td><td char="." align="char">0.769</td><td char="." align="char">0.735</td><td char="." align="char">0.772</td><td char="." align="char"><bold>0.340</bold></td></tr></tbody></table><table-wrap-foot><p>The highest value in each column is highlighted in bold</p></table-wrap-foot></table-wrap></p>
      <p id="Par53">Furthermore, we analyzed the performance of pLMSNOSite and base models under various controlled specificity values. As shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>, we can observe that the proposed pLMSNOSite approach performs better in terms of MCC and sensitivity at various values of controlled specificity. Also, we can concur that as the models become more specific, pLMSNOSite is still able to outperform the base models.<fig id="Fig8"><label>Fig. 8</label><caption><p>Comparison of MCC and Sensitivity of pLMSNOSite with base models under different controlled specificity values</p></caption><graphic xlink:href="12859_2023_5164_Fig8_HTML" id="MO8"/></fig></p>
      <p id="Par54">It must be noted that pLMSNOSite was selected as the final predictor based on the cross-validation experiments, and these results were presented to simply assess the performance of the base models and meta-model on the independent test set.</p>
    </sec>
    <sec id="Sec22">
      <title>Comparison with other existing tools using an independent test set</title>
      <p id="Par55">Finally, we compared the performance of our approach (pLMNOSite) with other existing SNO site prediction tools using an independent test set described in the Benchmark dataset section. Specifically, our approach was compared against widely available tools such as GPS-SNO [<xref ref-type="bibr" rid="CR9">9</xref>], SNOSite [<xref ref-type="bibr" rid="CR10">10</xref>], iSNO-PseAAC [<xref ref-type="bibr" rid="CR11">11</xref>], DeepNitro [<xref ref-type="bibr" rid="CR15">15</xref>], and PreSNO [<xref ref-type="bibr" rid="CR16">16</xref>]. It must be pointed out that the same training and independent test set used by PreSNO predictor was employed for our analysis for fair comparison. The results of the comparison are presented in Table <xref rid="Tab7" ref-type="table">7</xref> and note that the results for other predictors were adopted from PreSNO [<xref ref-type="bibr" rid="CR16">16</xref>]. It can be observed from Table <xref rid="Tab7" ref-type="table">7</xref> that the pLMSNOSite achieves the best MCC (= 0.340) among the compared approaches showing an improvement of ∼35.0% in MCC compared to the next best approach (PreSNO). Additionally, it also exhibited an ∼21.7% increase in sensitivity and improvements in terms of specificity and accuracy. It is worth noting that pLMNOSite struck the most balance between sensitivity and specificity with a g-mean (geometric mean of sensitivity and specificity) of 0.754, a ∼10.6% improvement over PreSNO. Additionally, it can also be seen that the ProtT5 model alone has a better MCC (= 0.293) than the other compared approaches. Based on these results, it can be concluded that our novel approach termed pLMSNOSite is a robust predictor of S-nitrosylation sites in proteins.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Performance comparison of pLMSNOSite against other existing approaches using the independent test set</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Predictors</th><th align="left">TP</th><th align="left">FP</th><th align="left">TN</th><th align="left">FN</th><th align="left">ACC</th><th align="left">SN</th><th align="left">SP</th><th align="left">MCC</th><th align="left">AUROC</th></tr></thead><tbody><tr><td align="left">GPS-SNO</td><td char="." align="char">99</td><td char="." align="char">825</td><td char="." align="char">2337</td><td char="." align="char">253</td><td char="." align="char">0.693</td><td char="." align="char">0.281</td><td char="." align="char">0.739</td><td char="." align="char">0.014</td><td align="left">0.523</td></tr><tr><td align="left">iSNO-PseAAC</td><td char="." align="char">101</td><td char="." align="char">768</td><td char="." align="char">2394</td><td char="." align="char">251</td><td char="." align="char">0.710</td><td char="." align="char">0.287</td><td char="." align="char">0.757</td><td char="." align="char">0.031</td><td align="left">–</td></tr><tr><td align="left">SNOSite</td><td char="." align="char">235</td><td char="." align="char">1749</td><td char="." align="char">1413</td><td char="." align="char">117</td><td char="." align="char">0.469</td><td char="." align="char">0.668</td><td char="." align="char">0.447</td><td char="." align="char">0.069</td><td align="left">–</td></tr><tr><td align="left">DeepNitro</td><td char="." align="char">202</td><td char="." align="char">776</td><td char="." align="char">2386</td><td char="." align="char">148</td><td char="." align="char">0.737</td><td char="." align="char">0.578</td><td char="." align="char">0.737</td><td char="." align="char">0.222</td><td align="left">0.731</td></tr><tr><td align="left">PreSNO</td><td char="." align="char">211</td><td char="." align="char">733</td><td char="." align="char">2431</td><td char="." align="char">141</td><td char="." align="char">0.752</td><td char="." align="char">0.604</td><td char="." align="char">0.769</td><td char="." align="char">0.252</td><td align="left"><bold>0.756</bold></td></tr><tr><td align="left">pLMSNOSite</td><td char="." align="char"><bold>258</bold></td><td char="." align="char"><bold>718</bold></td><td char="." align="char"><bold>2446</bold></td><td char="." align="char"><bold>93</bold></td><td char="." align="char"><bold>0.769</bold></td><td char="." align="char"><bold>0.735</bold></td><td char="." align="char"><bold>0.773</bold></td><td char="." align="char"><bold>0.340</bold></td><td align="left">0.754</td></tr></tbody></table><table-wrap-foot><p>The highest values in each column are highlighted in bold</p><p>Note that the values for other approaches were adopted from PreSNO. Although same independent test set was used for all the approaches, there is a slight variation in the number of total positive and negative sites. Nevertheless, the integrity of comparison is not compromised at all</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec23">
      <title>t-SNE visualization of pLMSNOSite</title>
      <p id="Par56">Additionally, we used t-distributed stochastic neighbor embedding (t-SNE) [<xref ref-type="bibr" rid="CR37">37</xref>] to project the learned features from the final hidden layer into R<sup>2</sup> cartesian space. With a perplexity value of 50 and a learning rate of 500, the t-SNE was visualized from the training data using a scatter plot (Fig. <xref rid="Fig9" ref-type="fig">9</xref>). It can be inferred from the plot that the boundary of separation between SNO sites (blue data points) and non-SNO sites (orange data points) is quite pronounced indicating that the proposed stacked generalization approach is able to discriminate between the positive sites and the negative sites.<fig id="Fig9"><label>Fig. 9</label><caption><p>2D t-SNE visualization of the learned features from training data by pLMSNOSite</p></caption><graphic xlink:href="12859_2023_5164_Fig9_HTML" id="MO9"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec24">
    <title>Discussions and conclusions</title>
    <p id="Par57">Protein S-nitrosylation is one of the important protein post-translational modifications that is responsible for regulating protein functions and cell signaling of all main classes of proteins. In this work, we developed a computational tool to predict protein S-nitrosylation sites called pLMSNOSite that combines a supervised embedding layer model and a protein language model (based on ProtT5) using a stacked generalization approach. Based on independent test results, pLMSNOSite shows better performance than the compared existing tools. As can be seen from the results, the improved performance of our approach can mainly be attributed to the new embedding representation obtained from ProtT5 (a protein language model). One of the benefits of language models like ProtT5 is that it is learned on overall sequence to extract the contextualized embedding of the site of interest as a consequence of which the dependency on defining local contextual information of the site based on the window size (which demands additional overhead for hyperparameter tuning) is averted.  Based on the experimental results, it can be concluded that pLMSNOSite is a promising tool for predicting protein S-nitrosylation sites. The trained pLMSNOSite model and related dataset are provided in our public GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/KCLabMTU/pLMSNOSite">https://github.com/KCLabMTU/pLMSNOSite</ext-link>) for the community.</p>
    <p id="Par58">As in pLMSNOSite, the representation of protein sequences using protein language model could be explored to improve other protein bioinformatics tasks like protein-drug interaction prediction [<xref ref-type="bibr" rid="CR38">38</xref>]. Essentially, by representing the protein target using pLMs we may expect improved protein-drug interaction prediction. Additionally, the protein language model could be used for improved protein–protein interaction prediction (PPI) [<xref ref-type="bibr" rid="CR39">39</xref>] where representations for both proteins can be extracted using pLMs. Although pLMSNOSite shows promising performance, the predictive performance of pLMSNOSite could be improved by leveraging the vast amount of structural data made available due to the success of AlphaFold2 [<xref ref-type="bibr" rid="CR18">18</xref>]. Additionally, pLMSNOSite only uses sequence features from ProtT5 language model for feature extraction but there are other recent protein language models (e.g. ESM-2 [<xref ref-type="bibr" rid="CR24">24</xref>]) and exploration of these language models for SNO site prediction could be other important future work. Since our method uses ProtT5, our method might require appropriate computational resources for very long protein sequences.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec25">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2023_5164_MOESM1_ESM.docx">
            <caption>
              <p><bold>Additional file 1.</bold> Contains supplementary tables and figures referred to in the manuscript. In sections 1, 2, and 3, we describe various ML/DL architectures and their respective hyperparameters. <bold>Table S1</bold>. Hyperparameter search space for models in the ProtT5 module. <bold>Table S2</bold>. Hyperparameter search space for models in the word embedding module. <bold>Table S3</bold>. Hyperparameter search space for models in the meta-classifier. <bold>Table S4</bold>. fivefold cross-validation results of Embedding2DCNN and ProtT5ANN when imbalanced learning (based on cost-sensitive learning) is performed. <bold>Table S5</bold>. Best combination (with respect to MCC) of window size and embedding dimension for each of the candidate models for the word embedding module on fivefold cross-validation. <bold>Figure S1</bold>. The sensitive analysis curves of each DL model in the word embedding module on fivefold cross-validation. <bold>Table S6</bold>. Comparison of ProtT5 with other pLMs such as ProtBERT (BERT-based ProtTrans family model) and Meta’s ESM-1 using independent testing. <bold>Figure S2</bold>. Frequency and WebLogo plots for train positive and train negative window sequences (window size = 37). <bold>Figure S3</bold>. Precision-Recall curves were produced for base models and pLMSNOSite using an imbalanced independent set and a balanced independent test set separately. <bold>Figure S4</bold>. Comparison of pLMSNOSite with other existing predictors based on precision values using an independent test set.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>SNO</term>
        <def>
          <p id="Par4">S-nitrosylation</p>
        </def>
      </def-item>
      <def-item>
        <term>LMs</term>
        <def>
          <p id="Par5">Language models</p>
        </def>
      </def-item>
      <def-item>
        <term>T5</term>
        <def>
          <p id="Par6">Text to text transfer transformer</p>
        </def>
      </def-item>
      <def-item>
        <term>t-SNE</term>
        <def>
          <p id="Par7">T-distributed stochastic neighbor embedding</p>
        </def>
      </def-item>
      <def-item>
        <term>PTM</term>
        <def>
          <p id="Par8">Post translational modification</p>
        </def>
      </def-item>
      <def-item>
        <term>MCC</term>
        <def>
          <p id="Par9">Mathew correlation coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p id="Par10">Receiver operating characteristics</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC</term>
        <def>
          <p id="Par11">Area under ROC curve</p>
        </def>
      </def-item>
      <def-item>
        <term>PR</term>
        <def>
          <p id="Par12">Precision-recall</p>
        </def>
      </def-item>
      <def-item>
        <term>ReLU</term>
        <def>
          <p id="Par13">Rectified linear unit</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par14">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p id="Par15">Long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>BiLSTM</term>
        <def>
          <p id="Par16">Bidirectional long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>ConvLSTM</term>
        <def>
          <p id="Par17">Convolutional long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>DL</term>
        <def>
          <p id="Par18">Deep learning</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par19">Support vector machine</p>
        </def>
      </def-item>
      <def-item>
        <term>LR</term>
        <def>
          <p id="Par20">Logistic regression</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p id="Par21">Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>PPI</term>
        <def>
          <p id="Par22">Protein–protein interaction</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We acknowledge the High-Performance Computing resources made available to us by Michigan Tech. We also acknowledge helpful discussions with Subash C. Pakhrin, Meenal Chaudhari, Hamid Ismail, Ženia Sidorov and Soufia Bahmani.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>PP, SP, HS, DK conceived of and designed the experiments. PP and SP performed the experiments and data analysis. PP, SP, DK wrote the paper. PP, SP, HS, and DK revised the manuscript. DK oversaw the overall project. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by National Science Foundation (NSF) grant nos. 1901793, 1564606 (to DK)  JSPS KAKENHI grant no.s JP19H04176 and JP22K19834 (to SH).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets, trained models, source codes, and other resources used in this study are publicly available <ext-link ext-link-type="uri" xlink:href="https://github.com/KCLabMTU/pLMSNOSite">https://github.com/KCLabMTU/pLMSNOSite</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par59">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par60">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par61">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fernando</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>S-nitrosylation: an emerging paradigm of redox signaling</article-title>
        <source>Antioxidants (Basel).</source>
        <year>2019</year>
        <volume>8</volume>
        <issue>9</issue>
        <fpage>404</fpage>
        <pub-id pub-id-type="doi">10.3390/antiox8090404</pub-id>
        <pub-id pub-id-type="pmid">31533268</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Martinez-Ruiz</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cadenas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lamas</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Nitric oxide signaling: classical, less classical, and nonclassical mechanisms</article-title>
        <source>Free Radic Biol Med</source>
        <year>2011</year>
        <volume>51</volume>
        <issue>1</issue>
        <fpage>17</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1016/j.freeradbiomed.2011.04.010</pub-id>
        <pub-id pub-id-type="pmid">21549190</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hess</surname>
            <given-names>DT</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein S-nitrosylation: purview and parameters</article-title>
        <source>Nat Rev Mol Cell Biol</source>
        <year>2005</year>
        <volume>6</volume>
        <issue>2</issue>
        <fpage>150</fpage>
        <lpage>166</lpage>
        <pub-id pub-id-type="doi">10.1038/nrm1569</pub-id>
        <pub-id pub-id-type="pmid">15688001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lamotte</surname>
            <given-names>O</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein S-nitrosylation: specificity and identification strategies in plants</article-title>
        <source>Front Chem</source>
        <year>2014</year>
        <volume>2</volume>
        <fpage>114</fpage>
        <pub-id pub-id-type="pmid">25750911</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jaffrey</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>SH</given-names>
          </name>
        </person-group>
        <article-title>The biotin switch method for the detection of S-nitrosylated proteins</article-title>
        <source>Sci STKE</source>
        <year>2001</year>
        <volume>2001</volume>
        <issue>86</issue>
        <fpage>L1</fpage>
        <pub-id pub-id-type="doi">10.1126/stke.2001.86.pl1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stamler</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Lamas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>FC</given-names>
          </name>
        </person-group>
        <article-title>Nitrosylation. The prototypic redox-based signaling mechanism</article-title>
        <source>Cell</source>
        <year>2001</year>
        <volume>106</volume>
        <issue>6</issue>
        <fpage>675</fpage>
        <lpage>683</lpage>
        <pub-id pub-id-type="doi">10.1016/S0092-8674(01)00495-0</pub-id>
        <pub-id pub-id-type="pmid">11572774</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marino</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Gladyshev</surname>
            <given-names>VN</given-names>
          </name>
        </person-group>
        <article-title>Structural analysis of cysteine S-nitrosylation: a modified acid-based motif and the emerging role of trans-nitrosylation</article-title>
        <source>J Mol Biol</source>
        <year>2010</year>
        <volume>395</volume>
        <issue>4</issue>
        <fpage>844</fpage>
        <lpage>859</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmb.2009.10.042</pub-id>
        <pub-id pub-id-type="pmid">19854201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Marletta</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Mechanisms of S-nitrosothiol formation and selectivity in nitric oxide signaling</article-title>
        <source>Curr Opin Chem Biol</source>
        <year>2012</year>
        <volume>16</volume>
        <issue>5–6</issue>
        <fpage>498</fpage>
        <lpage>506</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cbpa.2012.10.016</pub-id>
        <pub-id pub-id-type="pmid">23127359</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>GPS-SNO: computational prediction of protein S-nitrosylation sites with a modified GPS algorithm</article-title>
        <source>PLoS ONE</source>
        <year>2010</year>
        <volume>5</volume>
        <issue>6</issue>
        <fpage>e11290</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0011290</pub-id>
        <pub-id pub-id-type="pmid">20585580</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>TY</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SNOSite: exploiting maximal dependence decomposition to identify cysteine S-nitrosylation with substrate site specificity</article-title>
        <source>PLoS ONE</source>
        <year>2011</year>
        <volume>6</volume>
        <issue>7</issue>
        <fpage>e21849</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0021849</pub-id>
        <pub-id pub-id-type="pmid">21789187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>iSNO-PseAAC: predict cysteine S-nitrosylation sites in proteins by incorporating position specific amino acid propensity into pseudo amino acid composition</article-title>
        <source>PLoS ONE</source>
        <year>2013</year>
        <volume>8</volume>
        <issue>2</issue>
        <fpage>e55844</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0055844</pub-id>
        <pub-id pub-id-type="pmid">23409062</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hao</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SNOSID, a proteomic method for identification of cysteine S-nitrosylation sites in complex protein mixtures</article-title>
        <source>Proc Natl Acad Sci USA</source>
        <year>2006</year>
        <volume>103</volume>
        <issue>4</issue>
        <fpage>1012</fpage>
        <lpage>1017</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0508412103</pub-id>
        <pub-id pub-id-type="pmid">16418269</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pakhrin</surname>
            <given-names>SC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning-based advances in protein posttranslational modification site and protein cleavage prediction</article-title>
        <source>Methods Mol Biol</source>
        <year>2022</year>
        <volume>2499</volume>
        <fpage>285</fpage>
        <lpage>322</lpage>
        <pub-id pub-id-type="doi">10.1007/978-1-0716-2317-6_15</pub-id>
        <pub-id pub-id-type="pmid">35696087</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meng</surname>
            <given-names>LK</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Mini-review: recent advances in post-translational modification site prediction based on deep learning</article-title>
        <source>Comput Struct Biotechnol J</source>
        <year>2022</year>
        <volume>20</volume>
        <fpage>3522</fpage>
        <lpage>3532</lpage>
        <pub-id pub-id-type="doi">10.1016/j.csbj.2022.06.045</pub-id>
        <pub-id pub-id-type="pmid">35860402</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepNitro: prediction of protein nitration and nitrosylation sites by deep learning</article-title>
        <source>Genom Proteom Bioinform</source>
        <year>2018</year>
        <volume>16</volume>
        <issue>4</issue>
        <fpage>294</fpage>
        <lpage>306</lpage>
        <pub-id pub-id-type="doi">10.1016/j.gpb.2018.04.007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hasan</surname>
            <given-names>MM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prediction of S-nitrosylation sites by integrating support vector machines and random forest</article-title>
        <source>Mol Omics</source>
        <year>2019</year>
        <volume>15</volume>
        <issue>6</issue>
        <fpage>451</fpage>
        <lpage>458</lpage>
        <pub-id pub-id-type="doi">10.1039/C9MO00098D</pub-id>
        <pub-id pub-id-type="pmid">31710075</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Recent advances in predicting protein S-nitrosylation sites</article-title>
        <source>Biomed Res Int</source>
        <year>2021</year>
        <volume>2021</volume>
        <fpage>5542224</fpage>
        <pub-id pub-id-type="pmid">33628788</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Jumper J, et al. Highly accurate protein structure prediction with AlphaFold. Nature. 2021;596(7873): p. 583-+.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Badal</surname>
            <given-names>VD</given-names>
          </name>
          <name>
            <surname>Kundrotas</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Vakser</surname>
            <given-names>IA</given-names>
          </name>
        </person-group>
        <article-title>Natural language processing in text mining for structural modeling of protein complexes</article-title>
        <source>BMC Bioinform</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>1</issue>
        <fpage>84</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-018-2079-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Pokharel S, et al. NLP-based encoding techniques for prediction of post-translational modification sites and protein functions. In: K. Lukasz (ed) Machine learning in bioinformatics of protein sequences: algorithms, databases and resources for modern protein bioinformatics. World Scientific Publishing Company. 2023.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Elnaggar</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ProtTrans: towards cracking the language of lifes code through self-supervised deep learning and high performance computing</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell.</source>
        <year>2021</year>
        <volume>44</volume>
        <issue>10</issue>
        <fpage>7112</fpage>
        <lpage>7127</lpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Rives A, et al., Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc Natl Acad Sci USA. 2021;118(15).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Brandes N, et al. ProteinBERT: a universal deep-learning model of protein sequence and function. Bioinformatics. 2022.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Rives A, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc Natl Acad Sci USA. 2021. 118(15).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heinzinger</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Contrastive learning on protein embeddings enlightens midnight zone</article-title>
        <source>NAR Genom Bioinform</source>
        <year>2022</year>
        <volume>4</volume>
        <issue>2</issue>
        <fpage>lqac043</fpage>
        <pub-id pub-id-type="doi">10.1093/nargab/lqac043</pub-id>
        <pub-id pub-id-type="pmid">35702380</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Littmann</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein embeddings and deep learning predict binding residues for various ligand classes</article-title>
        <source>Sci Rep</source>
        <year>2021</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>23916</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-021-03431-4</pub-id>
        <pub-id pub-id-type="pmid">34903827</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pokharel</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improving protein succinylation sites prediction using embeddings from protein language model</article-title>
        <source>Sci Rep</source>
        <year>2022</year>
        <volume>12</volume>
        <issue>1</issue>
        <fpage>16933</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-022-21366-2</pub-id>
        <pub-id pub-id-type="pmid">36209286</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Godzik</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>
        <source>Bioinformatics</source>
        <year>2006</year>
        <volume>22</volume>
        <issue>13</issue>
        <fpage>1658</fpage>
        <lpage>1659</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btl158</pub-id>
        <pub-id pub-id-type="pmid">16731699</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raffel</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Exploring the limits of transfer learning with a unified text-to-text transformer</article-title>
        <source>J Mach Learn Res.</source>
        <year>2020</year>
        <volume>21</volume>
        <fpage>5485</fpage>
        <lpage>5551</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Introduction to convolutional neural network using Keras: an understanding from a statistician</article-title>
        <source>Commun Stat Appl Methods</source>
        <year>2019</year>
        <volume>26</volume>
        <issue>6</issue>
        <fpage>591</fpage>
        <lpage>610</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep neural network based predictions of protein interactions using primary sequences</article-title>
        <source>Molecules.</source>
        <year>2018</year>
        <volume>23</volume>
        <issue>8</issue>
        <fpage>1923</fpage>
        <pub-id pub-id-type="doi">10.3390/molecules23081923</pub-id>
        <pub-id pub-id-type="pmid">30071670</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ting</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Witten</surname>
            <given-names>IH</given-names>
          </name>
        </person-group>
        <article-title>Issues in stacked generalization</article-title>
        <source>J Artif Intell Res</source>
        <year>1999</year>
        <volume>10</volume>
        <fpage>271</fpage>
        <lpage>289</lpage>
        <pub-id pub-id-type="doi">10.1613/jair.594</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolpert</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>Stacked generalization</article-title>
        <source>Neural Netw</source>
        <year>1992</year>
        <volume>5</volume>
        <issue>2</issue>
        <fpage>241</fpage>
        <lpage>259</lpage>
        <pub-id pub-id-type="doi">10.1016/S0893-6080(05)80023-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hearst</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Support vector machines</article-title>
        <source>IEEE Intell Syst Their Appl</source>
        <year>1998</year>
        <volume>13</volume>
        <issue>4</issue>
        <fpage>18</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1109/5254.708428</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <year>2001</year>
        <volume>45</volume>
        <issue>1</issue>
        <fpage>5</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van der Maaten</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Visualizing data using t-SNE</article-title>
        <source>J Mach Learn Res</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>2579</fpage>
        <lpage>2605</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>BW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A novel method to predict drug-target interactions based on large-scale graph representation learning</article-title>
        <source>Cancers (Basel).</source>
        <year>2021</year>
        <volume>13</volume>
        <issue>9</issue>
        <fpage>2111</fpage>
        <pub-id pub-id-type="doi">10.3390/cancers13092111</pub-id>
        <pub-id pub-id-type="pmid">33925568</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Hu L, et al. A survey on computational models for predicting protein–protein interactions. Brief Bioinform. 2021;22(5).</mixed-citation>
    </ref>
  </ref-list>
</back>
