<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 2?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Cheminform</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Cheminform</journal-id>
    <journal-title-group>
      <journal-title>Journal of Cheminformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1758-2946</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5123367</article-id>
    <article-id pub-id-type="publisher-id">179</article-id>
    <article-id pub-id-type="doi">10.1186/s13321-016-0179-6</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Towards agile large-scale predictive modelling in drug discovery with flow-based programming design principles</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Lampa</surname>
          <given-names>Samuel</given-names>
        </name>
        <address>
          <email>samuel.lampa@farmbio.uu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Alvarsson</surname>
          <given-names>Jonathan</given-names>
        </name>
        <address>
          <email>jonathan.alvarsson@farmbio.uu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8083-2864</contrib-id>
        <name>
          <surname>Spjuth</surname>
          <given-names>Ola</given-names>
        </name>
        <address>
          <email>ola.spjuth@farmbio.uu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Department of Pharmaceutical Biosciences, Uppsala University, Box 591, 751 24 Uppsala, Sweden </aff>
      <aff id="Aff2"><label>2</label>Science for Life Laboratory, Uppsala University, Box 3037, 750 03 Uppsala, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>11</month>
      <year>2016</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>11</month>
      <year>2016</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2016</year>
    </pub-date>
    <volume>8</volume>
    <elocation-id>67</elocation-id>
    <history>
      <date date-type="received">
        <day>19</day>
        <month>7</month>
        <year>2016</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>11</month>
        <year>2016</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2016</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p> Predictive modelling in drug discovery is challenging to automate as it often contains multiple analysis steps and might involve cross-validation and parameter tuning that create complex dependencies between tasks. With large-scale data or when using computationally demanding modelling methods, e-infrastructures such as high-performance or cloud computing are required, adding to the existing challenges of fault-tolerant automation. Workflow management systems can aid in many of these challenges, but the currently available systems are lacking in the functionality needed to enable agile and flexible predictive modelling. We here present an approach inspired by elements of the flow-based programming paradigm, implemented as an extension of the Luigi system which we name SciLuigi. We also discuss the experiences from using the approach when modelling a large set of biochemical interactions using a shared computer cluster.<fig position="anchor" id="Figa"><label>Graphical abstract</label><caption><p>.</p></caption><graphic position="anchor" xlink:href="13321_2016_179_Figa_HTML" id="MO10"/></fig>
</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Predictive modelling</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Workflows</kwd>
      <kwd>Drug discovery</kwd>
      <kwd>Flow-based programming</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2016</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Predictive modelling is widely used in drug discovery with applications including prediction of interaction, inhibition and toxicity [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. In ligand-based approaches, such as quantitative structure-activity relationship (QSAR), machine learning is commonly used to correlate chemical structures with activity while ligands are described numerically using descriptors [<xref ref-type="bibr" rid="CR4">4</xref>]. Such modelling efforts consist of a set of computational tasks that are commonly invoked manually or with shell scripts that glue together multiple tasks into a simple form of pipeline. A computational task not uncommon in machine learning for drug discovery is a set of cross-validations nested with parameter sweeps to find optimal parameters for the model training. Such intricate sets of computations create complex task dependencies that are not always easy to encode in existing tools, if at all possible. Furthermore, as data sizes increase there is a need to use high-performance e-infrastructures such as compute clusters or cloud resources to carry out analyses. These add their own requirements, making reproducible, fault-tolerant automation even more difficult to achieve [<xref ref-type="bibr" rid="CR5">5</xref>].</p>
    <p>Scientific workflow management systems (WMS) are a possible solution in this context as they provide improved maintainability and robustness to failure over plain shell scripts. They provide this by describing the set of computations, the data they use and the dependencies between them in a generic way. Lower level details such as the logistics of data handling and task scheduling are left to the WMS. By hiding such technical details, they allow the researcher to focus on the research problem at hand when authoring the workflow rather than getting bogged down with peripheral matters. Thus, modifying the workflow connectivity becomes less complex and error-prone.</p>
    <p>Commonly used workflow tools for predictive modelling in drug discovery include KNIME [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>] and PipelinePilot [<xref ref-type="bibr" rid="CR8">8</xref>], where KNIME is an open source software with proprietary extensions and PipelinePilot is a proprietary software application. Both provide user interaction via a graphical user interface (GUI) where researchers can drag and drop components and build workflows for predictive modelling, among other things. While a GUI has clear advantages over text-based user interfaces for scientists lacking expertise in scripting or programming, it is unclear whether it provides any advantages over text-based user interfaces in terms of efficiency for expert users [<xref ref-type="bibr" rid="CR9">9</xref>]. Graphical rich clients typically put more requirements on the computer on which they are run, such as requiring a graphical desktop system, which is not always available on HPC systems. This means that the tool can not be deployed fully to such HPC systems. Instead, the graphical client has to be run on the user’s local computer even when the jobs are executed remotely. Also, even when a graphical desktop system is available on an HPC system, performance reasons might make it impractical to access a graphical client over a secure shell (SSH) connection, as is often needed.</p>
    <p>KNIME, by being the only open source tool of the mentioned tools, might be considered a good default choice for the types of use cases discussed in this study. However, the open source version of KNIME does not support HPC (“remote execution”), creation of libraries of custom, re-usable components (“custom node repository”) or detailed audit logging [<xref ref-type="bibr" rid="CR10">10</xref>], all of which are features of vital importance to the use cases discussed in this paper. See Table <xref rid="Tab1" ref-type="table">1</xref> for a comparison between KNIME and the solution presented in this paper.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Feature comparison: KNIME open source versus “Vanilla” Luigi and SciLuigi</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Feature</th><th align="left">KNIME open source</th><th align="left">Luigi</th><th align="left">SciLuigi</th></tr></thead><tbody><tr><td align="left">Authoring interface</td><td align="left">GUI (rich client)</td><td align="left">Text / CLI</td><td align="left">Text / CLI</td></tr><tr><td align="left">Implementation language</td><td align="left">Java</td><td align="left">Python</td><td align="left">Python</td></tr><tr><td align="left">Scheduling mode</td><td align="left">Independent threads</td><td align="left">Pull</td><td align="left">Pull</td></tr><tr><td align="left">HPC support</td><td align="left">No</td><td align="left">No</td><td align="left">Yes</td></tr><tr><td align="left">Custom re-usable components</td><td align="left">No</td><td align="left">No</td><td align="left">Yes</td></tr><tr><td align="left">Audit trail</td><td align="left">No</td><td align="left">No</td><td align="left">Yes</td></tr><tr><td align="left">Sub-workflows</td><td align="left">Yes</td><td align="left">No</td><td align="left">No</td></tr><tr><td align="left">Named ports</td><td align="left">Yes</td><td align="left">No</td><td align="left">Yes</td></tr><tr><td align="left">Nested loops</td><td align="left">Yes</td><td align="left">No</td><td align="left">Yes</td></tr><tr><td align="left">Interactive workflow debugging</td><td align="left">Partly</td><td align="left">Yes</td><td align="left">Yes</td></tr><tr><td align="left">CLI tool integration</td><td align="left">Yes</td><td align="left">Yes</td><td align="left">Yes</td></tr><tr><td align="left">Stream processing</td><td align="left">Yes</td><td align="left">No</td><td align="left">No</td></tr><tr><td align="left">Graphical workflow visualization</td><td align="left">Yes</td><td align="left">Yes</td><td align="left">Yes</td></tr><tr><td align="left">Supports scripting</td><td align="left">Yes</td><td align="left">Yes</td><td align="left">Yes</td></tr></tbody></table></table-wrap>
</p>
    <p>In the wider field of bioinformatics there are numerous scientific workflow tools available for analysis, e.g., in genomics and proteomics [<xref ref-type="bibr" rid="CR11">11</xref>], but a big proportion of these tools have various characteristics that limit their usefulness in highly complex analyses, such as when combining cross-validation and parameter sweeps. Furthermore, some tools do not support defining custom, re-usable components that can be assembled <italic>ad hoc</italic> for new workflows. In many WMS tools, complex workflows cannot be created without combining the workflow tool with shell scripts [<xref ref-type="bibr" rid="CR12">12</xref>], pointing to their limitations for complex use cases.</p>
    <p>Galaxy [<xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR15">15</xref>] and Yabi [<xref ref-type="bibr" rid="CR16">16</xref>] are GUI-centric tools or frameworks with a client/server architecture that require the installation of a server daemon and meta data to support automatic GUI generation. By their GUI-centric nature, they do not allow a level of programmability similar to the text-based tools, meaning that it is not equally easy to use programmatic constructs such as loops to automate repetitive workflow patterns such as parameter sweeps. Galaxy supports a REST-interface [<xref ref-type="bibr" rid="CR17">17</xref>] that can be used to provide this type of programmability, but this requires interfacing the tool with external scripts outside of the tool itself.</p>
    <p>Snakemake [<xref ref-type="bibr" rid="CR18">18</xref>], NextFlow [<xref ref-type="bibr" rid="CR19">19</xref>] and BPipe [<xref ref-type="bibr" rid="CR20">20</xref>] are text-based tools implemented as Domain Specific Languages (DSL). DSLs are mini-languages created specifically for the need of a specific domain [<xref ref-type="bibr" rid="CR21">21</xref>], such as the topic at hand, scientific workflows. While DSLs can simplify workflow writing by allowing the workflows to be defined in a language that more closely maps to the problem at hand [<xref ref-type="bibr" rid="CR22">22</xref>], they often impose limits on the types of workflows that can easily be modelled without having to modify the language itself [<xref ref-type="bibr" rid="CR23">23</xref>]. They also often require <italic>ad hoc</italic> solutions for integrating with existing version control software, editors and debuggers [<xref ref-type="bibr" rid="CR21">21</xref>]. Thus, DSLs can be too limiting for highly complex workflow constructs such as those in machine learning for drug discovery. This was perceived to be the case with Snakemake and BPipe. NextFlow’s DSL allows more flexibility due to its dataflow-based implementation, but does not support creating a library of reusable component definitions. Instead, NextFlow requires components to be defined in conjunction with the workflow definition [<xref ref-type="bibr" rid="CR24">24</xref>].</p>
    <p>Ruffus [<xref ref-type="bibr" rid="CR25">25</xref>] and Luigi [<xref ref-type="bibr" rid="CR26">26</xref>] are text-based tools exposed as programming libraries, meaning that their functionality is supposed to be used from within an existing scripting language such as Python. As programming libraries, they generally require more code for defining workflows compared to DSL tools but on the other hand provide greater flexibility, as they allow users to make use of the full power of the generic programming language in which they were implemented [<xref ref-type="bibr" rid="CR27">27</xref>].</p>
    <p>While Ruffus provides an API based on decorators, Luigi provides an object-oriented programming API, which can be perceived as more familiar to some developers. Luigi also allows more control over output file naming than Ruffus. Furthermore it has support for the Apache Hadoop [<xref ref-type="bibr" rid="CR28">28</xref>] and Apache Spark [<xref ref-type="bibr" rid="CR29">29</xref>] execution environments together with support for the local file system in the same framework. Figure <xref rid="Fig1" ref-type="fig">1</xref> gives an overview over Luigi’s relation to other workflow tools. Luigi thus was perceived as one of the most promising tools for use in the type of analyses described in this paper.<fig id="Fig1"><label>Fig. 1</label><caption><p>Sunburst diagram showing the hierarchical structure of the workflow tool landscape and Luigi’s position therein</p></caption><graphic xlink:href="13321_2016_179_Fig1_HTML" id="MO1"/></fig>
</p>
    <p>Despite these advantages, Luigi has shortcomings in some areas that can lead to brittle and hard-to-maintain workflow code limiting its applicability to complex analyses in drug discovery.</p>
    <p>Flow-based programming (see the “<xref rid="Sec15" ref-type="sec">Methods</xref>” section for details) is a paradigm developed for general purpose programs, suggesting a set of core design principles for achieving robust yet easy to modify component-oriented systems—a good description of what scientific workflow systems are aimed to be.</p>
    <p>With this in mind, we present below a solution for agile development of highly complex workflows in machine learning for drug discovery, based on selected design principles from flow-based programming combined with the Luigi workflow framework, which we have named SciLuigi. In addition, functionality commonly used in scientific workflows has been added that was not included in vanilla Luigi, such as support for an HPC resource manager and audit logging capabilities.</p>
    <p>The solution is demonstrated on a machine learning problem for modelling a large set of biochemical interactions using a shared computer cluster. Note that evaluating the actual modelling, and evaluation of the performance thereof, is outside the scope of this article, which is instead focused on solutions for the automation and coordination of such workflows, rather than the computational modelling methods themselves.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <p>Agile development of complex workflows in machine learning for drug discovery requires adequate workflow management tools that support the complexity of these analyses. To this end, we have developed a solution based on the Luigi workflow library.</p>
    <p>Compared with KNIME, the SciLuigi solution presented here provides three additions of vital importance for machine learning workflows in drug discovery: HPC support, ability to create a library of custom, re-usable components and detailed audit trails. See Table <xref rid="Tab1" ref-type="table">1</xref> for a detailed point-by-point comparison between KNIME, Luigi and SciLuigi.</p>
    <p>As described in detail in the “<xref rid="Sec15" ref-type="sec">Methods</xref>” section, Luigi has a number of limitations in relation to complex analyses for machine learning in drug discovery. To overcome these limitations, we extended Luigi with a selected set of design principles from the flow-based programming paradigm in an improved API. The resulting solution was packaged into a programming library named SciLuigi, which is available as open source on GitHub [<xref ref-type="bibr" rid="CR30">30</xref>].</p>
    <p>A summary of how the limitations in Luigi were solved in the SciLuigi approach is shown below.</p>
    <sec id="Sec3">
      <title>Separation of workflow definition from tasks</title>
      <p>Agile workflow design requires the ability to quickly re-wire workflow connectivity. This is not easily done if dependencies are hard-coded inside task definitions, as is done in Luigi. Thus, inspired by the flow-based programming principle of <italic>separate network definition</italic> (see “<xref rid="Sec15" ref-type="sec">Methods</xref>” section for details) we developed a programming API in SciLuigi that enables tasks to be instantiated and connected without changing their internal definition. See Fig. <xref rid="Fig2" ref-type="fig">2</xref> for a code example demonstrating this.<fig id="Fig2"><label>Fig. 2</label><caption><p>Code example of a simple workflow and tasks defined in SciLuigi. Out-port fields are functions that return a TargetInfo object, containing all info needed to retrieve both the target (file) with the data, as well as the task that produced it. In-port fields are assigned TargetInfo-returning methods from upstream tasks in the workflow definition, which is why we can write code that uses them in the out-port and run methods. See <italic>line 18</italic> for how the workflow connectivity is defined, by assigning the output of an out-port to an in-port</p></caption><graphic xlink:href="13321_2016_179_Fig2_HTML" id="MO2"/></fig>
</p>
    </sec>
    <sec id="Sec4">
      <title>Avoiding parameter duplication</title>
      <p>Another problem hindering agile workflow design in Luigi is unnecessary parameter duplication. Since upstream tasks in Luigi are instantiated inside the downstream tasks that depend on them, parameters have to be defined in all tasks downstream of the task in which they are used, just to forward their values. This creates an exponentially increasing amount of API dependencies between tasks that are not closely related. For large and complex workflows, this substantial maintenance overhead hinders agile workflow development. We have solved this in SciLuigi by embedding the workflow definition code in a workflow object which subclasses Luigi’s Task class.</p>
      <p>This allows inputs to be defined on the workflow object and for parameter values to be passed directly to the tasks that use them. Unnecessary dependencies between tasks, and duplicated code, are thus avoided, resulting in a more agile workflow development. See Fig. <xref rid="Fig3" ref-type="fig">3</xref> for an illustration of this problem and how it is solved in SciLuigi.<fig id="Fig3"><label>Fig. 3</label><caption><p>Solving the problem of duplicated parameters. This figure shows: <bold>a</bold> How parameters are defined and forwarded in vanilla Luigi; <bold>b</bold> How parameters are defined and forwarded in SciLuigi, using a wrapping workflow task; and <bold>c</bold> How SciLuigi also defines dependencies between the data on in- and out-ports rather than only directly between tasks. Commenting in more detail we see in <bold>a</bold> how every parameter definition has to be repeated for every downstream task, from where it was first defined. In <bold>b</bold>, we see that when the workflow is wrapped in a task, parameters only need to be duplicated once (in the workflow task). In <bold>c</bold>, we see how—in SciLuigi—dependencies are defined between in-ports and out-ports instead of directly between tasks. This means that task-specific information about names or positions of out-ports of upstream tasks are not contaminating downstream tasks</p></caption><graphic xlink:href="13321_2016_179_Fig3_HTML" id="MO3"/></fig>
</p>
    </sec>
    <sec id="Sec5">
      <title>Workflow definition in terms of data—not task dependencies</title>
      <p>For scientific workflows it is important to be able to specify task dependencies in terms of data and not only in terms of the finished execution of upstream tasks. This is because bioinformatics tools commonly produce or consume more than one data set at a time, making it important to capture this level of detail in the dependency specification. If not captured in the workflow definition, this logic needs to be captured in custom logic inside task definitions, creating hidden dependencies between tasks and hindering agile workflow development. See Fig. <xref rid="Fig4" ref-type="fig">4</xref> for an example of this.<fig id="Fig4"><label>Fig. 4</label><caption><p>Code example showing two tasks connected into a simple workflow in vanilla Luigi. The task MyFooReplacer depends on MyFooWriter. Note that there is no central workflow definition, but that dependencies are specified within individual tasks in their requires() method (in this case only in the MyFooReplacer). The parts highlighted with yellow in MyFooReplacer on lines 19 and 23 contain information that is specific to the upstream task MyFooWriter. This means that MyFooReplacer is not independent from this upstream task, and can thus not be connected to other upstream tasks without modifying its internal code</p></caption><graphic xlink:href="13321_2016_179_Fig4_HTML" id="MO4"/></fig>
</p>
      <p>To meet this need, we have developed an approach inspired by the principle of <italic>named ports</italic> from flow-based programming, in which each input and output of a workflow component is given a name and the workflow dependencies are defined between such pairs of inputs and outputs rather than between tasks. See Fig. <xref rid="Fig3" ref-type="fig">3</xref>c for an illustration of this.</p>
      <p>In detailed terms, task outputs in SciLuigi are defined by implementing methods with a special naming scheme starting with out_ and followed by a unique name. These methods return an object (of type TargetInfo), which keeps track of both the name of the file created for that output and a reference to the task that produced the output. Inputs are similarly defined by fields with a naming scheme starting with in_ followed by a unique name. Inputs are plain fields while outputs are implemented as methods. When developing workflows, these output methods are connected to the input fields of downstream tasks with a syntax similar to variable assignment. Downstream tasks will use the file name in the received (TargetInfo) object to find the data it needs as input and the reference to the upstream task to provide information about tasks it depends on. See Fig. <xref rid="Fig2" ref-type="fig">2</xref> for a code example demonstrating this.</p>
    </sec>
    <sec id="Sec6">
      <title>Automatic audit information</title>
      <p>In scientific workflows it is important to have a complete track record of what has been executed, including the command name, parameter values and execution times. Since Luigi lacks this feature out-of-the-box, we have extended Luigi with auditing functionality that stores important data about each execution of a task in a structured and easy to parse data format. This information is kept separate from the normal log function in Luigi to enable improved machine-readability.</p>
    </sec>
    <sec id="Sec7">
      <title>Helper functionality for running shell commands as batch HPC jobs</title>
      <p>Scientific workflows are commonly executed on e-infrastructures, such as High-Performance Computing (HPC) clusters, as well as on users’ local computers. To support this we have extended Luigi with helper methods that allow the choice between executing shell commands either as HPC jobs or on the local computer, based on a configuration parameter passed to the task.</p>
    </sec>
    <sec id="Sec8">
      <title>Case study</title>
      <p>In order to demonstrate the features of SciLuigi, we applied it to an example QSAR modelling application.</p>
      <sec id="Sec9">
        <title>Introduction</title>
        <p>The LIBLINEAR software [<xref ref-type="bibr" rid="CR31">31</xref>] constitutes a fast SVM implementation based on linear SVM. This case study is set up as a small study of the effect of training set size on modelling time and model performance. These kinds of studies can result in non-trivial workflows due to nested cross-validation and parameter sweeps needed to properly tune model parameters and evaluate model performance.</p>
      </sec>
      <sec id="Sec10">
        <title>Materials and methods</title>
        <p>We trained QSAR models using the LIBLINEAR software [<xref ref-type="bibr" rid="CR31">31</xref>] with molecules described by the signature descriptor [<xref ref-type="bibr" rid="CR32">32</xref>]. For linear SVM, the <italic>cost</italic> parameter needs to be tuned. We tested 15 values (0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1, 2, 3, 4, 5) in a 10-fold, cross-validated parameter sweep. Five different training set sizes (500, 1000, 2000, 4000, 8000) were tested and evaluated with a test set size of 1000. The data set consisted of 10,000 logarithmic solubility values chosen randomly from the 37,099 data points that were not given as ’larger than’ in a data set from Pubchem [<xref ref-type="bibr" rid="CR33">33</xref>]. SciLuigi was used to design and orchestrate the workflow using the components schematically outlined in Fig. <xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig5"><label>Fig. 5</label><caption><p>A representative part of the real life workflow from the example application. Note in particular the output “sparse training set” that is used in multiple downstream locations. In other words, the same data set is used by multiple processes in the workflow</p></caption><graphic xlink:href="13321_2016_179_Fig5_HTML" id="MO5"/></fig>
</p>
      </sec>
      <sec id="Sec11">
        <title>Results</title>
        <p>The plot in Fig. <xref rid="Fig6" ref-type="fig">6</xref> shows the execution time and performance for the training set sizes tested. The best cost values vary between 0.05 and 0.1 for the different training set sizes.<fig id="Fig6"><label>Fig. 6</label><caption><p>Jupyter notebook running the case study workflow. This <italic>screenshot</italic> shows the case study workflow running in a Jupyter notebook inside the virtual machine provided with the case study. The same Jupyter notebook is shown in two web browser windows arranged horizontally to enable showing multiple parts of the notebook in the same image. The <italic>left side</italic> of the image shows part of the workflow definition code, while the <italic>right side</italic> shows part of code for plotting the resulting values from the workflow run, and the resulting plot of RMSD values and training times, plotted against data set sizes</p></caption><graphic xlink:href="13321_2016_179_Fig6_HTML" id="MO6"/></fig>
</p>
      </sec>
      <sec id="Sec12">
        <title>Discussion and conclusion</title>
        <p>From the plot in Fig. <xref rid="Fig6" ref-type="fig">6</xref> it can be seen that this modelling approach does not work very well for really small data sets. This type of problem is greatly simplified using a workflow system with its workflow definition separated from task definitions and named ports, which allows connecting each input and output of each task independently.</p>
        <p>This case study is a subset of a larger study carried out previously [<xref ref-type="bibr" rid="CR34">34</xref>], which made use of 9 different training set sizes and also encompassed more computationally demanding modelling using LibSVM with the RBF kernel. As a runnable demonstration of the workflow, we also provide a virtual machine with a complete setup including a Jupyter [<xref ref-type="bibr" rid="CR35">35</xref>, <xref ref-type="bibr" rid="CR36">36</xref>] notebook for running and replicating the case study. This virtual machine is available as a pre-made image at [<xref ref-type="bibr" rid="CR37">37</xref>], while the code for creating the virtual image, including the workflow code and all dependencies, is available at [<xref ref-type="bibr" rid="CR38">38</xref>]. See Fig. <xref rid="Fig6" ref-type="fig">6</xref> for a screenshot of the Jupyter notebook where the case study workflow is being executed. Managing such large quantities of jobs (see Fig. <xref rid="Fig7" ref-type="fig">7</xref>) and resulting files in a fault-tolerant manner without a workflow management system is not a feasible approach. With the SciLuigi solution, the study could be carried out successfully on a shared computer cluster at the university high-performance computing centre [<xref ref-type="bibr" rid="CR34">34</xref>]. The complete workflow of this previous study is available at GitHub [<xref ref-type="bibr" rid="CR39">39</xref>].<fig id="Fig7"><label>Fig. 7</label><caption><p>Dependency graph shown in Luigi’s web based visualiser. The dependency graph shows the structure of part of a workflow consisting of cross-validation fold generation combined with multiple parameter sweeps, which generates the large number of tasks, represented by <italic>yellow</italic>, <italic>green</italic> or <italic>blue dots</italic> in the image. Note that in this visualization, “downstream” tasks are at the <italic>top</italic>, while the “upstream” tasks are <italic>below</italic>. The number of tasks in this particular workflow is over 6000</p></caption><graphic xlink:href="13321_2016_179_Fig7_HTML" id="MO7"/></fig>
</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Discussion</title>
    <p>The principle of <italic>separate network definition</italic>—the idea that connections are defined externally to the processes—is central to the flow-based programming paradigm, as it enables re-combining processes without changing their internals. This together with the idea of <italic>named ports</italic> for the inputs and outputs of processes, has proven very relevant in scientific workflow design as it allows an iterative, exploratory usage pattern which is common during the course of the scientific process. In the presented approach, these design principles have been applied in an improved API on top of the Luigi framework and demonstrated in workflow design for a machine learning problem in drug discovery. The presented approach, based on flow-based programming design principles, has turned out to make workflow code more manageable and less error-prone while also requiring drastically fewer code changes when adding new tasks to workflows.</p>
    <p>In fact, workflows could now be defined that we were unable to create in vanilla Luigi, due to the prohibitively complex workarounds needed to get them to work in vanilla Luigi. HPC support and a detailed audit trail were also found to be important for the sample application. In summary, the modelling efforts referred to would simply not have been possible to perform with Luigi without the development and application of the SciLuigi solution.</p>
    <p>Based on these experiences, we want to stress the importance in scientific workflows to (a) separate workflow connectivity information from task implementations and (b) allow connections between inputs and outputs of tasks to be defined separately. In other words, dependencies should be defined in terms of data rather than directly between tasks in order to capture all the information that constitute the workflow definition and which might change between workflow runs. If this is not done, one might end up in a situation where tasks have to be rewritten for each run, meaning that tasks are no longer fully re-usable. We also note that dependencies between tasks can always be inferred by the dependencies in the data produced by those tasks, so no information will be lost by defining dependencies in terms of the data.</p>
    <p>By wrapping the separated workflow definition in its own task, we were able to solve the problem of duplicated parameters. Together with the separated network definition, this was found to result in a more agile and flexible way of designing and implementing complex machine learning workflows.</p>
    <p>The fact that Luigi is a programming library has provided both benefits and drawbacks. A major advantage is that very complex workflow constructs can be constructed relatively easily, such as workflows that nest multiple parameter searches and cross-validation fold creation. For example, workflows with extensive, nested branchings, like in the aforementioned QSAR study [<xref ref-type="bibr" rid="CR34">34</xref>], can be naturally defined in SciLuigi by creating nested for-loops that instantiate the tasks, with one for-loop per branch-point in the workflow, be it a parameter sweep, cross-validation construct or something else. See an example of this from a real-world problem in [<xref ref-type="bibr" rid="CR40">40</xref>]. A drawback of Luigi being a programming library is that it does not come packed with all the convenience features that most WMS tools implemented as DSLs have, such as built-in audit logging and HPC resource manager integration. At the same time, being a programming library meant that it was relatively easy to work around the problems and limitations by extending it with the desired functionality without modifying the Luigi core library.</p>
    <p>In the QSAR study [<xref ref-type="bibr" rid="CR34">34</xref>], the audit feature of SciLuigi has turned out to be crucial for tracking errors and identifying mistakes in the workflow design as early as possible, and thus greatly helped to enable agile workflow design.</p>
    <p>Finally we note that workflows including tasks with a dynamic number of outputs supposed to be routed to different downstream tasks, is still an area with room for improvement. Such workflows can be handled by SciLuigi as long as the number of outputs can be calculated or retrieved in the workflow definition code—that is, in the scheduling phase of the workflow execution. On the other hand, if the number of outputs and subsequent number of downstream tasks to be instantiated can not be calculated at the time of scheduling the workflow, SciLuigi can not model this in a natural way. This situation can show up for example when splitting a data set of unknown size into chunks of a defined size or when reading data from a database and creating one new task per result row, both of which are not uncommon scenarios for workflows in drug discovery. We thus identify this as an important area for further research.</p>
  </sec>
  <sec id="Sec14">
    <title>Conclusions</title>
    <p>We present an approach for agile predictive modelling, combining design principles from flow-based programming with a workflow system. The developed SciLuigi library supports analysis of large data sets involving complex workflows with nested cross-validation and parameter sweeps, orchestrated on high-performance e-infrastructures. We envision that the approach will support data scientists in training and assessing machine learning models in drug discovery and related fields. The authors are aware of one company working in drug discovery testing out SciLuigi [<xref ref-type="bibr" rid="CR41">41</xref>]. As of this writing, the library has been bookmarked—or “star-marked”—93 times on GitHub and forked 20 times [<xref ref-type="bibr" rid="CR42">42</xref>], with at least 6 users sending in patches or suggestions for improvements.</p>
  </sec>
  <sec id="Sec15">
    <title>Methods</title>
    <sec id="Sec16">
      <title>QSAR modelling</title>
      <p>In quantitative structure-activity relationships (QSAR) molecular properties are modelled by describing molecules numerically using molecular descriptors and correlating these values to the properties [<xref ref-type="bibr" rid="CR43">43</xref>]. A common use case is to, by supervised machine learning, predict properties of molecules for which these properties are unknown. First the molecular descriptors are calculated and then a predictive model is constructed based on a set of known data. The model is said to “learn” or to “be trained”. The set of known data is often called a <italic>training set</italic> and the bigger the training set the better are the chances of getting a good model, i.e., the model has seen enough examples to adequately cover the relevant chemical space.</p>
      <p>For large data sets, the calculation of the molecular descriptors can require quite a lot of CPU time and so can the construction of the predictive model. Execution time of the descriptor calculation tends to increase linearly with increased training set size but the relationship between execution time for model building and training set size depends on the modelling approach. However, in general, better models require larger training sets, which in turn require more CPU hours. Many different molecular descriptors are in common use. We used molecular signatures [<xref ref-type="bibr" rid="CR32">32</xref>] which have proved to work well [<xref ref-type="bibr" rid="CR44">44</xref>–<xref ref-type="bibr" rid="CR46">46</xref>].</p>
      <p>Often, the QSAR model algorithms come with free parameters that need to be determined, e.g., support vector machines based on the radial basis function has the free parameters <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="italic">γ</mml:mi></mml:math><inline-graphic xlink:href="13321_2016_179_Article_IEq1.gif"/></alternatives></inline-formula> and <italic>cost</italic> [<xref ref-type="bibr" rid="CR47">47</xref>] and k-nearest neighbour has <italic>k</italic> [<xref ref-type="bibr" rid="CR46">46</xref>]. A common way of determining actual values for parameters such as these is a grid search or “parameter sweep”. A predetermined range of candidate values are tested one by one on a part of the training set and evaluated using another part of the training set as reference. The value that makes the model perform the best is then used to build the final model. This is commonly done using <italic>n</italic>-fold cross-validation in which the training set is split into <italic>n</italic> parts (often 10) and each of these parts is used as reference once, while the remaining parts in each such iteration are merged together as the training set. In the case of 10-fold cross-validation this means that 10 estimates are created where each is based on 90% of the training data. Finally, the mean or median of the predicted values for the parameter can be used when building the final model.</p>
    </sec>
    <sec id="Sec17">
      <title>Luigi</title>
      <p>Luigi is a batch workflow system written in Python and developed by the streaming music company Spotify, to help manage workloads of periodic analysis tasks like lists of top songs and artists for different periods of time. It is released as open source and freely available on GitHub [<xref ref-type="bibr" rid="CR26">26</xref>].</p>
      <p>Luigi is implemented as a programming library. In short, creating a task with its default API involves sub-classing the luigi.Task base class, adding fields for parameters and overriding a few class methods. Namely, the requires() method, which returns upstream tasks of the current task; the output() method, which returns all the outputs of the current task; and the run() method, which defines what the task does. An example definition of a task that depends on another task is available in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.</p>
      <p>Luigi provides automatic command-line interface generation based on parameters added to task classes. It also provides a central scheduler, a web-based workflow progress visualisation, logging facilities and a combination of support for Apache Hadoop [<xref ref-type="bibr" rid="CR28">28</xref>], Apache Spark [<xref ref-type="bibr" rid="CR29">29</xref>] and normal file systems in the same tool. Additionally, it includes a light-weight solution for distributing workloads across nodes without the need of a resource manager such as SLURM [<xref ref-type="bibr" rid="CR48">48</xref>].</p>
      <p>Luigi has certain characteristics which are problematic for complex use cases. A primary focus of Luigi is in situations with a relatively fixed workflow connectivity but with frequent variations in the parameter values, such as date ranges [<xref ref-type="bibr" rid="CR49">49</xref>]. This could be contrasted with scientific exploration where the workflow connectivity often varies extensively as well. More specifically, the relevant problems in Luigi are:<list list-type="order"><list-item><p>Dependencies are defined inside a tasks’ definition. The problem with this approach is that tasks are not fully independent and re-usable since they need to be rewritten every time the workflow connectivity changes. This means that tasks can not be kept in a common task library and plugged into workflows when needed.</p></list-item><list-item><p>Dependencies are specified directly between tasks rather than between the inputs and outputs of tasks. This means that tasks need to know the names of outputs of upstream tasks and need to implement code for looking up the correct output. This again ties two tasks together by their very definition such that they are not fully self-contained and interchangeable with other tasks that consume and produce data of the same format. This can be exemplified by the implementation of a Luigi task in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, lines 19 and 23, where we see that in the run() method of the downstream task, there is navigation code tied to the structure of an upstream task.</p></list-item><list-item><p>Parameter values in Luigi need to be provided each time a task is instantiated. This combined with the fact that tasks are instantiated inside downstream tasks during the scheduling phase, means that parameter values needed for a task’s instantiation also need to be known in all downstream tasks. In other words, the parameter value will need to be passed on all the way from the most downstream task of the workflow up to where it is actually used. This is illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a.</p></list-item></list>The above shortcomings imply that Luigi tasks are not fully independent – in many cases they need to contain parameter definitions not used by themselves but only serving to be passed to upstream tasks. Thus, swapping out one task in a workflow will require rewriting not only this task but also all downstream tasks (illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref>). This goes against the vision of agile ”pluggable”, component-oriented, iterative workflow construction, central to scientific computational use cases. However, this was not the primary focus when developing Luigi at Spotify [<xref ref-type="bibr" rid="CR49">49</xref>].</p>
    </sec>
    <sec id="Sec18">
      <title>Flow-based programming</title>
      <p>Flow-based programming (FBP) is a programming paradigm invented by John Paul Morrison at IBM in the late 1960’s to ease development of complex data processing programs in mainframe computers [<xref ref-type="bibr" rid="CR50">50</xref>, <xref ref-type="bibr" rid="CR51">51</xref>]. It is a definition for applications in general but many of the design patterns apply equally well to workflows, which can be seen as a form of application. In short, FBP is a specialised form of the dataflow programming paradigm [<xref ref-type="bibr" rid="CR52">52</xref>]. From dataflow, it takes the concept of “black box”, asynchronous processes which communicate via message passing over pre-defined connections. FBP adds the ideas of separate network definition, named in- and out-ports, channels with bounded buffers and information packets with defined lifetimes for the data exchange. From these principles, separate network definition and named ports are used in this study.</p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>DSL</term>
        <def>
          <p>domain-specific language</p>
        </def>
      </def-item>
      <def-item>
        <term>FBP</term>
        <def>
          <p>flow-based programming</p>
        </def>
      </def-item>
      <def-item>
        <term>GUI</term>
        <def>
          <p>graphical user interface</p>
        </def>
      </def-item>
      <def-item>
        <term>HPC</term>
        <def>
          <p>high-performance computing</p>
        </def>
      </def-item>
      <def-item>
        <term>QSAR</term>
        <def>
          <p>quantitative structure–activity relationship</p>
        </def>
      </def-item>
      <def-item>
        <term>WMS</term>
        <def>
          <p>workflow management system</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <title>Authors' contributions</title>
    <p>SL and OS conceived the project and approach. SL designed and implemented the SciLuigi library. JA, SL and OS contributed to the data analysis and example application. All authors read and approved the final manuscript.</p>
    <sec id="FPar1">
      <title>Acknowledgements</title>
      <p>The authors thank Wesley Schaal for proofreading the manuscript.</p>
    </sec>
    <sec id="FPar2">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="d30e1123">
      <title>Availability of data and materials</title>
      <p>The SciLuigi library is available at <ext-link ext-link-type="uri" xlink:href="http://github.com/pharmbio/sciluigi">http://github.com/pharmbio/sciluigi</ext-link>. The code for the study in which SciLuigi was used, is available at <ext-link ext-link-type="uri" xlink:href="http://github.com/pharmbio/mm%5fproject">http://github.com/pharmbio/mm_project</ext-link>.</p>
    </sec>
    <sec id="FPar3">
      <title>Funding</title>
      <p>This work was supported by the Swedish strategic research programme eSSENCE and the Swedish e-Science Research Centre (SeRC). The computations were performed on resources provided by SNIC through Uppsala Multidisciplinary Center for Advanced Computational Science (UPPMAX) under project b2013262. Furthermore, support by NBIS (National Bioinformatics Infrastructure Sweden) is gratefully acknowledged.</p>
    </sec>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Valerio</surname>
            <given-names>LG</given-names>
            <suffix>Jr</suffix>
          </name>
        </person-group>
        <article-title>Application of advanced in silico methods for predictive modeling and information integration</article-title>
        <source>Expert Opin Drug Metab Toxicol</source>
        <year>2012</year>
        <volume>8</volume>
        <issue>4</issue>
        <fpage>395</fpage>
        <lpage>398</lpage>
        <pub-id pub-id-type="doi">10.1517/17425255.2012.664636</pub-id>
        <pub-id pub-id-type="pmid">22432718</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gedeck</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Lewis</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>Exploiting QSAR models in lead optimization</article-title>
        <source>Curr Opin Drug Discov Dev</source>
        <year>2008</year>
        <volume>11</volume>
        <issue>4</issue>
        <fpage>569</fpage>
        <lpage>575</lpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Spycher</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Smejtek</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Netzeva</surname>
            <given-names>TI</given-names>
          </name>
          <name>
            <surname>Escher</surname>
            <given-names>BI</given-names>
          </name>
        </person-group>
        <article-title>Toward a class-independent quantitative structure-activity relationship model for uncouplers of oxidative phosphorylation</article-title>
        <source>Chem Res Toxicol</source>
        <year>2008</year>
        <volume>21</volume>
        <issue>4</issue>
        <fpage>911</fpage>
        <lpage>927</lpage>
        <pub-id pub-id-type="doi">10.1021/tx700391f</pub-id>
        <pub-id pub-id-type="pmid">18358007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Hansch C (1969) A quantitative approach to biochemical structure-activity relationships. Acc Chem Res 2:232–239</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Spjuth</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Bongcam-Rudloff</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hernández</surname>
            <given-names>GC</given-names>
          </name>
          <name>
            <surname>Forer</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Giovacchini</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Guimera</surname>
            <given-names>RV</given-names>
          </name>
          <name>
            <surname>Kallio</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Korpelainen</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kańduła</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Krachunov</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kreil</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Kulev</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Łabaj</surname>
            <given-names>PP</given-names>
          </name>
          <name>
            <surname>Lampa</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pireddu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Schönherr</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Siretskiy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vassilev</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Experiences with workflows for automating data-intensive bioinformatics</article-title>
        <source>Biol Direct</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1186/s13062-015-0071-8</pub-id>
        <pub-id pub-id-type="pmid">25564011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Berthold MR, Cebron N, Dill F, Gabriel TR, Kötter T, Meinl T, Ohl P, Sieb C, Thiel K, Wiswedel B (2007) KNIME: the Konstanz Information Miner. In: Studies in classification, data analysis, and knowledge organization (GfKL 2007). Springer, Berlin</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mazanetz</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Marmon</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Reisser</surname>
            <given-names>CBT</given-names>
          </name>
          <name>
            <surname>Morao</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Drug discovery applications for KNIME: an open source data mining platform</article-title>
        <source>Curr Top Med Chem</source>
        <year>2012</year>
        <volume>12</volume>
        <issue>18</issue>
        <fpage>1965</fpage>
        <lpage>1979</lpage>
        <pub-id pub-id-type="doi">10.2174/156802612804910331</pub-id>
        <pub-id pub-id-type="pmid">23110532</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">BIOVIA (2016) Pipeline pilot overview. <ext-link ext-link-type="uri" xlink:href="http://accelrys.com/products/collaborative-science/biovia-pipeline-pilot/">http://accelrys.com/products/collaborative-science/biovia-pipeline-pilot/</ext-link>. Accessed 5 April</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Chen J-W, Zhang J (2007) Comparing text-based and graphic user interfaces for novice and expert users. In: AMIA annual symposium proceedings, pp 125–129</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">KNIME Product Matrix. <ext-link ext-link-type="uri" xlink:href="https://www.knime.org/products/product-matrix">https://www.knime.org/products/product-matrix</ext-link>. Accessed 20 Sep 2016</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Leipzig J (2016) A review of bioinformatic pipeline frameworks. Brief Bioinform. doi:10.1093/bib/bbw020. pii: bbw020</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Breck E (2008) Zymake: a computational workflow system for machine learning and natural language processing. Software engineering, testing, and quality assurance for natural language processing, SETQA-NLP ’08 association for computational linguistics, Stroudsburg, pp 5–13</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goecks</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Nekrutenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences</article-title>
        <source>Genome Biol</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>8</issue>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1186/gb-2010-11-8-r86</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Blankenberg</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kuster</surname>
            <given-names>GV</given-names>
          </name>
          <name>
            <surname>Coraor</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Ananda</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Lazarus</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Mangan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nekrutenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Galaxy: a web-based genome analysis tool for experimentalists</source>
        <year>2010</year>
        <publisher-loc>Hoboken</publisher-loc>
        <publisher-name>John Wiley &amp; sons inc</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giardine</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Riemer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hardison</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Burhans</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Elnitski</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Blankenberg</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Albert</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Kent</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Nekrutenko</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Galaxy: a platform for interactive large-scale genome analysis</article-title>
        <source>Genome Res</source>
        <year>2005</year>
        <volume>15</volume>
        <issue>10</issue>
        <fpage>1451</fpage>
        <lpage>1455</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.4086505</pub-id>
        <pub-id pub-id-type="pmid">16169926</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hunter</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Macgregor</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Szabo</surname>
            <given-names>TO</given-names>
          </name>
          <name>
            <surname>Wellington</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Bellgard</surname>
            <given-names>MI</given-names>
          </name>
        </person-group>
        <article-title>Yabi: an online research environment for grid, high performance and cloud computing</article-title>
        <source>Source Code Biol Med</source>
        <year>2012</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1186/1751-0473-7-1</pub-id>
        <pub-id pub-id-type="pmid">22333270</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sloggett</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Goonasekera</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Afgan</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>BioBlend: automating pipeline analyses within Galaxy and CloudMan</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>13</issue>
        <fpage>1685</fpage>
        <lpage>1686</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt199</pub-id>
        <pub-id pub-id-type="pmid">23630176</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Köster</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rahmann</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Snakemake—a scalable bioinformatics workflow engine</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>19</issue>
        <fpage>2520</fpage>
        <lpage>2522</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts480</pub-id>
        <pub-id pub-id-type="pmid">22908215</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tommaso</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>Chatzou</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Baraja</surname>
            <given-names>PP</given-names>
          </name>
          <name>
            <surname>Notredame</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A novel tool for highly scalable computational pipelines</article-title>
        <source>Figshare</source>
        <year>2014</year>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sadedin</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Pope</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Oshlack</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Bpipe: a tool for running and managing bioinformatics pipelines</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>11</issue>
        <fpage>1525</fpage>
        <lpage>1526</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts167</pub-id>
        <pub-id pub-id-type="pmid">22500002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Spinellis</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Notable design patterns for domain-specific languages</article-title>
        <source>J Syst Softw</source>
        <year>2001</year>
        <volume>56</volume>
        <issue>1</issue>
        <fpage>91</fpage>
        <lpage>99</lpage>
        <pub-id pub-id-type="doi">10.1016/S0164-1212(00)00089-3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kosar</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Mernik</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>VJM</given-names>
          </name>
          <name>
            <surname>Črepinšek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Da</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Henriques</surname>
            <given-names>RP</given-names>
          </name>
        </person-group>
        <article-title>Comparing general-purpose and domain-specific languages: an empirical study</article-title>
        <source>Comput Sci Inf Syst</source>
        <year>2010</year>
        <volume>7</volume>
        <issue>2</issue>
        <fpage>247</fpage>
        <lpage>264</lpage>
        <pub-id pub-id-type="doi">10.2298/CSIS1002247K</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Deursen</surname>
            <given-names>AV</given-names>
          </name>
          <name>
            <surname>Klint</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Little languages: little maintenance?</article-title>
        <source>J Softw Maint Res Pract</source>
        <year>1998</year>
        <volume>10</volume>
        <issue>2</issue>
        <fpage>75</fpage>
        <lpage>92</lpage>
        <pub-id pub-id-type="doi">10.1002/(SICI)1096-908X(199803/04)10:2&lt;75::AID-SMR168&gt;3.0.CO;2-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Discussion on blog post. <ext-link ext-link-type="uri" xlink:href="http://bionics.it/posts/fbp-data-flow-syntax%23comment-2141038801">http://bionics.it/posts/fbp-data-flow-syntax#comment-2141038801</ext-link>. Accessed 18 April 2016</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goodstadt</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Ruffus: a lightweight Python library for computational pipelines</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <issue>21</issue>
        <fpage>2778</fpage>
        <lpage>2779</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq524</pub-id>
        <pub-id pub-id-type="pmid">20847218</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Luigi source code on GitHub. <ext-link ext-link-type="uri" xlink:href="https://github.com/spotify/luigi">https://github.com/spotify/luigi</ext-link>. Accessed 5 April 2016</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">van Deursen A (1997) Domain-specific languages versus object-oriented frameworks: a financial engineering case study. In: Smalltalk and Java in Industry and Academia, STJA’97, pp 35–39</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>White</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <source>Hadoop: the definitive guide</source>
        <year>2009</year>
        <edition>1</edition>
        <publisher-loc>Sebastopol</publisher-loc>
        <publisher-name>O’Reilly</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Zaharia M, Chowdhury M, Franklin MJ, Shenker S, Stoica I (2010) Spark: cluster computing with working sets. In: Proceedings of the 2nd USENIX conference on hot topics in cloud computing, pp 10</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">SciLuigi repository on Github. <ext-link ext-link-type="uri" xlink:href="http://github.com/pharmbio/sciluigi">http://github.com/pharmbio/sciluigi</ext-link>. Accessed 21 April 2016</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>R-E</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>K-W</given-names>
          </name>
          <name>
            <surname>Hsieh</surname>
            <given-names>C-J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X-R</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>C-J</given-names>
          </name>
        </person-group>
        <article-title>LIBLINEAR: a library for large linear classification</article-title>
        <source>J Mach Learn Res</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>1871</fpage>
        <lpage>1874</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Faulon</surname>
            <given-names>J-L</given-names>
          </name>
          <name>
            <surname>Visco</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Pophale</surname>
            <given-names>RS</given-names>
          </name>
        </person-group>
        <article-title>The signature molecular descriptor. 1. Using extended valence sequences in QSAR and QSPR studies</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>2003</year>
        <volume>43</volume>
        <issue>3</issue>
        <fpage>707</fpage>
        <lpage>720</lpage>
        <pub-id pub-id-type="doi">10.1021/ci020345w</pub-id>
        <pub-id pub-id-type="pmid">12767129</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">National Center for Biotechnology Information. PubChem BioAssay Database; AID=1996. <ext-link ext-link-type="uri" xlink:href="https://pubchem.ncbi.nlm.nih.gov/bioassay/1996">https://pubchem.ncbi.nlm.nih.gov/bioassay/1996</ext-link></mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alvarsson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lampa</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schaal</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wikberg</surname>
            <given-names>JES</given-names>
          </name>
          <name>
            <surname>Spjuth</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Large-scale ligand-based predictive modelling using support vector machines</article-title>
        <source>J Chem Inf</source>
        <year>2016</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>39</fpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pérez</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Granger</surname>
            <given-names>BE</given-names>
          </name>
        </person-group>
        <article-title>IPython: a system for interactive scientific computing</article-title>
        <source>Comput Sci Eng</source>
        <year>2007</year>
        <volume>9</volume>
        <issue>3</issue>
        <fpage>21</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.53</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Project Jupyter. <ext-link ext-link-type="uri" xlink:href="http://jupyter.org">http://jupyter.org</ext-link>. Accessed 18 Oct 2016</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Pre-made Virtual Machine image for the Case Study. <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.4038048">http://dx.doi.org/10.6084/m9.figshare.4038048</ext-link>. Accessed 18 Oct 2016. doi:10.6084/m9.figshare.4038048</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Github repository for the Case Study Virtual Machine. <ext-link ext-link-type="uri" xlink:href="https://github.com/pharmbio/bioimg-sciluigi-casestudy">https://github.com/pharmbio/bioimg-sciluigi-casestudy</ext-link>. Accessed 18 Oct 2016</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Project repository on Github. <ext-link ext-link-type="uri" xlink:href="http://github.com/pharmbio/mm%5fproject">http://github.com/pharmbio/mm_project</ext-link>. Accessed 21 April 2016</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Workflow file on Github. <ext-link ext-link-type="uri" xlink:href="https://github.com/pharmbio/mm%5fproject/blob/master/exp/20150627-crossval/wfmm.py">https://github.com/pharmbio/mm_project/blob/master/exp/20150627-crossval/wfmm.py</ext-link>. Accessed 21 April 2016</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">H3 Biomedicine’s fork of the SciLuigi source code on GitHub. <ext-link ext-link-type="uri" xlink:href="https://github.com/h3biomed/sciluigi">https://github.com/h3biomed/sciluigi</ext-link>. Accessed 18 April 2016</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Forks of the SciLuigi source code on GitHub. <ext-link ext-link-type="uri" xlink:href="https://github.com/pharmbio/sciluigi/network/members">https://github.com/pharmbio/sciluigi/network/members</ext-link>. Accessed 18 Oct 2016</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hansch</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Quantitative approach to biochemical structure–activity relationships</article-title>
        <source>Acc Chem Res</source>
        <year>1969</year>
        <volume>2</volume>
        <issue>8</issue>
        <fpage>232</fpage>
        <lpage>239</lpage>
        <pub-id pub-id-type="doi">10.1021/ar50020a002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Norinder</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Ek</surname>
            <given-names>ME</given-names>
          </name>
        </person-group>
        <article-title>QSAR investigation of NaV2.7 active compounds using the SVM/Signature approach and the Bioclipse Modeling platform</article-title>
        <source>Bioorg Med Chem Lett</source>
        <year>2013</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>261</fpage>
        <lpage>263</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bmcl.2012.10.102</pub-id>
        <pub-id pub-id-type="pmid">23177785</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Spjuth</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Georgiev</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Carlsson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Alvarsson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Berg</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Willighagen</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Wikberg</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Eklund</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Bioclipse-R: integrating management and visualization of life science data with statistical analysis</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>2</issue>
        <fpage>286</fpage>
        <lpage>289</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts681</pub-id>
        <pub-id pub-id-type="pmid">23178637</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alvarsson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Eklund</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Engkvist</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Spjuth</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Carlsson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wikberg</surname>
            <given-names>JE</given-names>
          </name>
          <name>
            <surname>Noeske</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Ligand-based target prediction with signature fingerprints</article-title>
        <source>J Chem Inf Model</source>
        <year>2014</year>
        <volume>54</volume>
        <issue>10</issue>
        <fpage>2647</fpage>
        <lpage>2653</lpage>
        <pub-id pub-id-type="doi">10.1021/ci500361u</pub-id>
        <pub-id pub-id-type="pmid">25230336</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alvarsson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Eklund</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Carlsson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Spjuth</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Wikberg</surname>
            <given-names>JE</given-names>
          </name>
        </person-group>
        <article-title>Benchmarking study of parameter variation when using signature fingerprints together with support vector machines</article-title>
        <source>J Chem Inf Model</source>
        <year>2014</year>
        <volume>54</volume>
        <issue>11</issue>
        <fpage>3211</fpage>
        <lpage>3217</lpage>
        <pub-id pub-id-type="doi">10.1021/ci500344v</pub-id>
        <pub-id pub-id-type="pmid">25318024</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Yoo AB, Jette MA, Grondona M (2003) SLURM: simple linux utility for resource management. In: Job scheduling strategies for parallel processing. Springer, Berlin, pp 44–60</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Example: top artists—luigi documentation. <ext-link ext-link-type="uri" xlink:href="http://luigi.readthedocs.org/en/stable/example%5ftop%5fartists.html">http://luigi.readthedocs.org/en/stable/example_top_artists.html</ext-link>. Accessed 13 April 2016</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Morrison JP (1994) Flow-based programming. In: Proceedings of the 1st international workshop on software engineering for parallel and distributed systems, pp 25–29</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Morrison</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <source>Flow-based programming: a new approach to application development</source>
        <year>2010</year>
        <edition>2</edition>
        <publisher-loc>Charleston</publisher-loc>
        <publisher-name>Self-published via CreateSpace</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Morrison JP (2016) Flow-based programming website. <ext-link ext-link-type="uri" xlink:href="http://www.jpaulmorrison.com/fbp/">http://www.jpaulmorrison.com/fbp/</ext-link>. Accessed 7 April 2016</mixed-citation>
    </ref>
  </ref-list>
</back>
