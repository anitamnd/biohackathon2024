<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6834530</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2019.00068</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ShuTu: Open-Source Software for Efficient and Accurate Reconstruction of Dendritic Morphology</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Jin</surname>
          <given-names>Dezhe Z.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/47950/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhao</surname>
          <given-names>Ting</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/513957/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hunt</surname>
          <given-names>David L.</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/773984/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tillage</surname>
          <given-names>Rachel P.</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/830969/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hsu</surname>
          <given-names>Ching-Lung</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/807524/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Spruston</surname>
          <given-names>Nelson</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c002">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1539/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Department of Physics and Center for Neural Engineering, The Pennsylvania State University</institution>, <addr-line>University Park, PA</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Janelia Research Campus, Howard Hughes Medical Institute</institution>, <addr-line>Ashburn, VA</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Gaute T. Einevoll, Norwegian University of Life Sciences, Norway</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Dieter Jaeger, Emory University, United States; Hidetoshi Ikeno, University of Hyogo, Japan</p>
      </fn>
      <corresp id="c001">*Correspondence: Dezhe Z. Jin <email>dzj2@psu.edu</email></corresp>
      <corresp id="c002">Nelson Spruston <email>sprustonn@janelia.hhmi.org</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>31</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>68</elocation-id>
    <history>
      <date date-type="received">
        <day>08</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>14</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2019 Jin, Zhao, Hunt, Tillage, Hsu and Spruston.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Jin, Zhao, Hunt, Tillage, Hsu and Spruston</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Neurons perform computations by integrating inputs from thousands of synapses—mostly in the dendritic tree—to drive action potential firing in the axon. One fruitful approach to studying this process is to record from neurons using patch-clamp electrodes, fill the recorded neurons with a substance that allows subsequent staining, reconstruct the three-dimensional architectures of the dendrites, and use the resulting functional and structural data to develop computer models of dendritic integration. Accurately producing quantitative reconstructions of dendrites is typically a tedious process taking many hours of manual inspection and measurement. Here we present ShuTu, a new software package that facilitates accurate and efficient reconstruction of dendrites imaged using bright-field microscopy. The program operates in two steps: (1) automated identification of dendritic processes, and (2) manual correction of errors in the automated reconstruction. This approach allows neurons with complex dendritic morphologies to be reconstructed rapidly and efficiently, thus facilitating the use of computer models to study dendritic structure-function relationships and the computations performed by single neurons.</p>
    </abstract>
    <kwd-group>
      <kwd>neuron morphology</kwd>
      <kwd>reconstruction</kwd>
      <kwd>dendrite</kwd>
      <kwd>automatic reconstruction method</kwd>
      <kwd>software</kwd>
    </kwd-group>
    <counts>
      <fig-count count="15"/>
      <table-count count="0"/>
      <equation-count count="0"/>
      <ref-count count="35"/>
      <page-count count="19"/>
      <word-count count="12065"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>The geometry of dendritic arbors directly influences synaptic integration and the resultant firing patterns of neurons (Henze et al., <xref rid="B13" ref-type="bibr">1996</xref>; Mainen and Sejnowski, <xref rid="B19" ref-type="bibr">1996</xref>; Stuart and Spruston, <xref rid="B27" ref-type="bibr">1998</xref>; Krichmar et al., <xref rid="B17" ref-type="bibr">2002</xref>). Dendritic morphologies vary widely across and within regions of the brain (Parekh and Ascoli, <xref rid="B22" ref-type="bibr">2013</xref>), so consideration of morphology is an important aspect of understanding the mechanisms by which different neurons carry out their unique functions. Intracellular recording of neurons is a common technique for studying dendritic integration of input signals (Hamill et al., <xref rid="B12" ref-type="bibr">1981</xref>; Stuart and Spruston, <xref rid="B27" ref-type="bibr">1998</xref>). To fully understand the implications of these experiments, numerical simulations of the recorded neurons are often needed (Jaeger, <xref rid="B14" ref-type="bibr">2001</xref>; Krichmar et al., <xref rid="B17" ref-type="bibr">2002</xref>; Gidon and Segev, <xref rid="B8" ref-type="bibr">2012</xref>; Menon et al., <xref rid="B20" ref-type="bibr">2013</xref>). Informative simulations require accurate reconstructions of the geometry of the recorded neurons, including branching structures and diameters of the branches.</p>
    <p>The traditional method of reconstructing neuron morphology requires intensive human labor (Zandt et al., <xref rid="B32" ref-type="bibr">2017</xref>). A slide containing a neuron filled with biocytin is mounted on a motorized stage and imaged using a video camera mounted to a bright-field microscope. The neuron image is displayed on a computer screen, and the reconstruction is done manually. The user clicks the mouse along the images of dendritic branches on the screen. While clicking, the user adjusts the cursor size to match the diameters, and turns the focus knob (<italic>z</italic> position) on the microscope to keep the branches in focus. Each click records the <italic>x</italic>, <italic>y</italic>, and <italic>z</italic> positions and the radius <italic>r</italic> at a single point, and connects the point to the previously clicked point. Bifurcations are marked and followed up sequentially. The morphology is recorded in a series of these clicked points.</p>
    <p>Manual reconstruction in this way is computationally straightforward. Since it requires no image storage or processing, the computational demand is minimal. However, there are several drawbacks, especially when the accuracy of reconstruction is crucial. Repetitive clicking while measuring the radii and turning the focus knob makes manual reconstruction labor-intensive and time-consuming. The problem is exacerbated at high magnification. To see fine processes of neurons, it is desirable to image neurons with an objective at 100× magnification and a large numerical aperture (Jaeger, <xref rid="B14" ref-type="bibr">2001</xref>; Brown et al., <xref rid="B3" ref-type="bibr">2011</xref>). In our experience, however, it can take 10–15 h or more of continuous work to reconstruct the dendritic tree of a pyramidal neuron in this way. Over this period of time, instability of the sample in the microscope can lead to problems. Furthermore, the accuracy of the reconstruction can suffer from fatigue-induced mistakes. Another problem with manual reconstruction is that the accuracy is hard to check independently because it is difficult to precisely align the previous reconstruction with the neuron image after remounting the slide.</p>
    <p>Automatic reconstruction of neuron morphology using computer algorithms promises to reduce manual labor and increase productivity. There have been intensive efforts toward this goal for decades (Capowski, <xref rid="B5" ref-type="bibr">1983</xref>). Recent work includes open-source projects, such as the Digital Reconstruction of Axonal and Dendritic Morphology Challenge (DIADEM) (Gillette et al., <xref rid="B9" ref-type="bibr">2011a</xref>,<xref rid="B10" ref-type="bibr">b</xref>; Liu, <xref rid="B18" ref-type="bibr">2011</xref>; Svoboda, <xref rid="B28" ref-type="bibr">2011</xref>) and the BigNeuron project (Peng et al., <xref rid="B24" ref-type="bibr">2015</xref>). Commercial software is also moving in this direction. A recent paper reviews many algorithms for automatic reconstruction proposed over the years (Acciai et al., <xref rid="B1" ref-type="bibr">2016</xref>). In our experience, however, available software still suffers from a variety of problems, including limited automation and tedious approaches for error correction. In particular, algorithms for automatic reconstruction of neurons stained with a dark reaction product are lacking. Thus, we sought to develop an open-source software platform that would overcome these limitations. In this paper, we describe our open-source software package, ShuTu (Chinese for “dendrite”)—a system for reconstructing biocytin-filled neurons efficiently and accurately by combining a novel automatic tracing algorithm and a graphic user interface (GUI) designed for efficient manual editing and error corrections. To avoid the impression of marketing our software, we make no attempt to compare it to other open-source or commercial software; instead, we encourage others to try it and judge for themselves.</p>
  </sec>
  <sec sec-type="results" id="s2">
    <title>Results</title>
    <p>We demonstrate the use and design of ShuTu by going through the steps involved in reconstructing a single CA3 pyramidal neuron from a mouse hippocampal slice. We then present reconstruction results for other cell types as well. Neurons were stained following patch-clamp recordings in brain slices prepared from 17 to 30 days-old male mice (C57Bl/6), using biocytin-containing intracellular solution and stained with a dark reaction product. Following recording and staining, neuron reconstruction proceeded according to the following steps: (1) image acquisition; (2) image processing; (3) automated reconstruction; (4) manual editing and error correction. Additional details regarding slice recordings and computer systems requirements are provided in the Materials and Methods section. Operational commands for ShuTu are provided in <xref ref-type="supplementary-material" rid="SM1">Appendix 1</xref>. Technical details regarding the algorithms used in ShuTu for automated reconstruction are provided in <xref ref-type="supplementary-material" rid="SM1">Appendix 2</xref>.</p>
    <sec>
      <title>Image Acquisition</title>
      <p>ShuTu uses tiles of tiff stacks covering the entire neuron (<xref ref-type="fig" rid="F1">Figure 1</xref>). Nearby tiles should overlap by ~20%, in order to facilitate accurate stitching of tiles into a single image. We imaged hippocampal neurons using a Zeiss AxioImager microscope with AxioCam and ZEN blue software. Once the boundary in the field of view (<italic>xy</italic>) and the range of the depths (<italic>z</italic>) that contain the neuron were set, the images at each tile position and depth were acquired automatically, and the positions of the these images were stored in an xml file (image metadata). Other microscope/software combinations can be used, as long as tiff stacks and their relative positions are provided to ShuTu (see Materials and Methods for details). It is also possible to use ShuTu to reconstruct neurons imaged using two-photon, confocal, or wide-field fluorescence microscopy (see Discussion). However, we focus our description of the software mainly to neurons stained using biocytin and a dark reaction product.</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p>Tiles of tiff stacks covering the entire neuron. <bold>(A)</bold> A mouse CA3 neuron imaged at 100× (biocytin fill, dark reaction product, bright-field microscopy, NA 1.4). There are 51 tiles covering the entire neuron. 2D projection is shown. <bold>(B)</bold> Dimensions of one tile. Each tiff stack consists of 224 planes of images. The distance between successive image planes is 0.5 μm. Four planes at different depths in the tiff stack indicated by the black rectangle in <bold>(A)</bold> are shown below.</p>
        </caption>
        <graphic xlink:href="fninf-13-00068-g0001"/>
      </fig>
      <p>The number of images required to capture the full three-dimensional (3D) morphology of a neuron depends on its size and the magnification of the microscope objective. The CA3 pyramidal neuron reconstructed here was relatively large and we imaged it using 100× objective (NA 1.4) (<xref ref-type="fig" rid="F1">Figure 1</xref>). Therefore a total of 51 tiles were required, with 224 images per tile (0.5 μm increments through the depth of the slice), thus yielding a total of 11,424 images. This neuron is contained in a volume of ~400 × 600 × 100 μm<sup>3</sup> and has a total dendritic length of ~8,800 μm. The full imaging process for the CA3 pyramidal neuron took ~2 h on the Zeiss microscope system we used. Faster imaging times (and fewer tiles) can be accomplished using lower magnification objectives, but in our experience 100× provides more accurate estimates of diameters for small-caliber dendrites. During imaging, care was taken to ensure that the microscope settings were optimized to obtain images of all dendrites, including those with the smallest diameter. This resulted in significant background noise, which was removed automatically in a final step of the reconstruction process (see below). We made no attempt to image or reconstruct axons, as these were of finer caliber than dendrites and for many neurons they were difficult to discern beyond a short distance from their origin near the soma.</p>
    </sec>
    <sec>
      <title>Image Processing</title>
      <p>Because the ZEN blue microscope software provides individual image files in each tile, ShuTu first converts the image files into tiff stacks using the image metadata file (xml) and parsing the file names for depth information. Each tile was imaged successively through the depth of the slice, so no alignment of the images is required to form a stack. As each stack consists of 224 images, about 5 min of CPU time was required for each stack (see Materials and Methods for the system used). The CA3 pyramidal neuron reconstructed here consists of 51 tiles, and creating the stacks required a total of just over 4 h. With multiple CPU cores and sufficient memory, ShuTu can automatically distribute the task across multiple cores in parallel, resulting in approximately linear reduction in the real time required to construct the stacks.</p>
      <p>After the tiff stacks are created, the tiles need to be stitched to find precise relative positions between the tiles. ShuTu also accomplishes this task in a parallel manner, requiring a similar amount of computational time as construction of the stacks. These two image processing steps are performed in series, but they can be executed sequentially without user intervention. In the case of our example CA3 pyramidal neuron, both of these steps were performed in just a few hours by using multiple CPU cores.</p>
    </sec>
    <sec>
      <title>Automated Reconstruction</title>
      <p>After image processing, ShuTu produces a draft reconstruction of the neuron using an automatic reconstruction algorithm (Materials and Methods). We devised the algorithm to specifically deal with several challenges posed by the bright-field images of biocytin-filled neurons (<xref ref-type="fig" rid="F2">Figure 2</xref>). One is background noise (<xref ref-type="fig" rid="F2">Figure 2A</xref>). While patching a neuron, biocytin can spill out and create blobs in the image stacks. Dirt or dust can be picked up, resulting in structures that share some features of neurites, especially as color information is not used. Second, during the process of fixing the tissue, thin dendrites can become beaded, with very faint signals between the beads (<xref ref-type="fig" rid="F2">Figure 2B</xref>). Third, close crossings of adjacent branches require special attention to resolve (<xref ref-type="fig" rid="F2">Figure 2C</xref>). Fourth, shadows of out-of-focus branches can be as strong as signals from thin dendrites in focus (<xref ref-type="fig" rid="F2">Figure 2D</xref>), making it hard to trace some dendrites without being fooled by the shadows. These challenges make it difficult to create a perfect reconstruction from automated algorithms. Our algorithm was designed to address many of these issues, but some manual correction is nevertheless required. In the following, we outline the steps involved in the algorithm, using the tile shown in <xref ref-type="fig" rid="F1">Figure 1B</xref> as an example. Technical details of the algorithm are presented in <xref ref-type="supplementary-material" rid="SM1">Appendix 2</xref>, which should be useful for adjusting the parameters for specific situations encountered by users.</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Aspects of bright-field images of biocytin-filled neurons that make automatic reconstruction challenging. Parts of images in single planes of the tiff stacks are shown. <bold>(A)</bold> Biocytin spills can create spurious signals. Dirt or dust can also add noise. <bold>(B)</bold> Thin branches can be broken into “beads.” <bold>(C)</bold> Close crossing between adjacent neuron branches. <bold>(D)</bold> A branch can cast bifurcating shadows in <italic>z</italic> with darkness level comparable to weak signals from nearby faint branches.</p>
        </caption>
        <graphic xlink:href="fninf-13-00068-g0002"/>
      </fig>
      <sec>
        <title>Conversion to Gray Scale and 2D Projection</title>
        <p>The color images are converted into grayscale images, and the pixel intensities are scaled so that the maximum is 1. A minimum intensity projection of the tiff stack is then created, which has the same dimension as a single 2D plane in the stack. The intensity at each pixel is chosen to be that of the darkest pixel among all pixels in the stack having the same <italic>xy</italic> position. This minimum intensity projection reveals all neurites in the tiff stack (<xref ref-type="fig" rid="F3">Figure 3A</xref>), along with noise from the sources mentioned above. To remove smooth variations due to uneven lighting, the 2D projection is blurred by Gaussian smoothing (<xref ref-type="fig" rid="F3">Figure 3B</xref>) and subtracted from the original 2D projection (<xref ref-type="fig" rid="F3">Figure 3C</xref>). Additionally, this process makes faint branches nearly as visible as well-stained ones (<xref ref-type="fig" rid="F3">Figure 3D</xref>); the inverse peaks corresponding to the branches in the intensity profile have more even heights after the background removal (purple curve) than before (green curve).</p>
        <fig id="F3" position="float">
          <label>Figure 3</label>
          <caption>
            <p>The process of creating a binary mask from 2D projection. <bold>(A)</bold> 2D projection from the image stack. The intensity profile across the green line is shown in <bold>(D)</bold>. <bold>(B)</bold> Smoothed background obtained from Gaussian smoothing of the 2D projection. <bold>(C)</bold> 2D projection after removing the smoothed background. The intensity across the purple line is shown in <bold>(D)</bold>. The red and blue arrows indicate the points to be tested with valley detectors in <bold>(F)</bold>. <bold>(D)</bold> Intensity across the midline in the original 2D projection [green line in <bold>(A)</bold>] and after removal of the background [purple line in <bold>(C)</bold>]. <bold>(E)</bold> Images of valley detectors at four orientations. <bold>(F)</bold> Responses to valley detectors at varying orientations for two points shown in <bold>(C)</bold> (red point, red line; blue point, blue line). λ<sub>1</sub> and λ<sub>2</sub> are the maximum and minimum responses, respectively. <bold>(G)</bold> The maximum responses to the valley detectors (λ<sub>1</sub>) for all pixels. <bold>(H)</bold> The binary mask obtained from thresholding λ<sub>1</sub>. <bold>(I)</bold> The smoothed binary mask.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0003"/>
        </fig>
      </sec>
      <sec>
        <title>Binary Mask</title>
        <p>The 2D projection is used to create a mask, which is a binary image with the white pixels indicating the neurites and dark pixels the background (<xref ref-type="fig" rid="F3">Figures 3E–I</xref>). An accurate mask is crucial for our reconstruction algorithm. Considering the intensity as heights, the neurites in the original 2D projection can be viewed as valleys of dark pixels. To create the mask, we evaluate the possibility that each pixel in the 2D projection belongs to a valley. This is accomplished by comparing the local patch of image centered at the pixel with valley detectors of varying orientations (<xref ref-type="fig" rid="F3">Figure 3E</xref>) (Frangi et al., <xref rid="B7" ref-type="bibr">1998</xref>). A valley detector is a 2D image consisting of an oriented dark band flanked by two bright bands. The response of the detector is the sum of the products of the corresponding pixels in the detector and the local patch (<xref ref-type="fig" rid="F3">Figure 3F</xref>). The response has a maximum (λ<sub>1</sub>) at one orientation, and a minimum (λ<sub>2</sub>) at the orthogonal orientation (<xref ref-type="fig" rid="F3">Figure 3F</xref>). If the local patch is nearly uniform in intensity, the response is close to zero at all orientations, and λ<sub>1</sub> is small (<xref ref-type="fig" rid="F3">Figure 3F</xref>, blue curve, which describes the responses at the blue pixel in <xref ref-type="fig" rid="F3">Figure 3C</xref>). In contrast, if the local patch contains a valley, the maximum response (λ<sub>1</sub>) is large and the minimum response (λ<sub>2</sub>) is small (<xref ref-type="fig" rid="F3">Figure 3F</xref>, red curve, at the red pixel in <xref ref-type="fig" rid="F3">Figure 3C</xref>). If the patch contains a crater corresponding to a blob, λ<sub>1</sub> can be large, but so can λ<sub>2</sub>, because there is no privileged orientation. These features are used to select pixels in valleys but not in blobs or in the background by thresholding λ<sub>1</sub> while also factoring in the difference between λ<sub>1</sub> and λ<sub>2</sub>, creating the binary mask (<xref ref-type="fig" rid="F3">Figure 3H</xref>). The mask is further smoothed to eliminate noisy speckles and rough edges in the boundaries, creating the smoothed mask (<xref ref-type="fig" rid="F3">Figure 3I</xref>).</p>
      </sec>
      <sec>
        <title>SWC Points</title>
        <p>We use the “SWC” format for representing the neuron morphology (Cannon et al., <xref rid="B4" ref-type="bibr">1998</xref>). It consists of a list of SWC points. An SWC point is defined by six values: ID, branch type, <italic>x</italic>-position, <italic>y</italic>-position, <italic>z</italic>-position, radius, and the ID of the parent SWC point (-1 if there no parent). ID is an integer unique to the point. The value for branch type can be 1 for soma; 2 for axon; 3 for basal dendrite; and 4 for apical dendrite. <italic>x, y, z</italic> specify the position of the point, which should be on the center line of the branch; and radius specifies the radius of the branch at that position. The parent ID gives the connections between the SWC points. An SWC point and its parent specifies a cylindrical trapezoid volume that represents the small segment of the branch between the positions of the two points.</p>
        <p>The mask is used to place SWC points along the neurites. The SWC points are placed along the centerlines of the binary mask (<xref ref-type="fig" rid="F4">Figure 4A</xref>). The radii of the SWC points are computed as the shortest distance from the positions of the SWC points to the nearest boundaries of the binary mask (<xref ref-type="fig" rid="F4">Figure 4B</xref>). To determine the depths of the SWC points in the original tiff stack, we dissect the centerlines into segments between end points and/or crossing points. These segments are called “xy-paths” (e.g., <xref ref-type="fig" rid="F4">Figure 4A</xref>, red arrow). Cutting through the tiff stack while following an xy-path, we create a “z-image” for that segment (<xref ref-type="fig" rid="F4">Figure 4C</xref>). This z-image contains all pixels in the tiff stack whose <italic>xy</italic> positions lie in the xy-path. The branch whose 2D projection falls on the xy-path manifests as a dark valley in the z-image spanning from the left edge to the right edge (<xref ref-type="fig" rid="F4">Figure 4C</xref>). ShuTu finds the line through the dark valley (red dotted line in <xref ref-type="fig" rid="F4">Figure 4C</xref>), from which the depths of the neurites (and the SWC points) are determined. The distance between successive SWC points is set to roughly the sum of their radii. The distance is made shorter when the radii changes rapidly along the centerlines to reflect large changes in short distances in the dendritic morphology.</p>
        <fig id="F4" position="float">
          <label>Figure 4</label>
          <caption>
            <p>Creating SWC structure from the mask. <bold>(A)</bold> The skeleton obtained by thinning the mask. <bold>(B)</bold> Distance map computed from the mask. The square region highlighted in <bold>(A)</bold> is shown. The brightness of each pixel is proportional to the distance to the nearest boundary in the mask. <bold>(C)</bold> Finding the depth of the path. The image is constructed by cutting through the stack in <italic>z</italic>-dimension following the <italic>xy</italic> path indicated by the arrow in <bold>(A)</bold>. The dark band is the neurite along the path. The dotted line is the depth (<italic>z</italic>) computed using the left-right shortest path algorithm. <bold>(D)</bold> The depth of a candidate SWC point is further adjusted using the intensity profile in <italic>z</italic> at the <italic>xy</italic> position of the candidate point (black line). The point of minimum intensity in the smoothed profile (green line) is set as the depth of the candidate point. The <italic>z</italic> distance in the graph is relative to the <italic>z</italic> position of the end point.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0004"/>
        </fig>
        <p>Invalid SWC points are automatically removed (see below regarding validity of SWC points), and the <italic>z</italic> of a valid SWC point is further adjusted to the nearby depth of minimum of intensity in <italic>z</italic>-dimension (<xref ref-type="fig" rid="F4">Figure 4D</xref>). Adjacent SWC points along one xy-path are connected. If the removal creates a large distance between two consecutive SWC points, they are not connected. Biologically, sharp turns in neurites are rare. Therefore, to safeguard against possible errors, we do not connect SWC points if doing so creates sharp angles in consecutive lines of connections. To avoid connecting branches far away in depth, SWC points are not connected if the difference in <italic>z</italic> is too large. These decisions depend on parameters set by the user (<xref ref-type="supplementary-material" rid="SM1">Appendix 2</xref>).</p>
      </sec>
      <sec>
        <title>Validity of SWC Points</title>
        <p>In some cases, the xy-paths from the centerlines of the binary mask are incorrect. For example, nearby branches can be merged in the mask. Checking the validity of the SWC points is thus crucial for eliminating mistakes. To do so, we take a square patch of the image centered at an SWC point in the plane of the point's depth. The size of the patch is set to four times the radius of the SWC point or 4 μm, whichever is greater. To reduce the possibility that a tilt in the intensity across the patch might interfere with the check, we subtract a linear fit to the intensity and scale the result to the original intensity range. We then create intensity profiles in eight directions centered at the SWC point (<xref ref-type="fig" rid="F5">Figure 5A</xref>). For each profile, we look for a significant inverse peak after smoothing the profile (<xref ref-type="fig" rid="F5">Figures 5B,C</xref>). The significance is checked against the baseline and fluctuations in the intensity. The baseline is set to the top 20% intensity value in the patch, and a parameter σ = 0.03 is used to characterize the fluctuations. A threshold, set to the half point between the maximum and minimum of the smoothed profile (<xref ref-type="fig" rid="F5">Figure 5B</xref>, dotted gray line), is used to judge whether the smoothed profile has two flanks. Another threshold, set to the baseline minus 2σ, is used to judge whether the inverse peak is deep enough (<xref ref-type="fig" rid="F5">Figure 5B</xref>, gray line). If both criteria are met, the profile is judged to have a significant inverse peak. The width of the inverse peak is the distance between the steepest descending point and the steepest ascending point of the peak, identified by the derivatives of the smooth profile (<xref ref-type="fig" rid="F5">Figures 5E,F</xref>).</p>
        <fig id="F5" position="float">
          <label>Figure 5</label>
          <caption>
            <p>Checking the validity of an SWC point. <bold>(A)</bold> A patch of image around an SWC point to be examined. The image is taken from the <italic>z</italic>-plane of the SWC point. Profiles of the intensities along eight directions are taken (straight lines; colors indicate angles). The green line is the profile chosen to adjust the SWC point. The red circle indicates the radius of the SWC point. <bold>(B)</bold> The profile (black, raw; green, smoothed) along the green line in <bold>(A)</bold>. The dotted gray line is the baseline, and the solid gray line is the threshold. An inverse peak is judged valid if the flanks of the smoothed profile go above the baseline, and the minimum value goes below the threshold. <bold>(C)</bold> Smoothed profiles for all eight directions. <bold>(D)</bold> The chosen profile (green) and the profile at the orthogonal direction (violet). The vertical lines are at the half radius points. Note that the center of the profiles are slightly shifted compared to those in <bold>(C)</bold>. For the SWC to be valid, the minimum intensity of the profile at the orthogonal direction must be below the threshold (gray line) within the vertical lines. <bold>(E)</bold> Smoothed derivative of the smoothed profile in <bold>(B)</bold>. The vertical lines indicate the local maxima of the derivatives. The distance between the vertical lines is the width of the peak. <bold>(F)</bold> Smoothed derivatives of the profiles for all eight directions. The profile with the minimum width is chosen.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0005"/>
        </fig>
        <p>If none of the profiles have a significant inverse peak, the SWC point is invalid. Otherwise, we chose the profile with the minimum width among the valid ones. In some cases, an SWC point can be at the edge of thick dendrite or soma (see below). To eliminate them, we check wether the intensity within the half radius of the SWC point is low enough (<xref ref-type="fig" rid="F5">Figure 5D</xref>). Specifically, we check that the intensity values of the smoothed profile (<xref ref-type="fig" rid="F5">Figure 5D</xref>, violet curve), orthogonal to the chosen profile (<xref ref-type="fig" rid="F5">Figure 5D</xref>, green curve) within the half radius (<xref ref-type="fig" rid="F5">Figure 5D</xref>, dotted vertical lines), is smaller than a threshold. This threshold is set to the maximum of the chosen profile within the range plus σ. If not dark enough, the SWC point is invalid.</p>
        <p>If the SWC point passes the validity test, we set its radius to the half width of the inverse peak in the chosen profile. Its <italic>xy</italic> position is adjusted to that of the inverse peak, and <italic>z</italic> position is adjusted to the depth of the nearby intensity minimum in <italic>z</italic> (<xref ref-type="fig" rid="F4">Figure 4D</xref>). To ensure that this adjusting process converges, we adjust each SWC point three times iteratively. If the final <italic>xy</italic> position shifts from the original position more than twice of the original radius, we mark the SWC point invalid since it is most likely created erroneously. Finally, if the final radius of the SWC is smaller than 0.2 μm or larger than 10 μm, the SWC is most likely due to noise and is marked invalid.</p>
      </sec>
      <sec>
        <title>Mark Pixels Occupied</title>
        <p>As the SWC points are created, we mark pixels in the tiff stack in the vicinity of the SWC points as occupied. The pixels around two connected SWC points, formed by two half cylinders and a trapezoidal prism, are marked as occupied (<xref ref-type="fig" rid="F6">Figure 6A</xref>). Before creating a new SWC point, we check whether its center point is marked as occupied; if so, no SWC point is created. This avoids creating redundant SWC points for the same piece of dendritic branch.</p>
        <fig id="F6" position="float">
          <label>Figure 6</label>
          <caption>
            <p>Mark pixels as occupied and extending SWC structure in 3D. <bold>(A)</bold> Mark pixels in tiff stack as occupied. The pixels around two connected SWC points (red spheres), formed by two half cylinders and a trapezoidal prism, are marked as occupied. <bold>(B)</bold> The candidate point for extension is searched in the plane of an end SWC point (yellow circle). Red circles are the SWC points that are connected to the end point. The search is done along a shortest distance path running through the neurite (white line). This path is determined by building the intensity-weighted shortest distance profile along an arc (blue line) enclosed by two black lines. The valid SWC point closest to the end point is selected as the new SWC point (orange circle). <bold>(C)</bold> The profile of the shortest distances along the arc [blue line in <bold>(B)</bold>]. The green line is the smoothed version. The angle is measured relative to the line connecting the end point to its connected SWC point [yellow line in <bold>(B)</bold>]. The minimum position in the smoothed profile is selected as the starting point of the shortest path shown in <bold>(B)</bold>.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0006"/>
        </fig>
      </sec>
      <sec>
        <title>Thick Dendrites and Soma</title>
        <p>The widths of dendrites can vary several-fold from the thin terminal dendrites to the thick apical dendrite near the soma. The thick dendrites and the soma can be missing from the binary mask, which is created with the valley detector tuned for detecting thin dendrites. Therefore, only edges of the thick dendrite and soma are captured in the mask, leading to invalid SWC points that are eliminated. To solve this issue, we separately detect the presence of thick dendrites and soma. The thick dendrites and soma are typically well-stained and show up as the darkest locations in the 2D projection. We use this fact to decide whether there are thick dendrites and soma that are not covered by existing SWC points. If the lowest intensities in the pixels covered by the existing SWC points are brighter than the lowest intensities in the 2D projection, we decide that the binary mask missed the soma or thick dendrite. We create a binary mask on a 2D projection, excluding pixels around the existing SWC points, by thresholding the pixel intensities of the 2D projection. New SWC points are added based on this new mask.</p>
      </sec>
      <sec>
        <title>Extending SWC Points in 3D</title>
        <p>The SWC structure created with 2D projections can contain errors. Typically the binary masks can be incomplete or incorrect in some parts due to weak signals, occlusions produced by branch crossing, or mergers of closely parallel branches. This leads to gaps in the SWC structure representing continuous dendritic branches. To bridge these gaps, we extend the SWC points in 3D (the tiff stack) from the end points in the SWC structure.</p>
        <p>To minimize interference from noise, we first delete isolated SWC points that are not connected to any other SWC points. We then mark pixels near the existing SWC points as occupied (<xref ref-type="fig" rid="F6">Figure 6B</xref>, red circles) to ensure that the extension does not create duplicate SWC points.</p>
        <p>From an end SWC point (<xref ref-type="fig" rid="F6">Figure 6B</xref>, yellow circle), we search for the next candidate SWC point. We draw an arc of radius 3 μm or twice the radius of the end point, whichever is greater, in the plane of the end point (<xref ref-type="fig" rid="F6">Figure 6B</xref>, blue arc). The arc spans from −π/3 to π/3 (<xref ref-type="fig" rid="F6">Figure 6B</xref>, black lines) relative to the line from the end point to its previously connected SWC point (<xref ref-type="fig" rid="F6">Figure 6B</xref>, yellow line). The shortest intensity-weighted distances from the points on the arc to the end point are computed (<xref ref-type="fig" rid="F6">Figure 6C</xref>, black line). With the smoothed profile of the distances, the point with the minimum distance is selected (<xref ref-type="fig" rid="F6">Figure 6C</xref>, green line), and the shortest distance path from this point to the end point is found, which should follow along the neurite (<xref ref-type="fig" rid="F6">Figure 6B</xref>, white line). The depth of the neurite is found using the xy-path technique (<xref ref-type="fig" rid="F4">Figure 4C</xref>) along the shortest distance path.</p>
        <p>The candidate SWC point for extension (<xref ref-type="fig" rid="F6">Figure 6B</xref>, orange circle) is placed on the shortest distance path, starting from the end point marching toward the arc. We test the validity of the candidate SWC point, during which the <italic>xy</italic> position and radius are adjusted. To cover weak branches, the test is made less stringent by accepting shallower inverse peaks in the intensity profiles used for the test (<xref ref-type="fig" rid="F5">Figures 5B,C</xref>). If accepted, the extension process continues from the new SWC point as the end point. If the candidate point is marked occupied, the extension stops, and the possibility of connecting the end point to the existing SWC points that marked the occupation are evaluated (see below). The extension stops if the test fails for all points along the shortest distance path.</p>
      </sec>
      <sec>
        <title>Connecting Broken Segments</title>
        <p>After extending SWC points in 3D, a continuous branch can still be represented with broken segments of SWC points, especially if the underlying signal is broken or there are closely crossing branches (e.g., <xref ref-type="fig" rid="F2">Figures 2B,C</xref>). We connect these segments with heuristic rules based on the distances between the end points, in order to recover the branch continuity (<xref ref-type="supplementary-material" rid="SM1">Appendix 2</xref>). After connecting the end points, the SWC structure for the tiff stack is complete.</p>
        <p>The results for our example tiff stack are shown in <xref ref-type="fig" rid="F7">Figures 7A–E</xref>, in which the SWC points are overlaid with the underlying image, and in <xref ref-type="fig" rid="F7">Figure 7F</xref>, in which the SWC structure is shown from four different view angles in 3D to reveal more details. For this particular tiff stack, the automated reconstruction is mostly accurate, except that an elongated piece of dirt is mistaken as a neurite, and a close crossing of two branches is incorrectly connected. These errors need to be corrected manually (see below).</p>
        <fig id="F7" position="float">
          <label>Figure 7</label>
          <caption>
            <p>The SWC structure overlaid on the image. <bold>(A)</bold> The SWC structure projected on to the image of 2D projection. Red circles are SWC points. Connections between them are indicated with red lines. <bold>(B–E)</bold> SWC structure overlaid at the specific planes in the tiff stack, zoomed in to show more details. Arrows indicate the corresponding regions in the 2D projection. <bold>(F)</bold> The SWC structure viewed from four different angles to reveal the 3D structure. The viewing angles are indicated with the directions of <italic>xyz</italic> coordinates.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0007"/>
        </fig>
      </sec>
      <sec>
        <title>Subdivision in <italic>z</italic></title>
        <p>The 2D projection can be complicated when there are many branches in one tiff stack, which often leads to missed branches due to occlusions. One way of mitigating this problem is to divide the tiff stack in <italic>z</italic> into several slabs with equal heights in <italic>z</italic>. SWC points are created separately for each slab as described above, and then combined for the entire stack. Extension from the end points is done with the entire stack. When branches extend across the boundaries between subdivisions of tiff stacks, they are automatically connected by extension from the end points, as described above.</p>
        <p>ShuTu allows the user to decide how many subdivisions (slabs) are necessary based on the complexity of the morphology and the thickness of the tiff stacks. The user should keep in mind that a large number of subdivision slows down the automated tracing. In our example neuron, we divided all tiff stacks into eight slabs.</p>
      </sec>
      <sec>
        <title>Combining SWCs</title>
        <p>The SWCs determined in individual tiles of tiff stacks are combined to form the SWC of the entire neuron. The positions of SWCs are shifted based on the relative coordinates obtained in the stitching process. The SWC points of individual stacks are read in sequentially. To avoid duplicated SWC points in the overlapping regions of adjacent stacks, pixels near the SWC points that are already read in are marked occupied. If the position of SWC points are at the marked pixels, they are deleted. After reading in the SWC points of all stacks, we extend the end points and connect them if they are nearby. Isolated short branches (&lt;20 μm) and small protrusions (&lt;5 SWC points) from main branches are deleted to reduce noise in the SWC structure. The resulting SWC structure for the example neuron is shown in <xref ref-type="fig" rid="F8">Figures 8A–D</xref>.</p>
        <fig id="F8" position="float">
          <label>Figure 8</label>
          <caption>
            <p>Combining SWCs from the stacks for the entire neuron. <bold>(A)</bold> 2D projection of the entire neuron. Individual tiff stacks are stitched together to obtain their relative coordinates. The stack in <xref ref-type="fig" rid="F7">Figure 7</xref> is highlighted with a black rectangle. <bold>(B)</bold> The SWC of the entire neuron is obtained by combining the SWCs of individual tiff stacks. The SWC points are overlaid onto the 2D projection. <bold>(C)</bold> 3D view of the SWC structure. The 3D box corresponds to the highlighted stack in <bold>(A)</bold>. <bold>(D)</bold> The 3D view from a different angle.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0008"/>
        </fig>
        <p>The entire process of automated reconstruction of the example neuron took about 4 h on our basic desktop system using three CPU cores (see Materials and Methods for the system specs). With more powerful computers, the time can be further reduced approximately linearly with the number of CPU cores used.</p>
      </sec>
    </sec>
    <sec>
      <title>Manual Editing and Error Correction</title>
      <p>The SWC structure created by the automatic algorithm requires editing, such as removing noise, tracing thin or faint dendrites, connecting ends, and correcting mistakes in the radii and positions of the SWC points and in the connections between them. We have designed ShuTu to make these operations easy for the user. In this section we highlight a number of editing techniques.</p>
      <sec>
        <title>Inspecting the Reconstruction</title>
        <p>The SWC structure can be examined in three modes: Tile Manager, Stack View, and 3D View (<xref ref-type="fig" rid="F9">Figure 9</xref>). In Tile Manager, the SWC structure is overlaid with 2D projection of the entire neuron (<xref ref-type="fig" rid="F9">Figure 9A</xref>). In this view, it is easy to identify missing, discontinuous, or incorrectly connected branches.</p>
        <fig id="F9" position="float">
          <label>Figure 9</label>
          <caption>
            <p>Three views for examining the SWC structure. <bold>(A)</bold> Tile manager. 2D projection of the stitched stacks is superimposed with the 2D projection of the SWC structure. <bold>(B)</bold> Stack view. One stack is loaded, with the SWC points in the stack overlaid onto the image. The 2D projection view of the stack can be created within this window. <bold>(C)</bold> SWC view. The 3D structure can be viewed from different angles and edited.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0009"/>
        </fig>
        <p>Double clicking on one tile in Tile Manager loads the tiff stack into Stack View (<xref ref-type="fig" rid="F9">Figure 9B</xref>), in which the SWC structure is overlaid with the image. The radii, depths, and connectivity of the SWC points can be examined in detail by scrolling up and down through the <italic>z</italic> dimension of the tiff stack.</p>
        <p>From Stack View, a 2D projection can be created by clicking on the Make Projection button (<xref ref-type="fig" rid="F10">Figure 10</xref>). There is an option to subdivide the stack into multiple slabs in <italic>z</italic>, in which case separate 2D projections are created. Subdivision is useful when the branching patterns are complicated. Mistakes in the reconstruction can be easily spotted in Projection View, including missed branches, broken points, incorrect connections, and inclusion of noise (<xref ref-type="fig" rid="F10">Figure 10</xref>). Incorrect positions and diameters for the SWC points are easy to identify as well.</p>
        <fig id="F10" position="float">
          <label>Figure 10</label>
          <caption>
            <p>In the Stack View, clicking on the Make Projection button creates the 2D projection of the tiff stack and the SWC structure. It is easy to spot mistakes in this view. SWC points can be removed and their properties changed. The connections between SWC points can be modified. Selecting one SWC point and pressing <monospace>z</monospace> locates the points in the Stack View for further examination and modification.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0010"/>
        </fig>
        <p>In 3D View, the SWC structure can be rotated and shifted in order to reveal incorrect connections, especially large jumps in <italic>z</italic>, which can be obscure in other views.</p>
        <p>Editing can be done in Stack View, Projection View, and 3D View. In all cases, after any editing, the SWC structure is updated in all views. A selected point can be deleted or moved and its radius can be modified. A selected point in Projection View or 3D View can also be located in Stack View for further examination and modification using the tiff stack.</p>
      </sec>
      <sec>
        <title>Adding SWC Points</title>
        <p>In Stack View, SWC points can be added in three ways. The first method is smart extension. The user selects an SWC point on a branch that needs extension, finds a target point on the branch and locates the focus plane in <italic>z</italic>, and then clicks on the target. SWC points will be added along the branch from the selected SWC point to the target point (<xref ref-type="fig" rid="F11">Figure 11A</xref>). The path is computed with the shortest distance algorithm, and the radii and positions of the SWC points are automatically calculated using the automated algorithm described above. The second method is manual extension. It is the same as the smart extension, except that the only point added is at the target point and its radius needs to be adjusted manually. The third method is mask-to-SWC (<xref ref-type="fig" rid="F11">Figure 11B</xref>). In Projection View, a mask along a branch is drawn by selecting the start and end points. The path is automatically computed with the shortest-distance algorithm. The mask can also be drawn manually. After the mask is completed, it is converted to SWC points along the branch. The positions and radii of the SWC points are computed automatically.</p>
        <fig id="F11" position="float">
          <label>Figure 11</label>
          <caption>
            <p>Creating SWC points. <bold>(A)</bold> In Stack View, an SWC point is selected. Find the target point by finding the focus plane of the branch. Clicking on the target point creates SWC points connecting the target point to the selected point along the branch. <bold>(B)</bold> In Projection View, pressing <monospace>r</monospace> starts mask creation. Click on the starting point and <monospace>Shift-click</monospace> on the end point along a branch creates a mask. Clicking on the <monospace>Mask</monospace> → <monospace>SWC</monospace> button creates SWC points along the mask.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0011"/>
        </fig>
        <p>These three ways of adding SWC points are complimentary. When the branch to be reconstructed is long, the mask-to-SWC method is efficient. However, it requires that the underlying signal is strong enough, otherwise the computation of the path and the depths can be inaccurate. When the branch to be covered is short, the smart extension method is efficient, although it also requires a relatively strong signal. Manual extension always works.</p>
        <p>ShuTu users can reconstruct the entire neuron with one of these three methods. The extension methods can be used after creating a single seed SWC point. However, the process is tedious because the focus plane must be located in every click. The mask-to-SWC method traces branches in 2D projections, and is therefore more efficient.</p>
      </sec>
      <sec>
        <title>Modifying Connections</title>
        <p>The end points in the SWC structure are highlighted with blue or yellow colors. In some cases, it is necessary to connect nearby points that have been incorrectly identified as end points. This can be done by selecting two end points and connecting them. If the distance between the two points are more than the sum of their radii, SWC points can also be added automatically while bridging the gap. A selected end point can also be automatically connected to its nearest neighbor.</p>
        <p>Making connections between two points are denied if they are already connected. If this happens to two nearby points that the user would like to connect, the user should look for a “loop” in the SWC structure connecting these two points due to incorrect connections made elsewhere. Selecting the two points highlights the loop. After breaking the incorrect connections, the two points can be connected.</p>
        <p>Incorrect connections can be broken after selecting two connected SWC points. The branching points are highlighted with green; these points need to be examined carefully for incorrect connections, especially when branches cross.</p>
        <p>All SWC points connected to a selected point can be highlighted (<xref ref-type="fig" rid="F12">Figure 12A</xref>). This is useful for finding broken connections in the SWC structure. Structures with short total length can also be selected (<xref ref-type="fig" rid="F12">Figure 12B</xref>), and can be deleted with a single command. This is useful to reduce noise in the automated reconstruction. At the end of the reconstruction, all SWC points that belong to the neuron should be connected. At this point, all remaining points (presumably noise) can be deleted by selecting all connected points in the neuron and deleting the unselected points with a single command.</p>
        <fig id="F12" position="float">
          <label>Figure 12</label>
          <caption>
            <p><bold>(A)</bold> In 3D View, selecting one SWC point and pressing <monospace>h-5</monospace> selects all SWC points connected to the selected. This operation is useful for detecting broken connections. <bold>(B)</bold> Pressing <monospace>h-7</monospace> selects all branches with total length smaller than a chosen threshold. This operation is useful for deleting noise.</p>
          </caption>
          <graphic xlink:href="fninf-13-00068-g0012"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>Reconstruction Efficiency</title>
      <p>To quantify the efficiency of reconstructing neurons through the automatic algorithm and manual editing, we counted the number of editing operations (NEO) required for achieving the final reconstructions, starting from the one generated by the automatic algorithm. The results for the example neuron are shown in <xref ref-type="fig" rid="F13">Figure 13</xref>. The SWC points that are added in the editing phase are shown in red, and those from the automatic reconstruction are shown in green (<xref ref-type="fig" rid="F13">Figures 13A,B</xref>). The added SWC points are about 5% of the total SWC points in the structure. The NEO is 439. Among the editing operations, extensions are dominant. Correcting connection mistakes are sizable as well. The manual time spent in repairing the automatic reconstruction was around 1.5 h.</p>
      <fig id="F13" position="float">
        <label>Figure 13</label>
        <caption>
          <p>Reconstructed neuron after editing. <bold>(A,B)</bold> Two different views of the reconstructed neurons. The SWC points from the automatic reconstruction are in green, and those added in the editing process are in red. <bold>(C)</bold> Top operations done in the editing process and the total number of edits.</p>
        </caption>
        <graphic xlink:href="fninf-13-00068-g0013"/>
      </fig>
      <p>The efficiency of reconstruction depends on the image quality and the complexity of the neuron morphology. For neurons with sparse processes, the automated reconstruction captures most of neuronal structure, and manual editing is not intensive. For the example shown in <xref ref-type="fig" rid="F14">Figure 14A</xref>, which is another mouse CA3 pyramidal neuron, simpler than the one shown in previous figures, the NEO is 88, and the time spent in editing was ~20 min. In contrast, when the processes are dense, the automated reconstruction contains many omissions and mistakes, so manual editing takes more effort. An example is shown in <xref ref-type="fig" rid="F14">Figure 14B</xref>, which is a rat CA1 pyramidal neuron; the NEO is 812, and the time spent in editing was ~1.9 h. Another example of a complex neuron is shown in <xref ref-type="fig" rid="F14">Figure 14C</xref>, which is a mouse Purkinje cell labeled with fluorescent dye and imaged with Zeiss 880 confocal microscope at 63× magnification. The increased complexity decreases the quality of automated reconstruction; ~2.4 h of editing was required and the NOE is 1,190. This example also shows that our software is not restricted to tracing biocytin-labeled neurons.</p>
      <fig id="F14" position="float">
        <label>Figure 14</label>
        <caption>
          <p>Three examples of reconstructions. Shown for each neuron are the 2D projection of the images, automated reconstruction on top of the 2D projection, and the final reconstruction (blue, soma; red, apical dendrite; green, basal dendrite; gold, axon). <bold>(A)</bold> A pyramidal neuron in the mouse CA3 region imaged at 100× (biocytin). <bold>(B)</bold> A pyramidal neuron in the rat CA1 region imaged at 63× (biocytin). <bold>(C)</bold> A mouse Purkinje cell imaged at 63× with confocal microscope (fluorescence).</p>
        </caption>
        <graphic xlink:href="fninf-13-00068-g0014"/>
      </fig>
    </sec>
    <sec>
      <title>Comparison to Other Algorithms for Automatic Reconstruction</title>
      <p>To assess the performance of our automated algorithm relative to other algorithms, we compared automatic tracing results of three approaches: our own, neuTube (Zhao et al., <xref rid="B33" ref-type="bibr">2011</xref>; Feng et al., <xref rid="B6" ref-type="bibr">2015</xref>), and Vaa3D (more specifically, Vaa3D-APP2) (Xiao and Peng, <xref rid="B31" ref-type="bibr">2013</xref>). Among many available automatic reconstruction algorithms (Acciai et al., <xref rid="B1" ref-type="bibr">2016</xref>), we selected neuTube and Vaa3D for comparison because they are widely known and adopted by the community.</p>
      <p>We selected a tile covering part of the basal dendrites in the biocytin-filled neuron shown in <xref ref-type="fig" rid="F1">Figure 1</xref>. Since neuTube and Vaa3D are designed for tracing dark-field (fluorescent) images, we converted the original bright-field image to a dark-field image. We tested several preprocessing methods for this purpose, including directly inverting the image intensities, local background subtraction provided by Vaa3D plugins (Peng et al., <xref rid="B25" ref-type="bibr">2010</xref>), and adaptive image enhancement developed for neuron tracing (Zhou et al., <xref rid="B34" ref-type="bibr">2015</xref>). We found that only the local background subtraction method, which computes the background of any given location by averaging its neighboring samples within a certain range, produced acceptable automatic reconstructions when the preprocessed image was supplied to neuTube or Vaa3D. In each program, we tuned adjustable parameters in the open-source code to obtain the best results.</p>
      <p>As shown in <xref ref-type="fig" rid="F15">Figure 15</xref>, our automated algorithm produced a reconstruction that required much less manual editing and corrections than neuTube or Vaa3D. Our automated reconstruction covered more neurites (<xref ref-type="fig" rid="F15">Figures 15A–C</xref>). After manual editing and corrections, the percentages of SWC points created by the automated reconstructions and retained in the final reconstructions were 95% of the total SWC points for our algorithm, 72% for neuTube, and 70% for Vaa3D. The NEO values were 139, 370, and 534, respectively (<xref ref-type="fig" rid="F15">Figure 15D</xref>); and the estimated manual time spent on editing and corrections were 17, 32, and 52 min, respectively (<xref ref-type="fig" rid="F15">Figure 15E</xref>). For neuTube, most manual time was spent on manually tracing missed faint branches. For Vaa3D, besides the missing branches, additional time was required to delete erroneous traces (see <xref ref-type="fig" rid="F15">Figure 15C</xref>); furthermore, the estimates of the branch radii were mostly far off, and adjusting the radii of the SWC points also contributed to the manual time. Comparisons of the automated reconstructions to a fully manual reconstruction of the tile further supported that our algorithm captures the dendrites more accurately than neuTube or Vaa3D (<xref ref-type="supplementary-material" rid="SM1">Supplementary Material</xref>).</p>
      <fig id="F15" position="float">
        <label>Figure 15</label>
        <caption>
          <p>Comparisons of automated reconstructions from ShuTu, neuTube, and Vaa3D. <bold>(A)</bold> Automated reconstruction generated by ShuTu on one image tile (red circles) overlaid on the 2D minimum-intensity projection of the tile (top). The tile covers part of the basal dendrites of the neuron shown in <xref ref-type="fig" rid="F1">Figure 1</xref>. The SWC structure after manual editing (below). Green indicates the SWC points automatically generated; red indicates those added during the editing. <bold>(B)</bold> Automated reconstruction generated by neuTube. <bold>(C)</bold> Automated reconstruction generated by Vaa3D. <bold>(D)</bold> Comparison of the NEO (number of editing operations) for the three methods. <bold>(E)</bold> Comparison of the estimated manual editing time for the three methods.</p>
        </caption>
        <graphic xlink:href="fninf-13-00068-g0015"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s3">
    <title>Discussion</title>
    <p>We have demonstrated how ShuTu can be used to reconstruct neuron morphology by converting microscope images to structures described by a collection of SWC points. Our goal is to provide a practical system that can be readily implemented and used in labs who need accurate dendritic reconstructions of neurons, often studied and stained following recordings with patch-clamp electrodes. As an open-source software package, ShuTu can be continuously improved by the community. We have also provided raw images of the example neurons (Jin, <xref rid="B15" ref-type="bibr">2019</xref>), which should be useful for testing and improving the software.</p>
    <p>Software for automated reconstruction is often evaluated by comparing automated reconstructions to manual reconstructions (Peng et al., <xref rid="B24" ref-type="bibr">2015</xref>; Acciai et al., <xref rid="B1" ref-type="bibr">2016</xref>). We have not done this systematically, because of our assertion that all automated reconstructions, including those produced by ShuTu, are error prone. Our approach was to develop software that used automated algorithms only as an initial step to assist manual reconstruction and minimize the amount of editing time required by the user. The objective, therefore, is the same as manual reconstructions; namely reconstructions that can be considered “gold standard” (Peng et al., <xref rid="B24" ref-type="bibr">2015</xref>), but are achieved in far less time than fully manual approaches.</p>
    <p>To achieve this objective, we complemented our automated reconstruction algorithms with editing functions in ShuTu's graphical user interface (GUI). These functions improve the efficiency of editing, and provide a tool for counting the number of editing operations (NEO). Our software should be judged on its ability to produce gold standard reconstructions while minimizing NEO. For example, our automatic reconstruction algorithm can be aggressive in finding neurites, including faint ones. This introduces noise into the automated reconstruction (and thus apparently “bad” initial reconstructions), but the noise can be easily edited out after genuine dendritic branches are annotated (<xref ref-type="fig" rid="F12">Figure 12</xref>).</p>
    <p>There are many parameters in our automated reconstruction algorithm. Users should experiment with these parameters, as the optimal settings may depend on properties of the images, which are likely to vary depending on staining and imaging procedures. Among the most important parameters are the distances between pixels and between the successive planes, which are determined by the image acquisition process. Also important is the number of subdivisions of one tiff stack. Since our algorithm relies on 2D projections, subdivision reduces overlap of neurites from different depths, thus improving reconstruction quality. Checking the validity of each SWC point is also critical, so the users should pay close attention to adjusting these parameters. <xref ref-type="supplementary-material" rid="SM1">Appendix 2</xref> describes other important parameters and technical details of the automatic reconstruction algorithm.</p>
    <p>There are a number of open-source software packages for reconstructing neurons, most notably Vaa3D (Peng et al., <xref rid="B23" ref-type="bibr">2014</xref>) and neuTube (Feng et al., <xref rid="B6" ref-type="bibr">2015</xref>). Vaa3D has extensive capabilities for processing images from various sources (Peng et al., <xref rid="B23" ref-type="bibr">2014</xref>). In contrast, we focused on optimizing our software for the particular application of neurons stained with a dark reaction product following patch-clamp recording. Although ShuTu can work for neurons stained in other ways (see <xref ref-type="fig" rid="F14">Figure 14C</xref>), we have no attempt to optimize it for use with multiple staining and imaging procedures. In addition, ShuTu was developed with a philosophy that perfect automatic reconstruction is difficult, if not impossible. Therefore, we emphasized the importance of manual annotation and error correction. In keeping with this philosophy, ShuTu includes a user-friendly GUI to facilitate these processes.</p>
    <p>Another open-source software package for neuron reconstruction—neuTube—also has a strong 3D capability for manipulating SWC structure (Feng et al., <xref rid="B6" ref-type="bibr">2015</xref>). As the GUI of ShuTu is based on the GUI of neuTube (T. Zhao is a contributor to both), many of the features of ShuTu are adaptations of neuTube. However, we should emphasize that the algorithm used for automated reconstruction in ShuTu is novel and unrelated to neuTube. Additionally, ShuTu includes several important extensions of the GUI. NeuTube was designed to deal with a single tiff stack. As such, it can only be used for reconstructing neurons fully contained in a single tiff stack. ShuTu is a complete solution that includes the capability to deal with multiple tiff stacks, including modules for processing and stitching the images. In the interactive mode, the neuron structure is represented in multiples ways that are all linked (<xref ref-type="fig" rid="F9">Figure 9</xref>), thus improving the ease and accuracy of editing the SWC structure.</p>
    <p>Our automatic reconstruction algorithm is specifically designed for meeting challenges of tracing from images obtained from biocytin-filled neurons. For this purpose, it outperforms neuTube and Vaa3D (<xref ref-type="fig" rid="F15">Figure 15</xref>). There are some caveats, however, as neuTube and Vaa3D were developed to trace neurons with fluorescent images. Although we preprocessed images to optimize the application of Vaa3D and neuTube to biocytin-filled neurons, there could be some other preprocessing methods that lead to better results. Also, the performance of the algorithms may strongly depend on images and neuron types. For comparing automatic reconstruction algorithms of neurons, a better approach could be something like DIADEM (Brown et al., <xref rid="B3" ref-type="bibr">2011</xref>), in which the creators themselves tune the parameters for a common dataset and a diverse group of users edit them and measure NEOs on a suitable GUI platform, such as ShuTu.</p>
    <p>Commercial solutions for neuron reconstruction also exist (e.g., Neurolucida 360, MBF Bioscience; Imaris FilamentTracer, Bitplane). Detailed comparison of ShuTu to these other software packages is difficult, as it requires mastery of all of them to be fair. We encourage authors and users of other software packages to test ShuTu on their dataset or test their favorites on the images used in this work. We welcome feedback from the user community.</p>
    <p>Staining neurons with biocytin is common in patch-clamp experiments. However, methods for reconstructing neurons based on biocytin are limited. When dealing with bright-field images like the biocytin data, a common strategy is to apply some preprocessing method first (Narayanaswamy et al., <xref rid="B21" ref-type="bibr">2011</xref>; Türetken et al., <xref rid="B30" ref-type="bibr">2011</xref>; Zhou et al., <xref rid="B34" ref-type="bibr">2015</xref>), making the images friendly for automatic reconstruction. Preprocessing, however, is often computationally intensive and does not guarantee good performance. ShuTu is specifically tailored to deal with inherent problems with images from biocytin filled neurons and does not require preprocessing.</p>
    <p>ShuTu is not restricted to biocytin-filled neurons. We have shown that it can also handle images from confocal and fluorescent microscopy, simply by inverting the images. However, we made no attempt to optimize ShuTu for this application. For now, we have chosen to leave this enhancement to future iterations (by us or others) and instead optimize ShuTu for one common method of staining and imaging patch-clamped neurons.</p>
    <p>Improving image quality will inevitably improve the efficiency and accuracy of neuron reconstruction. Users should ensure that high quality images are obtained. Tissue fixation and clearing processes can also influence the accuracy of the reconstructed neurons by causing tissue shrinkage or influencing image quality. To be accurate, these factors need to be quantified for specific experimental conditions and the dimensions of the reconstructed neurons need to be adjusted to account for shrinkage and distortion.</p>
    <p>Accurate estimates of dendritic diameter are important for computational modeling of neurons (Anwar et al., <xref rid="B2" ref-type="bibr">2014</xref>; Psarrou et al., <xref rid="B26" ref-type="bibr">2014</xref>). Our automated algorithm works quite well in determining the diameters, as shown in <xref ref-type="fig" rid="F7">Figures 7B,C</xref>. Manual adjustments of the diameters are rarely necessary during the manual editing stage. However, systematic biases can exist, especially if spines are densely stained and blurred in the images, for example in the Purkinje neuron shown in <xref ref-type="fig" rid="F14">Figure 14C</xref>. Users need to account for these biases before using the SWC structures for simulations.</p>
    <p>ShuTu has some limitations. It is not designed to trace axons, which are often too faint following biocytin staining in slices, and therefore difficult to trace automatically (and in many cases even manually). ShuTu also provides no mechanism for marking spines. It is possible that editing operations currently requiring human judgements, such as when dendritic branches closely cross each other, could be automated in the future using machine learning approaches (Turaga et al., <xref rid="B29" ref-type="bibr">2010</xref>).</p>
    <p>In conclusion, we have shown that ShuTu provides a practical solution for efficient and accurate reconstruction of neuron morphology. The open-source nature of the software will allow the research community to improve the tool further, and increased efficiency in neuronal reconstruction should facilitate more studies incorporating quantitative metrics of dendritic morphology and computer simulations of dendritic function.</p>
  </sec>
  <sec sec-type="materials and methods" id="s4">
    <title>Materials and Methods</title>
    <sec>
      <title>Whole-Cell Recording and Neuron Staining</title>
      <p>Acute hippocampal slices were prepared from mice and rats (17–30 days old). After animals were deeply anesthetized with isoflurane, they were decapitated and the brain rapidly removed into chilled cutting solution consisting of (in mM) 215 sucrose, 2.5 KCl, 20 glucose, 26 NaHCO<sub>3</sub>, 1.6 NaH<sub>2</sub>PO<sub>4</sub>, 1 CaCl<sub>2</sub>, 4 MgCl<sub>2</sub>, and 4 MgSO<sub>4</sub>. Hippocampi were dissected out and cut into 400 μm thick transverse sections on a Leica VT-1200S vibrating microslicer (Leica, Ltd., Germany). The cutting solution was slowly exchanged with artificial cerebrospinal fluid (ACSF) containing (in mM) 124 NaCl, 2.5 KCl, 10 glucose, 26 NaHCO<sub>3</sub>, 1.0 NaH<sub>2</sub>PO<sub>4</sub>, 2.0 CaCl<sub>2</sub>, and 1.0 MgCl<sub>2</sub>. The slices were incubated at room temperature for at least 1 h before recording, and then were transferred as needed to a submersion-type recording chamber perfused with ACSF at 2 ml/min. For rat hippocampal slices, the cutting solution contained (in mM) 204.5 sucrose, 2.5 KCl, 1.25 NaH<sub>2</sub>PO<sub>4</sub>, 28 NaHCO<sub>3</sub>, 7 dextrose, 3 Na-pyruvate, 1 Na-ascorbate, 0.5 CaCl<sub>2</sub>, 7 MgCl<sub>2</sub>, and the ACSF contained (in mM) 125 NaCl, 2.5 KCl, 1.25 NaH<sub>2</sub>PO<sub>4</sub>, 25 NaHCO<sub>3</sub>, 25 dextrose, 3 Na-pyruvate, 1 Na-ascorbate, 1.3 CaCl<sub>2</sub>, 1 MgCl<sub>2</sub>. 350-μm-thick slices were sectioned at an oblique angle. After 30–60 min of recovery in ACSF at 35–37°C, the chamber was maintained at room temperature. Both cutting and ACSF solutions were saturated with 95% O<sub>2</sub> and 5% CO<sub>2</sub> (pH 7.4).</p>
      <p>Whole-cell recordings were obtained by visualized patch techniques under IR-DIC optics. The recording pipette resistance ranged between 4 and 6 <italic>MΩ</italic>. Series resistance (6 - 15<italic>MΩ</italic>) and input resistance were monitored throughout each voltage-clamp or current-clamp recording. Recordings with &gt;10% change in series resistance were excluded. For mice, the intracellular pipette solution consisted of (in mM) 135 K-gluconate, 5 KCl, 1 CaCl<sub>2</sub>, 0.1 EGTA-Na, 10 HEPES, 10 glucose, 5 MgATP, 0.4 Na<sub>3</sub>GTP, and 0.1% biocytin, pH 7.2, 280–290 mOsm; for rats, the solution consisted of (in mM) 130 K-gluconate, 10 KCl, 10 Na<sub>2</sub>-phosphocreatin, 10 HEPES, 4 Mg-ATP, 0.3 Na-GTP, 50 μM Alexa Fluor 594 (Invitrogen, Waltham, MA), and 0.2% biocytin, pH 7.2, 295 mOsm. Resting potential ranged from −69 to −58 mV. Maximal recording time after dissection was 6 hr. Recording temperature was set to 32.0 ± 0.1 C° for mouse slices and to 33–35°C for rat slices using a TC-344A single-channel temperature controller (Warner Instruments, Inc, Hamden, CT, USA). All experiments were executed with a Dagan BVC-700A amplifier, digitized (3–5 kHz) using an ITC-16 analog-to-digital converter (Instrutech) or BNC-2090 and BNC-2110 boards (National Instruments, Austin, TX), and analyzed using custom-made software for IgorPro (Wavemetrics Inc., Lake Oswego, OR, USA). All chemicals were purchased from Sigma-Aldrich (St. Louis, MO, USA), Fisher Scientific (Fair Lawn, NJ) or Fluka (St. Louis, MO). Biocytin was purchased from Sigma-Aldrich. Neurons filled with biocytin were fixed (12–24 h) with paraformaldehyde (4%) after recording, then washed in 1X PBS solution. Biocytin staining was carried out with Vector PK4000 and SK4100 kits (Vector Laboratories, Burlingame, CA, USA). Tiled z-stack image acquisition was performed using a Zeiss AxioImager microscope with an AxioCam MRc camera (Zeiss) and ZEN software (blue edition; Zeiss) at 100× or 63× magnification.</p>
      <p>Sparse labeling of cerebellar Purkinje cells was achieved by <italic>in utero</italic> ventricular viral injection (100 nL per ventricle) at embryonic day 14 (E14) with an adeno-associated virus (Pseudo type 2.1) carrying a GFP payload expressed under the CAG promoter. Once animals reached post-natal day 30 (P30) they were transcardially perfused with 4% paraformaldehyde fixative. Fixed brains were then sectioned at 100 μm thickness (horizontal plane) using a Leica micro-slicer and mounted for microscopy. Tiled z-stack image acquisition was performed using a Zeiss 880 confocal microscope at 63× magnification.</p>
    </sec>
    <sec>
      <title>System Requirements and Installation</title>
      <p>ShuTu consists of two parts: one for processing images and automated reconstruction, and the other for viewing and editing the morphology using graphical user interface (GUI). The software requires installation of Open MPI and C compiler. The software package was tested on a desktop computer with Intel Core i7-4770 CPU@3.40GHz CPU and 16 GB memory, running Ubuntu 14.04 LTS. These are typical settings for current high-end desktop computers. Multiple processors are desirable since the algorithms are designed to utilize multiple processors to speed up computation. However, the memory usage must be monitored to make sure that the demand on memory does not exceed 100%. The number of processors used is specified as a parameter for the command line when running the code for automated reconstruction (see the section “Automated reconstruction” below).</p>
      <p>ShuTu can be downloaded from</p>
      <preformat><ext-link ext-link-type="uri" xlink:href="http://personal.psu.edu/dzj2/ShuTu/">http://personal.psu.edu/dzj2/ShuTu/</ext-link>,
</preformat>
      <p>or</p>
      <preformat><ext-link ext-link-type="uri" xlink:href="https://www.janelia.org/shutu.">https://www.janelia.org/shutu</ext-link>.
</preformat>
      <p>An installation script is provided for Ubuntu and Mac OSX systems. This download includes all source codes for processing images and automatic reconstruction. It also includes the GUI program for viewing and editing the morphology. The source code for the GUI program is available at</p>
      <preformat>
  <ext-link ext-link-type="uri" xlink:href="https://github.com/tingzhao/ShuTu">https://github.com/tingzhao/ShuTu</ext-link>.
</preformat>
      <p>On Windows 10, one can install the Ubuntu App and proceed as in Ubuntu, except that the GUI program is downloaded and installed separately and runs in Windows system.</p>
      <p>In the directory of ShuTu, one can run</p>
      <preformat>
sudo sh build.sh,
</preformat>
      <p>which checks and installs necessary software including Open MPI. The C programs are also compiled.</p>
    </sec>
    <sec>
      <title>Image Acquisition and Processing</title>
      <p>The software works with tiles of tiff stacks covering the entire neurons. Nearby tiles overlap, typically by 20%, to help fine tune the relative positions of the tiles (“stitching”). The names of the tiff stacks use the convention of a common string (<monospace>filenameCommon</monospace> followed by a number and <monospace>.tif</monospace>). With the <italic>x, y</italic> positions of the tiles specified, one can use the program <monospace>stitchTiles</monospace> to stitch the tiles. The results are stored in file <monospace>filenameCommon.json</monospace>.</p>
      <p>Modern microscopes often allow automatic generations of overlapping tiles of tiff stacks. In our case, we imaged hippocampal neurons with Zeiss Axio Imager with AxioCam and Zen blue software. Once the boundary in the field of view (<italic>XY</italic>) and the range of the depths (<italic>Z</italic>) that contain the neuron are set, the images at each tile position and depths are automatically taken, and the positions of the image are stored in an <monospace>xml</monospace> file. The filenames of these images contain information about the tile number and depth. Using them, we assemble all images at different depths for each tile into one tiff stack. The command is</p>
      <preformat>
mpirun -n numProc ./createTiffStacksZeiss 
dirData filenameCommon
</preformat>
      <p>Here <monospace>numProc</monospace> is the number of processors to be used; and <monospace>dirData</monospace> is the path to the directory in which the <monospace>xml</monospace> file resides. The images of the planes are stored in a subdirectory. <monospace>filenameCommon</monospace> is the common part of the names given by the user to the created tiff stacks.</p>
      <p>The user can generate the overlapping tiff stacks in other ways. The files should be named in the format of <monospace>filenameCommon1.tif</monospace>, <monospace>filenameCommon2.tif</monospace>, etc.</p>
      <p>The tiff stacks are preprocessed using the command</p>
      <preformat>
mpirun -n numProc ./processImages dirData
</preformat>
      <p>If the images are dark-field, the command should be</p>
      <preformat>
mpirun -n numProc ./processImages dirData 1
</preformat>
      <p>In this case, the images are inverted into bright-field images. The original images are renamed by adding <monospace>.org.tif</monospace> to the end of the original file names, and are moved to a directory <monospace>OriginalImages</monospace>.</p>
      <p>Stitching the images is done with program <monospace>stitchTiles</monospace>. It is assumed that in <monospace>dirData</monospace> there exists the <monospace>xml</monospace> file, generated by the Zen Blue software during automatic image acquisition; or a text file <monospace>tileSequences.txt</monospace>, with the following format explained with an example:</p>
      <preformat>
80
1, 2, 3, 4
1, 5
5, 6, 7, 8
6, 9
9
</preformat>
      <p>The first line is a single number specifying the shifts in percentages (100% overlap). The second line specifies the tile numbers of the first row scanning from left to right. The third line specifies two connected tiles in the first row and the second row. The fourth line specifies the tile numbers in the second row. This continues until the last row is specified. With one of these files in the directory, the command for stitching is</p>
      <preformat>
mpirun -n numProc ./stitchTiles dataDir
</preformat>
      <p>After stitching is done, one can proceed to reconstruct the neuron semi-automatically using the GUI program of <monospace>ShuTu</monospace> (see <xref ref-type="supplementary-material" rid="SM1">Appendix 1</xref>). Another choice is to run the automatic reconstruction algorithm to create a draft reconstruction and edit it using the GUI program (see below).</p>
      <p>In stitching, the precise offsets of nearby tiles are computed by maximizing phase correlation (Zitova and Flusser, <xref rid="B35" ref-type="bibr">2003</xref>). Using the maximum spanning tree algorithm (Graham and Hell, <xref rid="B11" ref-type="bibr">1985</xref>), a tree graph connecting all tiles and maximizing the sum of phase correlations along the connected nearby tiles is computed and used to set the relative coordinates of all tiles.</p>
      <p>If the entire neuron is contained in a single tiff stack, the above processing steps should be skipped.</p>
    </sec>
    <sec>
      <title>Automated Reconstruction</title>
      <p>The code for automated reconstruction is parallelized with MPI protocol, and runs with the command</p>
      <preformat>
mpirun -n numProc ./ShuTuAutoTrace dataDir
ShuTu.Parameters.dat,
</preformat>
      <p>where <monospace>ShuTu.Parameters.dat</monospace> is a text file that contains the parameters. This creates an SWC file <monospace>filenameCommon.auto.swc</monospace> in <monospace>dataDir</monospace>, which can be loaded in <monospace>ShuTu</monospace> for manual editing (<monospace>File</monospace>→<monospace>Load</monospace>
<monospace>SWC</monospace>).</p>
      <p>To automatically reconstruct neurites in a single tiff stack, one can run</p>
      <preformat>
./ShuTuAutoTraceOneStack dataDir/filename.
tif ShuTu.Parameters.dat.
</preformat>
      <p>The resulting SWC file is stored in <monospace>filename.tif.auto.swc</monospace> in <monospace>dataDir</monospace>. This should be used if the entire neuron is stored in a single tiff stack. It is also useful for tuning the parameters. Note that if the image is dark field, one needs to edit the parameter file and set the image type parameter to 1.</p>
    </sec>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>All datasets generated for this study are included in the article/<xref ref-type="supplementary-material" rid="SM1">Supplementary Material</xref>, and at <ext-link ext-link-type="uri" xlink:href="http://personal.psu.edu/dzj2/ShuTu/">http://personal.psu.edu/dzj2/ShuTu/</ext-link>.</p>
  </sec>
  <sec id="s6">
    <title>Ethics Statement</title>
    <p>The animal study was reviewed and approved by Institutional Animal Care and Use Committee of the Janelia Research Campus.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>DJ and TZ devised the algorithm and software. DH and C-LH performed patch-clamp recordings and imaged neurons. RT imaged neurons. DJ and NS wrote the paper.</p>
    <sec>
      <title>Conflict of Interest</title>
      <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <p>Research for DJ was supported by the Visiting Scientist Program at the Janelia Research Campus, Howard Hughes Medical Institute; and by the Huck Institute of Life Sciences at the Pennsylvania State University. We thank Linqing Feng for technical support on Windows platform, and Brenda Shields for histological processing. This manuscript was previously released as a Pre-Print at bioRxiv (Jin et al., <xref rid="B16" ref-type="bibr">2017</xref>).</p>
  </ack>
  <sec sec-type="supplementary-material" id="s8">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2019.00068/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fninf.2019.00068/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Data_Sheet_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acciai</surname><given-names>L.</given-names></name><name><surname>Soda</surname><given-names>P.</given-names></name><name><surname>Iannello</surname><given-names>G.</given-names></name></person-group> (<year>2016</year>). <article-title>Automated neuron tracing methods: an updated account</article-title>. <source>Neuroinformatics</source>
<volume>14</volume>, <fpage>353</fpage>–<lpage>367</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-016-9310-0</pub-id><?supplied-pmid 27447185?><pub-id pub-id-type="pmid">27447185</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anwar</surname><given-names>H.</given-names></name><name><surname>Roome</surname><given-names>C. J.</given-names></name><name><surname>Nedelescu</surname><given-names>H.</given-names></name><name><surname>Chen</surname><given-names>W.</given-names></name><name><surname>Kuhn</surname><given-names>B.</given-names></name><name><surname>De Schutter</surname><given-names>E.</given-names></name></person-group> (<year>2014</year>). <article-title>Dendritic diameters affect the spatial variability of intracellular calcium dynamics in computer models</article-title>. <source>Front. Cell. Neurosci.</source>
<volume>8</volume>:<fpage>168</fpage>. <pub-id pub-id-type="doi">10.3389/fncel.2014.00168</pub-id><?supplied-pmid 25100945?><pub-id pub-id-type="pmid">25100945</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>K. M.</given-names></name><name><surname>Barrionuevo</surname><given-names>G.</given-names></name><name><surname>Canty</surname><given-names>A. J.</given-names></name><name><surname>De Paola</surname><given-names>V.</given-names></name><name><surname>Hirsch</surname><given-names>J. A.</given-names></name><name><surname>Jefferis</surname><given-names>G. S. X. E.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>The DIADEM data sets: representative light microscopy images of neuronal morphology to advance automation of digital reconstructions</article-title>. <source>Neuroinformatics</source><volume>9</volume>, <fpage>143</fpage>–<lpage>157</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-010-9095-5</pub-id><?supplied-pmid 21249531?><pub-id pub-id-type="pmid">21249531</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cannon</surname><given-names>R. C.</given-names></name><name><surname>Turner</surname><given-names>D. A.</given-names></name><name><surname>Pyapali</surname><given-names>G. K.</given-names></name><name><surname>Wheal</surname><given-names>H. V.</given-names></name></person-group> (<year>1998</year>). <article-title>An on-line archive of reconstructed hippocampal neurons</article-title>. <source>J. Neurosci. Methods</source>
<volume>84</volume>, <fpage>49</fpage>–<lpage>54</lpage>. <?supplied-pmid 9821633?><pub-id pub-id-type="pmid">9821633</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Capowski</surname><given-names>J. J.</given-names></name></person-group> (<year>1983</year>). <article-title>An automatic neuron reconstruction system</article-title>. <source>J. Neurosci. Methods</source>
<volume>8</volume>, <fpage>353</fpage>–<lpage>364</lpage>. <?supplied-pmid 6353082?><pub-id pub-id-type="pmid">6353082</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>L.</given-names></name><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Kim</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Neutube 1.0: a new design for efficient neuron reconstruction software based on the swc format</article-title>. <source>Eneuro</source>
<volume>2</volume>:ENEURO-0049. <pub-id pub-id-type="doi">10.1523/ENEURO.0049-14.2015</pub-id><?supplied-pmid 26464967?><pub-id pub-id-type="pmid">26464967</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Frangi</surname><given-names>A. F.</given-names></name><name><surname>Niessen</surname><given-names>W. J.</given-names></name><name><surname>Vincken</surname><given-names>K. L.</given-names></name><name><surname>Viergever</surname><given-names>M. A.</given-names></name></person-group> (<year>1998</year>). <article-title>Multiscale vessel enhancement filtering</article-title>, in <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source> (<publisher-loc>Berlin; Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>130</fpage>–<lpage>137</lpage>.</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gidon</surname><given-names>A.</given-names></name><name><surname>Segev</surname><given-names>I.</given-names></name></person-group> (<year>2012</year>). <article-title>Principles governing the operation of synaptic inhibition in dendrites</article-title>. <source>Neuron</source>
<volume>75</volume>, <fpage>330</fpage>–<lpage>341</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2012.05.015</pub-id><?supplied-pmid 22841317?><pub-id pub-id-type="pmid">22841317</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillette</surname><given-names>T. A.</given-names></name><name><surname>Brown</surname><given-names>K. M.</given-names></name><name><surname>Ascoli</surname><given-names>G. A.</given-names></name></person-group> (<year>2011a</year>). <article-title>The DIADEM metric: comparing multiple reconstructions of the same neuron</article-title>. <source>Neuroinformatics</source>
<volume>9</volume>, <fpage>233</fpage>–<lpage>245</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9117-y</pub-id><?supplied-pmid 21519813?><pub-id pub-id-type="pmid">21519813</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillette</surname><given-names>T. A.</given-names></name><name><surname>Brown</surname><given-names>K. M.</given-names></name><name><surname>Svoboda</surname><given-names>K.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Ascoli</surname><given-names>G. A.</given-names></name></person-group> (<year>2011b</year>). <article-title>DIADEMchallenge.Org: a compendium of resources fostering the continuous development of automated neuronal reconstruction</article-title>. <source>Neuroinformatics</source>
<volume>9</volume>, <fpage>303</fpage>–<lpage>304</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9104-3</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>R. L.</given-names></name><name><surname>Hell</surname><given-names>P.</given-names></name></person-group> (<year>1985</year>). <article-title>On the history of the minimum spanning tree problem</article-title>. <source>Ann. Hist. Comput.</source>
<volume>7</volume>, <fpage>43</fpage>–<lpage>57</lpage>. <pub-id pub-id-type="doi">10.1109/MAHC.1985.10011</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamill</surname><given-names>O. P.</given-names></name><name><surname>Marty</surname><given-names>A.</given-names></name><name><surname>Neher</surname><given-names>E.</given-names></name><name><surname>Sakmann</surname><given-names>B.</given-names></name><name><surname>Sigworth</surname><given-names>F.</given-names></name></person-group> (<year>1981</year>). <article-title>Improved patch-clamp techniques for high-resolution current recording from cells and cell-free membrane patches</article-title>. <source>Pflügers Archiv Eur J Physiol.</source>
<volume>391</volume>, <fpage>85</fpage>–<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1007/BF00656997</pub-id><?supplied-pmid 6270629?><pub-id pub-id-type="pmid">6270629</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henze</surname><given-names>D. A.</given-names></name><name><surname>Cameron</surname><given-names>W. E.</given-names></name><name><surname>Barrionuevo</surname><given-names>G.</given-names></name></person-group> (<year>1996</year>). <article-title>Dendritic morphology and its effects on the amplitude and rise-time of synaptic signals in hippocampal CA3 pyramidal cells</article-title>. <source>J. Compar. Neurol.</source>
<volume>369</volume>, <fpage>331</fpage>–<lpage>344</lpage>. <pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19960603)369:3&lt;331::AID-CNE1&gt;3.0.CO;2-6</pub-id><?supplied-pmid 8743416?><pub-id pub-id-type="pmid">8743416</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>D.</given-names></name></person-group> (<year>2001</year>). <article-title>Accurate reconstruction of neuronal morphology</article-title>, in <source>Computational Neuroscience: Realistic Modeling for Experimentalists</source>, ed <person-group person-group-type="editor"><name><surname>De Schutter</surname><given-names>E.</given-names></name></person-group> (<publisher-loc>Boca Raton, FL</publisher-loc>: <publisher-name>CRC Press</publisher-name>), <fpage>159</fpage>–<lpage>178</lpage>.</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>D. Z.</given-names></name></person-group> (<year>2019</year>). <source>Website for ShuTu</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="http://personal.psu.edu/dzj2/ShuTu/">http://personal.psu.edu/dzj2/ShuTu/</ext-link></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>D. Z.</given-names></name><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Hunt</surname><given-names>D. L.</given-names></name><name><surname>Pearcy</surname><given-names>R.</given-names></name><name><surname>Hsu</surname><given-names>C.-L.</given-names></name><name><surname>Spruston</surname><given-names>N.</given-names></name></person-group> (<year>2017</year>). <article-title>ShuTu: open-source software for efficient and accurate reconstruction of dendritic morphology</article-title>. <source>bioRxiv [preprint]</source>
<fpage>226548</fpage>
<pub-id pub-id-type="doi">10.1101/226548</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krichmar</surname><given-names>J. L.</given-names></name><name><surname>Nasuto</surname><given-names>S. J.</given-names></name><name><surname>Scorcioni</surname><given-names>R.</given-names></name><name><surname>Washington</surname><given-names>S. D.</given-names></name><name><surname>Ascoli</surname><given-names>G. A.</given-names></name></person-group> (<year>2002</year>). <article-title>Effects of dendritic morphology on CA3 pyramidal cell electrophysiology: a simulation study</article-title>. <source>Brain Res.</source>
<volume>941</volume>, <fpage>11</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1016/s0006-8993(02)02488-5</pub-id><?supplied-pmid 12031543?><pub-id pub-id-type="pmid">12031543</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y.</given-names></name></person-group> (<year>2011</year>). <article-title>The DIADEM and beyond</article-title>. <source>Neuroinformatics</source>
<volume>9</volume>, <fpage>99</fpage>–<lpage>102</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9102-5</pub-id><?supplied-pmid 21431331?><pub-id pub-id-type="pmid">21431331</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mainen</surname><given-names>Z. F.</given-names></name><name><surname>Sejnowski</surname><given-names>T. J.</given-names></name></person-group> (<year>1996</year>). <article-title>Influence of dendritic structure on firing pattern in model neocortical neurons</article-title>. <source>Nature</source>
<volume>382</volume>, <fpage>363</fpage>–<lpage>366</lpage>. <pub-id pub-id-type="doi">10.1038/382363a0</pub-id><?supplied-pmid 8684467?><pub-id pub-id-type="pmid">8684467</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menon</surname><given-names>V.</given-names></name><name><surname>Musial</surname><given-names>T. F.</given-names></name><name><surname>Liu</surname><given-names>A.</given-names></name><name><surname>Katz</surname><given-names>Y.</given-names></name><name><surname>Kath</surname><given-names>W. L.</given-names></name><name><surname>Spruston</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2013</year>). <article-title>Balanced synaptic impact via distance-dependent synapse distribution and complementary expression of ampars and nmdars in hippocampal dendrites</article-title>. <source>Neuron</source><volume>80</volume>, <fpage>1451</fpage>–<lpage>1463</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.027</pub-id><?supplied-pmid 24360547?><pub-id pub-id-type="pmid">24360547</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narayanaswamy</surname><given-names>A.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Roysam</surname><given-names>B.</given-names></name></person-group> (<year>2011</year>). <article-title>3-D image pre-processing algorithms for improved automated tracing of neuronal arbors</article-title>. <source>Neuroinformatics</source>
<volume>9</volume>, <fpage>219</fpage>–<lpage>231</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9116-z</pub-id><?supplied-pmid 21537877?><pub-id pub-id-type="pmid">21537877</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parekh</surname><given-names>R.</given-names></name><name><surname>Ascoli</surname><given-names>G. A.</given-names></name></person-group> (<year>2013</year>). <article-title>Neuronal morphology goes digital: a research hub for cellular and system neuroscience</article-title>. <source>Neuron</source>
<volume>77</volume>, <fpage>1017</fpage>–<lpage>1038</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.03.008</pub-id><?supplied-pmid 23522039?><pub-id pub-id-type="pmid">23522039</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Bria</surname><given-names>A.</given-names></name><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Iannello</surname><given-names>G.</given-names></name><name><surname>Long</surname><given-names>F.</given-names></name></person-group> (<year>2014</year>). <article-title>Extensible visualization and analysis for multidimensional images using Vaa3D</article-title>. <source>Nat. Protoc.</source>
<volume>9</volume>, <fpage>193</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1038/nprot.2014.011</pub-id><?supplied-pmid 24385149?><pub-id pub-id-type="pmid">24385149</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Hawrylycz</surname><given-names>M.</given-names></name><name><surname>Roskams</surname><given-names>J.</given-names></name><name><surname>Hill</surname><given-names>S.</given-names></name><name><surname>Spruston</surname><given-names>N.</given-names></name><name><surname>Meijering</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Bigneuron: large-scale 3d neuron reconstruction from optical microscopy images</article-title>. <source>Neuron</source><volume>87</volume>, <fpage>252</fpage>–<lpage>256</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.036</pub-id><?supplied-pmid 26182412?><pub-id pub-id-type="pmid">26182412</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H.</given-names></name><name><surname>Ruan</surname><given-names>Z.</given-names></name><name><surname>Long</surname><given-names>F.</given-names></name><name><surname>Simpson</surname><given-names>J. H.</given-names></name><name><surname>Myers</surname><given-names>E. W.</given-names></name></person-group> (<year>2010</year>). <article-title>V3d enables real-time 3d visualization and quantitative analysis of large-scale biological image data sets</article-title>. <source>Nat. Biotechnol.</source>
<volume>28</volume>:<fpage>348</fpage>. <pub-id pub-id-type="doi">10.1038/nbt.1612</pub-id><?supplied-pmid 20231818?><pub-id pub-id-type="pmid">20231818</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Psarrou</surname><given-names>M.</given-names></name><name><surname>Stefanou</surname><given-names>S. S.</given-names></name><name><surname>Papoutsi</surname><given-names>A.</given-names></name><name><surname>Tzilivaki</surname><given-names>A.</given-names></name><name><surname>Cutsuridis</surname><given-names>V.</given-names></name><name><surname>Poirazi</surname><given-names>P.</given-names></name></person-group> (<year>2014</year>). <article-title>A simulation study on the effects of dendritic morphology on layer v prefrontal pyramidal cell firing behavior</article-title>. <source>Front. Cell. Neurosci.</source>
<volume>8</volume>:<fpage>287</fpage>. <pub-id pub-id-type="doi">10.3389/fncel.2014.00287</pub-id><?supplied-pmid 25278837?><pub-id pub-id-type="pmid">25278837</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stuart</surname><given-names>G.</given-names></name><name><surname>Spruston</surname><given-names>N.</given-names></name></person-group> (<year>1998</year>). <article-title>Determinants of voltage attenuation in neocortical pyramidal neuron dendrites</article-title>. <source>J. Neurosci.</source>
<volume>18</volume>, <fpage>3501</fpage>–<lpage>3510</lpage>. <?supplied-pmid 9570781?><pub-id pub-id-type="pmid">9570781</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Svoboda</surname><given-names>K.</given-names></name></person-group> (<year>2011</year>). <article-title>The past, present, and future of single neuron reconstruction</article-title>. <source>Neuroinformatics</source>
<volume>9</volume>, <fpage>97</fpage>–<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9097-y</pub-id><?supplied-pmid 21279476?><pub-id pub-id-type="pmid">21279476</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turaga</surname><given-names>S. C.</given-names></name><name><surname>Murray</surname><given-names>J. F.</given-names></name><name><surname>Jain</surname><given-names>V.</given-names></name><name><surname>Roth</surname><given-names>F.</given-names></name><name><surname>Helmstaedter</surname><given-names>M.</given-names></name><name><surname>Briggman</surname><given-names>K.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>Convolutional networks can learn to generate affinity graphs for image segmentation</article-title>. <source>Neural Comput.</source><volume>22</volume>, <fpage>511</fpage>–<lpage>538</lpage>. <pub-id pub-id-type="doi">10.1162/neco.2009.10-08-881</pub-id><?supplied-pmid 19922289?><pub-id pub-id-type="pmid">19922289</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Türetken</surname><given-names>E.</given-names></name><name><surname>González</surname><given-names>G.</given-names></name><name><surname>Blum</surname><given-names>C.</given-names></name><name><surname>Fua</surname><given-names>P.</given-names></name></person-group> (<year>2011</year>). <article-title>Automated reconstruction of dendritic and axonal trees by global optimization with geometric priors</article-title>. <source>Neuroinformatics</source>
<volume>9</volume>, <fpage>279</fpage>–<lpage>302</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9122-1</pub-id><?supplied-pmid 21573886?><pub-id pub-id-type="pmid">21573886</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>H.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2013</year>). <article-title>App2: automatic tracing of 3d neuron morphology based on hierarchical pruning of a gray-weighted image distance-tree</article-title>. <source>Bioinformatics</source>
<volume>29</volume>, <fpage>1448</fpage>–<lpage>1454</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btt170</pub-id><?supplied-pmid 23603332?><pub-id pub-id-type="pmid">23603332</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zandt</surname><given-names>B. J.</given-names></name><name><surname>Losnegård</surname><given-names>A.</given-names></name><name><surname>Hodneland</surname><given-names>E.</given-names></name><name><surname>Veruki</surname><given-names>M. L.</given-names></name><name><surname>Lundervold</surname><given-names>A.</given-names></name><name><surname>Hartveit</surname><given-names>E.</given-names></name></person-group> (<year>2017</year>). <article-title>Semi-automatic 3d morphological reconstruction of neurons with densely branching morphology: Application to retinal aii amacrine cells imaged with multi-photon excitation microscopy</article-title>. <source>J. Neurosci. Methods</source>
<volume>279</volume>, <fpage>101</fpage>–<lpage>118</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.01.008</pub-id><?supplied-pmid 28115187?><pub-id pub-id-type="pmid">28115187</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>T.</given-names></name><name><surname>Xie</surname><given-names>J.</given-names></name><name><surname>Amat</surname><given-names>F.</given-names></name><name><surname>Clack</surname><given-names>N.</given-names></name><name><surname>Ahammad</surname><given-names>P.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Automated reconstruction of neuronal morphology based on local geometrical and global structural models</article-title>. <source>Neuroinformatics</source><volume>9</volume>, <fpage>247</fpage>–<lpage>261</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-011-9120-3</pub-id><?supplied-pmid 21547564?><pub-id pub-id-type="pmid">21547564</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z.</given-names></name><name><surname>Sorensen</surname><given-names>S.</given-names></name><name><surname>Zeng</surname><given-names>H.</given-names></name><name><surname>Hawrylycz</surname><given-names>M.</given-names></name><name><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>). <article-title>Adaptive image enhancement for tracing 3d morphologies of neurons and brain vasculatures</article-title>. <source>Neuroinformatics</source>
<volume>13</volume>, <fpage>153</fpage>–<lpage>166</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-014-9249-y</pub-id><?supplied-pmid 25310965?><pub-id pub-id-type="pmid">25310965</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zitova</surname><given-names>B.</given-names></name><name><surname>Flusser</surname><given-names>J.</given-names></name></person-group> (<year>2003</year>). <article-title>Image registration methods: a survey</article-title>. <source>Image Vis. Comput.</source>
<volume>21</volume>, <fpage>977</fpage>–<lpage>1000</lpage>. <pub-id pub-id-type="doi">10.1016/S0262-8856(03)00137-9</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
