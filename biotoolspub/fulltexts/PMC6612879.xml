<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612879</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz338</article-id>
    <article-id pub-id-type="publisher-id">btz338</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Comparative and Functional Genomics</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Weighted elastic net for unsupervised domain adaptation with application to age prediction from DNA methylation data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Handl</surname>
          <given-names>Lisa</given-names>
        </name>
        <xref ref-type="aff" rid="btz338-aff1">1</xref>
        <xref ref-type="aff" rid="btz338-aff2">2</xref>
        <xref ref-type="aff" rid="btz338-aff3">3</xref>
        <xref ref-type="corresp" rid="btz338-cor1"/>
        <!--<email>lisa.handl@uni-tuebingen.de</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jalali</surname>
          <given-names>Adrin</given-names>
        </name>
        <xref ref-type="aff" rid="btz338-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Scherer</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="btz338-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Eggeling</surname>
          <given-names>Ralf</given-names>
        </name>
        <xref ref-type="aff" rid="btz338-aff2">2</xref>
        <xref ref-type="aff" rid="btz338-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pfeifer</surname>
          <given-names>Nico</given-names>
        </name>
        <xref ref-type="aff" rid="btz338-aff1">1</xref>
        <xref ref-type="aff" rid="btz338-aff2">2</xref>
        <xref ref-type="aff" rid="btz338-aff3">3</xref>
        <xref ref-type="corresp" rid="btz338-cor1"/>
        <!--<email>pfeifer@informatik.uni-tuebingen.de</email>-->
      </contrib>
    </contrib-group>
    <aff id="btz338-aff1"><label>1</label>Department for Computational Biology and Applied Algorithmics, Max Planck Institute for Informatics, Saarbrücken, Germany</aff>
    <aff id="btz338-aff2"><label>2</label>Department of Computer Science, University of Tübingen, Tübingen, Germany</aff>
    <aff id="btz338-aff3"><label>3</label>Institute for Biomedical Informatics, University of Tübingen, Tübingen, Germany</aff>
    <author-notes>
      <corresp id="btz338-cor1">To whom correspondence should be addressed. <email>lisa.handl@uni-tuebingen.de</email> or <email>pfeifer@informatik.uni-tuebingen.de</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i154</fpage>
    <lpage>i163</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz338.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Predictive models are a powerful tool for solving complex problems in computational biology. They are typically designed to predict or classify data coming from the same unknown distribution as the training data. In many real-world settings, however, uncontrolled biological or technical factors can lead to a distribution mismatch between datasets acquired at different times, causing model performance to deteriorate on new data. A common additional obstacle in computational biology is scarce data with many more features than samples. To address these problems, we propose a method for unsupervised domain adaptation that is based on a weighted elastic net. The key idea of our approach is to compare dependencies between inputs in training and test data and to increase the cost of differently behaving features in the elastic net regularization term. In doing so, we encourage the model to assign a higher importance to features that are robust and behave similarly across domains.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We evaluate our method both on simulated data with varying degrees of distribution mismatch and on real data, considering the problem of age prediction based on DNA methylation data across multiple tissues. Compared with a non-adaptive standard model, our approach substantially reduces errors on samples with a mismatched distribution. On real data, we achieve far lower errors on cerebellum samples, a tissue which is not part of the training data and poorly predicted by standard models. Our results demonstrate that unsupervised domain adaptation is possible for applications in computational biology, even with many more features than samples.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/PfeiferLabTue/wenda">https://github.com/PfeiferLabTue/wenda</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">German Federal Ministry of Education and Research</named-content>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Machine learning has gained wide popularity in recent years and has proved its potential to solve important problems in computational biology on many occasions (<xref rid="btz338-B4" ref-type="bibr">Almagro Armenteros <italic>et al.</italic>, 2017</xref>; <xref rid="btz338-B5" ref-type="bibr">Angermueller <italic>et al.</italic>, 2017</xref>; <xref rid="btz338-B14" ref-type="bibr">Farh <italic>et al.</italic>, 2015</xref>; <xref rid="btz338-B32" ref-type="bibr">Jansen <italic>et al.</italic>, 2003</xref>; <xref rid="btz338-B33" ref-type="bibr">Krogan <italic>et al.</italic>, 2006</xref>). Enabled by the increasing amounts of available data, predictive models have the potential to uncover new relationships, e.g. between genotypes and phenotypes (<xref rid="btz338-B35" ref-type="bibr">Leffler <italic>et al.</italic>, 2017</xref>; <xref rid="btz338-B47" ref-type="bibr">Stranger <italic>et al.</italic>, 2011</xref>), and to improve health care by offering treatment decision support systems to predict critical events (<xref rid="btz338-B27" ref-type="bibr">Hoiles and van der Schaar, 2016</xref>) or a patient’s response to treatment (<xref rid="btz338-B36" ref-type="bibr">Lengauer and Sing, 2006</xref>). </p>
    <p>Traditionally, machine learning assumes that the training data originates from the same distribution as the data on which the learned model is later applied. While this assumption forms the statistical basis of all standard models, it is often violated in real-world settings. If new data does not have exactly the same distribution as the training data, learned relationships may no longer be valid, causing model performance to deteriorate.</p>
    <p>For example, a model may be developed in a highly controlled setting, but when it is later put to use in the real world, the conditions are less ideal. New data might be measured in different institutions with different devices or protocols, or batch effects might lead to differences in the distributions of data acquired at different times (<xref rid="btz338-B2" ref-type="bibr">Akey <italic>et al.</italic>, 2007</xref>; <xref rid="btz338-B34" ref-type="bibr">Leek <italic>et al.</italic>, 2010</xref>). Biological variability can also lead to a distribution mismatch, e.g. when cell composition or other confounders cannot be precisely controlled (<xref rid="btz338-B44" ref-type="bibr">Saito and Sætrom, 2012</xref>). A distribution mismatch may even arise intentionally, if training data for the problem of interest are not directly available and different but related data are used as a replacement, e.g. for knowledge transfer between species.</p>
    <p>Building predictive models that perform well even on data with a certain distribution mismatch with respect to the training data is known as domain adaptation (<xref rid="btz338-B40" ref-type="bibr">Pan and Yang, 2010</xref>; <xref rid="btz338-B41" ref-type="bibr">Patel <italic>et al.</italic>, 2015</xref>). The general setting considers data from two domains with different but related underlying distributions: a source domain, from which a sufficient amount of labeled data is available, and a target domain, from which little or no labeled data are available. The goal is to predict well on the target domain while training (mostly) on source domain data. There are multiple flavors of domain adaptation, differing in how much information from the target domain is known.</p>
    <p>A particularly challenging variant is unsupervised domain adaptation (<xref rid="btz338-B39" ref-type="bibr">Margolis, 2011</xref>), where only unlabeled examples from the target domain are available for training. In this setting, there is no direct way to measure a model’s predictive performance on the target domain during training. It is necessary to make assumptions on the structure of the distribution mismatch, which can vary with the data type or application of interest. Otherwise, the source and target distributions could be arbitrarily far apart, eliminating any chance of successful prediction. For some applications, e.g. in computer vision for object recognition from digital images, unsupervised domain adaptation has been studied extensively with promising results (<xref rid="btz338-B3" ref-type="bibr">Aljundi <italic>et al.</italic>, 2015</xref>; <xref rid="btz338-B20" ref-type="bibr">Gong <italic>et al.</italic>, 2012</xref>, <xref rid="btz338-B21" ref-type="bibr">2013</xref>) and especially domain adaptation methods based on (deep) neural networks have proven successful (<xref rid="btz338-B18" ref-type="bibr">Ganin <italic>et al.</italic>, 2016</xref>; <xref rid="btz338-B38" ref-type="bibr">Long <italic>et al.</italic>, 2016</xref>).</p>
    <p>Despite the recent success of deep learning methods, applications in computational biology often demand other approaches since models are required to be interpretable and data are less abundant. A popular example are regularized regression models like the elastic net (<xref rid="btz338-B56" ref-type="bibr">Zou and Hastie, 2005</xref>), which limit the complexity of a model by penalizing large coefficients. Such models are well suited for prediction problems with a much larger number of possibly correlated features than samples, and are thus frequently used in computational biology (<xref rid="btz338-B19" ref-type="bibr">Garnett <italic>et al.</italic>, 2012</xref>; <xref rid="btz338-B29" ref-type="bibr">Hughey and Butte, 2015</xref>; <xref rid="btz338-B45" ref-type="bibr">Schmidt <italic>et al.</italic>, 2017</xref>). Specifically, the elastic net uses a convex combination of <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub> penalty, combining advantages of LASSO (<xref rid="btz338-B51" ref-type="bibr">Tibshirani, 1996</xref>) and ridge regression (<xref rid="btz338-B26" ref-type="bibr">Hoerl and Kennard, 1970</xref>) regarding sparsity and the handling of correlated features.</p>
    <p>In this article we propose <italic>wenda</italic> (<bold>w</bold>eighted <bold>e</bold>lastic <bold>n</bold>et for unsupervised <bold>d</bold>omain <bold>a</bold>daptation). Our method compares the dependency structure between inputs in source and target domain to measure how similar features behave. It then encourages the use of similarly behaving features using a target domain-specific feature weighting. We build on ideas from <xref rid="btz338-B31" ref-type="bibr">Jalali and Pfeifer (2016)</xref> to measure the similarity of features in source and target domain, but do not use strict feature selection or a predefined set of weak learners. Instead, we learn a full weighted model for each considered target domain. <italic>Wenda</italic> retains all advantages of the standard elastic net regarding interpretability and the effects of regularization, but prioritizes features according to how well they agree in both domains.</p>
    <p>As a concrete application example, we consider the problem of age prediction from DNA methylation data across tissues. DNA methylation is a well-studied epigenetic mark, which has been shown to play a role in important gene regulatory processes like the long-term repression of genes, genomic imprinting and X-chromosome inactivation (<xref rid="btz338-B46" ref-type="bibr">Schübeler, 2015</xref>). In addition, DNA methylation patterns of genomic DNA have been found to be associated with its donor’s chronological age (<xref rid="btz338-B7" ref-type="bibr">Bell <italic>et al.</italic>, 2012</xref>; <xref rid="btz338-B25" ref-type="bibr">Heyn <italic>et al.</italic>, 2012</xref>; <xref rid="btz338-B48" ref-type="bibr">Teschendorff <italic>et al.</italic>, 2013a</xref>). Several studies used DNA methylation data to predict donor age and elastic net models turned out to be particularly useful for this task (<xref rid="btz338-B15" ref-type="bibr">Florath <italic>et al.</italic>, 2014</xref>; <xref rid="btz338-B23" ref-type="bibr">Hannum <italic>et al.</italic>, 2013</xref>; <xref rid="btz338-B28" ref-type="bibr">Horvath, 2013</xref>). While these models were trained on the DNA methylation and chronological age of healthy donors, their predictions are interpreted as a biological epigenetic age. Increased epigenetic aging could be linked to lifestyle factors and disease history, suggesting that the epigenetic age contains useful information on an individual’s health status.</p>
    <p>DNA methylation patterns are known to be highly tissue specific (<xref rid="btz338-B52" ref-type="bibr">Varley <italic>et al.</italic>, 2013</xref>; <xref rid="btz338-B55" ref-type="bibr">Ziller <italic>et al.</italic>, 2013</xref>). While some age-associated changes in DNA methylation are similar across tissues (<xref rid="btz338-B9" ref-type="bibr">Christensen <italic>et al.</italic>, 2009</xref>; <xref rid="btz338-B54" ref-type="bibr">Zhu <italic>et al.</italic>, 2018</xref>), this does not hold for all of them (<xref rid="btz338-B12" ref-type="bibr">Day <italic>et al.</italic>, 2013</xref>; <xref rid="btz338-B16" ref-type="bibr">Fraser <italic>et al.</italic>, 2005</xref>). Predicting age on different tissues than the ones that are available for training can therefore be seen as an unsupervised domain adaptation problem. As more tissue-specific data have recently become available (<xref rid="btz338-B1" ref-type="bibr">Aguet <italic>et al.</italic>, 2017</xref>), predicting age on data from multiple tissues can serve as an example for many future prediction scenarios, making this problem an ideal candidate for evaluating <italic>wenda</italic> on real biological data.</p>
    <p>We consider DNA methylation data from multiple tissues and explicitly unmatched tissue compositions in training and test set. Compared with a non-adaptive standard model, we show that our method strongly improves performance on samples from the cerebellum of the human brain, which were not part of the training data and very poorly predicted by a non-adaptive standard model. In addition, we study the performance of <italic>wenda</italic> in simulation experiments, where it is possible to vary the severity of the distribution mismatch between domains in a controlled setting. We show that our method reduces test error compared with a simple elastic net without domain adaptation also in this scenario, suggesting a wide applicability in computational biology.</p>
  </sec>
  <sec>
    <title>2 The <italic>wenda</italic> method</title>
    <p>We assume to have <italic>n</italic> labeled examples, <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, from the source domain and <italic>m</italic> labeled examples, <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, from the target domain. In both domains, the inputs, <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, are <italic>p</italic>-dimensional vectors with <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow></mml:math></inline-formula>, and the outputs, <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, are scalars. The goal of our method is to use the source domain examples and the target domain inputs to come up with a good prediction of target domain output. The data in source and target domain follow two different joint probability distributions <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, respectively. A classical assumption in domain adaptation, called the covariate shift assumption, is that the difference between these distributions arises only from the inputs, i.e. <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≠</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, while the conditional distributions, <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, are identical. We weaken this assumption by allowing some features to have a different influence on the output in source and target domain. More precisely, we assume that a subset <italic>M</italic> of all <italic>p</italic> features, <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi>M</mml:mi><mml:mo>⊂</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, that shares the same dependency structure in source and target domain will also have the same influence on <italic>Y</italic> in both domains. Features which are not in <italic>M</italic> might influence <italic>Y</italic> differently in source and target domain. More formally, the core assumption is
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>≈</mml:mo><mml:mo>  </mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mtext>for</mml:mtext><mml:mo> </mml:mo><mml:mtext>all</mml:mtext><mml:mo> </mml:mo><mml:mi>f</mml:mi><mml:mo>∈</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>⇒</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>≈</mml:mo><mml:mo>  </mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <italic>X<sub>f</sub></italic> and <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denote feature <italic>f</italic> and all features except <italic>f</italic> in <italic>X</italic>, respectively, and <italic>X<sub>M</sub></italic> is the subvector of <italic>X</italic> containing only features in <italic>M</italic>. We propose a model-based approach to quantify how well <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> agree for different features. Instead of strictly including or excluding features, we enforce stronger regularization on features for which larger differences exist. This allows for a tradeoff between a feature’s suitability for adaptation and its importance for prediction. If <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo><mml:mo>∖</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo><mml:mo>∖</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> differ noticeably, reducing the influence of features outside <italic>M</italic> on the model should improve its robustness and capability to transfer between domains.</p>
    <p><italic>Wenda</italic> consists of the following three main components, which we describe in detail in the following sections: 
<list list-type="order"><list-item><p><italic>Feature models:</italic> We estimate the dependency structure between inputs in the source domain using Bayesian models.</p></list-item><list-item><p><italic>Confidence scores:</italic> We evaluate the estimated input dependency structure on the target domain to quantify the confidence into each feature for domain adaptation.</p></list-item><list-item><p><italic>Final adaptive model:</italic> We train the final model on source domain data while adjusting the strength of regularization for each feature depending on its confidence.</p></list-item></list>For simplicity, we explain this method considering only one target domain even though it can easily be applied to multiple target domains as we do in Sections 3 and 4.</p>
    <sec>
      <title>2.1 Feature models</title>
      <p>We capture the dependency structure between inputs in the source domain using Bayesian models. For each feature <italic>f</italic>, we train a model <italic>g<sub>f</sub></italic> which predicts <italic>f</italic> based on all other features using the source domain inputs, <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, as training data. These feature models estimate all conditional distributions <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Since we consider high-dimensional feature spaces, we use Gaussian process models (<xref rid="btz338-B43" ref-type="bibr">Rasmussen and Williams, 2006</xref>) with a simple linear kernel and additive noise. This model has two hyper parameters, the variance of the prior on the coefficients <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, and the variance of the noise <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, which we determine by maximum marginal likelihood for each feature. More precisely, we write <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> for the vector containing feature <italic>f</italic>, and <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for the <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-matrix containing all remaining features of the training samples, and maximize
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"/><mml:mtd columnalign="left"/><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>π</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
Here <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is the linear kernel matrix, <italic>I<sub>n</sub></italic> is the <italic>n</italic>-dimensional identity matrix and <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:mo>|</mml:mo><mml:mo>.</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> denotes the determinant. Given <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, the posterior distribution of the coefficients, <italic>ω</italic>, of the linear model is Gaussian and has the closed-form solution
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>ω</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>;</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The advantage of using Bayesian models in this step is that they offer not only a single prediction, but a posterior distribution including uncertainty information.</p>
    </sec>
    <sec>
      <title>2.2 Confidence scores</title>
      <p>This uncertainty information can be used to define a score that quantifies how closely each feature in the target domain follows the source-domain dependency structure. Consider a given test input, <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and feature, <italic>f</italic>. We denote the value of <italic>f</italic> in <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the values of all features except <italic>f</italic> in <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Given <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the feature model <italic>g<sub>f</sub></italic> outputs a posterior distribution, describing which values of <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> would be expected according to the source-domain dependency structure. For Gaussian processes this is a normal distribution, <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We quantify how well the observed value, <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, fits to this predicted distribution using the confidence proposed by <xref rid="btz338-B31" ref-type="bibr">Jalali and Pfeifer (2016)</xref>,
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mo>Φ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE39"><mml:math id="IM39"><mml:mo>Φ</mml:mo></mml:math></inline-formula> denotes the cumulative distribution function of a standard normal distribution. This confidence is the probability that a value as far from <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:msub><mml:mrow><mml:mo>μ</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mo>¬</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> or further occurs in the posterior distribution predicted by <italic>g<sub>f</sub></italic>. We define the confidence of feature <italic>f</italic> for prediction on the target domain as the average of <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> over all target inputs,
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
For each feature, <italic>c<sub>f</sub></italic> describes how well the source-domain dependencies of feature <italic>f</italic> fit in the target domain and, according to the core assumption stated in <xref ref-type="disp-formula" rid="E1">Equation (1)</xref>, how suitable <italic>f</italic> is for the considered domain adaptation task.</p>
    </sec>
    <sec>
      <title>2.3 Final adaptive model</title>
      <p>To predict the output, <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, in the target domain, we train a final model on the source domain data using the confidences defined in <xref ref-type="disp-formula" rid="E5">Equation (5)</xref> to prioritize features. Here we use a weighted version of the elastic net, which scales the contributions of features to the regularization term according to predefined feature weights. The weighted elastic net solves the problem
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>arg</mml:mtext><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>β</mml:mo></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">RSS</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mi mathvariant="italic">J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>α</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>α</mml:mo><mml:mo stretchy="true">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:msubsup><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:mi mathvariant="italic">RSS</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the residual sum of squares on the training data, <italic>w<sub>f</sub></italic> are the feature weights, <inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:mo>λ</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is the regularization parameter and <inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:mo>α</mml:mo><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> determines the proportion of <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub> penalty. If <italic>w<sub>f</sub></italic> =1 for all features, <xref ref-type="disp-formula" rid="E7">Equation (7)</xref> reduces to the standard elastic net penalty. We choose these feature weights based on the confidences defined in <xref ref-type="disp-formula" rid="E5">Equation (5)</xref> to encourage the use of features which were estimated to be useful for domain adaptation. More precisely, we set
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>k </italic>&gt;<italic> </italic>0 is a user-specified model parameter. This means that coefficients of features with a low confidence are penalized more severely than coefficients of high-confidence features. The parameter <italic>k</italic> controls how exactly confidences are translated into weights. For <italic>k </italic>=<italic> </italic>1, the feature weight increases linearly with decreasing confidence, for higher values of <italic>k</italic> the model puts an increasingly high penalty on very low confidences while penalizing medium to high confidences less severely. The resulting model still attempts to predict well on the training data by achieving a small <inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:mi mathvariant="italic">RSS</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>β</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, but is encouraged to prefer features with high confidence. It takes into account both a feature’s importance for predicting the output according to the source domain data and its confidence, i.e. its estimated suitability for domain adaptation.</p>
    </sec>
    <sec>
      <title>2.4 The challenge of parameter selection</title>
      <p><italic>Wenda</italic> has three external parameters: the weighting parameter <italic>k</italic>, the proportion of <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub> penalty <italic>α</italic> and the regularization parameter <italic>λ</italic>. Parameters <italic>α</italic> and <italic>λ</italic> are inherited from the standard elastic net and usually optimized via cross-validation on the training data. Alternatively, <italic>α</italic> is sometimes treated as a design choice (<xref rid="btz338-B28" ref-type="bibr">Horvath, 2013</xref>; <xref rid="btz338-B29" ref-type="bibr">Hughey and Butte, 2015</xref>), as its effect, i.e. the interpolation between ridge regression and LASSO, is fairly straightforward to interpret.</p>
      <p>Cross-validation approximates the error on unseen samples drawn from the same distribution as the training data. The goal of unsupervised domain adaptation, however, is to achieve low error on samples from the target domain, which follow a different distribution. The absence of labeled output examples from the target domain for training is an obstacle for model selection. While parameters can be optimized with respect to the source-domain distribution, it is uncertain whether they generalize to the target domain. Furthermore, simultaneously optimizing multiple parameters constitutes a non-negligible computational burden.</p>
      <p>Considering these aspects, we treat <italic>α</italic> as a design choice and keep it fixed at <inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>. Parameter <italic>λ</italic> determines the strength of regularization and can thus not be globally set to one value that performs well across different datasets. Since data-dependent tuning of <italic>λ</italic> is inevitable, we evaluate and compare two approaches, which are described in Sections 2.5 and 2.6. The parameter <italic>k</italic> is introduced by our method, so we evaluate its sensitivity in the empirical studies (Sections 3 and 4).</p>
    </sec>
    <sec>
      <title>2.5 <italic>Wenda-pn</italic>: prior knowledge on size of mismatch</title>
      <p>In <italic>wenda</italic>, <italic>λ</italic> does not only affect the strength of regularization but also how strongly the feature weights are taken into account. For very small <italic>λ</italic>, e.g. all features are weakly penalized and differences among feature weights have only a minor influence. For large <italic>λ</italic>, redistributing coefficients between features with different weights can strongly change the value of the objective function, giving feature weights a large influence on the final result. Hence, for any target domain <italic>T</italic>, the optimal value, <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, depends on how much adaptation is needed for transfer between the source and target domain.</p>
      <p>If the size or severity of the distribution mismatch between domains has a major influence on which <italic>λ</italic> is optimal, prior knowledge on the similarity between the domains could help to choose <italic>λ</italic>. Note that prior knowledge here refers to information known from other sources, but not to a prior distribution in the Bayesian sense. This approach requires:
<list list-type="order"><list-item><p>A quantitative measure of similarity or dissimilarity between source domain and target domain(s).</p></list-item><list-item><p>A mapping from domain (dis)similarity to a good choice of <italic>λ</italic>.</p></list-item></list>If and how prior knowledge on domain similarity is available depends on the application and will be described in Sections 3.3 and 4.2 for the datasets used in this work.</p>
      <p>The mapping usually has to be estimated from data, which is possible if multiple target domains, <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>ℓ</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, are considered and labeled examples are available for some of them. We model <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as a linear function of domain similarity since <italic>λ</italic> is non-negative and typically chosen from a grid of equidistant points on a logarithmic scale (<xref rid="btz338-B17" ref-type="bibr">Friedman <italic>et al.</italic>, 2010</xref>).</p>
      <p>We call the version of <italic>wenda</italic> using prior knowledge <italic>wenda-pn</italic> and evaluate it using the following cross-validation scheme. We first partition the indexes <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> of all available target domains into two subsets, <italic>I</italic><sub>1</sub> and <italic>I</italic><sub>2</sub>. For all <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> we determine <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> by varying <italic>λ</italic> on a grid and choosing the value which leads to the lowest mean absolute error (MAE) on the target domain <italic>T<sub>i</sub></italic>, disclosing the corresponding labels. Next, we fit the model for the relationship between domain similarity and <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> via least squares, using <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and the corresponding domain similarities as training data. With this model we predict <inline-formula id="IE57"><mml:math id="IM57"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> for all <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and measure the resulting performance of <italic>wenda-pn</italic>. This process is repeated for multiple splits of the target domains into subsets <italic>I</italic><sub>1</sub> and <italic>I</italic><sub>2</sub>. The exact number and ratio of splits is problem dependent and will be described in Sections 3.3 and 4.2.</p>
    </sec>
    <sec>
      <title>2.6 <italic>Wenda-cv</italic>: cross-validation on training data</title>
      <p>If no knowledge on domain similarity is available, an alternative option is to still use cross-validation on the training data to determine <italic>λ</italic>. Cross-validation will choose a regularization strength which is optimal on the source domain for the given feature weights, rather than the target domain. Including the feature weighting can still lead to an improvement compared with a standard elastic net, but choosing <italic>λ</italic> with cross-validation on source domain data may not fully exploit its potential. We call this version of our method <italic>wenda-cv</italic>.</p>
    </sec>
    <sec>
      <title>2.7 Implementation</title>
      <p>We implemented all models in python 3.5.4., the source code is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/PfeiferLabTue/wenda">https://github.com/PfeiferLabTue/wenda</ext-link>). For computing the regularization paths of (weighted or unweighted) elastic net models, we used python-glmnet (<xref rid="btz338-B10" ref-type="bibr">Civis Analytics, 2016</xref>), a python wrapper around the original Fortran code which is also the basis of the R package glmnet (<xref rid="btz338-B17" ref-type="bibr">Friedman <italic>et al.</italic>, 2010</xref>). For optimizing the Gaussian process models needed for the feature models described in Section 2.1, we used the python package GPy (<xref rid="btz338-B22" ref-type="bibr">GPy, 2012</xref>). </p>
    </sec>
  </sec>
  <sec>
    <title>3 Experiments on simulated data</title>
    <p>To evaluate how <italic>wenda</italic> performs on datasets with varying degrees of domain mismatch in a controlled setting, we simulate multiple datasets with dependent inputs and a defined distribution mismatch between source and target domain. In each simulated dataset we use 1000 inputs, 3000 training samples from the source domain and 1000 test samples from the target domain. To account for variability, we run 10 fully independent simulations.</p>
    <sec>
      <title>3.1 Source domain model</title>
      <p>We model the complex dependency structure between inputs using Bayesian networks (<xref rid="btz338-B42" ref-type="bibr">Pearl, 1988</xref>) with Gaussian marginal distributions. For each simulation, we first randomly generate 20 directed acyclic graphs (DAGs) with 50 nodes each and a maximum degree of 5 (indegree + outdegree) using BNGenerator (<xref rid="btz338-B30" ref-type="bibr">Ide and Cozman, 2002</xref>). These graphs model 20 groups of input variables with dependencies within but not between groups. BNGenerator uses a Markov chain Monte Carlo approach to sample uniformly from all possible DAGs which satisfy the specified constraints. It additionally outputs categorical distributions and conditional distributions for the nodes, which we ignore for this application. Instead of categorical distributions, we assign independent standard normal distributions to all root nodes and define the distributions of all child nodes as linear combinations of their parent nodes plus a fixed amount of Gaussian noise. To control the variance of child nodes, we move through each graph according to its topological ordering, draw random weights for parent edges from a standard normal distribution, and scale them to achieve a total variance of 1 (including noise). We set the noise variance for input dependencies to <inline-formula id="IE59"><mml:math id="IM59"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mo>ε</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, i.e. 10% of the marginal variance of each node.</p>
      <p>For the output, we use a sparse linear model with Gaussian noise. We randomly choose 20 out of 1000 coefficients to be nonzero, one in each of the 20 graphs. As for the relationships between inputs, we set the noise variance to <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, draw the nonzero coefficients from a standard normal distribution and scale them to achieve variance 1.</p>
    </sec>
    <sec>
      <title>3.2 Target domain model</title>
      <p>To model target domain data with a distribution mismatch, we start from the source domain model, but make changes to some of the variables and their influence on the output. The Bayesian networks allow us to directly change dependencies between inputs in the model, instead of just distorting simulated data. Depending on the degree of domain mismatch we wish to introduce, we randomly pick a certain number of the 20 graphs representing the inputs and multiply the weights of all their edges with −1, thus inverting the dependencies they have in the source domain. This is an attractive choice because it specifically changes the dependencies of inputs while not strongly distorting their marginal distributions. In addition, we change the influence of these altered variables on the output by setting the corresponding coefficients in the output model to zero. In each simulation, we consider four different target domains with varying size of distribution mismatch: no mismatch, 10%, 20% and 30% altered variables. When training the weighted models, we average confidences only over groups of 100 samples at a time, to account for the variability in feature weights caused by smaller target domain sample sizes.</p>
    </sec>
    <sec>
      <title>3.3 Prior knowledge on domain mismatch</title>
      <p>Incorporating knowledge on the size of the domain mismatch is simple for simulated data since the ground truth of how many variables were altered is known. We define domain similarity as the fraction of unchanged variables and use leave-one-out cross-validation on the four sizes of distribution mismatch to evaluate the performance of <italic>wenda-pn</italic> (Section 2.5). When predicting with <italic>wenda-pn</italic> for the target domains with a certain size of distribution mismatch, we use the remaining target domains (from all simulations) to learn the relationship between domain similarity and <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>3.4 Baseline models</title>
      <p>We compare the results of <italic>wenda-pn</italic> and <italic>wenda-cv</italic> on the simulated datasets to two baseline models. The first is a simple elastic net without feature weights (<italic>en</italic>), which is the natural baseline for our adaptive model. Here we choose <inline-formula id="IE62"><mml:math id="IM62"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> in agreement with <italic>wenda</italic>, and determine <italic>λ</italic> via 10-fold cross-validation on the training data.</p>
      <p>The second baseline is a weighted elastic net with a simpler feature weighting, for which we use the abbreviation <italic>wenda-mar</italic>. This model has the same structure as proposed in Section 2, but feature weights are computed based on the marginal distributions of features instead of the dependency structure between them, eliminating the need to train feature models as described in Section 2.1. It still detects differences between the distributions of inputs in source and target domain, but does not utilize dependencies between features to do so. More precisely, the confidence defined in <xref ref-type="disp-formula" rid="E4">Equation (4)</xref> is replaced by the simplified version
<disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:mi>min</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE63"><mml:math id="IM63"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the empirical cumulative distribution function of feature <italic>f</italic> in the training data. As in <italic>wenda-pn</italic> and <italic>wenda-cv</italic>, we average these confidences over all target-domain inputs and translate them to feature weights in analogy to <xref ref-type="disp-formula" rid="E5">Equations (5)</xref> and <xref ref-type="disp-formula" rid="E8">(8)</xref>. Consistently with <italic>wenda-pn</italic> and <italic>wenda-cv</italic>, we keep <inline-formula id="IE64"><mml:math id="IM64"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> fixed and report results for multiple values of <italic>k</italic>. To determine the regularization parameter <italic>λ</italic>, we use 10-fold cross-validation on the training data.</p>
      <p>The score <inline-formula id="IE65"><mml:math id="IM65"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>f</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is chosen to be very similar to <xref ref-type="disp-formula" rid="E4">Equation (4)</xref>. A comparison of <italic>wenda-mar</italic> to an alternative score based on KL divergence can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figures S1 and S2</xref>.</p>
    </sec>
    <sec>
      <title>3.5 Results on simulated data</title>
      <p><xref ref-type="fig" rid="btz338-F1">Figure 1</xref> summarizes the MAE of <italic>wenda-pn</italic>, <italic>wenda-cv</italic> and <italic>wenda-mar</italic> on the simulated test data. We report all errors relative to the MAE of the standard (unweighted) elastic net (<italic>en</italic>), the error bars indicate mean and standard deviation over 10 simulations. A similar plot of the correlation between true and predicted output is shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S3</xref>. 
</p>
      <fig id="btz338-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>Mean absolute error (MAE) of <italic>wenda-pn</italic>, <italic>wenda-cv</italic> and <italic>wenda-mar</italic> on simulated test data. Each row shows results on one target domain (no mismatch, 10–30% altered variables). We report all errors relative to the MAE of <italic>en</italic> showing the mean±standard deviation over 10 simulations</p>
        </caption>
        <graphic xlink:href="btz338f1"/>
      </fig>
      <p>With <italic>wenda-pn</italic> we obtain considerable improvements for the intermediate target domains with 10% and 20% altered variables, reducing the MAE of <italic>en</italic> by up to 18.7% and 26.2%, respectively. For the more extreme target domains the results are mixed. With 30% altered variables we still observe an improvement for some values of <italic>k</italic>, but the variability is very high (both within one choice and between choices of <italic>k</italic>). For the target domain without mismatch, the MAE even increases compared with <italic>en</italic> for high values of <italic>k</italic>. This can be explained by the cross-validation scheme we employ to learn the relationship between <inline-formula id="IE66"><mml:math id="IM66"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and domain similarity (Section 3.3). For each size of distribution mismatch, the model describing this relationship has been trained on the remaining target domains. This is an interpolation for the intermediate target domains (10% and 20% altered variables), but an extrapolation for the target domains with 30% altered variables and no mismatch. Extrapolation is a harder problem and can lead to a less accurate estimate of <inline-formula id="IE67"><mml:math id="IM67"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and increased variability.</p>
      <p>It should be noted that using domain adaptation even though prior knowledge suggests that there is no distribution mismatch between domains is not a realistic scenario. We include the results of <italic>wenda-pn</italic> on data without distribution mismatch for the sake of completeness.</p>
      <p>The other two weighted models, <italic>wenda-cv</italic> and <italic>wenda-mar</italic> show no or only very little improvement over <italic>en</italic>. On target domains with mismatch, <italic>wenda-cv</italic> consistently receives a slightly lower MAE than <italic>en</italic>, but the improvement is only 7.6% at best. It uses the same feature weights as <italic>wenda-pn</italic>, but obviously chooses a less suitable value for <italic>λ</italic>. The simpler confidences used by <italic>wenda-mar</italic> can only pick up changes in the marginal distributions of features, not in their dependency structure, leading to almost the same results as <italic>en</italic>. Only for 30% altered variables a slight improvement can be noted. Since marginal distributions are only altered very subtly in the target domain model, we expected a weak performance of <italic>wenda-mar</italic> in this simulation study.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Age prediction from DNA methylation data</title>
    <p>Now we consider our primary application on real data, i.e. the problem of age prediction from DNA methylation data across multiple tissues.</p>
    <sec>
      <title>4.1 DNA methylation dataset and preprocessing</title>
      <p>We use DNA methylation data and donor age from two sources, the Cancer Genome Atlas (TCGA; <xref rid="btz338-B8" ref-type="bibr">Chang <italic>et al.</italic>, 2013</xref>) and the Gene Expression Omnibus (GEO; <xref rid="btz338-B13" ref-type="bibr">Edgar <italic>et al.</italic>, 2002</xref>). We include only DNA methylation data which were measured with the Illumina Infinium HumanMethylation450 BeadChip and only samples from healthy tissue. Using RnBeads (<xref rid="btz338-B6" ref-type="bibr">Assenov <italic>et al.</italic>, 2014</xref>), we perform several preprocessing steps on the DNA methylation data. In particular, we remove SNPs and gonosomal CpGs, and normalize the data with the BMIQ method (<xref rid="btz338-B49" ref-type="bibr">Teschendorff <italic>et al.</italic>, 2013b</xref>). In addition, we impute missing values (&lt;0.5% of all measurements) using 10-nearest-neighbor imputation in the R package impute (<xref rid="btz338-B24" ref-type="bibr">Hastie <italic>et al.</italic>, 2017</xref>). Finally, we split the dataset into a training and test set with 1866 and 1001 samples, respectively.</p>
      <p>The final training set contains data from 19 different tissues, with a focus on blood, and from donors with a chronological age ranging from 0 to 103 years. The test set consists of data from 13 different tissues initially, including blood as well as tissues which are not present in the training data, e.g. samples from the cerebellum of the human brain. We slightly aggregate them, combining ‘blood’, ‘whole blood’ and ‘menstrual blood’, as well as ‘Brain MedialFrontalCortex’ and ‘Brain FrontalCortex’ to increase sample sizes per tissue. The range of ages represented in the test set is 0–70 years. When applying <italic>wenda</italic>, we keep the training set fixed and consider each tissue in the test set as a separate target domain.</p>
      <p>To limit the computational burden of training feature models, we reduce the initial number of 466 094 features to 12 980 using a standard elastic net model with <inline-formula id="IE68"><mml:math id="IM68"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> and fixed regularization parameter, <italic>λ</italic>=1.1 × 10<sup>−4</sup>. Furthermore, we use the following transformation for the chronological ages, which was proposed by <xref rid="btz338-B28" ref-type="bibr">Horvath (2013)</xref>. We transform all training ages with the function
<disp-formula id="E10"><mml:math id="M10"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext>adult</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mo> </mml:mo><mml:mi>y</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext>adult</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext>adult</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext>adult</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
with adult age <inline-formula id="IE69"><mml:math id="IM69"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext>adult</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></inline-formula> prior to training, and later re-transform the model’s predictions with the inverse function, <inline-formula id="IE70"><mml:math id="IM70"><mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. This transformation is logarithmic for ages below and linear for ages above <inline-formula id="IE71"><mml:math id="IM71"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mtext>adult</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which is motivated by the fact that the methylation landscape changes more quickly and dramatically in childhood and adolescence than later in life. Subsequently, we standardize all data to zero mean and unit variance.</p>
    </sec>
    <sec>
      <title>4.2 Prior knowledge on domain mismatch</title>
      <p>As prior knowledge for <italic>wenda-pn</italic> (Section 2.5), we make use of published data on similarities between human tissues. The GTEx consortium published an analysis of a large dataset of (among others) genotype and gene expression data across 42 human tissues (<xref rid="btz338-B1" ref-type="bibr">Aguet <italic>et al.</italic>, 2017</xref>). In this article, <xref rid="btz338-B1" ref-type="bibr">Aguet <italic>et al.</italic> (2017)</xref> identified tissue-specific expression quantitative trait loci (eQTLs), i.e. locations in the genome where genetic variants have a significant effect on gene expression levels. Furthermore, the authors estimated tissue-specific effect sizes for each eQTL using a linear mixed model, and reported the correlation (Spearman’s <italic>ρ</italic>) of effect sizes between all pairs of tissues (see <xref ref-type="fig" rid="btz338-F2">Figure 2a</xref> in <xref rid="btz338-B1" ref-type="bibr">Aguet <italic>et al.</italic>, 2017</xref>), providing a comprehensive measure of tissue similarity. Here we focus on the correlations reported for cis-eQTLs, where the location of the genetic variation is within 1 Mb of the target gene’s transcription start site, since these were identified in larger numbers and with a lower false discovery rate than trans-eQTLs.
</p>
      <fig id="btz338-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>(<bold>a</bold>) Mean absolute error of <italic>en-ls</italic> and <italic>wenda-pn</italic> with <italic>k </italic>=<italic> </italic>3 per test tissue. We show the mean ± standard deviation over 10 runs of 10-fold cross-validation for <italic>en-ls</italic>, and over all splits of the test tissues where the tissue of interest was in the evaluation set for <italic>wenda-pn</italic>. Predicted versus true chronological age for typical runs of <italic>en-ls</italic> (<bold>b</bold>) and <italic>wenda-pn</italic> with <italic>k </italic>=<italic> </italic>3 (<bold>c</bold>). In each plot, we show samples colored by tissue. As a typical run for <italic>en-ls</italic> we show the one with closest to median performance on cerebellum samples and full test set. For <italic>wenda-pn</italic>, we choose a typical run for each tissue: among all models with this tissue in the holdout set, we plot predictions of the one with closest to median performance</p>
        </caption>
        <graphic xlink:href="btz338f2"/>
      </fig>
      <p>We map each tissue in our data to the corresponding tissue(s) contained in the GTEx study, allowing multiple matches if the GTEx classification is more detailed than the one available for our data <xref ref-type="supplementary-material" rid="sup1">(Supplementary Table S1)</xref>. Next, we compute similarities between tissues in our data by looking up (and potentially averaging) the similarities between matched GTEx tissues. Finally, we define the similarity between each target domain and the source domain as the average over all pairwise similarities between samples from the two sets. Our data contains several samples from tissues for which no close match is available in the GTEx data (240 samples in the training set, 56 in the test set). For these we impute the similarity to other tissues with the mean of all pairwise tissue similarities.</p>
      <p>When evaluating the performance of <italic>wenda-pn</italic>, we repeatedly split the test tissues into one part for fitting the relationship between domain similarity and <inline-formula id="IE72"><mml:math id="IM72"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and one part for evaluation (Section 2.5). Here, we iterate over all combinations of 3 tissues with at least 20 samples each for training and evaluate the performance on the remaining tissues.</p>
    </sec>
    <sec>
      <title>4.3 Baseline models</title>
      <p>We compare <italic>wenda-pn</italic> and <italic>wenda-cv</italic> to the two baseline models described in Section 3.4 with the following minor modification: instead of using a simple elastic net directly, we use <italic>en</italic> followed by a linear least-squares fit based only on features which received nonzero coefficients in <italic>en</italic>. We refer to this baseline as <italic>en-ls</italic>. This model type was suggested by <xref rid="btz338-B28" ref-type="bibr">Horvath (2013)</xref> for age prediction from DNA methylation data, who reported that the subsequent least-squares fit reduced test errors on his dataset. We observe a similar effect on our data, where <italic>en-ls</italic> produces lower test errors than <italic>en</italic> on cerebellum samples while making almost no difference on the remaining samples.</p>
    </sec>
    <sec>
      <title>4.4 Results on DNA methylation data</title>
      <p>We compare the results of <italic>wenda-pn</italic>, <italic>wenda-cv</italic> and the two baseline models on the dataset described in Section 4.1 and measure performance by MAE on the test set (<xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S4</xref> for correlation instead of MAE). Due to the heterogeneous nature of the data, the random split of the training data used for 10-fold cross-validation has a large influence on the results, especially for <italic>en-ls</italic>. Hence, we report the mean and standard deviation over 10 runs. For <italic>wenda-pn</italic>, we do not perform cross-validation on the training data but iterate over multiple splits of the test tissues to learn the relationship between domain similarity and <inline-formula id="IE73"><mml:math id="IM73"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Here, we measure MAE only on samples which were not used for the similarity-lambda fit, and report mean and standard deviation over all splits.</p>
      <p>When training the weighted models, we regard each tissue in the test dataset as a separate target domain. To be precise, we average the confidences defined in <xref ref-type="disp-formula" rid="E5">Equation (5)</xref> only over samples of the same tissue and train a separate model for each tissue, using always the same training data but tissue-specific feature weights.</p>
      <p>With <italic>en-ls</italic> we obtain an MAE of 6.19 ± 0.90 years on the full test set. <xref ref-type="fig" rid="btz338-F2">Figure 2a</xref> illustrates the MAE of <italic>en-ls</italic> and a representative example of a weighted model (<italic>wenda-pn</italic>, <italic>k </italic>=<italic> </italic>3) on each test tissue. It shows that <italic>en-ls</italic> yields a considerably higher MAE on cerebellum samples than on other tissues. <xref ref-type="fig" rid="btz338-F2">Figure 2b</xref> shows the predicted versus true ages for the test set in a typical cross-validation run, colored by tissue, and reveals that the predicted age is consistently far below the true chronological age. Both plots demonstrate that <italic>en-ls</italic> predicts age well on all test tissues except cerebellum. In fact, on cerebellum samples <italic>en-ls</italic> produces an MAE of 18.75 ± 7.18 years.</p>
      <p>Cerebellum samples are especially hard to predict for two reasons: they are not represented in the training data and they are known to be biologically very different even from other brain tissues regarding function and gene expression patterns (<xref rid="btz338-B1" ref-type="bibr">Aguet <italic>et al.</italic>, 2017</xref>; <xref rid="btz338-B16" ref-type="bibr">Fraser <italic>et al.</italic>, 2005</xref>). Therefore, the focus of our evaluation is whether domain adaptation as implemented by <italic>wenda</italic> can improve performance on these samples.</p>
      <p>The predictions of <italic>wenda-pn</italic> with <italic>k </italic>=<italic> </italic>3 versus the true ages are shown in <xref ref-type="fig" rid="btz338-F2">Figure 2c</xref>. Here, we plot the predictions of a typical run for each tissue by choosing the model with closest to median performance among all models with this tissue in the holdout set. The ages predicted by <italic>wenda-pn</italic> for cerebellum samples are far closer to the corresponding true ages than they were for <italic>en-ls</italic> (<xref ref-type="fig" rid="btz338-F2">Fig. 2b</xref>), and predictions of <italic>wenda-pn</italic> on the remaining test tissues are of a similar quality as those of <italic>en-ls</italic>. This observation is confirmed by the quantitative comparison in <xref ref-type="fig" rid="btz338-F2">Figure 2a</xref>, where <italic>wenda-pn</italic> has far lower errors than <italic>en-ls</italic> on cerebellum samples, and similar or better performance than <italic>en-ls</italic> on the remaining test tissues.</p>
      <p>While <italic>en-ls</italic> predicts age far worse on cerebellum samples than on other tissues, <italic>wenda-pn</italic> shows no major difference in prediction quality between cerebellum samples and the remaining test tissues. Consequently, <italic>wenda-pn</italic> demonstrates to be considerably more robust to the distribution mismatch between cerebellum samples and the training data than <italic>en-ls</italic>.</p>
      <p><xref ref-type="fig" rid="btz338-F3">Figure 3</xref> shows the MAE of all models on cerebellum samples. Here, all weighted models strongly improve upon <italic>en-ls</italic>. The lowest errors on cerebellum samples are achieved by <italic>wenda-cv</italic>, reaching as low as 6.07 ± 0.10 years for <italic>k </italic>=<italic> </italic>4. This is closely followed by <italic>wenda-pn</italic>, which achieves an MAE between 7.60 and 8.70 years on average on cerebellum samples for <inline-formula id="IE74"><mml:math id="IM74"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>. Even <italic>wenda-mar</italic>, which uses only marginal distributions to weight features, improves upon <italic>en-ls</italic> with an MAE of 9.42 ± 0.69 years at best. All weighted models achieve their best result for <italic>k</italic> between 2 and 4 with not too much variation in this range. However, even when <italic>k</italic> is far from optimal for cerebellum samples, they still perform better than <italic>en-ls</italic>.
</p>
      <fig id="btz338-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Mean absolute error of all models on cerebellum samples. We show the mean and standard deviation over 10 runs of 10-fold cross-validation or, in case of <italic>wenda-pn</italic>, over all splits where cerebellum samples were in the evaluation set</p>
        </caption>
        <graphic xlink:href="btz338f3"/>
      </fig>
      <p>A comparison of the MAE of all models on the full test set is shown in <xref ref-type="fig" rid="btz338-F4">Figure 4</xref> and indicates an overall similar performance of <italic>wenda</italic> and the two baselines. For <inline-formula id="IE75"><mml:math id="IM75"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>, <italic>wenda-cv</italic> and <italic>wenda-mar</italic> yield a slightly lower MAE than <italic>en-ls</italic>, and for large <italic>k</italic>, <italic>wenda-cv</italic> and <italic>wenda-pn</italic> yield a slightly higher MAE than <italic>en-ls</italic>. Given that <italic>en-ls</italic> already shows acceptable performance on all tissues except cerebellum, we did not expect a big improvement here. The results show, however, that the improvement on cerebellum samples is not bought by a loss of performance on other tissues.
</p>
      <fig id="btz338-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Mean absolute error (MAE) of all models on the full test set of DNA methylation data. We show the mean and standard deviation over 10 runs of 10-fold cross-validation. In case of <italic>wenda-pn</italic>, we compute the MAE only based on samples in the evaluation set, and plot the mean and standard deviation over all considered splits of the test tissues</p>
        </caption>
        <graphic xlink:href="btz338f4"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion</title>
    <p>Predictive models are widely used in computational biology, but differences between the distribution of their training data and new data to which they are later applied can severely threaten their performance. In this article we propose <italic>wenda</italic>, a method for unsupervised domain adaptation based on the elastic net. It detects differences in the dependency structure between inputs in source and target domain and enforces stronger regularization on features that behave differently. Our method is different from previous studies on the combination of the elastic net and domain adaptation techniques (<xref rid="btz338-B37" ref-type="bibr">Li <italic>et al.</italic>, 2015</xref>; <xref rid="btz338-B53" ref-type="bibr">Wachinger and Reuter, 2016</xref>). Both consider only the easier problem of supervised domain adaptation, i.e. the situation where some labeled examples from the target domain are available for training, and are not applicable in the setting we consider. Our method is also different from the approach proposed by <xref rid="btz338-B11" ref-type="bibr">Cortes and Mohri (2011)</xref>, which uses a sample weighting rather than a feature weighting and is thus better suited for situations with <italic>n </italic>&gt;<italic> p</italic> than for the ones we consider.</p>
    <p>The key idea of our approach, which separates it from many other domain adaptation methods, is to learn the dependency structure between inputs for calculating feature weights. This property is of particular relevance to applications within computational biology where, in contrast to, e.g. image analysis, the dependency structure is irregular and not known a priori. For example, even distant locations in the (epi)genome can interact and form complex gene regulatory networks, which vary with cell type and differentiation state (<xref rid="btz338-B50" ref-type="bibr">Thompson <italic>et al.</italic>, 2015</xref>). While we used Gaussian process models with linear kernels as feature models, any other Bayesian model type would be applicable in principle, subject only to the data and computational resources.</p>
    <p>Like any domain adaptation method, <italic>wenda</italic> makes the assumption that source and target distribution are not too far apart, so that some features are useful for predicting the output and behave similarly in source and target domain. Another central assumption of our method is that the dependency structure between inputs is informative of which features are useful for domain adaptation. There are certain extreme cases, where this is clearly violated. For example, when features are entirely independent, the distribution predicted by each feature model <italic>g<sub>f</sub></italic> would be approximately the feature’s marginal distribution, and <italic>wenda-pn</italic> and <italic>wenda-cv</italic> would behave similarly to <italic>wenda-mar</italic>. Another such case is the presence of duplicates or extremely strong correlations between variables. These could arise, e.g. in sequencing-based methylation assays, where the DNA methylation of consecutive CpG sites is highly correlated in all tissues. Thus, each feature would always be well predicted by its neighbor, regardless of changes on a larger scale. In situations like this, we suggest to aggregate extremely correlated features before training, which is also advisable for a standard elastic net.</p>
    <p>Our method is computationally demanding since it requires to train one Bayesian model per feature (for confidence estimation) and one weighted elastic net per target domain (for prediction). While both of these steps can be parallelized to speed up calculations, fitting the feature models remains challenging for large datasets. For example, training 12 980 feature models for the DNA methylation data on 10 CPUs of the type Intel Xeon CPU E7-4850 with 2.30 GHz takes about 51 h.</p>
    <p>However, the structure of <italic>wenda</italic> allows additional speed-ups, as feature models have to be trained only once (as long as the training data remain fixed) and can be reused to predict on multiple target domains or with different parameter settings. If the confidence scores for a given test dataset are precomputed as well, the final model for one target domain is only a weighted elastic net trained on the training data, whose regularization path can be computed quickly, e.g. with glmnet. With the same computational setup as before and with precomputed feature models and confidence scores, training all models required for <italic>wenda-pn</italic> with <italic>k </italic>=<italic> </italic>3 (<xref ref-type="fig" rid="btz338-F2">Fig. 2c</xref>) takes about 43 s.</p>
    <p><italic>Wenda</italic> allows to incorporate prior knowledge on the size of the domain mismatch (<italic>wenda-pn</italic>), but a simplified version can also be applied without it (<italic>wenda-cv</italic>). <italic>Wenda-cv</italic> uses cross-validation on the training data to determine <italic>λ</italic>, which is not ideal in a domain adaptation setting. Nevertheless, our results on the DNA methylation data demonstrate that it can still lead to a surprisingly large improvement over a non-adaptive model. This makes it a valuable alternative to <italic>wenda-pn</italic>, especially if no prior knowledge on the size of domain mismatch is available.</p>
    <p><italic>Wenda</italic> introduces a new parameter <italic>k</italic>, which controls how confidences are translated into feature weights. We empirically studied the impact of choosing <italic>k</italic> on the MAE and observed satisfying performance in the interval <inline-formula id="IE76"><mml:math id="IM76"><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. Hence, <italic>k </italic>=<italic> </italic>3 might constitute a relatively robust choice for future applications, albeit it is unlikely that any single parameter choice is optimal for each and every target domain. We note that <italic>wenda</italic> never performs substantially worse than the non-adaptive reference. Hence, the precise value of <italic>k</italic> determines only the magnitude of improvement obtained and a suboptimal choice poses relatively little risk. Nevertheless, without labeled training examples from the target domain, parameter selection remains a non-trivial problem. Finding a data-driven way to determine an optimal choice for <italic>k</italic>, or evaluating whether <italic>α</italic> can be optimized additionally, are challenging themes for future research.</p>
  </sec>
  <sec>
    <title>6 Conclusions</title>
    <p>In this article we propose <italic>wenda</italic>, a method for unsupervised domain adaptation which is based on the elastic net and utilizes dependencies between inputs to detect differences between source and target domain. Using a weighted elastic net penalty, <italic>wenda</italic> enforces stronger regularization on features that behave differently in the two domains, reducing the effects of a distribution mismatch.</p>
    <p>We compare two variants of our method, <italic>wenda-pn</italic> and <italic>wenda-cv</italic>, on simulated datasets and on real data, where we considered the problem of age prediction from DNA methylation data across tissues. Our experimental results demonstrate that both variants can reduce test errors on samples with a distribution mismatch. While <italic>wenda-cv</italic> outperforms the non-adaptive reference only on real data, <italic>wenda-pn</italic> strongly reduces errors on test samples with a distribution mismatch both on real and simulated data, which makes it the more promising variant for future applications.</p>
    <p>From a wider perspective, this article demonstrates that the ambitious goal of unsupervised domain adaptation is indeed feasible not only for big data analysis with deep learning methods, but also for traditional machine learning methods that are useful for analyzing relatively small datasets as they frequently occur in computational biology and medicine.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz338_Supplementary_Data</label>
      <media xlink:href="btz338_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We would like to thank Dr Alexis Battle and Ben Strober for kindly providing the matrix of similarities plotted in Figure 2a in <xref rid="btz338-B1" ref-type="bibr">Aguet <italic>et al.</italic> (2017)</xref>. We additionally thank Martina Feierabend for reviewing the mapping of tissues between their data and ours.</p>
    <sec>
      <title>Funding</title>
      <p>This work was prepared within the project <italic>XplOit</italic> of the initiative ‘i: DSem—Integrative Datensemantik in der Systemmedizin’, funded by the German Federal Ministry of Education and Research (BMBF).</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz338-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Aguet</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Genetic effects on gene expression across human tissues</article-title>. <source>Nature</source>, <volume>550</volume>, <fpage>204</fpage>–<lpage>213</lpage>.<pub-id pub-id-type="pmid">29022597</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Akey</surname><given-names>J.M.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>On the design and analysis of gene expression studies in human populations</article-title>. <source>Nat. Genet</source>., <volume>39</volume>, <fpage>807</fpage>–<lpage>808</lpage>.<pub-id pub-id-type="pmid">17597765</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B3">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Aljundi</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) Landmarks-based kernelized subspace alignment for unsupervised domain adaptation. In: <italic>Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),</italic> 2015: 8–10 June, 2015, pp. <fpage>56</fpage>–<lpage>63</lpage>. Boston, Massachusetts, USA.</mixed-citation>
    </ref>
    <ref id="btz338-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Almagro Armenteros</surname><given-names>J.J.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>DeepLoc: prediction of protein subcellular localization using deep learning</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>3387</fpage>–<lpage>3395</lpage>.<pub-id pub-id-type="pmid">29036616</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Angermueller</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>DeepCpG: accurate prediction of single-cell DNA methylation states using deep learning</article-title>. <source>Genome Biol</source>., <volume>18</volume>, <fpage>67.</fpage><pub-id pub-id-type="pmid">28395661</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Assenov</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Comprehensive analysis of DNA methylation data with RnBeads</article-title>. <source>Nat. Methods</source>, <volume>11</volume>, <fpage>1138</fpage>–<lpage>1140</lpage>.<pub-id pub-id-type="pmid">25262207</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bell</surname><given-names>J.T.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Epigenome-wide scans identify differentially methylated regions for age and age-related phenotypes in a healthy ageing population</article-title>. <source>PLOS Genet</source>., <volume>8</volume>, <fpage>e1002629.</fpage><pub-id pub-id-type="pmid">22532803</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>The cancer genome atlas pan-cancer analysis project</article-title>. <source>Nat. Genet</source>., <volume>45</volume>, <fpage>1113</fpage>–<lpage>1120</lpage>.<pub-id pub-id-type="pmid">24071849</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Christensen</surname><given-names>B.C.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Aging and environmental exposures alter tissue-specific DNA methylation dependent upon CpG island context</article-title>. <source>PLOS Genet</source>., <volume>5</volume>, <fpage>e1000602.</fpage><pub-id pub-id-type="pmid">19680444</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B10">
      <mixed-citation publication-type="other">Civis Analytics (since <year>2016</year>) python-glmnet: A Python Port of the glmnet Package for Fitting Generalized Linear Models via Penalized Maximum Likelihood. Python Package Version 2.0.0. <ext-link ext-link-type="uri" xlink:href="http://github.com/civisanalytics/python-glmnet">http://github.com/civisanalytics/python-glmnet</ext-link> (10 May 2019, date last accessed)</mixed-citation>
    </ref>
    <ref id="btz338-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Cortes</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Mohri</surname><given-names>M.</given-names></name></person-group> (<year>2011</year>) Domain adaptation in regression. In: <italic>Proceedings of the 2011 International Conference on Algorithmic Learning Theory (ALT),</italic> 5–7 <italic>October, 2011</italic>, pp. <fpage>308</fpage>–<lpage>323</lpage>. Espoo, Finland.</mixed-citation>
    </ref>
    <ref id="btz338-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Day</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Differential DNA methylation with age displays both common and dynamic features across human tissues that are influenced by CpG landscape</article-title>. <source>Genome Biol</source>., <volume>14</volume>, <fpage>R102.</fpage><pub-id pub-id-type="pmid">24034465</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Edgar</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>Gene expression omnibus: NCBI gene expression and hybridization array data repository</article-title>. <source>Nucleic Acids Res</source>., <volume>30</volume>, <fpage>207</fpage>–<lpage>210</lpage>.<pub-id pub-id-type="pmid">11752295</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Farh</surname><given-names>K.K.-H.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Genetic and epigenetic fine mapping of causal autoimmune disease variants</article-title>. <source>Nature</source>, <volume>518</volume>, <fpage>337</fpage>–<lpage>343</lpage>.<pub-id pub-id-type="pmid">25363779</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Florath</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Cross-sectional and longitudinal changes in DNA methylation with age: an epigenome-wide analysis revealing over 60 novel age-associated CpG sites</article-title>. <source>Hum. Mol. Genet</source>., <volume>23</volume>, <fpage>1186</fpage>–<lpage>1201</lpage>.<pub-id pub-id-type="pmid">24163245</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fraser</surname><given-names>H.B.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) 
<article-title>Aging and gene expression in the primate brain</article-title>. <source>PLOS Biol</source>., <volume>3</volume>, <fpage>e274.</fpage><pub-id pub-id-type="pmid">16048372</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friedman</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>J. Stat. Softw</source>., <volume>33</volume>, <fpage>1</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ganin</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Domain-adversarial training of neural networks</article-title>. <source>J. Mach. Learn. Res</source>., <volume>17</volume>, <fpage>1</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garnett</surname><given-names>M.J.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Systematic identification of genomic markers of drug sensitivity in cancer cells</article-title>. <source>Nature</source>, <volume>483</volume>, <fpage>570</fpage>–<lpage>575</lpage>.<pub-id pub-id-type="pmid">22460902</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) Geodesic flow kernel for unsupervised domain adaptation. In: <italic>Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>, <italic>16–21 June, 2012</italic>, pp. 2066–2073. Rhode Island, USA.</mixed-citation>
    </ref>
    <ref id="btz338-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Gong</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Connecting the dots with landmarks: discriminatively learning domain-invariant features for unsupervised domain adaptation. In: <italic>Proceedings of the 30th International Conference on Machine Learning (ICML), 16–21 June, 2013</italic>, pp. 222–230. Atlanta, Georgia, USA.</mixed-citation>
    </ref>
    <ref id="btz338-B22">
      <mixed-citation publication-type="other">GPy (since <year>2012</year>) <italic>GPy: A Gaussian Process Framework in Python</italic> Python Package Version 1.5.3. <ext-link ext-link-type="uri" xlink:href="http://github.com/SheffieldML/GPy">http://github.com/SheffieldML/GPy</ext-link> (10 May 2019, date last accessed).</mixed-citation>
    </ref>
    <ref id="btz338-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hannum</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Genome-wide methylation profiles reveal quantitative views of human aging rates</article-title>. <source>Mol. Cell</source>, <volume>49</volume>, <fpage>359</fpage>–<lpage>367</lpage>.<pub-id pub-id-type="pmid">23177740</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <italic>impute: Imputation for Microarray Data</italic> R Package Version 1.52.0. <ext-link ext-link-type="uri" xlink:href="http://www.bioconductor.org/packages/release/bioc/html/impute.html">http://www.bioconductor.org/packages/release/bioc/html/impute.html</ext-link> (10 May 2019, date last accessed).</mixed-citation>
    </ref>
    <ref id="btz338-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heyn</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Distinct DNA methylomes of newborns and centenarians</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>., <volume>109</volume>, <fpage>10522</fpage>–<lpage>10527</lpage>.<pub-id pub-id-type="pmid">22689993</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hoerl</surname><given-names>A.E.</given-names></name>, <name name-style="western"><surname>Kennard</surname><given-names>R.W.</given-names></name></person-group> (<year>1970</year>) 
<article-title>Ridge regression: biased estimation for nonorthogonal problems</article-title>. <source>Technometrics</source>, <volume>12</volume>, <fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hoiles</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>van der Schaar</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>) 
<article-title>A non-parametric learning method for confidently estimating patient’s clinical state and dynamics</article-title>. <source>Adv. Neural Inform. Process. Syst</source>., <volume>29</volume>, <fpage>2020</fpage>–<lpage>2028</lpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Horvath</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>) 
<article-title>DNA methylation age of human tissues and cell types</article-title>. <source>Genome Biol</source>., <volume>14</volume>, <fpage>R115.</fpage><pub-id pub-id-type="pmid">24138928</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hughey</surname><given-names>J.J.</given-names></name>, <name name-style="western"><surname>Butte</surname><given-names>A.J.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Robust meta-analysis of gene expression using the elastic net</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>e79.</fpage><pub-id pub-id-type="pmid">25829177</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Ide</surname><given-names>J.S.</given-names></name>, <name name-style="western"><surname>Cozman</surname><given-names>F.G.</given-names></name></person-group> (<year>2002</year>) Random generation of Bayesian networks. In: <italic>Advances in Artificial Intelligence,</italic> Lecture Notes in Computer Science, Springer, Berlin.</mixed-citation>
    </ref>
    <ref id="btz338-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jalali</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Pfeifer</surname><given-names>N.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Interpretable per case weighted ensemble method for cancer associations</article-title>. <source>BMC Genom</source>., <volume>17</volume>, <fpage>501</fpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jansen</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>A Bayesian networks approach for predicting protein-protein interactions from genomic data</article-title>. <source>Science</source>, <volume>302</volume>, <fpage>449</fpage>–<lpage>453</lpage>.<pub-id pub-id-type="pmid">14564010</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krogan</surname><given-names>N.J.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Global landscape of protein complexes in the yeast <italic>Saccharomyces cerevisiae</italic></article-title>. <source>Nature</source>, <volume>440</volume>, <fpage>637</fpage>–<lpage>643</lpage>.<pub-id pub-id-type="pmid">16554755</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leek</surname><given-names>J.T.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Tackling the widespread and critical impact of batch effects in high-throughput data</article-title>. <source>Nat. Rev. Genet</source>., <volume>11</volume>, <fpage>733</fpage>–<lpage>739</lpage>.<pub-id pub-id-type="pmid">20838408</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leffler</surname><given-names>E.M.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Resistance to malaria through structural variation of red blood cell invasion receptors</article-title>. <source>Science</source>, <volume>356</volume>, <fpage>eaam6393.</fpage><pub-id pub-id-type="pmid">28522690</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lengauer</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Sing</surname><given-names>T.</given-names></name></person-group> (<year>2006</year>) 
<article-title>Bioinformatics-assisted anti-HIV therapy</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>4</volume>, <fpage>790</fpage>–<lpage>797</lpage>.<pub-id pub-id-type="pmid">16980939</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Constrained elastic net based knowledge transfer for healthcare information exchange</article-title>. <source>Data Min. Knowl. Discov</source>., <volume>29</volume>, <fpage>1094</fpage>–<lpage>1112</lpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Unsupervised domain adaptation with residual transfer networks</article-title>. <source>Adv. Neural Inform. Process. Syst</source>., <volume>29</volume>, <fpage>136</fpage>–<lpage>144</lpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Margolis</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>) A Literature Review of Domain Adaptation with Unlabeled Data<italic>.</italic> Technical Report, University of Washington.</mixed-citation>
    </ref>
    <ref id="btz338-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>S.J.</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>Q.</given-names></name></person-group> (<year>2010</year>) 
<article-title>A survey on transfer learning</article-title>. <source>IEEE Trans. Knowl. Data Eng</source>., <volume>22</volume>, <fpage>1345</fpage>–<lpage>1359</lpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Patel</surname><given-names>V.M.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Visual domain adaptation: a survey of recent advances</article-title>. <source>IEEE Signal Process. Mag</source>., <volume>32</volume>, <fpage>53</fpage>–<lpage>69</lpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name></person-group> (<year>1988</year>) <source>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</source>. 
<publisher-name>Morgan Kaufmann Publishers Inc</publisher-name>, 
<publisher-loc>San Francisco</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz338-B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Rasmussen</surname><given-names>C.E.</given-names></name>, <name name-style="western"><surname>Williams</surname><given-names>C.K.I.</given-names></name></person-group> (<year>2006</year>) <source>Gaussian Processes for Machine Learning</source>. 
<publisher-name>MIT Press</publisher-name>, 
<publisher-loc>Cambridge, MA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz338-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saito</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Sætrom</surname><given-names>P.</given-names></name></person-group> (<year>2012</year>) 
<article-title>Target gene expression levels and competition between transfected and endogenous microRNAs are strong confounding factors in microRNA high-throughput experiments</article-title>. <source>Silence</source>, <volume>3</volume>, <fpage>3.</fpage><pub-id pub-id-type="pmid">22325809</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schmidt</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Combining transcription factor binding affinities with open-chromatin data for accurate gene expression prediction</article-title>. <source>Nucleic Acids Res</source>., <volume>45</volume>, <fpage>54</fpage>–<lpage>66</lpage>.<pub-id pub-id-type="pmid">27899623</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schübeler</surname><given-names>D.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Function and information content of DNA methylation</article-title>. <source>Nature</source>, <volume>517</volume>, <fpage>321</fpage>–<lpage>326</lpage>.<pub-id pub-id-type="pmid">25592537</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stranger</surname><given-names>B.E.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Progress and promise of genome-wide association studies for human complex trait genetics</article-title>. <source>Genetics</source>, <volume>187</volume>, <fpage>367</fpage>–<lpage>383</lpage>.<pub-id pub-id-type="pmid">21115973</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teschendorff</surname><given-names>A.E.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Age-associated epigenetic drift: implications, and a case of epigenetic thrift?</article-title><source>Hum. Mol. Genet</source>., <volume>22</volume>, <fpage>R7</fpage>–<lpage>R15</lpage>.<pub-id pub-id-type="pmid">23918660</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Teschendorff</surname><given-names>A.E.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>A beta-mixture quantile normalization method for correcting probe design bias in Illumina Infinium 450 k DNA methylation data</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>189</fpage>–<lpage>196</lpage>.<pub-id pub-id-type="pmid">23175756</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thompson</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Comparative analysis of gene regulatory networks: from network reconstruction to evolution</article-title>. <source>Annu. Rev. Cell Dev. Biol</source>., <volume>31</volume>, <fpage>399</fpage>–<lpage>428</lpage>.<pub-id pub-id-type="pmid">26355593</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group> (<year>1996</year>) 
<article-title>Regression shrinkage and selection via the lasso</article-title>. <source>J. Royal Stat. Soc. Ser. B (Stat. Methodol.)</source>, <volume>58</volume>, <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
    </ref>
    <ref id="btz338-B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Varley</surname><given-names>K.E.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Dynamic DNA methylation across diverse human cell lines and tissues</article-title>. <source>Genome Res</source>., <volume>23</volume>, <fpage>555</fpage>–<lpage>567</lpage>.<pub-id pub-id-type="pmid">23325432</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wachinger</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Reuter</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Domain adaptation for Alzheimer’s disease diagnostics</article-title>. <source>NeuroImage</source>, <volume>139</volume>, <fpage>470</fpage>–<lpage>479</lpage>.<pub-id pub-id-type="pmid">27262241</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Cell and tissue type independent age-associated DNA methylation changes are not rare but common</article-title>. <source>Aging</source>, <volume>10</volume>, <fpage>3541</fpage>–<lpage>3557</lpage>.<pub-id pub-id-type="pmid">30482885</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B55">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ziller</surname><given-names>M.J.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Charting a dynamic DNA methylation landscape of the human genome</article-title>. <source>Nature</source>, <volume>500</volume>, <fpage>477</fpage>–<lpage>481</lpage>.<pub-id pub-id-type="pmid">23925113</pub-id></mixed-citation>
    </ref>
    <ref id="btz338-B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zou</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Hastie</surname><given-names>T.</given-names></name></person-group> (<year>2005</year>) 
<article-title>Regularization and variable selection via the elastic net</article-title>. <source>J. Royal Stat. Soc. Ser. B (Stat. Methodol.)</source>, <volume>67</volume>, <fpage>301</fpage>–<lpage>320</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
