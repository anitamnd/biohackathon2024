<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Ecol Evol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Ecol Evol</journal-id>
    <journal-id journal-id-type="doi">10.1002/(ISSN)2045-7758</journal-id>
    <journal-id journal-id-type="publisher-id">ECE3</journal-id>
    <journal-title-group>
      <journal-title>Ecology and Evolution</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-7758</issn>
    <publisher>
      <publisher-name>John Wiley and Sons Inc.</publisher-name>
      <publisher-loc>Hoboken</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8928879</article-id>
    <article-id pub-id-type="doi">10.1002/ece3.8701</article-id>
    <article-id pub-id-type="publisher-id">ECE38701</article-id>
    <article-categories>
      <subj-group subj-group-type="article-subject-classification">
        <subject>Behavioural Ecology</subject>
      </subj-group>
      <subj-group subj-group-type="article-subject-classification">
        <subject>Zoology</subject>
      </subj-group>
      <subj-group subj-group-type="overline">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Research Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BOVIDS: A deep learning‐based software package for pose estimation to evaluate nightly behavior and its application to common elands (<italic toggle="no">Tragelaphus oryx</italic>) in zoos</article-title>
      <alt-title alt-title-type="left-running-head">GÜBERT et al.</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="ece38701-cr-0001" contrib-type="author" corresp="yes">
        <name>
          <surname>Gübert</surname>
          <given-names>Jennifer</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1224-4817</contrib-id>
        <xref rid="ece38701-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <address>
          <email>guebert@bio.uni-frankfurt.de</email>
        </address>
      </contrib>
      <contrib id="ece38701-cr-0002" contrib-type="author">
        <name>
          <surname>Hahn‐Klimroth</surname>
          <given-names>Max</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-3995-419X</contrib-id>
        <xref rid="ece38701-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="ece38701-cr-0003" contrib-type="author">
        <name>
          <surname>Dierkes</surname>
          <given-names>Paul W.</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6046-6406</contrib-id>
        <xref rid="ece38701-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="ece38701-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">Faculty of Biological Sciences</named-content>
      <named-content content-type="organisation-division">Bioscience Education and Zoo Biology</named-content>
      <institution>Goethe University</institution>
      <city>Frankfurt</city>
      <country country="DE">Germany</country>
    </aff>
    <aff id="ece38701-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <named-content content-type="organisation-division">Faculty of Computer Science</named-content>
      <institution>TU Dortmund University</institution>
      <city>Dortmund</city>
      <country country="DE">Germany</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label><bold>Correspondence</bold><break/>
Jennifer Gübert, Faculty of Biological Sciences, Bioscience Education and Zoo Biology, Goethe University, Frankfurt, Germany.<break/>
Email: <email>guebert@bio.uni-frankfurt.de</email>
<break/>
</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <volume>12</volume>
    <issue seq="650">3</issue>
    <issue-id pub-id-type="doi">10.1002/ece3.v12.3</issue-id>
    <elocation-id>e8701</elocation-id>
    <history>
      <date date-type="rev-recd">
        <day>13</day>
        <month>2</month>
        <year>2022</year>
      </date>
      <date date-type="received">
        <day>10</day>
        <month>11</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>2</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <!--&#x000a9; 2022 Published by John Wiley & Sons Ltd.-->
      <copyright-statement content-type="article-copyright">© 2022 The Authors. <italic toggle="yes">Ecology and Evolution</italic> published by John Wiley &amp; Sons Ltd.</copyright-statement>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="file:ECE3-12-e8701.pdf"/>
    <abstract id="ece38701-abs-0001">
      <title>Abstract</title>
      <p>Only a few studies on the nocturnal behavior of African ungulates exist so far, with mostly small sample sizes. For a comprehensive understanding of nocturnal behavior, the data basis needs to be expanded. Results obtained by observing zoo animals can provide clues for the study of wild animals and furthermore contribute to a better understanding of animal welfare and better husbandry conditions in zoos. The current contribution reduces the lack of data in two ways. First, we present a stand‐alone open‐source software package based on deep learning techniques, named <italic toggle="yes">B</italic>ehavioral <italic toggle="yes">O</italic>bservations by <italic toggle="yes">V</italic>ideos and <italic toggle="yes">I</italic>mages using <italic toggle="yes">D</italic>eep‐Learning <italic toggle="yes">S</italic>oftware (BOVIDS). It can be used to identify ungulates in their enclosure and to determine the three behavioral poses “Standing,” “Lying—head up,” and “Lying—head down” on 11,411 h of video material with an accuracy of 99.4%. Second, BOVIDS is used to conduct a case study on 25 common elands (<italic toggle="yes">Tragelaphus oryx</italic>) out of 5 EAZA zoos with a total of 822 nights, yielding the first detailed description of the nightly behavior of common elands. Our results indicate that age and sex are influencing factors on the nocturnal activity budget, the length of behavioral phases as well as the number of phases per behavioral state during the night while the keeping zoo has no significant influence. It is found that males spend more time in REM sleep posture than females while young animals spend more time in this position than adult ones. Finally, the results suggest a rhythm between the Standing and Lying phases among common elands that opens future research directions.</p>
    </abstract>
    <abstract abstract-type="graphical" id="ece38701-abs-0002">
      <p>The goal of this paper is two‐fold. First, a case study on 25 common elands (<italic toggle="yes">Tragelaphus oryx</italic>) out of 5 EAZA zoos with a total of 11,411 h of video material out of 822 nights is conducted, yielding the first detailed description of the nightly behavior of common elands. The second goal is the presentation of an open‐source software package named BOVIDS which is based on recent machine learning techniques and is designed to handle large‐scale studies of nocturnal ungulates’ behavior.<boxed-text position="anchor" content-type="graphic" id="ece38701-blkfxd-0001"><graphic xlink:href="ECE3-12-e8701-g002.jpg" position="anchor" id="jats-graphic-1"/></boxed-text>
</p>
    </abstract>
    <kwd-group>
      <kwd id="ece38701-kwd-0001">deep learning</kwd>
      <kwd id="ece38701-kwd-0002">nightly behavior</kwd>
      <kwd id="ece38701-kwd-0003">posture estimation</kwd>
      <kwd id="ece38701-kwd-0004">REM sleep</kwd>
      <kwd id="ece38701-kwd-0005">
        <italic toggle="no">Tragelaphus oryx</italic>
      </kwd>
      <kwd id="ece38701-kwd-0006">video action classification</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="funding-0001">
        <funding-source>von Opel Hessische Zoostiftung</funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="11"/>
      <table-count count="2"/>
      <page-count count="23"/>
      <word-count count="18467"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>March 2022</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.1.2 mode:remove_FC converted:15.03.2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <p content-type="self-citation">
      <mixed-citation publication-type="journal" id="ece38701-cit-1001"><string-name><surname>Gübert</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Hahn‐Klimroth</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Dierkes</surname>, <given-names>P. W.</given-names></string-name> (<year>2022</year>). <article-title>BOVIDS: A deep learning‐based software package for pose estimation to evaluate nightly behavior and its application to common elands (<italic toggle="no">Tragelaphus oryx</italic>) in zoos</article-title>. <source>Ecology and Evolution</source>, <volume>12</volume>, <elocation-id>e8701</elocation-id>. <pub-id pub-id-type="doi">10.1002/ece3.8701</pub-id>
</mixed-citation>
    </p>
  </notes>
</front>
<body id="ece38701-body-0001">
  <sec id="ece38701-sec-0001">
    <label>1</label>
    <title>INTRODUCTION</title>
    <sec id="ece38701-sec-0002">
      <label>1.1</label>
      <title>General</title>
      <p>The nocturnal behavior of many African mammals is poorly studied. It is known that the behavioral patterns can vary greatly between day and night, as many large herbivorous mammals spend, especially in winter, most of their sleeping time during the night, while the activity patterns emerge primarily at daytime (Bennie et al., <xref rid="ece38701-bib-0002" ref-type="bibr">2014</xref>; Davimes et al., <xref rid="ece38701-bib-0011" ref-type="bibr">2018</xref>; Gravett et al., <xref rid="ece38701-bib-0018" ref-type="bibr">2017</xref>; Wu et al., <xref rid="ece38701-bib-0071" ref-type="bibr">2018</xref>). For a comprehensive understanding of diurnal rhythms, a behavioral description of the entire diurnal cycle is necessary. Currently, there are only few contributions studying the nocturnal behavior. It is much more accessible to observe zoo animals at night rather than animals in their natural habitat due to much easier installation options of the required equipment (Ryder &amp; Feistner, <xref rid="ece38701-bib-0053" ref-type="bibr">1995</xref>). In order not to disturb the animals, camera recordings are a good mean of data collection in this case. Data collected in zoos can be valuable to study animal's behavior. In various species, there are no differences found in the behavior of animals in the wild and in captivity (Hollén &amp; Manser, <xref rid="ece38701-bib-0023" ref-type="bibr">2007</xref>; Melfi &amp; Feistner, <xref rid="ece38701-bib-0036" ref-type="bibr">2002</xref>). This was verified recently for basic nocturnal activities like being in the REM sleep position between giraffes in zoos and in the wild (Burger et al., <xref rid="ece38701-bib-0006" ref-type="bibr">2020</xref>). Therefore, studies conducted in zoos can provide a good basis for describing the animals’ nocturnal behavior and the obtained results can subsequently serve as starting information for observations in the field (Burger et al., <xref rid="ece38701-bib-0006" ref-type="bibr">2020</xref>). In addition, a deeper knowledge of nocturnal behavior inside zoo enclosures could contribute information to further improve animal management and husbandry in zoos (Brando &amp; Buchanan‐Smith, <xref rid="ece38701-bib-0005" ref-type="bibr">2018</xref>) and provide conclusions on animal welfare (Walsh et al., <xref rid="ece38701-bib-0068" ref-type="bibr">2019</xref>). One explicit example is that REM sleep appears to be an important indicator of stress in giraffes (Sicks, <xref rid="ece38701-bib-0057" ref-type="bibr">2016</xref>), which can be measured by noninvasive methods.</p>
      <p>To describe nocturnal behavior unambiguously, reliable data are needed, especially because there are few comparisons in literature. This means that it would be preferable to observe multiple individuals of a species over a longer period of time to accurately describe the average behavior. Additionally, it is necessary to obtain data not only on one but on various species to close the existing knowledge gap. The extraction of meaningful information as well as a detailed evaluation of a mass of recorded data requires modern techniques to automate parts of this data mining process (Beery et al., <xref rid="ece38701-bib-0001" ref-type="bibr">2020</xref>; Lürig et al., <xref rid="ece38701-bib-0032" ref-type="bibr">2021</xref>; Norouzzadeh et al., <xref rid="ece38701-bib-0041" ref-type="bibr">2018</xref>). In the last decade, various computer vision and deep learning techniques found their way into behavioral biology and ecology (Chakravarty et al., <xref rid="ece38701-bib-0009" ref-type="bibr">2020</xref>; Dell et al., <xref rid="ece38701-bib-0012" ref-type="bibr">2014</xref>; Eikelboom et al., <xref rid="ece38701-bib-0013" ref-type="bibr">2019</xref>; Gerovichev et al., <xref rid="ece38701-bib-0017" ref-type="bibr">2021</xref>; Norouzzadeh et al., <xref rid="ece38701-bib-0040" ref-type="bibr">2021</xref>; Schneider et al., <xref rid="ece38701-bib-0055" ref-type="bibr">2018</xref>, <xref rid="ece38701-bib-0054" ref-type="bibr">2020</xref>; Valletta et al., <xref rid="ece38701-bib-0067" ref-type="bibr">2017</xref>), facilitating the task of dealing with a large dataset. Unfortunately, automatization of the evaluation of video recordings is challenging if the video recordings suffer from a very low framerate (lower than 5 fps), much background noise, or heavy truncation effects, as is usual in observations in stables as zoo enclosures, or even in installments in the wild. More precisely, background noise appearing in such recordings is, for instance, due to light reflections caused by infrared emitters and particulate matter caused by the hay, while truncation and occlusion effects appear if the camera is not able to capture the whole enclosure or there are multiple overlapping animals in one stable. It is to emphasize that those negative effects are stronger the more general the setup is. Systems for automatic detection of flies or mice under perfect laboratory conditions (Graving et al., <xref rid="ece38701-bib-0019" ref-type="bibr">2019</xref>; Kabra et al., <xref rid="ece38701-bib-0027" ref-type="bibr">2013</xref>; Pereira et al., <xref rid="ece38701-bib-0043" ref-type="bibr">2020</xref>) need to be much less robust to such effects than the system at hand for enclosures and stables. Of course, installments in the wild, like camera‐trap studies, must deal with even more noise and truncation.</p>
    </sec>
    <sec id="ece38701-sec-0003">
      <label>1.2</label>
      <title>Our contribution</title>
      <p>One of the two main objectives of this work tackles this challenge by making BOVIDS (<italic toggle="yes">B</italic>ehavioral <italic toggle="yes">O</italic>bservations by <italic toggle="yes">V</italic>ideos and <italic toggle="yes">I</italic>mages using <italic toggle="yes">D</italic>eep‐Learning <italic toggle="yes">S</italic>oftware) available, which is a stand‐alone software package based on deep learning techniques. To the best of our knowledge, this is the first fully open‐source software package tackling the task of evaluating the nocturnal behavior of stalled animals that contains functionalities required for data preparation, training of the deep learning parts, data prediction, and data presentation. More precisely, BOVIDS can be used to evaluate video recordings of stalled ungulates recorded at 1 fps regarding two classification tasks: “binary classification” (a two‐class classification task) and “total classification” (a three‐class classification task), which are defined by Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>) as follows. First, if an animal is not present on an image, the desired label is Out (being out of view) in both tasks. Second, in the total classification task, the three postures Standing, Lying—head up (LHU), and Lying—head down (LHD) need to be distinguished which will be properly defined in Section 2.2. The binary classification task asks only to distinguish Standing and Lying (combining LHU and LHD) if the animal is present. All discussed software as well as detailed instructions can be found in our GitHub repository: <ext-link xlink:href="https://github.com/Klimroth/BOVIDS" ext-link-type="uri">https://github.com/Klimroth/BOVIDS</ext-link> and on Zenodo (<ext-link xlink:href="https://doi.org/10.5281/zenodo.6143896" ext-link-type="uri">https://doi.org/10.5281/zenodo.6143896</ext-link>).</p>
      <p>As a second part of the paper, a case study is conducted that explains how BOVIDS can be applied by behavioral biologists to their own data and which statistical analyses can be directly conducted on the output of the software package. In this study, the nocturnal activity budget of 25 common elands is analyzed. To the best of our knowledge, the case study provides the first description of the nocturnal behavior of common elands. Over 11,000 h (822 nights) of video material from five different EAZA zoos were evaluated, a task that seems challenging in the absence of automatic evaluation and it is described in detail how BOVIDS can be used to observe and analyze several important behavioral biological key figures of nocturnal activity. The results contain activity budgets, which show the percentages of all examined behavioral states, a visualization of the Standing–Lying rhythm as well as an analysis of the possible influencing factors age, sex, and zoo husbandry.</p>
    </sec>
    <sec id="ece38701-sec-0004">
      <label>1.3</label>
      <title>Related work</title>
      <p>As mentioned earlier, several computational systems have found their way into behavioral biology and ecology (Chakravarty et al., <xref rid="ece38701-bib-0009" ref-type="bibr">2020</xref>; Dell et al., <xref rid="ece38701-bib-0012" ref-type="bibr">2014</xref>; Eikelboom et al., <xref rid="ece38701-bib-0013" ref-type="bibr">2019</xref>; Norouzzadeh et al., <xref rid="ece38701-bib-0040" ref-type="bibr">2021</xref>; Valletta et al., <xref rid="ece38701-bib-0067" ref-type="bibr">2017</xref>). Such systems are explicitly designed with respect to the underlying data. In the easiest tasks, cameras can be installed in a laboratory such that the recordings feature a high contrast between animals and the background as well as other laboratory conditions like a given steady camera angle and low background noise. Examples for such systems working with data of <italic toggle="yes">Drosophila</italic> flies or mice are <italic toggle="yes">JAABA</italic> (Kabra et al., <xref rid="ece38701-bib-0027" ref-type="bibr">2013</xref>), <italic toggle="yes">DeepBehavior</italic> (Graving et al., <xref rid="ece38701-bib-0019" ref-type="bibr">2019</xref>), and <italic toggle="yes">SLEAP</italic> (Pereira et al., <xref rid="ece38701-bib-0043" ref-type="bibr">2020</xref>). When data are recorded either in the natural habitat or in different zoo enclosures, it is much more challenging to collect appropriate data that are amenable to automatic evaluation, for instance due to variations in weather, brightness, and background. Furthermore, different cameras can rarely be adjusted in a way such that the recording angle matches the given requirements or to ensure that animals are not highly truncated. It is to emphasize that there are examples of systems that deal with those challenges. One approach under varying brightness conditions distinguishes the poses “Lying” and “Standing” of cows in free‐stall stables (Porto et al., <xref rid="ece38701-bib-0045" ref-type="bibr">2013</xref>). Furthermore, one success story is the work by Norouzzadeh et al. (<xref rid="ece38701-bib-0041" ref-type="bibr">2018</xref>, <xref rid="ece38701-bib-0040" ref-type="bibr">2021</xref>) whose system can automatically detect and count different species, and some shown behaviors using camera trap images of the Serengeti dataset (Swanson et al., <xref rid="ece38701-bib-0060" ref-type="bibr">2015</xref>). Similar systems working with camera trap images in the wild are presented by Schneider et al. (<xref rid="ece38701-bib-0054" ref-type="bibr">2020</xref>, <xref rid="ece38701-bib-0055" ref-type="bibr">2018</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="materials-and-methods" id="ece38701-sec-0005">
    <label>2</label>
    <title>MATERIALS AND METHODS</title>
    <p>As the purpose of this paper is two‐fold, this section is divided into several parts. In the section <italic toggle="yes">Data evaluation</italic>, methods and material used to collect the data of the case study and to evaluate the findings statistically are presented. Subsequently, the behavioral states of interest are defined properly in section <italic toggle="yes">Ethogram</italic>, whereas the section <italic toggle="yes">Foundations of Deep Learning</italic> introduces important concepts of machine learning used by BOVIDS. Finally, the section BOVIDS introduces and describes the single parts of the software package itself in more detail.</p>
    <sec id="ece38701-sec-0006">
      <label>2.1</label>
      <title>Data evaluation</title>
      <p>The dataset includes nights of 25 common elands (<italic toggle="yes">Tragelaphus oryx</italic>), whereas the number of nights per individual ranges from 15 to 49. In total, 822 nights with 11,411 h of video material are present. The data were collected in winter seasons between 2017 and 2020 in a total of five EAZA zoos in Germany (Allwetterzoo Münster, Erlebnis‐Zoo Hannover, Opel‐Zoo Kronberg, Zoo Dortmund and Zoom Erlebniswelt Gelsenkirchen). A detailed overview about the used data is given in Table <xref rid="ece38701-tbl-0001" ref-type="table">A1</xref>. For further analysis the individuals are categorized as follows: “young,” ranging from birth until the time of weaning with about six months, “subadult,” older than six months until sexual maturity with about two years of age and “adult” afterwards. These categories are chosen according to the information distributed across multiple prior works (Groves &amp; Leslie, <xref rid="ece38701-bib-0020" ref-type="bibr">2011</xref>; Myers et al., <xref rid="ece38701-bib-0039" ref-type="bibr">2021</xref>; Puschmann et al., <xref rid="ece38701-bib-0046" ref-type="bibr">2009</xref>; Tacutu et al., <xref rid="ece38701-bib-0061" ref-type="bibr">2013</xref>).</p>
      <p>All collected data are in the form of video recordings. The cameras used are capable of night vision due to built‐in infrared emitters (Lupus LE139HD or Lupus LE338HD with the recording device LUPUSTEC LE800HD or TECHNAXX PRO HD 720P). The recordings are made with a frame rate of 1 fps and the resolution ranges from 704 × 576 px to 1920 × 1080 px. Recording takes place in the stable during night, the time of the absence of animal keepers, which mostly ranges from 17:00 to 07:00 (14 h). In some cases, the recording time is 18:00 to 07:00 (13 h).</p>
      <p>The data were recorded continuously providing an exact time span for every behavior with a start and an end time (Martin &amp; Bateson, <xref rid="ece38701-bib-0035" ref-type="bibr">2015</xref>). The manually annotation was governed by the open‐source program BORIS, Version 7.7.3 (Friard &amp; Gamba, <xref rid="ece38701-bib-0016" ref-type="bibr">2016</xref>) and consists of 2374 h of video material out of 170 nights. BOVIDS requires the use of multiple deep neural networks for object detection (OD) and action classification (AC) as explained by Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>) and in the following section. To train an initial object detection network, at least 400 images of every enclosure were annotated using LabelImg (Tzutalin, <xref rid="ece38701-bib-0066" ref-type="bibr">2015</xref>) resulting in 11,326 images of common elands and 49,437 images of various African ungulates as already elaborated by Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>). Following the prescribed approach, the initial action classification networks were not only trained using 170 recordings (66,466 images) of common elands but also 113,407 images of other African ungulates with comparable postures. Furthermore, two rounds of offline hard example mining (OHEM) were conducted using additionally 14,381 images of common elands and 50,262 images of other African ungulates. Finally, the action classifiers used for common elands stalled together were fine‐tuned by 24,304 images stemming from manually annotated video files and 7377 images generated through OHEM. Detailed information can be found in Table <xref rid="ece38701-tbl-0001" ref-type="table">A1</xref>.</p>
      <p>All statistical analyses are conducted with R Studio (R Core Team, <xref rid="ece38701-bib-0048" ref-type="bibr">2014</xref>) and the figures, which are not given by BOVIDS, are produced using the core functionalities of R and the package ggplot2 (Wickham, <xref rid="ece38701-bib-0070" ref-type="bibr">2016</xref>). Statistical tests are performed differently for continuous and ordinal data. To conduct a two‐factor analysis of variance (ANOVA) on continuous data, normality is required which is tested by Shapiro–Wilk test for any behavior class. In case of significant deviation from normality (<italic toggle="yes">p</italic> &lt; .05), a normality transformation is applied to the data by R’s “bestNormalize” package (Peterson &amp; Cavanaugh, <xref rid="ece38701-bib-0044" ref-type="bibr">2020</xref>). To analyze differences between multiple groups on ordinal data, a Kruskal‐Wallis test is applied. Finally, as post hoc tests on all pairs of potentially significant factors, a collection of unpaired t‐tests is applied in the continuous case and, respectively, a collection of Wilcoxon tests in the ordinal case. The alpha level is adjusted by the Bonferroni–Holm adjustment in each case.</p>
    </sec>
    <sec id="ece38701-sec-0007">
      <label>2.2</label>
      <title>Ethogram</title>
      <p>The focus of this paper is to distinguish between three postures: Standing, Lying—head up (LHU), and Lying—head down (LHD). Finally, if there is no animal present, the assigned label is out of view (Out). The latter label can also be given if only a small part of the animal is visible, from which the posture cannot be inferred. Furthermore, the class Lying is defined as the union of LHU and LHD. The binary classification task which distinguishes Standing, Lying, and Out allows to analyze rhythms over the night as the categories “activity” and “rest” are the most prominently measured behavior stages to examine diurnal rhythms (Merrow et al., <xref rid="ece38701-bib-0037" ref-type="bibr">2005</xref>). In the following ethogram, based on that of Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>), the three behavioral states are defined and shown in Figure <xref rid="ece38701-fig-0001" ref-type="fig">1</xref>.
<list list-type="simple" id="ece38701-list-0001"><list-item><p>Standing: The animal stands in an upright position on all four hooves. The exact behavior is neglected, thus the animal could be, for instance, feeding, resting, or ruminating.</p></list-item><list-item><p>Lying—head up (LHU): The animal lies down, and its head is lifted. The behavioral state does not distinguish if the animal is awake or in non‐REM sleep. As before, the precise behavior is neglected.</p></list-item><list-item><p>Lying—head down (LHD): The animal is lying with its head resting on the ground. The head's position is beside the body or sometimes in front of it.</p></list-item></list>
</p>
      <fig position="float" fig-type="FIGURE" id="ece38701-fig-0001">
        <label>FIGURE 1</label>
        <caption>
          <p>The three observed behavioral states: Standing, Lying—head up, Lying—head down, from left to right of common elands</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8701-g010" position="anchor" id="jats-graphic-3"/>
      </fig>
      <p>It is crucial to notice that LHD is the typical REM (rapid eye movement) sleep posture. REM sleep is recognized through various behavioral components as the animal is lying with its head resting due to postural atonia (Lima et al., <xref rid="ece38701-bib-0029" ref-type="bibr">2005</xref>; Zepelin et al., <xref rid="ece38701-bib-0074" ref-type="bibr">2005</xref>). This characteristically REM sleep position can be used to estimate the REM sleep, a common approach in the study of behavior of common elands (Zizkova et al., <xref rid="ece38701-bib-0075" ref-type="bibr">2013</xref>) and cows (Ternman et al., <xref rid="ece38701-bib-0064" ref-type="bibr">2014</xref>).</p>
    </sec>
    <sec id="ece38701-sec-0008">
      <label>2.3</label>
      <title>Foundations of Deep Learning</title>
      <p>In supervised machine learning tasks, one is usually interested to design a system that allows automatic prediction of new data based on manually annotated examples (Russell &amp; Norvig, <xref rid="ece38701-bib-0052" ref-type="bibr">2016</xref>). In this contribution, two excessively studied supervised learning tasks are employed: object detection and action classification.</p>
      <p>In the easiest variant of the object detection task, an image is given as an input and the system is asked to draw a bounding box around the objects appearing in the image (bounding box regression) and to assign a class label that describes the content of each bounding box (classification). On a very high‐level description, there are two different approaches to this task. In one‐step object‐detection a bounding box is drawn, and the corresponding label is assigned simultaneously while in two‐step object‐detection, those tasks are conducted sequentially (Jiao et al., <xref rid="ece38701-bib-0026" ref-type="bibr">2019</xref>). Well‐known representatives of one‐step solutions are yolo and SSD while there are various well‐known two‐step architectures like FasterRCNN, MaskRCNN, or EfficientDet. Without going into much detail, comparably modern one‐step architectures are mostly faster at the task as two‐step architectures but perform slightly worse in the classification part (Ouchra &amp; Belangour, <xref rid="ece38701-bib-0042" ref-type="bibr">2021</xref>).</p>
      <p>Similarly, there is a huge set of deep learning architectures designed for the action classification task. In the easiest variant, an image is given, and the system needs to assign one unique class label out of a given set of labels (Lu &amp; Weng, <xref rid="ece38701-bib-0031" ref-type="bibr">2007</xref>). Prominent architectures are ResNet (He et al., <xref rid="ece38701-bib-0022" ref-type="bibr">2016</xref>), EfficientNet (Tan &amp; Le, <xref rid="ece38701-bib-0063" ref-type="bibr">2019</xref>), or CoAtNet (Dai et al., <xref rid="ece38701-bib-0010" ref-type="bibr">2021</xref>). The performance of such a classifier is measured by two important metrics: the accuracy as well as the f‐score (Tharwat, <xref rid="ece38701-bib-0065" ref-type="bibr">2021</xref>).</p>
      <p>Suppose a sequence of <italic toggle="yes">n</italic> images is predicted and image <italic toggle="yes">i</italic> gets label <italic toggle="yes">s<sub>i</sub>
</italic> assigned by the classifier while its correct label, called ground‐truth, was <italic toggle="yes">t<sub>i</sub>
</italic>. Suppose furthermore that classes 0, 1, …, <italic toggle="yes">k</italic> exist. Therefore, there are two sequences of labels <mml:math id="jats-math-1"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>…</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>…</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math> which represent the classification by the neural network and the ground‐truth, respectively.</p>
      <p>The accuracy is defined as the proportion of correctly labeled images among all images, or formally,<disp-formula id="ece38701-disp-0001"><mml:math id="jats-math-2" display="block"><mml:mrow><mml:mtext>accuracy</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced close="|" open="|"><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mfenced><mml:mi>n</mml:mi></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p>
      <p>The accuracy is a good indicator on how well a model performs on average, but if there are some underrepresented classes, the model's performance on those classes is not properly described by the accuracy. The f‐score, the harmonic mean of precision and recall, is a measure that describes the performance of a model per class quite well. To this end, let <mml:math id="jats-math-3"><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="|" open="|"><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:math> be the number of true positives classified by the model of class <italic toggle="yes">c</italic> and <mml:math id="jats-math-4"><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="|" open="|"><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:math> be the number of false positives, respectively. Analogously, define <mml:math id="jats-math-5"><mml:mrow><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="|" open="|"><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:math> as the number of false negatives of class <italic toggle="yes">c</italic>. Then, the <italic toggle="yes">f</italic>‐score of class <italic toggle="yes">c</italic> can be expressed as<disp-formula id="ece38701-disp-0002"><mml:math id="jats-math-6" display="block"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>- score</mml:mtext></mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>·</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p>
      <p>While accuracy and f‐score capture important aspects of a deep learning model, only optimizing with respect to those metrics might not be sufficient in certain applications. Video action classification is such an example. Given a video file, the task is to train a model that can accurately predict the observed action at each time‐step of the video file. Very short misclassified sequences in a long video are clearly not captured by the f‐score or the accuracy but it causes classification flickering which might be problematic if one is interested in key quantities like the average length of certain activities. There are various recent developments in video action classification, most building up on so‐called “recurrent neural networks,” which have in common that multiple dimensions of the data given in the videos are used (Xu et al., <xref rid="ece38701-bib-0072" ref-type="bibr">2016</xref>). First, there is a spatial dimension which is the evaluation of a single frame of the video file by classical action classification. Second, there is a temporal dimension given as the single frames are coming as a sequence and the evolution over time contains information. Capturing the temporal dimension with state‐of‐the‐art approaches becomes hard if the framerate of the video is very low (See &amp; Rahman, <xref rid="ece38701-bib-0056" ref-type="bibr">2015</xref>). A more classical approach toward employing the temporal dimension is the “multiple‐frame encoding” (Franche &amp; Coulombe, <xref rid="ece38701-bib-0015" ref-type="bibr">2012</xref>; Ji et al., <xref rid="ece38701-bib-0025" ref-type="bibr">2013</xref>) in which subsequent frames are merged into one image that is fed into the model. This approach allows capturing the temporal dimension even given a low framerate, but it is inferior to more involved strategies as soon as the framerate increases (Xu et al., <xref rid="ece38701-bib-0072" ref-type="bibr">2016</xref>). This multiple‐frame encoding will also be used in the present contribution, as the available video material is recorded with 1 frame per second.</p>
      <p>In supervised learning tasks, a user presents the model a set of examples and the model is built upon those examples. This procedure is called training. More precisely, it is usual to split this set of examples into two parts: a training set and a validation set. During training, the accuracies of the model with respect to the training set as well as to the validation set are constantly measured and the model is optimized regarding the performance on the training set. In the survey by Wang et al. (<xref rid="ece38701-bib-0069" ref-type="bibr">2020</xref>), different metrics as the accuracy as target functions of this optimization process are discussed. While the performance on the training and validation data is of great theoretical interest, in applications, one is interested in the so‐called generalization accuracy. To measure this accuracy, a third dataset of manually annotated data points is required, the test set. The important difference between training and validation set is that the images in the test set were not presented to the model during training and, therefore, the model's performance on these data is a good indicator on how well the model will perform in an application. It is well‐known that the performance on the test set is better, the more similar the testing images are to the images presented during training. The discrepancy between the distribution of training images and testing images is called distribution shift and machine learning models are known to be brittle even to small distribution shifts (Quiñonero‐Candela et al., <xref rid="ece38701-bib-0047" ref-type="bibr">2008</xref>) and, therefore, one tries to find a set of training images that represents the images in the application as best as possible.</p>
    </sec>
    <sec id="ece38701-sec-0009">
      <label>2.4</label>
      <title>BOVIDS</title>
      <p>BOVIDS is an end‐to‐end software package which automatically identifies individuals of ungulates and their postures in videos. The detection itself is based on a sequential application of object detection and video action classification governed by state‐of‐the‐art deep neural networks, yolov4 (Bochkovskiy et al., <xref rid="ece38701-bib-0003" ref-type="bibr">2020</xref>), and EfficientNet‐B3 (Tan &amp; Le, <xref rid="ece38701-bib-0063" ref-type="bibr">2019</xref>), see Figure <xref rid="ece38701-fig-0002" ref-type="fig">2</xref>. As explained, there are two classification tasks (total classification and binary classification). The object detector is used uniformly for both tasks while different sets of action classifiers are trained for either recognition of three classes or two classes, respectively.</p>
      <fig position="float" fig-type="FIGURE" id="ece38701-fig-0002">
        <label>FIGURE 2</label>
        <caption>
          <p>Visualization of the sequential application of the yolov4 object detector and the EfficientNet‐B3 action classifier</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8701-g008" position="anchor" id="jats-graphic-5"/>
      </fig>
      <p>It is important to emphasize that the following description is meant to present one possible way of using a deep‐learning pipeline, starting from data preparation, over training and evaluation, and ending with the preparation of real data for statistical analyses. Hereby, the used deep learning models perform well on testing sets and are known to be fast (Bochkovskiy et al., <xref rid="ece38701-bib-0003" ref-type="bibr">2020</xref>; Tan &amp; Le, <xref rid="ece38701-bib-0063" ref-type="bibr">2019</xref>). The description is not meant to be the single possible way of implementing such a system. As will be shown, the system is easy to apply, and the results are satisfactory from a biologist's point of view.</p>
      <sec id="ece38701-sec-0010">
        <label>2.4.1</label>
        <title>Overview</title>
        <p>This section is devoted to give a short overview about BOVIDS’ functionality. The system is designed to achieve a good performance in long‐term studies using video recordings in enclosures. This includes observation of zoo animals as well as farm animal husbandry. The goal is to tell the posture (Standing, LHU, LHD) of the observed animals at any time in the video with high precision to describe its fundamental behavior as well as possible.</p>
        <p>Manual annotation of a video file of one night (14 h) by a trained person requires roughly about two hours which indicates that only a few video files out of a longer observation period can be evaluated manually. This is a challenge as one is confronted with two problems in designing a valid training set for a deep learning model. First, the postures Standing, LHU, and LHD are highly imbalanced such that out of 14 h of video material, only a small portion can be easily used in a training set. It is of course possible to train on imbalanced data, but even this has limitations (Liu et al., <xref rid="ece38701-bib-0030" ref-type="bibr">2019</xref>). Second, on different nights, the video recordings may vary due to changes in external conditions, like brightness or positioning of hay. Therefore, data recorded on different nights undergo a mild distribution shift. As manual annotation of many nights is very time‐consuming and annotation of random periods of each night might cause an even more severe class imbalance, this contribution suggests an adaptation of a process called “offline hard example mining” (Felzenszwalb et al., <xref rid="ece38701-bib-0014" ref-type="bibr">2010</xref>). This approach tries to minimize human working load by the cost of higher computational cost in an iterative process. Miao et al. (<xref rid="ece38701-bib-0038" ref-type="bibr">2021</xref>) conducted an extensive study on such iterative processes and analyzed its performance with respect to deep‐learning models that evaluate camera‐trap images.</p>
        <p>In the following section, a high‐level sketch of the functionalities of BOVIDS is given and the details can be found in the subsequent sections. BOVIDS is divided into four components:
<list list-type="simple" id="ece38701-list-0002"><list-item><p>BOV 1. Data collection,</p></list-item><list-item><p>BOV 2. Object detection (OD),</p></list-item><list-item><p>BOV 3. Action classification (AC),</p></list-item><list-item><p>BOV 4. Data prediction.</p></list-item></list>
</p>
        <p>While a part of BOV 4 is a significantly improved and extended version of work presented in an earlier contribution (Hahn‐Klimroth et al., <xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>), the newly developed components BOV 1–BOV 3 allow an interested user to apply the complete system comfortably to their own data. The software package consists of various small python scripts that allow to handle large datasets more conveniently and prepare the data in a way that can be used to apply the prediction pipeline BOV 4.</p>
        <p>BOV 1 allows to convert video recordings directly from the LUPUS observation system. To annotate new data automatically, the prediction pipeline of BOVIDS (BOV 4) is used. The necessary scripts to prepare the training and validation set and to conduct the training are presented in BOV 2 for the object detector, while BOV 3 provides these functionalities with regard to the action classifier. Furthermore, those sections contain a description of one possibility to fine‐tune the models and achieve a good performance. Finally, multiple tools to measure the accuracy of the prediction and to detect systematic errors by BOVIDS are provided in BOV 4. Also, tools to represent and visualize the data that are a good starting point to apply further statistical methods are presented in this section. A visualization of the complete process is given in Figure <xref rid="ece38701-fig-0003" ref-type="fig">3</xref>.</p>
        <fig position="float" fig-type="FIGURE" id="ece38701-fig-0003">
          <label>FIGURE 3</label>
          <caption>
            <p>Overview of the System BOVIDS and all its categories</p>
          </caption>
          <graphic xlink:href="ECE3-12-e8701-g007" position="anchor" id="jats-graphic-7"/>
        </fig>
      </sec>
      <sec id="ece38701-sec-0011">
        <label>2.4.2</label>
        <title>BOV 1: Data preparation</title>
        <p>BOVIDS creates a collection of video files, one per night automatically if the data are recorded by the LUPUS observation system. If some data are missing due to power failure, the missing frames can be filled with a sequence of black frames to ensure a joint observation time over all video files. Such sequences of black frames will be labeled as Out by BOVIDS during prediction and, therefore, represent reality quite well.</p>
      </sec>
      <sec id="ece38701-sec-0012">
        <label>2.4.3</label>
        <title>BOV 2: Training an object detector (OD)</title>
        <p>The final object detector is trained following the subsequent procedure:
<list list-type="simple" id="ece38701-list-0003"><list-item><p>OD 1. Manual annotation of images.</p></list-item><list-item><p>OD 2. Train a first version of the object detector.</p></list-item><list-item><p>OD 3. Offline hard example mining (OHEM).
</p><list list-type="simple" id="ece38701-list-0004"><list-item><p>a. Automatic annotation of unseen data.</p></list-item><list-item><p>b. Evaluation of the suggested bounding boxes.</p></list-item><list-item><p>c. Retrain the deep neural network.</p></list-item></list></list-item></list>
</p>
        <p>In the initial annotation task (OD 1), between 400 and 800 images are sampled stemming from multiple videos per enclosure over the observation period to increase the data variability. The number of images sampled in total depends on how much data there are overall, how difficult the detection appears to be, and whether individuals need to be distinguished. Those images are annotated manually by a freely available third‐party software package called LabelImg (Tzutalin, <xref rid="ece38701-bib-0066" ref-type="bibr">2015</xref>) and the initial training can be performed (OD 2). Hereby, 5% of the data is used as the validation set while 95% of the data is used for training.</p>
        <p>To run an adapted version of the so‐called “offline hard example mining” (Felzenszwalb et al., <xref rid="ece38701-bib-0014" ref-type="bibr">2010</xref>), in short OHEM (OD 3), the object detector is used to automatically annotate 300–600 images out of unseen videos of the same set of enclosures (OD 3a). The quality of each such automatically drawn bounding box is evaluated. Hereby, a human assigns one out of four classes (good, okay, poor, swapped) to each bounding box (OD 3b) which is visualized in Figure <xref rid="ece38701-fig-0004" ref-type="fig">4</xref>. If the bounding boxes are satisfyingly accurate, the procedure stops at this point. Otherwise, the bounding boxes evaluated as poor or swapped are corrected manually using LabelImg. Those bounding boxes can be seen as “hard examples” as the current version of the object detector struggles at prediction. The freshly corrected annotations together with the well‐evaluated bounding boxes are used to increase the training set of the object detector and the object detector is trained on this new, extended set. Again, 5% of the existing data is used for validation. This procedure can be repeated until satisfying results are achieved. In the conducted case study, one iteration sufficed to achieve a decent accuracy. After having trained an accurately working object detector, this object detector is one ingredient required to generate a training set for the action classifiers.</p>
        <fig position="float" fig-type="FIGURE" id="ece38701-fig-0004">
          <label>FIGURE 4</label>
          <caption>
            <p>Example of the four classes that can be given in evaluation, good (green), okay (yellow), bad (red), and swapped (blue)</p>
          </caption>
          <graphic xlink:href="ECE3-12-e8701-g004" position="anchor" id="jats-graphic-9"/>
        </fig>
      </sec>
      <sec id="ece38701-sec-0013">
        <label>2.4.4</label>
        <title>BOV 3: Action classification (AC)</title>
        <p>The action classifier's goal is to predict the pose of an individual on a single image (single‐frame, SF) to capture the spatial dimension of the video, respectively, on four subsequent images placed next to each other (multiple‐frame, MF) to capture the temporal dimension. The case study suggests that the following iterative process generates a well‐performing action classifier and finds a good balance between accuracy of the deep learning model and human annotation time.
<list list-type="simple" id="ece38701-list-0005"><list-item><p>AC 1. Annotation of few video files.</p></list-item><list-item><p>AC 2. Training of a first version of the ACs.
</p><list list-type="simple" id="ece38701-list-0006"><list-item><p>a. Preparation of an initial training set.</p></list-item><list-item><p>b. Training of the ACs.</p></list-item></list></list-item><list-item><p>AC 3. One or multiple rounds of OHEM
</p><list list-type="simple" id="ece38701-list-0007"><list-item><p>a. Prediction of many new video files.</p></list-item><list-item><p>b. Extracting hard as well as random examples as single images.</p></list-item><list-item><p>c. Manually evaluating the performance on those examples.</p></list-item><list-item><p>d. Retrain the network based on the evaluated images.</p></list-item></list></list-item></list>
</p>
        <p>When starting from scratch, it is most convenient to annotate the behavior of each single frame of a video by annotating the whole video (AC 1), for instance using the third‐party software package BORIS (Friard &amp; Gamba, <xref rid="ece38701-bib-0016" ref-type="bibr">2016</xref>). In the conducted case study, video material corresponding to 170 nights was annotated manually, see Table <xref rid="ece38701-tbl-0001" ref-type="table">A1</xref>. To generate the training set, equally many images (single‐frame and multiple‐frame encoded) of each posture (Standing, LHU and LHD) are extracted from the annotated video files by using the previously trained object detector. This balancing is one possible way to ensure that training of the action classifiers works decently (Japkowicz &amp; Stephen, <xref rid="ece38701-bib-0024" ref-type="bibr">2002</xref>). The reader should be aware that there are different strategies to deal with class imbalance that will not be discussed in this contribution (Liu et al., <xref rid="ece38701-bib-0030" ref-type="bibr">2019</xref>). Due to the class balancing and the underrepresentation of LHD in the video data, it is possible to extract roughly 500 images per class and per 14‐hour video on our dataset. To start training, a training set with 90% of those images and a validation set with 10% of those images are created (AC 2a). Finally, four EfficientNet‐B3 CNNs, namely the single‐frame classifier and the multiple‐frame classifier for both (binary and total) classification tasks (AC 2b) are trained.</p>
        <p>These first versions of the action classifiers are supposed to work quite decently on the videos used for the training, but it is likely that the classification accuracy is worse on different videos of the same animal in which the arrangement of the enclosure as well as the light conditions might be quite different due to the already discussed distribution shift (Quiñonero‐Candela et al., <xref rid="ece38701-bib-0047" ref-type="bibr">2008</xref>). For this reason, it seems sensible to reduce the distribution shift between the training set and the data required to be predicted by increasing the variability of the training data. To this end, we adapt the classical offline hard example mining to the setting at hand (AC 3) as follows. First, a fairly large number of momentarily not annotated video files will be predicted by BOVIDS (AC 3a). The accuracy of this prediction is expected to be at least 90% as Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>) already discussed. Therefore, BOVIDS provides an educated guess on the labels of each time interval of many video files that could not have been annotated manually without spending too much human annotation time. Based on those predicted labels, one samples a decent number of images in almost balanced classes distributed over the whole observation time (AC 3b). In the conducted case study, 72,020 images were sampled that way. These images are close to a uniform sample of the data on balanced classes of the whole underlying data and can, therefore, be referred to as “random” examples. These examples can now be evaluated by a human observer in a moderate amount of time (AC 3c). It is to be stressed at this point that a decent classifier is a critical ingredient: As the classes are highly unbalanced, random sampling without an educated guess would result in a set of images with almost no LHD, therefore, this simple process would not be possible to be used for generating a balanced training set.</p>
        <p>Besides mining such random examples, it is also possible to extract “hard” examples easily. In this contribution, a hard example is defined as an image for which either the certainty of classification by the action classifier is small or if it belongs to a time interval of which the predictions of the single‐frame and multiple‐frame action classifier disagree. It is supposed that neural networks can be finetuned efficiently by hard examples (Felzenszwalb et al., <xref rid="ece38701-bib-0014" ref-type="bibr">2010</xref>). Therefore, instead of only generating random samples distributed across the observation time, it is possible to nudge the training set into a direction such that information from momentarily hard to classify data gets boosted.</p>
        <p>Based on the human evaluation of the single images it is now possible to retrain the action classifiers on a much broader dataset that really represents the distribution of the data that needs to be classified. At this point, the training classes might get slightly unbalanced if the human annotation deviates strongly from the automatic one. In this case standard techniques like random upsampling might be considered (Branco et al., <xref rid="ece38701-bib-0004" ref-type="bibr">2016</xref>) and are provided by BOVIDS, of course, different ways to deal with this imbalance can also be employed (Liu et al., <xref rid="ece38701-bib-0030" ref-type="bibr">2019</xref>). Once a decent object detector and a well‐performing action classifier are generated, all data can be predicted once more and the performance of BOVIDS can be measured.</p>
        <p>At this point, we want to emphasize that training and validation data are generated as usual in machine learning for the object detection and the action classification tasks. However, generation of a suitable testing set and choosing a decent evaluation metric is more involved as the performances of the object detector and the action classifiers as single systems are subordinate to the outcome of their sequential application. This will be discussed in detail in Section 2.4.5.2.</p>
      </sec>
      <sec id="ece38701-sec-0014">
        <label>2.4.5</label>
        <title>BOV 4: Data prediction</title>
        <p>The data prediction step consists of three major parts (DP 1–DP 3) that are discussed in this section and are read as:
<list list-type="simple" id="ece38701-list-0008"><list-item><p>DP 1: Prediction
</p><list list-type="simple" id="ece38701-list-0009"><list-item><p>P 1. Object detection phase</p></list-item><list-item><p>P 2. Action classification phase</p></list-item><list-item><p>P 3. Postprocessing phase.</p></list-item></list></list-item><list-item><p>DP 2: Data evaluation</p></list-item><list-item><p>DP 3: Data presentation.</p></list-item></list>
</p>
        <sec id="ece38701-sec-0015">
          <title>DP 1: The prediction pipeline</title>
          <p>The system of Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>) predicts a video file in three phases:
<list list-type="simple" id="ece38701-list-0010"><list-item><p>P 1. Object detection phase</p></list-item><list-item><p>P 2. Action classification phase</p></list-item><list-item><p>P 3. Postprocessing phase.</p></list-item></list>
</p>
          <p>These phases must not be confused with BOV 2 and BOV 3 that describe how to train the required deep neural networks while P 1–P 3 are phases within the prediction pipeline of Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>) that require the previously trained networks. These phases are briefly explained below, and improvements and new features provided by BOVIDS, in contrast to the original system, are highlighted.</p>
          <p>In the object detection phase (P 1), the system will first decompose a video file into so‐called “time intervals”. This is a discretization of the continuous data into packages of seven seconds each. More precisely, for each time interval the prediction pipeline will collect four images. Then, the object detector is used to identify the animal present in the images or, respectively, declare that no animal is present. While this step is governed by a Mask‐RCNN network by Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>) in the current version the architecture is changed to the much more recent yolov4 network as implemented by Taipingeric (<xref rid="ece38701-bib-0062" ref-type="bibr">2020</xref>) which improves the classification accuracy (Bochkovskiy et al., <xref rid="ece38701-bib-0003" ref-type="bibr">2020</xref>) and significantly speeds up the complete prediction pipeline by approximately 40% on the same hardware. The merit of this step is two‐fold. First, as pointed out by Yosinski et al. (<xref rid="ece38701-bib-0073" ref-type="bibr">2014</xref>), it increases the similarity between images taken from different enclosures. This dramatically improves the chance of meaningful learning of the poses from various videos. Second, it is used to distinguish between distinct individuals within the same enclosure. At the end of the object detection phase, each time interval is represented in two ways for every individual: As a sequence of single images (single‐frame) and additionally as one image in which these images are placed next to each other (multiple‐frame encoded representation (Franche &amp; Coulombe, <xref rid="ece38701-bib-0015" ref-type="bibr">2012</xref>; Ji et al., <xref rid="ece38701-bib-0025" ref-type="bibr">2013</xref>)).</p>
          <p>The subsequent step, the action classification phase (P 2) to determine the behavioral classes, is a classical image classification task. For both, the single‐ and multiple‐frame representations, this task is governed by two independently trained EfficientNetB3 CNNs per time interval. The final prediction for any time interval is calculated as the average over both outcomes. Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>) already describe that the “total classification” task (distinguishing Standing, LHU, LHD) might be much more difficult than the “binary classification” task (distinguishing Standing and Lying) and gives the possibility to map the final prediction from LHU and LHD to Lying. The approach of BOVIDS toward this binary task is slightly different. It is necessary to train a set of independent networks that purely govern this binary classification such that possible features can eventually be better learned.</p>
          <p>To reduce classification flickering, Hahn‐Klimroth et al. (<xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>) propose a set of postprocessing rules (P 3) which are applied to the sequence of classifications of time intervals. Those postprocessing rules dismiss very short sequences of a specific action as those sequences are likely to stem from short periods of false classifications. In the current setting the set of postprocessing rules is extended. It is now possible to handle flickering between Out and a specific behavior more smoothly to incorporate short periods in which the object detector failed to detect or identify the present individual. Of course, such a postprocessing step might dismiss short phases which are present in the data. Therefore, choosing an appropriate set of rules is a trade‐off between a stronger methodological error (errors made by BOVIDS through misclassification of short events) and a systematic error (errors caused by dismissing short phases on real data). BOVIDS contains tools for a systematic study of both types of errors. If the systematic error is appropriate for the application, one can compare BOVIDS’ prediction with the postprocessed real data to describe the methodological error.</p>
          <p>In the present work, the chosen set of postprocessing rules varies significantly between the binary and the total classification task. Indeed, as the binary classification task is meant to study longer periods of Standing and Lying, phases up to 5 min are dismissed. Furthermore, in the total classification task, it is distinguished between adult common elands and nonadult common elands as the latter show shorter phases than the adult individuals. A detailed overview over the used postprocessing rules can be found in Table <xref rid="ece38701-tbl-0002" ref-type="table">A2</xref>.</p>
        </sec>
        <sec id="ece38701-sec-0016">
          <title>DP 2: Data evaluation</title>
          <p>As the prediction of a deep learning‐based system works, in the end, as a black box, it is very important to assure the quality of the prediction regarding all quantities of interest. Therefore, it is crucial to define a valid testing set and appropriate evaluation metrics. Due to the iterative process on how the training set was found, the images used for training the action classifiers are an almost uniform sample from the whole observation period. Thus, any specific video is an adequate sample to determine the expected accuracy which implies that a good testing set is given by the already manually annotated videos. Observe that during training only the object detector and the action classifiers as single systems were evaluated with respect to a validation set but ultimately, it is more important that the prediction of a complete video is accurate with respect to biologically interesting quantities.</p>
          <p>To quantify the accuracy of the prediction on the testing set, performance indicators from machine learning theory as well as biological key figures are evaluated by the following four quality criteria.
<list list-type="simple" id="ece38701-list-0011"><list-item><p>QC 1. Analysis of the object detector per night (“detection density”).</p></list-item><list-item><p>QC 2. Accuracy and f‐score as well as a comparison of the number of phases, the median phase length, and the overall percentage per activity class between BOVIDS’ prediction and the manual annotation.</p></list-item><list-item><p>QC 3. Number, length, and type of misclassified sequences.</p></list-item><list-item><p>QC 4. Visual checking for outliers.</p></list-item></list>
</p>
          <p>While QC 2 and QC 3 are quality criteria which can be only evaluated with respect to the testing set, QC 1 and QC 4 can be applied to all predicted data.</p>
          <p>In the first step (QC 1), the performance of the object detector should be checked in detail. It may happen that the object detector fails to detect the individual in certain videos quite often, which could be due to different light conditions or maybe because of heavy truncation. Of course, it is also possible that the individuals are Out for a longer period. To check the performance, BOVIDS outputs an overview that reports the percentage of detections of an individual by the object detector per video. If this value turns out to be noticeably low, one should check the original data to see if this low “detection density” can be explained.</p>
          <p>Second, if the object detector works satisfactorily well and a good set of postprocessing rules was defined, the performance of the classification part of BOVIDS needs to be analyzed. Accuracy and f‐score (QC 2) are standard tools to measure the performance of a deep learning system. Those metrics are applied with respect to the postprocessed data in comparison to the manually annotated data to which the postprocessing rules were also applied. Further highly relevant biological quantities are the percentage per behavioral class and the median phase length where the latter is not evaluated appropriately by accuracy and f‐score. Finally, it is important to understand which kind of misclassifications occur and to, potentially, derive patterns. To analyze QC 2 and QC 3, BOVIDS contains a tool that allows to report the accuracy, f‐score, deviation in the number of phases as well as a detailed description of misclassified sequences.</p>
          <p>If QC 1–QC 3 are satisfactorily met, BOVIDS can be used to generate a final prediction of the unlabeled videos. Of course, QC 1 should be applied to unlabeled videos as well as it is a good indicator whether the object detector works well on a specific video. But even if the object detector detects an object quite frequently, it might happen that BOVIDS provides poor quality on a specific night if there are problems in the original data: for instance, individuals could be heavily truncated on a specific night. In those cases, it is reasonable to assume that the activity budget of the individual looks significantly different as in other videos which can be checked rather easily visually by searching for such outliers (QC 4). To this end, a short graphical representation of the activity budget in a video is generated by BOVIDS (see Figure <xref rid="ece38701-fig-0005" ref-type="fig">5</xref>) which can be used to identify those outliers. If the graphical representation of a night is conspicuous, one can check the original data on a sample basis to investigate BOVIDS’ performance.</p>
          <fig position="float" fig-type="FIGURE" id="ece38701-fig-0005">
            <label>FIGURE 5</label>
            <caption>
              <p>Example of one night of one common eland with the plotted phases of the three behavioral states of the total system given by BOVIDS to look for quality criteria QC 4</p>
            </caption>
            <graphic xlink:href="ECE3-12-e8701-g001" position="anchor" id="jats-graphic-11"/>
          </fig>
        </sec>
        <sec id="ece38701-sec-0017">
          <title>DP 3: Data presentation</title>
          <p>BOVIDS provides further functionalities to present the produced data elegantly which will be briefly described in this section and shown in more detail with the data of the case study in the results’ section. Next to the graphical representation (see QC 4) of each night, BOVIDS produces a document that contains an overview of the most important statistical key quantities, for instance, the percentages of the single behaviors in the activity budget. Finally, BOVIDS can be used to generate an overview about an individual's behavior over all evaluated nights or even about a species’ average behavior over all nights of all individuals. Furthermore, first graphical representations of the nightly activity are given as can be seen in Figure <xref rid="ece38701-fig-0006" ref-type="fig">6</xref>.</p>
          <fig position="float" fig-type="FIGURE" id="ece38701-fig-0006">
            <label>FIGURE 6</label>
            <caption>
              <p>Timeline containing the percentage of all behavioral states and their means over all nights of all analyzed individuals of common elands. The visualization is smoothed by a rolling average over 3 min. (a) is the binary classification and contains 822 nights of 25 individuals, and (b) is the total classification containing 589 nights of 16 individuals</p>
            </caption>
            <graphic xlink:href="ECE3-12-e8701-g003" position="anchor" id="jats-graphic-13"/>
          </fig>
        </sec>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="ece38701-sec-0018">
    <label>3</label>
    <title>RESULTS</title>
    <sec id="ece38701-sec-0019">
      <label>3.1</label>
      <title>BOVIDS' performance in the case study</title>
      <p>This section is devoted to reporting the validity of postprocessing rules and the quality criteria QC 1–QC 4 in the case study. Subsequently, in the next section, the nocturnal behavior of the common elands is presented.</p>
      <p>A set of postprocessing rules can be considered as valid if the systematic error induced by these rules is negligible for the quantities of interest. In the dataset at hand and in both classification tasks, the accuracy of the postprocessed data ranges from 99.6% to 100% and even the f‐score of all activity classes lies constantly over 99.2%. Accordingly, the percentage per night per individual of all behavioral classes under both classification tasks are approximated up to an error of 0.02% in the worst case. Moreover, the average median phase length per individual is overshot by 21s of 1796s (Standing), 34s of 1375s (LHU) and 24s of 322s (LHD) in the total classification task while those values are 130s of 1834s (Standing) and 239s of 4226s (Lying) under binary classification. The number of phases per activity class is underestimated, more precisely, the mean deviation over all individuals is −0.29 of 8.2 (Standing), −1.02 of 23.0 (LHU), and −0.67 of 14.6 (LHD) in the total classification task while it is −1.4 of 8.9 (Standing) and −0.9 of 8.5 (Lying) in the binary classification system.</p>
      <p>To analyze the quality criteria, the predictions of BOVIDS are compared to the manually annotated and postprocessed nights. All nights in which individuals were at least 20% of the time Out, either by BOVIDS’ prediction, or, if manually annotated by the humans’ prediction, were dismissed as such nights do not yield good evidence on the individual's activity budget. The results of all quality criteria are presented in this section.</p>
      <p>On the analysis of the accuracy (QC 2) of BOVIDS’ prediction with respect to the manually annotated postprocessed data, the following results are found. The median accuracy per night lies at 99.4% with a 0.25‐quantile of 99.1% and a 0.75‐quantile of 99.4% in the total classification task. Furthermore, the median f‐scores turn out to be 99.6% (Standing), 99.5% (LHU), and 96.3% (LHD) with minima 94.4% (Standing), 95.4% (LHU), and 93.2% (LHD). In the binary classification task, the corresponding values read as follows. The median accuracy is 99.8% with a 0.25‐quantile of 99.4% and a 0.75‐quantile of 99.8% while the f‐scores are at least 93.1% (Standing) and 97.1% (Lying) with a median of 99.5% and 99.8%. Furthermore, the percentage of each behavioral class per individual is approximated up to at most 0.03% in both classification tasks. In the total classification system, the mean deviation in the number of phases is 0.34 of 7.9 (Standing), 0.53 of 22.0 (LHU), and 0.37 of 13.9 (LHD). The values in the binary classification task are 0.05 of 7.5 (Standing) and 0.03 of 7.6 (Lying). Finally, the median phase length per individual is underestimated by −22.6s of 1817.6s (Standing), by −117.0s of 1409.9s (LHU), and −1.8s of 345.6s (LHD) in the total classification task. In the binary classification system, those values turn out to be −2.87s of 1970.9s (Standing) and −14.7s of 4464.5s (Lying).</p>
      <p>The next quality criteria to analyze is the number, length, and type of misclassified sequences (QC3). In the total classification task, we find, overall, 179 misclassified sequences in 62 nights (thus, on average, 2.9 sequences per night). Out of 179 sequences, 49 sequences are misclassifications between a real behavior and being Out and in 65 cases, BOVIDS predicted LHD while the actual behavior was LHU. The remaining 65 sequences were mostly short confusions between Standing and LHU. In contrast, in the binary classification task, there are 181 misclassified sequences in 170 nights (on average 1.1 sequences per night) out of which 78 are confusions between a behavioral class and Out, in 78 cases, BOVIDS predicts Standing while the human label is Lying and in 27 cases vice versa. Furthermore, out of the 181 sequences, 46 misclassifications are sequences of length at most 1 min and 47 additional misclassifications are below 5 min.</p>
      <p>Quality criteria QC 1 and QC 4 are with respect to all predicted nights. Hereby, QC 1 checks the performance of the object detector. The detection density per individual ranges from 87.2% to 100% while its median turns out to be 99.8% with a 0.25‐quantile of 97.5% and a 0.75‐quantile of 100%. To analyze the last quality criteria (QC 4), namely, to look for apparent outliers, BOVIDS creates one plot per predicted night (for the binary and for the total classification task, respectively) representing the timely course of the behavioral phases (see Figure <xref rid="ece38701-fig-0005" ref-type="fig">5</xref>). There are few apparent outliers on data which were not manually labeled, and the automatic annotation was checked randomly. In most cases, it was found that BOVIDS’ prediction is correct even if it seemed to be suspicious. The observed misclassifications during this step fit exactly into the description of the errors in QC 3 and the frequency is comparable.</p>
    </sec>
    <sec id="ece38701-sec-0020">
      <label>3.2</label>
      <title>The nocturnal behavior of common elands</title>
      <p>The data presentation tools of BOVIDS give a first visual result regarding the relative distribution of the behavioral states, their means over all nights, and the rhythm of phases of behavioral states (see Figure <xref rid="ece38701-fig-0006" ref-type="fig">6</xref>). The underlying data are normalized to the behavioral states excluding Out. The optically conjectured increase of Lying over the night between 19:00 and 06:00 in the binary classification task is confirmed by a linear regression (<italic toggle="yes">R</italic>
<sup>2</sup> = .799 and <italic toggle="yes">p</italic> &lt; .0001). In addition to the visual representation, BOVIDS’ output consists of tables, including a summary table for every individual containing relevant statistical key values as well as a list of the number of phases, durations, and the percentage of behaviors per night. This significantly facilitates the creation of an activity budget (see Figure <xref rid="ece38701-fig-0007" ref-type="fig">7</xref>) and provides a first insight into the nocturnal behavior of common elands. The graphical representation yields to the conjecture that there might be differences in the total duration of the behaviors per night between certain groups of individuals. Those differences are analyzed rigorously in the following.</p>
      <fig position="float" fig-type="FIGURE" id="ece38701-fig-0007">
        <label>FIGURE 7</label>
        <caption>
          <p>Activity budgets of all analyzed common elands: (a) is the binary classification with 822 nights of 25 individuals, and (b) is the total classification with 589 nights of 16 individuals. <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_01</italic> to <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_05</italic> are male adult individuals and <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_06</italic> to <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_17</italic> are female adult individuals, while <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_18</italic> to <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_21</italic> are subadults and <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_22</italic> to <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_25</italic> are young individuals</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8701-g006" position="anchor" id="jats-graphic-15"/>
      </fig>
      <p>The data with respect to Standing and LHU can be assumed to be normally distributed (p_Standing = 0.9524 and p_LHU = 0.2715) while the total duration per night of LHD deviates significantly from normality (p_LHD = 0.0015) and is transformed. First, adult male and adult female individuals are compared to investigate sex differences. Afterwards, age‐specific analyses’ will be conducted within the group of female individuals as there is only one nonadult male individual in the sample. To investigate the differences based on sex and to account for possible influences by the housing conditions, a two‐factor ANOVA is conducted with the factors keeping zoo and sex between the adult animals for each behavior of the total classification system (<italic toggle="yes">n</italic> = 9 individuals with 328 nights consisting of 4 males with 151 nights and 5 females with 177 nights). The holding zoo can be withdrawn as a significant factor (<italic toggle="yes">p</italic> &gt; .37), but the sex has a significant influence on LHD (<italic toggle="yes">p</italic> = .0281), whereby the males’ values exceed the females’, see Figure <xref rid="ece38701-fig-0008" ref-type="fig">8</xref>(a). Finally, a two‐factor ANOVA with factors keeping zoo and age within all female individuals in the total classification system (n = 11 individuals with 411 nights consisting of 3 young with 118, 3 subadults with 116 and 5 adults with 177 nights) is conducted. Again, the holding zoo can be withdrawn as a factor (<italic toggle="yes">p</italic> &gt; .58), but the age influences the total duration of Standing (p_young‐adult = 0.0038) and LHD (p_young‐adult = 0.0009; p_subadult‐adult = 0.0136) significantly as a corresponding post hoc analysis verifies. Hereby, nonadult individuals spend more time on LHD than adult ones, whereby adult ones spend more time Standing, see Figure <xref rid="ece38701-fig-0008" ref-type="fig">8</xref>(b). While the age comparison could only be carried out for female individuals, it is an advantageous circumstance that one individual could be recorded once as the subadult male individual (<italic toggle="yes">T</italic>.<italic toggle="yes">oryx_18</italic>) and moved during the observation phase to a different zoo in which it was observed as an adult male (<italic toggle="yes">T</italic>.<italic toggle="yes">oryx_01</italic>). This allows for a direct comparison of the behavior between the subadult and adult age of this individual as the husbandry conditions in the zoos studied were already considered negligible. An unpaired t‐test shows that the total amount of Standing (<italic toggle="yes">p</italic> &lt; .0001) and LHD (<italic toggle="yes">p</italic> = .0001) differs significantly between the two observation periods of this individual, confirming the previously found results in differences due to age.</p>
      <fig position="float" fig-type="FIGURE" id="ece38701-fig-0008">
        <label>FIGURE 8</label>
        <caption>
          <p>Comparison with respect to the total duration of each behavior per night in the total system. (a) Sex comparison (with <italic toggle="yes">n</italic> = 9 individuals with 328 nights, consisting of 4 males with 151 nights and 5 females with 177 nights) in which significant differences in LHD (<italic toggle="yes">p</italic> = .0281) arise. (b) Age comparison with (<italic toggle="yes">n</italic> = 11 individuals with 411 nights, consisting of 3 young individuals with 118, 3 subadults with 116 and 5 adults with 177 nights) that yields significant differences in Standing (p_young‐adult = .0038) and LHD (p_young‐adult = .0009; p_subadult‐adult = .0136)</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8701-g011" position="anchor" id="jats-graphic-17"/>
      </fig>
      <p>A second variable of interest is the length of each behavioral phase. Regarding this quantity, the binary classification system (Standing and Lying) was used for the analysis as well as the duration of LHD from the total classification system as one Lying phase might be interrupted by several events of LHD. A Wilcoxon test reveals that there are significant differences (<italic toggle="yes">p</italic> = .0003) in the median length of phases per individual within Lying between males and females (<italic toggle="yes">n</italic> = 17 individuals with 539, consisting of 5 males with 179 nights and 12 females with 360 nights). For this reason, these two groups were analyzed separately. Within the females (<italic toggle="yes">n</italic> = 19 individuals with 613 nights, consisting of 4 young with 137 nights, 3 subadults with 116 and 12 adults with 360 nights), a post hoc analysis shows significant differences in the median duration of the Standing phases between young and adult individuals (p_Standing = 0.0033) and no significant differences between young and subadult animals (p_Standing = 0.1143, p_Lying = 0.629). Therefore, a detailed analysis is made after splitting into three categories, adult male, adult female, and nonadult (young and subadult) individuals. Figure <xref rid="ece38701-fig-0009" ref-type="fig">9</xref> visualizes the distribution of the phase length regarding these categories. In median, the adult males exhibit the longest Lying phases with 89.6 min, followed by the nonadult animals (78.5 min) while the females show, with 59.3 min, the shortest Lying phases. While this is also true for the first and third quartile, the longest Lying event is achieved by the nonadults with 369.7 min. Within Standing, nonadult individuals seem to show a shorter median phase length (21.2 min) than adults (35.5 female, 30.8 male). With respect to phases of LHD, adult male individuals and nonadult individuals show, with a median value of 4.6 min and, respectively, 4.4 min a slightly longer duration than adult females with a median of 3.7 min. Nevertheless, the longest observed phase of LHD was by nonadult individuals (47.8 min) followed by the male adults (32.9 min) and the female adults (14.8 min).</p>
      <fig position="float" fig-type="FIGURE" id="ece38701-fig-0009">
        <label>FIGURE 9</label>
        <caption>
          <p>(a) For all 25 common elands, the distribution of the length of phases is in minutes of Standing and Lying from the binary classification task plotted and the animals are classified as adult male (<italic toggle="yes">n</italic> = 5 individuals with 179 nights), adult female (<italic toggle="yes">n</italic> = 12 individuals with 360 nights) and nonadult animals (<italic toggle="yes">n</italic> = 8 with 280 nights). (b) Only the 16 common elands evaluated by the total classification system are used. The length of phases in minutes of LHD are plotted and the animals are classified as adult male (<italic toggle="yes">n</italic> = 4 individuals with 151 nights), adult female (<italic toggle="yes">n</italic> = 5 individuals with 177 nights), and nonadult animals (<italic toggle="yes">n</italic> = 7 individuals with 261 nights)</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8701-g009" position="anchor" id="jats-graphic-19"/>
      </fig>
      <p>Beside the length of the phases, the number of phases per night is also an interesting parameter. Figure <xref rid="ece38701-fig-0010" ref-type="fig">10</xref> visualizes the number of Lying phases (binary classification system) as well as the number of LHD phases (total classification system). Note that the number of Standing phases equals the number of Lying phases ±1. The above illustration highlights the different age categories of young, subadults, and adults, with sex being distinguished in the adult category. The phases in Lying (see Figure <xref rid="ece38701-fig-0010" ref-type="fig">10</xref>(a)) appear to be constant across individuals and differences between sex and age groups are not evident. The situation is different when it comes to LHD, where the young animals have a significantly higher number of phases than the adults. The subadults tend to have slightly more LHD phases than the adults, but they are already closer to the values of the adults than to those of the young.</p>
      <fig position="float" fig-type="FIGURE" id="ece38701-fig-0010">
        <label>FIGURE 10</label>
        <caption>
          <p>Number of phases for every individual marked are the groups, adult male, adult female, subadult, and young for (a) lying and (b) LHD</p>
        </caption>
        <graphic xlink:href="ECE3-12-e8701-g005" position="anchor" id="jats-graphic-21"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="ece38701-sec-0021">
    <label>4</label>
    <title>DISCUSSION</title>
    <sec id="ece38701-sec-0022">
      <label>4.1</label>
      <title>BOVIDS</title>
      <sec id="ece38701-sec-0023">
        <label>4.1.1</label>
        <title>Performance in the case study</title>
        <p>In this section, the validity of the postprocessing rules as well as the four quality criteria are discussed. As can be seen in section <italic toggle="yes">BOVIDS’ performance in the case study</italic>, only very few activity phases are dismissed on manually annotated nights when the selected postprocessing rules are applied. Furthermore, both the accuracy and the f‐scores are close to 100%, so that overall, the set of postprocessing rules seems to be valid from a computer science point of view. Furthermore, the percentage of each behavioral class is very well approximated in both classification tasks, so that no mentionable errors occur. Not very surprisingly, the postprocessed data contains few phases less and slightly longer median phase lengths as very short events are dismissed, so the postprocessing rules imply almost no bias in the real data. These values are of course a bit higher in the binary classification task, since longer phases up to five minutes are not considered. But firstly, even this choice does not imply much bias in the data, and secondly, the few short events of Standing and Lying do not significantly affect the animals’ rhythms. Of course, neglecting the short events also increases the median phase length. However, this happens only very moderately, by a factor of between 5.6% (Standing) and 7.5% (LHD). It will be seen later that the methodological error will underestimate those quantities with respect to the postprocessed data slightly. Therefore, the errors partly account for each other.</p>
        <p>The object detector seems to work very well (QC 1) as the median object detection density is very high. On nights with a lower detection density, the video material was checked manually, and it can be observed that the individuals were mostly Out if the object detector did not find them, or only small parts are visible at the border of the video recording.</p>
        <p>Subsequently, quality criteria QC 2 and QC 3 are discussed. Since the number of phases per activity class and the phase length analysis refer to Standing and Lying from the binary classification task as well as LHD from the total classification task, the discussion focuses on the reliability of these quantities. Overall, the accuracy and the f‐score of BOVIDS’ prediction are very high for machine learning‐based predictions. Recent studies on comparable hard data, such as that of Porto et al. (<xref rid="ece38701-bib-0045" ref-type="bibr">2013</xref>) on the discrimination of Standing and Lying behavior on video recordings of cows in stables, achieve an average accuracy of 92%. Our accuracies of 99.8% in the binary classification task and 99.4% in the total classification task clearly exceed this value. Furthermore, even the median f‐score of the highly underrepresented class LHD is, with 96.4%, astonishingly high for a deep learning system. These values directly show that the percentage of each behavioral class is predicted very accurately and that there is no methodological bias in the expected activity budget.</p>
        <p>Moreover, video action classifiers tend, normally, to so‐called classification flickering, thus very short phases of misclassifications which do not really influence the accuracy and the f‐score of the prediction system but have huge influence on the number of phases per activity. The postprocessing rules are meant to take care of this effect (Hahn‐Klimroth et al., <xref rid="ece38701-bib-0021" ref-type="bibr">2021</xref>). The results show that BOVIDS succeeds in underestimating or overestimating the number of phases per activity class only very slightly on average. More precisely, the number of LHD phases is overestimated by 2.7% on average and the number of Standing and Lying phases is only overestimated by less than 1%. The median phase length is approximated very accurately as well, as it is only underestimated by at most 0.5% on average. It can be noted that even in enclosures containing two different individuals, BOVIDS’ prediction does not become significantly worse. This has two reasons: First, and most importantly, the used object detector seems to be able to discriminate between two individuals very accurately. Secondly, the action classifier seems to be very robust against truncation effects when, for example, the bounding boxes of the two animals overlap.</p>
        <p>In summary, the activity budget per night is predicted without any bias, as expected, while the median phase length per activity class is overestimated due to postprocessing rules by a moderate factor of no more than 7.0%. Thus, the automatic prediction is very precise with respect to the postprocessed data. Furthermore, the overall accurate description of the three poses Standing, LHU, and LHD by BOVIDS can be seen in connection with the types of misclassifications occurring on the testing data. All misclassifications between Out and a real activity class are due to heavy truncation or occluding effects in which a human annotator might see hooves or small parts of the animal and is able to safely infer the behavior, but a machine cannot. In this case, it is favorable if the object detector does not find the animal in the first place. Furthermore, almost all misclassifications between LHU and LHD can be explained by the fact that common elands show, from time to time, a grooming behavior at their hind leg which is, on a single image, hard to distinguish from LHD. Such errors need to, of course, be considered and analyzed, but do not seem to be fixable by more training data or fine‐tuning the networks if the input data format does not change significantly. As mentioned earlier, the median phase length as well as the median number of phases per night are only slightly overestimated. In the binary classification task, there are some short misclassifications with respect to the postprocessed data less than five minutes in length. These errors are just delayed transitions between the behavioral states due to, for instance, the applied rolling average during postprocessing. Therefore, these misclassifications neither influence the number of phases of Standing and Lying nor the animal's rhythms, but only slightly change the total duration of a specific phase. Finally, there are few misclassifications that are, probably, unavoidable in a deep learning classification task. Of course, accuracy can, in principle, always be improved by additional rounds of example mining and fine‐tuning the action classifiers, but it is questionable whether an even higher median accuracy of 99.4% can be reached on a three‐class classification task.</p>
        <p>A natural question, of course, is how well the findings from the test series can be generalized to unseen data of the same enclosures. Recall that the action classifiers are, in the end, trained on a random collection of images over the whole observation time due to offline hard example mining. Therefore, the testing set can be considered an almost random sample which includes a few more difficult images as expected on a random balanced sample. Thus, the analysis of the performance on the manually annotated nights (the testing set) yields a very good approximation of the overall performance. This claim is also supported by the analysis of QC 4. The type and frequency of errors on randomly selected, nonmanually annotated nights were found to be comparable to those in the test set.</p>
        <p>Finally, even if BOVIDS makes a small number of mistakes that would not occur if a trained observer manually annotated the data, the very large dataset overcompensates those few errors. Another approach to generating a large dataset is to have different, probably untrained, human observers annotate a comparable number of nights. Apart from the much higher cost, it is supposed that the interobserver reliability might be worse than the reliability of BOVIDS. Overall, our findings show that BOVIDS performs very accurately in the case study and its predictions can be safely used to generate a large amount of annotated data, which would not have been easily possible without automation.</p>
      </sec>
      <sec id="ece38701-sec-0024">
        <label>4.1.2</label>
        <title>Challenges and limitations</title>
        <p>As for any deep learning‐based classifier, there are various challenges to overcome during fine‐tuning the underlying model. Even after extensive fine‐tuning, there will be cases in which the system fails. While the last paragraph already discussed that small errors are overcompensated by evaluation of much data, this section is devoted to exploring typical misclassifications that arise if BOVIDS is used.</p>
        <p>A major challenge is given by highly truncated sequences of video material. In many applications, it is not possible to install cameras in a way that allows recording of every edge of the enclosure. This can cause misclassifications during action‐classification. Indeed, if only small parts of an animal can be seen, like only its hoofs or its head, and the object detector draws a decent bounding box, it is even for trained humans hard or even impossible to classify the behavior. To overcome this issue, it is possible to classify bounding boxes that are close to the image's border by a deterministic rule. A natural choice might be Out but in special cases one might use information about the recorded enclosure to infer the behavior in the truncated area. In‐depth observation of own data is necessary to identify those regions of the enclosure in which severe truncation effects might occur and to define proper rules on how to deal with them.</p>
        <p>Another challenge arises if the animal is not present in a sequence of images. It is possible that an object in the enclosure like a trough might be falsely classified as an animal in this case. This issue can be addressed by more training steps of the object detector or by increasing the so‐called minimum confidence score: an object detector does not only suggest a bounding box and a class label but also returns a confidence score between zero and one. If a threshold of this value is defined near one, misclassifications are expected to be very rare, but the bounding boxes of animals are also more easily discarded. Finding a good threshold depends highly on the application and should, therefore, be tested.</p>
        <p>A third type of errors might occur in enclosures in which multiple individuals are stalled together as the object detector might swap the individual's labels. In this case, short sequences of the proposed behavior can be false because the wrong individual is observed. There is no direct way to overcome this issue. In the case study, the object detector was tested excessively and worked very decently. But it is crucial to test the object detector's performance in the described fashion (see OD 3b). In future, implementations, one could extend BOVIDS to track bounding boxes from frame to frame. But on the technical side, the changes between consecutive frames might be too severe on recordings with 1fps to apply classical tracking methods. One possibility to deal with this problem would be to increase the recording's quality. This might give a second improvement. For instance, one could record with a much higher framerate that allows to use modern deep learning techniques like recurrent neural networks to capture the temporal dimension of the behavioral states more precisely. This comes with two challenges that may not be forgotten. First, it would require significantly more memory space. Second, it would also increase the computational cost. The current implementation predicts one hour of video material in approximately 5 min on mediocre hardware (RTX 2060 GPU) which would be exceeded significantly if more frames per second would need to be evaluated. If many video files need to be predicted in large‐scale studies, this might be a limiting factor. It is moreover to emphasize that under the described classification tasks the accuracy achieved by BOVIDS is highly satisfactory and it is unlikely that it can be much further improved. Nevertheless, techniques that use more temporal information might be able to capture short phases of certain behaviors more reliably. Behaviors that cannot be identified on a single image, or, more precisely, on four consecutive frames, cannot be detected in the current version. In the case study, grooming events at the hind legs (LHU) were sometimes predicted as LHD because the poses are close to each other. While normally misclassifications can be reduced by more rounds of offline hard example mining, it is presumably not possible to distinguish short grooming events and LHD within the given system. In the case study, these events were rare and therefore tolerable, but such analyses need to be conducted if the system should be applied to new data. During manual checking of samples, even trained humans were not able to reliably distinguish between those events and LHD on the given data. Of course, if the raw video material is used, this task is much easier, and one might hope to describe such events even more accurately using different architectures.</p>
      </sec>
      <sec id="ece38701-sec-0025">
        <label>4.1.3</label>
        <title>Universality and future directions</title>
        <p>A major strength of BOVIDS might be its adjustability to different settings. If the three positions Standing, LHU, and LHD need to be detected from video files, the system can be used on data of ungulates. Furthermore, in principle, any pose that is reliably detectable on single images can be predicted by the discussed deep learning framework. BOVIDS is tested extensively on the data of common elands and other African bovids stemming from various zoo enclosures. It is, therefore, reasonable to assume that, given sufficient training material, its performance is equally high under varying conditions. For instance, it is likely to perform well in the observation of various ungulates of different sizes from multiple continents in zoo enclosures or the analysis of livestock's behavior in stables. Since the present data are recorded in very different enclosures with partly high degrees of truncation and background noise, BOVIDS might perform well in outdoor enclosures as well if the camera installment is flawlessly possible in the sense that the whole outdoor enclosure can be recorded which would extend the set of research questions that can be tackled with this technique.</p>
        <p>A further research direction would be the analysis of BOVIDS’ performance on data of larger groups of ungulates. While technically the detection of individuals works the same, it is clearly a much more difficult task to distinguish many individuals from each other than it is to identify two individuals reliably. It might be tempting to extend BOVIDS’ functionality in cases in which reliable distinguishing between different individuals is not possible. This might be due to the number of individuals and their optical similarity. For instance, if individual detection fails in large groups, one could implement a scan‐sampling method that allows to at least report an average behavior of all the individuals.</p>
        <p>Moreover, the object detection phase can be used to identify different behavioral classes. If during a phase of Standing the bounding box's positions exhibit strong variability, this is a good indication for movement of the animal. Furthermore, it is possible to describe the individual's favorite positions within its enclosure and to keep track of the probability of the presence of the individuals at different spatial positions which can help to improve housing conditions in zoos. Both extensions suffer one technical challenge. Normally, one camera records an enclosure and, therefore, one can only work with a two‐dimensional projection of the actual positions. Depending on the camera positioning, movements into certain directions cannot be captured correctly. The same challenge applies to the description of the probability of the presence at spatial positions. If due to the camera's angle the bounding boxes are quite large in comparison to the whole image, such a description becomes meaningless. But overall, we believe that in many enclosures this approach can be implemented within the current deep learning system and can deliver more information on ungulate's behavior.</p>
        <p>Furthermore, it is to discuss whether the iterative process used to create a reasonable training set could be improved. The degree of automation of the system at hand resembles more the one of a “machine‐assisted” evaluation of video material than the one of an autonomous deep learning system. Such iterative processes to obtain reliable machine learning models is extensively studied in a recent publication of Miao et al. (<xref rid="ece38701-bib-0038" ref-type="bibr">2021</xref>) at the example of camera‐trap images. The findings of the aforementioned publication as well as the findings of the current paper indicate that such a partly automated system reduces the time required by a researcher to evaluate data dramatically.</p>
        <p>A similar question arises regarding the technical details of the training step of the action classifiers. To conquer data imbalance, the current contribution employs upsampling and downsampling techniques (Branco et al., <xref rid="ece38701-bib-0004" ref-type="bibr">2016</xref>) and achieves good results. Nevertheless, it is tempting to try different training procedures to deal with the imbalance, as recently suggested by Liu et al. (<xref rid="ece38701-bib-0030" ref-type="bibr">2019</xref>).</p>
        <p>Finally, it was already discussed that the deep learning architectures yolov4 and EfficientNet‐B3 are used because they are fast and show state‐of‐the‐art performance on testing sets. In principle, those architectures can be easily replaced if a novel approach performs even better. It is important to emphasize that the technical main contribution of BOVIDS is the sequential application of an object detector and a pair of action classifiers that capture the spatial and temporal dimension of the video data in the described fashion. The explicit implementation of these classifiers is independent from this approach and, therefore, it might be tempting to conduct comparative studies regarding the performance of different recent deep learning architectures within the proposed system.</p>
      </sec>
    </sec>
    <sec id="ece38701-sec-0026">
      <label>4.2</label>
      <title>The nocturnal behavior of common elands</title>
      <p>A first finding is that independent from the factors age, sex, and keeping zoo, all individuals exhibit a higher percentage of Lying than Standing during the night. As the night progresses, the percentage of Lying increases significantly. This is in line to findings of similar studies on African elephants (<italic toggle="yes">Loxodonta africana</italic>), blue wildebeest (<italic toggle="yes">Connochaetes taurinus</italic>), or Arabian oryx (<italic toggle="yes">Oryx leucoryx</italic>), where the observed animals also show most of the sleeping behavior or inactivity in the second part of the night (Davimes et al., <xref rid="ece38701-bib-0011" ref-type="bibr">2018</xref>; Gravett et al., <xref rid="ece38701-bib-0018" ref-type="bibr">2017</xref>; Malungo et al., <xref rid="ece38701-bib-0034" ref-type="bibr">2021</xref>).</p>
      <p>When considering the LHD, it should be noted that this posture most likely corresponds to the typical REM (rapid eye movement) sleep posture. As mentioned in the ethogram section, a behavioral component to recognize REM sleep is the head being down due to postural atonia (Lima et al., <xref rid="ece38701-bib-0029" ref-type="bibr">2005</xref>; Zepelin et al., <xref rid="ece38701-bib-0074" ref-type="bibr">2005</xref>). In this study, we use this characteristically REM sleep posture to determine REM sleep. This approach is in line with the study by Zizkova et al. (<xref rid="ece38701-bib-0075" ref-type="bibr">2013</xref>) on common elands and the study by Ternman et al. (<xref rid="ece38701-bib-0064" ref-type="bibr">2014</xref>) on cows, which shows that REM sleep can be detected with sufficient certainty based on behavioral surveys. This procedure is also supported by a study on lesser mouse‐deer (<italic toggle="yes">Tragulus kanchil</italic>), which shows that REM sleep can be divided into two categories, one of which is the most common, where the head lies down most of the time, making this a valid indicator to recognize REM sleep in behavioral studies (Lyamin et al., <xref rid="ece38701-bib-0033" ref-type="bibr">2021</xref>).</p>
      <p>Sex has been found to have an influence on the total amount of LHD during the night. The REM sleep periods of adult females last slightly longer than those of adult males, a fact which is also known across multiple phylogenetic states, for birds and mammals (Cajochen et al., <xref rid="ece38701-bib-0008" ref-type="bibr">2006</xref>; Rattenborg et al., <xref rid="ece38701-bib-0049" ref-type="bibr">2017</xref>; Steinmeyer et al., <xref rid="ece38701-bib-0059" ref-type="bibr">2010</xref>). However, other studies show that there are no sex differences when individuals are similar sized between the sexes, while dissimilar‐sized animals should have differences (Ruckstuhl &amp; Kokko, <xref rid="ece38701-bib-0050" ref-type="bibr">2002</xref>). In common elands, males are larger than females (Leslie, <xref rid="ece38701-bib-0028" ref-type="bibr">2011</xref>; Myers et al., <xref rid="ece38701-bib-0039" ref-type="bibr">2021</xref>), confirming the differences found between the sexes. In addition, Standing was found to increase with age. Interestingly, this finding is supported by the recording of a male individual at both subadult and adult age, which shows a significant increase in the total amount of Standing per night. Our results are in line with previous results on different mammals, as age is known to be an influencing factor for activity/rest cycles (Ruckstuhl &amp; Neuhaus, <xref rid="ece38701-bib-0051" ref-type="bibr">2009</xref>; Siegel, <xref rid="ece38701-bib-0058" ref-type="bibr">2005</xref>; Steinmeyer et al., <xref rid="ece38701-bib-0059" ref-type="bibr">2010</xref>). Moreover, age also influences REM sleep behavior in mammals and birds (Cajochen et al., <xref rid="ece38701-bib-0008" ref-type="bibr">2006</xref>; Rattenborg et al., <xref rid="ece38701-bib-0049" ref-type="bibr">2017</xref>; Ruckstuhl &amp; Kokko, <xref rid="ece38701-bib-0050" ref-type="bibr">2002</xref>; Steinmeyer et al., <xref rid="ece38701-bib-0059" ref-type="bibr">2010</xref>). This effect was also observed in the common elands in this study, where the extent of LHD differs between the three age classes—young, subadults, and adults. A study on Giraffes (<italic toggle="yes">Giraffa camelopardalis</italic>) also shows that age and sex have an influence on the behavior Standing, while only age has an influence on REM sleep (Burger et al., <xref rid="ece38701-bib-0007" ref-type="bibr">2021</xref>). The study by Burger et al. (<xref rid="ece38701-bib-0007" ref-type="bibr">2021</xref>) further reveals that housing conditions can be discarded as an influencing factor for both behaviors. These results correspond to the results in this study with common elands, where the keeping zoo and thus housing conditions can also be discarded as influencing factors. Of course, the factor housing condition consists of several factors such as, among others, enclosure size, and the presence or absence of other types of animals in the vicinity or lighting conditions. While the recorded data do not allow to evaluate each possibly influencing factor individually, our study reveals that the sum of those effects is negligible and can be discarded.</p>
      <p>Besides the total amount of time during the night, the duration of the single phases is also of interest. Here, the males differ from the females within Lying, whereby males show longer Lying phases than females. This fits with the result that adult males have a higher amount of LHD. Also, the age has an influence on the lengths of the phases. The nonadult animals exhibit shorter periods of Standing and longer periods of Lying than the adult ones. This also matches with the results regarding the nocturnal activity budgets. Within LHD the number of phases vary between the different categories of individuals. The mean phase length of LHD in all adult common elands is 9.5 min on average, with females slightly below this at 8.8 min and males slightly above at 10.2 min. These phase lengths are consistent with those of male Arabian oryx (<italic toggle="yes">Oryx leucoryx</italic>), which have a mean phase length of 7 ± 2 min in the dark in winter, and 10.5 ± 1.5 min over the 24‐h cycle (Davimes et al., <xref rid="ece38701-bib-0011" ref-type="bibr">2018</xref>).</p>
      <p>Finally, the number of phases is an interesting key figure in behavioral analysis. Within Lying and Standing it is noticeable that almost all animals show a very similar number of phases. Here, of the 25 animals observed, 23 have a median between 7 and 9 phases per night with quite a little variation per individual. The other two animals are moderate outliers downward. In addition, the mean ranges between 6.6 and 9.1 within 22 individuals and within all individuals, the SEM is at most 0.36 indicating a constant behavior within the single individuals. This result suggests that certain rhythms are present and should be investigated in more detail in further analyses, because the course over the night also suggests certain rhythms. Within LHD, a different picture of the underlying distributions emerges. Here, the adult individuals show a lower proportion than the nonadult individuals, and within the nonadult individuals there are also strong differences between the young and the subadult individuals. Only a few exceptions are evident, which can be explained as follows. <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_22</italic> is clearly different from the veined young and is closer to the values of the subadult individuals. However, <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_22</italic> is also the oldest animal among the group of young ones. Furthermore, <italic toggle="yes">T</italic>.<italic toggle="yes">oryx_17</italic>, which is the oldest animal in the case study, has a higher median number of phases than the other adult animals, especially the female ones. Excluding these exceptions, young individuals have a median of 40–42 phases of LHD and subadults show 13–15 phases. In contrast, adult females have 7–9 phases of LHD and adult males 9–11 phases. This again indicates differences between the sexes and high similarities within each group of individuals. Again, it seems that certain rhythms are present depending on the sex and the age but being independent of the specific individual. This observation might be the starting point of a much more detailed analysis of rhythms in African ungulates’ behavior.</p>
    </sec>
  </sec>
  <sec sec-type="COI-statement" id="ece38701-sec-0028">
    <title>CONFLICT OF INTEREST</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec id="ece38701-sec-0029">
    <title>AUTHOR CONTRIBUTIONS</title>
    <p><bold>Jennifer Gübert:</bold> Conceptualization (lead); Data curation (lead); Formal analysis (lead); Methodology (supporting); Visualization (equal); Writing – original draft (equal). <bold>Max Hahn‐Klimroth:</bold> Formal analysis (supporting); Methodology (lead); Software (lead); Visualization (supporting); Writing – original draft (equal). <bold>Paul W. Dierkes:</bold> Funding acquisition (lead); Project administration (lead); Supervision (lead); Visualization (equal); Writing – original draft (supporting).</p>
  </sec>
</body>
<back>
  <ack id="ece38701-sec-0027">
    <title>ACKNOWLEDGMENTS</title>
    <p>We thank the Opel‐Zoo Foundation Professorship in Zoo Biology of the <italic toggle="yes">von Opel Hessische Zoostiftung</italic> who funded the research leading to the results. The authors gained great support from directors and curators of the participating zoos (in alphabetical order): Allwetterzoo Münster, Erlebnis‐Zoo Hannover, Opel‐Zoo Kronberg, Zoo Dortmund, Zoom Erlebniswelt Gelsenkirchen. Also, the animal keepers of these zoos promoted this study mainly by assisting and allowing the data collection and providing information about the animals. Isabel Seyrling and Franziska Zölzer assisted in the installation of the cameras. Finally, we thank the two unknown reviewers for their detailed reading of the manuscript and their valuable remarks that helped to improve the paper significantly. Open Access funding enabled and organized by Projekt DEAL.</p>
  </ack>
  <sec sec-type="data-availability" id="ece38701-sec-0031">
    <title>DATA AVAILABILITY STATEMENT</title>
    <p>The python code is available at GitHub: <ext-link xlink:href="https://github.com/Klimroth/BOVIDS" ext-link-type="uri" specific-use="software is-supplemented-by">https://github.com/Klimroth/BOVIDS</ext-link> and on Zenodo (<ext-link xlink:href="https://doi.org/10.5281/zenodo.6143896" ext-link-type="uri" specific-use="dataset is-supplemented-by">https://doi.org/10.5281/zenodo.6143896</ext-link>).</p>
  </sec>
  <ref-list content-type="cited-references" id="ece38701-bibl-0001">
    <title>REFERENCES</title>
    <ref id="ece38701-bib-0001">
      <mixed-citation publication-type="book" id="ece38701-cit-0001"><string-name><surname>Beery</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Rathod</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Votel</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Huang</surname>, <given-names>J.</given-names></string-name> (<year>2020</year>). <part-title>Context R‐CNN: Long term temporal context for per‐camera object detection</part-title>. In <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source> (pp. <fpage>13075</fpage>–<lpage>13085</lpage>).</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0002">
      <mixed-citation publication-type="journal" id="ece38701-cit-0002"><string-name><surname>Bennie</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Duffy</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Inger</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Gaston</surname>, <given-names>K. J.</given-names></string-name> (<year>2014</year>). <article-title>Biogeography of time partitioning in mammals</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>111</volume>, <fpage>13727</fpage>–<lpage>13732</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1216063110</pub-id>
<pub-id pub-id-type="pmid">25225371</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0003">
      <mixed-citation publication-type="book" id="ece38701-cit-0003"><string-name><surname>Bochkovskiy</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>C.‐Y.</given-names></string-name>, &amp; <string-name><surname>Liao</surname>, <given-names>H.‐Y.‐ M.</given-names></string-name> (<year>2020</year>). <source>YOLOv4: Optimal speed and accuracy of object detection</source>. arXiv [Preprint]. <ext-link xlink:href="https://arxiv.org/pdf/2004.10934" ext-link-type="uri">https://arxiv.org/pdf/2004.10934</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0004">
      <mixed-citation publication-type="journal" id="ece38701-cit-0004"><string-name><surname>Branco</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Torgo</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Ribeiro</surname>, <given-names>R. P.</given-names></string-name> (<year>2016</year>). <article-title>A survey of predictive modeling on imbalanced domains</article-title>. <source>ACM Computing Surveys</source>, <volume>49</volume>, <fpage>1</fpage>–<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1145/2907070</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0005">
      <mixed-citation publication-type="journal" id="ece38701-cit-0005"><string-name><surname>Brando</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Buchanan‐Smith</surname>, <given-names>H. M.</given-names></string-name> (<year>2018</year>). <article-title>The 24/7 approach to promoting optimal welfare for captive wild animals</article-title>. <source>Behavioural Processes</source>, <volume>156</volume>, <fpage>83</fpage>–<lpage>95</lpage>. <pub-id pub-id-type="doi">10.1016/j.beproc.2017.09.010</pub-id>
<pub-id pub-id-type="pmid">29113925</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0006">
      <mixed-citation publication-type="journal" id="ece38701-cit-0006"><string-name><surname>Burger</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Fennessy</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fennessy</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Dierkes</surname>, <given-names>P. W.</given-names></string-name> (<year>2020</year>). <article-title>Nightly selection of resting sites and group behavior reveal antipredator strategies in giraffe</article-title>. <source>Ecology and Evolution</source>, <volume>10</volume>, <fpage>2917</fpage>–<lpage>2927</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.6106</pub-id>
<pub-id pub-id-type="pmid">32211165</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0007">
      <mixed-citation publication-type="journal" id="ece38701-cit-0007"><string-name><surname>Burger</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Hartig</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Dierkes</surname>, <given-names>P. W.</given-names></string-name> (<year>2021</year>). <article-title>Biological and environmental factors as sources of variation in nocturnal behavior of giraffe</article-title>. <source>Zoo Biology</source>, <volume>40</volume>, <fpage>171</fpage>–<lpage>181</lpage>. <pub-id pub-id-type="doi">10.1002/zoo.21596</pub-id>
<pub-id pub-id-type="pmid">33666286</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0008">
      <mixed-citation publication-type="journal" id="ece38701-cit-0008"><string-name><surname>Cajochen</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Münch</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Knoblauch</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Blatter</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Wirz‐Justice</surname>, <given-names>A.</given-names></string-name> (<year>2006</year>). <article-title>Age‐related changes in the circadian and homeostatic regulation of human sleep</article-title>. <source>Chronobiology International</source>, <volume>23</volume>, <fpage>461</fpage>–<lpage>474</lpage>. <pub-id pub-id-type="doi">10.1080/07420520500545813</pub-id>
<pub-id pub-id-type="pmid">16687319</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0009">
      <mixed-citation publication-type="journal" id="ece38701-cit-0009"><string-name><surname>Chakravarty</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Cozzi</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Dejnabadi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Léziart</surname>, <given-names>P.‐A.</given-names></string-name>, <string-name><surname>Manser</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ozgul</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Aminian</surname>, <given-names>K.</given-names></string-name> (<year>2020</year>). <article-title>Seek and learn: Automated identification of microevents in animal behaviour using envelopes of acceleration data and machine learning</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>11</volume>, <fpage>1639</fpage>–<lpage>1651</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.13491</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0010">
      <mixed-citation publication-type="book" id="ece38701-cit-0010"><string-name><surname>Dai</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Le</surname>, <given-names>Q. V.</given-names></string-name>, &amp; <string-name><surname>Tan</surname>, <given-names>M.</given-names></string-name> (<year>2021</year>). <part-title>CoAtNet: Marrying convolution and attention for all data sizes</part-title>. In <source>Thirty‐Fifth conference on neural information processing systems</source>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0011">
      <mixed-citation publication-type="journal" id="ece38701-cit-0011"><string-name><surname>Davimes</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Alagaili</surname>, <given-names>A. N.</given-names></string-name>, <string-name><surname>Bhagwandin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bertelsen</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Mohammed</surname>, <given-names>O. B.</given-names></string-name>, <string-name><surname>Bennett</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Manger</surname>, <given-names>P. R.</given-names></string-name>, &amp; <string-name><surname>Gravett</surname>, <given-names>N.</given-names></string-name> (<year>2018</year>). <article-title>Seasonal variations in sleep of free‐ranging Arabian oryx (<italic toggle="yes">Oryx leucoryx</italic>) under natural hyperarid conditions</article-title>. <source>Sleep</source>, <volume>41</volume>, <pub-id pub-id-type="doi">10.1093/sleep/zsy038</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0012">
      <mixed-citation publication-type="journal" id="ece38701-cit-0012"><string-name><surname>Dell</surname>, <given-names>A. I.</given-names></string-name>, <string-name><surname>Bender</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Couzin</surname>, <given-names>I. D.</given-names></string-name>, <string-name><surname>de Polavieja</surname>, <given-names>G. G.</given-names></string-name>, <string-name><surname>Noldus</surname>, <given-names>L. P. J. J.</given-names></string-name>, <string-name><surname>Pérez‐Escudero</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Straw</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Wikelski</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Brose</surname>, <given-names>U.</given-names></string-name> (<year>2014</year>). <article-title>Automated image‐based tracking and its application in ecology</article-title>. <source>Trends in Ecology &amp; Evolution</source>, <volume>29</volume>, <fpage>417</fpage>–<lpage>428</lpage>. <pub-id pub-id-type="doi">10.1016/j.tree.2014.05.004</pub-id>
<pub-id pub-id-type="pmid">24908439</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0013">
      <mixed-citation publication-type="journal" id="ece38701-cit-0013"><string-name><surname>Eikelboom</surname>, <given-names>J. A. J.</given-names></string-name>, <string-name><surname>Wind</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>van de Ven</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kenana</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Schroder</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>de Knegt</surname>, <given-names>H. J.</given-names></string-name>, <string-name><surname>van Langevelde</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Prins</surname>, <given-names>H. H. T.</given-names></string-name> (<year>2019</year>). <article-title>Improving the precision and accuracy of animal population estimates with aerial image object detection</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>10</volume>, <fpage>1875</fpage>–<lpage>1887</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.13277</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0014">
      <mixed-citation publication-type="journal" id="ece38701-cit-0014"><string-name><surname>Felzenszwalb</surname>, <given-names>P. F.</given-names></string-name>, <string-name><surname>Girshick</surname>, <given-names>R. B.</given-names></string-name>, <string-name><surname>McAllester</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Ramanan</surname>, <given-names>D.</given-names></string-name> (<year>2010</year>). <article-title>Object detection with discriminatively trained part‐based models</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>32</volume>, <fpage>1627</fpage>–<lpage>1645</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2009.167</pub-id>
<pub-id pub-id-type="pmid">20634557</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0015">
      <mixed-citation publication-type="book" id="ece38701-cit-0015"><string-name><surname>Franche</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Coulombe</surname>, <given-names>S.</given-names></string-name> (<year>2012</year>). <part-title>A multi‐frame and multi‐slice H.264 parallel video encoding approach with simultaneous encoding of prediction frames</part-title>. In <source>Second International Conference on Consumer Electronics, Communications and Networks (CECNet)</source> (pp. <fpage>3034</fpage>–<lpage>3038</lpage>). <pub-id pub-id-type="doi">10.1109/CECNet.2012.6202018</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0016">
      <mixed-citation publication-type="journal" id="ece38701-cit-0016"><string-name><surname>Friard</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Gamba</surname>, <given-names>M.</given-names></string-name> (<year>2016</year>). <article-title>BORIS: A free, versatile open‐source event‐logging software for video/audio coding and live observations</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>7</volume>, <fpage>1325</fpage>–<lpage>1330</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.12584</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0017">
      <mixed-citation publication-type="journal" id="ece38701-cit-0017"><string-name><surname>Gerovichev</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sadeh</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Winter</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Bar‐Massada</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Keasar</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Keasar</surname>, <given-names>C.</given-names></string-name> (<year>2021</year>). <article-title>High throughput data acquisition and deep learning for insect ecoinformatics</article-title>. <source>Frontiers in Ecology and Evolution</source>, <volume>9</volume>, <fpage>309</fpage>. <pub-id pub-id-type="doi">10.3389/fevo.2021.600931</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0018">
      <mixed-citation publication-type="journal" id="ece38701-cit-0018"><string-name><surname>Gravett</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bhagwandin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutcliffe</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Landen</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Chase</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Lyamin</surname>, <given-names>O. I.</given-names></string-name>, <string-name><surname>Siegel</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Manger</surname>, <given-names>P. R.</given-names></string-name> (<year>2017</year>). <article-title>Inactivity/sleep in two wild free‐roaming African elephant matriarchs ‐ Does large body size make elephants the shortest mammalian sleepers?</article-title>
<source>PLoS One</source>, <volume>12</volume>, <elocation-id>e0171903</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pone.0171903</pub-id>
<pub-id pub-id-type="pmid">28249035</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0019">
      <mixed-citation publication-type="journal" id="ece38701-cit-0019"><string-name><surname>Graving</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Chae</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Naik</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Koger</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Costelloe</surname>, <given-names>B. R.</given-names></string-name>, &amp; <string-name><surname>Couzin</surname>, <given-names>I. D.</given-names></string-name> (<year>2019</year>). <article-title>DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning</article-title>. <source>Elife</source>, <volume>8</volume>, <fpage>1</fpage>–<lpage>42</lpage>. <pub-id pub-id-type="doi">10.7554/eLife.47994</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0020">
      <mixed-citation publication-type="book" id="ece38701-cit-0020"><string-name><surname>Groves</surname>, <given-names>C. P.</given-names></string-name>, &amp; <string-name><surname>Leslie</surname>, <given-names>D. M.</given-names><suffix>Jr</suffix></string-name>. (<year>2011</year>). <part-title>Family Bovidae (Hollow‐horned Ruminants)</part-title>. In <person-group person-group-type="editor"><string-name><given-names>D. E.</given-names><surname>Wilson</surname></string-name></person-group> &amp; <person-group person-group-type="editor"><string-name><given-names>R. A.</given-names><surname>Mittermeier</surname></string-name></person-group> (Eds.), <source>Handbook of the mammals of the World: Hoofed Mammals</source> (pp. <fpage>444</fpage>–<lpage>780</lpage>). <publisher-name>Lynx Edicions</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0021">
      <mixed-citation publication-type="journal" id="ece38701-cit-0021"><string-name><surname>Hahn‐Klimroth</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kapetanopoulos</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Gübert</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Dierkes</surname>, <given-names>P. W.</given-names></string-name> (<year>2021</year>). <article-title>Deep learning‐based pose estimation for African ungulates in zoos</article-title>. <source>Ecology and Evolution</source>, <volume>11</volume>, <fpage>6015</fpage>–<lpage>6032</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.7367</pub-id>
<pub-id pub-id-type="pmid">34141199</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0022">
      <mixed-citation publication-type="book" id="ece38701-cit-0022"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name> (<year>2016</year>). <part-title>Deep residual learning for image recognition</part-title>. In <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (pp. <fpage>770</fpage>–<lpage>778</lpage>). <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0023">
      <mixed-citation publication-type="journal" id="ece38701-cit-0023"><string-name><surname>Hollén</surname>, <given-names>L. I.</given-names></string-name>, &amp; <string-name><surname>Manser</surname>, <given-names>M. B.</given-names></string-name> (<year>2007</year>). <article-title>Persistence of alarm‐call behaviour in the absence of predators: A comparison between wild and captive‐born Meerkats (<italic toggle="yes">Suricata suricatta</italic>)</article-title>. <source>Ethology</source>, <volume>113</volume>, <fpage>1038</fpage>–<lpage>1047</lpage>. <pub-id pub-id-type="doi">10.1111/j.1439-0310.2007.01409.x</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0024">
      <mixed-citation publication-type="journal" id="ece38701-cit-0024"><string-name><surname>Japkowicz</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Stephen</surname>, <given-names>S.</given-names></string-name> (<year>2002</year>). <article-title>The class imbalance problem: A systematic study</article-title>. <source>Intelligent Data Analysis</source>, <volume>6</volume>, <fpage>429</fpage>–<lpage>449</lpage>. <pub-id pub-id-type="doi">10.5555/1293951.1293954</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0025">
      <mixed-citation publication-type="journal" id="ece38701-cit-0025"><string-name><surname>Ji</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>K.</given-names></string-name> (<year>2013</year>). <article-title>3D convolutional neural networks for human action recognition</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>35</volume>, <fpage>221</fpage>–<lpage>231</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2012.59</pub-id>
<pub-id pub-id-type="pmid">22392705</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0026">
      <mixed-citation publication-type="journal" id="ece38701-cit-0026"><string-name><surname>Jiao</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Feng</surname>, <given-names>Z.</given-names></string-name>, &amp; <string-name><surname>Qu</surname>, <given-names>R.</given-names></string-name> (<year>2019</year>). <article-title>A survey of deep learning‐based object detection</article-title>. <source>IEEE Access</source>, <volume>7</volume>, <fpage>128837</fpage>–<lpage>128868</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2939201</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0027">
      <mixed-citation publication-type="journal" id="ece38701-cit-0027"><string-name><surname>Kabra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Robie</surname>, <given-names>A. A.</given-names></string-name>, <string-name><surname>Rivera‐Alba</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Branson</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Branson</surname>, <given-names>K.</given-names></string-name> (<year>2013</year>). <article-title>JAABA: interactive machine learning for automatic annotation of animal behavior</article-title>. <source>Nature Methods</source>, <volume>10</volume>, <fpage>64</fpage>–<lpage>67</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2281</pub-id>
<pub-id pub-id-type="pmid">23202433</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0028">
      <mixed-citation publication-type="book" id="ece38701-cit-0028"><string-name><surname>Leslie</surname>, <given-names>D. M.</given-names><suffix>Jr</suffix></string-name>. (<year>2011</year>). <part-title>Family Bovidae (Hollow‐horned Ruminants): Species accounts of <italic toggle="yes">Taurotragus oryx</italic>
</part-title>. In <person-group person-group-type="editor"><string-name><given-names>D. E.</given-names><surname>Wilson</surname></string-name></person-group> &amp; <person-group person-group-type="editor"><string-name><given-names>R. A.</given-names><surname>Mittermeier</surname></string-name></person-group> (Eds.), <source>Handbook of the mammals of the world: Hoofed Mammals</source> (<fpage>617</fpage> pp.). <publisher-name>Lynx Edicions</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0029">
      <mixed-citation publication-type="journal" id="ece38701-cit-0029"><string-name><surname>Lima</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Rattenborg</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Lesku</surname>, <given-names>J. A.</given-names></string-name>, &amp; <string-name><surname>Amlaner</surname>, <given-names>C. J.</given-names></string-name> (<year>2005</year>). <article-title>Sleeping under the risk of predation</article-title>. <source>Animal Behaviour</source>, <volume>70</volume>, <fpage>723</fpage>–<lpage>736</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2005.01.008</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0030">
      <mixed-citation publication-type="book" id="ece38701-cit-0030"><string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Miao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Zhan</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gong</surname>, <given-names>B.</given-names></string-name>, &amp; <string-name><surname>Yu</surname>, <given-names>S. X.</given-names></string-name> (<year>2019</year>). <part-title>Large‐scale long‐tailed recognition in an open world</part-title>. In <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>, (pp. <fpage>2532</fpage>–<lpage>2541</lpage>). <pub-id pub-id-type="doi">10.1109/CVPR.2019.00264</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0031">
      <mixed-citation publication-type="journal" id="ece38701-cit-0031"><string-name><surname>Lu</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Weng</surname>, <given-names>Q.</given-names></string-name> (<year>2007</year>). <article-title>A survey of image classification methods and techniques for improving classification performance</article-title>. <source>International Journal of Remote Sensing</source>, <volume>28</volume>, <fpage>823</fpage>–<lpage>870</lpage>. <pub-id pub-id-type="doi">10.1080/01431160600746456</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0032">
      <mixed-citation publication-type="journal" id="ece38701-cit-0032"><string-name><surname>Lürig</surname>, <given-names>M. D.</given-names></string-name>, <string-name><surname>Donoughe</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Svensson</surname>, <given-names>E. I.</given-names></string-name>, <string-name><surname>Porto</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Tsuboi</surname>, <given-names>M.</given-names></string-name> (<year>2021</year>). <article-title>Computer vision, machine learning, and the promise of phenomics in ecology and evolutionary biology</article-title>. <source>Frontiers in Ecology and Evolution</source>, <volume>9</volume>, <fpage>148</fpage>. <pub-id pub-id-type="doi">10.3389/fevo.2021.642774</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0033">
      <mixed-citation publication-type="journal" id="ece38701-cit-0033"><string-name><surname>Lyamin</surname>, <given-names>O. I.</given-names></string-name>, <string-name><surname>Siegel</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Nazarenko</surname>, <given-names>E. A.</given-names></string-name>, &amp; <string-name><surname>Rozhnov</surname>, <given-names>V. V.</given-names></string-name> (<year>2021</year>). <article-title>Sleep in the lesser mouse‐deer (<italic toggle="yes">Tragulus kanchil</italic>)</article-title>. <source>Sleep</source>, <fpage>zsab199</fpage>. <pub-id pub-id-type="doi">10.1093/sleep/zsab199</pub-id>
<pub-id pub-id-type="pmid">34370021</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0034">
      <mixed-citation publication-type="journal" id="ece38701-cit-0034"><string-name><surname>Malungo</surname>, <given-names>I. B.</given-names></string-name>, <string-name><surname>Gravett</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bhagwandin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Davimes</surname>, <given-names>J. G.</given-names></string-name>, &amp; <string-name><surname>Manger</surname>, <given-names>P. R.</given-names></string-name> (<year>2021</year>). <article-title>Sleep in two free‐roaming blue wildebeest (<italic toggle="yes">Connochaetes taurinus</italic>), with observations on the agreement of polysomnographic and actigraphic techniques</article-title>. <source>IBRO Neuroscience Reports</source>, <volume>10</volume>, <fpage>142</fpage>–<lpage>152</lpage>. <pub-id pub-id-type="doi">10.1016/j.ibneur.2021.02.005</pub-id>
<pub-id pub-id-type="pmid">34179868</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0035">
      <mixed-citation publication-type="book" id="ece38701-cit-0035"><string-name><surname>Martin</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Bateson</surname>, <given-names>P. P. G.</given-names></string-name> (<year>2015</year>). <source>Measuring behaviour: An introductory guide</source>. <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0036">
      <mixed-citation publication-type="journal" id="ece38701-cit-0036"><string-name><surname>Melfi</surname>, <given-names>V. A.</given-names></string-name>, &amp; <string-name><surname>Feistner</surname>, <given-names>A. T.</given-names></string-name> (<year>2002</year>). <article-title>A comparison of the activity budgets of wild and captive Sulawesi crested black macaques (<italic toggle="yes">Macaca nigra</italic>)</article-title>. <source>Animal Welfare (South Mimms, England)</source>, <volume>11</volume>, <fpage>213</fpage>–<lpage>222</lpage>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0037">
      <mixed-citation publication-type="journal" id="ece38701-cit-0037"><string-name><surname>Merrow</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Spoelstra</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Roenneberg</surname>, <given-names>T.</given-names></string-name> (<year>2005</year>). <article-title>The circadian cycle: Daily rhythms from behaviour to genes</article-title>. <source>EMBO Reports</source>, <volume>6</volume>, <fpage>930</fpage>–<lpage>935</lpage>. <pub-id pub-id-type="doi">10.1038/sj.embor.7400541</pub-id>
<pub-id pub-id-type="pmid">16222241</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0038">
      <mixed-citation publication-type="journal" id="ece38701-cit-0038"><string-name><surname>Miao</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Gaynor</surname>, <given-names>K. M.</given-names></string-name>, <string-name><surname>Palmer</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>S. X.</given-names></string-name>, &amp; <string-name><surname>Getz</surname>, <given-names>W. M.</given-names></string-name> (<year>2021</year>). <article-title>Iterative human and automated identification of wildlife images</article-title>. <source>Nature Machine Intelligence</source>, <volume>3</volume>, <fpage>885</fpage>–<lpage>895</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-021-00393-0</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0039">
      <mixed-citation publication-type="book" id="ece38701-cit-0039"><string-name><surname>Myers</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Espinosa</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Parr</surname>, <given-names>C. S.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hammond</surname>, <given-names>G. S.</given-names></string-name>, &amp; <string-name><surname>Dewey</surname>, <given-names>T. A.</given-names></string-name> (<year>2021</year>). <source>The animal diversity web</source>. <ext-link xlink:href="https://animaldiversity.org" ext-link-type="uri">https://animaldiversity.org</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0040">
      <mixed-citation publication-type="journal" id="ece38701-cit-0040"><string-name><surname>Norouzzadeh</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Morris</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Beery</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Joshi</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Jojic</surname>, <given-names>N.</given-names></string-name>, &amp; <string-name><surname>Clune</surname>, <given-names>J.</given-names></string-name> (<year>2021</year>). <article-title>A deep active learning system for species identification and counting in camera trap images</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>12</volume>, <fpage>150</fpage>–<lpage>161</lpage>. <pub-id pub-id-type="doi">10.1111/2041-210X.13504</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0041">
      <mixed-citation publication-type="journal" id="ece38701-cit-0041"><string-name><surname>Norouzzadeh</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kosmala</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Swanson</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Palmer</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Packer</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Clune</surname>, <given-names>J.</given-names></string-name> (<year>2018</year>). <article-title>Automatically identifying, counting, and describing wild animals in camera‐trap images with deep learning</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>115</volume>, <fpage>E5716</fpage>–<lpage>E5725</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1719367115</pub-id>
<pub-id pub-id-type="pmid">29871948</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0042">
      <mixed-citation publication-type="book" id="ece38701-cit-0042"><string-name><surname>Ouchra</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Belangour</surname>, <given-names>A.</given-names></string-name> (<year>2021</year>). <part-title>Object detection approaches in images: A survey</part-title>. In <source>Thirteenth International Conference on Digital Image Processing</source> (pp. <fpage>132</fpage>–<lpage>141</lpage>). <pub-id pub-id-type="doi">10.1117/12.2601452</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0043">
      <mixed-citation publication-type="journal" id="ece38701-cit-0043"><string-name><surname>Pereira</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Tabris</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Ravindranath</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Papadoyannis</surname>, <given-names>E. S.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>Z. Y.</given-names></string-name>, <string-name><surname>Turner</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>McKenzie‐Smith</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Kocher</surname>, <given-names>S. D.</given-names></string-name>, <string-name><surname>Falkner</surname>, <given-names>A. L.</given-names></string-name>, <string-name><surname>Shaevitz</surname>, <given-names>J. W.</given-names></string-name>, &amp; <string-name><surname>Murthy</surname>, <given-names>M.</given-names></string-name> (<year>2020</year>). <article-title>SLEAP: Multi‐animal pose tracking</article-title>. <source>Biorxiv</source> [Preprint]. <ext-link xlink:href="https://www.biorxiv.org/content/" ext-link-type="uri">https://www.biorxiv.org/content/</ext-link>
<pub-id pub-id-type="doi">10.1101/2020.08.31.276246v1</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0044">
      <mixed-citation publication-type="journal" id="ece38701-cit-0044"><string-name><surname>Peterson</surname>, <given-names>R. A.</given-names></string-name>, &amp; <string-name><surname>Cavanaugh</surname>, <given-names>J. E.</given-names></string-name> (<year>2020</year>). <article-title>Ordered quantile normalization: A semiparametric transformation built for the cross‐validation era</article-title>. <source>Journal of Applied Statistics</source>, <volume>47</volume>, <fpage>2312</fpage>–<lpage>2327</lpage>. <pub-id pub-id-type="doi">10.1080/02664763.2019.1630372</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0045">
      <mixed-citation publication-type="journal" id="ece38701-cit-0045"><string-name><surname>Porto</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Arcidiacono</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Anguzza</surname>, <given-names>U.</given-names></string-name>, &amp; <string-name><surname>Cascone</surname>, <given-names>G.</given-names></string-name> (<year>2013</year>). <article-title>A computer vision‐based system for the automatic detection of lying behaviour of dairy cows in free‐stall barns</article-title>. <source>Biosystems Engineering</source>, <volume>115</volume>, <fpage>184</fpage>–<lpage>194</lpage>. <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2013.03.002</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0046">
      <mixed-citation publication-type="book" id="ece38701-cit-0046"><string-name><surname>Puschmann</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Zscheile</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Zscheile</surname>, <given-names>K.</given-names></string-name> (<year>2009</year>). <source><italic toggle="yes">Säugetiere: Zootierhaltung</italic>. Tiere in menschlicher Obhut</source>. <publisher-name>Harri Deutsch</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0047">
      <mixed-citation publication-type="book" id="ece38701-cit-0047"><string-name><surname>Quiñonero‐Candela</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sugiyama</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schwaighofer</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Lawrence</surname>, <given-names>N. D.</given-names></string-name> (Eds.) (<year>2008</year>). <source>Dataset shift in machine learning</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0048">
      <mixed-citation publication-type="book" id="ece38701-cit-0048"><collab collab-type="authors">R Core Team</collab>
. (<year>2014</year>). <source>R: A language and environment for statistical computing</source>. <publisher-name>R Foundation for Statistical</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0049">
      <mixed-citation publication-type="journal" id="ece38701-cit-0049"><string-name><surname>Rattenborg</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>de La Iglesia</surname>, <given-names>H. O.</given-names></string-name>, <string-name><surname>Kempenaers</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Lesku</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Meerlo</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Scriba</surname>, <given-names>M. F.</given-names></string-name> (<year>2017</year>). <article-title>Sleep research goes wild: new methods and approaches to investigate the ecology, evolution and functions of sleep</article-title>. <source>Philosophical Transactions of the Royal Society of London B Biological Sciences</source>, <volume>372</volume>, <fpage>20160251</fpage>. <pub-id pub-id-type="doi">10.1098/rstb.2016.0251</pub-id>
<pub-id pub-id-type="pmid">28993495</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0050">
      <mixed-citation publication-type="journal" id="ece38701-cit-0050"><string-name><surname>Ruckstuhl</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Kokko</surname>, <given-names>H.</given-names></string-name> (<year>2002</year>). <article-title>Modelling sexual segregation in ungulates: effects of group size, activity budgets and synchrony</article-title>. <source>Animal Behaviour</source>, <volume>64</volume>, <fpage>909</fpage>–<lpage>914</lpage>. <pub-id pub-id-type="doi">10.1006/anbe.2002.2015</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0051">
      <mixed-citation publication-type="journal" id="ece38701-cit-0051"><string-name><surname>Ruckstuhl</surname>, <given-names>K. E.</given-names></string-name>, &amp; <string-name><surname>Neuhaus</surname>, <given-names>P.</given-names></string-name> (<year>2009</year>). <article-title>Activity budgets and sociality in a monomorphic ungulate: The African oryx (<italic toggle="yes">Oryx gazella</italic>)</article-title>. <source>Canadian Journal of Zoology</source>, <volume>87</volume>, <fpage>165</fpage>–<lpage>174</lpage>. <pub-id pub-id-type="doi">10.1139/Z08-148</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0052">
      <mixed-citation publication-type="book" id="ece38701-cit-0052"><string-name><surname>Russell</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Norvig</surname>, <given-names>P.</given-names></string-name> (<year>2016</year>). <source>Artificial intelligence: A modern approach</source>. <publisher-name>Pearson</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0053">
      <mixed-citation publication-type="journal" id="ece38701-cit-0053"><string-name><surname>Ryder</surname>, <given-names>O. A.</given-names></string-name>, &amp; <string-name><surname>Feistner</surname>, <given-names>A. T. C.</given-names></string-name> (<year>1995</year>). <article-title>Research in zoos: A growth area in conservation</article-title>. <source>Biodiversity and Conservation</source>, <volume>4</volume>, <fpage>671</fpage>–<lpage>677</lpage>. <pub-id pub-id-type="doi">10.1007/BF00222522</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0054">
      <mixed-citation publication-type="journal" id="ece38701-cit-0054"><string-name><surname>Schneider</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Greenberg</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>G. W.</given-names></string-name>, &amp; <string-name><surname>Kremer</surname>, <given-names>S. C.</given-names></string-name> (<year>2020</year>). <article-title>Three critical factors affecting automated image species recognition performance for camera traps</article-title>. <source>Ecology and Evolution</source>, <volume>10</volume>, <fpage>3503</fpage>–<lpage>3517</lpage>. <pub-id pub-id-type="doi">10.1002/ece3.6147</pub-id>
<pub-id pub-id-type="pmid">32274005</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0055">
      <mixed-citation publication-type="book" id="ece38701-cit-0055"><string-name><surname>Schneider</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Taylor</surname>, <given-names>G. W.</given-names></string-name>, &amp; <string-name><surname>Kremer</surname>, <given-names>S. C.</given-names></string-name> (<year>2018</year>). <part-title>Deep Learning Object detection methods for ecological camera trap data</part-title>. In <source>15th Conference on Computer and Robot Vision (CRV)</source> (pp. <fpage>321</fpage>–<lpage>328</lpage>). <pub-id pub-id-type="doi">10.1109/CRV.2018.00052</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0056">
      <mixed-citation publication-type="book" id="ece38701-cit-0056"><string-name><surname>See</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Rahman</surname>, <given-names>S.</given-names></string-name> (<year>2015</year>). <part-title>On the effects of low video quality in human action recognition</part-title>. In <source>International Conference on Digital Image Computing: Techniques and Applications (DICTA)</source> (pp. <fpage>1</fpage>–<lpage>8</lpage>). <pub-id pub-id-type="doi">10.1109/DICTA.2015.7371292</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0057">
      <mixed-citation publication-type="journal" id="ece38701-cit-0057"><string-name><surname>Sicks</surname>, <given-names>F.</given-names></string-name> (<year>2016</year>). <article-title>REM sleep as indicator for stress in giraffes (<italic toggle="yes">Giraffa camelopardalis</italic>)</article-title>. <source>Mammalian Biology</source>, <volume>81</volume>, <fpage>16</fpage>. <pub-id pub-id-type="doi">10.1016/j.mambio.2016.07.052</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0058">
      <mixed-citation publication-type="journal" id="ece38701-cit-0058"><string-name><surname>Siegel</surname>, <given-names>J. M.</given-names></string-name> (<year>2005</year>). <article-title>Clues to the functions of mammalian sleep</article-title>. <source>Nature</source>, <volume>437</volume>, <fpage>1264</fpage>–<lpage>1271</lpage>. <pub-id pub-id-type="doi">10.1038/nature04285</pub-id>
<pub-id pub-id-type="pmid">16251951</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0059">
      <mixed-citation publication-type="journal" id="ece38701-cit-0059"><string-name><surname>Steinmeyer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Schielzeth</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Mueller</surname>, <given-names>J. C.</given-names></string-name>, &amp; <string-name><surname>Kempenaers</surname>, <given-names>B.</given-names></string-name> (<year>2010</year>). <article-title>Variation in sleep behaviour in free‐living blue tits, <italic toggle="yes">Cyanistes caeruleus</italic>: effects of sex, age and environment</article-title>. <source>Animal Behaviour</source>, <volume>80</volume>, <fpage>853</fpage>–<lpage>864</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2010.08.005</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0060">
      <mixed-citation publication-type="journal" id="ece38701-cit-0060"><string-name><surname>Swanson</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kosmala</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lintott</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Simpson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Packer</surname>, <given-names>C.</given-names></string-name> (<year>2015</year>). <article-title>Snapshot Serengeti, high‐frequency annotated camera trap images of 40 mammalian species in an African savanna</article-title>. <source>Scientific Data</source>, <volume>2</volume>, <fpage>150026</fpage>. <pub-id pub-id-type="doi">10.1038/sdata.2015.26</pub-id>
<pub-id pub-id-type="pmid">26097743</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0061">
      <mixed-citation publication-type="journal" id="ece38701-cit-0061"><string-name><surname>Tacutu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Craig</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Budovsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wuttke</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lehmann</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Taranukha</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Costa</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fraifeld</surname>, <given-names>V. E.</given-names></string-name>, &amp; <string-name><surname>de Magalhães</surname>, <given-names>J. P.</given-names></string-name> (<year>2013</year>). <article-title>Human ageing genomic resources: Integrated databases and tools for the biology and genetics of ageing</article-title>. <source>Nucleic Acids Research</source>, <volume>41</volume>, <fpage>D1027</fpage>–<lpage>D1033</lpage>. <pub-id pub-id-type="doi">10.1093/nar/gks1155</pub-id>
<pub-id pub-id-type="pmid">23193293</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0062">
      <mixed-citation publication-type="book" id="ece38701-cit-0062"><collab collab-type="authors">Taipingeric</collab>
(<year>2020</year>). <source>yolo‐v4‐tf.keras</source>. <ext-link xlink:href="https://github.com/taipingeric/yolo-v4-tf.keras" ext-link-type="uri">https://github.com/taipingeric/yolo‐v4‐tf.keras</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0063">
      <mixed-citation publication-type="book" id="ece38701-cit-0063"><string-name><surname>Tan</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Le</surname>, <given-names>Q. V.</given-names></string-name> (<year>2019</year>). <part-title>EfficientNet: Rethinking model scaling for convolutional neural networks</part-title>. In <source>International Conference on Learning Representations (ICLR)</source> (pp. <fpage>6105</fpage>–<lpage>6114</lpage>).</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0064">
      <mixed-citation publication-type="journal" id="ece38701-cit-0064"><string-name><surname>Ternman</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Pastell</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Agenäs</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Strasser</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Winckler</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Nielsen</surname>, <given-names>P. P.</given-names></string-name>, &amp; <string-name><surname>Hänninen</surname>, <given-names>L.</given-names></string-name> (<year>2014</year>). <article-title>Agreement between different sleep states and behaviour indicators in dairy cows</article-title>. <source>Applied Animal Behaviour Science</source>, <volume>160</volume>, <fpage>12</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1016/j.applanim.2014.08.014</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0065">
      <mixed-citation publication-type="journal" id="ece38701-cit-0065"><string-name><surname>Tharwat</surname>, <given-names>A.</given-names></string-name> (<year>2021</year>). <article-title>Classification assessment methods</article-title>. <source>Applied Computing and Informatics</source>, <volume>17</volume>, <fpage>168</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1016/j.aci.2018.08.003</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0066">
      <mixed-citation publication-type="book" id="ece38701-cit-0066"><collab collab-type="authors">Tzutalin</collab>
(<year>2015</year>). <source>LabelImg</source>. <ext-link xlink:href="https://github.com/tzutalin/labelImg" ext-link-type="uri">https://github.com/tzutalin/labelImg</ext-link>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0067">
      <mixed-citation publication-type="journal" id="ece38701-cit-0067"><string-name><surname>Valletta</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Torney</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kings</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Thornton</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Madden</surname>, <given-names>J.</given-names></string-name> (<year>2017</year>). <article-title>Applications of machine learning in animal behaviour studies</article-title>. <source>Animal Behaviour</source>, <volume>124</volume>, <fpage>203</fpage>–<lpage>220</lpage>. <pub-id pub-id-type="doi">10.1016/j.anbehav.2016.12.005</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0068">
      <mixed-citation publication-type="journal" id="ece38701-cit-0068"><string-name><surname>Walsh</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Binding</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Holmes</surname>, <given-names>L.</given-names></string-name> (<year>2019</year>). <article-title>While you were sleeping…</article-title>. <source>Zooquaria</source>, <volume>105</volume>, <fpage>28</fpage>–<lpage>29</lpage>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0069">
      <mixed-citation publication-type="journal" id="ece38701-cit-0069"><string-name><surname>Wang</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Zhao</surname>, <given-names>K.</given-names></string-name>, &amp; <string-name><surname>Tian</surname>, <given-names>Y.</given-names></string-name> (<year>2020</year>). <article-title>A Comprehensive survey of loss functions in machine learning</article-title>. <source>Annals of Data Science</source>, <fpage>1</fpage>–<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1007/s40745-020-00253-5</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0070">
      <mixed-citation publication-type="book" id="ece38701-cit-0070"><string-name><surname>Wickham</surname>, <given-names>H.</given-names></string-name> (<year>2016</year>). <source>ggplot2: Elegant graphics for data analysis</source>. <publisher-name>Springer‐Verlag</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0071">
      <mixed-citation publication-type="journal" id="ece38701-cit-0071"><string-name><surname>Wu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>H.</given-names></string-name>, &amp; <string-name><surname>Feng</surname>, <given-names>J.</given-names></string-name> (<year>2018</year>). <article-title>Arms race of temporal partitioning between carnivorous and herbivorous mammals</article-title>. <source>Scientific Reports</source>, <volume>8</volume>, <fpage>1713</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-20098-6</pub-id>
<pub-id pub-id-type="pmid">29379083</pub-id></mixed-citation>
    </ref>
    <ref id="ece38701-bib-0072">
      <mixed-citation publication-type="book" id="ece38701-cit-0072"><string-name><surname>Xu</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Deng</surname>, <given-names>W.</given-names></string-name> (<year>2016</year>). <part-title>Recurrent convolutional neural network for video classification</part-title>. In <source>IEEE International Conference on Multimedia and Expo (ICME)</source> (pp. <fpage>1</fpage>–<lpage>6</lpage>). <pub-id pub-id-type="doi">10.1109/ICME.2016.7552971</pub-id>
</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0073">
      <mixed-citation publication-type="journal" id="ece38701-cit-0073"><string-name><surname>Yosinski</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Clune</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Lipson</surname>, <given-names>H.</given-names></string-name> (<year>2014</year>). <part-title>How transferable are features in deep neural networks?</part-title> In <source>Proceedings of the 27th International Conference on Neural Information Processing Systems</source> (vol. <volume>2</volume>, pp. <fpage>3320</fpage>–<lpage>3328</lpage>).</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0074">
      <mixed-citation publication-type="book" id="ece38701-cit-0074"><string-name><surname>Zepelin</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Siegel</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Tobler</surname>, <given-names>I.</given-names></string-name> (<year>2005</year>). <part-title>Chapter 8 – Mammalian sleep</part-title>. In <person-group person-group-type="editor"><string-name><given-names>M. H.</given-names><surname>Kryger</surname></string-name></person-group>, <person-group person-group-type="editor"><string-name><given-names>T.</given-names><surname>Roth</surname></string-name></person-group>, &amp; <person-group person-group-type="editor"><string-name><given-names>W. C.</given-names><surname>Dement</surname></string-name></person-group> (Eds.) <source>Principles and practice of sleep medicine</source> (pp. <fpage>91</fpage>–<lpage>100</lpage>). <publisher-name>Elsevier</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ece38701-bib-0075">
      <mixed-citation publication-type="journal" id="ece38701-cit-0075"><string-name><surname>Zizkova</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Kotrba</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Kocisova</surname>, <given-names>A.</given-names></string-name> (<year>2013</year>). <article-title>Effect of changes in behaviour on the heart rate and its diurnal variation in a male and a female eland (<italic toggle="yes">Taurotragus oryx</italic>)</article-title>. <source>Agricultura Tropica et Subtropica</source>, <volume>46</volume>, <fpage>29</fpage>–<lpage>33</lpage>. <pub-id pub-id-type="doi">10.2478/ats-2013-0005</pub-id>
</mixed-citation>
    </ref>
  </ref-list>
  <app-group>
    <app id="ece38701-app-0001" content-type="APPENDIX">
      <label>APPENDIX A</label>
      <sec id="ece38701-sec-0032">
        <title>Overview data</title>
        <p>A detailed overview about the used data is given in Table <xref rid="ece38701-tbl-0001" ref-type="table">A1</xref>. Hereby, for every individual the categories age, sex, and the keeping zoo as well as the stabling conditions are contained. The exact age of the observed individuals ranges from one month to 16.5 years categorized as follows: “young” ranges from birth until the time of weaning with about 6 months, then the individuals become “subadult” until sexual maturity with about 2 years of age and after that they are listed as “adult.”</p>
        <table-wrap position="anchor" id="ece38701-tbl-0001" content-type="TABLE">
          <label>TABLE A1</label>
          <caption>
            <p>The common elands observed in this study and their individual factors age (categorical: young, subadult and adult) and sex</p>
          </caption>
          <table frame="hsides" rules="groups">
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <thead valign="top">
              <tr style="border-bottom:solid 1px #000000">
                <th align="left" valign="top" rowspan="1" colspan="1">Individual</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Age</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Sex</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Keeping</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Stabling</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Nights</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Duration (h)</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Nights per hand</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Pictures</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Binary</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Total</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_01</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">m</td>
                <td align="left" rowspan="1" colspan="1">Zoo_1</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">49</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">404</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_02</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">m</td>
                <td align="left" rowspan="1" colspan="1">Zoo_4</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">29</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">10</td>
                <td align="left" rowspan="1" colspan="1">544</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_03</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">m</td>
                <td align="left" rowspan="1" colspan="1">Zoo_3</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">38</td>
                <td align="char" char="–" rowspan="1" colspan="1">18–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">517</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_04</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">m</td>
                <td align="left" rowspan="1" colspan="1">Zoo_5</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">28</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">15</td>
                <td align="left" rowspan="1" colspan="1">860</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_05</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">m</td>
                <td align="left" rowspan="1" colspan="1">Zoo_2</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">35</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">4</td>
                <td align="left" rowspan="1" colspan="1">519</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_06</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_1</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">49</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">404</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_07</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_4</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">29</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">10</td>
                <td align="left" rowspan="1" colspan="1">487</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_08</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_4</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">29</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">10</td>
                <td align="left" rowspan="1" colspan="1">519</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_09</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_4</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">29</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">10</td>
                <td align="left" rowspan="1" colspan="1">504</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_10</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_4</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">15</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">10</td>
                <td align="left" rowspan="1" colspan="1">512</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_11</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_3</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">21</td>
                <td align="char" char="–" rowspan="1" colspan="1">18–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">550</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_12</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_5</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">28</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">11</td>
                <td align="left" rowspan="1" colspan="1">513</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_13</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_5</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">28</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">14</td>
                <td align="left" rowspan="1" colspan="1">541</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_14</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_2</td>
                <td align="left" rowspan="1" colspan="1">Together</td>
                <td align="left" rowspan="1" colspan="1">35</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">604</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_15</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_2</td>
                <td align="left" rowspan="1" colspan="1">Together</td>
                <td align="left" rowspan="1" colspan="1">34</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">604</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_16</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_4</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">25</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">10</td>
                <td align="left" rowspan="1" colspan="1">557</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_17</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Adult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_3</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">38</td>
                <td align="char" char="–" rowspan="1" colspan="1">18–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">511</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_18</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Subadult</td>
                <td align="left" rowspan="1" colspan="1">m</td>
                <td align="left" rowspan="1" colspan="1">Zoo_5</td>
                <td align="left" rowspan="1" colspan="1">Together</td>
                <td align="left" rowspan="1" colspan="1">27 (28)</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">17 (18)</td>
                <td align="left" rowspan="1" colspan="1">502</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_19</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Subadult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_1</td>
                <td align="left" rowspan="1" colspan="1">Together</td>
                <td align="left" rowspan="1" colspan="1">49</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">636</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_20</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Subadult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_2</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">34</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">4</td>
                <td align="left" rowspan="1" colspan="1">519</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_21</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Subadult</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_2</td>
                <td align="left" rowspan="1" colspan="1">Single</td>
                <td align="left" rowspan="1" colspan="1">33</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">4</td>
                <td align="left" rowspan="1" colspan="1">519</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_22</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Young</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_1</td>
                <td align="left" rowspan="1" colspan="1">Together</td>
                <td align="left" rowspan="1" colspan="1">49</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">636</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_23</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Young</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_5</td>
                <td align="left" rowspan="1" colspan="1">Together</td>
                <td align="left" rowspan="1" colspan="1">22 (28)</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">15 (18)</td>
                <td align="left" rowspan="1" colspan="1">502</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_24</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Young</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_2</td>
                <td align="left" rowspan="1" colspan="1">Together</td>
                <td align="left" rowspan="1" colspan="1">35</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">604</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">
                  <italic toggle="yes">T.oryx_25</italic>
                </td>
                <td align="left" rowspan="1" colspan="1">Young</td>
                <td align="left" rowspan="1" colspan="1">f</td>
                <td align="left" rowspan="1" colspan="1">Zoo_2</td>
                <td align="left" rowspan="1" colspan="1">Together</td>
                <td align="left" rowspan="1" colspan="1">34</td>
                <td align="char" char="–" rowspan="1" colspan="1">17–7</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">604</td>
                <td align="left" rowspan="1" colspan="1">x</td>
                <td align="left" rowspan="1" colspan="1">x</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot id="ece38701-ntgp-0001">
            <title>Note</title>
            <fn id="ece38701-note-0001">
              <p>Further, the housing zoo and the given stabling conditions (standing single or together), are contained. The duration gives the recording start and end time and the totally recorded number of nights as well as the manually annotated number of nights are listed, if nights had to be removed because of an object detection density score smaller than 80% the used number of nights are listed with the real number of nights in parentheses. Finally, the number of pictures describes the number of annotated images in the object detection training set after OHEM. Observe that <italic toggle="yes">T.oryx_01</italic> and <italic toggle="yes">T.oryx_18</italic> is the same individual recorded at different times after moving from one zoo to another. Also, it is marked if the individuals are evaluated with the total or binary classification system.</p>
            </fn>
          </table-wrap-foot>
          <permissions>
            <copyright-holder>John Wiley &amp; Sons, Ltd</copyright-holder>
          </permissions>
        </table-wrap>
      </sec>
      <sec id="ece38701-sec-0033">
        <title>Postprocessing rules</title>
        <p>This section contains the post‐processing rules applied to BOVIDS’ prediction for both classification tasks. With respect to the total classification task, different sets of rules are applied for adult common elands and nonadult common elands, because nonadult individuals show shorter phases.</p>
        <p>The order of the applied rolling average varies between the three sets of rules. A higher order reduces flickering but is likely to dismiss (very) short events. Therefore, the order of the rolling average was set to 3 in the total classification task for nonadult individuals, to 4 in the total classification task for adult individuals and to 5 in the binary classification task.</p>
        <p>Regarding dismissing short phases, the quantity “minimum length” is introduced followed by a three‐character code. If this code is XYZ, this is meant to be read as follows. Suppose a phase of behavior Y lies in between a phase of behavior X and behavior Z, then the event will be dismissed (marked as X) if it consists of less time‐intervals than indicated by the minimum length of XYZ. In those codes, Standing is abbreviated to “A,” LHU to “L” and LHD to “S” in the total classification task. In the binary classification task, “A” means Standing and “L” means Lying. “O” stands for Out in both tasks. *X* is meant to be read as any combination YXZ where Y and Z do not equal X. The applied rules of dismissing short phases can be found in Table <xref rid="ece38701-tbl-0002" ref-type="table">A2</xref>.</p>
        <p>Regarding the special state Out, the post‐processing rules are a bit more elaborated. If flickering between Out and a real behavioral state occurs, this is very likely due to a failure of the object detector if an animal is occluded or truncated. Therefore, if a sequence of a specific behavioral state X (Standing, Lying, LHU or LHD) is interrupted by phases of Out, the Out phases are dismissed under the following conditions. First, each single phase of Out must be shorter than 27 time‐intervals (total) or 135 time‐intervals (binary). Second, the total percentage of X in the sequence needs to exceed 20%.</p>
        <table-wrap position="anchor" id="ece38701-tbl-0002" content-type="TABLE">
          <label>TABLE A2</label>
          <caption>
            <p>Overview about the minimum length a specific behavioral phase needs to have in order not to be dismissed in the post‐processing step</p>
          </caption>
          <table frame="hsides" rules="groups">
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <col style="border-right:solid 1px #000000" span="1"/>
            <thead valign="top">
              <tr style="border-bottom:solid 1px #000000">
                <th align="left" valign="top" rowspan="1" colspan="1">Behavior code</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Total adult</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Total nonadult</th>
                <th align="left" valign="top" rowspan="1" colspan="1">Binary</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">SLS</td>
                <td align="char" char="." rowspan="1" colspan="1">3</td>
                <td align="char" char="." rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SLA</td>
                <td align="char" char="." rowspan="1" colspan="1">3</td>
                <td align="char" char="." rowspan="1" colspan="1">3</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ALS</td>
                <td align="char" char="." rowspan="1" colspan="1">3</td>
                <td align="char" char="." rowspan="1" colspan="1">3</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ALA</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">45</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">OLA</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">45</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">OLS</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ALO</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">45</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SLO</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SAS</td>
                <td align="char" char="." rowspan="1" colspan="1">25</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SAL</td>
                <td align="char" char="." rowspan="1" colspan="1">25</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LAS</td>
                <td align="char" char="." rowspan="1" colspan="1">25</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LAL</td>
                <td align="char" char="." rowspan="1" colspan="1">25</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">45</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LAO</td>
                <td align="char" char="." rowspan="1" colspan="1">25</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">45</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">OAL</td>
                <td align="char" char="." rowspan="1" colspan="1">25</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">45</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">OAS</td>
                <td align="char" char="." rowspan="1" colspan="1">25</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SAO</td>
                <td align="char" char="." rowspan="1" colspan="1">25</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ASA</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ASL</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LSA</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="char" char="." rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LSL</td>
                <td align="char" char="." rowspan="1" colspan="1">2</td>
                <td align="char" char="." rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">LSO</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">OSL</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ASO</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">OSA</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="left" rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">*O*</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="char" char="." rowspan="1" colspan="1">9</td>
                <td align="left" rowspan="1" colspan="1">45</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot id="ece38701-ntgp-0002">
            <title>Note</title>
            <fn id="ece38701-note-0002">
              <p>The value is to be read as time‐intervals where 1 time‐interval consists of 7 seconds. Standing is abbreviated to “A,” LHU to “L” and LHD to “S” in the total classification task. In the binary classification task, “A” means Standing and “L” means Lying. “O” stands for Out in both tasks.</p>
            </fn>
          </table-wrap-foot>
          <permissions>
            <copyright-holder>John Wiley &amp; Sons, Ltd</copyright-holder>
          </permissions>
        </table-wrap>
      </sec>
    </app>
  </app-group>
</back>
