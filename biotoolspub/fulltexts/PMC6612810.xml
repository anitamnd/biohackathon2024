<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612810</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz333</article-id>
    <article-id pub-id-type="publisher-id">btz333</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Studies of Phenotypes and Clinical Applications</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Block HSIC Lasso: model-free biomarker detection for ultra-high dimensional data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Climente-González</surname>
          <given-names>Héctor</given-names>
        </name>
        <xref ref-type="aff" rid="btz333-aff1">1</xref>
        <xref ref-type="aff" rid="btz333-aff2">2</xref>
        <xref ref-type="aff" rid="btz333-aff3">3</xref>
        <xref ref-type="aff" rid="btz333-aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Azencott</surname>
          <given-names>Chloé-Agathe</given-names>
        </name>
        <xref ref-type="aff" rid="btz333-aff1">1</xref>
        <xref ref-type="aff" rid="btz333-aff2">2</xref>
        <xref ref-type="aff" rid="btz333-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kaski</surname>
          <given-names>Samuel</given-names>
        </name>
        <xref ref-type="aff" rid="btz333-aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yamada</surname>
          <given-names>Makoto</given-names>
        </name>
        <xref ref-type="aff" rid="btz333-aff4">4</xref>
        <xref ref-type="aff" rid="btz333-aff6">6</xref>
        <xref ref-type="corresp" rid="btz333-cor1"/>
        <!--<email>myamada@i.kyoto-u.ac.jp</email>-->
      </contrib>
    </contrib-group>
    <aff id="btz333-aff1"><label>1</label>Institut Curie, PSL Research University, Paris, France</aff>
    <aff id="btz333-aff2"><label>2</label>INSERM, U900, Paris, France</aff>
    <aff id="btz333-aff3"><label>3</label>MINES ParisTech, PSL Research University, CBIO-Centre for Computational Biology, Paris, France</aff>
    <aff id="btz333-aff4"><label>4</label>RIKEN AIP, Tokyo, Japan</aff>
    <aff id="btz333-aff5"><label>5</label>Department of Computer Science, Aalto University, Espoo, Finland</aff>
    <aff id="btz333-aff6"><label>6</label>Department of intelligence science and technology, Kyoto University, Kyoto, Japan</aff>
    <author-notes>
      <corresp id="btz333-cor1">To whom correspondence should be addressed. <email>myamada@i.kyoto-u.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i427</fpage>
    <lpage>i435</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz333.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Finding non-linear relationships between biomolecules and a biological outcome is computationally expensive and statistically challenging. Existing methods have important drawbacks, including among others lack of parsimony, non-convexity and computational overhead. Here we propose block HSIC Lasso, a non-linear feature selector that does not present the previous drawbacks.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We compare block HSIC Lasso to other state-of-the-art feature selection techniques in both synthetic and real data, including experiments over three common types of genomic data: gene-expression microarrays, single-cell RNA sequencing and genome-wide association studies. In all cases, we observe that features selected by block HSIC Lasso retain more information about the underlying biology than those selected by other techniques. As a proof of concept, we applied block HSIC Lasso to a single-cell RNA sequencing experiment on mouse hippocampus. We discovered that many genes linked in the past to brain development and function are involved in the biological differences between the types of neurons.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Block HSIC Lasso is implemented in the Python 2/3 package pyHSICLasso, available on PyPI. Source code is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/riken-aip/pyHSICLasso">https://github.com/riken-aip/pyHSICLasso</ext-link>).</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">European Union’s Horizon 2020 research and innovation program</named-content>
        </funding-source>
        <award-id>666003</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Academy of Finland</named-content>
          <named-content content-type="funder-identifier">10.13039/501100002341</named-content>
        </funding-source>
        <award-id>292334</award-id>
        <award-id>319264</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">JST</named-content>
          <named-content content-type="funder-identifier">10.13039/501100001695</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">MEXT</named-content>
          <named-content content-type="funder-identifier">10.13039/501100001700</named-content>
        </funding-source>
        <award-id>16H06299</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Biomarker discovery, the goal of many bioinformatics experiments, aims at identifying a few key biomolecules that explain most of an observed phenotype. Without a strong prior hypothesis, these molecular markers have to be identified from data generated by high-throughput technologies. Unfortunately, finding relevant molecules is a combinatorial problem: for <italic>d</italic> features, <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> binary choices must be considered. As the number of features vastly exceeds the number of samples, biomarker discovery is a high-dimensional problem. The statistical challenges posed by such high-dimensional spaces have been thoroughly reviewed elsewhere (<xref rid="btz333-B3" ref-type="bibr">Clarke <italic>et al.</italic>, 2008</xref>; <xref rid="btz333-B13" ref-type="bibr">Johnstone and Titterington, 2009</xref>). In general, due to the <italic>curse of dimensionality</italic>, fitting models in many dimensions and on a small number of samples is extremely hard. Moreover, since biology is complex, a simple statistical model such as a linear regression might not be able to find important biomarkers. Those that are found in such experiments are often hard to reproduce, suggesting overfitting. Exploring the solution space and finding true biomarkers are not only statistically challenging, but also computationally expensive.</p>
    <p>In machine learning terms, biomarker discovery can be formulated as a problem of feature selection: identifying the best subset of features to separate between categories, or to predict a continuous response. In the past decades, many feature selection algorithms that deal with high-dimensional datasets have been proposed. Due to the difficulties posed by high-dimensionality, linear methods tend to be the feature selector of choice in bioinformatics. A widely used linear feature selector is the Least Absolute Shrinkage and Selection Operator, or Lasso (<xref rid="btz333-B23" ref-type="bibr">Tibshirani, 1996</xref>). Lasso fits a linear model between the input features and phenotype by minimizing the sum of the least square loss and an <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> penalty term. The balance between the least square loss and the penalty ensures that the model explains the linear combination of features, while keeping the number of features in the model small. However, in many instances biological phenomena do not behave linearly. In such cases, there is no guarantee that Lasso can capture those non-linear relationships or an appropriate effect size to represent them.</p>
    <p>In the past decade, several non-linear feature selection algorithms for high-dimensional datasets have been proposed. One of the most widely used, called Sparse Additive Model, or SpAM (<xref rid="btz333-B20" ref-type="bibr">Ravikumar <italic>et al.</italic>, 2009</xref>), models the outcome as a sparse linear combination of non-linear functions based on kernels. However, since SpAM assumes an additive model over the selected features, it cannot select important features if the phenotype cannot be represented by the additive functions of input features—for example, if there exist a multiplicative relationship between features (<xref rid="btz333-B28" ref-type="bibr">Yamada <italic>et al.</italic>, 2014</xref>).</p>
    <p>Another family of non-linear feature selectors are association-based: they compute the statistical association score between each input feature and the outcome, and rank features accordingly. Since these approaches do not assume any model about the output, they can detect important features as long as an association exists. When using a non-linear association measure, such as the mutual information (<xref rid="btz333-B4" ref-type="bibr">Cover and Thomas, 2006</xref>) or the Hilbert–Schmidt Independence Criterion (HSIC) (<xref rid="btz333-B8" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>), they select the features with the strongest dependence with the phenotype. However, association-based methods do not account for the redundancy between the features, which is frequent in biological datasets, since they do not model relationships between features. Hence, many redundant features are typically selected, hindering interpretability. This is important in applications like drug target discovery, where only a small number of targets can be validated, and it is crucial to discriminate the most important target out of many other top-ranked targets.</p>
    <p>To deal with the problem of redundant features, <xref rid="btz333-B18" ref-type="bibr">Peng <italic>et al.</italic> (2005)</xref> proposed the minimum redundancy maximum relevance (mRMR) algorithm. mRMR can select a set of non-redundant features that have high association to the phenotype, while penalizing the selection of mutually dependent features. <xref rid="btz333-B5" ref-type="bibr">Ding and Peng (2005)</xref> used mRMR to extract biomarkers from microarray data, finding that the selected genes captured better the variability in the phenotypes than those identified by state-of-the-art approaches. However, mRMR has three main drawbacks: the optimization problem is discrete; it must be solved by a greedy approach and the mutual information estimation is difficult (<xref rid="btz333-B26" ref-type="bibr">Walters-Williams and Li, 2009</xref>). Moreover, it is unknown whether the objective function of mRMR has good theoretical properties such as submodularity (<xref rid="btz333-B7" ref-type="bibr">Fujishige, 2005</xref>), which would guarantee the optimality of the solution.</p>
    <p>Recently, <xref rid="btz333-B28" ref-type="bibr">Yamada <italic>et al.</italic> (2014)</xref> proposed a kernel-based mRMR algorithm called HSIC Lasso. Instead of mutual information, HSIC Lasso employs the HSIC (<xref rid="btz333-B8" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>) to measure dependency between variables. In addition, it uses an <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> penalty term to select a small number of features. This results in a convex optimization problem, for which one can therefore find a globally optimal solution. In practice, HSIC Lasso has been found to outperform mRMR in several experimental settings (<xref rid="btz333-B28" ref-type="bibr">Yamada <italic>et al.</italic>, 2014</xref>). However, HSIC Lasso is memory intensive: its memory complexity is <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic>d</italic> is the number of features and <italic>n</italic> is the number of samples. Hence, HSIC Lasso cannot be applied to datasets with thousands of samples, nowadays widespread in biology. A MapReduce version of HSIC Lasso has been proposed to address this drawback, and it is able to select features in ultra-high dimensional settings (10<sup>6</sup> features, 10<sup>4</sup> samples) in a matter of hours (<xref rid="btz333-B29" ref-type="bibr">Yamada <italic>et al.</italic>, 2018</xref>). However, it requires a large number of computing nodes, inaccessible to common laboratories. Since it relies on the Nyström approximation of Gram matrices (<xref rid="btz333-B21" ref-type="bibr">Schölkopf and Smola, 2002</xref>), the final optimization problem is no longer convex, and hence finding a globally optimal solution cannot be easily guaranteed.</p>
    <p>In this article, we propose block HSIC Lasso: a simple yet effective non-linear feature selection algorithm based on HSIC Lasso. The key idea is to use the recently proposed block HSIC estimator (<xref rid="btz333-B30" ref-type="bibr">Zhang <italic>et al.</italic>, 2018</xref>) to estimate the HSIC terms. By splitting the data in blocks of size <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:mi>B</mml:mi><mml:mo>≪</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, the memory complexity of HSIC Lasso goes from <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> down to <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">dnB</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Moreover the optimization problem of the block HSIC Lasso remains convex. Through its application to synthetic data and biological datasets, we show that block HSIC Lasso can be applied to a variety of settings and compares favorably with the vanilla HSIC Lasso algorithm and other feature selection approaches, linear and non-linear, as it selects features more informative of the biological outcome. Further considerations on the state of the art and the relevance of block HSIC Lasso can be found in <xref ref-type="supplementary-material" rid="sup1">Supplementary File 1</xref>.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Problem formulation</title>
      <p>Assume a dataset with <italic>n</italic> samples described by <italic>d</italic> real-valued features, each corresponding to a biomolecule (e.g. the expression of one transcript, or the number of major alleles observed at a given SNP), and a label, continuous or binary, describing the outcome of interest (e.g. the abundance of a target protein, or disease status). We denote the <italic>i</italic>th sample by <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:math></inline-formula> denotes transpose; and its label by <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:mi mathvariant="script">Y</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> for a binary outcome, corresponding to a classification problem, and <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi mathvariant="script">Y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></inline-formula> for a continuous outcome, corresponding to a regression problem. In addition, we denote by <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> the <italic>k</italic>th feature in the data.</p>
      <p>The goal of supervised feature selection is to find <italic>m</italic> features (<inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≪</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula>) that are the most relevant for predicting the output <italic>y</italic> for a sample <inline-formula id="IE15"><mml:math id="IM15"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula>.</p>
    </sec>
    <sec>
      <title>2.2 HSIC Lasso</title>
      <p>Measuring the dependence between two random variables <italic>X</italic> and <italic>Y</italic> can be achieved by the HSIC (<xref rid="btz333-B8" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>):
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>HSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:mi>K</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:mi>L</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></inline-formula> are positive definite kernels, and <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes the expectation over independent pairs (<italic>x</italic>, <italic>y</italic>) and <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> drawn from <italic>p</italic>(<italic>x</italic>, <italic>y</italic>). <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mtext>HSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is equal to 0 if <italic>X</italic> and <italic>Y</italic> are independent, and is non-negative otherwise.</p>
      <p>In practice, for a given Gram matrix <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, computed from the <italic>k</italic>th feature, and a given output Gram matrix <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, the normalized variant of HSIC is computed using its <italic>V</italic>-statistic estimator as (<xref rid="btz333-B29" ref-type="bibr">Yamada <italic>et al.</italic>, 2018</xref>)
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo> </mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where for a Gram matrix <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is defined as <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">HKH</mml:mi><mml:mo>/</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">HKH</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> a centering matrix defined by <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>δ</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>. Here <italic>δ<sub>ij</sub></italic> is equal to 1 if <italic>i </italic>=<italic> j</italic> and 0 otherwise, and <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:mtext>tr</mml:mtext></mml:mrow></mml:math></inline-formula> denotes the trace. Note that we employ the normalized variant of the original empirical HSIC.</p>
      <p>The largest the value of <inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and the more dependent the <italic>k</italic>th feature and the outcome are. <xref rid="btz333-B22" ref-type="bibr">Song <italic>et al.</italic> (2012)</xref> therefore proposed to perform feature selection by ranking the features by descending value of <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
      <p>With HSIC Lasso, <xref rid="btz333-B28" ref-type="bibr">Yamada <italic>et al.</italic> (2014)</xref> extend the work of <xref rid="btz333-B22" ref-type="bibr">Song <italic>et al.</italic> (2012)</xref> so as to avoid selecting multiple redundant features. For this purpose, they introduce a vector <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> of feature weights and solve the following optimization problem:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>The first term enforces selected features that are highly dependent on the phenotype; the second term penalizes selecting mutually dependent features and the third term enforces selecting a small number of features. The selected features are those that have a non-zero coefficient <italic>α<sub>k</sub></italic>. Here <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:mo>λ</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is a regularization parameter that controls the sparsity of the solution: the larger <italic>λ</italic>, the fewer features have a non-zero coefficient.</p>
      <p>The HSIC Lasso optimization problem can be rewritten as
<disp-formula id="E4"><mml:math id="M4"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mo> </mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:mtext>vec</mml:mtext><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo>↦</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nn</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> is the vectorization operator. Using this formulation, we can solve the problem using an off-the-shelf non-negative Lasso solver.</p>
      <p>HSIC Lasso performs well for high-dimensional data. However, it requires a large memory space (<inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), since it stores <italic>d</italic> Gram matrices. To handle this issue, two approximation methods have been proposed. The first approach uses a memory lookup to dramatically reduce the memory space (<xref rid="btz333-B28" ref-type="bibr">Yamada <italic>et al.</italic>, 2014</xref>). However, since this method needs to perform a large number of memory lookups, it is computationally expensive. Another approach (<xref rid="btz333-B29" ref-type="bibr">Yamada <italic>et al.</italic>, 2018</xref>) is to rewrite the problem using the Nyström approximation (<xref rid="btz333-B21" ref-type="bibr">Schölkopf and Smola, 2002</xref>) and solve the problem using a cluster. However using the Nyström approximation makes the problem non-convex.</p>
    </sec>
    <sec>
      <title>2.3 Block HSIC Lasso</title>
      <p>In this article, we propose an alternative HSIC Lasso method for large-scale problems, the <italic>block HSIC Lasso</italic>, which is convex and can be efficiently solved on a reasonably sized server.</p>
      <p>Block HSIC Lasso employs the block HSIC estimator (<xref rid="btz333-B30" ref-type="bibr">Zhang <italic>et al.</italic>, 2018</xref>) instead of the V-statistics estimator of <xref ref-type="disp-formula" rid="E2">Equation (2)</xref>. More specifically, to compute the block HSIC, we first partition the training dataset into <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula> partitions <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <italic>B</italic> is the number of samples in each block. Note that the block size <italic>B</italic> is set to a relatively small number such as 10 or 20 (<inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:mi>B</mml:mi><mml:mo>≪</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>). Then, the block HSIC estimator can be written as
<disp-formula id="E5"><mml:math id="M5"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>B</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> represents the <italic>k</italic>th feature vector of the <inline-formula id="IE38"><mml:math id="IM38"><mml:mi>ℓ</mml:mi></mml:math></inline-formula>th partition. Note that the computation of <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> requires <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> memory space. Therefore, the required memory for the block HSIC estimator is <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:mi mathvariant="italic">nB</mml:mi><mml:mo>≪</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      <p>If we denote by <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> the restriction of <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to the <inline-formula id="IE45"><mml:math id="IM45"><mml:mi>ℓ</mml:mi></mml:math></inline-formula>th partition, and by <inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>×</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> the restriction of <inline-formula id="IE47"><mml:math id="IM47"><mml:mi mathvariant="bold-italic">L</mml:mi></mml:math></inline-formula> to the <inline-formula id="IE48"><mml:math id="IM48"><mml:mi>ℓ</mml:mi></mml:math></inline-formula>th partition, then
<disp-formula id="E6"><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>vec</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mtext>vec</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Block HSIC Lasso is obtained by replacing the HSIC estimator <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with the block HSIC estimator <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="E3">Equation (3)</xref>:
<disp-formula id="E7"><mml:math id="M7"><mml:mrow><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:munderover><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>Using the vectorization operator, the block estimator is written as
<disp-formula id="E8"><mml:math id="M8"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where
<disp-formula id="E9"><mml:math id="M9"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:msqrt><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtext>vec</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mtext>vec</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nB</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:msqrt><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtext>vec</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mtext>vec</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nB</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p>Hence, block HSIC Lasso can also be written as
<disp-formula id="E10"><mml:math id="M10"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nB</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      <p>Since the objective function of block HSIC Lasso is convex, we can obtain a globally optimal solution. As with HSIC Lasso, we can solve block HSIC Lasso using an off-the-shelf Lasso solver. Here, we use the non-negative least angle regression-LASSO, or LARS-LASSO (<xref rid="btz333-B6" ref-type="bibr">Efron <italic>et al.</italic>, 2004</xref>), to solve the problem in a greedy manner. Rather than setting the hyperparameter <italic>λ</italic>, for example by cross-validation, which would be computationally intensive, this allows us to use a predefined number of features to select.</p>
      <p>The required memory space for block HSIC Lasso is <italic>O</italic>(<italic>dnB</italic>), which compares favorably to vanilla HSIC Lasso’s <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>; as the block size <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:mi>B</mml:mi><mml:mo>≪</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, the memory space is dramatically reduced. However, the computational cost of the proposed method is still large when both <italic>d</italic> and <italic>n</italic> are large. Thus, we implemented the proposed algorithm using multiprocessing by parallelizing the computation of <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. Thanks to the combination of block HSIC Lasso and the multiprocessing implementation, we can efficiently find solutions on large datasets with a reasonably sized server.</p>
    </sec>
    <sec>
      <title>2.4 Improving selection stability using bagging</title>
      <p>Since we need to compute block HSIC of the paired data <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>ℓ</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> with a fixed partition, the performance can be highly affected by the partition. Thus, we propose to use a bagging version of the block HSIC estimator. Given <italic>M</italic> random permutations of the <italic>n</italic> samples, we define <italic>bagging block HSIC</italic> as
<disp-formula id="E11"><mml:math id="M11"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="italic">bb</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mfrac><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the <italic>k</italic>th feature vector restricted to the <inline-formula id="IE57"><mml:math id="IM57"><mml:mi>ℓ</mml:mi></mml:math></inline-formula>th block as defined by the <italic>m</italic>th permutation,
<disp-formula id="E12"><mml:math id="M12"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:mrow></mml:msqrt><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nBM</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E13"><mml:math id="M13"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:mrow></mml:msqrt><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>⊤</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nBM</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
and <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nB</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE59"><mml:math id="IM59"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nB</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the vectors of the <italic>m</italic>th block HSIC Lasso, respectively.</p>
      <p>Hence, bagging block HSIC Lasso can be written as
<disp-formula id="E14"><mml:math id="M14"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nBM</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      <p>We consider the bagging part to be an integral part of the block HSIC Lasso algorithm. That is why, in this text, every time we mention ‘block HSIC Lasso’, we refer to bagging block HSIC Lasso.</p>
      <p>Note that the memory space <italic>O</italic>(<italic>dnBM</italic>) required for <italic>B </italic>=<italic> </italic>60 and <italic>M </italic>=<italic> </italic>1 is equivalent to <italic>B </italic>=<italic> </italic>30 and <italic>M </italic>=<italic> </italic>2. Empirically, we found that they were providing equivalent feature selection accuracy (Section 4.4).</p>
    </sec>
    <sec>
      <title>2.5 Adjusting for covariates</title>
      <p>Data analysis tasks in bioinformatics can often be confounded by technical (e.g. batch) or biological variables (e.g. age), which might mask the relevant variables. To adjust for their effect, we consider the following variant of the block HSIC Lasso:
<disp-formula id="E15"><mml:math id="M15"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>−</mml:mo><mml:mo>β</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:mo>β</mml:mo><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is a tuning parameter and
<disp-formula id="E16"><mml:math id="M16"><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:msqrt><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtext>vec</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mtext>vec</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">K</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nB</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>
contains the covariate information. <inline-formula id="IE62"><mml:math id="IM62"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the Gram matrix computed from the covariate input matrix <inline-formula id="IE63"><mml:math id="IM63"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Since for most purposes in bioinformatics we want to remove all information from the covariates, we set <italic>β</italic> to
<disp-formula id="E17"><mml:math id="M17"><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
which is the solution of <inline-formula id="IE64"><mml:math id="IM64"><mml:mrow><mml:msub><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mo>β</mml:mo></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>−</mml:mo><mml:mo>β</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. Here, we used the property <inline-formula id="IE65"><mml:math id="IM65"><mml:mrow><mml:msub><mml:mrow><mml:mtext>HSIC</mml:mtext></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Experimental setup</title>
    <sec>
      <title>3.1 Feature selection methods</title>
      <p><bold>HSIC Lasso and block HSIC Lasso:</bold> We used HSIC Lasso and block HSIC Lasso implemented in the Python 2/3 package <italic>pyHSICLasso</italic>. In block HSIC Lasso, <italic>M</italic> was set to 3 in all experimental settings; the block size <italic>B</italic> was set on an experiment-dependent fashion. In all the experiments, when we wanted to select <italic>k</italic> features, HSIC Lasso versions were required to first retrieve 50 features, and then the top <italic>k</italic> features were selected as the solution.</p>
      <p>In this article, we use the following kernels:
<list list-type="bullet"><list-item><p>The RBF Gaussian kernel for pairs of continuous variables, of continuous outcomes, or one of each, and for pairs of a continuous variable and categorical outcome:
<disp-formula id="E21"><mml:math id="M18"><mml:mrow><mml:mi>K</mml:mi><mml:mo>:</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>↦</mml:mo><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE66"><mml:math id="IM66"><mml:mrow><mml:msup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is the bandwidth of the kernel;</p></list-item><list-item><p>The normalized Delta kernel for categorical variables (or outcomes):
<disp-formula id="E22"><mml:math id="M19"><mml:mrow><mml:mi>L</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>↦</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo> </mml:mo><mml:mtext>if</mml:mtext><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo> </mml:mo><mml:mtext>otherwise</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>n<sub>c</sub></italic> is the number of samples in class <italic>c</italic>.</p></list-item></list></p>
      <p><bold>mRMR:</bold> mRMR selects features that are highly associated with the outcome and are non-redundant (<xref rid="btz333-B18" ref-type="bibr">Peng <italic>et al.</italic>, 2005</xref>). To that end, it uses mutual information between different variables and between the outcome and the variables.</p>
      <p>We used a C++ implementation of mRMR (<xref rid="btz333-B17" ref-type="bibr">Peng, 2005</xref>). The maximum number of samples and the maximum number of features were set to the actual number of samples and features in the data. In regression problems, discretization was set to binarization.</p>
      <p><bold>LARS:</bold> LARS is a forward stage-wise feature selector (<xref rid="btz333-B6" ref-type="bibr">Efron <italic>et al.</italic>, 2004</xref>). It is an efficient way of solving the same problem as Lasso. We used the SPAMS implementation of LARS (<xref rid="btz333-B16" ref-type="bibr">Mairal <italic>et al.</italic>, 2010</xref>), with the default parameters. Note that this is not the implementation of LARS that we use in (block) HSIC Lasso, which is the non-negative LARS solver implemented in <italic>pyHSICLasso</italic>.</p>
    </sec>
    <sec>
      <title>3.2 Evaluation of the selected features</title>
      <p><bold>Selection accuracy on simulated data:</bold> We simulated high-dimensional data where only a few variables were truly related to the outcome. We used these datasets to evaluate the ability of the tested algorithms to find the true causal variables, instead of others, likely spuriously correlated to the outcome. To that end, we requested each algorithm to retrieve the known number of causal features. Then, we studied how many of them were actually causal.</p>
      <p><bold>Classification with a random forest:</bold> In classification datasets, we evaluated the amount of information retained in the features selected by a given method by evaluating the performance of a random forest classifier based only on those features. We used random forests because of their ability to handle non-linearities. We split the data between a training and a test set, and selected features on the training set only. We estimated the best parameters by cross-validation on the training set: the number of trees (200, 500), the maximum depth of the threes (4, 6, 8), the number of features to consider (<inline-formula id="IE67"><mml:math id="IM67"><mml:mrow><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula>), and the criterion to measure the quality of the chosen features (Gini impurity, information gain). Then, we trained a model with those parameters on the training set and made predictions on a separate testing set to estimate prediction accuracy.</p>
    </sec>
    <sec>
      <title>3.3 Datasets</title>
      <p>We evaluated the performance of the different algorithms on synthetic data and four types of real-world high-dimensional datasets (<xref rid="btz333-T1" ref-type="table">Table 1</xref>). In our experiments on real-world datasets, we restricted ourselves to classification problems. All discussed methods can however handle regression problems (continuous-valued outcomes) as well, as we show on synthetic data.</p>
      <table-wrap id="btz333-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Summary description of benchmark datasets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Type</th>
              <th rowspan="1" colspan="1">Dataset</th>
              <th rowspan="1" colspan="1">Features (<italic>d</italic>)</th>
              <th rowspan="1" colspan="1">Samples (<italic>n</italic>)</th>
              <th rowspan="1" colspan="1">Classes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Image</td>
              <td rowspan="1" colspan="1">AR10P</td>
              <td rowspan="1" colspan="1">2400</td>
              <td rowspan="1" colspan="1">130</td>
              <td rowspan="1" colspan="1">10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">PIE10P</td>
              <td rowspan="1" colspan="1">2400</td>
              <td rowspan="1" colspan="1">210</td>
              <td rowspan="1" colspan="1">10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">PIX10P</td>
              <td rowspan="1" colspan="1">10 000</td>
              <td rowspan="1" colspan="1">100</td>
              <td rowspan="1" colspan="1">10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">ORL10P</td>
              <td rowspan="1" colspan="1">10 000</td>
              <td rowspan="1" colspan="1">100</td>
              <td rowspan="1" colspan="1">10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Microarray</td>
              <td rowspan="1" colspan="1">CLL-SUB-111</td>
              <td rowspan="1" colspan="1">11 340</td>
              <td rowspan="1" colspan="1">111</td>
              <td rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">GLIOMA</td>
              <td rowspan="1" colspan="1">4434</td>
              <td rowspan="1" colspan="1">50</td>
              <td rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">SMK-CAN-187</td>
              <td rowspan="1" colspan="1">19 993</td>
              <td rowspan="1" colspan="1">187</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">TOX-171</td>
              <td rowspan="1" colspan="1">5748</td>
              <td rowspan="1" colspan="1">171</td>
              <td rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">
                <xref rid="btz333-B9" ref-type="bibr">Haber <italic>et al.</italic> (2017)</xref>
              </td>
              <td rowspan="1" colspan="1">15 972</td>
              <td rowspan="1" colspan="1">7216</td>
              <td rowspan="1" colspan="1">19</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">scRNA-seq</td>
              <td rowspan="1" colspan="1">
                <xref rid="btz333-B10" ref-type="bibr">Habib <italic>et al.</italic> (2016)</xref>
              </td>
              <td rowspan="1" colspan="1">25 393</td>
              <td rowspan="1" colspan="1">13 302</td>
              <td rowspan="1" colspan="1">8</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">
                <xref rid="btz333-B25" ref-type="bibr">Villani <italic>et al.</italic> (2017)</xref>
              </td>
              <td rowspan="1" colspan="1">23 395</td>
              <td rowspan="1" colspan="1">1140</td>
              <td rowspan="1" colspan="1">10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GWA data</td>
              <td rowspan="1" colspan="1">RA versus controls</td>
              <td rowspan="1" colspan="1">352 773</td>
              <td rowspan="1" colspan="1">3451</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">T1D versus controls</td>
              <td rowspan="1" colspan="1">352 853</td>
              <td rowspan="1" colspan="1">3443</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">T2D versus controls</td>
              <td rowspan="1" colspan="1">353 046</td>
              <td rowspan="1" colspan="1">3456</td>
              <td rowspan="1" colspan="1">2</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p><bold>Synthetic data:</bold> We simulated random matrices of features <inline-formula id="IE68"><mml:math id="IM68"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. A number of variables were selected as related to the phenotype, and functions that are non-linear in the data range were selected (cosine, sine and square) and combined additively to create the outcome vector <inline-formula id="IE69"><mml:math id="IM69"><mml:mi mathvariant="bold-italic">y</mml:mi></mml:math></inline-formula>.</p>
      <p><bold>Images:</bold> Facial recognition is a classification problem classically used to evaluate non-linear feature selection methods, as only a few of all features are expected to be relevant for the outcome, in a non-linear fashion. We used four face image datasets from the Arizona State University feature selection repository (<xref rid="btz333-B15" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>)): pixraw10P, warpAR10P, orlraws10P and warpPIE10P.</p>
      <p><bold>Gene expression microarrays:</bold> We analyzed four gene expression microarray datasets from Arizona State University feature selection repository (<xref rid="btz333-B15" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>). The phenotypes were subtypes of B-cell chronic lymphocytic leukemia (CLL-SUB-111), hepatocyte phenotypes under different diets (TOX-171), glioma (GLIOMA) and smoking-driven carcinogenesis (SMK-CAN-187).</p>
      <p><bold>Single-cell RNA-seq:</bold> Single-cell RNA-seq (scRNA-seq) measures gene expression at cell resolution, allowing to characterize the diversity in a tissue. We performed feature selection on the three most popular datasets in the Broad Institute’s Single Cell Portal, related to mouse small intestinal epithelium (<xref rid="btz333-B9" ref-type="bibr">Haber <italic>et al.</italic>, 2017</xref>), mouse hippocampus (<xref rid="btz333-B10" ref-type="bibr">Habib <italic>et al.</italic>, 2016</xref>) and human blood cells (<xref rid="btz333-B25" ref-type="bibr">Villani <italic>et al.</italic>, 2017</xref>). Missing gene expressions were imputed with MAGIC (<xref rid="btz333-B24" ref-type="bibr">van Dijk <italic>et al.</italic>, 2018</xref>).</p>
      <p><bold>GWA datasets:</bold> We studied the WTCCC1 datasets (<xref rid="btz333-B1" ref-type="bibr">Burton <italic>et al.</italic>, 2007</xref>) for rheumatoid arthritis (RA), type 1 diabetes (T1D) and type 2 diabetes (T2D) (2000 samples each), using the 1958BC cohort as control (1504 samples). Affymetrix 500K was used for genotyping. We removed the samples and the SNPs that did not pass WTCCC’s quality controls, as well as SNPs in sex chromosomes and those that were not genotyped in both cases and controls. Missing genotypes were imputed with CHIAMO. Lastly, individuals with &gt;10% genotype missing rate, and SNPs with &gt;10% genotype missing rate, MAF &lt; 5% or not in HWE (<italic>P</italic>-value &lt; 0.001) were removed. The remaining missing genotypes were replaced by the major allele in homozygosis.</p>
      <p><bold>Preprocessing:</bold> Images, microarrays and scRNA-seq data were normalized feature-wise by subtracting the mean and dividing by the standard deviation. GWAS data did not undergo any normalization.</p>
    </sec>
    <sec>
      <title>3.4 Computational resources</title>
      <p>We ran the experiments on synthetic data, images, microarrays and scRNA-seq on CentOS 7 machines with Intel Xeon 2.6 GHz and 50 GB RAM memory. For the GWA datasets experiments, we used a CentOS 7 server with 96 core Intel Xeon 2.2 GHz and 1 TB RAM memory.</p>
    </sec>
    <sec>
      <title>3.5 Software availability and reproducibility</title>
      <p>Block HSIC Lasso was implemented in the Python 2/3 package <italic>pyHSICLasso</italic>. The source code is available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/riken-aip/pyHSICLasso">https://github.com/riken-aip/pyHSICLasso</ext-link>), and the package can be installed from PyPI (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/pyHSICLasso">https://pypi.org/project/pyHSICLasso</ext-link>). All analyses in this article and the scripts needed to reproduce them are also available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/hclimente/nori">https://github.com/hclimente/nori</ext-link>).</p>
    </sec>
  </sec>
  <sec>
    <title>4 Results</title>
    <sec>
      <title>4.1 Block HSIC Lasso performance is comparable to state of the art</title>
      <p>At first, we worked on synthetic, non-linear data (Section 3.2). We generated synthetic data with combinations of the following experimental parameters: <inline-formula id="IE70"><mml:math id="IM70"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo> </mml:mo><mml:mn>000</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> samples; <inline-formula id="IE71"><mml:math id="IM71"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>500</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo> </mml:mo><mml:mn>000</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> features; and 5, 10 and 20 causal features, that is, features truly related to the outcome. We evaluated the performance of different feature selectors at retrieving the causal features. These conditions range from an ideal setting, where the number of features is smaller than the number of samples, to an ultra-high dimensional scenario, where spurious dependencies among variables, and between those and the outcome are bound to occur.</p>
      <p>Each of the methods was required to select as many features as the number of true causal features. In <xref ref-type="fig" rid="btz333-F1">Figure 1</xref>, we show the proportion of the causal features retrieved by each method. The different versions of HSIC Lasso outperform the other approaches in virtually all settings. Block HSIC Lasso with decreasing block sizes results in worse performances. As expected, vanilla HSIC Lasso outperforms the block versions in accuracy, but increases memory use. Crucially, block HSIC Lasso on a larger number of samples performs better than vanilla HSIC Lasso on fewer samples. Hence, when the number of samples is in the thousands, it is better to apply block HSIC Lasso on the whole dataset, than to apply vanilla HSIC Lasso on a subsample.
</p>
      <fig id="btz333-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>Percentage of true causal features extracted by different feature selectors. Each data point represents the mean over 10 replicates, and the error bars represent the standard error of the mean. Lines are discontinued when the algorithm required more memory than the provided (50 GB). Note that in some conditions mRMR’s line cannot be seen due to the overlap with LARS</p>
        </caption>
        <graphic xlink:href="btz333f1"/>
      </fig>
      <p>We wanted to test these conclusions using a non-linear, real-world dataset. We selected four image-based face recognition tasks (Section 3.3). In this case, we selected different numbers of features (10, 20, 30, 40 and 50). Then, we trained random forest classifiers on these subsets of the features, and compared the accuracy of the different classifiers on a test set (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S1</xref>). Block HSIC Lasso displayed a performance comparable to vanilla HSIC Lasso, and comparable or superior to the other methods. This is remarkable, since it shows that, in many practical cases, block HSIC Lasso does not need more samples to achieve vanilla HSIC Lasso performance.</p>
    </sec>
    <sec>
      <title>4.2 Adjusting by covariates improves feature selection</title>
      <p>To evaluate the impact of covariate adjustment, we worked on a synthetic dataset (Section 3.2) with the following experimental parameters: <inline-formula id="IE72"><mml:math id="IM72"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></inline-formula>; <inline-formula id="IE73"><mml:math id="IM73"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>500</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>5</mml:mn></mml:mrow><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo> </mml:mo><mml:mn>000</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> features; seven causal features. Two covariates were generated by taking two causal features and adding Gaussian noise (mean = 0; standard deviation = 0.5). In the experiment shown in <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S2</xref>, we tested the ability of (block) HSIC Lasso to retrieve exclusively the remaining five causal features adjusting for the covariates. We observe that block HSIC Lasso is able to find more relevant features when it adjusts for known covariates.</p>
    </sec>
    <sec>
      <title>4.3 Block HSIC Lasso is computationally efficient</title>
      <p>In our experiments on synthetic data, vanilla HSIC Lasso runs into memory issues already with 1000 samples (<xref ref-type="fig" rid="btz333-F1">Fig. 1</xref>). This experiment shows how block HSIC Lasso keeps the good properties of HSIC Lasso, while extending it to more experimental settings. Block HSIC Lasso with <italic>B </italic>=<italic> </italic>20 reaches the memory limit only at 10 000 samples, which is already sufficient for most common bioinformatics applications. If larger datasets need to be handled, it can be done by using smaller block sizes or a larger computer cluster.</p>
      <p>We next quantified the computational efficiency improvement the block HSIC estimator brings. We compared the runtime and the peak memory usage in the highest dimensional setting where all methods could run (<inline-formula id="IE74"><mml:math id="IM74"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula>, 20 causal features) (<xref ref-type="fig" rid="btz333-F2">Fig. 2</xref>). We observe how, as expected, block HSIC Lasso requires an order of magnitude less memory than vanilla HSIC Lasso. Block versions also run notoriously faster, thanks to the lower number of operations and the parallelization. mRMR is 10 times faster than block HSIC Lasso, at the expense of a clearly lower accuracy. However, a fraction of this gap is likely due to mRMR having been implemented in C++, while HSIC Lasso is written in Python. In this regard, there is potential for other faster implementations of (block) HSIC Lasso.
</p>
      <fig id="btz333-F2" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Computational resources used by the different methods. (<bold>A</bold>) Time elapsed in a multiprocess setting. (<bold>B</bold>) Memory usage in a single-core setting. (<bold>C</bold>) Number of correct features retrieved on synthetic data (<inline-formula id="IE75"><mml:math id="IM75"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula>, 20 causal features) by block HSIC Lasso at different block sizes <italic>B</italic> and number of permutations <italic>M</italic></p>
        </caption>
        <graphic xlink:href="btz333f2"/>
      </fig>
    </sec>
    <sec>
      <title>4.4 Block HSIC Lasso improves with more permutations</title>
      <p>We were interested in the trade-off between the block size and the number of permutations, which affect both the computation time and accuracy of the result. We tested the performance of block HSIC Lasso with <inline-formula id="IE76"><mml:math id="IM76"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>60</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE77"><mml:math id="IM77"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> in datasets of <inline-formula id="IE78"><mml:math id="IM78"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> and 20 causal features. As expected, causal feature recovery increases with <italic>M</italic> and <italic>B</italic> (<xref ref-type="fig" rid="btz333-F2">Fig. 2C</xref>), as the HSIC estimator approaches its true value.</p>
      <p>The memory usage <inline-formula id="IE79"><mml:math id="IM79"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">dnBM</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of several of the conditions was the same, e.g. <italic>B </italic>=<italic> </italic>10, <italic>M </italic>=<italic> </italic>3 and <italic>B </italic>=<italic> </italic>30, <italic>M </italic>=<italic> </italic>1. Such conditions are indistinct from the points of view of both accuracy, and memory requirements. In practice, we found no major differences in runtime between different combinations of <italic>B</italic> and <italic>M</italic>. Hence, a reasonable strategy is to fix <italic>B</italic> to a given size, and tune the <italic>M</italic> to the available memory/desired amount of information. This strategy, however, should be adapted to fit properties of the data. More specifically, GWAS data are notably sparse, and as result a small block size would result in many blocks consisting entirely of zeros, which would hence be uninformative. In such cases, it might be interesting to prioritize larger block sizes, and fewer permutations.</p>
    </sec>
    <sec>
      <title>4.5 Block HSIC Lasso finds more relevant features</title>
      <p>We tested the dimensionality reduction potential of different feature selectors. We selected a variable number of features from different multi-class biological datasets, then used a random forest classifier to retrieve the original classes (Section 3.2). The underlying assumption is that only selected features which are biologically relevant will be useful to classify unseen data. To that end, we evaluated the classification ability of the biomarkers selected in four gene expression microarrays (<xref ref-type="fig" rid="btz333-F3">Fig. 3</xref>) and three scRNA-seq experiments (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S3</xref>). Unsurprisingly, we observe that non-linear feature selectors perform notably better than linear selectors. Of the non-linear methods, in virtually all cases block HSIC Lasso showed similar or superior performance to mRMR. Interestingly, as little as 20 selected genes retain enough information to achieve a plateau accuracy in most experiments.
</p>
      <fig id="btz333-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Random forest classification accuracy of microarray gene expression samples after feature extraction by the different methods. The gray line represents the mean accuracy of 10 classifiers trained on all the dataset</p>
        </caption>
        <graphic xlink:href="btz333f3"/>
      </fig>
      <p>Surveying <inline-formula id="IE80"><mml:math id="IM80"><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> SNPs in <inline-formula id="IE81"><mml:math id="IM81"><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> patients, genome-wide association (GWA) datasets are among the most high dimensional in biology, an unbalance which worsens the statistical and computational challenges. We performed the same evaluation on three WTCCC1 phenotypes (Section 3.3). As a baseline, we also computed the accuracy of a classifier trained on all the SNPs (<xref ref-type="supplementary-material" rid="sup1">Supplementary Table S1</xref>). We observe that a feature selection prior step is not always favorable: LARS worsens the classification accuracy by 5–10%. On top of that, LARS could not select any SNP in 2 out of the 15 experimental settings. On the other hand, non-linear methods improve the classification accuracy by 10%, with mRMR and block HSIC Lasso achieving similar accuracies. In fact, those two selected the same 14 out of 30 SNPs when we selected 10 SNPs in each the three datasets with each method (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. 5</xref>).</p>
    </sec>
    <sec>
      <title>4.6 Block HSIC Lasso is robust to ill-conditioned problems</title>
      <p>Single-cell RNA-seq datasets differ from microarray datasets in two ways. First, the number of features is larger, equaling the number of genes in the annotation (<inline-formula id="IE82"><mml:math id="IM82"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>20</mml:mn><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></inline-formula>). Second, the expression matrices are very sparse, due to biological variability (genes actually not expressed in a particular cell) and dropouts (genes whose expression levels have not been measured, usually because they are low, i.e. technical zeroes). In summary, the problem is severely ill conditioned, and the feature selectors need to deal with this issue. We observed that block HSIC Lasso runs reliably when faced with variations in the data, even on ill-conditioned problems like scRNA-seq. In the different scRNA-seq datasets, LARS was unable to select the requested number of biomarkers in any of the cases, returning always a lower number (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S4</xref>). mRMR did in all cases. However, the implementation of mRMR that we used crashed while selecting features on the full <xref rid="btz333-B25" ref-type="bibr">Villani <italic>et al.</italic> (2017)</xref> dataset.</p>
    </sec>
    <sec>
      <title>4.7 Block HSIC Lasso for biomarker discovery</title>
      <sec>
        <title>4.7.1 New biomarkers in mouse hippocampus scRNA-seq</title>
        <p>To study the potential of block HSIC lasso for biomarker discovery in scRNA-seq data, we focused on the mouse hippocampus dataset from <xref rid="btz333-B10" ref-type="bibr">Habib <italic>et al.</italic> (2016)</xref>, as a list of <inline-formula id="IE83"><mml:math id="IM83"><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>669</mml:mn></mml:mrow></mml:math></inline-formula> known biomarkers for the different cell types is also provided by the authors. We requested block HSIC Lasso, mRMR and LARS to select the best 20 genes for classification of 8 cell types (<xref ref-type="supplementary-material" rid="sup1">Supplementary Table S2</xref>). The cell types were four different hippocampal anatomical subregions (DG, CA1, CA2 and CA3), glial cells, ependymal, GABAergic and unidentified cells.</p>
        <p>The overlap between the genes selected by different algorithms was empty. We compared the selected genes to the known biomarkers. Out of the 20 genes selected by mRMR, 14 are known biomarkers, a number that goes down to 0 in the case of block HSIC Lasso (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S4A</xref>). Hence, these 20 genes, which are sufficient for accurately separating the cell types, are potential novel biomarkers. However, we have no reason to believe that HSIC Lasso generally has a higher tendency to return novel genes than other approaches; we merely emphasize that it suggests alternative, statistically plausible biological hypotheses that can be worth investigating.</p>
        <p>We therefore evaluated whether the novel genes found by block HSIC Lasso participate in biological functions known to be different between the cell classes. To obtain the biological processes responsible for the differences between classes, we mapped the known biomarkers to GO Biological process categories using the GO2MSIG database (<xref rid="btz333-B19" ref-type="bibr">Powell, 2014</xref>). Then we repeated the process using the genes selected by the different feature selectors, and compared the overlap between them. The overlap between the different techniques increases when we consider the biological process instead of specific genes (<xref ref-type="supplementary-material" rid="sup1">Supplementary Fig. S4B</xref>). Specifically, one biological process term that is shared between mRMR and block HSIC Lasso, ‘Adult behavior’ (associated to <italic>Sez6</italic> and <italic>Klhl1</italic>, respectively), is clearly related to hippocampus function. This reinforces the notion that the selected genes are relevant for the studied phenotypes.</p>
        <p>Then we focused on potential biomarkers and biologically interesting molecules among those genes selected by block HSIC Lasso. As it is designed specifically to select non-redundant features, often-used GO enrichment analyses are not meaningful: we expect genes belonging to the same GO annotation to be correlated, and HSIC lasso should not accumulate them. Among the top five genes, two mapped to a biological processes known to be involved: the aforementioned <italic>Klhl1</italic> and <italic>Pou3f1</italic> (related to Schwann cell development). <italic>Klhl1</italic> is a gene expressed in seven of the studied cell types and which has been related to neuron development in the past (<xref rid="btz333-B11" ref-type="bibr">He <italic>et al.</italic>, 2006</xref>). <italic>Pouf1</italic> is a transcription factor which in the past has been linked to myelination, and neurological damage in its absence (<xref rid="btz333-B12" ref-type="bibr">Jaegle <italic>et al.</italic>, 1996</xref>). The only gene among the top five that was expressed exclusively in one of the clusters is the micro RNA <italic>Mir670</italic>, expressed exclusively in CA1. According to miRDB (<xref rid="btz333-B27" ref-type="bibr">Wong and Wang, 2015</xref>), <italic>Mir670</italic> top predicted target of its 3’ arm is <italic>Pcnt</italic>, which is involved in neocortex development.</p>
      </sec>
      <sec>
        <title>4.7.2 GWAS without assumptions on genetic architecture</title>
        <p>We applied block HSIC Lasso (<italic>B </italic>=<italic> </italic>60, <italic>M </italic>=<italic> </italic>1) to three GWA datasets (Section 3.3). It is typical in GWAS to assume a genetic model before performing statistical testing of associations between SNPs and the phenotype. Two common, well-known models are the additive model—the minor allele in homozygosity has twice the effect as the minor allele in heterozygosity—and the dominant model—any number of copies of the minor allele have a phenotypic outcome. Using non-linear models such as block HSIC Lasso to explore the relationship between SNPs and outcome is attractive since no assumptions are needed on how individual SNPs affect the trait. The only assumption is that the phenotype can be explained by a combination of main effects, as block HSIC Lasso does not account for epistasis. On top of that, by penalizing the selection of redundant features, block HSIC Lasso avoids selecting multiple SNPs in high linkage disequilibrium.</p>
        <p>In our experiments, we selected 10 SNPs with block HSIC Lasso for each of the three phenotypes. These are the SNPs that best balance high relatedness to the phenotype and not giving redundant information, be it through linkage disequilibrium or through an underlying shared biological mechanism. We compared these SNPs to those selected by the univariate statistical tests implemented in PLINK 1.9 (<xref rid="btz333-B2" ref-type="bibr">Chang <italic>et al.</italic>, 2015</xref>). Some of them explicitly account for non-linearity by considering dominant and recessive models of inheritance. The number of SNPs that were positive in at least one test were disparate between the studied phenotypes: all 10 in T1D, 5 in RA, and only 2 in T2D.</p>
        <p>Specifically, we compared the genome-wide genotypic <italic>P</italic>-values to the SNPs selected by block HSIC Lasso (<xref ref-type="fig" rid="btz333-F4">Fig. 4</xref>). In T1D, block HSIC Lasso selected SNPs among those with the most extreme p-values. However, not being constrained by a conservative <italic>P</italic>-value threshold, block HSIC Lasso selects five and eight SNPs in RA and T2D, respectively, with non-Bonferroni significant <italic>P</italic>-values when they improve classification accuracy Interestingly, one of these SNPs can be physically mapped to PFKM (<xref rid="btz333-B14" ref-type="bibr">Keildson <italic>et al.</italic>, 2014</xref>), a gene previously identified in genome-wide studies of T2D. The selected SNPs are scattered all across the genome, displaying the lack of redundancy between them. This strategy gives a more representative set of SNPs than other approaches common in bioinformatics, like selecting the smallest 10 <italic>P</italic>-values.
</p>
        <fig id="btz333-F4" orientation="portrait" position="float">
          <label>Fig. 4.</label>
          <caption>
            <p>Manhattan plot of the GWA datasets using <italic>P</italic>-values from the genotypic test. A constant of <inline-formula id="IE84"><mml:math id="IM84"><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>220</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> was added to all <italic>P</italic>-values to allow plotting <italic>P</italic>-values of 0. SNPs in black are the SNPs selected by block HSIC Lasso (<italic>B </italic>=<italic> </italic>20), 10 per phenotype. When SNPs are located within the boundaries of a gene (±50 kb), the gene name is indicated. The red line represents the Bonferroni threshold with <inline-formula id="IE85"><mml:math id="IM85"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula></p>
          </caption>
          <graphic xlink:href="btz333f4"/>
        </fig>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion</title>
    <p>In this work, we presented block HSIC Lasso, a non-linear feature selector. Block HSIC Lasso retains the properties of HSIC Lasso while extending its applicability to larger datasets. Among the attractive properties of block HSIC Lasso we find, first, its ability to handle both linear and non-linear relationships between the variables and the outcome. Second, block HSIC Lasso has a convex formulation, ensuring that a global solution exists, and that it is accessible. Third, the HSIC score can be accurately estimated, as opposed to other measures of non-linearity like mutual information. Fourth, block HSIC Lasso’s memory consumption scales linearly with respect to both the number of features and the number of samples. In addition, block HSIC Lasso can be easily adapted to different problems via different kernel functions that better capture similarities in new datasets. Lastly, block HSIC Lasso can be adjusted for covariates known to affect the outcome, which helps removing confounding effects from the analysis. Due to all these properties, we show how block HSIC Lasso outperforms all other algorithms in the tested conditions.</p>
    <p>Block HSIC Lasso can be applied to different kinds of datasets. As other non-linear methods, block HSIC Lasso is particularly useful when we do not want to make strong assumptions about how the causal variables relate to the outcome. Thanks to the advantages mentioned above, HSIC Lasso and block HSIC Lasso tend to outperform other state-of-the-art approaches in terms of both causal features retrieval in simulated data, and classification accuracy on real-world datasets.</p>
    <p>Whereas the Lasso is limited to selecting at most as many features as there are available samples (<italic>n</italic>), for block HSIC Lasso the limitation is <italic>nBM</italic>. Hence, even if the number of samples is small, block HSIC Lasso can be used to select a larger number of features. If <italic>nBM</italic> is still limiting, one could replace the <inline-formula id="IE86"><mml:math id="IM86"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> regularization with an elastic-net regularization. However, in most cases, we expect block HSIC Lasso to be used to select a small number of features.</p>
    <p>Regarding its potential in bioinformatics, we applied block HSIC Lasso to images, microarrays, single-cell RNA-seq and GWAS. The two latter involve thousands of samples, making it unfeasible to run vanilla HSIC Lasso on a regular server because of its memory requirements. The selected biomarkers are biologically plausible, agree with the outcome of other methods and provide a good classification accuracy when used to train a classifier. Such a ranking is useful, for instance, when selecting SNPs or genes to assay in <italic>in vitro</italic> experiments.</p>
    <p>Block HSIC Lasso’s main drawback is the memory complexity, markedly lower than in vanilla HSIC Lasso but still <inline-formula id="IE87"><mml:math id="IM87"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">dnB</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Memory issues might appear in low-memory servers in cases with a large number of samples <italic>n</italic>, of features <italic>d</italic>, or both. However, through our work on GWA datasets, the largest type of dataset in bioinformatics, we show that working on these datasets is feasible. Another drawback, which block HSIC Lasso shares with the other non-linear methods, is their black box nature. Block HSIC Lasso looks for biomarkers which, after an unknown, non-linear transformation, would allow a linear separation between the samples. Unfortunately, we cannot access this transformed space and explore it, which makes the results hard to interpret.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>Computational resources and support were provided by RIKEN AIP. H.C-G. was funded by the European Union’s Horizon 2020 research and innovation program under the Marie Skłodowska-Curie [666003]. S.K. was supported by the Academy of Finland (292334, 319264). M.Y. was supported by the JST PRESTO program JPMJPR165A and partly supported by MEXT KAKENHI 16H06299 and the RIKEN engineering network funding.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz333_Supplementary_Data</label>
      <media xlink:href="btz333_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz333-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Burton</surname><given-names>P.R.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Genome-wide association study of 14,000 cases of seven common diseases and 3,000 shared controls</article-title>. <source>Nature</source>, <volume>447</volume>, <fpage>661</fpage>–<lpage>678</lpage>.<pub-id pub-id-type="pmid">17554300</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>C.C.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Second-generation PLINK: rising to the challenge of larger and richer datasets</article-title>. <source>GigaScience</source>, <volume>4</volume>, <fpage>7</fpage>.<pub-id pub-id-type="pmid">25722852</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Clarke</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>The properties of high-dimensional data spaces: implications for exploring gene and protein expression data</article-title>. <source>Nat. Rev. Cancer</source>, <volume>8</volume>, <fpage>37</fpage>–<lpage>49</lpage>.<pub-id pub-id-type="pmid">18097463</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Cover</surname><given-names>T.M.</given-names></name>, <name name-style="western"><surname>Thomas</surname><given-names>J.A.</given-names></name></person-group> (<year>2006</year>) <source>Elements of Information Theory</source>, <edition>2</edition>nd edn. 
<publisher-name>John Wiley &amp; Sons, Inc</publisher-name>, 
<publisher-loc>Hoboken, NJ</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz333-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ding</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>) 
<article-title>Minimum redundancy feature selection from microarray gene expression data</article-title>. <source>J. Bioinform. Comput. Biol</source>., <volume>3</volume>, <fpage>185</fpage>–<lpage>205</lpage>.<pub-id pub-id-type="pmid">15852500</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Efron</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>Least angle regression</article-title>. <source>Ann. Statist</source>., <volume>32</volume>, <fpage>407</fpage>–<lpage>499</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Fujishige</surname><given-names>S.</given-names></name></person-group> (<year>2005</year>) <source>Submodular Functions and Optimization</source>, Vol. <volume>58</volume>
<publisher-name>Elsevier</publisher-name>, 
<publisher-loc>Boston</publisher-loc>. </mixed-citation>
    </ref>
    <ref id="btz333-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gretton</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) 
<article-title>Measuring statistical dependence with Hilbert–Schmidt norms</article-title>. In: <source>International Conference on Algorithmic Learning Theory (ALT), Singapore</source>, pp. <fpage>63</fpage>–<lpage>77</lpage>. </mixed-citation>
    </ref>
    <ref id="btz333-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Haber</surname><given-names>A.L.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>A single-cell survey of the small intestinal epithelium</article-title>. <source>Nature</source>, <volume>551</volume>, <fpage>333</fpage>–<lpage>339</lpage>.<pub-id pub-id-type="pmid">29144463</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Habib</surname><given-names>N.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Div-Seq: single-nucleus RNA-Seq reveals dynamics of rare adult newborn neurons</article-title>. <source>Science</source>, <volume>353</volume>, <fpage>925</fpage>–<lpage>928</lpage>.<pub-id pub-id-type="pmid">27471252</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Targeted deletion of a single Sca8 ataxia locus allele in mice causes abnormal gait, progressive loss of motor coordination, and Purkinje cell dendritic deficits</article-title>. <source>J. Neurosci</source>., <volume>26</volume>, <fpage>9975</fpage>–<lpage>9982</lpage>.<pub-id pub-id-type="pmid">17005861</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jaegle</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>1996</year>) 
<article-title>The POU factor Oct-6 and Schwann cell differentiation</article-title>. <source>Science</source>, <volume>273</volume>, <fpage>507</fpage>–<lpage>510</lpage>.<pub-id pub-id-type="pmid">8662541</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Johnstone</surname><given-names>I.M.</given-names></name>, <name name-style="western"><surname>Titterington</surname><given-names>D.M.</given-names></name></person-group> (<year>2009</year>) 
<article-title>Statistical challenges of high-dimensional data</article-title>. <source>Philos. Trans. Series A Math. Phys. Eng. Sci</source>., <volume>367</volume>, <fpage>4237</fpage>–<lpage>4253</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Keildson</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Expression of phosphofructokinase in skeletal muscle is influenced by genetic variation and associated with insulin sensitivity</article-title>. <source>Diabetes</source>, <volume>63</volume>, <fpage>1154</fpage>–<lpage>1165</lpage>.<pub-id pub-id-type="pmid">24306210</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Feature selection: a data perspective</article-title>. <source>ACM Comp. Surveys</source>, <volume>50</volume>, <fpage>94.</fpage></mixed-citation>
    </ref>
    <ref id="btz333-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mairal</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Online learning for matrix factorization and sparse coding</article-title>. <source>J. Mach. Learn. Res</source>., <volume>11</volume>, <fpage>19</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>H.</given-names></name></person-group> (<year>2005</year>) mrmr. <ext-link ext-link-type="uri" xlink:href="http://home.penglab.com/proj/mRMR/">http://home.penglab.com/proj/mRMR/</ext-link> (15 June 2018, date last accessed). </mixed-citation>
    </ref>
    <ref id="btz333-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Peng</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) 
<article-title>Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>27</volume>, <fpage>1226</fpage>–<lpage>1237</lpage>.<pub-id pub-id-type="pmid">16119262</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Powell</surname><given-names>J.A.C.</given-names></name></person-group> (<year>2014</year>) 
<article-title>GO2MSIG, an automated GO based multi-species gene set generator for gene set enrichment analysis</article-title>. <source>BMC Bioinformatics</source>, <volume>15</volume>, <fpage>146.</fpage><pub-id pub-id-type="pmid">24884810</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ravikumar</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Sparse additive models</article-title>. <source>J. R. Statist. Soc. Series B Statist. Methodol</source>., <volume>71</volume>, <fpage>1009</fpage>–<lpage>1030</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Schölkopf</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Smola</surname><given-names>A.J.</given-names></name></person-group> (<year>2002</year>) <source>Learning with Kernels</source>. 
<publisher-name>MIT Press</publisher-name>, 
<publisher-loc>Cambridge, MA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btz333-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Feature selection via dependence maximization</article-title>. <source>J. Mach. Learn. Res</source>., <volume>13</volume>, <fpage>1393</fpage>–<lpage>1434</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group> (<year>1996</year>) 
<article-title>Regression shrinkage and selection via the Lasso</article-title>. <source>J. R. Statist. Soc. Series B Methodol</source>., <volume>58</volume>, <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>van Dijk</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Recovering gene interactions from single-cell data using data diffusion</article-title>. <source>Cell</source>, <volume>174</volume>, <fpage>716</fpage>–<lpage>729</lpage>.<pub-id pub-id-type="pmid">29961576</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Villani</surname><given-names>A.-C.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Single-cell RNA-seq reveals new types of human blood dendritic cells, monocytes, and progenitors</article-title>. <source>Science</source>, <volume>356</volume>, <fpage>925</fpage>–<lpage>928</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Walters-Williams</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group> (<year>2009</year>) <chapter-title>Estimation of mutual information: a survey</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Wen</surname><given-names>P.</given-names></name></person-group><etal>et al</etal> (eds), <source>Rough Sets and Knowledge Technology</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>389</fpage>–<lpage>396</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wong</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>X.</given-names></name></person-group> (<year>2015</year>) 
<article-title>miRDB: an online resource for microRNA target prediction and functional annotations</article-title>. <source>Nucleic Acids Res</source>., <volume>43</volume>, <fpage>D146</fpage>–<lpage>D152</lpage>.<pub-id pub-id-type="pmid">25378301</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yamada</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>High-dimensional feature selection by feature-wise kernelized lasso</article-title>. <source>Neural Computation</source>, <volume>26</volume>, <fpage>185</fpage>–<lpage>207</lpage>.<pub-id pub-id-type="pmid">24102126</pub-id></mixed-citation>
    </ref>
    <ref id="btz333-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yamada</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Ultra high-dimensional nonlinear feature selection for big biological data</article-title>. <source>IEEE Trans. Knowl. Data Eng</source>., <volume>30</volume>, <fpage>1352</fpage>–<lpage>1365</lpage>.</mixed-citation>
    </ref>
    <ref id="btz333-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Large-scale kernel methods for independence testing</article-title>. <source>Statist. Comput</source>., <volume>28</volume>, <fpage>113</fpage>–<lpage>130</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
