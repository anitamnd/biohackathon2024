<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">sensors</journal-id>
    <journal-title-group>
      <journal-title>Sensors (Basel, Switzerland)</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1424-8220</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7085520</article-id>
    <article-id pub-id-type="doi">10.3390/s20051439</article-id>
    <article-id pub-id-type="publisher-id">sensors-20-01439</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>VersaVIS—An Open Versatile Multi-Camera Visual-Inertial Sensor Suite</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7346-2941</contrib-id>
        <name>
          <surname>Tschopp</surname>
          <given-names>Florian</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
        <xref rid="c1-sensors-20-01439" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7954-6468</contrib-id>
        <name>
          <surname>Riner</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3276-4067</contrib-id>
        <name>
          <surname>Fehr</surname>
          <given-names>Marius</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
        <xref ref-type="aff" rid="af2-sensors-20-01439">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3162-0363</contrib-id>
        <name>
          <surname>Bernreiter</surname>
          <given-names>Lukas</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6070-1760</contrib-id>
        <name>
          <surname>Furrer</surname>
          <given-names>Fadri</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0209-5915</contrib-id>
        <name>
          <surname>Novkovic</surname>
          <given-names>Tonci</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pfrunder</surname>
          <given-names>Andreas</given-names>
        </name>
        <xref ref-type="aff" rid="af3-sensors-20-01439">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2972-6011</contrib-id>
        <name>
          <surname>Cadena</surname>
          <given-names>Cesar</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2760-7983</contrib-id>
        <name>
          <surname>Siegwart</surname>
          <given-names>Roland</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4808-0831</contrib-id>
        <name>
          <surname>Nieto</surname>
          <given-names>Juan</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-20-01439">1</xref>
      </contrib>
    </contrib-group>
    <aff id="af1-sensors-20-01439"><label>1</label>Autonomous Systems Lab, ETH Zurich, 8092 Zurich, Switzerland; <email>michael.riner-kuhn@mavt.ethz.ch</email> (M.R.); <email>marius.fehr@mavt.ethz.ch</email> (M.F.); <email>lukas.bernreiter@mavt.ethz.ch</email> (L.B.); <email>fadri.furrer@mavt.ethz.ch</email> (F.F.); <email>tonci.novkovic@mavt.ethz.ch</email> (T.N.); <email>cesarc@ethz.ch</email> (C.C.); <email>rsiegwart@ethz.ch</email> (R.S.); <email>nietoj@ethz.ch</email> (J.N.)</aff>
    <aff id="af2-sensors-20-01439"><label>2</label>Voliro Airborne Robotics, 8092 Zurich, Switzerland</aff>
    <aff id="af3-sensors-20-01439"><label>3</label>Sevensense Robotics, 8092 Zurich, Switzerland; <email>andreas.pfrunder@sevensense.ch</email></aff>
    <author-notes>
      <corresp id="c1-sensors-20-01439"><label>*</label>Correspondence: <email>florian.tschopp@mavt.ethz.ch</email>; Tel.: +41-44-632-0318</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>06</day>
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <volume>20</volume>
    <issue>5</issue>
    <elocation-id>1439</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>1</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>03</day>
        <month>3</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 by the authors.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="open-access">
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Robust and accurate pose estimation is crucial for many applications in mobile robotics. Extending visual Simultaneous Localization and Mapping (SLAM) with other modalities such as an inertial measurement unit (IMU) can boost robustness and accuracy. However, for a tight sensor fusion, accurate time synchronization of the sensors is often crucial. Changing exposure times, internal sensor filtering, multiple clock sources and unpredictable delays from operation system scheduling and data transfer can make sensor synchronization challenging. In this paper, we present VersaVIS, an Open Versatile Multi-Camera Visual-Inertial Sensor Suite aimed to be an efficient research platform for easy deployment, integration and extension for many mobile robotic applications. VersaVIS provides a complete, open-source hardware, firmware and software bundle to perform time synchronization of multiple cameras with an IMU featuring exposure compensation, host clock translation and independent and stereo camera triggering. The sensor suite supports a wide range of cameras and IMUs to match the requirements of the application. The synchronization accuracy of the framework is evaluated on multiple experiments achieving timing accuracy of less than <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Furthermore, the applicability and versatility of the sensor suite is demonstrated in multiple applications including visual-inertial SLAM, multi-camera applications, multi-modal mapping, reconstruction and object based mapping.</p>
    </abstract>
    <kwd-group>
      <kwd>visual-inertial SLAM</kwd>
      <kwd>time synchronization</kwd>
      <kwd>sensor fusion</kwd>
      <kwd>embedded</kwd>
      <kwd>camera</kwd>
      <kwd>IMU</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-sensors-20-01439">
    <title>1. Introduction</title>
    <p>Autonomous mobile robots are well established in controlled environments such as factories where they rely on external infrastructure such as magnetic tape on the floor or beacons. However, in unstructured and changing environments, robots need to be able to plan their way and interact with their environment which, as a first step, requires accurate positioning [<xref rid="B1-sensors-20-01439" ref-type="bibr">1</xref>]. In mobile robotic applications, visual sensors can provide solutions for odometry and Simultaneous Localization and Mapping (SLAM), achieving good accuracy and robustness. Using additional sensor modalities such as inertial measurement units (IMUs) [<xref rid="B2-sensors-20-01439" ref-type="bibr">2</xref>,<xref rid="B3-sensors-20-01439" ref-type="bibr">3</xref>,<xref rid="B4-sensors-20-01439" ref-type="bibr">4</xref>] can additionally improve robustness and accuracy for a wide range of applications. For many frameworks, a time offset or delay between modalities can lead to bad results or even render the whole approach unusable. There are frameworks such as VINS-Mono [<xref rid="B4-sensors-20-01439" ref-type="bibr">4</xref>] that can estimate a time offset during estimation, however, convergence of the estimation can be improved by accurate timestamping, that is, assigning timestamps to the sensor data that precisely correspond to measurement time from a common clock across all sensors. For a reliable and accurate sensor fusion, all sensors need to provide timestamps matchable between sensor modalities. Typically, the readout of camera image and IMU measurement are not on the same device resulting in a clock offset between the two timestamps which is hard to predict due to universal serial bus (USB) buffer delay, operating system (OS) scheduling, changing exposure times and internal sensor filtering. Therefore, measurement correspondences between modalities are ambiguous and assignment on the host is not trivial. Some of those challenges can be improved on using passive synchronization algorithms [<xref rid="B5-sensors-20-01439" ref-type="bibr">5</xref>]. However, unobservable time delays still remain. TriggerSync [<xref rid="B6-sensors-20-01439" ref-type="bibr">6</xref>] proposes a synchronization framework for triggered sensors that does not require additional hardware. However, this only works when all sensors are triggered simultaneously, therefore rendering exposure compensation impossible (see <xref ref-type="sec" rid="sec2dot1dot1-sensors-20-01439">Section 2.1.1</xref>). Furthermore, TriggerSync is not robust against wrong association of trigger pulses due to unexpected delays and can achieve high accuracy only when a low latency connection to the host computer is available such as RS-232 [<xref rid="B7-sensors-20-01439" ref-type="bibr">7</xref>]. A very recent combined software-hardware synchronization method is described by Lu et al. [<xref rid="B8-sensors-20-01439" ref-type="bibr">8</xref>] which is also based on strictly simultaneously triggered sensors not taking changing exposure times into account.</p>
    <p>Currently, there is no reference sensor synchronization framework for data collection, which makes it hard to compare results obtained in various publications that deal with visual-inertial (VI) SLAM.</p>
    <p>Commercially and academically established sensors such as the Skybotix VI-Sensor [<xref rid="B9-sensors-20-01439" ref-type="bibr">9</xref>] used in the EuROC micro aerial vehicle (MAV) dataset [<xref rid="B10-sensors-20-01439" ref-type="bibr">10</xref>] and the PennCOSYVIO dataset [<xref rid="B11-sensors-20-01439" ref-type="bibr">11</xref>], Intel RealSense [<xref rid="B12-sensors-20-01439" ref-type="bibr">12</xref>], SkyAware sensor based on work from Honegger et al. [<xref rid="B13-sensors-20-01439" ref-type="bibr">13</xref>] or PIRVS [<xref rid="B14-sensors-20-01439" ref-type="bibr">14</xref>] are either unavailable and/or are limited in hardware configuration regarding image sensor, lens, camera baseline and IMU. Furthermore, it is often impossible to add further extensions to those frameworks to enable fusion with other modalities such as Light Detection and Ranging sensor (LiDAR) sensors or external illumination.</p>
    <p>Different public datasets such as the KITTI dataset [<xref rid="B15-sensors-20-01439" ref-type="bibr">15</xref>], the North Campus Long-Term (NCLT) dataset [<xref rid="B16-sensors-20-01439" ref-type="bibr">16</xref>] or the Zurich Urban MAV dataset [<xref rid="B17-sensors-20-01439" ref-type="bibr">17</xref>] all feature vision and inertial sensors. However, the sensor modalities are not synchronized in hardware rendering multiple VI SLAM approaches challenging. The most similar work to ours can be found in the Technische Universität München (TUM) Visual-Inertial dataset [<xref rid="B18-sensors-20-01439" ref-type="bibr">18</xref>]. They combine two cameras triggered on a Genuino 101 with an IMU read out by the same microcontroller unit (MCU). Exposure times are estimated using an on-board light sensor. Unfortunately, the sensor system was not evaluated, it is not publicly available and details about exposure-time compensation, host synchronization and overall time synchronization accuracy are omitted.</p>
    <p>In this paper, we introduce VersaVIS (available at: <uri xlink:href="www.github.com/ethz-asl/versavis">www.github.com/ethz-asl/versavis</uri>) shown in <xref ref-type="fig" rid="sensors-20-01439-f001">Figure 1</xref>, the first open-source framework that is able to accurately synchronize a large range of camera and IMU sensors. The sensor suite is aimed for the research community to enable rapid prototyping of affordable sensor setups in different fields in mobile robotics where visual navigation is important. Special emphasis is put on an easy integration for different applications and easy extensibility by being based on well-known open-source frameworks such as Arduino [<xref rid="B19-sensors-20-01439" ref-type="bibr">19</xref>] and the Robot Operating System (ROS) [<xref rid="B20-sensors-20-01439" ref-type="bibr">20</xref>].</p>
    <p>The remainder of the paper is organized as follows—in <xref ref-type="sec" rid="sec2-sensors-20-01439">Section 2</xref>, the sensor suite is described in detail including all of its features. <xref ref-type="sec" rid="sec3-sensors-20-01439">Section 3</xref> provides evaluations of the synchronization accuracy of the proposed framework. Finally, <xref ref-type="sec" rid="sec4-sensors-20-01439">Section 4</xref> showcases the use of the Open Versatile Multi-Camera Visual-Inertial Sensor Suite (VersaVIS) in multiple applications while <xref ref-type="sec" rid="sec5-sensors-20-01439">Section 5</xref> provides a conclusion with an outlook on future work.</p>
  </sec>
  <sec id="sec2-sensors-20-01439">
    <title>2. The Visual-Inertial Sensor Suite</title>
    <p>The proposed sensor suite consists of three different parts, (i) the firmware which runs on the MCU, (ii) the host driver running on a ROS enabled machine, and (iii) the hardware trigger printed circuit board (PCB). An overview of the framework is provided in <xref ref-type="fig" rid="sensors-20-01439-f002">Figure 2</xref>. Here, the procedure is described for a reference setup consisting of two cameras and a Serial Peripheral Interface (SPI) enabled IMU.</p>
    <p>The core component of VersaVIS is the MCU. First of all, it is used to periodically trigger the IMU readout together with setting the timestamps and sending the data to the host. Furthermore, the MCU sends triggering pulses to the cameras to start image exposure. This holds for both independent cameras and stereo cameras (see <xref ref-type="sec" rid="sec2dot1-sensors-20-01439">Section 2.1</xref>). After successful exposure, the MCU reads the exposure time by listening to the cameras’ exposure signal in order to perform exposure compensation described in <xref ref-type="sec" rid="sec2dot1dot1-sensors-20-01439">Section 2.1.1</xref> and setting mid-exposure timestamps. The timestamps are sent to the host together with a strictly increasing sequence number. The image data is hereby sent directly from the camera to the host computer to avoid massive amounts of data through the MCU. This enables to use high-resolution cameras even with a low-performance MCU.</p>
    <p>Finally, the host computer merges image timestamps from the MCU with the corresponding image messages based on a sequence number (see <xref ref-type="sec" rid="sec2dot1dot1-sensors-20-01439">Section 2.1.1</xref>).</p>
    <sec id="sec2dot1-sensors-20-01439">
      <title>2.1. Firmware</title>
      <p>The MCU is responsible for triggering the devices at the correct time and to capture timestamps of the triggered sensor measurements. This is based on the usage of hardware timers and external signal interrupts.</p>
      <sec id="sec2dot1dot1-sensors-20-01439">
        <title>2.1.1. Standard Cameras</title>
        <p>In the scope of the MCU, standard cameras are considered sensors that are triggerable with signal pulses and with non-zero data measurement time, that is, image exposure time. Furthermore, the sensors need to provide an exposure signal (often called strobe signal) which indicates the exposure state of the sensor. While the trigger pulse and the timestamp are both created on the MCU based on its internal clock, the image data is transferred via USB or Ethernet directly to the host computer. To enable correct association of the image data and timestamp on the host computer (see <xref ref-type="sec" rid="sec2dot2dot1-sensors-20-01439">Section 2.2.1</xref>), both the timestamp from VersaVIS and the image data are assigned an independent sequence number <inline-formula><mml:math id="mm2"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm3"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, respectively. The mapping between these sequence numbers is determined during initialization as a simultaneous start of the cameras and the trigger board cannot be guaranteed.
<list list-type="bullet"><list-item><p>Initialization procedure: After startup of the camera and trigger board, corresponding sequence numbers are found by very slowly triggering the camera without exposure time compensation. Corresponding sequence numbers are then determined by closest timestamps, see <xref ref-type="sec" rid="sec2dot2dot1-sensors-20-01439">Section 2.2.1</xref>. This holds true if the triggering period time is significantly longer than the expected delay and jitter on the host computer. As soon as the sequence number offset <inline-formula><mml:math id="mm4"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is determined, exposure compensation mode at full frame rate can be used.</p></list-item><list-item><p>Exposure time compensation: Performing auto-exposure (AE), the camera adapts its exposure time to the current illumination resulting in a non-constant exposure time. Furgale et al. [<xref rid="B22-sensors-20-01439" ref-type="bibr">22</xref>] showed, that mid-exposure timestamping is beneficial for image based state estimation, especially when using global shutter cameras. Instead of periodically triggering the camera, a scheme proposed by Nikolic et al. [<xref rid="B9-sensors-20-01439" ref-type="bibr">9</xref>] is employed. The idea is to trigger the camera for a periodic mid-exposure timestamp by starting exposure half the exposure time earlier to its mid-exposure timestamp as shown in <xref ref-type="fig" rid="sensors-20-01439-f003">Figure 3</xref> for <monospace>cam0</monospace>, <monospace>cam1</monospace> and <monospace>cam2</monospace>. The exposure time return signal is used to time the current exposure time and calculate the offset to the mid-exposure timestamp of the next image. Using this approach, corresponding measurements can be obtained even if multiple cameras do not share the same exposure time (e.g., <monospace>cam0</monospace> and <monospace>cam2</monospace> in <xref ref-type="fig" rid="sensors-20-01439-f003">Figure 3</xref>).</p></list-item><list-item><p>Master-slave mode: Using two cameras in a stereo setup compared to a monocular camera can retrieve metric scale by stereo matching. This can enable certain applications where IMU excitation is not high enough and therefore biases are not fully observable without this scale input for example, for rail vehicles described in <xref ref-type="sec" rid="sec4dot2-sensors-20-01439">Section 4.2</xref>. Furthermore, it can also provide more robustness. To perform accurate and efficient stereo matching, it is highly beneficial if keypoints from the same spot have a similar appearance. This can be achieved by using the exact same exposure time on both cameras. Thereby, one camera serves as the master performing AE while the other adapts its exposure time. This is achieved by routing the exposure signal from <monospace>cam0</monospace> directly to the trigger of <monospace>cam1</monospace> and also using it to determine the exposure time for compensation.</p></list-item></list></p>
      </sec>
      <sec id="sec2dot1dot2-sensors-20-01439">
        <title>2.1.2. Other Triggerable Sensors</title>
        <p>Some sensors enable measurement triggering but do not require or offer the possibility to do exposure compensation for example, thermal cameras or ToF cameras. These typically do not allow for adaptive exposure compensation but rather have a fixed exposure/integration time. They can be treated the same as a standard camera, but with fixed exposure time.</p>
        <p>Sensors that provide immediate measurements (such as external IMUs) do not need an exposure time compensation and can just use a standard timer. Note that the timestamp for those sensors are still captured on the MCU and therefore correspond to the other sensor modalities.</p>
      </sec>
      <sec id="sec2dot1dot3-sensors-20-01439">
        <title>2.1.3. Other Non-Triggerable Sensors</title>
        <p>In robotics, it is often useful to perform sensor fusion with multiple available sensor modalities such as wheel odometers or LiDAR sensors [<xref rid="B23-sensors-20-01439" ref-type="bibr">23</xref>]. Most of such sensor hardware do not allow triggering or low-level sensor readout but send the data continuously to a host computer. In order to enable precise and accurate sensor fusion, having corresponding timestamps of all sensor modalities is often crucial.</p>
        <p>As most such additional sensors produce their timestamps based on the host clock or synchronized to the host clock, VersaVIS performs time <italic>translation</italic> to the host. In this context, clock <italic>synchronization</italic> refers to modifying the slave clock speed to align to the master clock while clock <italic>translation</italic> refers to translating the timestamp of the slave clock to the time of the master clock [<xref rid="B7-sensors-20-01439" ref-type="bibr">7</xref>]. An on-board Kalman filter (KF) is deployed to estimate clock skew <inline-formula><mml:math id="mm5"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula> and clock offset <inline-formula><mml:math id="mm6"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:math></inline-formula> using
<disp-formula id="FD1-sensors-20-01439"><label>(1)</label><mml:math id="mm7"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>·</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="sans-serif">Δ</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm8"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm9"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the timestamps on the host and slave (in this case the VersaVIS MCU) respectively, <italic>k</italic> is the update step, <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula> is the time since the last KF update and <inline-formula><mml:math id="mm11"><mml:mrow><mml:mi mathvariant="sans-serif">Δ</mml:mi></mml:mrow></mml:math></inline-formula> refers to the initial clock offset set at the initial connection between host and slave.</p>
        <p>Periodically, VersaVIS performs a filter update by requesting the current time of the host
<disp-formula id="FD2-sensors-20-01439"><label>(2)</label><mml:math id="mm12"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced separators="" open="(" close=")"><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm13"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the time at sending the request and <inline-formula><mml:math id="mm14"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the time when receiving the answer from the host assuming that communication time delay between host and slave is symmetric. The filter update is then performed using standard KF equations:<disp-formula id="FD3-sensors-20-01439"><label>(3)</label><mml:math id="mm15"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>·</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mover accent="true"><mml:mi>η</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>η</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mover accent="true"><mml:mi mathvariant="sans-serif">P</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>·</mml:mo><mml:msup><mml:mi mathvariant="sans-serif">P</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:msup><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>⊤</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mfenced open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mi>η</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm16"><mml:mrow><mml:mover accent="true"><mml:mo>·</mml:mo><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> depicts the prediction, <inline-formula><mml:math id="mm17"><mml:mrow><mml:mi mathvariant="sans-serif">P</mml:mi></mml:mrow></mml:math></inline-formula> is the covariance matrix and <inline-formula><mml:math id="mm18"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the noise parameters of the clock offset and clock skew, respectively. The measurement residual <inline-formula><mml:math id="mm19"><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> can be written as
<disp-formula id="FD4-sensors-20-01439"><label>(4)</label><mml:math id="mm20"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mi>ϵ</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mi mathvariant="sans-serif">Δ</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where the Kalman gain <inline-formula><mml:math id="mm21"><mml:mrow><mml:mi mathvariant="sans-serif">K</mml:mi></mml:mrow></mml:math></inline-formula> and the measurement update can be derived using the standard KF equations.</p>
      </sec>
    </sec>
    <sec id="sec2dot2-sensors-20-01439">
      <title>2.2. Host Computer Driver</title>
      <p>The host computer needs to run a lightweight application in order to make sure the data from VersaVIS can be correctly used in the ROS environment.</p>
      <sec id="sec2dot2dot1-sensors-20-01439">
        <title>2.2.1. Synchronizer</title>
        <p>The host computer needs to take care of merging together the image data directly from the camera sensors and the image timestamps from the VersaVIS triggering board.</p>
        <p>During initialization, timestamps from VersaVIS <inline-formula><mml:math id="mm22"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and timestamps from image data <inline-formula><mml:math id="mm23"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are assigned based on minimal time difference within a threshold for each connected camera separately such as
<disp-formula id="FD5-sensors-20-01439"><label>(5)</label><mml:math id="mm24"><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo form="prefix">arg min</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:msubsup><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm25"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the sequence number offset, <inline-formula><mml:math id="mm26"><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm27"><mml:mrow><mml:msub><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the sets of available sequence numbers from the camera and from VersaVIS, respectively, and <inline-formula><mml:math id="mm28"><mml:mrow><mml:msup><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the timestamp corresponding to the sequence number <italic>n</italic>. As the images are triggered very slowly (e.g., <inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mi>Hz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>), the USB buffer and OS scheduling jitter is assumed to be negligible.</p>
        <p>As soon as <inline-formula><mml:math id="mm30"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is constant and time offsets are small, the trigger board is notified about the initialization status of the camera. With all cameras initialized, normal triggering mode (e.g., high frequency) can be activated.</p>
        <p>During normal mode, image data (directly from the camera) and image timestamps (from VersaVIS triggering board) are associated based on the sequence number like
<disp-formula id="FD6-sensors-20-01439"><label>(6)</label><mml:math id="mm31"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:msubsup><mml:mo>≡</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      </sec>
      <sec id="sec2dot2dot2-sensors-20-01439">
        <title>2.2.2. IMU Receiver</title>
        <p>In addition to the camera data, in a setup where the IMU is triggered and read out by the VersaVIS triggering board, the IMU message should only hold minimal information to minimize bandwidth requirements and therefore needs to be reassembled into a full IMU message on the host computer.</p>
      </sec>
    </sec>
    <sec id="sec2dot3-sensors-20-01439">
      <title>2.3. VersaVIS Triggering Board</title>
      <p>One main part of VersaVIS is the triggering board which is a MCU-based custom PCB shown in <xref ref-type="fig" rid="sensors-20-01439-f001">Figure 1</xref>d that is used to connect all sensors and performs sensor synchronization. For easy extensibility and integration, the board is compatible with the Arduino environment [<xref rid="B19-sensors-20-01439" ref-type="bibr">19</xref>]. In the reference design, the board supports up to three independently triggered cameras with a four pin connector. Furthermore, SPI, Inter-Integrated Circuit (I<inline-formula><mml:math id="mm32"><mml:mrow><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>C) or Universal Asynchronous Receiver Transmitter (UART) can be used to interface with an IMU or other sensors. <xref rid="sensors-20-01439-t001" ref-type="table">Table 1</xref> shows the specifications of the triggering board. The board is connected to the host computer using USB and communicates with ROS using <italic>rosserial</italic> [<xref rid="B24-sensors-20-01439" ref-type="bibr">24</xref>].</p>
    </sec>
  </sec>
  <sec id="sec3-sensors-20-01439">
    <title>3. Evaluations</title>
    <p>In this section, several evaluations are carried out that show the synchronization accuracy of different modules of the VersaVIS framework.</p>
    <sec id="sec3dot1-sensors-20-01439">
      <title>3.1. Camera-Camera</title>
      <p>The first important characteristic of a good multi-camera time synchronization is that multiple corresponding camera images capture the same information. This is especially important when multiple cameras are used for state estimation (see <xref ref-type="sec" rid="sec4dot2-sensors-20-01439">Section 4.2</xref>).</p>
      <p>For the purpose of evaluating the synchronization accuracy of multi-camera triggering, we captured a stream of images of a light emitting diode (LED) timing board shown in <xref ref-type="fig" rid="sensors-20-01439-f004">Figure 4</xref> with two independently triggered but synchronized and exposure-compensated cameras. The board features eight counting LEDs (the left and right most LEDs are always on and used for position reference). The board is changing state whenever a trigger is received. The LEDs are organized in two groups. The right group of four indicates one count each, while the left group is binary encoded resulting in a counter overflow at 64. The board is triggered with <inline-formula><mml:math id="mm33"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mi>kHz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> aligned with the mid-exposure timestamps of the images. Furthermore, both cameras are operated at a rate of <inline-formula><mml:math id="mm34"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo> </mml:mo><mml:mi>Hz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and with a fixed exposure time of <inline-formula><mml:math id="mm35"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p>For a successful synchronization of the sensors, images captured at the same time-step <italic>i</italic> with both cameras should show the same bit count <inline-formula><mml:math id="mm36"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with at most two of the striding LEDs on (since the board changes state at mid-exposure) and also the correct increment between images of <inline-formula><mml:math id="mm37"><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="mm38"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. <xref ref-type="fig" rid="sensors-20-01439-f005">Figure 5</xref> shows results of three consecutive image pairs. All image pairs (left and right) show the same LED count while consecutive images show the correct increment of 100. An image stream containing 400 image pairs was inspected without any case of wrong increment or non-matching pairs.</p>
      <p>We can therefore conclude that the time synchronization of two cameras has an accuracy better than <inline-formula><mml:math id="mm39"><mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> confirming an accurate time synchronization.</p>
    </sec>
    <sec id="sec3dot2-sensors-20-01439">
      <title>3.2. Camera-IMU</title>
      <p>For many visual-inertial odometry (VIO) algorithms such as ROVIO [<xref rid="B2-sensors-20-01439" ref-type="bibr">2</xref>] or OKVIS [<xref rid="B3-sensors-20-01439" ref-type="bibr">3</xref>], accurate time synchronization of camera image and IMU measurement is crucial for a robust and accurate operation.</p>
      <p>Typically, time offsets between camera and IMU can be the result of data transfer delay, OS scheduling, clock offsets of measurement devices, changing exposure times or internal filtering of IMU measurements. Thereby, only offsets that are not constant are critical as constant offsets can be calibrated. Namely, offsets that are typically changing such as OS scheduling, clocks on different measurement devices or a not compensated changing exposure time should be avoided.</p>
      <p>Using the camera-IMU calibration framework Kalibr [<xref rid="B22-sensors-20-01439" ref-type="bibr">22</xref>], a time offset between camera and IMU measurement can be determined by optimizing the extrinsic calibration together with a constant offset between both modalities. To test the consistency of the time offset, multiple datasets <italic>N</italic> were recorded with VersaVIS using different configurations for the IMU filtering and exposure times. The window width <italic>B</italic> of the deployed Barlett window finite impulse response (FIR) filter on the IMU [<xref rid="B25-sensors-20-01439" ref-type="bibr">25</xref>] was set to <inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> while the exposure time <inline-formula><mml:math id="mm41"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was set to AE or fixed to <inline-formula><mml:math id="mm42"><mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>}</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p>Furthermore, also the Skybotix VI-Sensor [<xref rid="B9-sensors-20-01439" ref-type="bibr">9</xref>] and Intel RealSense T265 [<xref rid="B12-sensors-20-01439" ref-type="bibr">12</xref>] were tested for reference.</p>
      <p><xref rid="sensors-20-01439-t002" ref-type="table">Table 2</xref> and <xref ref-type="fig" rid="sensors-20-01439-f006">Figure 6</xref> show import indicators of the calibration quality for multiple datasets. The reprojection error represents how well the lens and distortion model fit the actual lens and how well the movement of the camera agrees with the movement of the IMU after calibration. The reprojection error of VersaVIS and VI-Sensor are both low and consistent meaning that the the calibration converged to a consistent extrinsic transformation between camera and IMU and both sensor measurements agree well. Furthermore, the reprojection errors are independent of the filter and exposure time configuration showing that exposure compensation is working as expected. On the other side, the RealSense shows high reprojection errors because of the sensor’s fisheye lenses which turn out to be hard to calibrate even with the available fisheye lens models [<xref rid="B26-sensors-20-01439" ref-type="bibr">26</xref>] in Kalibr.</p>
      <p>This also becomes visible in the acceleration and gyroscope errors where the errors are very low as a result to the poorly fitting lens model. Due to the high influence of the lense model on the overall objective function of camera to IMU calibration, the reprojection error part of the objective function is mainly dominated by the poorly fitting lense model. This causes erroneous gradients, and can result in a higher weighting of the transformation between body and IMU compared to the weighting of the transformation between body and camera, which is a sub-optimal local minimum. Kalibr therefore estimates the body spline to be mainly represented by the IMU and neglects image measurements. For VersaVIS, both errors are highly dependent on the IMU filter showing a decrease of error with more aggressive filtering due to minimized noise. However, also here similar or lower errors can be achieved using VersaVIS compared to the VI-Sensor.</p>
      <p>Finally, the time offset between camera and IMU measurements shows that both VersaVIS and the VI-Sensor possess a similar accuracy in time synchronization as the standard deviations are low and the time offsets consistent indicating synchronization accuracy below <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:mn>0.05</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. However, the time offset is highly dependent on the IMU filter configuration. Therefore, the delay should be compensated either on the driver side or on the estimator side when more aggressive filtering is used (e.g., to reduce the influence of vibrations). Furthermore, the time offset is independent of exposure time and camera indicating again that exposure compensation is working as intended. RealSense shows inconsistent time offset estimations with a bi-modal distribution delimited by half the inter-frame time of ≈15 ms indicating that for some datasets, there might be image measurements shifts by one frame.</p>
    </sec>
    <sec id="sec3dot3-sensors-20-01439">
      <title>3.3. VersaVIS-Host</title>
      <p>As mentioned in <xref ref-type="sec" rid="sec2dot1dot3-sensors-20-01439">Section 2.1.3</xref>, not all sensors are directly compatible with VersaVIS. The better the clock translation of different measurement devices, the better the sensor fusion.</p>
      <p>Thanks to the bi-directional connection between VersaVIS and host computer, clock translation requests can be sent from VersaVIS and the response from the host can be analyzed.</p>
      <p>Such requests are sent every second. <xref ref-type="fig" rid="sensors-20-01439-f007">Figure 7</xref> shows the evolution of the KF states introduced in <xref ref-type="sec" rid="sec2dot1dot3-sensors-20-01439">Section 2.1.3</xref>. In this experiment, there is a clock skew between the host computer and VersaVIS resulting in a constantly decreasing offset after KF convergence. This highlights the importance of estimating the skew when using time translation.</p>
      <p><xref ref-type="fig" rid="sensors-20-01439-f008">Figure 8</xref> shows results of the residual <inline-formula><mml:math id="mm44"><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> and the innovation terms of the clock offset <inline-formula><mml:math id="mm45"><mml:mrow><mml:mi>δ</mml:mi></mml:mrow></mml:math></inline-formula> and skew <inline-formula><mml:math id="mm46"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula> after startup and in convergence. After approximately <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:mn>60</mml:mn><mml:mo> </mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the residual drops below <inline-formula><mml:math id="mm48"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and keeps oscillating at <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mn>5</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> due to USB jitter. However, thanks to the KF, this error is smoothed to a zero mean innovation of the offset of <inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.2</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> resulting in a clock translation accuracy of <inline-formula><mml:math id="mm51"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mn>0.2</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. The influence of the skew innovation can be neglected with the short update time of <inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
    </sec>
  </sec>
  <sec id="sec4-sensors-20-01439">
    <title>4. Applications</title>
    <p>This section validates the flexibility, robustness and accuracy of our system with several different sensor setups using VersaVIS, utilized in different applications.</p>
    <sec id="sec4dot1-sensors-20-01439">
      <title>4.1. Visual-Inertial SLAM</title>
      <p>The main purpose of a VI sensor is to perform odometry estimation and mapping. For that purpose, we collected a dataset walking around in our lab with different sensor setups including VersaVIS equipped with a FLIR BFS-U3-04S2M-CS camera and the Analog Devices ADIS16448 IMU shown in <xref ref-type="fig" rid="sensors-20-01439-f001">Figure 1</xref>a, a Skybotix VI-Sensor [<xref rid="B9-sensors-20-01439" ref-type="bibr">9</xref>] and an Intel RealSense T265 all attached to the same rigid body. For reference, we also evaluated the use of the FLIR camera together with the IMU of the VI-Sensor as a non-synchronized sensor setup. Since both sensors communicate with the host, software time translation is available.</p>
      <p><xref ref-type="fig" rid="sensors-20-01439-f009">Figure 9</xref> shows an example of the feature tracking window of ROVIO [<xref rid="B2-sensors-20-01439" ref-type="bibr">2</xref>], a filtering based monocular visual-inertial odometry algorithm, on the dataset. Both VersaVIS and the VI-Sensor show many well tracked features even during fast motions depicted on the image. Due to the high field of view (FOV) of the RealSense cameras and not perfectly fitting lens model, some of the keypoints do not reproject correctly to the image plane resulting in falsely warped keypoints. With a non-synchronized sensor (VersaVIS non-synced), many of the keypoints cannot be correctly tracked as the IMU measurements and the image measurements do not agree well.</p>
      <p><xref ref-type="fig" rid="sensors-20-01439-f010">Figure 10</xref> shows trajectories obtained with the procedure described above. While all sensors provide useful output depending on the application, VersaVIS shows the lowest drift. VI-Sensor and RealSense suffer from their specific camera hardware where the VI-Sensor has inferior lenses and camera chips (visible in motion blur) and RealSense has camera lenses where no well-fitting lens model is available in the used frameworks. Without synchronization, the trajectory becomes more jittery resulting in potentially unstable estimator also visible in the large scale offset and shows higher drift. Using batch optimization and loop closure [<xref rid="B27-sensors-20-01439" ref-type="bibr">27</xref>], the trajectory of VersaVIS can be further optimized (VersaVIS opt). However, the discrepancy between VersaVIS and VersaVIS opt is small indicating an already good odometry performance.</p>
    </sec>
    <sec id="sec4dot2-sensors-20-01439">
      <title>4.2. Stereo Visual-Inertial Odometry on Rail Vehicle</title>
      <p>The need for public transportation is heavily increasing while current infrastructure is reaching its limits. Using on-board sensors with reliable and accurate positioning, the efficiency of the infrastructure could be highly increased [<xref rid="B21-sensors-20-01439" ref-type="bibr">21</xref>]. However, this requires the fusion of multiple independent positioning modalities of which one could be visual-aided odometry.</p>
      <p>For this purpose, VersaVIS was combined with two global-shutter cameras arranged in a fronto-parallel stereo setup using master-slave triggering (see <xref ref-type="sec" rid="sec2dot1-sensors-20-01439">Section 2.1</xref>) and a compact, precision six degrees of freedom IMU shown in <xref ref-type="fig" rid="sensors-20-01439-f001">Figure 1</xref>c. In comparison to many commercial sensors, the camera has to provide a high frame-rate to be able to get a reasonable number of frames per displacement, even at higher speeds and feature a high dynamic range to deal with the challenging lighting conditions. Furthermore, due to the constraint motion of the vehicle and low signal to noise ratio (SNR), the IMU should be of a high quality and also should be temperature calibrated to deal with temperature changes due to direct sunlight. The sensor specifications are summarized in <xref rid="sensors-20-01439-t003" ref-type="table">Table 3</xref>.</p>
      <p>Multiple datasets were recorded with VersaVIS on a tram and evaluated against real time kinematics (RTK) global navigation satellite system (GNSS) ground-truth. The evaluation was performed using a sequence-based approach [<xref rid="B28-sensors-20-01439" ref-type="bibr">28</xref>] and shows that by using stereo cameras and tightly synchronized IMU measurements, robustness can be improved and accurate odometry up to <inline-formula><mml:math id="mm53"><mml:mrow><mml:mrow><mml:mn>1.11</mml:mn><mml:mo> </mml:mo><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the travelled distance evaluated on <inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo> </mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> sequences on railway scenarios and speeds up to <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mn>52.4</mml:mn><mml:mo> </mml:mo><mml:mrow><mml:mi>km</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> can be achieved. This corresponds to a median error of <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:mn>55.5</mml:mn><mml:mo> </mml:mo><mml:mi>cm</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> per <inline-formula><mml:math id="mm57"><mml:mrow><mml:mrow><mml:mn>50</mml:mn><mml:mo> </mml:mo><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> travelled. For more details, please refer to our previous work [<xref rid="B21-sensors-20-01439" ref-type="bibr">21</xref>].</p>
    </sec>
    <sec id="sec4dot3-sensors-20-01439">
      <title>4.3. Multi-Modal Mapping and Reconstruction</title>
      <p>VI mapping as described in <xref ref-type="sec" rid="sec4dot1-sensors-20-01439">Section 4.1</xref> can provide reliable pose estimates in many different environments. However, for applications that require precise mapping of structures in GPS denied, visually degraded environments, such as mines and caves, additional sensors are required. The multi-modal sensor setup as seen in <xref ref-type="fig" rid="sensors-20-01439-f001">Figure 1</xref>a was specifically developed for mapping research in these challenging conditions. The fact that VersaVIS provides time synchronization to the host computer greatly facilitates the addition of other sensors. The prerequisite is that these additional sensors are time synchronized with the host computer as well, which in our case, an Ouster OS-1 64-beam LiDAR, is done over Precision Time Protocol (PTP). The absence of light in these underground environments also required the addition of an artificial lighting source that fulfills very specific requirements to support the camera system for pose estimation and mapping. The main challenge was to achieve the maximum amount of light, equally distributed across the environment (i.e., ambient light), while at the same time being bound by power and cooling limitations. To that end a pair of high-powered, camera-shutter-synchronized LEDs, similar to to the system shown by Nikolic et al. [<xref rid="B29-sensors-20-01439" ref-type="bibr">29</xref>], are employed. VersaVIS provides a trigger signal to the LED control board, which represents the union of all camera exposure signals, ensuring that all images are fully illuminated while at the same time minimizing the power consumption and heat generation. This allows operating the LEDs at a significantly higher brightness level than during continuous operation.</p>
      <p><xref ref-type="fig" rid="sensors-20-01439-f011">Figure 11</xref> shows an example of the processed multi-modal sensor data. VI odometry [<xref rid="B2-sensors-20-01439" ref-type="bibr">2</xref>] and mapping [<xref rid="B27-sensors-20-01439" ref-type="bibr">27</xref>] including local refinements based on LiDAR data and Truncated Signed Distance Function (TSDF)-based surface reconstruction [<xref rid="B30-sensors-20-01439" ref-type="bibr">30</xref>] were used to precisely map the 3D structure of parts of an abandoned iron mine in Switzerland.</p>
    </sec>
    <sec id="sec4dot4-sensors-20-01439">
      <title>4.4. Object Based Mapping</title>
      <p>Robots that operate in changing environments benefit from using maps that are based on physical objects instead of abstract points. Such object based maps are more consistent if individual objects move, as only a movement of an object must be detected instead of each point on the moved object that is part of the map. Object based maps are also a better representation for manipulation tasks. The elements in the map are typically directly the objects of interest in such tasks.</p>
      <p>For the object based mapping application, a sensor setup with a depth and an RGB camera, and an IMU was assembled, see <xref ref-type="fig" rid="sensors-20-01439-f001">Figure 1</xref>b. A Pico Monstar, which is a ToF camera that provides <inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:mn>352</mml:mn><mml:mo>×</mml:mo><mml:mn>287</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> resolution depth images at up to <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:mn>60</mml:mn><mml:mo> </mml:mo><mml:mi>Hz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, was combined with a FLIR BFS-U3-16S2C-CS <inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mn>1.6</mml:mn><mml:mo> </mml:mo><mml:mi>MP</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> color camera, and an Analog Devices ADIS16448 IMU. To obtain accurate pose estimates, the IMU and color camera were used for odometry and localization [<xref rid="B27-sensors-20-01439" ref-type="bibr">27</xref>].</p>
      <p>With these poses, and together with the depth measurements, an approach from Reference [<xref rid="B31-sensors-20-01439" ref-type="bibr">31</xref>] was used to reconstruct the scene and extract object instances. An example of such a segmentation map is shown in <xref ref-type="fig" rid="sensors-20-01439-f012">Figure 12</xref>a, and objects that were extracted and inserted into a database in <xref ref-type="fig" rid="sensors-20-01439-f012">Figure 12</xref>b,c.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec5-sensors-20-01439">
    <title>5. Conclusions</title>
    <p>We presented a hardware synchronization suite for multi-camera VI sensors consisting of the full hardware design, firmware and host driver software. The sensor suite supports multiple beneficial features such as exposure time compensation and host time translation and can be used in both independent and master-slave multi-camera mode.</p>
    <p>The time synchronization performance is analyzed separately for camera-camera synchronization, camera-IMU synchronization and VersaVIS-host clock translation. All modules achieve time synchronization accuracy of <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:mi>ms</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> which is expected to be accurate enough for most mobile robotic applications.</p>
    <p>The benefits and great versatility range of the sensor suite are demonstrated on multiple applications including hand-held VIO, multi-camera VI applications on rail vehicles as well as large scale environment reconstruction and object based mapping.</p>
    <p>For the benefit of the community, all hardware and software components are completely open-source with a permissive license and based on easily available hardware and development software. This paper and the accompanying framework can also serve as a freely available reference design for research and industry as it summarizes solution approaches to multiple challenges of developing a synchronized multi-modal sensor setup.</p>
    <p>The research community can easily adopt, adapt and extend this sensor setup and rapid-prototype custom sensor setups for a variety of robotic applications. Many experimental features showcase the easy extensibility of the framework:<list list-type="bullet"><list-item><p>Illumination module: The VersaVIS triggering board can be paired with LEDs shown in <xref ref-type="fig" rid="sensors-20-01439-f001">Figure 1</xref>a which are triggered corresponding to the longest exposure time of the connected cameras. Thanks to the synchronization, the LEDs can be operated at a higher brightness as which would be possible in continuous operation.</p></list-item><list-item><p>IMU output: The SPI output of the board enables to use the same IMU which is used in the VI setup for a low-level controller such as the PixHawk [<xref rid="B33-sensors-20-01439" ref-type="bibr">33</xref>] used in MAV control.</p></list-item><list-item><p>Pulse per second (PPS) sync: Some sensors such as specific LiDARs allow synchronization to a PPS signal provided by for example, a Global Position System (GPS) receiver or real-time clock. Using the external clock input on the triggering board, VersaVIS can be extended to synchronize to the PPS source.</p></list-item><list-item><p>LiDAR synchronization: The available auxiliary interface on VersaVIS could be used to tightly integrate LiDAR measurements by getting digital pulses from the LiDAR corresponding to taken measurements. The merging procedure would then be similar to the one described in <xref ref-type="sec" rid="sec2dot2dot1-sensors-20-01439">Section 2.2.1</xref> for cameras with fixed exposure time.</p></list-item></list></p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>The authors would like to thank Gabriel Waibel for his work adapting the firmware for a newer MCU and Hannes Sommer for providing the LED triggering board.</p>
  </ack>
  <fn-group>
    <fn>
      <p><bold>Sample Availability:</bold> Samples of the VersaVIS triggering board are available from the authors.</p>
    </fn>
  </fn-group>
  <notes>
    <title>Author Contributions</title>
    <p>Conceptualization, F.T., M.R., M.F., A.P., C.C., R.S. and J.N.; Data curation, F.T., M.F., L.B., F.F. and T.N.; Formal analysis, F.T.; Funding acquisition, C.C., R.S. and J.N.; Investigation, F.T., M.F., L.B., F.F. and T.N.; Methodology, F.T., M.R. and A.P.; Project administration, C.C., R.S. and J.N.; Resources, F.T., M.F., L.B., F.F. and T.N.; Software, F.T. and M.R.; Supervision, C.C., R.S. and J.N.; Validation, F.T.; Visualization, F.T., M.F., L.B., F.F. and T.N.; Writing–original draft, F.T., M.F., L.B., F.F. and T.N.; Writing–review &amp; editing, F.T., M.F., L.B., F.F., T.N. and J.N. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>This work was partly supported by Siemens Mobility, Germany and by the National Center of Competence in Research (NCCR) Robotics through the Swiss National Science Foundation.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="B1-sensors-20-01439">
      <label>1.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Nourbakhsh</surname>
            <given-names>I.R.</given-names>
          </name>
          <name>
            <surname>Scaramuzza</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <source>Introduction to Autonomous Mobile Robots</source>
        <edition>2nd ed.</edition>
        <publisher-name>MIT Press</publisher-name>
        <publisher-loc>Camebridge, MA, USA</publisher-loc>
        <year>2011</year>
        <fpage>472</fpage>
      </element-citation>
    </ref>
    <ref id="B2-sensors-20-01439">
      <label>2.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bloesch</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Omari</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Hutter</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Robust visual inertial odometry using a direct EKF-based approach</article-title>
        <source>Proceedings of the IEEE International Conference on Intelligent Robots and Systems</source>
        <conf-loc>Hamburg, Germany</conf-loc>
        <conf-date>28 September–2 October 2015</conf-date>
        <fpage>298</fpage>
        <lpage>304</lpage>
        <pub-id pub-id-type="doi">10.1109/IROS.2015.7353389</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-sensors-20-01439">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leutenegger</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lynen</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Bosse</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Furgale</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Keyframe-based visual–inertial odometry using nonlinear optimization</article-title>
        <source>Int. J. Robot. Res.</source>
        <year>2015</year>
        <volume>34</volume>
        <fpage>314</fpage>
        <lpage>334</lpage>
        <pub-id pub-id-type="doi">10.1177/0278364914554813</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-sensors-20-01439">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qin</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator</article-title>
        <source>IEEE Trans. Robot.</source>
        <year>2018</year>
        <volume>34</volume>
        <fpage>1004</fpage>
        <lpage>1020</lpage>
        <pub-id pub-id-type="doi">10.1109/TRO.2018.2853729</pub-id>
      </element-citation>
    </ref>
    <ref id="B5-sensors-20-01439">
      <label>5.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Olson</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>A passive solution to the sensor synchronisation problem</article-title>
        <source>Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</source>
        <conf-loc>Taipei, Taiwan</conf-loc>
        <conf-date>18–22 October 2010</conf-date>
        <fpage>1059</fpage>
        <lpage>1064</lpage>
      </element-citation>
    </ref>
    <ref id="B6-sensors-20-01439">
      <label>6.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>English</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ross</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Ball</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Upcroft</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Corke</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>TriggerSync: A time synchronisation tool</article-title>
        <source>Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</source>
        <conf-loc>Seattle, WA, USA</conf-loc>
        <conf-date>26–30 May 2015</conf-date>
        <fpage>6220</fpage>
        <lpage>6226</lpage>
      </element-citation>
    </ref>
    <ref id="B7-sensors-20-01439">
      <label>7.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sommer</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Khanna</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Gilitschenski</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>A low-cost system for high-rate, high-accuracy temporal calibration for LIDARs and cameras</article-title>
        <source>Proceedings of the IEEE International Conference on Intelligent Robots and Systems</source>
        <conf-loc>Vancouver, BC, Canada</conf-loc>
        <conf-date>24–28 September 2017</conf-date>
        <pub-id pub-id-type="doi">10.1109/IROS.2017.8206042</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-sensors-20-01439">
      <label>8.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>IEEE 1588-based general and precise time synchronization method for multiple sensors*</article-title>
        <source>In Proceeding of the IEEE International Conference on Robotics and Biomimetics</source>
        <conf-loc>Dali, China</conf-loc>
        <conf-date>6–8 December 2019</conf-date>
        <fpage>2427</fpage>
        <lpage>2432</lpage>
      </element-citation>
    </ref>
    <ref id="B9-sensors-20-01439">
      <label>9.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Nikolic</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Rehder</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Burri</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Gohl</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Leutenegger</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Furgale</surname>
            <given-names>P.T.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>A synchronized visual-inertial sensor system with FPGA pre-processing for accurate real-time SLAM</article-title>
        <source>In Proceeding of the IEEE International Conference on Robotics and Automation</source>
        <conf-loc>Hong Kong, China</conf-loc>
        <conf-date>31 May–7 June 2014</conf-date>
        <fpage>431</fpage>
        <lpage>437</lpage>
        <pub-id pub-id-type="doi">10.1109/ICRA.2014.6906892</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-sensors-20-01439">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burri</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Nikolic</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Gohl</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Rehder</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Omari</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Achtelik</surname>
            <given-names>M.W.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>The EuRoC micro aerial vehicle datasets</article-title>
        <source>Int. J. Robot. Res.</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1157</fpage>
        <lpage>1163</lpage>
        <pub-id pub-id-type="doi">10.1177/0278364915620033</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-sensors-20-01439">
      <label>11.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pfrommer</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Sanket</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Daniilidis</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Cleveland</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>PennCOSYVIO: A challenging visual inertial odometry benchmark</article-title>
        <source>In Proceeding of the IEEE International Conference on Robotics and Automation</source>
        <conf-loc>Singapore</conf-loc>
        <conf-date>29 May–3 June 2017</conf-date>
        <fpage>3554</fpage>
        <lpage>3847</lpage>
      </element-citation>
    </ref>
    <ref id="B12-sensors-20-01439">
      <label>12.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Intel Corporation</collab>
        </person-group>
        <article-title>Intel® RealSense™ Tracking Camera T265</article-title>
        <year>2019</year>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.intelrealsense.com/tracking-camera-t265/">https://www.intelrealsense.com/tracking-camera-t265/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-02-01">(accessed on 1 February 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B13-sensors-20-01439">
      <label>13.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Honegger</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Sattler</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Pollefeys</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Embedded real-time multi-baseline stereo</article-title>
        <source>In Proceeding of the IEEE International Conference on Robotics and Automation</source>
        <conf-loc>Singapore</conf-loc>
        <conf-date>29 May–3 June 2017</conf-date>
      </element-citation>
    </ref>
    <ref id="B14-sensors-20-01439">
      <label>14.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>PIRVS: An Advanced Visual-Inertial SLAM System with Flexible Sensor Fusion and Hardware Co-Design</article-title>
        <source>In Proceeding of the 2018 IEEE International Conference on Robotics and Automation (ICRA)</source>
        <conf-loc>Brisbane, Australia</conf-loc>
        <conf-date>21–25 May 2018</conf-date>
      </element-citation>
    </ref>
    <ref id="B15-sensors-20-01439">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Geiger</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lenz</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Stiller</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Urtasun</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Vision meets robotics: The KITTI dataset</article-title>
        <source>Int. J. Robot. Res.</source>
        <year>2013</year>
        <volume>32</volume>
        <fpage>1231</fpage>
        <lpage>1237</lpage>
        <pub-id pub-id-type="doi">10.1177/0278364913491297</pub-id>
      </element-citation>
    </ref>
    <ref id="B16-sensors-20-01439">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carlevaris-Bianco</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ushani</surname>
            <given-names>A.K.</given-names>
          </name>
          <name>
            <surname>Eustice</surname>
            <given-names>R.M.</given-names>
          </name>
        </person-group>
        <article-title>University of Michigan North Campus long-term vision and lidar dataset</article-title>
        <source>Int. J. Robot. Res.</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1023</fpage>
        <lpage>1035</lpage>
        <pub-id pub-id-type="doi">10.1177/0278364915614638</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-sensors-20-01439">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Majdik</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Till</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Scaramuzza</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The Zurich urban micro aerial vehicle dataset</article-title>
        <source>Int. J. Robot. Res.</source>
        <year>2017</year>
        <volume>36</volume>
        <pub-id pub-id-type="doi">10.1177/0278364917702237</pub-id>
      </element-citation>
    </ref>
    <ref id="B18-sensors-20-01439">
      <label>18.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Schubert</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Goll</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Demmel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Usenko</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Stückler</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cremers</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The TUM VI benchmark for evaluating visual-inertial odometry</article-title>
        <source>In Proceeding of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source>
        <conf-loc>Madrid, Spain</conf-loc>
        <conf-date>1–5 October 2018</conf-date>
      </element-citation>
    </ref>
    <ref id="B19-sensors-20-01439">
      <label>19.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Arduino Inc</collab>
        </person-group>
        <article-title>Arduino Zero</article-title>
        <year>2019</year>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://store.arduino.cc/arduino-zero">https://store.arduino.cc/arduino-zero</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-02-01">(accessed on 1 February 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B20-sensors-20-01439">
      <label>20.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Stanford Artificial Intelligence Laboratory</collab>
        </person-group>
        <source>Robotic Operating System</source>
        <publisher-name>Stanford Artificial Intelligence Laboratory</publisher-name>
        <publisher-loc>Stanford, CA, USA</publisher-loc>
        <year>2018</year>
      </element-citation>
    </ref>
    <ref id="B21-sensors-20-01439">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tschopp</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Palmer</surname>
            <given-names>A.W.</given-names>
          </name>
          <name>
            <surname>Nourani-Vatani</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Cadena Lerma</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Experimental Comparison of Visual-Aided Odometry Methods for Rail Vehicles</article-title>
        <source>IEEE Robot. Autom. Lett.</source>
        <year>2019</year>
        <volume>4</volume>
        <fpage>1815</fpage>
        <lpage>1822</lpage>
        <pub-id pub-id-type="doi">10.1109/LRA.2019.2897169</pub-id>
      </element-citation>
    </ref>
    <ref id="B22-sensors-20-01439">
      <label>22.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Furgale</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Rehder</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Unified temporal and spatial calibration for multi-sensor systems</article-title>
        <source>Proceedings of the IEEE International Conference on Intelligent Robots and Systems</source>
        <conf-loc>Tokyo, Japan</conf-loc>
        <conf-date>3–7 November 2013</conf-date>
        <fpage>1280</fpage>
        <lpage>1286</lpage>
        <pub-id pub-id-type="doi">10.1109/IROS.2013.6696514</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-sensors-20-01439">
      <label>23.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Visual-lidar odometry and mapping: Low-drift, robust, and fast</article-title>
        <source>Proceedings of the 2015 IEEE International Conference on Robotics and Automation (ICRA)</source>
        <conf-loc>Seattle, WA, USA</conf-loc>
        <conf-date>26–30 May 2015</conf-date>
        <fpage>2174</fpage>
        <lpage>2181</lpage>
      </element-citation>
    </ref>
    <ref id="B24-sensors-20-01439">
      <label>24.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Ferguson</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bouchier</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Purvis</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Rosserial</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://wiki.ros.org/rosserial">http://wiki.ros.org/rosserial</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-02-01">(accessed on 1 February 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B25-sensors-20-01439">
      <label>25.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <collab>Analog Devices</collab>
        </person-group>
        <source>ADIS16448BLMZ: Compact, Precision Ten Degrees of Freedom Inertial Sensor</source>
        <comment>Technical Report</comment>
        <publisher-name>Analog Devices</publisher-name>
        <publisher-loc>Norwood, MA, USA</publisher-loc>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="B26-sensors-20-01439">
      <label>26.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Usenko</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Demmel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Cremers</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The double sphere camera model</article-title>
        <source>Proceedings of the 2018 International Conference on 3D Vision (3DV)</source>
        <conf-loc>Verona, Italy</conf-loc>
        <conf-date>5–8 September 2018</conf-date>
        <publisher-name>IEEE</publisher-name>
        <publisher-loc>Hoboken, NJ, USA</publisher-loc>
        <year>2018</year>
        <fpage>552</fpage>
        <lpage>560</lpage>
        <pub-id pub-id-type="doi">10.1109/3DV.2018.00069</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-sensors-20-01439">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schneider</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Dymczyk</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Fehr</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Egger</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Lynen</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Gilitschenski</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>maplab: An open framework for research in visual-inertial mapping and localization</article-title>
        <source>IEEE Robot. Autom. Lett.</source>
        <year>2018</year>
        <volume>3</volume>
        <fpage>1418</fpage>
        <lpage>1425</lpage>
        <pub-id pub-id-type="doi">10.1109/LRA.2018.2800113</pub-id>
      </element-citation>
    </ref>
    <ref id="B28-sensors-20-01439">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nourani-Vatani</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Borges</surname>
            <given-names>P.V.K.</given-names>
          </name>
        </person-group>
        <article-title>Correlation-based visual odometry for ground vehicles</article-title>
        <source>J. Field Robot.</source>
        <year>2011</year>
        <volume>28</volume>
        <fpage>742</fpage>
        <lpage>768</lpage>
        <pub-id pub-id-type="doi">10.1002/rob.20407</pub-id>
      </element-citation>
    </ref>
    <ref id="B29-sensors-20-01439">
      <label>29.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Nikolic</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Burri</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rehder</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Leutenegger</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Huerzeler</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>A UAV system for inspection of industrial facilities</article-title>
        <source>Proceedings of the IEEE Aerospace Conference</source>
        <conf-loc>Big Sky, MT, USA</conf-loc>
        <conf-date>2–9 March 2013</conf-date>
      </element-citation>
    </ref>
    <ref id="B30-sensors-20-01439">
      <label>30.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Oleynikova</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Fehr</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Voxblox: Incremental 3D Euclidean Signed Distance Fields for on-board MAV planning</article-title>
        <source>Proceedings of the IEEE International Conference on Intelligent Robots and Systems</source>
        <conf-loc>Vancouver, BC, Canada</conf-loc>
        <conf-date>24–28 September 2017</conf-date>
        <fpage>1366</fpage>
        <lpage>1373</lpage>
        <pub-id pub-id-type="doi">10.1109/IROS.2017.8202315</pub-id>
      </element-citation>
    </ref>
    <ref id="B31-sensors-20-01439">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Furrer</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Novkovic</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Fehr</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Grinvald</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cadena</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Modelify: An Approach to Incrementally Build 3D Object Models for Map Completion</article-title>
        <source>Int. J. Robot. Res.</source>
        <year>2019</year>
        <fpage>submitted</fpage>
      </element-citation>
    </ref>
    <ref id="B32-sensors-20-01439">
      <label>32.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Furrer</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Novkovic</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Fehr</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Gawel</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Grinvald</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Sattler</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Nieto</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Incremental Object Database: Building 3D Models from Multiple Partial Observations</article-title>
        <source>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source>
        <conf-loc>Madrid, Spain</conf-loc>
        <conf-date>1–5 October 2018</conf-date>
      </element-citation>
    </ref>
    <ref id="B33-sensors-20-01439">
      <label>33.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Meier</surname>
            <given-names>L.</given-names>
          </name>
          <collab>Auterion</collab>
        </person-group>
        <source>Pixhawk 4</source>
        <publisher-name>Auterion</publisher-name>
        <publisher-loc>Zurich, Switzerland</publisher-loc>
        <year>2019</year>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="sensors-20-01439-f001" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <p>The VersaVIS sensor in different configurations. VersaVIS is able to synchronize multiple sensor modalities such as inertial measurement units (IMU) and cameras (e.g., monochrome, color, ToF and thermal) but can also be used in conjunction with additional sensors such as LiDARs. (<bold>a</bold>) Lidarstick; (<bold>b</bold>) RGB-D-I sensor; (<bold>c</bold>) Stereo VI Sensor [<xref rid="B21-sensors-20-01439" ref-type="bibr">21</xref>]; (<bold>d</bold>) VersaVIS triggering board.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g001"/>
  </fig>
  <fig id="sensors-20-01439-f002" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <p>Design overview of VersaVIS. The microcontroller unit (MCU) embedded on the triggering board visible in <xref ref-type="fig" rid="sensors-20-01439-f001">Figure 1</xref>d sends triggers to both IMU and the connected cameras. Image data is directly transferred to the host computer where it is combined with the timestamps from the MCU.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g002"/>
  </fig>
  <fig id="sensors-20-01439-f003" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <p>Exposure time compensation for multi-camera VI sensor setup (adapted from Reference [<xref rid="B9-sensors-20-01439" ref-type="bibr">9</xref>]). The blue lines indicate corresponding measurements.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g003"/>
  </fig>
  <fig id="sensors-20-01439-f004" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <p>LED timing board with indicated state numbers. The left and right most LEDs are used for position reference. The four right counting LEDs are striding while the four counting LEDs on the left side are binary encoded.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g004"/>
  </fig>
  <fig id="sensors-20-01439-f005" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <p>Measurements of two independently triggered synchronized cameras with exposure compensation (left <monospace>cam0</monospace>, right <monospace>cam2</monospace>). Both cameras show strictly the same image with at most two of the striding LEDs on. The increment between consecutive measurements adds up correctly and overflows at a count of 64. (<bold>a</bold>) LED count <inline-formula><mml:math id="mm63"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>b</bold>) LED count <inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo> </mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm65"><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="mm66"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>116</mml:mn><mml:mo>%</mml:mo><mml:mn>64</mml:mn><mml:mo>=</mml:mo><mml:mn>52</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>c</bold>) LED count <inline-formula><mml:math id="mm640"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo> </mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="mm650"><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="mm660"><mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>216</mml:mn><mml:mo>%</mml:mo><mml:mn>64</mml:mn><mml:mo>=</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g005"/>
  </fig>
  <fig id="sensors-20-01439-f006" orientation="portrait" position="float">
    <label>Figure 6</label>
    <caption>
      <p>Distribution of camera to IMU calibration results using the different sensors and different sensor configurations for VersaVIS. Reprojection errors for RealSense are not visible as they are out of view, see <xref rid="sensors-20-01439-t002" ref-type="table">Table 2</xref>. While reprojection, gyroscope and acceleration errors can be evaluated per measurement in each dataset and therefore shown as mean and standard deviation, the optimized time offset is one value over the whole dataset and shown as individual results.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g006"/>
  </fig>
  <fig id="sensors-20-01439-f007" orientation="portrait" position="float">
    <label>Figure 7</label>
    <caption>
      <p>KF filter states after startup and in convergence. The offset is constantly decreasing after convergence due to a clock skew difference between VersaVIS and host computer.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g007"/>
  </fig>
  <fig id="sensors-20-01439-f008" orientation="portrait" position="float">
    <label>Figure 8</label>
    <caption>
      <p>KF residual and innovation terms after startup and in convergence. The jitter in the serial-USB interface is directly influencing the residual resulting in oscillating errors. However, as this jitter has zero-mean, the achieved clock synchronization/translation has a much higher accuracy visible in the innovation term of the clock offset.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g008"/>
  </fig>
  <fig id="sensors-20-01439-f009" orientation="portrait" position="float">
    <label>Figure 9</label>
    <caption>
      <p>Feature tracking of ROVIO [<xref rid="B2-sensors-20-01439" ref-type="bibr">2</xref>] on a dataset recorded in our lab. Inliers of the feature tracking are shown in green while outliers are shown in red.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g009"/>
  </fig>
  <fig id="sensors-20-01439-f010" orientation="portrait" position="float">
    <label>Figure 10</label>
    <caption>
      <p>Trajectories of different sensor setups using ROVIO [<xref rid="B2-sensors-20-01439" ref-type="bibr">2</xref>] on a dataset recorded in our lab. While all of the sensors provide a useful output, VersaVIS shows the lowest amount of drift while the non-synchronized sensor shows a lot of jitter in the estimation.</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g010"/>
  </fig>
  <fig id="sensors-20-01439-f011" orientation="portrait" position="float">
    <label>Figure 11</label>
    <caption>
      <p>3D reconstruction of an underground mine in Gonzen CH based on LiDAR data with poses provided by VI SLAM [<xref rid="B27-sensors-20-01439" ref-type="bibr">27</xref>]. (<bold>a</bold>) Accumulated points clouds using only VI SLAM poses; (<bold>b</bold>) Dense reconstruction example based on [<xref rid="B30-sensors-20-01439" ref-type="bibr">30</xref>]; (<bold>c</bold>) Visible details of wheels lying on the ground using den se reconstruction based on Reference [<xref rid="B30-sensors-20-01439" ref-type="bibr">30</xref>].</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g011"/>
  </fig>
  <fig id="sensors-20-01439-f012" orientation="portrait" position="float">
    <label>Figure 12</label>
    <caption>
      <p>Object segmentation and reconstruction based on References [<xref rid="B31-sensors-20-01439" ref-type="bibr">31</xref>,<xref rid="B32-sensors-20-01439" ref-type="bibr">32</xref>] using data from the RGB-D-I sensor and camera poses obtained with maplab [<xref rid="B27-sensors-20-01439" ref-type="bibr">27</xref>]. (<bold>a</bold>) Scene reconstruction using the RGB-D-I sensor; (<bold>b</bold>) Segmented objects in a parking garage using [<xref rid="B31-sensors-20-01439" ref-type="bibr">31</xref>]; (<bold>c</bold>) Merging of objects from the database [<xref rid="B31-sensors-20-01439" ref-type="bibr">31</xref>].</p>
    </caption>
    <graphic xlink:href="sensors-20-01439-g012"/>
  </fig>
  <table-wrap id="sensors-20-01439-t001" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-20-01439-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Hardware characteristics for the VersaVIS triggering board.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">MCU</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Hardware Interface</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Host Interface</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Weight</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Size</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Price</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ARM M0+</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SPI, I<inline-formula><mml:math id="mm67"><mml:mrow><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>C, UART</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Serial-USB 2.0</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm68">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>15.2</mml:mn>
                    <mml:mo> </mml:mo>
                    <mml:mi mathvariant="normal">g</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm69">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>62</mml:mn>
                    <mml:mo>×</mml:mo>
                    <mml:mn>40</mml:mn>
                    <mml:mo>×</mml:mo>
                    <mml:mn>13.4</mml:mn>
                    <mml:mo> </mml:mo>
                    <mml:mi>mm</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;100$</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-20-01439-t002" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-20-01439-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>Mean values of camera to IMU calibration results for different sensors and sensor configurations for VersaVIS.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
</th>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
            <italic>B</italic>
          </th>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">
            <inline-formula>
              <mml:math id="mm70">
                <mml:mrow>
                  <mml:mstyle mathvariant="bold">
                    <mml:msub>
                      <mml:mi mathvariant="bold-italic">t</mml:mi>
                      <mml:mi mathvariant="bold-italic">e</mml:mi>
                    </mml:msub>
                  </mml:mstyle>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </th>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">N</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Reprojection Error <inline-formula><mml:math id="mm71"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="bold">pixel</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula></th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Gyroscope Error <inline-formula><mml:math id="mm72"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mo>[</mml:mo><mml:mi mathvariant="bold">rad</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="bold">s</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula></th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Std</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mean</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Std</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.100078</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.064657</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.009890</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.006353</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm73">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn mathvariant="bold">0.098548</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.063781</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.007532</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.005617</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.101866</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.067196</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.007509</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.005676</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm74">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mo> </mml:mo>
                    <mml:mi>ms</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.121552</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.075939</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.006756</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.004681</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm75">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>3</mml:mn>
                    <mml:mo> </mml:mo>
                    <mml:mi>ms</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.108760</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm76">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn mathvariant="bold">0.062456</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.006483</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.004360</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm77">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>5</mml:mn>
                    <mml:mo> </mml:mo>
                    <mml:mi>ms</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.114614</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.074536</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.006578</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.00428</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VI-Sensor</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">×</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">40</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.106839</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.083605</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.008915</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.007425</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Realsense</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">×</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.436630</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.355895</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm78">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn mathvariant="bold">0.000000</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm79">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn mathvariant="bold">0.000005</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
</td>
          <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
            <inline-formula>
              <mml:math id="mm80">
                <mml:mrow>
                  <mml:mi mathvariant="bold-italic">B</mml:mi>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
            <inline-formula>
              <mml:math id="mm81">
                <mml:mrow>
                  <mml:msub>
                    <mml:mi mathvariant="bold-italic">t</mml:mi>
                    <mml:mi mathvariant="bold-italic">e</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" colspan="1">
            <bold>N</bold>
          </td>
          <td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
            <bold>Accelerometer error</bold>
            <inline-formula>
              <mml:math id="mm82">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mi mathvariant="bold">m</mml:mi>
                    <mml:mo>/</mml:mo>
                    <mml:msup>
                      <mml:mi mathvariant="bold">s</mml:mi>
                      <mml:mn mathvariant="bold">2</mml:mn>
                    </mml:msup>
                    <mml:mo>]</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td colspan="2" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
            <bold>Time offset</bold>
            <inline-formula>
              <mml:math id="mm83">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mi mathvariant="bold">ms</mml:mi>
                    <mml:mo>]</mml:mo>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>Mean</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>Std</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>Mean</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>Std</bold>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.143162</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.191270</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.552360</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.034126</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.083576</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.130018</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.260927</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.035812</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.030261</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.018168</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">19.951467</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.049712</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm84">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mo> </mml:mo>
                    <mml:mi>ms</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.026765</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.014890</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">20.007137</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.047525</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm85">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>3</mml:mn>
                    <mml:mo> </mml:mo>
                    <mml:mi>ms</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.024309</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.014367</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">20.002966</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.035952</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VersaVIS</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm86">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn>5</mml:mn>
                    <mml:mo> </mml:mo>
                    <mml:mi>ms</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.027468</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.016553</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">19.962924</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm87">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn mathvariant="bold">0.029872</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">VI-Sensor</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">×</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">40</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.044845</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.042446</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm88">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn mathvariant="bold">1.173100</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.046410</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Realsense</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">×</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">AE</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">40</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm89">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn mathvariant="bold">0.000000</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-formula>
              <mml:math id="mm90">
                <mml:mrow>
                  <mml:mrow>
                    <mml:mn mathvariant="bold">0.000002</mml:mn>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </inline-formula>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.884808</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.977421</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-20-01439-t003" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-20-01439-t003_Table 3</object-id>
    <label>Table 3</label>
    <caption>
      <p>Sensor specifications deployed for data collection on rail vehicles [<xref rid="B21-sensors-20-01439" ref-type="bibr">21</xref>].</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Device</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Type</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Specification</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Camera</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Basler acA1920-155uc</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Frame-rate <inline-formula><mml:math id="mm91"><mml:mrow><mml:mrow><mml:mn>20</mml:mn><mml:mo> </mml:mo><mml:mi>fps</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (The hardware is able to capture up to <inline-formula><mml:math id="mm92"><mml:mrow><mml:mrow><mml:mn>155</mml:mn><mml:mo> </mml:mo><mml:mi>fps</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.),<break/>Resolution <inline-formula><mml:math id="mm93"><mml:mrow><mml:mrow><mml:mn>1920</mml:mn><mml:mo>×</mml:mo><mml:mn>1200</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, Dynamic range <inline-formula><mml:math id="mm94"><mml:mrow><mml:mrow><mml:mn>73</mml:mn><mml:mo> </mml:mo><mml:mi>dB</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Lense</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Edmund Optics</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Focal length <inline-formula><mml:math id="mm95"><mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo> </mml:mo><mml:mi>mm</mml:mi><mml:mo>≈</mml:mo><mml:mn>70</mml:mn><mml:mo form="prefix">deg</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> opening angle; Aperture <inline-formula><mml:math id="mm96"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>/</mml:mo><mml:mn>5.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">IMU</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">ADIS16445</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Temperature calibrated, <inline-formula><mml:math id="mm97"><mml:mrow><mml:mrow><mml:mn>300</mml:mn><mml:mo> </mml:mo><mml:mi>Hz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm98"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mn>250</mml:mn><mml:mo> </mml:mo><mml:mo form="prefix">deg</mml:mo><mml:mo>/</mml:mo><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm99"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mn>49</mml:mn><mml:mo> </mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi mathvariant="normal">s</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula></td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
