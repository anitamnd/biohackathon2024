<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9682683</article-id>
    <article-id pub-id-type="publisher-id">5051</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-05051-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BioByGANS: biomedical named entity recognition by fusing contextual and syntactic features through graph attention network in node classification framework</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zheng</surname>
          <given-names>Xiangwen</given-names>
        </name>
        <address>
          <email>zhengxw@bmi.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Du</surname>
          <given-names>Haijian</given-names>
        </name>
        <address>
          <email>390834526@qq.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Luo</surname>
          <given-names>Xiaowei</given-names>
        </name>
        <address>
          <email>lxw920701@163.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tong</surname>
          <given-names>Fan</given-names>
        </name>
        <address>
          <email>tongf@bmi.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Song</surname>
          <given-names>Wei</given-names>
        </name>
        <address>
          <email>songwei@medpeer.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Zhao</surname>
          <given-names>Dongsheng</given-names>
        </name>
        <address>
          <email>dszhao@bmi.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.410740.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 1803 4911</institution-id><institution>Academy of Military Medical Sciences, </institution></institution-wrap>Beijing, 100039 China </aff>
      <aff id="Aff2"><label>2</label>Beijing MedPeer Information Technology Co., Ltd, Beijing, 102300 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>22</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>501</elocation-id>
    <history>
      <date date-type="received">
        <day>5</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>10</day>
        <month>11</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Automatic and accurate recognition of various biomedical named entities from literature is an important task of biomedical text mining, which is the foundation of extracting biomedical knowledge from unstructured texts into structured formats. Using the sequence labeling framework and deep neural networks to implement biomedical named entity recognition (BioNER) is a common method at present. However, the above method often underutilizes syntactic features such as dependencies and topology of sentences. Therefore, it is an urgent problem to be solved to integrate semantic and syntactic features into the BioNER model.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we propose a novel biomedical named entity recognition model, named BioByGANS (<bold>BioB</bold>ERT/SpaC<bold>y</bold>-<bold>G</bold>raph <bold>A</bold>ttention <bold>N</bold>etwork-<bold>S</bold>oftmax), which uses a graph to model the dependencies and topology of a sentence and formulate the BioNER task as a node classification problem. This formulation can introduce more topological features of language and no longer be only concerned about the distance between words in the sequence. First, we use periods to segment sentences and spaces and symbols to segment words. Second, contextual features are encoded by BioBERT, and syntactic features such as part of speeches, dependencies and topology are preprocessed by SpaCy respectively. A graph attention network is then used to generate a fusing representation considering both the contextual features and syntactic features. Last, a softmax function is used to calculate the probabilities and get the results. We conduct experiments on 8 benchmark datasets, and our proposed model outperforms existing BioNER state-of-the-art methods on the BC2GM, JNLPBA, BC4CHEMD, BC5CDR-chem, BC5CDR-disease, NCBI-disease, Species-800, and LINNAEUS datasets, and achieves F1-scores of 85.15%, 78.16%, 92.97%, 94.74%, 87.74%, 91.57%, 75.01%, 90.99%, respectively.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">The experimental results on 8 biomedical benchmark datasets demonstrate the effectiveness of our model, and indicate that formulating the BioNER task into a node classification problem and combining syntactic features into the graph attention networks can significantly improve model performance.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Biomedical named entity recognition</kwd>
      <kwd>Text mining</kwd>
      <kwd>BioBERT</kwd>
      <kwd>SpaCy</kwd>
      <kwd>Graph attention network</kwd>
      <kwd>Contextual features</kwd>
      <kwd>Syntactic features</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par18">Biomedical named entity recognition (BioNER), which is a subdivision of named entity recognition (NER) [<xref ref-type="bibr" rid="CR1">1</xref>], aims to identify the mention of biomedical named entities such as genes, proteins, diseases, drugs, species, etc. in texts [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. Automatically and accurately extracting biomedical named entities is the prerequisite for extracting biomedical knowledge from unstructured texts into structured formats, which helps researchers track and summarize the knowledge contained in the extensive scientific literature in a timely manner.</p>
    <p id="Par19">With the in-depth study of deep learning [<xref ref-type="bibr" rid="CR4">4</xref>], deep learning methods have been widely used in the field of natural language processing (NLP). BioNER, modeled as a sequence labeling problem [<xref ref-type="bibr" rid="CR5">5</xref>], can then be solved end-to-end by deep learning methods, which avoids manual feature engineering and improves the performance to a certain extent. However, a major problem is the lack of large-scale high-quality annotated training data. In addition, the sequence labeling framework underutilizes and fails to explicitly exploit the topological information of language to some extent.</p>
    <p id="Par20">Pre-trained models, such as Word2Vec [<xref ref-type="bibr" rid="CR6">6</xref>], ELMo [<xref ref-type="bibr" rid="CR7">7</xref>], and BERT [<xref ref-type="bibr" rid="CR8">8</xref>], first use a self-supervised learning strategy to learn distributed representations of words in the large-scale unlabeled corpus and then perform finetune according to downstream tasks. Among them, BERT, a state-of-the-art (SOTA) regressive language model, provides an important foundation for various NLP downstream tasks. The pre-training strategy is also implemented in the biomedical field, and models such as BioWord2Vec [<xref ref-type="bibr" rid="CR9">9</xref>], BioELMo [<xref ref-type="bibr" rid="CR10">10</xref>], and BioBERT [<xref ref-type="bibr" rid="CR11">11</xref>], have already been proposed. However, syntactic features of texts including part of speeches, constituencies, and dependencies [<xref ref-type="bibr" rid="CR12">12</xref>], which should play an important role in NLP tasks, are currently under-considered by pre-trained models. As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, a sentence graph is built based on syntactic features. Different from the sequence structure, the graph structure can better define the semantic distance between words, which may help to implement the NLP tasks.<fig id="Fig1"><label>Fig. 1</label><caption><p>An example of using the graph to model the topology of a sentence. The topology of a sentence is a graph instead of a sequence. As shown in the figure, challenge is neighbor of recurred in the sentence graph, though challenge is far away from recurred in the sequence. There exists 2 biomedical NEs in the sentence, which serve as subject and object structure respectively. The sentence graph reflects the correlation path and indicates the syntactic relationship between the 2 entities, which helps to implement the NER task</p></caption><graphic xlink:href="12859_2022_5051_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par21">In this paper, inspired by the topological information of language, we use a graph to model a sentence. Using a graph to model sentences can introduce more topological information of language, and make the model focus more on topological distances rather than sequential distances between words. Then, we formulate the BioNER task as a node classification problem and propose an end-to-end model, BioByGANS, which integrates both contextual and syntactic features of texts. Among them, contextual features include but not limited to semantic, position and morphological information, and syntactic features include part of speech and dependency information. First, we use periods to segment sentences and spaces and symbols to segment words. Second, contextual features and syntactic features of a sentence are encoded via a pre-trained model BioBERT and a NLP tool SpaCy [<xref ref-type="bibr" rid="CR13">13</xref>] respectively. A graph attention network (GAT) [<xref ref-type="bibr" rid="CR14">14</xref>] is then used to generate representations according to the topology of the sentence and contextual features. Last, a softmax function is used to calculate the probabilities and get the results. We evaluate our model on 8 biomedical benchmark datasets, and the experimental results show that BioByGANS outperforms the current SOTA models. Our contributions are summarized as follows:<list list-type="order"><list-item><p id="Par22">A novel word representation which fuses contextual features (BioBERT) and syntactic features including part of speeches and dependencies (SpaCy), and is further optimized via graph attention network.</p></list-item><list-item><p id="Par23">A novel method of formulating BioNER task as a node classification problem based on sentence-level topological features.</p></list-item></list></p>
    <p id="Par24">The remainder of this paper is organized as follows. In chapter II, related work of BioNER is presented. In chapter III, the proposed model is introduced in detail. And chapter IV presents the designed experiments and results. Finally, the paper is concluded in chapter V.</p>
  </sec>
  <sec id="Sec2">
    <title>Related work</title>
    <p id="Par25">IN this chapter, the recent progress of BioNER is first introduced. The preliminary knowledge of components in our model, including text distribution representation, syntactic features, and graph neural networks, is then presented.</p>
    <sec id="Sec3">
      <title>Recent progress of BioNER</title>
      <p id="Par26">Traditional methods for BioNER include rule-based [<xref ref-type="bibr" rid="CR15">15</xref>], dictionary-based [<xref ref-type="bibr" rid="CR16">16</xref>], and machine learning methods [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. However, these methods rely heavily on hand-crafted work. Furthermore, the above hand-crafted work is entity-specific, which has poor robustness and cannot solve the problem of polysemy and out of vocabulary (OOV) [<xref ref-type="bibr" rid="CR19">19</xref>].</p>
      <p id="Par27">Recently, deep learning based methods have already been widely used in BioNER field. The end-to-end strategy avoids manual feature engineering and improves the performance of such models. [<xref ref-type="bibr" rid="CR20">20</xref>–<xref ref-type="bibr" rid="CR23">23</xref>] used long short-term memory (LSTM) networks, and [<xref ref-type="bibr" rid="CR24">24</xref>–<xref ref-type="bibr" rid="CR26">26</xref>] used conditional random field (CRF) to recognize biomedical entities. [<xref ref-type="bibr" rid="CR27">27</xref>] realized BioNER based on a semi-Markov classifier and a dictionary-based postprocessing method. [<xref ref-type="bibr" rid="CR20">20</xref>] and [<xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR31">31</xref>] implemented the task based on BiLSTM-CRF framework. Specifically, [<xref ref-type="bibr" rid="CR20">20</xref>] proposed a document-level attention coefficient to transmit features between sentences and realizes NER for chemicals. [<xref ref-type="bibr" rid="CR28">28</xref>] used a character-level BiLSTM and a word-level BiLSTM respectively to obtain morphologic features and contextual features of words. [<xref ref-type="bibr" rid="CR29">29</xref>] made a more accurate prediction by exchanging information from single-task models for genes, chemicals, and diseases respectively. [<xref ref-type="bibr" rid="CR30">30</xref>] built a dictionary based on the disease ontology and then constructs a document-level attention layer by using the dictionary. [<xref ref-type="bibr" rid="CR31">31</xref>] used a multi-task learning strategy, and got improved by sharing parameters between tasks. [<xref ref-type="bibr" rid="CR32">32</xref>] is an improved BioNER model by leveraging syntactic information through a key-value memory network.</p>
      <p id="Par28">Pre-trained language models have also been applied in BioNER, and got SOTA performances. BioBERT [<xref ref-type="bibr" rid="CR11">11</xref>], a model further trained in abstracts and full texts of biomedical publications from PubMed and PMC on the foundation of BERT, achieved high performance based on the pretraining-finetuning strategy. [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>], based on BioBERT, implemented BioNER by leveraging the strategy of machine reading comprehension and multi-task learning respectively. BioELECTRA [<xref ref-type="bibr" rid="CR35">35</xref>], a pre-trained model based on the generative adversarial strategy, is a lighting pre-trained model in biomedical NLP field, which can also be applied in BioNER task.</p>
    </sec>
    <sec id="Sec4">
      <title>Text distributed representation</title>
      <p id="Par29">Distributed representation of texts is the basis of using deep learning methods to realize the downstream tasks of NLP. Language models [<xref ref-type="bibr" rid="CR36">36</xref>] utilize probability distribution to quantitatively model natural language, and one important method is to map natural language into vector space [<xref ref-type="bibr" rid="CR37">37</xref>]. Text distributed representation goes through the following 3 stages.</p>
      <p id="Par30">Word embedding methods based on statistics include one-hot encoding, bag of words [<xref ref-type="bibr" rid="CR38">38</xref>], and TF-IDF [<xref ref-type="bibr" rid="CR39">39</xref>]. The above methods, however, neglect the word order and contextual characteristics of the text to some extent. In addition, they cannot deal well with the large-scale corpus, nor can they calculate the similarity between words.</p>
      <p id="Par31">Word2Vec [<xref ref-type="bibr" rid="CR6">6</xref>] employs the continuous bag of words (CBOW) and Skip-Gram, and obtains the distributed representation based on the local context. FastText [<xref ref-type="bibr" rid="CR40">40</xref>] obtains the morphological features of words based on the word-level sliding windows, which solves the OOV problem. Glove [<xref ref-type="bibr" rid="CR41">41</xref>] generates the word representation based on a co-occurrence matrix to integrate the global information into the representation. However, the above representations are fixedly stored, which ignores the different contexts of words, and fails to solve the problem of polysemy.</p>
      <p id="Par32">Using dynamic contextual word distributed representations including CoVe [<xref ref-type="bibr" rid="CR42">42</xref>], ELMo [<xref ref-type="bibr" rid="CR7">7</xref>], GPT [<xref ref-type="bibr" rid="CR43">43</xref>], and BERT [<xref ref-type="bibr" rid="CR8">8</xref>] to solve NLP problems has become a trend because the representation can be adjusted with the change of context. ELMo [<xref ref-type="bibr" rid="CR7">7</xref>], a pre-trained model based on Bi-LSTM [<xref ref-type="bibr" rid="CR44">44</xref>], uses the next word prediction task to learn the word representation. BERT [<xref ref-type="bibr" rid="CR8">8</xref>] generates the representation based on the transformer [<xref ref-type="bibr" rid="CR45">45</xref>], and obtains sentence-level features through masked language modeling (MLM) and next sentence prediction (NSP).</p>
    </sec>
    <sec id="Sec5">
      <title>Syntactic features</title>
      <p id="Par33">The part of grammar that presents a speaker’s knowledge of sentences and their structures is called syntax, which is the sentence patterns of language [<xref ref-type="bibr" rid="CR12">12</xref>]. The syntax is one of the important research objects and the important characteristics of downstream tasks in NLP field. So far, several NLP tools have been open source, including NLTK [<xref ref-type="bibr" rid="CR46">46</xref>], StanfordNLP [<xref ref-type="bibr" rid="CR47">47</xref>], SpaCy. Among them, SpaCy is a fast, powerful and lightweight NLP tool for various languages, of which the functions include tokenizer, tagger, parser, etc. In addition, SpaCy achieves a precision of 98% on part of speech tagging, and 95% on parsing, which can be considered as a reliable NLP tool.</p>
      <p id="Par34">Syntactic Features should be one of the important features for NER. Hamon et al. [<xref ref-type="bibr" rid="CR48">48</xref>] constructed rules based on syntactic features and implemented BioNER based on these rules. However, at present, the mainstream methods mainly consider the contextual features of sentences and implement NER based on neural networks. And there are few NER methods paying attention to both contextual and syntactic features. [<xref ref-type="bibr" rid="CR32">32</xref>] uses a key-value memory network to fuse syntactic information, which ignores the topological information of sentences to some extent.</p>
    </sec>
    <sec id="Sec6">
      <title>Graph neural networks</title>
      <p id="Par35">Recently, considering that data in some application scenarios are generated from non-Euclidean spaces, researchers pay more attention to applying deep learning technology to graphic data, and the graph neural network (GNN) comes into being [<xref ref-type="bibr" rid="CR49">49</xref>].</p>
      <p id="Par36">As one of the GNNs, GCN [<xref ref-type="bibr" rid="CR50">50</xref>] analyzes the feature of a node based on both the node and its neighbors and can implement tasks such as node classification, link prediction, and recommendation. However, GCN assigns the same weight to neighbor nodes, and GCN fails to fuse node features in dynamic graphs. Different from the fixed kernel of GCN, GAT [<xref ref-type="bibr" rid="CR14">14</xref>] is a graph neural network based on the masked self-attention mechanism which dynamically calculates weights for neighbors according to the topology of a graph and further generates the representation of the central node.</p>
      <p id="Par37">GNNs have also been applied in NER task recently. [<xref ref-type="bibr" rid="CR51">51</xref>] uses BERT and 2 GNN layers to implement NER in general domain. As for syntactic features in their work, only dependency graph is used, which underutilizes the part of speeches and other dependency features [<xref ref-type="bibr" rid="CR52">52</xref>]. uses BERT and a GCN layer to implement a nested NER task. They first tag part of speeches and dependencies based the proposed model, and optimize the word representation through a GCN layer. However, the GCN used in their work is not suitable for various and flexible structure of sentences, which may cause a certain limitation.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Methodology</title>
    <sec id="Sec8">
      <title>Formulating the BioNER task as a node classification problem</title>
      <p id="Par38">To introduce the syntactic features, we use an undirected graph to model the topology of a sentence. Robinson pointed out that only one element is independent in a sentence, and all others depend directly on some element [<xref ref-type="bibr" rid="CR53">53</xref>]. In other words, the dependencies of a sentence can be modeled via a graph, where elements/tokens are modeled as nodes, and dependencies are modeled as edges between nodes. In addition, features of elements/tokens, including distributed representations, are modeled as attributions of the corresponding nodes. An example of using the graph to model the topology of a sentence is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.</p>
      <p id="Par39">SpaCy (version 3.2.1, with the package en_core_web_trf) is utilized in this paper to parse sentences, and for a sentence <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq1.gif"/></alternatives></inline-formula>, part of speeches <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}=({p}_{1}, {p}_{1}, \dots , {p}_{n})$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq2.gif"/></alternatives></inline-formula>, and dependencies <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Dep}_{i}=\left\{\left({h}_{1},{t}_{1},{d}_{1}\right), \left({h}_{2},{t}_{2},{d}_{2}\right), \dots , \left({h}_{m},{t}_{m},{d}_{m}\right)\right\},$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">Dep</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="}" open="{"><mml:mfenced close=")" open="("><mml:msub><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:mfenced close=")" open="("><mml:msub><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfenced><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:mfenced close=")" open="("><mml:msub><mml:mi>h</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq3.gif"/></alternatives></inline-formula> are thereout obtained, where <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p}_{j}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq4.gif"/></alternatives></inline-formula> is the corresponding part of speech of the <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j$$\end{document}</tex-math><mml:math id="M10"><mml:mi>j</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq5.gif"/></alternatives></inline-formula>-th token, and <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({h}_{l},{t}_{l},{d}_{l}\right)$$\end{document}</tex-math><mml:math id="M12"><mml:mfenced close=")" open="("><mml:msub><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq6.gif"/></alternatives></inline-formula> represents the head token, tail token, and type of dependency in the <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><mml:math id="M14"><mml:mi>l</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq7.gif"/></alternatives></inline-formula>-th dependency triple. Then, the corresponding nodes of <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${h}_{l}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${t}_{l}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>t</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq9.gif"/></alternatives></inline-formula> are neighbors of each other in the graph. Moreover, part of speeches and dependencies are also further encoded, which is presented in Section C.</p>
      <p id="Par40">We also construct a <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Adjacent\_Matrix$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>A</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>_</mml:mi><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq10.gif"/></alternatives></inline-formula> to quantize the topology of a sentence, which is a symmetric matrix. For <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${a}_{ij}\in Adjacent\_Matrix$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mi>d</mml:mi><mml:mi>j</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>_</mml:mi><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq11.gif"/></alternatives></inline-formula> we have,<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a_{ij} = \left\{ {\begin{array}{*{20}l} {1,} \hfill &amp; {if\;i\;is\;j^{\prime}s\;neighbor} \hfill \\ {0,} \hfill &amp; {else} \hfill \\ \end{array} } \right.$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.277778em"/><mml:mi>i</mml:mi><mml:mspace width="0.277778em"/><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.277778em"/><mml:msup><mml:mi>j</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>s</mml:mi><mml:mspace width="0.277778em"/><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="italic">else</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par41">After converting sentences of datasets into a set of graphs, the BioNER task can then be formulated as a node classification problem. Each graph is composed of nodes with attributes and edges, where a node represents a token, an edge represents the dependency between tokens and an attribute indicates the distributed representation of the corresponding token. And our goal is to classify the above nodes into corresponding labels.</p>
    </sec>
    <sec id="Sec9">
      <title>Overall architecture of BioByGANS</title>
      <p id="Par42">We propose an end-to-end model for BioNER in the node classification framework. The proposed model, BioByGANS, is ulteriorly divided into 3 modules, including the input, representation, and output module. The overall architecture is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig2"><label>Fig. 2</label><caption><p>The overall architecture of BioByGANS. Sentences are input into BioBERT to get contextual embeddings. Meanwhile, SpaCy is used to parse sentences and get syntactic embeddings according to part of speeches and dependencies, and adjacent matrix based on dependencies. A graph attention network layer, which is defined based on the adjacent matrix, receives the concatenation of contextual and syntactic embeddings as the input, and generates a fusing node representation considering both the contextual features and syntactic features. Last, a softmax function is used to calculate the probabilities and get the results as BIO labels</p></caption><graphic xlink:href="12859_2022_5051_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par43">First, for biomedical texts, we use periods to segment sentences and spaces and symbols to segment words. And thus, each sentence is transformed into a sequence of tokens. Second, contextual features and syntactic features of a sentence are encoded via the BioBERT and SpaCy respectively. In addition, the topology of a sentence is converted into an adjacent matrix through SpaCy. A graph attention network is then used to generate representations of nodes based on the adjacent matrix. The mutli-head attention mechanism is also utilized to capture richer information in sentence graphs. Last, we use a softmax function to compute the probability distribution of labels using the final distributed representation of a node, and get the output.</p>
    </sec>
    <sec id="Sec10">
      <title>Encoding module of BioByGANS</title>
      <p id="Par44">In this paper, we use BioBERT [<xref ref-type="bibr" rid="CR11">11</xref>] and SpaCy [<xref ref-type="bibr" rid="CR13">13</xref>] to encode contextual and syntactic features respectively. The outputs,<inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Cont$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mi mathvariant="italic">Cont</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq12.gif"/></alternatives></inline-formula> and <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Synt$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mi mathvariant="italic">Synt</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq13.gif"/></alternatives></inline-formula>, are 768-dimensional and 54-dimensional vectors respectively for each token.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Cont=BioBERT\left(Sentence\right) Synt=SpaCy\left(Sentence\right)$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>B</mml:mi><mml:mi>E</mml:mi><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mfenced close=")" open="("><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mfenced><mml:mi>S</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>C</mml:mi><mml:mi>y</mml:mi><mml:mfenced close=")" open="("><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par45">Specifically, syntactic features are divided into part of speeches and dependencies. We first get the statistics about the frequency distribution of part of speeches and dependencies of biomedical named entities in corpora, and we use a frequency of 1% as the threshold to filter part of speeches and dependencies. Then, we get 10 outstanding part of speeches, including ‘NOUN’, ‘PROPN’, ‘PUNCT’, ‘NUM’, ‘ADJ’, ‘VERB’, ‘SYM’, ‘CCONJ’, ‘ADP’, and ‘PART’, and 18 outstanding dependencies, including ‘compound’, ‘punct’, ‘nmod’, ‘amod’, ‘pobj’, ‘conj’, ‘appos’, ‘det’, ‘nummod’, ‘npadvmod’, ‘cc’, ‘nsubj’, ‘dobj’, ‘prep’, ‘nsubjpass’, ‘acl’, ‘root’ and ‘case’.</p>
      <p id="Par46">In addition, for part of speeches, we introduce ‘OTHERS’ to represent the set of indistinctive part of speeches. In addition, we also introduce ‘CLS’, ‘SEP’, and ‘X’ for BioBERT’s tokenization, where ‘CLS’ represents the start tag of a sentence, ‘SEP’ represents the end tag of a sentence, and ‘X’ represents tokens starting with ‘##’. For dependencies, we introduce ‘others’ to represent the set of indistinctive dependencies, and we introduce ‘next’ to link the tokens which are segmented from a single word. Moreover, given that dependencies have directions, we make a distinction between in-degree and out-degree for each dependency, which means each dependency is represented by two dimensions. We choose the one-hot strategy to encode the above features instead of trainable parameters which is randomly initialized to learn co-relations between syntactic labels. Because the unremarkable part of speeches and dependencies are aggregated as ‘OTHERS’ and ‘others’ respectively, and the learning process of co-relations between redefined labels may introduce additional confusion. And we get a vector of 54 dimensions for a token, which is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Fig. 3</label><caption><p>An example of using the one-hot strategy to encode syntactic features. In this example, dyskinesia, which is a named entity, is a “PROPN” in the sentence, and has 2 dependencies with other words. Moreover, the tokenizer of BioBERT cuts dyskinesia into 5 word-pieces, which inherit the syntactic features of the original word in the sentence, and are linked by “next” to indicate the order of word-pieces</p></caption><graphic xlink:href="12859_2022_5051_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec11">
      <title>Graph attention network module of BioByGANS</title>
      <p id="Par47">In this paper, based on the result of the encoding module, we use a GAT to generate the distributed representation, <inline-formula id="IEq14"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Rep$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi mathvariant="italic">Rep</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq14.gif"/></alternatives></inline-formula>, that is,<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Rep=GAT(Cont|\left|Synt\right)$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mfenced close=")" open="|"><mml:mi>S</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par48">where <inline-formula id="IEq15"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$||$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq15.gif"/></alternatives></inline-formula> denotes concatenation operation. The <inline-formula id="IEq16"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Rep$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi mathvariant="italic">Rep</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq16.gif"/></alternatives></inline-formula> is modeled as the attribute of each node, which is utilized for classification.</p>
      <p id="Par49">Graph Attention Network (GAT) [<xref ref-type="bibr" rid="CR14">14</xref>] is a graph neural network based on the masked self-attention mechanism. and can dynamically calculate weights for neighbors according to the topology of a graph and further generates the representation of the central node. And in this paper, we design a GAT for sentence graphs.</p>
      <p id="Par50">First, we use a linear transformation to calculate the coefficient between node <inline-formula id="IEq17"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M40"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq17.gif"/></alternatives></inline-formula> and node <inline-formula id="IEq18"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j$$\end{document}</tex-math><mml:math id="M42"><mml:mi>j</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq18.gif"/></alternatives></inline-formula>. That is,<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${coef}_{ij}=LeakyReLU({\overrightarrow{\mathbf{a}}}^{T}[\mathbf{W}{\overrightarrow{h}}_{i}||\mathbf{W}{\overrightarrow{h}}_{j}]) , j\in {N}_{i}$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">coef</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">a</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq19"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overrightarrow{h}}_{i}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq19.gif"/></alternatives></inline-formula> and <inline-formula id="IEq20"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overrightarrow{h}}_{j}$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq20.gif"/></alternatives></inline-formula> donates the initial attributes of node <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M50"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq21.gif"/></alternatives></inline-formula> and <inline-formula id="IEq22"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j$$\end{document}</tex-math><mml:math id="M52"><mml:mi>j</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq22.gif"/></alternatives></inline-formula> respectively, <inline-formula id="IEq23"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${N}_{i}$$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq23.gif"/></alternatives></inline-formula> donates the set of neighbors of node <inline-formula id="IEq24"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M56"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq24.gif"/></alternatives></inline-formula>, <inline-formula id="IEq25"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf{W}\in {{\varvec{R}}}^{F\times {F}^{\mathrm{^{\prime}}}}$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq25.gif"/></alternatives></inline-formula> and <inline-formula id="IEq26"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{{\varvec{a}}}\in {{\varvec{R}}}^{{2F}^{\mathrm{^{\prime}}}\times 1}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>F</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msup><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq26.gif"/></alternatives></inline-formula> are trainable matrices, <inline-formula id="IEq27"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$||$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq27.gif"/></alternatives></inline-formula> denotes concatenation operation, and <inline-formula id="IEq28"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T$$\end{document}</tex-math><mml:math id="M64"><mml:mi>T</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq28.gif"/></alternatives></inline-formula> denotes transposition operation.</p>
      <p id="Par51">Next, we normalize the coefficients using a softmax function,<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{ij}={softmax}_{j}\left({coef}_{ij}\right)=\frac{\mathrm{exp}\left({coef}_{ij}\right)}{{\sum }_{k\in {N}_{i}}\mathrm{exp}\left({coef}_{ik}\right)}$$\end{document}</tex-math><mml:math id="M66" display="block"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">softmax</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="italic">coef</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="italic">coef</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi mathvariant="normal">exp</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mrow><mml:mi mathvariant="italic">coef</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ik</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par52">Then, the normalized attention coefficients are used to calculate the attribute of the central node <inline-formula id="IEq29"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><mml:math id="M68"><mml:mi>i</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq29.gif"/></alternatives></inline-formula>,<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overrightarrow{h}}_{i}^{\mathrm{^{\prime}}}=\sigma \left({\sum }_{j\in {N}_{i}}{\alpha }_{ij} \mathbf{W}{\overrightarrow{h}}_{j}\right)$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq30"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M72"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq30.gif"/></alternatives></inline-formula> is an activation function, and we use an ELU function in this paper.</p>
      <p id="Par53">We also introduce a multi-head attention mechanism to enhance the ability of our model. Each independent attention head calculates coefficients according to Eq. <xref rid="Equ5" ref-type="">5</xref> and generates a representation of the central node according to Eq. <xref rid="Equ6" ref-type="">6</xref>. And the outputs of attention heads are concatenated.<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overrightarrow{\mathrm{h}}}_{i}^{\mathrm{^{\prime}}}={||}_{m=1}^{M} \sigma \left({\sum }_{j\in {N}_{i}}{\alpha }_{ij}^{m} {\mathbf{W}}_{m}{\overrightarrow{h}}_{j}\right)$$\end{document}</tex-math><mml:math id="M74" display="block"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="normal">h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq31"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$||$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq31.gif"/></alternatives></inline-formula> denotes concatenation operation, <inline-formula id="IEq32"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{ij}^{m}$$\end{document}</tex-math><mml:math id="M78"><mml:msubsup><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq32.gif"/></alternatives></inline-formula> donates the coefficient from the <inline-formula id="IEq33"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m$$\end{document}</tex-math><mml:math id="M80"><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq33.gif"/></alternatives></inline-formula> th attention head and <inline-formula id="IEq34"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{W}}_{m}\in {{\varvec{R}}}^{{F}^{\mathrm{^{\prime}}}}$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">R</mml:mi></mml:mrow></mml:mrow><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msup></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq34.gif"/></alternatives></inline-formula> donates the transformation matrix of the <inline-formula id="IEq35"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m$$\end{document}</tex-math><mml:math id="M84"><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq35.gif"/></alternatives></inline-formula> th attention head. In this way, the initial attributes <inline-formula id="IEq36"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$({\overrightarrow{h}}_{1}, {\overrightarrow{h}}_{2}, \dots , {\overrightarrow{h}}_{n})$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq36.gif"/></alternatives></inline-formula> are transferred into final attributes <inline-formula id="IEq37"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$({\overrightarrow{h}}_{1}^{\mathrm{^{\prime}}}, {\overrightarrow{h}}_{2}^{\mathrm{^{\prime}}}, \dots , {\overrightarrow{h}}_{n}^{\mathrm{^{\prime}}})$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:msup><mml:mrow/><mml:mo>′</mml:mo></mml:msup></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5051_Article_IEq37.gif"/></alternatives></inline-formula> through a GAT, which is used to make the final inference.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Results and discussion</title>
    <sec id="Sec13">
      <title>Datasets and experimental settings</title>
      <p id="Par54">We develop the project in the TensorFlow environment. We use 8 biomedical corpora covering genes, proteins, species, diseases, and chemicals. The statistics of the datasets are presented in Table <xref rid="Tab1" ref-type="table">1</xref>, and a description of the datasets is as follows.<list list-type="bullet"><list-item><p id="Par55"><italic>BC2GM</italic> [<xref ref-type="bibr" rid="CR54">54</xref>] is a dataset for the BioCreative II Gene Mention Recognition task. It is composed of 20130 sentences from abstracts of biomedical publications and is annotated with more than 24000 gene and protein mentions.</p></list-item><list-item><p id="Par56"><italic>JNLPBA</italic> [<xref ref-type="bibr" rid="CR55">55</xref>] is a biomedical dataset derived from the GENIA version 3.02 corpus and created by a controlled search on MEDLINE. It includes over 2000 abstracts of biomedical publications and is annotated with multiple classes of entity types.</p></list-item><list-item><p id="Par57"><italic>Species-800</italic> [<xref ref-type="bibr" rid="CR56">56</xref>] is a manually annotated corpus, which is composed of 800 publications from PubMed. And entities mainly belong to the species category.</p></list-item><list-item><p id="Par58"><italic>LINNAEUS</italic> [<xref ref-type="bibr" rid="CR57">57</xref>] is an open-source corpus for species entities and is composed of annotated full-text data from 100 publications. Additionally, entities in LINNAEUS are also normalized and mapped to NCBI taxonomy IDs. And it is also used by BioNER tools such as BioBERT as a benchmark.</p></list-item><list-item><p id="Par59"><italic>BC5CDR</italic> [<xref ref-type="bibr" rid="CR58">58</xref>] is a dataset for BioCreative V Chemical-Disease Relation Recognition task. For the BioNER task, it can also be divided into two categories, BC5CDR-Disease and BC5CDR-Chemical. It is composed of the abstracts of biomedical publications and annotated with chemicals, disease, and chemical-disease relationships in texts.</p></list-item><list-item><p id="Par60"><italic>NCBI-Disease</italic> [<xref ref-type="bibr" rid="CR59">59</xref>] is composed of abstracts of 793 publications in PubMed and annotated with nearly 7000 disease mentions. In addition, most of the mentions are mapped to the NCBI concept vocabulary.</p></list-item><list-item><p id="Par61"><italic>BC4CHEMD</italic> [<xref ref-type="bibr" rid="CR60">60</xref>] is a dataset for BioCreative IV Chemical Compound and Drug Name Recognition task. It is an open-source, manually annotated chemical corpus, and consists of 10000 abstracts of PubMed publications.</p></list-item></list><table-wrap id="Tab1"><label>Table 1</label><caption><p>Statistics of the biomedical datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Entity type</th><th align="left">Corpus name</th><th align="left">Annotated sentences</th><th align="left">Sentence max length</th></tr></thead><tbody><tr><td align="left">Gene/Protein</td><td align="left">BC2GM</td><td align="left">20,130</td><td align="left">206</td></tr><tr><td align="left"/><td align="left">JNLPBA</td><td align="left">22,401</td><td align="left">221</td></tr><tr><td align="left">Species</td><td align="left">Species-800</td><td align="left">8193</td><td align="left">143</td></tr><tr><td align="left"/><td align="left">LINNAEUS</td><td align="left">23,151</td><td align="left">246</td></tr><tr><td align="left">Disease</td><td align="left">BC5CDR-Disease</td><td align="left">13,938</td><td align="left">225</td></tr><tr><td align="left"/><td align="left">NCBI-Disease</td><td align="left">7287</td><td align="left">123</td></tr><tr><td align="left">Chemical</td><td align="left">BC4CHEMD</td><td align="left">87,674</td><td align="left">225</td></tr><tr><td align="left"/><td align="left">BC5CDR-Chemical</td><td align="left">13,938</td><td align="left">225</td></tr></tbody></table></table-wrap></p>
      <p id="Par62">Datasets we use in this paper are pre-preprocessed and provided by Lee et al. [<xref ref-type="bibr" rid="CR11">11</xref>], each of which is divided into training, developing, and testing sets, and we use precision, recall, and F1-score to evaluate our model, of which the calculation formulas are as follows.<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Precision=\frac{TP}{TP+FP}$$\end{document}</tex-math><mml:math id="M90" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Recall=\frac{TP}{TP+FN}$$\end{document}</tex-math><mml:math id="M92" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F1\_Score=\frac{2\times Precision\times Recall}{Precision+Recall}$$\end{document}</tex-math><mml:math id="M94" display="block"><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mi>_</mml:mi><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="12859_2022_5051_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where TP donates true positive, FP donates false positive, and FN donates false negative. Specifically, during the inferring process, a positive of our model contains all the words in its left and right boundaries (entities with multi-word must be completely captured), which is the same strategy of the baseline methods in the following section. Moreover, hyper-parameters of BioByGANS are listed in Table <xref rid="Tab2" ref-type="table">2</xref>, where msl denotes max sequence length, bs means batch size, lr means learning rate of the model, and layer means the number of GAT layers, and head and unit mean the numbers of heads and units in each GAT layer.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The hyper-parameters of BioByGANS</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">msl</th><th align="left">bs</th><th align="left">lr</th><th align="left">Layer</th><th align="left">Head</th><th align="left">Unit</th></tr></thead><tbody><tr><td align="left">BC2GM</td><td align="left">256</td><td align="left">32</td><td align="left">5e−5</td><td align="left">2</td><td align="left">12</td><td align="left">64</td></tr><tr><td align="left">BC4CHEMD</td><td align="left">256</td><td align="left">32</td><td align="left">3e−5</td><td align="left">4</td><td align="left">12</td><td align="left">64</td></tr><tr><td align="left">BC5CDR-chem</td><td align="left">256</td><td align="left">32</td><td align="left">3e−5</td><td align="left">4</td><td align="left">12</td><td align="left">64</td></tr><tr><td align="left">BC5CDR-disease</td><td align="left">256</td><td align="left">32</td><td align="left">3e−5</td><td align="left">4</td><td align="left">12</td><td align="left">64</td></tr><tr><td align="left">NCBI-disease</td><td align="left">256</td><td align="left">32</td><td align="left">5e−5</td><td align="left">4</td><td align="left">8</td><td align="left">96</td></tr><tr><td align="left">JNLPBA</td><td align="left">256</td><td align="left">32</td><td align="left">5e−5</td><td align="left">2</td><td align="left">8</td><td align="left">96</td></tr><tr><td align="left">LINNAEUS</td><td align="left">256</td><td align="left">32</td><td align="left">5e−5</td><td align="left">4</td><td align="left">12</td><td align="left">64</td></tr><tr><td align="left">Species-800</td><td align="left">256</td><td align="left">32</td><td align="left">5e−5</td><td align="left">2</td><td align="left">12</td><td align="left">64</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec14">
      <title>BioByGANS vs. baseline methods on performance</title>
      <p id="Par63">In this section, the baseline models used in comparison to our proposed model are presented, and the results of baseline models are obtained from their original publications. Some of the following methods focuses on one or more biomedical corpora, and only BioBERT has results for all 8 corpora.</p>
      <p id="Par64">Tables <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref>, <xref rid="Tab5" ref-type="table">5</xref> and <xref rid="Tab6" ref-type="table">6</xref> shows the experimental results on BioNER datasets, where p denotes precision, r denotes recall, and f1 denotes f1-score. We choose thChem [<xref ref-type="bibr" rid="CR25">25</xref>], TaggerOne [<xref ref-type="bibr" rid="CR27">27</xref>], BiLSTM-CRF [<xref ref-type="bibr" rid="CR28">28</xref>], Att-BiLSTM-CRF [<xref ref-type="bibr" rid="CR20">20</xref>], CollaboNet [<xref ref-type="bibr" rid="CR29">29</xref>], DABLC [<xref ref-type="bibr" rid="CR30">30</xref>], MTM-CW [<xref ref-type="bibr" rid="CR31">31</xref>], BioKMNER [<xref ref-type="bibr" rid="CR32">32</xref>], BioELECTRA [<xref ref-type="bibr" rid="CR35">35</xref>], BioBERT-MRC [<xref ref-type="bibr" rid="CR33">33</xref>], MTL-LS [<xref ref-type="bibr" rid="CR34">34</xref>], and BioBERT [<xref ref-type="bibr" rid="CR11">11</xref>] as our baseline methods. And BioBERT is the main baseline for our model.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of BioNER for chemicals</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method\Dataset</th><th align="left" colspan="3">BC4CHEMD</th><th align="left" colspan="3">BC5CDR-chemical</th></tr><tr><th align="left">p</th><th align="left">r</th><th align="left">f1</th><th align="left">p</th><th align="left">r</th><th align="left">f1</th></tr></thead><tbody><tr><td align="left">tmChem [<xref ref-type="bibr" rid="CR25">25</xref>]</td><td align="left">89.09</td><td align="left">85.75</td><td align="left">87.39</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">TaggerOne [<xref ref-type="bibr" rid="CR27">27</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">94.20</td><td align="left">88.80</td><td align="left">91.40</td></tr><tr><td align="left">BiLSTM-CRF [<xref ref-type="bibr" rid="CR28">28</xref>]</td><td align="left">91.31</td><td align="left">87.73</td><td align="left">89.48</td><td align="left">92.82</td><td align="left">88.52</td><td align="left">90.62</td></tr><tr><td align="left">Att-BiLSTM-CRF [<xref ref-type="bibr" rid="CR20">20</xref>]</td><td align="left">92.29</td><td align="left">90.01</td><td align="left">91.14</td><td align="left">93.49</td><td align="left">91.68</td><td align="left">92.57</td></tr><tr><td align="left">CollaboNet [<xref ref-type="bibr" rid="CR29">29</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">94.26</td><td align="left">92.38</td><td align="left">93.31</td></tr><tr><td align="left">MTM-CW [<xref ref-type="bibr" rid="CR31">31</xref>]</td><td align="left">91.30</td><td align="left">87.53</td><td align="left">89.37</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">BioKMNER [<xref ref-type="bibr" rid="CR32">32</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">94.00</td></tr><tr><td align="left">BioBERT-MRC [<xref ref-type="bibr" rid="CR33">33</xref>]</td><td align="left"><bold>93.89</bold></td><td align="left">91.96</td><td align="left">92.92</td><td align="left">94.37</td><td align="left">94.00</td><td align="left">94.19</td></tr><tr><td align="left">MTL-LS [<xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">92.42</td><td align="left">–</td><td align="left">–</td><td align="left">93.83</td></tr><tr><td align="left">BioELECTRA[<xref ref-type="bibr" rid="CR35">35</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">93.60</td></tr><tr><td align="left">BioBERT [<xref ref-type="bibr" rid="CR11">11</xref>]</td><td align="left">92.80</td><td align="left">91.92</td><td align="left">92.36</td><td align="left">93.68</td><td align="left">93.26</td><td align="left">93.47</td></tr><tr><td align="left"><bold>Proposed</bold></td><td align="left">93.42</td><td align="left"><bold>92.52</bold></td><td align="left"><bold>92.97</bold></td><td align="left"><bold>94.53</bold></td><td align="left"><bold>94.95</bold></td><td align="left"><bold>94.74</bold></td></tr></tbody></table><table-wrap-foot><p>Bold indicates the best performances of models in each subtask</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison of BioNER for diseases</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method\Dataset</th><th align="left" colspan="3">NCBI-disease</th><th align="left" colspan="3">BC5CDR-disease</th></tr><tr><th align="left">p</th><th align="left">r</th><th align="left">f1</th><th align="left">p</th><th align="left">r</th><th align="left">f1</th></tr></thead><tbody><tr><td align="left">TaggerOne [<xref ref-type="bibr" rid="CR27">27</xref>]</td><td align="left">85.10</td><td align="left">80.80</td><td align="left">82.90</td><td align="left">85.20</td><td align="left">80.20</td><td align="left">82.60</td></tr><tr><td align="left">BiLSTM-CRF [<xref ref-type="bibr" rid="CR28">28</xref>]</td><td align="left">86.11</td><td align="left">85.49</td><td align="left">85.80</td><td align="left">87.60</td><td align="left">86.25</td><td align="left">86.92</td></tr><tr><td align="left">CollaboNet [<xref ref-type="bibr" rid="CR29">29</xref>]</td><td align="left">85.61</td><td align="left">82.61</td><td align="left">84.08</td><td align="left">85.61</td><td align="left">82.61</td><td align="left">84.08</td></tr><tr><td align="left">MTM-CW [<xref ref-type="bibr" rid="CR31">31</xref>]</td><td align="left">85.86</td><td align="left">86.42</td><td align="left">86.14</td><td align="left"><bold>89.10</bold></td><td align="left">88.47</td><td align="left"><bold>88.78</bold></td></tr><tr><td align="left">DABLC [<xref ref-type="bibr" rid="CR30">30</xref>]</td><td align="left">88.30</td><td align="left">89.01</td><td align="left">88.60</td><td align="left"><bold>89.10</bold></td><td align="left">87.50</td><td align="left">88.30</td></tr><tr><td align="left">BioKMNER [<xref ref-type="bibr" rid="CR32">32</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">90.08</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">BioBERT-MRC [<xref ref-type="bibr" rid="CR33">33</xref>]</td><td align="left">89.67</td><td align="left">90.42</td><td align="left">90.04</td><td align="left">88.61</td><td align="left">87.07</td><td align="left">87.83</td></tr><tr><td align="left">MTL-LS [<xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">89.25</td><td align="left">–</td><td align="left">–</td><td align="left">87.28</td></tr><tr><td align="left">BioELECTRA[<xref ref-type="bibr" rid="CR35">35</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">89.38</td><td align="left">–</td><td align="left">–</td><td align="left">85.84</td></tr><tr><td align="left">BioBERT [<xref ref-type="bibr" rid="CR11">11</xref>]</td><td align="left">88.22</td><td align="left">91.25</td><td align="left">89.71</td><td align="left">86.47</td><td align="left">87.84</td><td align="left">87.15</td></tr><tr><td align="left"><bold>Proposed</bold></td><td align="left"><bold>89.99</bold></td><td align="left"><bold>93.20</bold></td><td align="left"><bold>91.57</bold></td><td align="left">86.69</td><td align="left"><bold>88.82</bold></td><td align="left">87.74</td></tr></tbody></table><table-wrap-foot><p>Bold indicates the best performances of models in each subtask</p></table-wrap-foot></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Comparison of BioNER for genes and proteins</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method\dataset</th><th align="left" colspan="3">BC2GM</th><th align="left" colspan="3">JNLPBA</th></tr><tr><th align="left">p</th><th align="left">r</th><th align="left">f1</th><th align="left">p</th><th align="left">r</th><th align="left">f1</th></tr></thead><tbody><tr><td align="left">BiLSTM-CRF [<xref ref-type="bibr" rid="CR28">28</xref>]</td><td align="left">81.57</td><td align="left">79.48</td><td align="left">80.51</td><td align="left">71.35</td><td align="left">75.74</td><td align="left">73.48</td></tr><tr><td align="left">CollaboNet [<xref ref-type="bibr" rid="CR29">29</xref>]</td><td align="left">80.49</td><td align="left">78.99</td><td align="left">79.73</td><td align="left">74.43</td><td align="left">83.22</td><td align="left">78.58</td></tr><tr><td align="left">MTM-CW [<xref ref-type="bibr" rid="CR31">31</xref>]</td><td align="left">82.10</td><td align="left">79.42</td><td align="left">80.74</td><td align="left">70.91</td><td align="left">76.34</td><td align="left">73.52</td></tr><tr><td align="left">BioKMNER [<xref ref-type="bibr" rid="CR32">32</xref>]</td><td align="left">-</td><td align="left">-</td><td align="left">84.92</td><td align="left">-</td><td align="left">-</td><td align="left">77.72</td></tr><tr><td align="left">BioBERT-MRC [<xref ref-type="bibr" rid="CR33">33</xref>]</td><td align="left"><bold>87.04</bold></td><td align="left">83.98</td><td align="left"><bold>85.48</bold></td><td align="left"><bold>75.96</bold></td><td align="left">82.13</td><td align="left">78.93</td></tr><tr><td align="left">MTL-LS [<xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left"><bold>–</bold></td><td align="left"><bold>–</bold></td><td align="left">82.92</td><td align="left"><bold>–</bold></td><td align="left"><bold>–</bold></td><td align="left"><bold>–</bold></td></tr><tr><td align="left">BioELECTRA[<xref ref-type="bibr" rid="CR35">35</xref>]</td><td align="left"><bold>–</bold></td><td align="left"><bold>–</bold></td><td align="left">84.69</td><td align="left"><bold>–</bold></td><td align="left"><bold>–</bold></td><td align="left"><bold>80.07</bold></td></tr><tr><td align="left">BioBERT [<xref ref-type="bibr" rid="CR11">11</xref>]</td><td align="left">84.32</td><td align="left">85.12</td><td align="left">84.72</td><td align="left">72.24</td><td align="left">83.56</td><td align="left">77.49</td></tr><tr><td align="left"><bold>Proposed</bold></td><td align="left">84.97</td><td align="left"><bold>85.32</bold></td><td align="left">85.15</td><td align="left">72.69</td><td align="left"><bold>84.54</bold></td><td align="left">78.16</td></tr></tbody></table><table-wrap-foot><p>Bold indicates the best performances of models in each subtask</p></table-wrap-foot></table-wrap><table-wrap id="Tab6"><label>Table 6</label><caption><p>Comparison of BioNER for species</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method\dataset</th><th align="left" colspan="3">LINNAEUS</th><th align="left" colspan="3">Species-800</th></tr><tr><th align="left">p</th><th align="left">r</th><th align="left">f1</th><th align="left">p</th><th align="left">r</th><th align="left">f1</th></tr></thead><tbody><tr><td align="left">BioKMNER [<xref ref-type="bibr" rid="CR32">32</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">88.79</td><td align="left">–</td><td align="left">–</td><td align="left"><bold>76.21</bold></td></tr><tr><td align="left">MTL-LS [<xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left">–</td><td align="left">–</td><td align="left">86.37</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">BioBERT [<xref ref-type="bibr" rid="CR11">11</xref>]</td><td align="left">90.77</td><td align="left">85.83</td><td align="left">88.24</td><td align="left"><bold>72.80</bold></td><td align="left">75.36</td><td align="left">74.06</td></tr><tr><td align="left"><bold>Proposed</bold></td><td align="left"><bold>93.91</bold></td><td align="left"><bold>88.25</bold></td><td align="left"><bold>90.99</bold></td><td align="left">71.53</td><td align="left"><bold>78.83</bold></td><td align="left">75.01</td></tr></tbody></table><table-wrap-foot><p>Bold indicates the best performances of models in each subtask</p></table-wrap-foot></table-wrap></p>
      <p id="Par65">Compared with other methods, the proposed model and BioBERT get good performance in most datasets, which indicates models based on pre-training strategy are more stable than other methods. Compared with BioBERT, our model gets significant promote on recall in all 8 datasets, which increases 3.47% at most (Species-800), due to the supplement of syntactic features. That is, if a token, of which the part of speech is ‘NOUN’, is linked with a token with ‘VERB’ as its part of speech through ‘nsubj’, the former has a higher probability of being an entity. In addition, our model gets promote on precision in 7 datasets, which increases 1.10% in average,except Species-800 (−1.27%). Overall, the proposed model gets higher F1-score in all 8 datasets than BioBERT, which increases 2.75% at most and 0.43% at least.</p>
      <p id="Par66">As shown in Tables <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>, BioBERT-MRC gets a slimly better precision in BC4CHEMD, and gets better precision in BC5CDR-disease, BC2GM and JNLPBA than our model because its queries are pre-defined based on prior knowledge in biomedical field and suitable for the above datasets. However, BioBERT-MRC gets both worse precision in NCBI-Disease and BC5CDR-Chemical and worse recall in all datasets than our model, which indicates the performance of BioBERT-MRC depends on the quality of the queries and matched-degree between queries and corpora. In addition, a prerequisite of BioBERT-MRC is carefully designed queries, which needs more manual costs and means additional prior information beyond the original corpus is transmitted to the model. Compared to the automatically end-to-end processing of the same corpus of our model, the result of BioBERT-MRC is biased to some extent, and the performance of BioBERT-MRC is not robust enough.</p>
      <p id="Par67">As shown in Tables <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>, MTM-CW gets better precision and F1 score in BC5CDR-Disease, CollaboNet gets a better precision and BioELECTRA gets a better F1-score in JNLPBA than our model. However, the above 3 methods get poorer precision, recall, and F1-score than the proposed model in other datasets, which indicates their performance is unstable to some extent. For instance, MTM-CW gets 4.64% lower on F1-score (JNLPBA), BioELECTRA gets 1.90% lower on F1-score (BC5CDR-Disease) and CollaboNet gets 4% lower on all evaluation metrics (BC2GM) than our model. CollaboNet and MTM-CW are multi-task models, and the above 2 corpora have both annotations for multiple entity types in each sentence, which means multi-task learning may improves the model for corpora with annotations of multi-type entities.</p>
      <p id="Par68">As shown in Table <xref rid="Tab6" ref-type="table">6</xref>, although BioKMNER gets better F1-score in Species-800, it gets poorer performances in any other datasets than our model. For example, BioKMNER achieves 1.49% lower on F1-score (NCBI-disease) than our model.</p>
      <p id="Par69">Overall, our model outperforms other models in BioNER tasks for different entities in most datasets on precision, recall and F1 score because of its ability to capture both contextual and syntactic features, and make good use of the topology information.</p>
    </sec>
    <sec id="Sec15">
      <title>The effect of different parameters of GAT on performance</title>
      <p id="Par70">In this section, we first compare the effect of various layers of GAT on performance of BioByGANS. Specifically, we fix the number of heads and units of each layer as 12 and 64, and prepare 5 alternative parameters, 1, 2, 4, 8, and 12, to explore the effect of GAT layer change. In addition, we use 4 corpora, BC2GM, BC5CDR-chem, BC5CDR-disease, and Species-800, for the test.</p>
      <p id="Par71">Figure <xref rid="Fig4" ref-type="fig">4</xref> illustrates the performance comparison for various layers of GAT. This experiment result shows that appropriately increasing layers of GAT are useful for BioNER tasks on most corpora. Moreover, we noticed that the performance on BC5CDR-chem with 4 GAT layers (94.74% in the average F1-score) is superior to it with 1 GAT layer (94.20% in the average F1-score). On the other hand, if the GAT is too deep, the model may get the poor performance. The performance on BC5CDR-chem with 4 GAT layers is superior to it with 12 GAT layer (93.39% in the average F1-score), which is caused by overfitting. As the result shows, 2 or 4 is a suitable choice for the number of GAT layers. In addition, as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, our model gets best performance on BC2GM and Species-800 with 2 GAT layers, while on BC5CDR-chem and BC5CDR-disease with 4 GAT layers. We investigate sentences from these biomedical datasets and notice that the characteristics of the syntactic expressions are quite different from each other because research papers on different topics may have their own sentence structures and style, which indicates that the choice of numbers of GAT layer depends on the syntactic expression of sentences in corpora.<fig id="Fig4"><label>Fig. 4</label><caption><p>The performance comparison for different number of GAT layers</p></caption><graphic xlink:href="12859_2022_5051_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par72">Then, we compare the effect of various heads&amp;units of each GAT layer on performance. To guarantee the output dimension fixed, we make the product of units and heads in each layer constant. Specifically, we fix the number of layers as 1, and prepare 6 alternative parameter groups, (1,768), (2,384), (4,192), (6,128), (8,96), and (12,64), to explore the effect of GAT head&amp;unit change. In addition, we use 2 corpora, BC5CDR-chem and NCBI-disease for the test.</p>
      <p id="Par73">Figure <xref rid="Fig5" ref-type="fig">5</xref> illustrates the performance comparison for various heads&amp;units of GAT. This experiment result shows the GAT with multi-head attention mechanism obtains a more comprehensive representation for nodes. We noticed that the model gets the best performances with 12 heads on BC5CDR-chem, and 8 heads on NCBI-disease, which is better than the model without additional attention heads. Overall, the multi-head attention mechanism can get node features from different aspects, and the parameters of attention heads and units need to be selected according to the actual data.<fig id="Fig5"><label>Fig. 5</label><caption><p>The performance comparison for different number of head&amp;units for each GAT layer</p></caption><graphic xlink:href="12859_2022_5051_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec16">
      <title>Ablation studies</title>
      <p id="Par74">To better understand the relative importance of each component for the token representation, we perform ablation studies in this section. Specifically, we fix the number of heads and units of each layer as 12 and 64, and layers as 4. Moreover, we use BC5CDR-chem for the test. As shown in Table <xref rid="Tab7" ref-type="table">7</xref>, removing the syntactic features significantly impairs the performance of the model, of which F1-score drops 1.24%.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Ablation studies on BC5CDR-chem corpus</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Representation component</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">BioBERT + POS + Dependency</td><td char="." align="char"><bold>94.53</bold></td><td char="." align="char"><bold>94.95</bold></td><td char="." align="char"><bold>94.74</bold></td></tr><tr><td align="left">-POS</td><td char="." align="char">94.11</td><td char="." align="char">94.41</td><td char="." align="char">94.26</td></tr><tr><td align="left">-Dependency</td><td char="." align="char">94.10</td><td char="." align="char">94.74</td><td char="." align="char">94.42</td></tr><tr><td align="left">-POS and Dependency</td><td char="." align="char">93.13</td><td char="." align="char">93.87</td><td char="." align="char">93.50</td></tr></tbody></table><table-wrap-foot><p>Bold indicates the best performances of models in each subtask</p></table-wrap-foot></table-wrap></p>
      <p id="Par75">Then, we evaluate the impact of part of speech (POS) and dependency solely. And the results show that model with part of speech component increases 0.92% in F1-score, and model with dependency increases 0.76% in F1-score, which means the syntactic features do help improve the model performance. Moreover, according to the present results, it can be concluded that both POS and dependency contribute to model performance. We also notice that model without dependency performs better than model without POS, which may indicate that POS is a more important feature for NER, or that a portion of dependency feature (geometric topology) has been utilized in the form of adjacent matrix in GAT layers. However, the syntactic parse results from SpaCy are not 100% precise. Especially, the performance of SpaCy for POS tagging is better than that for dependency parsing. Therefore, the above result is also probably because SpaCy leads more noise in dependency parsing than POS tagging. Above all, it is difficult to assert which contributes more to the NER task, POS or dependency. Further research is needed to prove which is more important to NER task.</p>
      <p id="Par76">Moreover, we investigate the impact of the topological feature to our model. A model with dependency represents a model with topological information, which is because topology contains not only the graph structure (geometry), but also attributes of nodes and edges in graph, which are both from the syntactic dependency tree. As shown in Table <xref rid="Tab7" ref-type="table">7</xref>, model without POS (“-POS”) outperforms model without POS and dependency (“-POS &amp; Dependency”), and gets a promotion of 0.98%, 0.54% and 0.76% on precision, recall and f1-score respectively, which indicates the ability of our model to capture topological information, and topology does have positive effects on model performance.</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Discussion and limitation</title>
    <p id="Par77">Our model achieves better performances for the following reasons. (1) BioByGANS learns better word representations from BioBERT and SpaCy respectively, which involves contextual and syntactic features. (2) We use a graph to model the topology of a sentence and formulate the BioNER task as a node classification problem, which is then solved through GAT layers. Because the topology of a sentence is more of a graph than a sequence according to the constituency and dependency.</p>
    <p id="Par78">As for the generalization of our model, the proposed model framework is able to be migrated to other domains instead of being limited to the biomedical domain. BioNER is more complicated than NER in general domains to some extent. Compared with general texts, biological texts contain more entity types, and each sentence may contain multiple types and numbers of named entities. Besides, biological texts contain longer and more complex sentences. Hence, the proposed model may also get good performances in NER of other fields theoretically.</p>
    <p id="Par79">As for the processing time of the model, we have conducted experiments to test the speed of our model in training process and inferring process. The experimental environment is a 24-core, Inter® Xeon® Gold 6248R CPU, 3.0 GHz-frequency, with a single A100 PCIE 40 GB GPU and 512 MB-memory server. The operating system is 64-bit Ubuntu 16.04.4 LTS (GNU / Linux 4.13.0–36 -generic x86_64). And we have performed the test on BC5CDR-chem dataset, with epochs as 50, max sequence length as 256, batch size as 32, learning rate as 3e-5, graph attention layers as 4, and head&amp;units as 12&amp;64. As for the training process, the model cost about 95 min, where it took about 5 min to preprocess the sentences in training set of BC5CDR-chem, and it took about 90 min to train the neural network architecture, including fine-tuning BioBERT and training the graph attention network and classifier. And as for the inferring process, our model implemented NER for more than 1700 sentences in a minute, which indicates the efficiency of our proposed model. One point that should be concerned about the inferring process is that the calculation of the neural network is fast, which took only about 50 s, while the preprocessing time for sentences in testing set was about 2 min. Considering the CPU-based preprocessing is independent from the GPU-based neural network inferring, our model can be further accelerated through pipelining and parallel processing. We have also tested the speed of BioBERT with the same hyper-parameters, of which the result is BioBERT took about 60 min to training and fine-tuning, and it could process sentences in testing set of BC5CDR-chem in about 35 s during the inferring process. Considering the significant improvement in performance compared with BioBERT, the extra time cost of our model is tolerable. In addition, [<xref ref-type="bibr" rid="CR51">51</xref>] and [<xref ref-type="bibr" rid="CR52">52</xref>] also applied SpaCy to preprocess the syntactic features, which also indicates SpaCy is a reliable and efficient tool to implement this task and saves as much processing time as possible.</p>
    <p id="Par80">However, the proposed model has some limitations. First, errors from SpaCy in tagging part of speeches and dependencies may cause the error drift of our model. For example, using a NLP tool which is specifically for biomedical field may reduce the error drift to some extent [<xref ref-type="bibr" rid="CR61">61</xref>]. The transformer-based biomedical language package for SpaCy need to be trained in the future work. In addition, a transfer learning strategy which combines BioNER with the tagging of syntactic labels may also help to improve the model. Second, as for the decoder of BioByGANS, the softmax function fails to take advantage of the transition probability of labels between nodes, which may cause the error of our model. As can be seen in Table <xref rid="Tab4" ref-type="table">4</xref>, precision on BC5CDR-disease of our model is not good enough, so we investigate the error instances, which is shown in Table <xref rid="Tab8" ref-type="table">8</xref>. For case 1&amp;2, a coordinative component of disease entities and an object structure which describe pathological states are recognized respectively. For case 3, the modifier of an entity is recognized as the beginning of it. That means, the model expands the searching scope of entities according to syntactic features, which increases the recall but sacrifices the precision.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Error instances in BC5CDR-disease dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">No</th><th align="left">Gold standard</th><th align="left">Result of BioByGANS</th></tr></thead><tbody><tr><td align="left">1</td><td align="left"><bold>Azotemia</bold>, body fluid insufficiency and <bold>bacterial infections</bold> were frequently found in these patients</td><td align="left"><bold>Azotemia</bold>, <bold>body fluid insufficiency</bold> and <bold>bacterial infections</bold> were frequently found in these patients</td></tr><tr><td align="left">2</td><td align="left">RESULTS: Both forms of stress led to prolonged but reversible systolic and diastolic dysfunction</td><td align="left">RESULTS: Both forms of stress led to prolonged but reversible <bold>systolic and diastolic dysfunction</bold></td></tr><tr><td align="left">3</td><td align="left">A case of bilateral <bold>optic neuropathy</bold> in a patient on tacrolimus (FK506) therapy after liver transplantation</td><td align="left">A case of <bold>bilateral optic neuropathy</bold> in a patient on tacrolimus (FK506) therapy after liver transplantation</td></tr></tbody></table><table-wrap-foot><p>Bold indicates the best performances of models in each subtask</p></table-wrap-foot></table-wrap></p>
  </sec>
  <sec id="Sec18">
    <title>Conclusion</title>
    <p id="Par81">In this paper, we use BioBERT in the node classification framework to implement BioNER and propose an end-to-end method, BioByGANS. Compared with using BioBERT in the sequence labeling framework, BioByGANS has a significantly stronger ability to recognize various biomedical entities. Moreover, the proposed model can solve the problem of underutilizing syntactic features. The experiment results show that our model outperforms different baseline models in most biomedical datasets, which demonstrates the effectiveness and robustness of BioByGANS. In the future, we plan to design a decoder for the graphic topology, such as a nonlinear-chain CRF, to get better performances for distributed representations generated by graph neural networks in the node classification framework. Moreover, we plan to search for a better syntactic parsing tool which is specifically for biomedical texts and based on deep neural networks to achieve better performance. Multi-task learning should also be considered to improve the model.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>BioNER</term>
        <def>
          <p id="Par4">Biomedical named entity recognition</p>
        </def>
      </def-item>
      <def-item>
        <term>BioBERT</term>
        <def>
          <p id="Par5">Biomedical bidirectional encoder representations from transformers</p>
        </def>
      </def-item>
      <def-item>
        <term>NER</term>
        <def>
          <p id="Par6">Named entity recognition</p>
        </def>
      </def-item>
      <def-item>
        <term>OOV</term>
        <def>
          <p id="Par7">Out of vocabulary</p>
        </def>
      </def-item>
      <def-item>
        <term>NLP</term>
        <def>
          <p id="Par8">Natural language processing</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p id="Par9">Long short Term Memory</p>
        </def>
      </def-item>
      <def-item>
        <term>CRF</term>
        <def>
          <p id="Par10">Conditional random field</p>
        </def>
      </def-item>
      <def-item>
        <term>SOTA</term>
        <def>
          <p id="Par11">State of the art</p>
        </def>
      </def-item>
      <def-item>
        <term>GAT</term>
        <def>
          <p id="Par12">Graph attention network</p>
        </def>
      </def-item>
      <def-item>
        <term>CBOW</term>
        <def>
          <p id="Par13">Continuous bag of words</p>
        </def>
      </def-item>
      <def-item>
        <term>MLM</term>
        <def>
          <p id="Par14">Masked language model</p>
        </def>
      </def-item>
      <def-item>
        <term>NSP</term>
        <def>
          <p id="Par15">Next sentence prediction</p>
        </def>
      </def-item>
      <def-item>
        <term>GNN</term>
        <def>
          <p id="Par16">Graph neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>GCN</term>
        <def>
          <p id="Par17">Graph convolutional network</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Our gratitude goes to the developers of corpora used in this paper, including BC2GM, JNLPBA, BC5CDR, BC4CHEMD, LINNAEUS, Species-800, NCBI-Disease. Their excellent work and the public resources enable us to engage in this research.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>XZ designs the study, implements the code, performs the experiments, analyzes the results and writes the paper. HD implements the code and analyzes the results. XL implements the code and performs the experiments. FT analyzes the results and writes the paper. WS analyzes the results and writes the paper. DZ designs the study, analyzes the results and writes the paper. All authors read and approved the final manuscripts.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>We make the source code and model available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zxw1995shawn/BioByGANS">https://github.com/zxw1995shawn/BioByGANS</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par82">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par83">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par84">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chinchor</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Robinson</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>MUC-7 named entity task definition</article-title>
        <source>Proc 7th Conf Message Underst</source>
        <year>1997</year>
        <volume>29</volume>
        <fpage>1</fpage>
        <lpage>21</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alshaikhdeeb</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Ahmad</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Biomedical named entity recognition: a review</article-title>
        <source>Int J Adv Sci Eng Inf Technol</source>
        <year>2016</year>
        <volume>6</volume>
        <issue>6</issue>
        <fpage>889</fpage>
        <lpage>895</lpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Perera</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Named entity recognition and relation detection for biomedical information extraction</article-title>
        <source>Front Cell Dev Biol</source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>673</fpage>
        <?supplied-pmid 32984300?>
        <pub-id pub-id-type="pmid">32984300</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sutskever</surname>
            <given-names>I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sequence to sequence learning with neural networks</article-title>
        <source>Proc 27th Int Conf Neural Inf Process Syst</source>
        <year>2014</year>
        <volume>2</volume>
        <fpage>3104</fpage>
        <lpage>3112</lpage>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Mikolov T et al. Efficient estimation of word representations in vector space. In: Proceedings of Workshop at International Conference on Learning Representations (ICLR). 2013.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Peters ME et al. Deep contextualized word representations. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, New Orleans, Louisiana. 2018; 1, p. 2227–2237</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Devlin J et al. Bert: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2018; 1, p. 4171–4186.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BioWordVec, improving biomedical word embeddings with subword information and MeSH</article-title>
        <source>Sci Data</source>
        <year>2019</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="pmid">30647409</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Jin Q et al. Probing biomedical embeddings from language models. In: Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP. 2019; p. 82–89.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <issue>4</issue>
        <fpage>1234</fpage>
        <lpage>1240</lpage>
        <?supplied-pmid 31501885?>
        <pub-id pub-id-type="pmid">31501885</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Fromkin</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <source>An Introduction to Language</source>
        <year>2013</year>
        <edition>10</edition>
        <publisher-loc>Victoria</publisher-loc>
        <publisher-name>Cengage Learning</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Honnibal M, Montani I. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. 2017; Homepage: <ext-link ext-link-type="uri" xlink:href="https://spacy.io/">https://spacy.io/</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Veličković, P., et al. Graph attention networks. In: Proceedings of International Conference on Learning Representations (ICLR). 2018.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fukuda</surname>
            <given-names>K-I</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Toward information extraction: identifying protein names from biological papers</article-title>
        <source>Pac Symp Biocomput</source>
        <year>1998</year>
        <volume>707</volume>
        <issue>18</issue>
        <fpage>707</fpage>
        <lpage>718</lpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krauthammer</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Using BLAST for identifying gene and protein names in journal articles</article-title>
        <source>Gene</source>
        <year>2000</year>
        <volume>259</volume>
        <issue>1–2</issue>
        <fpage>245</fpage>
        <lpage>252</lpage>
        <?supplied-pmid 11163982?>
        <pub-id pub-id-type="pmid">11163982</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Kazama JI et al. Tuning support vector machines for biomedical named entity recognition. In: Proceedings of the ACL-02 workshop on Natural language processing in the biomedical domain, PA, USA. 2002; 3, p. 1–8.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Zhao S. Named entity recognition in biomedical texts using an HMM model. In: Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), 2004; p. 87–90.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning methods for biomedical named entity recognition: a survey and qualitative comparison</article-title>
        <source>Brief Bioinform</source>
        <year>2021</year>
        <volume>22</volume>
        <issue>6</issue>
        <fpage>282</fpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Luo</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>8</issue>
        <fpage>1381</fpage>
        <lpage>1388</lpage>
        <?supplied-pmid 29186323?>
        <pub-id pub-id-type="pmid">29186323</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dang</surname>
            <given-names>TH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>D3NER: biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embeddings of various linguistic information</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>20</issue>
        <fpage>3539</fpage>
        <lpage>3546</lpage>
        <?supplied-pmid 29718118?>
        <pub-id pub-id-type="pmid">29718118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Tong F et al. A deep network based integrated model for disease named entity recognition. In: Proceedings of 2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2017; p. 618–621.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Tong F et al. Using deep neural network to recognize mutation entities in biomedical literature. In: Proceedings of 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2018; p. 2329–2332.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>C-H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>tmVar: a text mining approach for extracting sequence variants in biomedical literature</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>11</issue>
        <fpage>1433</fpage>
        <lpage>1439</lpage>
        <?supplied-pmid 23564842?>
        <pub-id pub-id-type="pmid">23564842</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>tmChem: a high performance approach for chemical named entity recognition and normalization</article-title>
        <source>J Cheminform</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="pmid">25705261</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>C-H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>GNormPlus: an integrative approach for tagging genes, gene families, and protein domains</article-title>
        <source>BioMed Res Int</source>
        <year>2015</year>
        <volume>2015</volume>
        <fpage>918710</fpage>
        <lpage>918710</lpage>
        <?supplied-pmid 26380306?>
        <pub-id pub-id-type="pmid">26380306</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>TaggerOne: joint named entity recognition and normalization with semi-Markov Models</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>18</issue>
        <fpage>2839</fpage>
        <lpage>2846</lpage>
        <?supplied-pmid 27283952?>
        <pub-id pub-id-type="pmid">27283952</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Lample G et al. Neural architectures for named entity recognition. In: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016; p. 260–270.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yoon</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Collabonet: collaboration of deep neural networks for biomedical named entity recognition</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>10</issue>
        <fpage>55</fpage>
        <lpage>65</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Document-level attention-based BiLSTM-CRF incorporating disease dictionary for disease named entity recognition</article-title>
        <source>Comput Biol Med</source>
        <year>2019</year>
        <volume>108</volume>
        <fpage>122</fpage>
        <lpage>132</lpage>
        <?supplied-pmid 31003175?>
        <pub-id pub-id-type="pmid">31003175</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cross-type biomedical named entity recognition with deep multi-task learning</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>10</issue>
        <fpage>1745</fpage>
        <lpage>1752</lpage>
        <?supplied-pmid 30307536?>
        <pub-id pub-id-type="pmid">30307536</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tian</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improving biomedical named entity recognition with syntactic information</article-title>
        <source>BMC Bioinform</source>
        <year>2020</year>
        <volume>21</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>17</lpage>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Biomedical named entity recognition using BERT in the machine reading comprehension framework</article-title>
        <source>J Biomed Inform</source>
        <year>2021</year>
        <volume>118</volume>
        <fpage>103799</fpage>
        <?supplied-pmid 33965638?>
        <pub-id pub-id-type="pmid">33965638</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chai</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Hierarchical shared transfer learning for biomedical named entity recognition</article-title>
        <source>BMC Bioinform</source>
        <year>2022</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>14</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Kanakarajan K et al. BioELECTRA: pretrained biomedical text encoder using discriminators. In: Proceedings of the 20th Workshop on Biomedical Language Processing. 2021; p. 143–154.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bellegarda</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>Statistical language model adaptation: review and perspectives</article-title>
        <source>Speech Commun</source>
        <year>2004</year>
        <volume>42</volume>
        <issue>1</issue>
        <fpage>93</fpage>
        <lpage>108</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Mikolov T et al. Distributed representations of words and phrases and their compositionality. In: Proceedings of the 26th International Conference on Neural Information Processing Systems-Volume 2, NY, USA. 2013; 2, p. 3111–3119.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Understanding bag-of-words model: a statistical framework</article-title>
        <source>Int J Mach Learn Cybern</source>
        <year>2010</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>43</fpage>
        <lpage>52</lpage>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ramos</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Using tf-idf to determine word relevance in document queries</article-title>
        <source>Proc First Instr Conf Mach Learn</source>
        <year>2003</year>
        <volume>242</volume>
        <issue>1</issue>
        <fpage>29</fpage>
        <lpage>48</lpage>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Joulin A et al. Fasttext. zip: Compressing text classification models. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1612.03651">arXiv:1612.03651</ext-link>; 2016.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Pennington J et al. Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014; p. 1532–1543.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">McCann B et al. Learned in translation: Contextualized word vectors. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, NY, USA, 2017; p. 6297–6308.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Radford A et al. Improving language understanding by generative pre-training. 2018.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <?supplied-pmid 9377276?>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <mixed-citation publication-type="other">Vaswani A et al. Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017; p. 6000–6010.</mixed-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Bird S, Loper E. NLTK: the natural language toolkit. In: Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics. 2004; 1, p. 63–70.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Manning CD et al. The Stanford CoreNLP natural language processing toolkit. In: Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, 2014; p. 55–60.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamon</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Grabar</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Linguistic approach for identification of medication names and related information in clinical narratives</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2010</year>
        <volume>17</volume>
        <issue>5</issue>
        <fpage>549</fpage>
        <lpage>554</lpage>
        <?supplied-pmid 20819862?>
        <pub-id pub-id-type="pmid">20819862</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A comprehensive survey on graph neural networks</article-title>
        <source>IEEE Trans Neural Netw Learn Syst</source>
        <year>2020</year>
        <volume>32</volume>
        <issue>1</issue>
        <fpage>4</fpage>
        <lpage>24</lpage>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Kipf TN, Welling M. Semi-supervised classification with graph convolutional networks. In: Proceedings of the 5th International Conference on Learning Representations (ICLR). 2017.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Chen P et al. Explicitly capturing relations between entity mentions via graph neural networks for domain-specific named entity recognition. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol. 2). 2021; p. 735–742.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tran</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Syntactically-informed word representations from graph neural network</article-title>
        <source>Neurocomputing</source>
        <year>2020</year>
        <volume>413</volume>
        <fpage>431</fpage>
        <lpage>443</lpage>
        <?supplied-pmid 33162674?>
        <pub-id pub-id-type="pmid">33162674</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Robinson JJ. Dependency structures and transformational rules. Language. 1970; p. 259–285.</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Overview of BioCreative II gene mention recognition</article-title>
        <source>Genome Biol</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>2</issue>
        <fpage>1</fpage>
        <lpage>19</lpage>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Kim J-D et al. Introduction to the bio-entity recognition task at JNLPBA. In: Proceedings of the international joint workshop on natural language processing in biomedicine and its applications, 2004; p. 70–75.</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pafilis</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The SPECIES and ORGANISMS resources for fast and accurate identification of taxonomic names in text</article-title>
        <source>PLoS ONE</source>
        <year>2013</year>
        <volume>8</volume>
        <issue>6</issue>
        <fpage>e65390</fpage>
        <?supplied-pmid 23823062?>
        <pub-id pub-id-type="pmid">23823062</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerner</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>LINNAEUS: a species name identification system for biomedical literature</article-title>
        <source>BMC Bioinformat</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>17</lpage>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BioCreative V CDR task corpus: a resource for chemical disease relation extraction</article-title>
        <source>Database</source>
        <year>2016</year>
        <volume>2016</volume>
        <fpage>baw068</fpage>
        <?supplied-pmid 27161011?>
        <pub-id pub-id-type="pmid">27161011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doğan</surname>
            <given-names>RI</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>NCBI disease corpus: a resource for disease name recognition and concept normalization</article-title>
        <source>J Biomed Inform</source>
        <year>2014</year>
        <volume>47</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <?supplied-pmid 24393765?>
        <pub-id pub-id-type="pmid">24393765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The CHEMDNER corpus of chemicals and drugs and its annotation principles</article-title>
        <source>J Cheminformat</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>17</lpage>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kanerva</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dependency parsing of biomedical text with BERT</article-title>
        <source>BMC Bioinformat</source>
        <year>2020</year>
        <volume>21</volume>
        <issue>23</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
