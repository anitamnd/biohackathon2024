<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archivearticle1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Proc Math Phys Eng Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Proc Math Phys Eng Sci</journal-id>
    <journal-id journal-id-type="publisher-id">RSPA</journal-id>
    <journal-id journal-id-type="hwp">royprsa</journal-id>
    <journal-title-group>
      <journal-title>Proceedings. Mathematical, Physical, and Engineering Sciences</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1364-5021</issn>
    <issn pub-type="epub">1471-2946</issn>
    <publisher>
      <publisher-name>The Royal Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9199075</article-id>
    <article-id pub-id-type="doi">10.1098/rspa.2021.0916</article-id>
    <article-id pub-id-type="publisher-id">rspa20210916</article-id>
    <article-categories>
      <subj-group subj-group-type="discipline-codes">
        <compound-subject>
          <compound-subject-part content-type="code">1003</compound-subject-part>
        </compound-subject>
        <compound-subject>
          <compound-subject-part content-type="code">1008</compound-subject-part>
        </compound-subject>
        <compound-subject>
          <compound-subject-part content-type="code">1009</compound-subject-part>
        </compound-subject>
      </subj-group>
      <subj-group subj-group-type="subject-codes">
        <compound-subject>
          <compound-subject-part content-type="code">50</compound-subject-part>
        </compound-subject>
        <compound-subject>
          <compound-subject-part content-type="code">175</compound-subject-part>
        </compound-subject>
        <compound-subject>
          <compound-subject-part content-type="code">30</compound-subject-part>
        </compound-subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Research Articles</subject>
      </subj-group>
      <subj-group subj-group-type="type-of-publication">
        <subject>Research Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Stability selection enables robust learning of differential equations from limited noisy data</article-title>
      <alt-title alt-title-type="short">Stability selection enables robust learning of differential equations from limited noisy data</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0003-2702-4427</contrib-id>
        <name>
          <surname>Maddu</surname>
          <given-names>Suryanarayana</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Software" vocab-term-identifier="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Visualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing – original draft" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-–-original-draft/">Writing – original draft</role>
        <xref rid="af1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="af2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="af3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="af4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cheeseman</surname>
          <given-names>Bevan L.</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing – review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-–-review-editing/">Writing – review &amp; editing</role>
        <xref rid="af1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="af2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="af3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="af4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">http://orcid.org/0000-0003-4414-4340</contrib-id>
        <name>
          <surname>Sbalzarini</surname>
          <given-names>Ivo F.</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing – review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-–-review-editing/">Writing – review &amp; editing</role>
        <email>sbalzarini@mpi-cbg.de</email>
        <xref rid="af1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="af2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="af3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="af4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3821-7083</contrib-id>
        <name>
          <surname>Müller</surname>
          <given-names>Christian L.</given-names>
        </name>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Resources" vocab-term-identifier="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role vocab="credit" vocab-identifier="http://credit.niso.org/" vocab-term="Writing – review &amp; editing" vocab-term-identifier="http://credit.niso.org/contributor-roles/writing-–-review-editing/">Writing – review &amp; editing</role>
        <xref rid="af5" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <aff id="af1">
        <label>
          <sup>1</sup>
        </label>
        <addr-line>Faculty of Computer Science, <institution>Technische Universität Dresden</institution>, Dresden, <country>Germany</country></addr-line>
      </aff>
      <aff id="af2">
        <label>
          <sup>2</sup>
        </label>
        <addr-line><institution>Max Planck Institute of Molecular Cell Biology and Genetics</institution>, Dresden, <country>Germany</country></addr-line>
      </aff>
      <aff id="af3">
        <label>
          <sup>3</sup>
        </label>
        <addr-line><institution>Center for Systems Biology Dresden</institution>, Dresden, <country>Germany</country></addr-line>
      </aff>
      <aff id="af4">
        <label>
          <sup>4</sup>
        </label>
        <addr-line><institution>Cluster of Excellence Physics of Life</institution>, TU Dresden, <country>Germany</country></addr-line>
      </aff>
      <aff id="af5">
        <label>
          <sup>5</sup>
        </label>
        <addr-line>Center for Computational Mathematics, <institution>Flatiron Institute</institution>, New York, NY, <country>USA</country></addr-line>
      </aff>
    </contrib-group>
    <author-notes>
      <fn fn-type="other">
        <p>Electronic supplementary material is available online at <uri xlink:href="https://doi.org/10.6084/m9.figshare.c.6016866">https://doi.org/10.6084/m9.figshare.c.6016866</uri>.</p>
      </fn>
    </author-notes>
    <pub-date publication-format="print" date-type="pub">
      <month>6</month>
      <year>2022</year>
      <string-date>June 2022</string-date>
    </pub-date>
    <pub-date publication-format="electronic" date-type="pub">
      <day>15</day>
      <month>6</month>
      <year>2022</year>
      <string-date>June 15, 2022</string-date>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>6</month>
      <year>2022</year>
      <string-date>June 15, 2022</string-date>
    </pub-date>
    <volume>478</volume>
    <issue>2262</issue>
    <elocation-id>20210916</elocation-id>
    <history>
      <date date-type="received">
        <day>1</day>
        <month>12</month>
        <year>2021</year>
        <string-date>December 1, 2021</string-date>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>5</month>
        <year>2022</year>
        <string-date>May 12, 2022</string-date>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Authors.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Published by the Royal Society under the terms of the Creative Commons Attribution License <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>, which permits unrestricted use, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="rspa.2021.0916.pdf"/>
    <abstract>
      <p>We present a statistical learning framework for robust identification of differential equations from noisy spatio-temporal data. We address two issues that have so far limited the application of such methods, namely their robustness against noise and the need for manual parameter tuning, by proposing stability-based model selection to determine the level of regularization required for reproducible inference. This avoids manual parameter tuning and improves robustness against noise in the data. Our stability selection approach, termed PDE-STRIDE, can be combined with any sparsity-promoting regression method and provides an interpretable criterion for model component importance. We show that the particular combination of stability selection with the iterative hard-thresholding algorithm from compressed sensing provides a fast and robust framework for equation inference that outperforms previous approaches with respect to accuracy, amount of data required, and robustness. We illustrate the performance of PDE-STRIDE on a range of simulated benchmark problems, and we demonstrate the applicability of PDE-STRIDE on real-world data by considering purely data-driven inference of the protein interaction network for embryonic polarization in <italic toggle="yes">Caenorhabditis elegans</italic>. Using fluorescence microscopy images of <italic toggle="yes">C. elegans</italic> zygotes as input data, PDE-STRIDE is able to learn the molecular interactions of the proteins.</p>
    </abstract>
    <kwd-group>
      <kwd>statistical learning theory</kwd>
      <x xml:space="preserve">, </x>
      <kwd>sparse regression</kwd>
      <x xml:space="preserve">, </x>
      <kwd>differential equations</kwd>
      <x xml:space="preserve">, </x>
      <kwd>stability selection</kwd>
      <x xml:space="preserve">, </x>
      <kwd>PAR proteins</kwd>
      <x xml:space="preserve">, </x>
      <kwd>machine learning</kwd>
    </kwd-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Bundesministerium für Bildung und Forschung</institution>
            <institution-id>http://dx.doi.org/10.13039/501100002347</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>ScaDS.AI</award-id>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Deutsche Forschungsgemeinschaft</institution>
            <institution-id>http://dx.doi.org/10.13039/501100001659</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>EXC-2068</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>June 29, 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <label>1<x xml:space="preserve">. </x></label>
    <title>Introduction</title>
    <p>Predictive mathematical models, validated in experiments, are of key importance for the scientific understanding of natural phenomena. While this approach has been particularly successful in describing spatio-temporal dynamical systems in physics and engineering, it has not seen the same degree of success in other scientific fields, such as neuroscience, biology, finance, and ecology. This is because the underlying first principles in these areas remain largely elusive. Nevertheless, modelling in those areas has seen increasing use and relevance to help formulate simplified mathematical equations where sufficient observational data are available for validation [<xref rid="RSPA20210916C1" ref-type="bibr">1</xref>–<xref rid="RSPA20210916C5" ref-type="bibr">5</xref>]. In biology, modern high-throughput technologies have enabled collection of large-scale datasets, ranging from genomics, proteomics and metabolomics data, to microscopy images and videos of cells and tissues. These datasets are routinely used to infer parameters in hypothesized models, or to perform model selection among a finite number of alternative hypotheses [<xref rid="RSPA20210916C6" ref-type="bibr">6</xref>–<xref rid="RSPA20210916C8" ref-type="bibr">8</xref>]. The amount and quality of biological data, as well as the performance of computing hardware and computational methods, have now reached a level that promises direct inference of mathematical models of biological processes from the available experimental data. Such data-driven approaches seem particularly valuable in cell and developmental biology, where first principles are hard to come by, but large-scale imaging data are available, along with an accepted consensus of which phenomena a model could possibly entail. In such scenarios, data-driven modelling approaches have the potential to uncover the unknown first principles underlying the observed biological dynamics.</p>
    <p>Biological dynamics can be formalized at different scales, from discrete molecular processes to the continuum mechanics of tissues. Here, we consider the macroscopic, continuum scale where spatio-temporal dynamics are modelled by partial differential equations (PDEs) over coarse-grained state variables [<xref rid="RSPA20210916C3" ref-type="bibr">3</xref>,<xref rid="RSPA20210916C9" ref-type="bibr">9</xref>]. PDE models have been used to successfully address a range of biological problems from embryo patterning [<xref rid="RSPA20210916C10" ref-type="bibr">10</xref>] to modelling gene-expression networks [<xref rid="RSPA20210916C11" ref-type="bibr">11</xref>,<xref rid="RSPA20210916C12" ref-type="bibr">12</xref>] to predictive models of cell and tissue mechanics during growth and development [<xref rid="RSPA20210916C13" ref-type="bibr">13</xref>,<xref rid="RSPA20210916C14" ref-type="bibr">14</xref>]. They have shown their potential to recapitulate experimental observables in cases where the underlying physical phenomena are known or have been postulated [<xref rid="RSPA20210916C15" ref-type="bibr">15</xref>–<xref rid="RSPA20210916C17" ref-type="bibr">17</xref>]. In many biological systems, however, the governing PDEs are not (yet) known, which slows progress in discovering the underlying physical principles. Thus, it is desirable to verify existing models and discover new ones by extracting governing laws directly from measured spatio-temporal data.</p>
    <p>For given observable spatio-temporal dynamics, with no governing PDE known, several proposals have been put forward to learn mathematically and physically interpretable PDE models. The earliest work in this direction [<xref rid="RSPA20210916C18" ref-type="bibr">18</xref>] frames the problem of ‘PDE learning’ as a multivariate nonlinear regression problem where each component in the design matrix consists of space and time differential operators and low-order nonlinearities computed directly from data. Then, the alternating conditional expectation (ACE) algorithm [<xref rid="RSPA20210916C19" ref-type="bibr">19</xref>] is used to compute both optimal element-wise nonlinear transformations of each component and their associated coefficients. In [<xref rid="RSPA20210916C20" ref-type="bibr">20</xref>], the problem is formulated as a linear regression problem with a fixed predefined set of space and time differential operators and polynomial transformations that are computed directly from data. Then, backward elimination is used to identify a compact set of PDE components by minimizing the least-square error of the full model and pruning terms that worsen the fit the least. In the statistics literature [<xref rid="RSPA20210916C21" ref-type="bibr">21</xref>,<xref rid="RSPA20210916C22" ref-type="bibr">22</xref>], the PDE learning problem has been formulated as a Bayesian estimation problem where the observed dynamics are learned via non-parametric approximation, and a PDE representation serves as the prior to compute the posterior estimates of the PDE coefficients. Recent influential work revived the idea of jointly learning the structure <italic toggle="yes">and</italic> the coefficients of PDE models from data in discrete space and time using sparse regression [<xref rid="RSPA20210916C23" ref-type="bibr">23</xref>–<xref rid="RSPA20210916C25" ref-type="bibr">25</xref>]. Approaches such as SINDy [<xref rid="RSPA20210916C23" ref-type="bibr">23</xref>] and PDE-FIND [<xref rid="RSPA20210916C24" ref-type="bibr">24</xref>] compute a large pre-assembled dictionary of possible PDE terms from data and identify the most promising components by penalized linear regression. PDE-FIND was able to learn different types of PDEs from simulated spatio-temporal data, including Burgers, Kuramoto-Sivashinsky, reaction-diffusion, and Navier–Stokes equations. PDE-FIND’s performance was evaluated on noise-free simulated data as well as data with up to 1% additive noise and showed a critical dependence on proper tuning of the regularization parameters, which are typically unknown in practice. Recent works attempted to alleviate this dependence by using Bayesian sparse regression for model uncertainty quantification [<xref rid="RSPA20210916C26" ref-type="bibr">26</xref>] or information criteria for parameter tuning [<xref rid="RSPA20210916C27" ref-type="bibr">27</xref>]. There is also a growing body of literature that considers deep neural networks for PDE learning [<xref rid="RSPA20210916C28" ref-type="bibr">28</xref>–<xref rid="RSPA20210916C30" ref-type="bibr">30</xref>]. For instance, the deep feed-forward network PDE-NET [<xref rid="RSPA20210916C28" ref-type="bibr">28</xref>] has been shown to directly learn computable, discretized forms of the underlying governing PDEs for forecasting [<xref rid="RSPA20210916C28" ref-type="bibr">28</xref>,<xref rid="RSPA20210916C31" ref-type="bibr">31</xref>]. PDE-NET exploits the connection between differential operators and order-of-sum rules of convolution filters [<xref rid="RSPA20210916C32" ref-type="bibr">32</xref>] to constrain network layers to learning valid discretized differential operators. The forecasting capability of this approach was numerically demonstrated for predefined linear differential operator templates. A compact and interpretable symbolic identification of the PDE structure, however, is not available with this approach.</p>
    <p>Here, we ask the question whether and how it is possible to extend the class of sparse regression inference methods to work on limited amounts of noisy experimental data. We present a statistical learning framework, PDE-STRIDE (STability-based Robust IDEntification of PDEs), to robustly infer PDE models from noisy spatio-temporal data without requiring manual tuning of learning parameters, such as regularization constants. PDE-STRIDE is based on the statistical principle of stability selection [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>,<xref rid="RSPA20210916C34" ref-type="bibr">34</xref>], which provides an interpretable criterion for any term’s inclusion in the learned PDE in a data-driven manner. Stability selection can be used with any sparsity-promoting regression method, including LASSO [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>,<xref rid="RSPA20210916C35" ref-type="bibr">35</xref>], iterative hard thresholding (IHT) [<xref rid="RSPA20210916C36" ref-type="bibr">36</xref>], hard thresholding pursuit (HTP) [<xref rid="RSPA20210916C37" ref-type="bibr">37</xref>] or sequential thresholding ridge regression (STRidge) [<xref rid="RSPA20210916C24" ref-type="bibr">24</xref>]. PDE-STRIDE therefore provides a drop-in solution to rendering existing inference tools more robust, while reducing the need for parameter tuning. In our benchmarks, the combination of stability selection with de-biased iterative hard thresholding (IHT-d) empirically shows the best performance and highest consistency w.r.t. perturbations of the dictionary matrix and the sampling of the data.</p>
    <p>This paper is organized as follows: §2 provides the mathematical formulation of the sparse regression problem and discusses how the design matrix is assembled. We also review the concepts of regularization paths and stability selection and discuss how they are combined in the proposed method. The numerical results in §3 highlight the performance and robustness of PDE-STRIDE for recovering different PDEs from noise-corrupted data. We also perform achievability analysis of PDE-STRIDE + IHT-d for consistency and convergence of the recovery probabilities with increasing sample size. Section 4 demonstrates that the robustness of the proposed method is sufficient for real-world applications. We consider learning a protein interaction model from noisy biological microscopy images of membrane protein dynamics in a <italic toggle="yes">Caenorhabditis elegans</italic> zygote. Section 5 provides a summary of our results and highlights future challenges for data-driven PDE learning.</p>
  </sec>
  <sec id="s2">
    <label>2<x xml:space="preserve">. </x></label>
    <title>Problem formulation and optimization</title>
    <p>We outline the problem formulation underlying the data-driven PDE inference considered here. We review important sparse regression techniques and introduce the concept of stability selection used in PDE-STRIDE.</p>
    <sec id="s2a">
      <label>(a)<x xml:space="preserve"> </x></label>
      <title>Problem formulation for partial differential equations learning</title>
      <p>We propose a framework for stable estimation of the structure and parameters of the governing equations of continuous dynamical systems from discrete spatio-temporal measurements or observations. Specifically, we consider PDEs for the multidimensional state variable <inline-formula><mml:math id="IM1"><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> of the form shown in equation (<xref rid="RSPA20210916M2x1" ref-type="disp-formula">2.1</xref>), composed of polynomial nonlinearities (e.g. <inline-formula><mml:math id="IM2"><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>), spatial derivatives (e.g., <inline-formula><mml:math id="IM3"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) and the parametric dependence modelled through <inline-formula><mml:math id="IM4"><mml:mi>Ξ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>.
<disp-formula id="RSPA20210916M2x1"><label>2.1</label><mml:math id="DM1" display="block"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>Ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
Here, <inline-formula><mml:math id="IM5"><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is the function map that models the spatio-temporal nonlinear dynamics of the system. We limit ourselves to forms of the function map <inline-formula><mml:math id="IM6"><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> that can be written as linear combinations of polynomial nonlinearities, spatial derivatives and combinations of both. For instance, for a one-dimensional (<inline-formula><mml:math id="IM7"><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>) state variable <inline-formula><mml:math id="IM8"><mml:mi>u</mml:mi></mml:math></inline-formula>, the function map can take the form
<disp-formula id="RSPA20210916M2x2"><label>2.2</label><mml:math id="DM2" display="block"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mi>u</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder></mml:math></disp-formula>
where <inline-formula><mml:math id="IM9"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> are the coefficients of the PDE components for <inline-formula><mml:math id="IM10"><mml:mi>k</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. The continuous PDE of the form described in equation (<xref rid="RSPA20210916M2x2" ref-type="disp-formula">2.2</xref>), with appropriate coefficients <inline-formula><mml:math id="IM11"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, holds true for all continuous space and time points <inline-formula><mml:math id="IM12"><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> in the domain of the model. Numerical solutions of the PDE try to satisfy the equality relation in equation (<xref rid="RSPA20210916M2x2" ref-type="disp-formula">2.2</xref>) for reconstituting the nonlinear dynamics of a dynamical system at discrete space and time points <inline-formula><mml:math id="IM13"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. We assume that we have access to <inline-formula><mml:math id="IM14"><mml:mi>N</mml:mi></mml:math></inline-formula> noisy observational data points <inline-formula><mml:math id="IM15"><mml:msub><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the state variable <inline-formula><mml:math id="IM16"><mml:mi>u</mml:mi></mml:math></inline-formula> at such discrete space and time points. The measurement errors are assumed to be independent and identically distributed following a normal distribution with mean zero and variance <inline-formula><mml:math id="IM17"><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p>
      <p>We follow earlier works [<xref rid="RSPA20210916C20" ref-type="bibr">20</xref>,<xref rid="RSPA20210916C24" ref-type="bibr">24</xref>,<xref rid="RSPA20210916C25" ref-type="bibr">25</xref>] and construct a large dictionary of potential PDE components using discrete approximations of the terms from the data <inline-formula><mml:math id="IM18"><mml:msub><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. For instance, for the one-dimensional example in equation (<xref rid="RSPA20210916M2x2" ref-type="disp-formula">2.2</xref>), the discrete approximation of <inline-formula><mml:math id="IM19"><mml:mi>p</mml:mi></mml:math></inline-formula> PDE terms can be written in vectorized form as a linear regression problem
<disp-formula id="RSPA20210916M2x3"><label>2.3</label><mml:math id="DM3" display="block"><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi>u</mml:mi></mml:mtd><mml:mtd><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow></mml:munder><mml:mi>ξ</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>Here, the left-hand side vector <inline-formula><mml:math id="IM20"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> contains the discrete approximations of the time derivatives of <inline-formula><mml:math id="IM21"><mml:mi>u</mml:mi></mml:math></inline-formula> at the data points and represents the response or outcome vector of the linear regression. Each column of the dictionary or design matrix <inline-formula><mml:math id="IM22"><mml:mi>Θ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> represents the discrete approximation of one PDE component, i.e. one of the terms in equation (<xref rid="RSPA20210916M2x2" ref-type="disp-formula">2.2</xref>), at the <inline-formula><mml:math id="IM23"><mml:mi>N</mml:mi></mml:math></inline-formula> discretization points in space and time. Each column is interpreted as a potential predictor of the response vector <inline-formula><mml:math id="IM24"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>. The vector <inline-formula><mml:math id="IM25"><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>…</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the vector of unknown PDE coefficients, i.e. the pre-factors of the terms in equation (<xref rid="RSPA20210916M2x2" ref-type="disp-formula">2.2</xref>).</p>
      <p>Both <inline-formula><mml:math id="IM26"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="IM27"><mml:mi>Θ</mml:mi></mml:math></inline-formula> need to be constructed from numerical approximations of the temporal and spatial derivatives of the observed state variables. There is a rich literature in numerical analysis on this topic (e.g. [<xref rid="RSPA20210916C38" ref-type="bibr">38</xref>,<xref rid="RSPA20210916C39" ref-type="bibr">39</xref>]). Here, we approximate the time derivatives by first-order forward finite differences from <inline-formula><mml:math id="IM28"><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> (i.e. the explicit Euler scheme) after initial denoising of the data. Similarly, the spatial derivatives are computed by second-order central finite differences. For denoising, we use truncated singular value decomposition (SVD) with a cut-off at the elbow of the singular values curve, as shown in the electronic supplementary material, figures S1 and S2.</p>
      <p>Given the general linear regression ansatz in equation (<xref rid="RSPA20210916M2x3" ref-type="disp-formula">2.3</xref>), we formulate the data-driven PDE inference problem as a regularized optimization problem of the form
<disp-formula id="RSPA20210916M2x4"><label>2.4</label><mml:math id="DM4" display="block"><mml:msup><mml:mrow><mml:mover><mml:mi>ξ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
where <inline-formula><mml:math id="IM29"><mml:msup><mml:mrow><mml:mover><mml:mi>ξ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the minimizer of the objective function, <inline-formula><mml:math id="IM30"><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is a smooth convex data-fitting function, <inline-formula><mml:math id="IM31"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> a regularization or penalty function and <inline-formula><mml:math id="IM32"><mml:mi>λ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> is a scalar regularization parameter that balances data fitting and regularization. The function <inline-formula><mml:math id="IM33"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is not necessarily convex or differentiable. We follow previous works [<xref rid="RSPA20210916C20" ref-type="bibr">20</xref>,<xref rid="RSPA20210916C24" ref-type="bibr">24</xref>,<xref rid="RSPA20210916C25" ref-type="bibr">25</xref>] and consider the standard least-squares data-fitting term
<disp-formula id="RSPA20210916M2x5"><label>2.5</label><mml:math id="DM5" display="block"><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula>
The choice of the penalty function <inline-formula><mml:math id="IM34"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> influences the properties of the coefficient estimates <inline-formula><mml:math id="IM35"><mml:msup><mml:mrow><mml:mover><mml:mi>ξ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>. We seek to identify a small subset of PDE components among the <inline-formula><mml:math id="IM36"><mml:mi>p</mml:mi></mml:math></inline-formula> possible ones that accurately predict the time evolution of the state variables [<xref rid="RSPA20210916C23" ref-type="bibr">23</xref>–<xref rid="RSPA20210916C25" ref-type="bibr">25</xref>]. This implies that we want to identify a sparse coefficient vector <inline-formula><mml:math id="IM37"><mml:msup><mml:mrow><mml:mover><mml:mi>ξ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula>, thus resulting in an <italic toggle="yes">simple and interpretable</italic> PDE model. This can be achieved through sparsity-promoting penalty functions <inline-formula><mml:math id="IM38"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. We next consider different choices for <inline-formula><mml:math id="IM39"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> that enforce sparsity in the coefficient vector and review algorithms that solve the associated optimization problems.</p>
    </sec>
    <sec id="s2b">
      <label>(b)<x xml:space="preserve"> </x></label>
      <title>Sparse optimization for partial differential equations learning</title>
      <p>The least-squares loss in equation (<xref rid="RSPA20210916M2x5" ref-type="disp-formula">2.5</xref>) can be combined with different sparsity-promoting penalty functions <inline-formula><mml:math id="IM40"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. The prototypical example is the <inline-formula><mml:math id="IM41"><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>-norm <inline-formula><mml:math id="IM42"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> leading to the LASSO formulation of sparse linear regression [<xref rid="RSPA20210916C35" ref-type="bibr">35</xref>]:
<disp-formula id="RSPA20210916M2x6"><label>2.6</label><mml:math id="DM6" display="block"><mml:msup><mml:mrow><mml:mover><mml:mi>ξ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
The LASSO objective comprises a convex smooth loss and a convex non-smooth regularizer. For this class of problems, efficient optimization algorithms exist that can exploit the properties of the functions and come with convergence guarantees. Important examples include coordinate-descent algorithms [<xref rid="RSPA20210916C40" ref-type="bibr">40</xref>,<xref rid="RSPA20210916C41" ref-type="bibr">41</xref>] and proximal algorithms, including the Douglas–Rachford algorithm [<xref rid="RSPA20210916C42" ref-type="bibr">42</xref>] and the projected (or proximal) gradient method, also known as the forward-backward algorithm [<xref rid="RSPA20210916C43" ref-type="bibr">43</xref>]. In signal processing, the latter schemes are also known as iterative shrinkage-thresholding algorithms (ISTA, see [<xref rid="RSPA20210916C44" ref-type="bibr">44</xref>] and references therein), which can be extended to non-convex penalties. Although LASSO has been previously used for PDE learning [<xref rid="RSPA20210916C25" ref-type="bibr">25</xref>], the statistical performance of LASSO estimates is known to deteriorate if certain conditions on the design matrix are not met. For example, the studies in [<xref rid="RSPA20210916C45" ref-type="bibr">45</xref>,<xref rid="RSPA20210916C46" ref-type="bibr">46</xref>] provide sufficient and necessary conditions, called the <italic toggle="yes">irrepresentable conditions</italic>, for consistent variable selection using LASSO, essentially excluding strong correlations of the predictors in the design matrix. These conditions are, however, difficult to check in practice, as they require knowledge of the true components of the model. One way to relax these conditions is via randomization. The randomized LASSO [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>] therefore considers the objective
<disp-formula id="RSPA20210916M2x7"><label>2.7</label><mml:math id="DM7" display="block"><mml:msup><mml:mrow><mml:mover><mml:mi>ξ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>λ</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
where each <inline-formula><mml:math id="IM43"><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is an <italic toggle="yes">i.i.d.</italic> random variable uniformly distributed over <inline-formula><mml:math id="IM44"><mml:mo stretchy="false">[</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> with <inline-formula><mml:math id="IM45"><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>. For <inline-formula><mml:math id="IM46"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, randomized LASSO reduces to standard LASSO. Randomized LASSO has been shown to successfully overcome the limitations of LASSO in handling correlated components in the dictionary [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>], while simultaneously preserving the overall convexity of the objective function. As part of our PDE-STRIDE framework, we evaluate the performance of randomized LASSO in the context of PDE learning using cyclical coordinate descent [<xref rid="RSPA20210916C41" ref-type="bibr">41</xref>].</p>
      <p>The sparsity-promoting property of the (weighted) <inline-formula><mml:math id="IM47"><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>-norm comes at the expense of considerable bias in the estimation of the non-zero coefficients [<xref rid="RSPA20210916C46" ref-type="bibr">46</xref>], thus leading to reduced variable selection performance in practice. This drawback can be alleviated by using non-convex penalty functions [<xref rid="RSPA20210916C47" ref-type="bibr">47</xref>,<xref rid="RSPA20210916C48" ref-type="bibr">48</xref>], allowing near-optimal variable selection performance at the cost of needing to solve a non-convex optimization problem. For instance, using the <inline-formula><mml:math id="IM48"><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>-‘norm’ (which counts the number of non-zero elements of a vector) as regularizer <inline-formula><mml:math id="IM49"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> leads to the NP-hard problem
<disp-formula id="RSPA20210916M2x8"><label>2.8</label><mml:math id="DM8" display="block"><mml:msup><mml:mrow><mml:mover><mml:mi>ξ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">(</mml:mo></mml:mrow><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="2.047em" minsize="2.047em">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
This formulation has found widespread applications in compressed sensing and signal processing. Algorithms that deliver approximate solutions to equation (<xref rid="RSPA20210916M2x8" ref-type="disp-formula">2.8</xref>) include greedy optimization strategies like orthogonal matching pursuit [<xref rid="RSPA20210916C49" ref-type="bibr">49</xref>,<xref rid="RSPA20210916C50" ref-type="bibr">50</xref>], compressed sampling matching pursuit (CoSaMP) [<xref rid="RSPA20210916C51" ref-type="bibr">51</xref>] and subspace pursuit [<xref rid="RSPA20210916C52" ref-type="bibr">52</xref>]. We here consider the IHT algorithm [<xref rid="RSPA20210916C36" ref-type="bibr">36</xref>,<xref rid="RSPA20210916C53" ref-type="bibr">53</xref>], which belongs to the class of ISTA algorithms. Given the design matrix <inline-formula><mml:math id="IM50"><mml:mi>Θ</mml:mi></mml:math></inline-formula> and the measurement vector <inline-formula><mml:math id="IM51"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, IHT computes sparse solutions <inline-formula><mml:math id="IM52"><mml:mrow><mml:mover><mml:mi>ξ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> by applying a nonlinear shrinkage (thresholding) operator to gradient descent steps in an iterative manner. One step in the iterative scheme reads
<disp-formula id="RSPA20210916M2x9"><label>2.9</label><mml:math id="DM9" display="block"><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>Θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:msqrt><mml:mi>λ</mml:mi></mml:msqrt><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd><mml:mtd><mml:mtext>otherwise.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
The operator <inline-formula><mml:math id="IM53"><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is the nonlinear hard-thresholding operator. Convergence of the above iteration is guaranteed if and only if <inline-formula><mml:math id="IM54"><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula> in each iteration for some constant <inline-formula><mml:math id="IM55"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>c</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. Under the condition that <inline-formula><mml:math id="IM56"><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, the IHT algorithm is guaranteed to not increase the cost function in equation (<xref rid="RSPA20210916M2x8" ref-type="disp-formula">2.8</xref>) (Lemma 1 in [<xref rid="RSPA20210916C36" ref-type="bibr">36</xref>]). The IHT algorithm can be viewed as a thresholded version of the classic Landweber iteration [<xref rid="RSPA20210916C54" ref-type="bibr">54</xref>]. The fixed points <inline-formula><mml:math id="IM57"><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> for which <inline-formula><mml:math id="IM58"><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> for the nonlinear operator <inline-formula><mml:math id="IM59"><mml:mi>T</mml:mi></mml:math></inline-formula> in equation (<xref rid="RSPA20210916M2x9" ref-type="disp-formula">2.9</xref>) are local minima of the cost function in equation (<xref rid="RSPA20210916M2x8" ref-type="disp-formula">2.8</xref>) (Lemma 3 in [<xref rid="RSPA20210916C36" ref-type="bibr">36</xref>]). Under the same condition on the design matrix, i.e. <inline-formula><mml:math id="IM60"><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>Θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, the optimal solution of the cost function in equation (<xref rid="RSPA20210916M2x8" ref-type="disp-formula">2.8</xref>) thus belongs to the set of fixed points of the IHT algorithm (theorem 2 in [<xref rid="RSPA20210916C36" ref-type="bibr">36</xref>] and theorem 12 in [<xref rid="RSPA20210916C55" ref-type="bibr">55</xref>]). Although the IHT algorithm comes with theoretical convergence guarantees, the resulting fixed points are not necessarily sparse [<xref rid="RSPA20210916C36" ref-type="bibr">36</xref>].</p>
      <p>Here, we propose a modification of the IHT algorithm that will prove to be particularly well suited for solving PDE learning problems. Following a proposal in [<xref rid="RSPA20210916C37" ref-type="bibr">37</xref>] for the HTP algorithm, we equip the IHT algorithm with an additional debiasing step. This involves solving at each iteration a least-squares problem restricted to the support <inline-formula><mml:math id="IM61"><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>k</mml:mi><mml:mo>:</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>≠</mml:mo><mml:mn>0</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula> obtained from the <inline-formula><mml:math id="IM62"><mml:mi>n</mml:mi></mml:math></inline-formula>-th IHT iteration. We refer to this form of IHT as <italic toggle="yes">iterative hard thresholding with debiasing</italic> (IHT-d). In this two-step algorithm, the standard IHT step serves to extract the explanatory variables, while the debiasing step approximately debiases (or re-fits) the coefficients restricted to the currently active support [<xref rid="RSPA20210916C56" ref-type="bibr">56</xref>]. Rather than solving the least-squares problem to optimality, we use gradient descent steps until a loose upper bound on the least-squares re-fit is satisfied, <inline-formula><mml:math id="IM63"><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>≤</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula>. This prevents over-fitting by attributing low confidence to large supports, which reduces computational overhead and renders the algorithm practical. The complete IHT-d procedure is detailed in Algorithm 1 in the electronic supplementary material. In PDE-STRIDE, we compare IHT-d with a heuristic iterative algorithm, Sequential Thresholding of Ridge regression (STRidge), that also uses <inline-formula><mml:math id="IM64"><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> penalization and is available in PDE-FIND [<xref rid="RSPA20210916C24" ref-type="bibr">24</xref>].</p>
    </sec>
    <sec id="s2c">
      <label>(c)<x xml:space="preserve"> </x></label>
      <title>Stability selection</title>
      <p>The practical performance of sparse optimization techniques in PDE learning hinges on proper selection of the regularization parameter <inline-formula><mml:math id="IM65"><mml:mi>λ</mml:mi></mml:math></inline-formula> that balances model fit and model complexity. In model discovery tasks on real experimental data, a wrong choice of the regularization parameter could result in incorrect PDE model selection even if true model discovery would have been, in principle, achievable. In statistics, a large number of tuning parameter selection criteria are available, ranging from cross-validation approaches [<xref rid="RSPA20210916C57" ref-type="bibr">57</xref>] to information criteria [<xref rid="RSPA20210916C58" ref-type="bibr">58</xref>], or formulations that allow joint learning of model coefficients and tuning parameters [<xref rid="RSPA20210916C59" ref-type="bibr">59</xref>,<xref rid="RSPA20210916C60" ref-type="bibr">60</xref>]. Here, we advocate stability-based model selection [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>] for robust PDE learning. The statistical principle of stability [<xref rid="RSPA20210916C61" ref-type="bibr">61</xref>] has been put forward as one of the pillars of modern data science and statistics. It provides an intuitive approach to model selection [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>,<xref rid="RSPA20210916C34" ref-type="bibr">34</xref>,<xref rid="RSPA20210916C62" ref-type="bibr">62</xref>]. Stability selection has found widespread application from the analysis of gene regulatory networks [<xref rid="RSPA20210916C63" ref-type="bibr">63</xref>] to graphical models [<xref rid="RSPA20210916C64" ref-type="bibr">64</xref>] and ecological studies [<xref rid="RSPA20210916C65" ref-type="bibr">65</xref>] .</p>
      <p>In the context of sparse regression, stability selection [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>] proceeds as follows (see also <xref rid="RSPA20210916F1" ref-type="fig">figure 1</xref> for an illustration): given a design matrix <inline-formula><mml:math id="IM66"><mml:mi>Θ</mml:mi></mml:math></inline-formula> and measurement vector <inline-formula><mml:math id="IM67"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, generate random subsample indices <inline-formula><mml:math id="IM68"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>⊂</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:math></inline-formula> of equal size <inline-formula><mml:math id="IM69"><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:math></inline-formula> and produce reduced sub-designs <inline-formula><mml:math id="IM70"><mml:mi>Θ</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="IM71"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:math></inline-formula> by choosing rows according to the index set <inline-formula><mml:math id="IM72"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. For each of the resulting <inline-formula><mml:math id="IM73"><mml:mi>B</mml:mi></mml:math></inline-formula> subproblems, apply a sparse regression technique and systematically record the recovered supports <inline-formula><mml:math id="IM74"><mml:msup><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:math></inline-formula>, as a function of <inline-formula><mml:math id="IM75"><mml:mi>λ</mml:mi></mml:math></inline-formula> over a <italic toggle="yes">regularization path</italic>
<inline-formula><mml:math id="IM76"><mml:mi>Λ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>. The values of <inline-formula><mml:math id="IM77"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="IM78"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> are data-dependent and are easily computable for generalized linear models with convex penalties [<xref rid="RSPA20210916C41" ref-type="bibr">41</xref>]. In our case, the parameter <inline-formula><mml:math id="IM79"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> for the non-convex problem in equation (<xref rid="RSPA20210916M2x8" ref-type="disp-formula">2.8</xref>) can be determined from optimality conditions (Theorem 12 in [<xref rid="RSPA20210916C55" ref-type="bibr">55</xref>] and Theorem 1 in [<xref rid="RSPA20210916C36" ref-type="bibr">36</xref>]). The lower bound <inline-formula><mml:math id="IM80"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is set to <inline-formula><mml:math id="IM81"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ϵ</mml:mi><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> with default <inline-formula><mml:math id="IM82"><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula>. The <inline-formula><mml:math id="IM83"><mml:mi>λ</mml:mi></mml:math></inline-formula>-dependent <italic toggle="yes">importance measure</italic> for each coefficient <inline-formula><mml:math id="IM84"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is then computed as
<disp-formula id="RSPA20210916M2x10"><label>2.10</label><mml:math id="DM10" display="block"><mml:msubsup><mml:mrow><mml:mover><mml:mi>Π</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mn mathvariant="double-struck">1</mml:mn></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
where <inline-formula><mml:math id="IM85"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> are the independent random sub-samples. The importance measure <inline-formula><mml:math id="IM86"><mml:msubsup><mml:mrow><mml:mover><mml:mi>Π</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> of each model coefficient can be plotted across the regularization path, resulting in a component stability profile (see <xref rid="RSPA20210916F1" ref-type="fig">figure 1</xref><italic toggle="yes">f</italic> for an illustration). This visualization provides an intuitive overview of the importance of the different model components. Different from the original stability selection work [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>], we define the stable components of the model as
<disp-formula id="RSPA20210916M2x11"><label>2.11</label><mml:math id="DM11" display="block"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>k</mml:mi><mml:mo>:</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>Π</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:mo>≥</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
Here, <inline-formula><mml:math id="IM87"><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub></mml:math></inline-formula> denotes the stability threshold parameter, which can be set to <inline-formula><mml:math id="IM88"><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.7</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>]. We always use the default setting <inline-formula><mml:math id="IM89"><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:math></inline-formula>. During exploratory data analysis, the threshold <inline-formula><mml:math id="IM90"><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub></mml:math></inline-formula> can also be set through visual inspection of the stability plots, allowing principled exploration of alternative PDE models. The importance measures <inline-formula><mml:math id="IM91"><mml:msubsup><mml:mrow><mml:mover><mml:mi>Π</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> also provide an interpretable criterion for a model component’s stability against random sub-sampling of the data and changes to the dictionary design, guiding the user to build the right model with high probability. As we show in the numerical experiments, stability selection thus ensures robustness against varying dictionary size, different types of data sampling, noise in the data and variability of the sub-optimal solutions when non-convex penalties are used. All of these properties are critical for consistent and reproducible model learning in real-world applications. Under certain conditions, stability selection also provides an upper bound on the expected number of false positives [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>]. Such guarantees are not generally assured by any sparsity-promoting regression method in isolation [<xref rid="RSPA20210916C34" ref-type="bibr">34</xref>]. For instance, stability selection combined with randomized LASSO (equation (<xref rid="RSPA20210916M2x7" ref-type="disp-formula">2.7</xref>) with <inline-formula><mml:math id="IM92"><mml:mi>α</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula>) is consistent for variable selection even when the irrepresentable condition is violated [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>].
<fig position="float" id="RSPA20210916F1"><label>Figure 1<x xml:space="preserve">. </x></label><caption><p>Enabling data-driven mathematical model discovery through stability selection. We outline the necessary steps in our method for learning PDE models from spatio-temporal data. (<italic toggle="yes">a</italic>) Extract spatio-temporal profiles from microscopy videos of the chosen state variables. Data courtesy of Grill Laboratory, MPI-CBG/TU Dresden [<xref rid="RSPA20210916C66" ref-type="bibr">66</xref>]. (<italic toggle="yes">b</italic>) Compile the design matrix <inline-formula><mml:math id="IM93"><mml:mi>Θ</mml:mi></mml:math></inline-formula> and the measurement vector <inline-formula><mml:math id="IM94"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> from the data. (<italic toggle="yes">c</italic>) Construct multiple linear systems of reduced size through random sub-sampling of the rows of the design matrix <inline-formula><mml:math id="IM95"><mml:mi>Θ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="IM96"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>. (<italic toggle="yes">d</italic>) Solve and record the sparse/penalized regression solutions independently for each sub-sample along the <inline-formula><mml:math id="IM97"><mml:mi>λ</mml:mi></mml:math></inline-formula>-paths. (<italic toggle="yes">e</italic>) Compute the importance measure <inline-formula><mml:math id="IM98"><mml:mi>Π</mml:mi></mml:math></inline-formula> for each component. The histogram shows the importance measure <inline-formula><mml:math id="IM99"><mml:mi>Π</mml:mi></mml:math></inline-formula> for all components at a particular value of <inline-formula><mml:math id="IM100"><mml:mi>λ</mml:mi></mml:math></inline-formula>. (<italic toggle="yes">f</italic>) Construct the stability plot by aggregating the importance measures along the <inline-formula><mml:math id="IM101"><mml:mi>λ</mml:mi></mml:math></inline-formula>-path, leading to separation of the noise variables (dashed black) from the stable components (coloured). Identify the most stable components by thresholding <inline-formula><mml:math id="IM102"><mml:mi>Π</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.8</mml:mn></mml:math></inline-formula>. (<italic toggle="yes">g</italic>) Build the PDE model from the identified components. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f01" position="float"/></fig></p>
    </sec>
  </sec>
  <sec id="s3">
    <label>3<x xml:space="preserve">. </x></label>
    <title>Numerical experiments on simulated data</title>
    <p>We present numerical experiments in order to benchmark the performance and robustness of PDE-STRIDE combined with different sparsity-promoting regression methods to infer PDEs from spatio-temporal data. To provide comparisons and benchmarks, we first use simulated data obtained by numerically solving known ground-truth PDEs, before applying our method to a real-world dataset from biology. The benchmark experiments on simulation data are presented in four subsections that demonstrate different aspects of the inference framework: §a demonstrates the use of different sparsity-promoting regression methods in our framework in a simple one-dimensional Burgers problem. Section b then compares their performance in order to choose the best regression method, IHT-d. In §c, stability selection is combined with IHT-d to recover two-dimensional vorticity-transport and three-dimensional reaction–diffusion PDEs from limited, noisy simulation data. Section d reports achievability results to quantify the robustness of stability selection to variations in dictionary size, sample size and noise levels. In all cases, we follow the PDE-STRIDE procedure as explained in the box below.</p>
    <boxed-text position="float">
      <p><italic toggle="yes"><bold>STability-based Robust IDEntification of PDEs (PDE-STRIDE)</bold><bold>.</bold> </italic>Given the noise-corrupted data <inline-formula><mml:math id="IM103"><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and a choice of regression method, e.g. (randomized) LASSO, IHT, HTP, IHT-d, STRidge:</p>
      <p>
        <list list-type="simple">
          <list-item>
            <label>(i)<x xml:space="preserve"> </x></label>
            <p>Apply any required denoising method on the noisy data and compute the spatial derivatives and nonlinearities to construct the design matrix <inline-formula><mml:math id="IM104"><mml:mi>Θ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and the time-derivatives vector <inline-formula><mml:math id="IM105"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> for suitable sample size and dictionary size, <inline-formula><mml:math id="IM106"><mml:mi>N</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="IM107"><mml:mi>p</mml:mi></mml:math></inline-formula>, respectively.</p>
          </list-item>
          <list-item>
            <label>(ii)<x xml:space="preserve"> </x></label>
            <p>Build the sub-samples <inline-formula><mml:math id="IM108"><mml:mi>Θ</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="IM109"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>, for <inline-formula><mml:math id="IM110"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>, by uniformly randomly sub-sampling of rows from the design matrix <inline-formula><mml:math id="IM111"><mml:mi>Θ</mml:mi></mml:math></inline-formula> and the corresponding rows from <inline-formula><mml:math id="IM112"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>. For every sub-sample <inline-formula><mml:math id="IM113"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, standardize the sub-design matrix <inline-formula><mml:math id="IM114"><mml:mi>Θ</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> such that <inline-formula><mml:math id="IM115"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:munderover><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="IM116"><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:munderover><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, for <inline-formula><mml:math id="IM117"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:math></inline-formula>. Here, <inline-formula><mml:math id="IM118"><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the element in row <inline-formula><mml:math id="IM119"><mml:mi>j</mml:mi></mml:math></inline-formula> and column <inline-formula><mml:math id="IM120"><mml:mi>k</mml:mi></mml:math></inline-formula> of the matrix <inline-formula><mml:math id="IM121"><mml:mi>Θ</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>. The corresponding measurement vector <inline-formula><mml:math id="IM122"><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> is centred to zero mean.</p>
          </list-item>
          <list-item>
            <label>(iii)<x xml:space="preserve"> </x></label>
            <p>Apply the sparsity-promoting regression method independently to each sub-sample <inline-formula><mml:math id="IM123"><mml:mi>Θ</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> to construct the <inline-formula><mml:math id="IM124"><mml:mi>λ</mml:mi></mml:math></inline-formula>-paths for <inline-formula><mml:math id="IM125"><mml:mi>M</mml:mi></mml:math></inline-formula> values of <inline-formula><mml:math id="IM126"><mml:mi>λ</mml:mi></mml:math></inline-formula> as discussed in §2c.</p>
          </list-item>
          <list-item>
            <label>(iv)<x xml:space="preserve"> </x></label>
            <p>Compute the importance measures <inline-formula><mml:math id="IM127"><mml:msubsup><mml:mrow><mml:mover><mml:mi>Π</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> of all dictionary components <inline-formula><mml:math id="IM128"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> along the discretized <inline-formula><mml:math id="IM129"><mml:mi>λ</mml:mi></mml:math></inline-formula>-paths, as discussed in §c. Select the stable support set <inline-formula><mml:math id="IM130"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext></mml:msub></mml:math></inline-formula> by applying the threshold <inline-formula><mml:math id="IM131"><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:math></inline-formula> to all <inline-formula><mml:math id="IM132"><mml:msub><mml:mrow><mml:mover><mml:mi>Π</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>. Solve a linear least-squares problem restricted to the support <inline-formula><mml:math id="IM133"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext></mml:msub></mml:math></inline-formula> to identify the coefficients of the learned model.</p>
          </list-item>
        </list>
      </p>
    </boxed-text>
    <sec id="s3a1">
      <label>(i)<x xml:space="preserve"> </x></label>
      <title>Adding noise to the simulation data</title>
      <p>Let <inline-formula><mml:math id="IM134"><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> be the vector of clean simulation data sampled in both space and time. This vector is corrupted with additive Gaussian noise to
<disp-formula id="RSPA20210916UM1"><mml:math id="DM12" display="block"><mml:mrow><mml:mover><mml:mi>u</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mo>+</mml:mo><mml:mi>ε</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>
such that <inline-formula><mml:math id="IM135"><mml:mi>ε</mml:mi><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mtext>Var</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is the additive Gaussian noise with an empirical variance of the entries in the vector <inline-formula><mml:math id="IM136"><mml:mi>u</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="IM137"><mml:mi>σ</mml:mi></mml:math></inline-formula> is the level of Gaussian noise added.</p>
    </sec>
    <sec id="s3a2">
      <label>(ii)<x xml:space="preserve"> </x></label>
      <title>Fixing the parameters for stability selection</title>
      <p>We propose that PDE-STRIDE combined with IHT-d provides a PDE learning method that only rarely requires parameter tuning. To demonstrate this, all stability selection parameters described in §c are fixed to their standard values throughout our numerical experiments. The choice of these standard parameter values is well-discussed in the literature [<xref rid="RSPA20210916C33" ref-type="bibr">33</xref>,<xref rid="RSPA20210916C41" ref-type="bibr">41</xref>,<xref rid="RSPA20210916C64" ref-type="bibr">64</xref>]. We thus fix: the repetition number <inline-formula><mml:math id="IM138"><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:math></inline-formula>, regularization path parameter <inline-formula><mml:math id="IM139"><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM140"><mml:mi>λ</mml:mi></mml:math></inline-formula>-path size <inline-formula><mml:math id="IM141"><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula> and the importance threshold <inline-formula><mml:math id="IM142"><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:math></inline-formula>. Using these standard values, the method works robustly across all tests presented here with no case-specific parameter tuning required. In both stability and regularization plots, we show normalized values of the regularization parameter <inline-formula><mml:math id="IM143"><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> on a decimal logarithmic scale. Although the stable component set <inline-formula><mml:math id="IM144"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is always evaluated at <inline-formula><mml:math id="IM145"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, as in equation (<xref rid="RSPA20210916M2x11" ref-type="disp-formula">2.11</xref>), the stability plots sometimes have their axes scaled to large or smaller ranges of <inline-formula><mml:math id="IM146"><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> for better visualization.</p>
    </sec>
    <sec id="s3b">
      <label>(a)<x xml:space="preserve"> </x></label>
      <title>One-dimensional Burgers equation with different sparsity promoters</title>
      <p>We show that stability selection can be combined with any sparsity-promoting penalized regression framework to learn PDE components from noisy and limited spatio-temporal data. We use simulated data of the one-dimensional Burgers equation
<disp-formula id="RSPA20210916M3x1"><label>3.1</label><mml:math id="DM13" display="block"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>
with identical boundary and initial conditions as used in [<xref rid="RSPA20210916C24" ref-type="bibr">24</xref>] to provide fair comparison between methods: periodic boundaries in space and the following Gaussian initial condition:
<disp-formula id="RSPA20210916UM2"><mml:math id="DM14" display="block"><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>−</mml:mo></mml:mrow><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>The simulation domain <inline-formula><mml:math id="IM147"><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> is divided uniformly into 256 Cartesian grid points in space and 1000 time points. The numerical solution is visualized in space-time in <xref rid="RSPA20210916F4" ref-type="fig">figure 4</xref>. The numerical solution was obtained using a parabolic method based on finite differences and time stepping using explicit Euler with step size <inline-formula><mml:math id="IM148"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:math></inline-formula> and the diffusion coefficient <inline-formula><mml:math id="IM149"><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula>.</p>
      <p>We test the combinations of stability selection with the three sparsity-promoting regression techniques described in §2b: randomized LASSO, STRidge and IHT-d. The top row of <xref rid="RSPA20210916F2" ref-type="fig">figure 2</xref> shows the regularization paths and the bottom row the corresponding stability plots for each component in the dictionary. The coloured solid lines correspond to the advection and diffusion terms of equation (<xref rid="RSPA20210916M3x1" ref-type="disp-formula">3.1</xref>) as given in the inset legend.
<fig position="float" id="RSPA20210916F2"><label>Figure 2<x xml:space="preserve">. </x></label><caption><p>Model selection with PDE-STRIDE for the one-dimensional Burgers equation. The top row shows regularization paths (see §c) for three sparsity-promoting regression techniques: randomized LASSO, STRidge and IHT-d all for the same design (<inline-formula><mml:math id="IM150"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM151"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>19</mml:mn></mml:math></inline-formula>). The bottom row shows the corresponding stability plots. The inset legend at the bottom shows the colour and line symbol correspondence with the dictionary. The ridge parameter <inline-formula><mml:math id="IM152"><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for STRidge is fixed to <inline-formula><mml:math id="IM153"><mml:msub><mml:mi>λ</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> [<xref rid="RSPA20210916C24" ref-type="bibr">24</xref>]. The value of <inline-formula><mml:math id="IM154"><mml:mi>α</mml:mi></mml:math></inline-formula> for the randomized LASSO is set to <inline-formula><mml:math id="IM155"><mml:mn>0.2</mml:mn></mml:math></inline-formula>. In all three cases, the standard threshold <inline-formula><mml:math id="IM156"><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:math></inline-formula> (red solid horizontal line) correctly identifies the true components. The <inline-formula><mml:math id="IM157"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> is set to <inline-formula><mml:math id="IM158"><mml:mn>0.001</mml:mn></mml:math></inline-formula> for randomized LASSO in order to demonstrate stability selection. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f02" position="float"/></fig></p>
      <p>Thresholding the importance measure at <inline-formula><mml:math id="IM159"><mml:mi>Π</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:math></inline-formula>, all sparsity-promoting regression methods are able to identify the correct components of the model and separate them from the noise variables (dashed black lines).</p>
    </sec>
    <sec id="s3c">
      <label>(b)<x xml:space="preserve"> </x></label>
      <title>Comparison between sparsity-promoting techniques</title>
      <p>Although stability selection can be used in conjunction with any <inline-formula><mml:math id="IM160"><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="IM161"><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> sparsity-promoting regression method, the question arises whether a particular choice of regression algorithm is particularly well suited for PDE learning. We therefore perform a systematic comparison between randomized LASSO, STRidge, orthogonal matching pursuit (OMP) [<xref rid="RSPA20210916C67" ref-type="bibr">67</xref>,<xref rid="RSPA20210916C68" ref-type="bibr">68</xref>] as implemented in <italic toggle="yes">scikit-learn</italic>, and IHT-d for recovering the one-dimensional Burgers equation under perturbations to the sample size <inline-formula><mml:math id="IM162"><mml:mi>N</mml:mi></mml:math></inline-formula>, the dictionary size <inline-formula><mml:math id="IM163"><mml:mi>p</mml:mi></mml:math></inline-formula> and the noise level <inline-formula><mml:math id="IM164"><mml:mi>σ</mml:mi></mml:math></inline-formula>. An experiment for a particular triple <inline-formula><mml:math id="IM165"><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is considered a success if there exists a <inline-formula><mml:math id="IM166"><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mi>Λ</mml:mi></mml:math></inline-formula> (see §c) for which the true PDE components are recovered. In <xref rid="RSPA20210916F3" ref-type="fig">figure 3</xref>, the success frequencies over 30 independent repetitions with uniformly random data sub-samples are shown for the four regression methods.
<fig position="float" id="RSPA20210916F3"><label>Figure 3<x xml:space="preserve">. </x></label><caption><p>Comparison between different sparse regression methods for the one-dimensional Burgers equation. Each coloured square corresponds to a design <inline-formula><mml:math id="IM167"><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> with certain sample size <inline-formula><mml:math id="IM168"><mml:mi>N</mml:mi></mml:math></inline-formula>, dictionary size <inline-formula><mml:math id="IM169"><mml:mi>p</mml:mi></mml:math></inline-formula> and noise level <inline-formula><mml:math id="IM170"><mml:mi>σ</mml:mi></mml:math></inline-formula>. Colour indicates the success frequency over 30 independent repetitions with uniformly random data samples, as given in the colour bar to the right. ‘Success’ is defined as the existence of <italic toggle="yes">a</italic>
<inline-formula><mml:math id="IM171"><mml:mi>λ</mml:mi></mml:math></inline-formula> value for which the correct PDE is recovered from the data. The columns compare four popular sparsity-promoting regression methods: randomized LASSO, STRidge, IHT-d and OMP (left to right), as labelled at the top. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f03" position="float"/></fig>
<fig position="float" id="RSPA20210916F4"><label>Figure 4<x xml:space="preserve">. </x></label><caption><p>Model selection with PDE-STRIDE+IHT-d for the one-dimensional Burgers equation. The top left image shows the numerical solution of the one-dimensional Burgers equations on <inline-formula><mml:math id="IM172"><mml:mn>256</mml:mn><mml:mo>×</mml:mo><mml:mn>1000</mml:mn></mml:math></inline-formula> space and time points, respectively. The stability plots for the design <inline-formula><mml:math id="IM173"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM174"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>19</mml:mn></mml:math></inline-formula> show clear separation of the true PDE components (in solid colour with line symbols as identified in the inset legend) from the noise components (dashed black). The inference power of the method is tested for additive Gaussian noise levels <inline-formula><mml:math id="IM175"><mml:mi>σ</mml:mi></mml:math></inline-formula> up to 4%. In all cases, perfect recovery is possible without parameter tuning. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f04" position="float"/></fig></p>
      <p>A first observation from <xref rid="RSPA20210916F3" ref-type="fig">figure 3</xref> is that <inline-formula><mml:math id="IM176"><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> solutions (here with STRidge, IHT-d and OMP) outperform relaxed <inline-formula><mml:math id="IM177"><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> solutions (here with randomized LASSO). We also observe that IHT-d performs better than the other three methods for large dictionary sizes <inline-formula><mml:math id="IM178"><mml:mi>p</mml:mi></mml:math></inline-formula>, high noise <inline-formula><mml:math id="IM179"><mml:mi>σ</mml:mi></mml:math></inline-formula> and small sample sizes <inline-formula><mml:math id="IM180"><mml:mi>N</mml:mi></mml:math></inline-formula>. Large dictionaries with higher-order derivatives computed from discrete data cause grouping (correlations between variables), for which randomized LASSO tends to select one variable from each group, ignoring the others [<xref rid="RSPA20210916C69" ref-type="bibr">69</xref>]. Thus, randomized LASSO fails to identify the true support consistently. STRidge shows good recovery for large dictionary sizes <inline-formula><mml:math id="IM181"><mml:mi>p</mml:mi></mml:math></inline-formula> with clean data, but it breaks down in the presence of noise in the data. OMP also uses <inline-formula><mml:math id="IM182"><mml:msub><mml:mi>ℓ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> regularization and therefore outperforms randomized LASSO, but fails to compete with STRidge and IHT-d. Of all four methods, IHT-d shows the best robustness to both noise and changes in the design. We note a decrease in inference power with increasing sample size <inline-formula><mml:math id="IM183"><mml:mi>N</mml:mi></mml:math></inline-formula>, especially for large <inline-formula><mml:math id="IM184"><mml:mi>p</mml:mi></mml:math></inline-formula> and high noise levels. This again can be attributed to correlations and groupings in the dictionary, which become more significant with increasing sample size <inline-formula><mml:math id="IM185"><mml:mi>N</mml:mi></mml:math></inline-formula>.</p>
      <p>Based on these results, we use IHT-d in the following sections as the sparsity-promoting regression method in PDE-STRIDE.</p>
    </sec>
    <sec id="s3d">
      <label>(c)<x xml:space="preserve"> </x></label>
      <title>Stability-based model inference</title>
      <p>We present benchmark results of PDE-STRIDE for PDE learning with IHT-d as the sparse regression method. This combination of methods is used to recover PDEs from limited noisy data obtained by numerical solution of the one-dimensional Burgers, two-dimensional vorticity-transport and three-dimensional Gray–Scott equations. Once the stable support <inline-formula><mml:math id="IM186"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> of the PDE model has been learned by PDE-STRIDE with IHT-d, the actual coefficient values of the non-zero components are determined by solving a linear least-squares problem restricted to the recovered support <inline-formula><mml:math id="IM187"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. More sophisticated methods could instead be used for PDE parameter estimation with known model structure [<xref rid="RSPA20210916C21" ref-type="bibr">21</xref>,<xref rid="RSPA20210916C22" ref-type="bibr">22</xref>]. As the sample size <inline-formula><mml:math id="IM188"><mml:mi>N</mml:mi></mml:math></inline-formula> exceeds the cardinality of the recovered support (<inline-formula><mml:math id="IM189"><mml:mi>N</mml:mi><mml:mo>≫</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>stable</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula>), however, we find that simple least-squares fits provide good estimates for the PDE coefficients.</p>
      <sec id="s3d1">
        <label>(i)<x xml:space="preserve"> </x></label>
        <title>One-dimensional Burgers equation</title>
        <p>We again consider the one-dimensional Burgers equation from equation (<xref rid="RSPA20210916M3x1" ref-type="disp-formula">3.1</xref>), using the same simulated data as in §a, to quantify the performance and robustness against noise of the PDE-STRIDE+IHT-d method. The results are shown in <xref rid="RSPA20210916F4" ref-type="fig">figure 4</xref> for a design with <inline-formula><mml:math id="IM190"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="IM191"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>19</mml:mn></mml:math></inline-formula>. Even on this small dataset, with a sample size comparable with dictionary size, our method recovers the correct model <inline-formula><mml:math id="IM192"><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula> with up to 4% noise on the data, although the least-squares fits of the coefficient values gradually deviate from their exact values (<xref rid="RSPA20210916TB1" ref-type="table">table 1</xref>).
<table-wrap position="float" id="RSPA20210916TB1"><label>Table 1<x xml:space="preserve">. </x></label><caption><p>Coefficient values of the recovered one-dimensional Burgers equation for different noise levels. The stable components of the PDE inferred from the plots in <xref rid="RSPA20210916F4" ref-type="fig">figure 4</xref> are <inline-formula><mml:math id="IM193"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula>. The ground-truth coefficient values are given in parentheses in the column headings.</p></caption><table frame="hsides" rules="groups" width="2pc"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM194"><mml:mi>u</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="IM195"><mml:mo>−</mml:mo></mml:math></inline-formula>1.0)</th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM196"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (0.1)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">clean</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM197"><mml:mo>−</mml:mo><mml:mn>1.0008</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.1000</td></tr><tr><td rowspan="1" colspan="1">1%</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM198"><mml:mo>−</mml:mo><mml:mn>0.9971</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.1016</td></tr><tr><td rowspan="1" colspan="1">2%</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM199"><mml:mo>−</mml:mo><mml:mn>0.9932</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.0997</td></tr><tr><td rowspan="1" colspan="1">3%</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM200"><mml:mo>−</mml:mo><mml:mn>0.9842</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.0976</td></tr><tr><td rowspan="1" colspan="1">4%</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM201"><mml:mo>−</mml:mo><mml:mn>0.9728</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.0984</td></tr><tr><td rowspan="1" colspan="1">5%</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM202"><mml:mo>−</mml:mo><mml:mn>0.9619</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.0967</td></tr></tbody></table></table-wrap></p>
        <p>For comparison, the corresponding stability plots for PDE-STRIDE + STRidge are shown in the electronic supplementary material, figure S3. STRidge creates many false positives even at mild noise levels (less than <inline-formula><mml:math id="IM203"><mml:mn>2</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:math></inline-formula>).</p>
      </sec>
      <sec id="s3d2">
        <label>(ii)<x xml:space="preserve"> </x></label>
        <title>Two-dimensional vorticity transport equation</title>
        <p>Next, we consider a two-dimensional domain and the vorticity transport equation given in equation (<xref rid="RSPA20210916M3x2" ref-type="disp-formula">3.2</xref>). The vorticity transport equation can be obtained by taking curl of the Navier–Stokes equations and imposing the incompressibility constraint <inline-formula><mml:math id="IM204"><mml:mi mathvariant="normal">∇</mml:mi><mml:mo>⋅</mml:mo><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> for the flow velocity field <inline-formula><mml:math id="IM205"><mml:mi>u</mml:mi></mml:math></inline-formula>. This results in a governing equation for the vorticity <inline-formula><mml:math id="IM206"><mml:mi>ω</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∇</mml:mi><mml:mo>×</mml:mo><mml:mi>u</mml:mi></mml:math></inline-formula>, which is a scalar in two dimensions
<disp-formula id="RSPA20210916M3x2"><label>3.2</label><mml:math id="DM15" display="block"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
This equation has numerous applications in oceanography and climate modelling [<xref rid="RSPA20210916C70" ref-type="bibr">70</xref>]. The velocity vector <inline-formula><mml:math id="IM207"><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> has two components, and <inline-formula><mml:math id="IM208"><mml:mi>μ</mml:mi></mml:math></inline-formula> is the fluid’s viscosity. For the numerical solution of the transport equation (<xref rid="RSPA20210916M3x2" ref-type="disp-formula">3.2</xref>) in the unit square, we impose a no-slip boundary condition at the left <inline-formula><mml:math id="IM209"><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, right <inline-formula><mml:math id="IM210"><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and bottom sides <inline-formula><mml:math id="IM211"><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and a shear flow boundary condition with boundary velocity components <inline-formula><mml:math id="IM212"><mml:msub><mml:mi>u</mml:mi><mml:mtext>boundary</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>2.0</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM213"><mml:msub><mml:mi>v</mml:mi><mml:mtext>boundary</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> on the top side <inline-formula><mml:math id="IM214"><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. This is the classic ‘lid-driven cavity’ problem from fluid mechanics. The simulation code was written using OpenFPM [<xref rid="RSPA20210916C71" ref-type="bibr">71</xref>] with explicit time stepping on a <inline-formula><mml:math id="IM215"><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>128</mml:mn></mml:math></inline-formula> grid in space. A Poisson equation was solved at every time step in order to correct the velocities to ensure divergence-freeness. The viscosity of the fluid simulated was set to <inline-formula><mml:math id="IM216"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.025</mml:mn></mml:math></inline-formula>. In <xref rid="RSPA20210916F5" ref-type="fig">figure 5</xref>, we show a single time snapshot of the two velocity components <inline-formula><mml:math id="IM217"><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and the vorticity <inline-formula><mml:math id="IM218"><mml:mi>ω</mml:mi></mml:math></inline-formula> in the numerical simulation, along with the locations of the 500 sample points used for PDE inference from these data.
<fig position="float" id="RSPA20210916F5"><label>Figure 5<x xml:space="preserve">. </x></label><caption><p>Numerical solution of the two-dimensional vorticity transport equation. We show the two components of the flow velocity <inline-formula><mml:math id="IM219"><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and the vorticity <inline-formula><mml:math id="IM220"><mml:mi>ω</mml:mi></mml:math></inline-formula> in the two-dimensional domain <inline-formula><mml:math id="IM221"><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>. The black dots in the right panel show the <inline-formula><mml:math id="IM222"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:math></inline-formula> sample points used for PDE inference. They are uniformly randomly distributed in the rectangular box <inline-formula><mml:math id="IM223"><mml:mo stretchy="false">[</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>1.0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f05" position="float"/></fig></p>
        <p><xref rid="RSPA20210916F6" ref-type="fig">Figure 6</xref> shows the results of applying PDE-STRIDE + IHT-d to recover the two-dimensional vorticity transport equation from these data samples. The results demonstrate consistent recovery of the true support of the PDE for different noise levels <inline-formula><mml:math id="IM224"><mml:mi>σ</mml:mi></mml:math></inline-formula>. The stable components <inline-formula><mml:math id="IM225"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula> correspond to the true terms of equation (<xref rid="RSPA20210916M3x2" ref-type="disp-formula">3.2</xref>). In <xref rid="RSPA20210916TB2" ref-type="table">table 2</xref>, we show the re-fitted coefficients for the recovered PDE components. Both the accuracy of the parameter fits and the separation between the true (coloured solid lines) and the noisy (black dashed lines) components deteriorate with increasing noise levels. In the electronic supplementary material, figure S4, we also report the plots when using STRidge in conjunction with stability selection for the same design and stability selection parameters. STRidge fails to recover the true support even at small noise levels.
<fig position="float" id="RSPA20210916F6"><label>Figure 6<x xml:space="preserve">. </x></label><caption><p>Model selection with PDE-STRIDE + IHT-d for the two-dimensional vorticity transport equation. The stability plots for the design <inline-formula><mml:math id="IM226"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM227"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>48</mml:mn></mml:math></inline-formula> show the separation of the true PDE components (in solid colour with line symbols as identified in the inset legend) from the noise components (dashed black). The inference power of the method is tested for additive Gaussian noise levels <inline-formula><mml:math id="IM228"><mml:mi>σ</mml:mi></mml:math></inline-formula> up to 5%. In all cases, perfect recovery is possible without parameter tuning. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f06" position="float"/></fig>
<table-wrap position="float" id="RSPA20210916TB2"><label>Table 2<x xml:space="preserve">. </x></label><caption><p>Coefficient values of the recovered two-dimensional vorticity transport equation for different noise levels. The stable components of the PDE inferred from the plots in <xref rid="RSPA20210916F6" ref-type="fig">figure 6</xref> are <inline-formula><mml:math id="IM229"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula>. The ground-truth coefficient values are given in parentheses in the column headings.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM230"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (0.025)</th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM231"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (0.025)</th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM232"><mml:mi>u</mml:mi><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="IM233"><mml:mo>−</mml:mo></mml:math></inline-formula>1.0)</th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM234"><mml:mi>v</mml:mi><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="IM235"><mml:mo>−</mml:mo></mml:math></inline-formula>1.0)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">clean</td><td rowspan="1" colspan="1">0.02504</td><td rowspan="1" colspan="1">0.02502</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM236"><mml:mo>−</mml:mo><mml:mn>0.9994</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM237"><mml:mo>−</mml:mo><mml:mn>1.0025</mml:mn></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1">1%</td><td rowspan="1" colspan="1">0.02501</td><td rowspan="1" colspan="1">0.02504</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM238"><mml:mo>−</mml:mo><mml:mn>0.9997</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM239"><mml:mo>−</mml:mo><mml:mn>1.0006</mml:mn></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1">2%</td><td rowspan="1" colspan="1">0.02492</td><td rowspan="1" colspan="1">0.0250</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM240"><mml:mo>−</mml:mo><mml:mn>1.0003</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM241"><mml:mo>−</mml:mo><mml:mn>0.9944</mml:mn></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1">3%</td><td rowspan="1" colspan="1">0.0247</td><td rowspan="1" colspan="1">0.0250</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM242"><mml:mo>−</mml:mo><mml:mn>1.004</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM243"><mml:mo>−</mml:mo><mml:mn>0.9841</mml:mn></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1">4%</td><td rowspan="1" colspan="1">0.0245</td><td rowspan="1" colspan="1">0.0251</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM244"><mml:mo>−</mml:mo><mml:mn>1.0091</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM245"><mml:mo>−</mml:mo><mml:mn>0.9748</mml:mn></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1">5%</td><td rowspan="1" colspan="1">0.0242</td><td rowspan="1" colspan="1">0.0251</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM246"><mml:mo>−</mml:mo><mml:mn>1.0083</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM247"><mml:mo>−</mml:mo><mml:mn>0.9586</mml:mn></mml:math></inline-formula></td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="s3d3">
        <label>(iii)<x xml:space="preserve"> </x></label>
        <title>Three-dimensional Gray–Scott equation</title>
        <p>Finally, we consider a problem in three dimensions, namely, the three-dimensional Gray–Scott reaction–diffusion model
<disp-formula id="RSPA20210916M3x3a"><label>3.3<italic toggle="yes">a</italic></label><mml:math id="DM16" display="block"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula>
and
<disp-formula id="RSPA20210916M3x3b"><label>3.3<italic toggle="yes">b</italic></label><mml:math id="DM17" display="block"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi mathvariant="normal">∂</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>v</mml:mi><mml:mo>.</mml:mo></mml:math></disp-formula>
Reaction and diffusion of two chemical species with scalar concentrations <inline-formula><mml:math id="IM248"><mml:mi>u</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="IM249"><mml:mi>v</mml:mi></mml:math></inline-formula> can produce a variety of patterns, reminiscent of those often observed in nature [<xref rid="RSPA20210916C72" ref-type="bibr">72</xref>]. This is, for example, used to describe skin patterning and pigmentation [<xref rid="RSPA20210916C73" ref-type="bibr">73</xref>]. This example also has coupled variables and is similar in structure to the real-world example discussed in §4.</p>
        <p>We simulate equation (3.3) using second-order central finite differences implemented in OpenFPM [<xref rid="RSPA20210916C71" ref-type="bibr">71</xref>]. A snapshot of the simulated concentration field <inline-formula><mml:math id="IM250"><mml:mi>u</mml:mi></mml:math></inline-formula> in the three-dimensional cube <inline-formula><mml:math id="IM251"><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2.5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is shown in <xref rid="RSPA20210916F7" ref-type="fig">figure 7</xref>. The simulation used <inline-formula><mml:math id="IM252"><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>128</mml:mn><mml:mo>×</mml:mo><mml:mn>128</mml:mn></mml:math></inline-formula> discretization points in space on a regular Cartesian mesh and explicit Euler time stepping with step size <inline-formula><mml:math id="IM253"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0005</mml:mn></mml:math></inline-formula> until a final simulated time of 5 seconds. The ground-truth model parameters used are: <inline-formula><mml:math id="IM254"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0.053</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM255"><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>0.014</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM256"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="IM257"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.
<fig position="float" id="RSPA20210916F7"><label>Figure 7<x xml:space="preserve">. </x></label><caption><p>Model selection with PDE-STRIDE+IHT-d for the three-dimensional Gray–Scott <inline-formula><mml:math id="IM258"><mml:mi>u</mml:mi></mml:math></inline-formula>-component equation. The top left figure shows a visualization of the scalar concentration field <inline-formula><mml:math id="IM259"><mml:mi>u</mml:mi></mml:math></inline-formula> in the three-dimensional simulation domain with red corresponding to high concentration and blue to low concentration. The stability plots for the design <inline-formula><mml:math id="IM260"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>400</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM261"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>69</mml:mn></mml:math></inline-formula> show good separation of the true PDE components (in solid colour with line symbols as identified in the inset legend) from the noise components (dashed black). The inference power of the method is tested for additive Gaussian noise levels <inline-formula><mml:math id="IM262"><mml:mi>σ</mml:mi></mml:math></inline-formula> up to 6%. In all the cases, perfect recovery was possible without parameter tuning. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f07" position="float"/></fig></p>
        <p>We test recovery of the ground-truth PDE from data sampled only from the small cube <inline-formula><mml:math id="IM263"><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>1.0</mml:mn><mml:mo>,</mml:mo><mml:mn>1.5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> in the centre of the domain. <xref rid="RSPA20210916F7" ref-type="fig">Figure 7</xref> shows the PDE-STRIDE+IHT-d results for the species <inline-formula><mml:math id="IM264"><mml:mi>u</mml:mi></mml:math></inline-formula>. All PDE components of the true equation (3.3<italic toggle="yes">a</italic>) are correctly identified for noise levels up to 6% with as few as <inline-formula><mml:math id="IM265"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>400</mml:mn></mml:math></inline-formula> samples for dictionary size <inline-formula><mml:math id="IM266"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>69</mml:mn></mml:math></inline-formula>. The plots for the <inline-formula><mml:math id="IM267"><mml:mi>v</mml:mi></mml:math></inline-formula> species are shown in the electronic supplementary material, figure S5. Although perfect recovery was not possible owing to the small diffusivity (<inline-formula><mml:math id="IM268"><mml:msub><mml:mi>D</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) of the <inline-formula><mml:math id="IM269"><mml:mi>v</mml:mi></mml:math></inline-formula> species, consistent and stable recovery of the reaction terms is seen. The re-fitted coefficients in the recovered PDEs for the <inline-formula><mml:math id="IM270"><mml:mi>u</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="IM271"><mml:mi>v</mml:mi></mml:math></inline-formula> species are reported in <xref rid="RSPA20210916TB3" ref-type="table">table 3</xref> and the electronic supplementary material, table S1, respectively.
<table-wrap position="float" id="RSPA20210916TB3"><label>Table 3<x xml:space="preserve">. </x></label><caption><p>Coefficients of the recovered <inline-formula><mml:math id="IM272"><mml:mi>u</mml:mi></mml:math></inline-formula>-component Gray–Scott reaction–diffusion equation for different noise levels. The stable components of the PDE inferred from the plots in <xref rid="RSPA20210916F7" ref-type="fig">figure 7</xref> are <inline-formula><mml:math id="IM273"><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula>. The ground-truth coefficient values are given in parentheses in the column headings.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">1 (0.014)</th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM274"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="IM275"><mml:mn>2.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>)</th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM276"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="IM277"><mml:mn>2.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>)</th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM278"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="IM279"><mml:mn>2.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>)</th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM280"><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>0.014</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula></th><th align="left" rowspan="1" colspan="1"><inline-formula><mml:math id="IM281"><mml:mi>u</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> (<inline-formula><mml:math id="IM282"><mml:mo>−</mml:mo></mml:math></inline-formula>1.0)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">clean</td><td rowspan="1" colspan="1">0.0140</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM283"><mml:mn>2.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM284"><mml:mn>2.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM285"><mml:mn>2.0</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM286"><mml:mo>−</mml:mo><mml:mn>0.0140</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM287"><mml:mo>−</mml:mo><mml:mn>1.0000</mml:mn></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1">2%</td><td rowspan="1" colspan="1">0.0142</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM288"><mml:mn>1.9664</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM289"><mml:mn>1.9565</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM290"><mml:mn>1.9869</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM291"><mml:mo>−</mml:mo><mml:mn>0.0143</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM292"><mml:mo>−</mml:mo><mml:mn>0.9915</mml:mn></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1">4%</td><td rowspan="1" colspan="1">0.0144</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM293"><mml:mn>1.9541</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM294"><mml:mn>1.8971</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM295"><mml:mn>1.8780</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM296"><mml:mo>−</mml:mo><mml:mn>0.0146</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM297"><mml:mo>−</mml:mo><mml:mn>0.9795</mml:mn></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1">6%</td><td rowspan="1" colspan="1">0.0150</td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM298"><mml:mn>2.0494</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM299"><mml:mn>1.8888</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM300"><mml:mn>1.8284</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM301"><mml:mo>−</mml:mo><mml:mn>0.0153</mml:mn></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM302"><mml:mo>−</mml:mo><mml:mn>0.9843</mml:mn></mml:math></inline-formula></td></tr></tbody></table></table-wrap></p>
        <p>For comparison, the results when using STRidge in conjunction with stability selection are shown in the electronic supplementary material, figures S6 and S7. STRidge is able to recover the complete form of equation (3.3), albeit only in the noise-free case. It fails to recover any of the two components when noise is added to the data.</p>
      </sec>
    </sec>
    <sec id="s3e">
      <label>(d)<x xml:space="preserve"> </x></label>
      <title>Achievability results</title>
      <p>We discuss the consistency and robustness of PDE-STRIDE + IHT-d with respect to design parameters including sample size <inline-formula><mml:math id="IM303"><mml:mi>N</mml:mi></mml:math></inline-formula>, dictionary size <inline-formula><mml:math id="IM304"><mml:mi>p</mml:mi></mml:math></inline-formula> and noise level <inline-formula><mml:math id="IM305"><mml:mi>σ</mml:mi></mml:math></inline-formula>. Achievability analysis provides a compact way of checking robustness and consistency of a model selection method for varying design parameters. It also provides approximate means to reveal the <italic toggle="yes">sample complexity</italic> of any <inline-formula><mml:math id="IM306"><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="IM307"><mml:msub><mml:mi>ℓ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> sparsity-promoting technique, i.e. the number of data points <inline-formula><mml:math id="IM308"><mml:mi>N</mml:mi></mml:math></inline-formula> required to recover the model with full probability. Specifically, given a sparsity-promoting regularizer, dictionary size <inline-formula><mml:math id="IM309"><mml:mi>p</mml:mi></mml:math></inline-formula>, sparsity <inline-formula><mml:math id="IM310"><mml:mi>k</mml:mi></mml:math></inline-formula> and noise level <inline-formula><mml:math id="IM311"><mml:mi>σ</mml:mi></mml:math></inline-formula>, we are interested in how the sample size <inline-formula><mml:math id="IM312"><mml:mi>N</mml:mi></mml:math></inline-formula> scales with <inline-formula><mml:math id="IM313"><mml:mi>p</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="IM314"><mml:mi>k</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="IM315"><mml:mi>σ</mml:mi></mml:math></inline-formula> for the recovery probability converging to one. The study in [<xref rid="RSPA20210916C74" ref-type="bibr">74</xref>] reported sharp phase transitions from failure to success for Gaussian random designs with increasing sample size <inline-formula><mml:math id="IM316"><mml:mi>N</mml:mi></mml:math></inline-formula> for LASSO-based sparsity solutions. The same study also provided sufficient lower bounds for sample size <inline-formula><mml:math id="IM317"><mml:mi>N</mml:mi></mml:math></inline-formula> as a function of <inline-formula><mml:math id="IM318"><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula> for full recovery probability. We ask the question whether sparse model selection with PDE-STRIDE + IHT-d exhibits similar sharp phase-transition behaviour. Given the dictionary components in PDE learning are compiled from derivatives and nonlinearities computed from noisy data, it is also interesting to observe whether full recovery is at all achieved and maintained with increasing sample size <inline-formula><mml:math id="IM319"><mml:mi>N</mml:mi></mml:math></inline-formula>. In the particular context of PDE learning, increasing dictionary size by including higher-order nonlinearities and higher-order derivatives tends to introduce strongly correlated components, which can negatively impact the inference power.</p>
      <p>In figures <xref rid="RSPA20210916F8" ref-type="fig">8</xref> and <xref rid="RSPA20210916F9" ref-type="fig">9</xref>, the achievability plots for the one-dimensional Burgers system and the <inline-formula><mml:math id="IM320"><mml:mi>u</mml:mi></mml:math></inline-formula>-component of the three-dimensional Gray–Scott system are shown, respectively. Each symbol in the figures shows the mean over 20 repetitions of an experiment with some design <inline-formula><mml:math id="IM321"><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> under random data sub-sampling. The Bernoulli variances are shown as coloured bands. An experiment with a design <inline-formula><mml:math id="IM322"><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is considered a success if and only if there exists a <inline-formula><mml:math id="IM323"><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mi>Λ</mml:mi></mml:math></inline-formula> for which the true PDE support is recovered by PDE-STRIDE+IHT-d with default importance threshold <inline-formula><mml:math id="IM324"><mml:msub><mml:mi>π</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:math></inline-formula>.
<fig position="float" id="RSPA20210916F8"><label>Figure 8<x xml:space="preserve">. </x></label><caption><p>Achievability plots for model selection with PDE-STRIDE+IHT-d for the one-dimensional Burgers equation. Each symbol is the mean over 20 repetitions of the inference for some design <inline-formula><mml:math id="IM325"><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> under random data sub-sampling. Different symbols and colours correspond to different dictionary sizes <inline-formula><mml:math id="IM326"><mml:mi>p</mml:mi></mml:math></inline-formula> as given in the inset legend. The coloured bands show the variance of the Bernoulli trials. Noise levels <inline-formula><mml:math id="IM327"><mml:mi>σ</mml:mi></mml:math></inline-formula> are given by the red percentage in each panel. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f08" position="float"/></fig>
<fig position="float" id="RSPA20210916F9"><label>Figure 9<x xml:space="preserve">. </x></label><caption><p>Achievability plots for model selection with PDE-STRIDE+IHT-d for the <inline-formula><mml:math id="IM328"><mml:mi>u</mml:mi></mml:math></inline-formula>-component of the three-dimensional Gray–Scott equation. Each symbol is the mean over 20 repetitions of the inference for some design <inline-formula><mml:math id="IM329"><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> under random data sub-sampling. Different symbols and colours correspond to different dictionary sizes <inline-formula><mml:math id="IM330"><mml:mi>p</mml:mi></mml:math></inline-formula> as given in the inset legend. The coloured bands show the variance of the Bernoulli trials. Noise levels <inline-formula><mml:math id="IM331"><mml:mi>σ</mml:mi></mml:math></inline-formula> are given by the red percentage in each panel. (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f09" position="float"/></fig></p>
      <p>In all cases, PDE-STRIDE + IHT-d is strongly consistent and highly robust. In addition, we also observe a sharp phase transition from failure to success with recovery probabilities quickly approaching one for increasing sample size beyond a certain threshold. This suggests that PDE-STRIDE not only enhances the inference power of IHT-d but also ensures consistency. The sharp phase transition also suggests the existence of a strict lower bound on the sample complexity, below which full recovery is not achievable [<xref rid="RSPA20210916C74" ref-type="bibr">74</xref>]. From the achievability plots, we estimate the sample complexity of the learned dynamical systems: for the one-dimensional Burgers equation, 90% success probability is achieved with as few as <inline-formula><mml:math id="IM332"><mml:mrow><mml:mo>≈</mml:mo></mml:mrow><mml:mn>70</mml:mn></mml:math></inline-formula> data points in the noise-free and <inline-formula><mml:math id="IM333"><mml:mrow><mml:mo>≈</mml:mo></mml:mrow><mml:mn>200</mml:mn></mml:math></inline-formula> data points in the noisy cases across different designs (<inline-formula><mml:math id="IM334"><mml:mi>p</mml:mi></mml:math></inline-formula>). For the three-dimensional Gray–Scott system, 90% success probability is achieved with as few as <inline-formula><mml:math id="IM335"><mml:mrow><mml:mo>≈</mml:mo></mml:mrow><mml:mn>200</mml:mn></mml:math></inline-formula> data points in the noise-free and <inline-formula><mml:math id="IM336"><mml:mrow><mml:mo>≈</mml:mo></mml:mrow><mml:mn>400</mml:mn></mml:math></inline-formula> in the noisy cases across different designs (<inline-formula><mml:math id="IM337"><mml:mi>p</mml:mi></mml:math></inline-formula>). This demonstrates that PDE-STRIDE + IHT-d is able to consistently and robustly learn PDE models from limited noisy data.</p>
    </sec>
  </sec>
  <sec id="s4">
    <label>4<x xml:space="preserve">. </x></label>
    <title>Data-driven inference on experimental data to explain <italic toggle="yes">C. elegans</italic> zygote polarization</title>
    <p>We showcase the applicability of the PDE-STRIDE with IHT-d to real experimental data. We use microscopy images to infer a model that explains early <italic toggle="yes">C. elegans</italic> embryo polarity establishment, and we use it to confirm a previous hypothesis about the underlying protein interaction network. Earlier studies of this process proposed a mechano-chemical mechanism for PAR protein polarization on the cell membrane [<xref rid="RSPA20210916C15" ref-type="bibr">15</xref>,<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>,<xref rid="RSPA20210916C66" ref-type="bibr">66</xref>]. They systematically showed that active cortical flows in the zygote provide sufficient bias to trigger symmetry breaking [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>]. The experiments conducted in [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>] measured the concentration of the anterior PAR complex (aPAR), the concentration of the posterior PAR complex (pPAR) and the cortical flow velocity field as a function of time (<xref rid="RSPA20210916F10" ref-type="fig">figure 10</xref><italic toggle="yes">a</italic>–<italic toggle="yes">d</italic>). The concentration and velocity fields were acquired on a grid with a resolution of <inline-formula><mml:math id="IM338"><mml:mn>60</mml:mn><mml:mo>×</mml:mo><mml:mn>55</mml:mn></mml:math></inline-formula> in space and time, respectively. Space was modelled one-dimensional along a circumferential line around the embryo in the mid-plane cross section as shown in <xref rid="RSPA20210916F10" ref-type="fig">figure 10</xref><italic toggle="yes">a</italic>.
<fig position="float" id="RSPA20210916F10"><label>Figure 10<x xml:space="preserve">. </x></label><caption><p>Data-driven inference of the regulatory network of PAR proteins from spatio-temporal data of a <italic toggle="yes">C. elegans</italic> zygote. (<italic toggle="yes">a</italic>) Final frame from a fluorescence microscopy video recorded by the Grill laboratory [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>] at MPI-CBG showing a mid-plane cross section through the ellipsoidal <inline-formula><mml:math id="IM339"><mml:mn>50</mml:mn><mml:mo> </mml:mo><mml:mrow><mml:mtext>μ</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:math></inline-formula> zygote. Over time, the pPAR protein complex (blue fluorescence signal) localizes to the posterior pole of the embryo (region ‘a’), whereas aPAR (red fluorescence signal) localizes anterior (regions ‘b’ and ‘c’). (<italic toggle="yes">b</italic>) Protein concentrations extracted from the fluorescence intensity along the one-dimensional midline between the two white ellipses in (<italic toggle="yes">a</italic>) for aPAR (red) and pPAR (blue) for different time points (symbols, see inset legend under (<italic toggle="yes">a</italic>)). (<italic toggle="yes">c</italic>) De-noised concentration profiles obtained by extracting the principle mode of the singular value decomposition (SVD) of the noisy data in (<italic toggle="yes">b</italic>). (<italic toggle="yes">d</italic>) De-noised cortical flow velocity profiles (first SVD mode) in the tangential direction along the circumferential line around the zygote for different times (colour, inset legend). (<italic toggle="yes">e</italic>,<italic toggle="yes">f</italic>) Stability plots obtained by using PDE-STRIDE + IHT-d to separate the stable model components (coloured solid lines with symbols, inset legend) from the noise components (black dashed lines) for aPAR (<italic toggle="yes">e</italic>) and pPAR (<italic toggle="yes">f</italic>). (<italic toggle="yes">g</italic>) Achievability plot to test the robustness and consistency of the inferred model for both aPAR (<inline-formula><mml:math id="IM340"><mml:msubsup><mml:mi>S</mml:mi><mml:mtext>stable</mml:mtext><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>P</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula>) and pPAR (<inline-formula><mml:math id="IM341"><mml:msubsup><mml:mi>S</mml:mi><mml:mtext>stable</mml:mtext><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula>) with increasing sample size <inline-formula><mml:math id="IM342"><mml:mi>N</mml:mi></mml:math></inline-formula>. (<italic toggle="yes">h</italic>,<italic toggle="yes">i</italic>) Simulation results (dashed lines) of the learned models for aPAR (<italic toggle="yes">h</italic>) and pPAR (<italic toggle="yes">i</italic>) compared with the denoised experimental data (solid lines with symbols) at different times (inset legend). (Online version in colour.)</p></caption><graphic xlink:href="rspa20210916f10" position="float"/></fig></p>
    <p>We challenge <inline-formula><mml:math id="IM343"><mml:mrow><mml:mi mathvariant="normal">PDE</mml:mi></mml:mrow><mml:mtext>-</mml:mtext><mml:mrow><mml:mi mathvariant="normal">STRIDE</mml:mi></mml:mrow><mml:mo> </mml:mo><mml:mo>+</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:mi mathvariant="normal">IHT</mml:mi></mml:mrow><mml:mtext>-</mml:mtext><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:math></inline-formula> to learn a differential equation model for the regulatory network of the interacting PAR proteins in a purely data-driven fashion from the one single microscopy video available as electronic supplementary material for [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>]. Given the high level of noise in the video, we limit our analysis to the first SVD mode of the data as shown in <xref rid="RSPA20210916F10" ref-type="fig">figure 10</xref><italic toggle="yes">c</italic>. We only consider the time after the initial advection trigger, when early domains of PAR proteins have already formed. PDE-STRIDE is then directed to learn an interpretable model that evolves the nascent protein domains to the fully developed polarity patterns shown in <xref rid="RSPA20210916F10" ref-type="fig">figure 10</xref><italic toggle="yes">a</italic>.</p>
    <p>The only mechanism included in this model is the chemical interaction between aPAR and pPAR of the general form
<disp-formula id="RSPA20210916M4x1"><label>4.1</label><mml:math id="DM18" display="block"><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mi>P</mml:mi><mml:mover><mml:mo>→</mml:mo><mml:mpadded width="+0.611em" lspace="0.278em" voffset=".15em"><mml:mi>k</mml:mi></mml:mpadded></mml:mover><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mi>P</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>
with unknown reactant and product stoichiometries <inline-formula><mml:math id="IM344"><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="IM345"><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> for aPAR and pPAR, respectively. The scalar fields <inline-formula><mml:math id="IM346"><mml:mi>A</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="IM347"><mml:mi>P</mml:mi></mml:math></inline-formula> are the concentrations of aPAR and pPAR, and <inline-formula><mml:math id="IM348"><mml:mi>k</mml:mi></mml:math></inline-formula> is the unknown reaction rate.</p>
    <p>In designing the dictionary <inline-formula><mml:math id="IM349"><mml:mi>Θ</mml:mi></mml:math></inline-formula>, the maximum allowed stoichiometry for reactants and products is restricted to 2, i.e. <inline-formula><mml:math id="IM350"><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula>. The PDE-STRIDE + IHT-d results for the learned reaction network from data are shown in <xref rid="RSPA20210916F10" ref-type="fig">figure 10</xref><italic toggle="yes">e</italic>,<italic toggle="yes">f</italic>. The stable components of the model for pPAR are <inline-formula><mml:math id="IM351"><mml:msubsup><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>A</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula>, and for aPAR they are <inline-formula><mml:math id="IM352"><mml:msubsup><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo fence="false" stretchy="false">}</mml:mo></mml:math></inline-formula> for <inline-formula><mml:math id="IM353"><mml:mi>N</mml:mi><mml:mo>≈</mml:mo><mml:mn>500</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM354"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula>. The achievability plot in <xref rid="RSPA20210916F10" ref-type="fig">figure 10</xref><italic toggle="yes">g</italic> confirms the consistency and robustness of the learned model across different sample sizes <inline-formula><mml:math id="IM355"><mml:mi>N</mml:mi></mml:math></inline-formula>. The inference method achieves full recovery probability for <inline-formula><mml:math id="IM356"><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>800</mml:mn></mml:math></inline-formula>.</p>
    <p>The final chemical reaction model learned by PDE-STRIDE + IHT-d is
<disp-formula id="RSPA20210916M4x2a"><label>4.2<italic toggle="yes">a</italic></label><mml:math id="DM19" display="block"><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mi>A</mml:mi><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula>
and
<disp-formula id="RSPA20210916M4x2b"><label>4.2<italic toggle="yes">b</italic></label><mml:math id="DM20" display="block"><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mi>P</mml:mi><mml:msup><mml:mi>A</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula>
where <inline-formula><mml:math id="IM357"><mml:msub><mml:mi>k</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="IM358"><mml:msub><mml:mi>k</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> are the kinetic rate constants for membrane association and dissociation of aPAR and pPAR, respectively. The coefficients <inline-formula><mml:math id="IM359"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="IM360"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represent the rates of membrane dissociation due to mutually antagonistic interactions between the protein complexes. The constants <inline-formula><mml:math id="IM361"><mml:msub><mml:mi>c</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="IM362"><mml:msub><mml:mi>c</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> can be interpreted as surface-to-volume conversion factors for the concentrations [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>]. This purely data-driven model recapitulates the mutual inhibitory nature of the PAR proteins [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>].</p>
    <p>Moreover, our method allows us to estimate the values of the unknown reaction-rate parameters by least-squares re-fitting on the recovered stable support. The results are shown in <xref rid="RSPA20210916TB4" ref-type="table">table 4</xref>.
<table-wrap position="float" id="RSPA20210916TB4"><label>Table 4<x xml:space="preserve">. </x></label><caption><p>Coefficients values of the inferred model in equation (4.2).</p></caption><table frame="hsides" rules="groups" width="3pc"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><tbody><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM363"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM364"><mml:mo>−</mml:mo><mml:mn>1.98</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mrow><mml:mtext>μ</mml:mtext></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM365"><mml:msub><mml:mi>c</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM366"><mml:mn>4.13</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mrow><mml:mtext>μ</mml:mtext></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM367"><mml:msub><mml:mi>k</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM368"><mml:mn>1.07</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM369"><mml:msub><mml:mi>k</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM370"><mml:mn>2.16</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mn>1</mml:mn><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM371"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM372"><mml:mo>−</mml:mo><mml:mn>2.79</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mrow><mml:mtext>μ</mml:mtext></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM373"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM374"><mml:mo>−</mml:mo><mml:mn>1.47</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:mrow><mml:mtext>μ</mml:mtext></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula></td></tr></tbody></table></table-wrap></p>
    <p>In the <xref rid="RSPA20210916F10" ref-type="fig">figure 10</xref><italic toggle="yes">h</italic>,<italic toggle="yes">i</italic>, we overlay the numerical solution of equation (4.2) (dashed lines) with the denoised experimental data (solid lines with symbols) at different time points (see inset legend) for both aPAR and pPAR. The de-noised spatio-temporal measurement data from the early domains are taken as the initial conditions for the simulation. The results show that the simple data-driven chemical reaction model is able to qualitatively match the temporal evolution of the PAR protein domains in the <italic toggle="yes">C. elegans</italic> zygote. Although the spatial patterns match well (e.g. the PAR domain sizes) for the two proteins, there exist non-negligible discrepancies between the simulation and the experiments in the time scales of the pPAR domain evolution. This is likely because our ODE model only includes the chemical reactions of the protein interactions, but neither the diffusion nor the advective flow of the proteins. In the electronic supplementary material, figure S8 (left), we show for <inline-formula><mml:math id="IM375"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="IM376"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula> that the advection and diffusion components of the aPAR species carry enough importance to be included in the stable set <inline-formula><mml:math id="IM377"><mml:msubsup><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mtext>stable</mml:mtext><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, but that this is not the case for the pPAR components. The preferential advective displacement of aPAR to the anterior side modelled by the advective term <inline-formula><mml:math id="IM378"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is also in line with experimental observations [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>]. However, such models with advection and diffusion components exhibit inconsistency for increasing sample size <inline-formula><mml:math id="IM379"><mml:mi>N</mml:mi></mml:math></inline-formula>, in contrast to our simple ODE model, as illustrated in <xref rid="RSPA20210916F10" ref-type="fig">figure 10</xref><italic toggle="yes">g</italic>. We therefore believe that it may be necessary to use structured sparsity for enforcing conservation laws through grouping [<xref rid="RSPA20210916C75" ref-type="bibr">75</xref>] in order to further develop this data-driven model to also include the mechanical aspects of the PAR system.</p>
  </sec>
  <sec id="s5">
    <label>5<x xml:space="preserve">. </x></label>
    <title>Conclusion and discussion</title>
    <p>We have addressed two key issues that have so far limited the application of sparse regression methods for automated PDE inference from noisy and limited data: the need for manual parameter tuning and the high sensitivity to noise in the data. We have shown that stability selection combined with any sparsity-promoting regression technique provides an appropriate level of regularization for consistent and robust recovery of the correct PDE model. Our numerical benchmarks suggested that iterative hard thresholding with de-biasing (IHT-d) is ideal in combination with stability selection to form a robust framework for PDE learning with minimal parameter tuning. This combination of methods outperformed all other tested algorithmic approaches with respect to identification performance, amount of data required and robustness to noise. The resulting stability-based PDE-STRIDE method was tested for robust recovery of the one-dimensional Burgers equation, two-dimensional vorticity transport equation and three-dimensional Gray–Scott reaction–diffusion equations from simulation data corrupted with up to 6% additive Gaussian noise. The achievability studies demonstrated the consistency and robustness of the PDE-STRIDE method for full recovery probability of the model with increasing sample size <inline-formula><mml:math id="IM380"><mml:mi>N</mml:mi></mml:math></inline-formula> and for varying dictionary size <inline-formula><mml:math id="IM381"><mml:mi>p</mml:mi></mml:math></inline-formula> and noise levels <inline-formula><mml:math id="IM382"><mml:mi>σ</mml:mi></mml:math></inline-formula>. In addition, we confirmed the sharp phase transition in recovery performance and noted that achievability plots provide a natural estimate for the sample complexity of the underlying nonlinear dynamical system. However, this empirical estimate of sample complexity depends on the choice of model selection scheme and on how the data are sampled.</p>
    <p>We demonstrated the capabilities of PDE-STRIDE by learning a protein-interaction model of embryo polarization directly from fluorescence microscopy images of a <italic toggle="yes">C. elegans</italic> zygote. The model recovered the regulatory reaction network of the involved proteins, complete with its parameter values in a purely data-driven manner, with no knowledge used about the underlying physics or symmetries. The thus-learned, data-derived model was able to correctly predict the spatio-temporal dynamics of the embryonic polarity system from the early spatial domains to the fully developed patterns as observed in the polarized <italic toggle="yes">C. elegans</italic> zygote. The model we inferred from image data using our method confirms both the structure and the mechanisms of physics-derived cell polarity models [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>]. Importantly, the mutually inhibitory interactions between the involved protein species, which have previously been discovered by extensive biochemical experimentation, were automatically and unambiguously extracted from the data [<xref rid="RSPA20210916C16" ref-type="bibr">16</xref>].</p>
    <p>Besides rendering sparse inference methods more robust to noise and to parameter settings, stability selection has the important conceptual benefit of also providing interpretable probabilistic importance measures for all model components. This enables modellers to construct their models with high fidelity, and to gain an intuition about correlations and sensitivities. Graphical inspection of stability plots provides additional freedom for intervention in semi-automated model discovery from data.</p>
    <p>We expect that statistical learning methods have the potential to enable robust, consistent and reproducible discovery of predictive and interpretable models directly from observational data. Our PDE-STRIDE framework provides a first step towards this goal, but several open issues remain. First, numerically approximating time and space derivatives in the noisy data is a challenge for noise levels higher than a few percent. This currently limits the noise robustness of the overall method, regardless of how robust the subsequent statistical inference is. The impact of noise becomes even more severe when exploring models with higher-order derivatives or stronger nonlinearities. Future work should focus on formulations that are robust to the choice of different derivative-discretization methods, while providing the necessary freedom to impose structure on the coefficients. Here, signal/noise decomposition techniques based on deep neural networks with time-stepping constraints [<xref rid="RSPA20210916C76" ref-type="bibr">76</xref>,<xref rid="RSPA20210916C77" ref-type="bibr">77</xref>] could provide better denoising when the noise has finite correlation length in space and time.</p>
    <p>Second, it would be desirable to have a principled way to constrain the learning process by physical priors, such as conservation laws and symmetries. Exploiting structural knowledge about the dynamical system is expected to greatly improve learning performance. Structured sparsity or grouping constraints from statistics may help express such prior knowledge in a sparse inference problem [<xref rid="RSPA20210916C75" ref-type="bibr">75</xref>,<xref rid="RSPA20210916C78" ref-type="bibr">78</xref>]. In the specific example of the PAR-polarity model, decades of experimentation and theory have revealed physical principles of mass conservation, detailed force balance in the cell cortex and antagonistic interactions between protein complexes. Using structured constraints to encode such physical principles leads to biologically plausible and physically consistent models, rather than models that simply fit the data [<xref rid="RSPA20210916C79" ref-type="bibr">79</xref>].</p>
    <p>In summary, we believe that data-driven model discovery has tremendous potential to provide novel insights into complex systems, in particular in biology. It provides an effective and complementary alternative to theory-driven approaches. We hope that the stability-based model selection method PDE-STRIDE presented here is going to contribute to the further development and adoption of these approaches in the sciences.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material position="float" content-type="local-data">
      <media xlink:href="rspa20210916_review_history.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgements</title>
    <p>We are grateful to the Grill laboratory at MPI-CBG, who provided us with high-quality spatial-temporal PAR concentration and flow field data. We also thank Nathan Kutz (University of Washington) and his group for making their code and data public.</p>
  </ack>
  <sec sec-type="data-availability" id="s6">
    <title>Data accessibility</title>
    <p>The git repository for the codes and data can be found at <uri xlink:href="https://git.mpi-cbg.de/mosaic/software/machine-learning/pde-stride">https://git.mpi-cbg.de/mosaic/software/machine-learning/pde-stride</uri>.</p>
    <p>The data are provided in the electronic supplementary material [<xref rid="RSPA20210916C80" ref-type="bibr">80</xref>].</p>
  </sec>
  <sec id="s7">
    <title>Authors' contributions</title>
    <p>S.M.: conceptualization, data curation, formal analysis, investigation, software, validation, visualization, writing—original draft; B.L.C.: data curation, formal analysis, resources, writing—review and editing; I.F.S.: conceptualization, funding acquisition, investigation, methodology, project administration, resources, supervision, writing—review and editing; C.L.M.: conceptualization, formal analysis, investigation, methodology, project administration, resources, supervision, validation, writing—review and editing.</p>
    <p>All authors gave final approval for publication and agreed to be held accountable for the work performed therein.</p>
  </sec>
  <sec id="s8">
    <title>Conflict of interest declaration</title>
    <p>We declare we have no competing interests.</p>
  </sec>
  <sec id="s9">
    <title>Funding</title>
    <p>This work was in parts supported by the German Research Foundation (DFG, Deutsche Forschungsgemeinschaft) under funding code EXC-2068, Cluster of Excellence ‘Physics of Life’, and by the Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig funded by the Federal Ministry of Education and Research (BMBF, Bundesministerium für Bildung und Forschung).</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="RSPA20210916C1">
      <label>1<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mogilner</surname><given-names>A</given-names></string-name>, <string-name><surname>Wollman</surname><given-names>R</given-names></string-name>, <string-name><surname>Marshall</surname><given-names>WF</given-names></string-name></person-group>. <year>2006</year><article-title>Quantitative modeling in cell biology: what is it good for?</article-title><source>Dev. Cell</source><volume><bold>11</bold></volume>, <fpage>279</fpage>-<lpage>287</lpage>. (<pub-id pub-id-type="doi">10.1016/j.devcel.2006.08.004</pub-id>)<pub-id pub-id-type="pmid">16950120</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C2">
      <label>2<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sbalzarini</surname><given-names>IF</given-names></string-name></person-group>. <year>2013</year><article-title>Modeling and simulation of biological systems from image data</article-title>. <source>Bioessays</source><volume><bold>35</bold></volume>, <fpage>482</fpage>-<lpage>490</lpage>. (<pub-id pub-id-type="doi">10.1002/bies.201200051</pub-id>)<pub-id pub-id-type="pmid">23533152</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C3">
      <label>3<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tomlin</surname><given-names>CJ</given-names></string-name>, <string-name><surname>Axelrod</surname><given-names>JD</given-names></string-name></person-group>. <year>2007</year><article-title>Biology by numbers: mathematical modelling in developmental biology</article-title>. <source>Nat. Rev. Genet.</source><volume><bold>8</bold></volume>, <fpage>331</fpage>. (<pub-id pub-id-type="doi">10.1038/nrg2098</pub-id>)<pub-id pub-id-type="pmid">17440530</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C4">
      <label>4<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Duffy</surname><given-names>DJ</given-names></string-name></person-group>. <year>2013</year><source>Finite difference methods in financial engineering: a partial differential equation approach</source>. <publisher-name>Chichester, UK: John Wiley &amp; Sons</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C5">
      <label>5<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adomian</surname><given-names>G</given-names></string-name></person-group>. <year>1995</year><article-title>Solving the mathematical models of neurosciences and medicine</article-title>. <source>Math. Comput. Simul.</source><volume><bold>40</bold></volume>, <fpage>107</fpage>-<lpage>114</lpage>. (<pub-id pub-id-type="doi">10.1016/0378-4754(95)00021-8</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C6">
      <label>6<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Donoho</surname><given-names>D</given-names></string-name></person-group>. <year>2017</year><article-title>50 years of data science</article-title>. <source>J. Comput. Graph. Stat.</source><volume><bold>26</bold></volume>, <fpage>745</fpage>-<lpage>766</lpage>. (<pub-id pub-id-type="doi">10.1080/10618600.2017.1384734</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C7">
      <label>7<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barnes</surname><given-names>CP</given-names></string-name>, <string-name><surname>Silk</surname><given-names>D</given-names></string-name>, <string-name><surname>Sheng</surname><given-names>X</given-names></string-name>, <string-name><surname>Stumpf</surname><given-names>MPH</given-names></string-name></person-group>. <year>2011</year><article-title>Bayesian design of synthetic biological systems</article-title>. <source>Proc. Natl Acad. Sci. USA</source><volume><bold>108</bold></volume>, <fpage>15 190</fpage>-<lpage>15 195</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.1017972108</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C8">
      <label>8<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Asmus</surname><given-names>J</given-names></string-name>, <string-name><surname>Müller</surname><given-names>CL</given-names></string-name>, <string-name><surname>Sbalzarini</surname><given-names>IF</given-names></string-name></person-group>. <year>2017</year><article-title>Lp-Adaptation: simultaneous design centering and robustness estimation of electronic and biological systems</article-title>. <source>Sci. Rep.</source><volume><bold>7</bold></volume>, <fpage>6660</fpage>. (<pub-id pub-id-type="doi">10.1038/s41598-017-03556-5</pub-id>)<pub-id pub-id-type="pmid">28751662</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C9">
      <label>9<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kitano</surname><given-names>H</given-names></string-name></person-group>. <year>2002</year><article-title>Computational systems biology</article-title>. <source>Nature</source><volume><bold>420</bold></volume>, <fpage>206</fpage>. (<pub-id pub-id-type="doi">10.1038/nature01254</pub-id>)<pub-id pub-id-type="pmid">12432404</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C10">
      <label>10<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gregor</surname><given-names>T</given-names></string-name>, <string-name><surname>Bialek</surname><given-names>W</given-names></string-name>, <string-name><surname>van Steveninck</surname><given-names>RRR</given-names></string-name>, <string-name><surname>Tank</surname><given-names>DW</given-names></string-name>, <string-name><surname>Wieschaus</surname><given-names>EF</given-names></string-name></person-group>. <year>2005</year><article-title>Diffusion and scaling during early embryonic pattern formation</article-title>. <source>Proc. Natl Acad. Sci. USA</source><volume><bold>102</bold></volume>, <fpage>18 403</fpage>-<lpage>18 407</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.0509483102</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C11">
      <label>11<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>T</given-names></string-name>, <string-name><surname>He</surname><given-names>HL</given-names></string-name>, <string-name><surname>Church</surname><given-names>GM</given-names></string-name></person-group>. <year>1999</year><comment>Modeling gene expression with differential equations. In <italic toggle="yes">Biocomputing’99</italic>, pp. 29–40. World Scientific</comment>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C12">
      <label>12<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ay</surname><given-names>A</given-names></string-name>, <string-name><surname>Arnosti</surname><given-names>DN</given-names></string-name></person-group>. <year>2011</year><article-title>Mathematical modeling of gene expression: a guide for the perplexed biologist</article-title>. <source>Crit. Rev. Biochem. Mol. Biol.</source><volume><bold>46</bold></volume>, <fpage>137</fpage>-<lpage>151</lpage>. (<pub-id pub-id-type="doi">10.3109/10409238.2011.556597</pub-id>)<pub-id pub-id-type="pmid">21417596</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C13">
      <label>13<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Prost</surname><given-names>J</given-names></string-name>, <string-name><surname>Jülicher</surname><given-names>F</given-names></string-name>, <string-name><surname>Joanny</surname><given-names>JF</given-names></string-name></person-group>. <year>2015</year><article-title>Active gel physics</article-title>. <source>Nat. Phys.</source><volume><bold>11</bold></volume>, <fpage>111</fpage>. (<pub-id pub-id-type="doi">10.1038/nphys3224</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C14">
      <label>14<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mietke</surname><given-names>A</given-names></string-name>, <string-name><surname>Jemseena</surname><given-names>V</given-names></string-name>, <string-name><surname>Kumar</surname><given-names>KV</given-names></string-name>, <string-name><surname>Sbalzarini</surname><given-names>IF</given-names></string-name>, <string-name><surname>Jülicher</surname><given-names>F</given-names></string-name></person-group>. <year>2019</year><article-title>Minimal model of cellular symmetry breaking</article-title>. <source>Phys. Rev. Lett.</source><volume><bold>123</bold></volume>, <fpage>188101</fpage>. (<pub-id pub-id-type="doi">10.1103/PhysRevLett.123.188101</pub-id>)<pub-id pub-id-type="pmid">31763902</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C15">
      <label>15<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Etemad-Moghadam</surname><given-names>B</given-names></string-name>, <string-name><surname>Guo</surname><given-names>S</given-names></string-name>, <string-name><surname>Kemphues</surname><given-names>KJ</given-names></string-name></person-group>. <year>1995</year><article-title>Asymmetrically distributed PAR-3 protein contributes to cell polarity and spindle alignment in early <italic toggle="yes">C. elegans</italic> embryos</article-title>. <source>Cell</source><volume><bold>83</bold></volume>, <fpage>743</fpage>-<lpage>752</lpage>. (<pub-id pub-id-type="doi">10.1016/0092-8674(95)90187-6</pub-id>)<pub-id pub-id-type="pmid">8521491</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C16">
      <label>16<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goehring</surname><given-names>NW</given-names></string-name>, <string-name><surname>Trong</surname><given-names>PK</given-names></string-name>, <string-name><surname>Bois</surname><given-names>JS</given-names></string-name>, <string-name><surname>Chowdhury</surname><given-names>D</given-names></string-name>, <string-name><surname>Nicola</surname><given-names>EM</given-names></string-name>, <string-name><surname>Hyman</surname><given-names>AA</given-names></string-name>, <string-name><surname>Grill</surname><given-names>SW</given-names></string-name></person-group>. <year>2011</year><article-title>Polarization of PAR proteins by advective triggering of a pattern-forming system</article-title>. <source>Science</source><volume><bold>334</bold></volume>, <fpage>1137</fpage>-<lpage>1141</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1208619</pub-id>)<pub-id pub-id-type="pmid">22021673</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C17">
      <label>17<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Münster</surname><given-names>S</given-names></string-name>, <string-name><surname>Jain</surname><given-names>A</given-names></string-name>, <string-name><surname>Mietke</surname><given-names>A</given-names></string-name>, <string-name><surname>Pavlopoulos</surname><given-names>A</given-names></string-name>, <string-name><surname>Grill</surname><given-names>SW</given-names></string-name>, <string-name><surname>Tomancak</surname><given-names>P</given-names></string-name></person-group>. <year>2019</year><article-title>Attachment of the blastoderm to the vitelline envelope affects gastrulation of insects</article-title>. <source>Nature</source><volume><bold>568</bold></volume>, <fpage>395</fpage>-<lpage>399</lpage>. (<pub-id pub-id-type="doi">10.1038/s41586-019-1044-3</pub-id>)<pub-id pub-id-type="pmid">30918398</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C18">
      <label>18<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Voss</surname><given-names>H</given-names></string-name>, <string-name><surname>Bünner</surname><given-names>M</given-names></string-name>, <string-name><surname>Abel</surname><given-names>M</given-names></string-name></person-group>. <year>1998</year><article-title>Identification of continuous, spatiotemporal systems</article-title>. <source>Phys. Rev. E</source><volume><bold>57</bold></volume>, <fpage>2820</fpage>. (<pub-id pub-id-type="doi">10.1103/PhysRevE.57.2820</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C19">
      <label>19<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname><given-names>L</given-names></string-name>, <string-name><surname>Friedman</surname><given-names>JH</given-names></string-name></person-group>. <year>1985</year><article-title>Estimating optimal transformations for multiple regression and correlation</article-title>. <source>J. Am. Stat. Assoc.</source><volume><bold>80</bold></volume>, <fpage>580</fpage>-<lpage>598</lpage>. (<pub-id pub-id-type="doi">10.1080/01621459.1985.10478157</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C20">
      <label>20<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bär</surname><given-names>M</given-names></string-name>, <string-name><surname>Hegger</surname><given-names>R</given-names></string-name>, <string-name><surname>Kantz</surname><given-names>H</given-names></string-name></person-group>. <year>1999</year><article-title>Fitting partial differential equations to space-time dynamics</article-title>. <source>Phys. Rev. E</source><volume><bold>59</bold></volume>, <fpage>337</fpage>. (<pub-id pub-id-type="doi">10.1103/PhysRevE.59.337</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C21">
      <label>21<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xun</surname><given-names>X</given-names></string-name>, <string-name><surname>Cao</surname><given-names>J</given-names></string-name>, <string-name><surname>Mallick</surname><given-names>B</given-names></string-name>, <string-name><surname>Maity</surname><given-names>A</given-names></string-name>, <string-name><surname>Carroll</surname><given-names>RJ</given-names></string-name></person-group>. <year>2013</year><article-title>Parameter estimation of partial differential equation models</article-title>. <source>J. Am. Stat. Assoc.</source><volume><bold>108</bold></volume>, <fpage>1009</fpage>-<lpage>1020</lpage>. (<pub-id pub-id-type="doi">10.1080/01621459.2013.794730</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C22">
      <label>22<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raissi</surname><given-names>M</given-names></string-name>, <string-name><surname>Perdikaris</surname><given-names>P</given-names></string-name>, <string-name><surname>Karniadakis</surname><given-names>GE</given-names></string-name></person-group>. <year>2017</year><article-title>Machine learning of linear differential equations using Gaussian processes</article-title>. <source>J. Comput. Phys.</source><volume><bold>348</bold></volume>, <fpage>683</fpage>-<lpage>693</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2017.07.050</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C23">
      <label>23<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brunton</surname><given-names>SL</given-names></string-name>, <string-name><surname>Proctor</surname><given-names>JL</given-names></string-name>, <string-name><surname>Kutz</surname><given-names>JN</given-names></string-name></person-group>. <year>2016</year><article-title>Discovering governing equations from data by sparse identification of nonlinear dynamical systems</article-title>. <source>Proc. Natl Acad. Sci. USA</source><volume><bold>113</bold></volume>, <fpage>3932</fpage>-<lpage>3937</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.1517384113</pub-id>)<pub-id pub-id-type="pmid">27035946</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C24">
      <label>24<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rudy</surname><given-names>SH</given-names></string-name>, <string-name><surname>Brunton</surname><given-names>SL</given-names></string-name>, <string-name><surname>Proctor</surname><given-names>JL</given-names></string-name>, <string-name><surname>Kutz</surname><given-names>JN</given-names></string-name></person-group>. <year>2017</year><article-title>Data-driven discovery of partial differential equations</article-title>. <source>Sci. Adv.</source><volume><bold>3</bold></volume>, <fpage>e1602614</fpage>. (<pub-id pub-id-type="doi">10.1126/sciadv.1602614</pub-id>)<pub-id pub-id-type="pmid">28508044</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C25">
      <label>25<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schaeffer</surname><given-names>H</given-names></string-name></person-group>. <year>2017</year><article-title>Learning partial differential equations via data discovery and sparse optimization</article-title>. <source>Proc. R. Soc. A</source><volume><bold>473</bold></volume>, <fpage>20160446</fpage>. (<pub-id pub-id-type="doi">10.1098/rspa.2016.0446</pub-id>)<pub-id pub-id-type="pmid">28265183</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C26">
      <label>26<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>S</given-names></string-name>, <string-name><surname>Lin</surname><given-names>G</given-names></string-name></person-group>. <year>2018</year><article-title>Robust data-driven discovery of governing physical laws with error bars</article-title>. <source>Proc. R. Soc. A</source><volume><bold>474</bold></volume>, <fpage>20180305</fpage>. (<pub-id pub-id-type="doi">10.1098/rspa.2018.0305</pub-id>)<pub-id pub-id-type="pmid">30333709</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C27">
      <label>27<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mangan</surname><given-names>NM</given-names></string-name>, <string-name><surname>Askham</surname><given-names>T</given-names></string-name>, <string-name><surname>Brunton</surname><given-names>SL</given-names></string-name>, <string-name><surname>Kutz</surname><given-names>JN</given-names></string-name>, <string-name><surname>Proctor</surname><given-names>JL</given-names></string-name></person-group>. <year>2019</year><article-title>Model selection for hybrid dynamical systems via sparse regression</article-title>. <source>Proc. R. Soc. A</source><volume><bold>475</bold></volume>, <fpage>1</fpage>-<lpage>22</lpage>. (<pub-id pub-id-type="doi">10.1098/rspa.2018.0534</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C28">
      <label>28<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Long</surname><given-names>Z</given-names></string-name>, <string-name><surname>Lu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Ma</surname><given-names>X</given-names></string-name>, <string-name><surname>Dong</surname><given-names>B</given-names></string-name></person-group>. <year>2018</year><comment>PDE-net: learning PDEs from data. In <italic toggle="yes">Int. Conf. on Machine Learning</italic>, <italic toggle="yes">10–15 July</italic>, pp. 3208–3216. PMLR</comment>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C29">
      <label>29<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raissi</surname><given-names>M</given-names></string-name>, <string-name><surname>Karniadakis</surname><given-names>GE</given-names></string-name></person-group>. <year>2018</year><article-title>Hidden physics models: machine learning of nonlinear partial differential equations</article-title>. <source>J. Comput. Phys.</source><volume><bold>357</bold></volume>, <fpage>125</fpage>-<lpage>141</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2017.11.039</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C30">
      <label>30<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raissi</surname><given-names>M</given-names></string-name>, <string-name><surname>Perdikaris</surname><given-names>P</given-names></string-name>, <string-name><surname>Karniadakis</surname><given-names>GE</given-names></string-name></person-group>. <year>2019</year><article-title>Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</article-title>. <source>J. Comput. Phys.</source><volume><bold>378</bold></volume>, <fpage>686</fpage>-<lpage>707</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2018.10.045</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C31">
      <label>31<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Long</surname><given-names>Z</given-names></string-name>, <string-name><surname>Lu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Dong</surname><given-names>B</given-names></string-name></person-group>. <year>2019</year><article-title>PDE-Net 2.0: learning PDEs from data with a numeric-symbolic hybrid deep network</article-title>. <source>J. Comput. Phys.</source><volume><bold>399</bold></volume>, <fpage>108925</fpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2019.108925</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C32">
      <label>32<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dong</surname><given-names>B</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>Q</given-names></string-name>, <string-name><surname>Shen</surname><given-names>Z</given-names></string-name></person-group>. <year>2017</year><article-title>Image restoration: wavelet frame shrinkage, nonlinear evolution PDEs, and beyond</article-title>. <source>Multiscale Model. Simul.</source><volume><bold>15</bold></volume>, <fpage>606</fpage>-<lpage>660</lpage>. (<pub-id pub-id-type="doi">10.1137/15M1037457</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C33">
      <label>33<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meinshausen</surname><given-names>N</given-names></string-name>, <string-name><surname>Bühlmann</surname><given-names>P</given-names></string-name></person-group>. <year>2010</year><article-title>Stability selection</article-title>. <source>J. R. Stat. Soc.: Ser. B (Stat. Methodol.)</source><volume><bold>72</bold></volume>, <fpage>417</fpage>-<lpage>473</lpage>. (<pub-id pub-id-type="doi">10.1111/j.1467-9868.2010.00740.x</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C34">
      <label>34<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shah</surname><given-names>RD</given-names></string-name>, <string-name><surname>Samworth</surname><given-names>RJ</given-names></string-name></person-group>. <year>2013</year><article-title>Variable selection with error control: another look at stability selection</article-title>. <source>J. R. Stat. Soc.: Ser. B (Stat. Methodol.)</source><volume><bold>75</bold></volume>, <fpage>55</fpage>-<lpage>80</lpage>. (<pub-id pub-id-type="doi">10.1111/j.1467-9868.2011.01034.x</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C35">
      <label>35<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tibshirani</surname><given-names>R</given-names></string-name></person-group>. <year>1996</year><article-title>Regression shrinkage and selection via the LASSO</article-title>. <source>J. R. Stat. Soc. Ser. B (Methodological)</source><volume><bold>58</bold></volume>, <fpage>267</fpage>-<lpage>288</lpage>. (<pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C36">
      <label>36<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blumensath</surname><given-names>T</given-names></string-name>, <string-name><surname>Davies</surname><given-names>ME</given-names></string-name></person-group>. <year>2008</year><article-title>Iterative thresholding for sparse approximations</article-title>. <source>J. Fourier Anal. Appl.</source><volume><bold>14</bold></volume>, <fpage>629</fpage>-<lpage>654</lpage>. (<pub-id pub-id-type="doi">10.1007/s00041-008-9035-z</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C37">
      <label>37<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Foucart</surname><given-names>S</given-names></string-name></person-group>. <year>2011</year><article-title>Hard thresholding pursuit: an algorithm for compressive sensing</article-title>. <source>SIAM J. Numer. Anal.</source><volume><bold>49</bold></volume>, <fpage>2543</fpage>-<lpage>2563</lpage>. (<pub-id pub-id-type="doi">10.1137/100806278</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C38">
      <label>38<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chartrand</surname><given-names>R</given-names></string-name></person-group>. <year>2011</year><article-title>Numerical differentiation of noisy, nonsmooth data</article-title>. <source>ISRN Appl. Math.</source><volume><bold>2011</bold></volume>, <fpage>1</fpage>-<lpage>11</lpage>. (<pub-id pub-id-type="doi">10.5402/2011/164564</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C39">
      <label>39<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stickel</surname><given-names>JJ</given-names></string-name></person-group>. <year>2010</year><article-title>Data smoothing and numerical differentiation by a regularization method</article-title>. <source>Comput. Chem. Eng.</source><volume><bold>34</bold></volume>, <fpage>467</fpage>-<lpage>475</lpage>. (<pub-id pub-id-type="doi">10.1016/j.compchemeng.2009.10.007</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C40">
      <label>40<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>TT</given-names></string-name>, <string-name><surname>Lange</surname><given-names>K</given-names></string-name></person-group>. <year>2008</year><article-title>Coordinate descent algorithms for LASSO penalized regression</article-title>. <source>Ann. Appl. Stat.</source><volume><bold>2</bold></volume>, <fpage>224</fpage>-<lpage>244</lpage>. (<pub-id pub-id-type="doi">10.1214/07-aoas147</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C41">
      <label>41<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J</given-names></string-name>, <string-name><surname>Hastie</surname><given-names>T</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>R</given-names></string-name></person-group>. <year>2010</year><article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>J. Stat. Softw.</source><volume><bold>33</bold></volume>, <fpage>1</fpage>. (<pub-id pub-id-type="doi">10.18637/jss.v033.i01</pub-id>)<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C42">
      <label>42<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eckstein</surname><given-names>J</given-names></string-name>, <string-name><surname>Bertsekas</surname><given-names>DP</given-names></string-name></person-group>. <year>1992</year><article-title>On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators</article-title>. <source>Math. Program.</source><volume><bold>55</bold></volume>, <fpage>293</fpage>-<lpage>318</lpage>. (<pub-id pub-id-type="doi">10.1007/BF01581204</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C43">
      <label>43<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Combettes</surname><given-names>PL</given-names></string-name>, <string-name><surname>Pesquet</surname><given-names>JC</given-names></string-name></person-group>. <year>2011</year><comment>Proximal splitting methods in signal processing. In <italic toggle="yes">Fixed-point algorithms for inverse problems in science and engineering</italic>, pp. 185–212. New York, NY: Springer</comment>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C44">
      <label>44<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beck</surname><given-names>A</given-names></string-name>, <string-name><surname>Teboulle</surname><given-names>M</given-names></string-name></person-group>. <year>2009</year><article-title>A fast iterative Shrinkage-Thresholding algorithm</article-title>. <source>Soc. Ind. Appl. Math. J. Imag. Sci.</source><volume><bold>2</bold></volume>, <fpage>183</fpage>-<lpage>202</lpage>. (<pub-id pub-id-type="doi">10.1137/080716542</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C45">
      <label>45<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Meinshausen</surname><given-names>N</given-names></string-name>, <string-name><surname>Bühlmann</surname><given-names>P</given-names></string-name></person-group>. <year>2006</year><article-title>High-dimensional graphs and variable selection with the LASSO</article-title>. <source>Ann. Stat.</source><volume><bold>34</bold></volume>, <fpage>1436</fpage>-<lpage>1462</lpage>. (<pub-id pub-id-type="doi">10.1214/009053606000000281</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C46">
      <label>46<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname><given-names>P</given-names></string-name>, <string-name><surname>Yu</surname><given-names>B</given-names></string-name></person-group>. <year>2006</year><article-title>On model selection consistency of LASSO</article-title>. <source>J. Mach. Learn. Res.</source><volume><bold>7</bold></volume>, <fpage>2541</fpage>-<lpage>2563</lpage>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C47">
      <label>47<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fan</surname><given-names>J</given-names></string-name>, <string-name><surname>Li</surname><given-names>R</given-names></string-name></person-group>. <year>2001</year><article-title>Variable selection via nonconcave penalized likelihood and its oracle properties</article-title>. <source>J. Am. Stat. Assoc.</source><volume><bold>96</bold></volume>, <fpage>1348</fpage>-<lpage>1360</lpage>. (<pub-id pub-id-type="doi">10.1198/016214501753382273</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C48">
      <label>48<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>CH</given-names></string-name></person-group>. <year>2010</year><article-title>Nearly unbiased variable selection under minimax concave penalty</article-title>. <source>Ann. Stat.</source><volume><bold>38</bold></volume>, <fpage>894</fpage>-<lpage>942</lpage>. (<pub-id pub-id-type="doi">10.1214/09-AOS729</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C49">
      <label>49<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tropp</surname><given-names>JA</given-names></string-name></person-group>. <year>2004</year><article-title>Greed is good: algorithmic results for sparse approximation</article-title>. <source>IEEE Trans. Inf. Theory</source><volume><bold>50</bold></volume>, <fpage>2231</fpage>-<lpage>2242</lpage>. (<pub-id pub-id-type="doi">10.1109/TIT.2004.834793</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C50">
      <label>50<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mallat</surname><given-names>SG</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z</given-names></string-name></person-group>. <year>1993</year><article-title>Matching pursuits with time-frequency dictionaries</article-title>. <source>IEEE Trans. Signal Process.</source><volume><bold>41</bold></volume>, <fpage>3397</fpage>-<lpage>3415</lpage>. (<pub-id pub-id-type="doi">10.1109/78.258082</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C51">
      <label>51<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Needell</surname><given-names>D</given-names></string-name>, <string-name><surname>Tropp</surname><given-names>JA</given-names></string-name></person-group>. <year>2009</year><article-title>CoSaMP: iterative signal recovery from incomplete and inaccurate samples</article-title>. <source>Appl. Comput. Harmon. Anal.</source><volume><bold>26</bold></volume>, <fpage>301</fpage>-<lpage>321</lpage>. (<pub-id pub-id-type="doi">10.1016/j.acha.2008.07.002</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C52">
      <label>52<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dai</surname><given-names>W</given-names></string-name>, <string-name><surname>Milenkovic</surname><given-names>O</given-names></string-name></person-group>. <year>2009</year><article-title>Subspace pursuit for compressive sensing signal reconstruction</article-title>. <source>IEEE Trans. Inf. Theory</source><volume><bold>55</bold></volume>, <fpage>2230</fpage>-<lpage>2249</lpage>. (<pub-id pub-id-type="doi">10.1109/TIT.2009.2016006</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C53">
      <label>53<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blumensath</surname><given-names>T</given-names></string-name>, <string-name><surname>Davies</surname><given-names>ME</given-names></string-name></person-group>. <year>2009</year><article-title>Iterative hard thresholding for compressed sensing</article-title>. <source>Appl. Comput. Harmon. Anal.</source><volume><bold>27</bold></volume>, <fpage>265</fpage>-<lpage>274</lpage>. (<pub-id pub-id-type="doi">10.1016/j.acha.2009.04.002</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C54">
      <label>54<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Herrity</surname><given-names>KK</given-names></string-name>, <string-name><surname>Gilbert</surname><given-names>AC</given-names></string-name>, <string-name><surname>Tropp</surname><given-names>JA</given-names></string-name></person-group>. <year>2006</year><comment>Sparse approximation via iterative thresholding. In <italic toggle="yes">2006 IEEE Int. Conf. on Acoustics Speech and Signal Processing Proc.</italic>, <italic toggle="yes">14–19 May</italic>, vol. 3, pp. III–III. IEEE</comment>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C55">
      <label>55<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tropp</surname><given-names>JA</given-names></string-name></person-group>. <year>2006</year><article-title>Just relax: convex programming methods for identifying sparse signals in noise</article-title>. <source>IEEE Trans. Inf. Theory</source><volume><bold>52</bold></volume>, <fpage>1030</fpage>-<lpage>1051</lpage>. (<pub-id pub-id-type="doi">10.1109/TIT.2005.864420</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C56">
      <label>56<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Figueiredo</surname><given-names>MA</given-names></string-name>, <string-name><surname>Nowak</surname><given-names>RD</given-names></string-name>, <string-name><surname>Wright</surname><given-names>SJ</given-names></string-name></person-group>. <year>2007</year><article-title>Gradient projection for sparse reconstruction: application to compressed sensing and other inverse problems</article-title>. <source>IEEE J. Sel. Top. Signal Process.</source><volume><bold>1</bold></volume>, <fpage>586</fpage>-<lpage>597</lpage>. (<pub-id pub-id-type="doi">10.1109/JSTSP.2007.910281</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C57">
      <label>57<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Kohavi</surname><given-names>R</given-names></string-name></person-group>. <year>1995</year><comment>A study of cross-validation and bootstrap for accuracy estimation and model selection. In <italic toggle="yes">Int. Joint Conf. of Artificial Intelligence</italic>, vol. 14, pp. 1137–1145</comment>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C58">
      <label>58<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwarz</surname><given-names>G</given-names></string-name></person-group>. <year>1978</year><article-title>Estimating the dimension of a model</article-title>. <source>Ann. Stat.</source><volume><bold>6</bold></volume>, <fpage>461</fpage>-<lpage>464</lpage>. (<pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C59">
      <label>59<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Lederer</surname><given-names>J</given-names></string-name>, <string-name><surname>Müller</surname><given-names>CL</given-names></string-name></person-group>. <year>2015</year><comment>Don’t fall for tuning parameters: Tuning-free variable selection in high dimensions with the TREX. In <italic toggle="yes">Proc. of the 29th AAAI Conf. on Artificial Intelligence (AAAI 2015)</italic>, <italic toggle="yes">Austin, TX</italic>, <italic toggle="yes">25–30 January</italic>, pp. 2729–2735. AAAI Press</comment>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C60">
      <label>60<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bien</surname><given-names>J</given-names></string-name>, <string-name><surname>Gaynanova</surname><given-names>I</given-names></string-name>, <string-name><surname>Lederer</surname><given-names>J</given-names></string-name>, <string-name><surname>Müller</surname><given-names>CL</given-names></string-name></person-group>. <year>2018</year><article-title>Non-convex global minimization and false discovery rate control for the TREX</article-title>. <source>J. Comput. Graph. Stat.</source><volume><bold>27</bold></volume>, <fpage>23</fpage>-<lpage>33</lpage>. (<pub-id pub-id-type="doi">10.1080/10618600.2017.1341414</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C61">
      <label>61<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>B</given-names></string-name></person-group>. <year>2013</year><article-title>Stability</article-title>. <source>Bernoulli</source><volume><bold>19</bold></volume>, <fpage>1484</fpage>-<lpage>1500</lpage>. (<pub-id pub-id-type="doi">10.3150/13-BEJSP14</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C62">
      <label>62<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>H</given-names></string-name>, <string-name><surname>Roeder</surname><given-names>K</given-names></string-name>, <string-name><surname>Wasserman</surname><given-names>L</given-names></string-name></person-group>. <year>2010</year><article-title>Stability approach to regularization selection (StARS) for high dimensional graphical models</article-title>. <source>Adv. Neural Inf. Process. Syst.</source><volume><bold>24</bold></volume>, <fpage>1432</fpage>-<lpage>1440</lpage>.<pub-id pub-id-type="pmid">25152607</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C63">
      <label>63<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Haury</surname><given-names>AC</given-names></string-name>, <string-name><surname>Mordelet</surname><given-names>F</given-names></string-name>, <string-name><surname>Vera-Licona</surname><given-names>P</given-names></string-name>, <string-name><surname>Vert</surname><given-names>JP</given-names></string-name></person-group>. <year>2012</year><article-title>TIGRESS: trustful inference of gene regulation using stability selection</article-title>. <source>BMC Syst. Biol.</source><volume><bold>6</bold></volume>, <fpage>145</fpage>. (<pub-id pub-id-type="doi">10.1186/1752-0509-6-145</pub-id>)<pub-id pub-id-type="pmid">23173819</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C64">
      <label>64<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bühlmann</surname><given-names>P</given-names></string-name>, <string-name><surname>Kalisch</surname><given-names>M</given-names></string-name>, <string-name><surname>Meier</surname><given-names>L</given-names></string-name></person-group>. <year>2014</year><article-title>High-dimensional statistics with a view toward applications in biology</article-title>. <source>Annu. Rev. Stat. Appl.</source><volume><bold>1</bold></volume>, <fpage>255</fpage>-<lpage>278</lpage>. (<pub-id pub-id-type="doi">10.1146/annurev-statistics-022513-115545</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C65">
      <label>65<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hothorn</surname><given-names>T</given-names></string-name>, <string-name><surname>Müller</surname><given-names>J</given-names></string-name>, <string-name><surname>Schröder</surname><given-names>B</given-names></string-name>, <string-name><surname>Kneib</surname><given-names>T</given-names></string-name>, <string-name><surname>Brandl</surname><given-names>R</given-names></string-name></person-group>. <year>2011</year><article-title>Decomposing environmental, spatial, and spatiotemporal components of species distributions</article-title>. <source>Ecol. Monogr.</source><volume><bold>81</bold></volume>, <fpage>329</fpage>-<lpage>347</lpage>. (<pub-id pub-id-type="doi">10.1890/10-0602.1</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C66">
      <label>66<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gross</surname><given-names>P</given-names></string-name>, <string-name><surname>Kumar</surname><given-names>KV</given-names></string-name>, <string-name><surname>Goehring</surname><given-names>NW</given-names></string-name>, <string-name><surname>Bois</surname><given-names>JS</given-names></string-name>, <string-name><surname>Hoege</surname><given-names>C</given-names></string-name>, <string-name><surname>Jülicher</surname><given-names>F</given-names></string-name>, <string-name><surname>Grill</surname><given-names>SW</given-names></string-name></person-group>. <year>2019</year><article-title>Guiding self-organized pattern formation in cell polarity establishment</article-title>. <source>Nat. Phys.</source><volume><bold>15</bold></volume>, <fpage>293</fpage>. (<pub-id pub-id-type="doi">10.1038/s41567-018-0358-7</pub-id>)<pub-id pub-id-type="pmid">31327978</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C67">
      <label>67<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Pati</surname><given-names>YC</given-names></string-name>, <string-name><surname>Rezaiifar</surname><given-names>R</given-names></string-name>, <string-name><surname>Krishnaprasad</surname><given-names>PS</given-names></string-name></person-group>. <year>1993</year><comment>Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In <italic toggle="yes">Proc. of 27th Asilomar Conf. on Signals, Systems and Computers</italic>, <italic toggle="yes">1–3 November</italic>, pp. 40–44. IEEE</comment>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C68">
      <label>68<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tropp</surname><given-names>JA</given-names></string-name>, <string-name><surname>Gilbert</surname><given-names>AC</given-names></string-name></person-group>. <year>2007</year><article-title>Signal recovery from random measurements via orthogonal matching pursuit</article-title>. <source>IEEE Trans. Inf. Theory</source><volume><bold>53</bold></volume>, <fpage>4655</fpage>-<lpage>4666</lpage>. (<pub-id pub-id-type="doi">10.1109/TIT.2007.909108</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C69">
      <label>69<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>Nan</surname><given-names>B</given-names></string-name>, <string-name><surname>Rosset</surname><given-names>S</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>J</given-names></string-name></person-group>. <year>2011</year><article-title>Random LASSO</article-title>. <source>Ann. Appl. Stat.</source><volume><bold>5</bold></volume>, <fpage>468</fpage>.<pub-id pub-id-type="pmid">22997542</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C70">
      <label>70<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Temam</surname><given-names>R</given-names></string-name></person-group>. <year>2001</year><source>Navier-Stokes equations: theory and numerical analysis</source>, vol. 343. Providence, RI: <publisher-name>American Mathematical Society</publisher-name>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C71">
      <label>71<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Incardona</surname><given-names>P</given-names></string-name>, <string-name><surname>Leo</surname><given-names>A</given-names></string-name>, <string-name><surname>Zaluzhnyi</surname><given-names>Y</given-names></string-name>, <string-name><surname>Ramaswamy</surname><given-names>R</given-names></string-name>, <string-name><surname>Sbalzarini</surname><given-names>IF</given-names></string-name></person-group>. <year>2019</year><article-title>OpenFPM: a scalable open framework for particle and particle-mesh codes on parallel computers</article-title>. <source>Comput. Phys. Commun.</source><volume><bold>241</bold></volume>, <fpage>155</fpage>-<lpage>177</lpage>. (<pub-id pub-id-type="doi">10.1016/j.cpc.2019.03.007</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C72">
      <label>72<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Meinhardt</surname><given-names>H</given-names></string-name></person-group>. <year>1982</year><comment>Models of biological pattern formation. New York</comment>, NY: Academic Press.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C73">
      <label>73<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manukyan</surname><given-names>L</given-names></string-name>, <string-name><surname>Montandon</surname><given-names>SA</given-names></string-name>, <string-name><surname>Fofonjka</surname><given-names>A</given-names></string-name>, <string-name><surname>Smirnov</surname><given-names>S</given-names></string-name>, <string-name><surname>Milinkovitch</surname><given-names>MC</given-names></string-name></person-group>. <year>2017</year><article-title>A living mesoscopic cellular automaton made of skin scales</article-title>. <source>Nature</source><volume><bold>544</bold></volume>, <fpage>173</fpage>. (<pub-id pub-id-type="doi">10.1038/nature22031</pub-id>)<pub-id pub-id-type="pmid">28406206</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C74">
      <label>74<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wainwright</surname><given-names>MJ</given-names></string-name></person-group>. <year>2009</year><article-title>Sharp thresholds for high-dimensional and noisy sparsity recovery using <inline-formula><mml:math id="IM383"><mml:msub><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>- constrained quadratic programming (LASSO)</article-title>. <source>IEEE Trans. Inf. Theory</source><volume><bold>55</bold></volume>, <fpage>2183</fpage>-<lpage>2202</lpage>. (<pub-id pub-id-type="doi">10.1109/TIT.2009.2016018</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C75">
      <label>75<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ward</surname><suffix>Jr</suffix><given-names>JH</given-names></string-name></person-group>. <year>1963</year><article-title>Hierarchical grouping to optimize an objective function</article-title>. <source>J. Am. Stat. Assoc.</source><volume><bold>58</bold></volume>, <fpage>236</fpage>-<lpage>244</lpage>. (<pub-id pub-id-type="doi">10.1080/01621459.1963.10500845</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C76">
      <label>76<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rudy</surname><given-names>SH</given-names></string-name>, <string-name><surname>Kutz</surname><given-names>JN</given-names></string-name>, <string-name><surname>Brunton</surname><given-names>SL</given-names></string-name></person-group>. <year>2019</year><article-title>Deep learning of dynamics and signal-noise decomposition with time-stepping constraints</article-title>. <source>J. Comput. Phys.</source><volume><bold>396</bold></volume>, <fpage>483</fpage>-<lpage>506</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jcp.2019.06.056</pub-id>)</mixed-citation>
    </ref>
    <ref id="RSPA20210916C77">
      <label>77<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Maddu</surname><given-names>S</given-names></string-name>, <string-name><surname>Sturm</surname><given-names>D</given-names></string-name>, <string-name><surname>Cheeseman</surname><given-names>BL</given-names></string-name>, <string-name><surname>Müller</surname><given-names>CL</given-names></string-name>, <string-name><surname>Sbalzarini</surname><given-names>IF</given-names></string-name></person-group>. <year>2021</year><comment>STENCIL-NET: data-driven solution-adaptive discretization of partial differential equations. (<uri xlink:href="http://arxiv.org/abs/2101.06182">http://arxiv.org/abs/2101.06182</uri>)</comment></mixed-citation>
    </ref>
    <ref id="RSPA20210916C78">
      <label>78<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="std"><person-group person-group-type="author"><string-name><surname>Kutz</surname><given-names>JN</given-names></string-name>, <string-name><surname>Rudy</surname><given-names>SH</given-names></string-name>, <string-name><surname>Alla</surname><given-names>A</given-names></string-name>, <string-name><surname>Brunton</surname><given-names>SL</given-names></string-name></person-group>. <year>2017</year><comment>Data-Driven discovery of governing physical laws and their parametric dependencies in engineering, physics and biology. In <italic toggle="yes">2017 IEEE 7th Int. Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)</italic>, <italic toggle="yes">10–13 December</italic>, pp. 1–5. IEEE</comment>.</mixed-citation>
    </ref>
    <ref id="RSPA20210916C79">
      <label>79<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maddu</surname><given-names>S</given-names></string-name>, <string-name><surname>Cheeseman</surname><given-names>BL</given-names></string-name>, <string-name><surname>Müller</surname><given-names>CL</given-names></string-name>, <string-name><surname>Sbalzarini</surname><given-names>IF</given-names></string-name></person-group>. <year>2021</year><article-title>Learning physically consistent differential equation models from data using group sparsity</article-title>. <source>Phys. Rev. E</source><volume><bold>103</bold></volume>, <fpage>042310</fpage>. (<pub-id pub-id-type="doi">10.1103/PhysRevE.103.042310</pub-id>)<pub-id pub-id-type="pmid">34005966</pub-id></mixed-citation>
    </ref>
    <ref id="RSPA20210916C80">
      <label>80<x xml:space="preserve">. </x></label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Maddu</surname><given-names>S</given-names></string-name>, <string-name><surname>Cheeseman</surname><given-names>BL</given-names></string-name>, <string-name><surname>Sbalzarini</surname><given-names>IF</given-names></string-name>, <string-name><surname>Müller</surname><given-names>CL</given-names></string-name></person-group>. <year>2022</year><data-title>Stability selection enables robust learning of differential equations from limited noisy data</data-title>. <source>Figshare</source>. <comment>(</comment><pub-id pub-id-type="doi">10.6084/m9.figshare.c.6016866</pub-id><comment>)</comment></mixed-citation>
    </ref>
  </ref-list>
</back>
