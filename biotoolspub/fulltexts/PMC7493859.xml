<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7493859</article-id>
    <article-id pub-id-type="publisher-id">3627</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-03627-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CORENup: a combination of convolutional and recurrent deep neural networks for nucleosome positioning identification</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Amato</surname>
          <given-names>Domenico</given-names>
        </name>
        <address>
          <email>domenico.amato01@unipa.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1602-0693</contrib-id>
        <name>
          <surname>Bosco</surname>
          <given-names>Giosue’ Lo</given-names>
        </name>
        <address>
          <email>giosue.lobosco@unipa.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rizzo</surname>
          <given-names>Riccardo</given-names>
        </name>
        <address>
          <email>riccardo.rizzo@icar.cnr.it</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.10776.37</institution-id><institution-id institution-id-type="ISNI">0000 0004 1762 5517</institution-id><institution>Dipartimento di Matematica e Informatica, Università degli studi di Palermo, </institution></institution-wrap>Via Archirafi, 34, Palermo, 90123 Italy </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.428936.2</institution-id><institution>Dipartimento di Scienze per l’Innovazione tecnologica, Istituto Euro-Mediterraneo di Scienza e Tecnologia, </institution></institution-wrap>Via Michele Miraglia, 20, Palermo, 9039 Italy </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.5326.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 1940 4177</institution-id><institution>CNR-ICAR, National Research Council of Italy, </institution></institution-wrap>Via Ugo La Malfa, 153, Palermo, 90146 Italy </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>9</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>9</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <issue>Suppl 8</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>326</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>6</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Nucleosomes wrap the DNA into the nucleus of the Eukaryote cell and regulate its transcription phase. Several studies indicate that nucleosomes are determined by the combined effects of several factors, including DNA sequence organization. Interestingly, the identification of nucleosomes on a genomic scale has been successfully performed by computational methods using DNA sequence as input data.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this work, we propose CORENup, a deep learning model for nucleosome identification. CORENup processes a DNA sequence as input using one-hot representation and combines in a parallel fashion a fully convolutional neural network and a recurrent layer. These two parallel levels are devoted to catching both <italic>non periodic</italic> and <italic>periodic</italic> DNA string features. A dense layer is devoted to their combination to give a final classification.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">Results computed on public data sets of different organisms show that CORENup is a state of the art methodology for nucleosome positioning identification based on a Deep Neural Network architecture. The comparisons have been carried out using two groups of datasets, currently adopted by the best performing methods, and CORENup has shown top performance both in terms of classification metrics and elapsed computation time.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Nucleosome classification</kwd>
      <kwd>Epigenetic</kwd>
      <kwd>Deep learning networks</kwd>
      <kwd>Recurrent neural networks</kwd>
    </kwd-group>
    <conference xlink:href="http://bioinformatics.it/">
      <conf-name>Annual Meeting of the Bioinformatics Italian Society (BITS 2019)</conf-name>
      <conf-acronym>BITS 2019</conf-acronym>
      <conf-loc>Palermo, Italy</conf-loc>
      <conf-date>26-28 June 2019</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>The eukaryote genome is packed as chromatin [<xref ref-type="bibr" rid="CR1">1</xref>], the fundamental unit of packaging is called nucleosome, and it consists of a histone octamer where about 147 bp of DNA is wrapped. Nucleosomes are separated from each other by sequences of DNA called linker DNA. Starting from this low-level organization, chromatin is coiled into many higher-order structures to finally form the chromosomes. Nucleosome positioning indicates the physical packaging of DNA driving the determination of the final architecture of chromatin in the cell [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>] both trough the DNA sequence itself and the interaction of other factors, including remodelling proteins [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref>], histone acetylation [<xref ref-type="bibr" rid="CR7">7</xref>] and others [<xref ref-type="bibr" rid="CR8">8</xref>]. The chromatin architecture of eukaryotic gene promoters is generally characterized by a nucleosome-free region, where nucleosomes frequently occupy specific positions. For this reason, nucleosomes affect gene regulation shaping the accessibility of transcription factors to occupy their binding sites [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>].</p>
    <p>Furthermore, nucleosomes influence also the accessibility of different regulative element to DNA, that are critical for other biological processes such as replication [<xref ref-type="bibr" rid="CR11">11</xref>] and recombination [<xref ref-type="bibr" rid="CR12">12</xref>]. For these reasons, understanding the structure and function of nucleosomes is of great interest in biology.</p>
    <p>The role of DNA sequence in causing nucleosome positions is clear from in vitro studies. Different DNA sequences show different affinities for the histone core. Early studies showed that many nucleosomal related sequences contain quasiperiodic nucleotide distributions [<xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR15">15</xref>] A comparison of nucleosome sequence maps in vivo and reconstituted in vitro exposes that the relative occupancies of each position are not the same [<xref ref-type="bibr" rid="CR16">16</xref>]. This in part indicates that nucleosomes in vitro are not regularly spaced, unlike nucleosomes in vivo. These observations led to the conclusion that genomic DNA may encode nucleosome positions [<xref ref-type="bibr" rid="CR17">17</xref>] opening the possibility to study combinatorial properties of DNA string related to nucleosome preference [<xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR20">20</xref>]. Recent studies have posed a limit to this deduction, the DNA sequence is for sure an important determinant in nucleosome positioning, but additional factors are needed to determine long-range chromatin organization [<xref ref-type="bibr" rid="CR21">21</xref>]. The recent MNae-seq experimental approach in nucleosome mapping has provided to the communities several high-resolution nucleosome maps. In 2019 these data have been systematically collected into a database, named NucMap [<xref ref-type="bibr" rid="CR22">22</xref>]. NucMap is an online database which includes 798 experimental data from 477 samples across 15 species, also supplying a set of very useful tools to visualize and compare the data. These high-resolution data leads to the development of many computational methodologies able to successfully process sequence information to predict the nucleosome presence[<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR26">26</xref>]. Taking into consideration these biological studies, and differently from other studies proposed so far, this work intends to try to understand at which extent the DNA sequence is solely responsible for nucleosome positioning. This investigation is carried out by a machine learning model, in particular a deep neural network, which processes only sequence information. In the past years, we gave several contributions to the study of deep learning networks for sequence classification [<xref ref-type="bibr" rid="CR26">26</xref>–<xref ref-type="bibr" rid="CR30">30</xref>]. In this work we present <italic>CORENup</italic>, an extension of our previous models by the integration of two different neural networks, each specialized in extracting specific features from sequences, i.e. <italic>non periodic</italic> and <italic>periodic</italic> features. For sure the automatic identification of nucleosome positions seems to have attracted several machine learning researchers, and very effective models have been proposed so far [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]. Among the most performing ones, we have to mention iNuc-PseKNC [<xref ref-type="bibr" rid="CR23">23</xref>]. It uses a Support Vector Machine with a radial basis function kernel and a novel feature-vector that incorporates six DNA local structural properties. Cross-validation tests on the three benchmark datasets have shown accuracy rates greater than 79%. The work posed a baseline for the machine learning methods, also providing three benchmark dataset very useful for the comparisons. The main issue of the method is that it needs a feature extraction phase for the sequence processing, i.e. the representation for the input data involve a specific preprocessing stage. Conversely, other authors have used the simple <italic>one hot representation</italic> obtaining greater accuracy, using deep neural network classifiers [<xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. Actually the model called <italic>Le-Nup</italic> [<xref ref-type="bibr" rid="CR25">25</xref>] is the top performer. In this work, we will compare CORENup and LeNup in terms of performance metrics, the complexity of the models, and elapsed computation times.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p>Machine learning systems need a mathematical representation of the input objects. This representation is usually obtained defining some quantitative features of the objects and reporting the corresponding measurement results in a representing vector. The choices made during the development of these representations can affect the performance of the whole system. Deep learning techniques can develop a representation of the input data without human guidance, this is important in many classification problems, for example in image processing where it is difficult to describe which features are suitable for a precise task. This property is useful in sequence classification, where it is very hard to spot useful features, and this is one of the reasons why deep learning networks are used.</p>
    <p>In the following sections, after the introduction of the sequence representation, we report the motivation of the CORENup architecture, compared with another deep learning neural network used for sequence classification.</p>
    <sec id="Sec3">
      <title>Sequence representation</title>
      <p>Machine learning algorithms process <italic>tensor data</italic>, so that DNA sequences should be converted to numerical representations. Fasta files containing DNA sequences are constituted by a set of strings from a finite alphabet <italic>Σ</italic>. This alphabet is restricted to four symbols, e.g. <italic>Σ</italic>={<italic>A</italic>,<italic>C</italic>,<italic>G</italic>,<italic>T</italic>}, corresponding to the 4 bases adenine, cytosine, guanine, thymine, if there is no uncertainty on base value for a specified sequence position, otherwise two or more alternative base values for a single position can be represented using the IUPAC notation, where, for example, the symbol W in a position stands for A or T.</p>
      <p>In this work the used sequences are from a 4-letters alphabet and sequence representation is the <italic>one-hot encoding</italic>, that transforms a sequence of length <italic>L</italic> into a matrix of dimension (4,<italic>L</italic>). A sketch of the one-hot representation is in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Matrix rows correspond to symbols in the alphabet, while columns indicate the positions in the sequence where the symbols are present. The matrix is binary, i.e. each column <italic>j</italic> have all zero values, but one in the row of the corresponding symbol.
<fig id="Fig1"><label>Fig. 1</label><caption><p>The one-hot representation A simple visualization of the build process for an one-hot representation</p></caption><graphic xlink:href="12859_2020_3627_Fig1_HTML" id="MO1"/></fig></p>
      <p>The one-hot representation is a sparse binary representation, suitable only for datasets made of fixed-length small sequences. The main advantage of this sequence representation is that the context of each position, i.e. the sequence of symbols, is preserved and this property will be exploited in the following.</p>
    </sec>
    <sec id="Sec4">
      <title>CORENup neural network model</title>
      <p>In nucleosome-linker classification one of the most recent and effective networks is the LeNup network [<xref ref-type="bibr" rid="CR25">25</xref>]. This network, as many deep learning systems, has a structure inspired by the Google Inception network [<xref ref-type="bibr" rid="CR33">33</xref>]. These systems are based on a cascade of cells made by convolutional layers in parallel. These convolutional layers have many kernels of dimension 3 or 5, that process the signal in parallel. The basic idea is that in sequence recognition problems it is necessary to look at sequence features obtained at different scales, and then integrate these features so that the next stage can extract more abstract information. LeNup and Inception networks use this approach to obtain a multi-level multi-scale representation of the input, and after many processing cells, these networks use this representation as input for the fully connected final layers. In these networks the number of parameters (weights of the neural network) can grow very quickly, in fact, the LeNup network has more than two millions of parameters.</p>
      <p>The integration of many features from layer to layer is the basic idea that also inspired this work, but we wanted to combine different sets of features coming from many sources. In the past, we investigated the use of convolutional and recurrent neural networks for sequence classification [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. We found that the two methods build two different sets of features from the sequence and that these features can be integrated to build a better classifier.</p>
      <p>In the following subsections, the mechanism of convolutional and recurrent (LSTM) networks are introduced, and the combination of the two in the CORENup network is explained.</p>
      <sec id="Sec5">
        <title>Convolutional neural networks</title>
        <p>In a convolutional layer each neuron has a receptive field that scans the input and during this scan builds the layer output. Assuming that the layer has <italic>N</italic> neurons, the receptive field of neuron <italic>i</italic> with <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$i= 1, 2, \dots N$\end{document}</tex-math><mml:math id="M2"><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>N</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3627_Article_IEq1.gif"/></alternatives></inline-formula> has a set of weights <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$w^{i}_{u}$\end{document}</tex-math><mml:math id="M4"><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3627_Article_IEq2.gif"/></alternatives></inline-formula>, with <italic>u</italic>=−<italic>n</italic>,⋯,<italic>n</italic>, where 2<italic>n</italic>+1 is the width of the receptive field of the neuron, and scans an input vector <bold>x</bold>∈ℜ<sup><italic>d</italic></sup>; given that the non-linearity of the neuron is a generic function <italic>ϕ</italic>, the output of the neuron <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$y^{i}_{k}$\end{document}</tex-math><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3627_Article_IEq3.gif"/></alternatives></inline-formula>, associated to the component <italic>k</italic> of the input vector <bold>x</bold>∈ℜ<sup><italic>d</italic></sup> is:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ y^{i}_{k} = \phi \left(\sum_{u=-n}^{n} w^{i}_{u} * x_{k-u} \right) \\ i= 1,2,\cdots N \\ k = n, n+1, \cdots, d-n  $$ \end{document}</tex-math><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mtext mathvariant="italic">Nk</mml:mtext><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mspace width="0.3em"/><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:math><graphic xlink:href="12859_2020_3627_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>In the architecture in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the first convolutional layer extracts from the sequences features that are contained in a narrow window, the combination of these features is processed by the second convolutional layer. The convolutional layer is stateless and the output depends only on the present input values.
<fig id="Fig2"><label>Fig. 2</label><caption><p>The ConvNet network. The convolutional network used for the classification in [<xref ref-type="bibr" rid="CR27">27</xref>]</p></caption><graphic xlink:href="12859_2020_3627_Fig2_HTML" id="MO2"/></fig></p>
        <p>The non-linear function <italic>ϕ</italic> is usually the Rectified Linear Unit (ReLU), defined as:
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ ReLU(x) = \left\{\begin{array}{ll} x \ if \ x \ge 0 \\ 0 \ if \ x &lt; 0 \end{array} \right.  $$ \end{document}</tex-math><mml:math id="M10"><mml:mtext mathvariant="italic">ReLU</mml:mtext><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">if</mml:mtext><mml:mspace width="1em"/><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="1em"/><mml:mtext mathvariant="italic">if</mml:mtext><mml:mspace width="1em"/><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3627_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>In the work [<xref ref-type="bibr" rid="CR27">27</xref>] we used these network to classify genetic sequences with good results, and this justifies its use in the CORENup architecture.</p>
      </sec>
      <sec id="Sec6">
        <title>LSTM network</title>
        <p>Recurrent neural networks have a state value, a sliding window scans the input vector and each input vector component updates an internal network state that contributes to generating the output signal. Long Short-Term Memory layer (LSTM) is a particular kind of recurrent neural network where special units called <italic>gates</italic> select the relevant input values used to update the network hidden state. In general LSTM layers process sequences exploiting information that is into the whole sequence, or into a very large window. While the convolutional layer process the whole input pattern in a single step, LSTM process the input pattern exploiting the sequence of features so that we have to introduce the time in the notation and we refer to the input <italic>x</italic> as the input at the time <italic>t</italic>, <italic>x</italic><sub><italic>t</italic></sub>. Assuming <italic>x</italic><sub><italic>t</italic></sub>∈ℜ<sup><italic>d</italic></sup> the LSTM has three gate quantities to take into account: the <italic>forget</italic> gate <italic>f</italic><sub><italic>t</italic></sub>∈ℜ<sup><italic>u</italic></sup>, the <italic>update activation</italic> gate <italic>i</italic><sub><italic>t</italic></sub>∈ℜ<sup><italic>u</italic></sup>, the <italic>output activation</italic> gate <italic>o</italic><sub><italic>t</italic></sub>∈ℜ<sup><italic>u</italic></sup>, where <italic>u</italic> is the number of hidden units of the network. These gates are all functions of the input <italic>x</italic><sub><italic>t</italic></sub> and of the hidden state of the network <italic>h</italic><sub><italic>t</italic>−1</sub>∈ℜ<sup><italic>u</italic></sup>; the <italic>h</italic><sub><italic>t</italic></sub> vector is also the output of the network. These activation are obtained from the following equations:
<disp-formula id="Equa"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\mathbf{f_{t}} = \sigma\left(\mathbf{W^{f}}\mathbf{x_{t}} + \mathbf{U^{f}}\mathbf{h_{t-1}} + \mathbf{b^{f}}\right) $$ \end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="12859_2020_3627_Article_Equa.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equb"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\mathbf{i_{t}} = \sigma(\mathbf{W^{i}}\mathbf{x_{t}} + \mathbf{U^{i}}\mathbf{h_{t-1}} + \mathbf{b^{i}}) $$ \end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2020_3627_Article_Equb.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equc"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\mathbf{o_{t}} = \sigma(\mathbf{W^{o}}\mathbf{x_{t}} + \mathbf{U^{o}}\mathbf{h_{t-1}} + \mathbf{b^{o}}) $$ \end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2020_3627_Article_Equc.gif" position="anchor"/></alternatives></disp-formula> where <italic>W</italic><sup><italic>f</italic></sup>,<italic>W</italic><sup><italic>i</italic></sup>,<italic>W</italic><sup><italic>o</italic></sup> are all weights matrices of dimension (<italic>u</italic>,<italic>d</italic>), and <italic>U</italic><sup><italic>f</italic></sup>,<italic>U</italic><sup><italic>i</italic></sup>,<italic>U</italic><sup><italic>o</italic></sup> are weights matrices of dimension (<italic>u</italic>,<italic>u</italic>). The gate values are obtained from the current input <italic>x</italic><sub><italic>t</italic></sub> and the past hidden state value <italic>h</italic><sub><italic>t</italic>−1</sub>. The current value of hidden state <italic>h</italic><sub><italic>t</italic></sub>, is obtained from the cell state signal <italic>s</italic><sub><italic>t</italic></sub> that collects the values of the forget gate, the update gate, and the output gate, and is updated by using:
<disp-formula id="Equd"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\mathbf{s_{t}} = \mathbf{f_{t}} \odot \mathbf{s_{t-1}} + \mathbf{i_{t}} \odot \tanh(\mathbf{W^{c}}\mathbf{x_{t}} + \mathbf{U^{c}}\mathbf{h_{t-1}} + \mathbf{b^{c}}) $$ \end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>⊙</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>⊙</mml:mo><mml:mo>tanh</mml:mo><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold"><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2020_3627_Article_Equd.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>The output of the network is calculated by using the following equation:
<disp-formula id="Eque"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\mathbf{h_{t}} = \tanh(\mathbf{s_{t}}) \odot \mathbf{o_{t}}\\ $$ \end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>=</mml:mo><mml:mo>tanh</mml:mo><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>)</mml:mo><mml:mo>⊙</mml:mo><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math><graphic xlink:href="12859_2020_3627_Article_Eque.gif" position="anchor"/></alternatives></disp-formula> where the symbol ⊙ indicates the multiplication element by element. In a preceding work [<xref ref-type="bibr" rid="CR26">26</xref>] we found that substituting a convolutional layer with an LSTM layer can improve the performance in sequence classification tasks, and the resulting network is in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. In this case, the LSTM works on sequence features obtained by the convolutional layer, and this can give better results.
<fig id="Fig3"><label>Fig. 3</label><caption><p>The LSTM network. The architecture with an LSTM network used in [<xref ref-type="bibr" rid="CR26">26</xref>]</p></caption><graphic xlink:href="12859_2020_3627_Fig3_HTML" id="MO3"/></fig></p>
      </sec>
      <sec id="Sec7">
        <title>Merging the architectures: the CORENup network</title>
        <p>As said before both the architectures have advantages so that we decided to integrate the two approaches in the network architecture in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The CORENup network is composed by an input convolutional layer, that extracts some raw features from the sequence, followed by two processing paths: convolutional and recurrent. The combination of these two processing paths is obtained putting side to side the output vectors from each path. Both paths output a 3D tensor, after the flatten layers the two-column tensors <italic>x</italic><sub><italic>L</italic></sub> and <italic>x</italic><sub><italic>c</italic></sub> are 1D tensors with different dimensions:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} \mathbf{x_{L}} \in \Re^{L} \\ \mathbf{x_{c}} \in \Re^{c} \end{aligned}  $$ \end{document}</tex-math><mml:math id="M22"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℜ</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℜ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2020_3627_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><fig id="Fig4"><label>Fig. 4</label><caption><p>The CORENup architecture. A representation of the CORENup architecture presented in this paper. The details of the architecture are reported in Table <xref rid="Tab1" ref-type="table">1</xref></p></caption><graphic xlink:href="12859_2020_3627_Fig4_HTML" id="MO4"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>CORENup structure</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" colspan="6"><bold>Features extraction path</bold></td></tr><tr><td align="left">Layer</td><td align="left">Kernel Dim</td><td align="left"># Hidden Units</td><td align="left">stride Dim</td><td align="left">Output Dim</td><td align="left"># Params</td></tr><tr><td align="left">Conv1D</td><td align="left">5</td><td align="left">50</td><td align="left">1</td><td align="left">147x50</td><td align="left">1.050</td></tr><tr><td align="left">MaxPool1D</td><td align="left">-</td><td align="left">-</td><td align="left">2</td><td align="left">73x50</td><td align="left">0</td></tr><tr><td align="left">Dropout 50%</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">73x50</td><td align="left">0</td></tr><tr><td align="left" colspan="6"><bold>LSTM Path</bold></td></tr><tr><td align="left">Layer</td><td align="left">Kernel Dim</td><td align="left"># Hidden Units</td><td align="left">stride Dim</td><td align="left">Output Dim</td><td align="left"># Params</td></tr><tr><td align="left">LSTM</td><td align="left">-</td><td align="left">50</td><td align="left">-</td><td align="left">73x50</td><td align="left">20.200</td></tr><tr><td align="left">Dropout 50%</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">73x50</td><td align="left">0</td></tr><tr><td align="left">Flatten</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">3.650x1</td><td align="left">0</td></tr><tr><td align="left" colspan="6"><bold>Convolutional path</bold></td></tr><tr><td align="left">Layer</td><td align="left">Kernel Dim</td><td align="left"># Hidden Units</td><td align="left">stride Dim</td><td align="left">Output Dim</td><td align="left"># Params</td></tr><tr><td align="left">Conv1D</td><td align="left">10</td><td align="left">50</td><td align="left">1</td><td align="left">73x50</td><td align="left">25.050</td></tr><tr><td align="left">MaxPool1D</td><td align="left">-</td><td align="left">-</td><td align="left">2</td><td align="left">36x50</td><td align="left">0</td></tr><tr><td align="left">Dropout 50%</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">36x50</td><td align="left">0</td></tr><tr><td align="left">Flatten</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">1.800x1</td><td align="left">0</td></tr><tr><td align="left" colspan="6"><bold>Dense path</bold></td></tr><tr><td align="left">Concatenate</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">5.450x1</td><td align="left">0</td></tr><tr><td align="left">Dense</td><td align="left">-</td><td align="left">370</td><td align="left">-</td><td align="left">370x1</td><td align="left">2.016.870</td></tr><tr><td align="left">Dropout 50%</td><td align="left">-</td><td align="left">-</td><td align="left">-</td><td align="left">370x1</td><td align="left">0</td></tr><tr><td align="left">Dense</td><td align="left">-</td><td align="left">1</td><td align="left">-</td><td align="left">1x1</td><td align="left">371</td></tr><tr><td align="left" colspan="6"><bold>Parameters count</bold></td></tr><tr><td align="left" colspan="3"/><td align="left" colspan="3"><bold># Parameters</bold></td></tr><tr><td align="left" colspan="3">Features Extraction Path</td><td align="left" colspan="3">1.050</td></tr><tr><td align="left" colspan="3">LSTM Path</td><td align="left" colspan="3">20.200</td></tr><tr><td align="left" colspan="3">Convolutional Path</td><td align="left" colspan="3">25.050</td></tr><tr><td align="left" colspan="3">Dense Path</td><td align="left" colspan="3">2.017.241</td></tr><tr><td align="left" colspan="3"><bold>Total</bold></td><td align="left" colspan="3">2.063.541</td></tr></tbody></table><table-wrap-foot><p>Notice that the dense layer contains the majority of the network parameters</p></table-wrap-foot></table-wrap></p>
        <p>The two vectors are combined to obtain the tensor <bold>X</bold> :
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \mathbf{X}= \left[ \begin{aligned} \mathbf{x_{L}}\\\mathbf{x_{c}} \end{aligned} \right]  $$ \end{document}</tex-math><mml:math id="M24"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3627_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <bold>X</bold>∈ℜ<sup><italic>L</italic>+<italic>c</italic></sup> is a column vector.</p>
        <p>The proposed CORENup network is not so deep compared to the LeNup network: the input signal moves across 4 layers, considering the parallel as two layers and including the fully connected ones, while LeNup has 5 gated convolution layers plus two fully connected. The CORENup spreads more in width than in-depth, but, due to its structure, it can obtain similar or better performance with smaller training time. The details of the CORENup structure are reported in Table <xref rid="Tab1" ref-type="table">1</xref>, where it is also reported the number of the parameter of the network.</p>
        <p>The CORENup has roughly the same number of parameters of the LeNup, but the architecture is quite different. The number of layers is less and the majority of the weights is concentrated on the first dense layer because the number of dimension of <bold>X</bold> is <italic>L</italic>+<italic>c</italic> where <italic>L</italic>=3650 and <italic>c</italic>=1800. Considering 370 hidden units in the first hidden layer, we have more than 2 millions of weights concentrated in just one layer. In the LeNup network, as well as other deep neural networks, the weights are spread over many layers so that updates require many calculations, due to the more backpropagation steps, while in the CORENup network the weights in the fully connected layer will be updated in a single back-propagation step.</p>
      </sec>
    </sec>
    <sec id="Sec8">
      <title>Datasets and training details</title>
      <p>To be comparable with state of the art methods, the CORENup network was tested using two group of datasets and two different folding techniques. Each of the two groups of datasets has a reference paper where the data have been collected and used to train one or more machine learning models. Adopting such datasets give us the possibility to compare CORENup with other methods, assuming to use the same experimental protocol of the other methods.</p>
      <p>The first group of datasets is composed of four sets of DNA sequences. The first three underly nucleosomes from Homo Sapiens (HS), Caenorhabditis Elegans (E) and Drosophila Melanogaster(DM). The details about how these data have been collected can be found in the paper by Guo et al. [<xref ref-type="bibr" rid="CR23">23</xref>] and in the references therein. The fourth dataset is about Saccharomyces Cerevisiae (Y) and is introduced in [<xref ref-type="bibr" rid="CR34">34</xref>].</p>
      <p>The best performer on this data is the LeNup neural network [<xref ref-type="bibr" rid="CR25">25</xref>]. Authors provide the source code of the method (<ext-link ext-link-type="uri" xlink:href="https://github.com/biomedBit/LeNup">githubrepository</ext-link> ), so that we have decided to run the experiments also using this methodology.</p>
      <p>Following the experimental protocol reported in the LeNup work [<xref ref-type="bibr" rid="CR25">25</xref>] we used a 20-fold cross-validation method for each dataset of the first group. The cardinality of each class, for each dataset, is reported in Table <xref rid="Tab2" ref-type="table">2</xref>. For each iteration, we use 1 fold to test model and the remaining 19 folds to train both CORENup and the state of art LeNup network.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Number of samples in the first group for each class</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Nucleosome</th><th align="left">Linker</th><th align="left">Total</th></tr></thead><tbody><tr><td align="left">HS</td><td align="left">2273</td><td align="left">2300</td><td align="left">4573</td></tr><tr><td align="left">DM</td><td align="left">2900</td><td align="left">2850</td><td align="left">5750</td></tr><tr><td align="left">E</td><td align="left">2567</td><td align="left">2608</td><td align="left">5175</td></tr><tr><td align="left">Y</td><td align="left">1880</td><td align="left">1740</td><td align="left">3620</td></tr></tbody></table><table-wrap-foot><p>HS represents the Homo Sapiens group; DM represents Drosophila Melanogaster group; E represents the Elegans group; Y represents Yeast group</p></table-wrap-foot></table-wrap></p>
      <p>The second group of datasets give us the possibility to test the prediction methods on different sequence classes of HS, DM and Y species. Such classes include whole-genome (WG) and promoter (PM) sequences of Y, and the largest chromosome (LC), promoter (PM) and 5’UTR exon regions (5U) sequences from DM and HS. The dataset is downloadable in terms of bed file as reported in the reference paper [<xref ref-type="bibr" rid="CR35">35</xref>]. To collect the sequences, we have used the coordinates in the bed files to fetch the nucleosomal and linker sequences using the genome files downloaded from the UCSC Table Browser. The distribution of the elements in the classes for this group of datasets is shown in Table <xref rid="Tab3" ref-type="table">3</xref>. These datasets were used as a benchmark for different methods available in literature [<xref ref-type="bibr" rid="CR35">35</xref>]. The protocol used by the author in the work, consisted in the extraction with replacement, for each dataset, of 100 samples of 100 sequences each. We decided to adopt the same protocol for CORENup and LeNup network. Every dataset was split in training and test sets a priori, in such a way that we had sufficient data to train a strong model and a large enough pool from which to extract 100 test samples of 100 items each.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Number of samples in the second group for each class</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Nucleosome</th><th align="left">Linker</th><th align="left">Total</th></tr></thead><tbody><tr><td align="left">HS - LC</td><td align="left">97209</td><td align="left">65563</td><td align="left">162772</td></tr><tr><td align="left">HS - PM</td><td align="left">56404</td><td align="left">44639</td><td align="left">101043</td></tr><tr><td align="left">HS - 5U</td><td align="left">11769</td><td align="left">4880</td><td align="left">16649</td></tr><tr><td align="left">DM - LC</td><td align="left">46054</td><td align="left">30458</td><td align="left">76512</td></tr><tr><td align="left">DM - PM</td><td align="left">48251</td><td align="left">28763</td><td align="left">77014</td></tr><tr><td align="left">DM - 5U</td><td align="left">4669</td><td align="left">2704</td><td align="left">7373</td></tr><tr><td align="left">Y - WG</td><td align="left">39661</td><td align="left">4824</td><td align="left">44485</td></tr><tr><td align="left">Y - PM</td><td align="left">1880</td><td align="left">4463</td><td align="left">31836</td></tr></tbody></table><table-wrap-foot><p>The labels HS, DM and Y have the same meaning as in Table <xref rid="Tab2" ref-type="table">2</xref>. LC represents the longest chromosome; P represents the promoter sequences; 5U represents the 5UTR Exon region; WG represents the whole genome</p></table-wrap-foot></table-wrap></p>
      <p>CORENup has been implemented using Tensorflow with Keras Backend API [<xref ref-type="bibr" rid="CR36">36</xref>] environment. It has been trained using the <italic>Adam</italic> optimizer [<xref ref-type="bibr" rid="CR37">37</xref>] for the computation of the stochastic gradient descent with binary cross-entropy as the loss function. Learning Rate has been set to 3×10<sup>−4</sup> and, to prevent overfitting, and L2 Regularisation with <italic>λ</italic>=1×10<sup>−3</sup> has been used. CORENup set the maximum number of epochs to 200 and used an Early Stopping rule for training that stops if the loss function, calculated on validation, doesn’t decrease for 5 consecutive epochs. The size of the validation has bee set to 10% of the training set. The output of both models ranged between 0 and 1, has been used to predict labels with a threshold of 0.5. Every output above this last one was classified as a nucleosome otherwise as a linker.</p>
    </sec>
  </sec>
  <sec id="Sec9" sec-type="results">
    <title>Results</title>
    <p>Considering the same processing principle of feature integration shared by CORENup and LeNup, and the similar number of parameters, we decided to compare our model with the LeNup network using the two groups of datasets introduced before. The results obtained from the first dataset are evaluated by using the typical metrics of the classification evaluation. For each of the 20-fold the accuracy (ACC), sensitivity (SENS), specificity (SPEC), Matthew’s correlation coefficient (MCC) are evaluated using the following formulas:</p>
    <p>
      <inline-formula id="IEq4">
        <alternatives>
          <tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ACC = \frac {TP + TN}{TP + FP + TN + FN}$\end{document}</tex-math>
          <mml:math id="M26">
            <mml:mtext mathvariant="italic">ACC</mml:mtext>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mtext mathvariant="italic">TP</mml:mtext>
                <mml:mo>+</mml:mo>
                <mml:mtext mathvariant="italic">TN</mml:mtext>
              </mml:mrow>
              <mml:mrow>
                <mml:mtext mathvariant="italic">TP</mml:mtext>
                <mml:mo>+</mml:mo>
                <mml:mtext mathvariant="italic">FP</mml:mtext>
                <mml:mo>+</mml:mo>
                <mml:mtext mathvariant="italic">TN</mml:mtext>
                <mml:mo>+</mml:mo>
                <mml:mtext mathvariant="italic">FN</mml:mtext>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
          <inline-graphic xlink:href="12859_2020_3627_Article_IEq4.gif"/>
        </alternatives>
      </inline-formula>
    </p>
    <p>
      <inline-formula id="IEq5">
        <alternatives>
          <tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$SENS = \frac {TP}{TP + FN}$\end{document}</tex-math>
          <mml:math id="M28">
            <mml:mtext mathvariant="italic">SENS</mml:mtext>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mtext mathvariant="italic">TP</mml:mtext>
              </mml:mrow>
              <mml:mrow>
                <mml:mtext mathvariant="italic">TP</mml:mtext>
                <mml:mo>+</mml:mo>
                <mml:mtext mathvariant="italic">FN</mml:mtext>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
          <inline-graphic xlink:href="12859_2020_3627_Article_IEq5.gif"/>
        </alternatives>
      </inline-formula>
    </p>
    <p>
      <inline-formula id="IEq6">
        <alternatives>
          <tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$SPEC = \frac {TN}{TN + FN}$\end{document}</tex-math>
          <mml:math id="M30">
            <mml:mtext mathvariant="italic">SPEC</mml:mtext>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mtext mathvariant="italic">TN</mml:mtext>
              </mml:mrow>
              <mml:mrow>
                <mml:mtext mathvariant="italic">TN</mml:mtext>
                <mml:mo>+</mml:mo>
                <mml:mtext mathvariant="italic">FN</mml:mtext>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
          <inline-graphic xlink:href="12859_2020_3627_Article_IEq6.gif"/>
        </alternatives>
      </inline-formula>
    </p>
    <p><inline-formula id="IEq7"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$MCC = \frac {TP * TN - FP * FN}{\sqrt {(TN + FN)*(TN + FP)*(TP + FN)*(TP + FP)}}$\end{document}</tex-math><mml:math id="M32"><mml:mtext mathvariant="italic">MCC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>∗</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>∗</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext><mml:mo>)</mml:mo><mml:mo>∗</mml:mo><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>)</mml:mo><mml:mo>∗</mml:mo><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext><mml:mo>)</mml:mo><mml:mo>∗</mml:mo><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2020_3627_Article_IEq7.gif"/></alternatives></inline-formula>where TP is true-positive, FP is false positive, TN is true negative and FN is false-negative. We indicate as positive the nucleosome class. We also calculated the Roc Curve to compare the prediction performance of the four methods and we reported the Area Under the Curve (AUC). The first set of results is related to the HS, DM, E and Y datasets, and in Table <xref rid="Tab4" ref-type="table">4</xref> the results obtained from two models used in our recent works, indicated as simple LSTM [<xref ref-type="bibr" rid="CR26">26</xref>] and ConvNet [<xref ref-type="bibr" rid="CR27">27</xref>] are reported, together with the CORENup and the LeNup networks. The two simpler networks have almost always worst performance than the more complex networks, confirming that with more parameters it is possible to improve the performance of the classifier. Looking at the CORENup column it is possible to see that the CORENup network outperforms the LeNup almost in all the cases; there are just two values below the LeNup performance in Human (HS) and Elegans (E) datasets.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Experimental results for the 20-Fold procedure with the first group of data-sets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left">LSTM</th><th align="left">ConvNet</th><th align="left">CORENup</th><th align="left">LeNup</th></tr></thead><tbody><tr><td align="left">HS</td><td align="left">ACC</td><td align="left">0,836 ±0,03</td><td align="left">0,83 ±0,03</td><td align="left"><bold>0,881</bold> ±0,04</td><td align="left">0,873 ±0,02</td></tr><tr><td align="left"/><td align="left">SENS</td><td align="left">0,898 ±0,03</td><td align="left">0,867 ±0,03</td><td align="left"><bold>0,931</bold> ±0,06</td><td align="left">0,839 ±0,03</td></tr><tr><td align="left"/><td align="left">SPEC</td><td align="left">0,792 ±0,03</td><td align="left">0,814 ±0,03</td><td align="left">0,843 ±0,02</td><td align="left"><bold>0,906</bold> ±0,04</td></tr><tr><td align="left"/><td align="left">MCC</td><td align="left">0,681 ±0,002</td><td align="left">0,666 ±0,0</td><td align="left">0,758 ±0,07</td><td align="left"><bold>0,762</bold> ±0,03</td></tr><tr><td align="left"/><td align="left">AUC</td><td align="left">0,92 ±0,03</td><td align="left">0,91 ±0,03</td><td align="left"><bold>0,93</bold> ±0,03</td><td align="left">0,928 ±0,01</td></tr><tr><td align="left">DM</td><td align="left">ACC</td><td align="left">0,854 ±0,04</td><td align="left">0,838 ±0,04</td><td align="left"><bold>0,882</bold> ±0,02</td><td align="left">0,875±0,02</td></tr><tr><td align="left"/><td align="left">SENS</td><td align="left">0,872 ±0,03</td><td align="left">0,816 ±0,03</td><td align="left"><bold>0,898</bold> ±0,02</td><td align="left">0,876±0,03</td></tr><tr><td align="left"/><td align="left">SPEC</td><td align="left">0,841 ±0,05</td><td align="left">0,838 ±0,06</td><td align="left"><bold>0,869</bold> ±0,02</td><td align="left">0,74 ±0,13</td></tr><tr><td align="left"/><td align="left">MCC</td><td align="left">0,71 ±0,003</td><td align="left">0,68 ±0,003</td><td align="left"><bold>0,766</bold> ±0,04</td><td align="left"><bold>0,766</bold> ±0,04</td></tr><tr><td align="left"/><td align="left">AUC</td><td align="left">0,93 ±0,02</td><td align="left">0,92 ±0,02</td><td align="left"><bold>0,94</bold> ±0,02</td><td align="left">0,937 ±0,02</td></tr><tr><td align="left">E</td><td align="left">ACC</td><td align="left">0,897 ±0,03</td><td align="left">0,895 ±0,03</td><td align="left"><bold>0,915</bold> ±0,05</td><td align="left">0,912±0,02</td></tr><tr><td align="left"/><td align="left">SENS</td><td align="left">0,938 ±0,03</td><td align="left">0,924 ±0,02</td><td align="left"><bold>0,958</bold> ±0,03</td><td align="left">0,885±0,02</td></tr><tr><td align="left"/><td align="left">SPEC</td><td align="left">0,865 ±0,04</td><td align="left">0,874 ±0,04</td><td align="left">0,882 ±0,03</td><td align="left"><bold>0,939</bold> ±0,02</td></tr><tr><td align="left"/><td align="left">MCC</td><td align="left">0,799 ±0,002</td><td align="left">0,795 ±0,001</td><td align="left"><bold>0,835</bold> ±0,07</td><td align="left">0,832±0,03</td></tr><tr><td align="left"/><td align="left">AUC</td><td align="left">0,96 ±0,02</td><td align="left">0,96 ±0,02</td><td align="left"><bold>0,96</bold> ±0,02</td><td align="left"><bold>0,96</bold> ±0,02</td></tr><tr><td align="left">Y</td><td align="left">ACC</td><td align="left">0,996 ±0,05</td><td align="left">0,996 ±0,06</td><td align="left"><bold>1,0</bold> ±0,002</td><td align="left">1,0 ±0,0</td></tr><tr><td align="left"/><td align="left">SENS</td><td align="left">0,998 ±0,05</td><td align="left">0,998 ±0,05</td><td align="left">0,999 ±0,005</td><td align="left"><bold>1,0</bold> ±0,0</td></tr><tr><td align="left"/><td align="left">SPEC</td><td align="left">0,995 ±0,07</td><td align="left">0,995 ±0,08</td><td align="left"><bold>1,0</bold> ±0,0</td><td align="left">1,0 ±0,0</td></tr><tr><td align="left"/><td align="left">MCC</td><td align="left">0,992 ±0,003</td><td align="left">0,993 ±0,002</td><td align="left">0,999 ±0,005</td><td align="left"><bold>1,0</bold> ±0,0</td></tr><tr><td align="left"/><td align="left">AUC</td><td align="left">0,99 ±0,0</td><td align="left">0,99 ±0,0</td><td align="left">0,99 ±0,0</td><td align="left"><bold>1,0</bold> ±0,0</td></tr></tbody></table><table-wrap-foot><p>The two networks LeNup and CORENup outperform the simpler networks in Figs. <xref rid="Fig2" ref-type="fig">2</xref> and <xref rid="Fig3" ref-type="fig">3</xref>. Best values are shown in boldface</p></table-wrap-foot></table-wrap></p>
  </sec>
  <sec id="Sec10" sec-type="discussion">
    <title>Discussions</title>
    <p>The obtained results sustain the idea that the architecture is important and simply adding more weights, for example making the network deeper, do not automatically improve the performance. In CORENup network the majority of the parameters are used on the first fully connected layer, where it is necessary to integrate features of different nature, non-periodic features obtained from the stateless convolutional layer, and periodic features from the recurrent LSTM layer, and even if they are more than 2 million they have a small impact on the training time. The first 4 rows of Tables <xref rid="Tab5" ref-type="table">5</xref> and <xref rid="Tab6" ref-type="table">6</xref> report the training time for the two networks on datasets HS, DM, E and Y. From Table <xref rid="Tab5" ref-type="table">5</xref> it is possible to see that the CORENup is about three times faster than the LeNup, while Table <xref rid="Tab6" ref-type="table">6</xref> reports the total training time until the algorithm stops. In two cases this time is longer for the LeNup, because the CORENup keeps improving its performance for more epochs, and the algorithm does not stop the training if there are still improvements on the classification results.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Time for Epochs</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">CORENup</th><th align="left">LeNup</th><th align="left">Overhead</th></tr></thead><tbody><tr><td align="left">HS</td><td align="left">5</td><td align="left">16</td><td align="left">0.31</td></tr><tr><td align="left">DM</td><td align="left">6</td><td align="left">20</td><td align="left">0.30</td></tr><tr><td align="left">E</td><td align="left">5</td><td align="left">18</td><td align="left">0.27</td></tr><tr><td align="left">Y</td><td align="left">4</td><td align="left">12</td><td align="left">0.33</td></tr><tr><td align="left">HS - LC</td><td align="left">63</td><td align="left">233</td><td align="left">0,27</td></tr><tr><td align="left">HS - PM</td><td align="left">44</td><td align="left">158</td><td align="left">0,28</td></tr><tr><td align="left">HS - 5U</td><td align="left">10</td><td align="left">35</td><td align="left">0,29</td></tr><tr><td align="left">DM - LC</td><td align="left">47</td><td align="left">171</td><td align="left">0,27</td></tr><tr><td align="left">DM - PM</td><td align="left">57</td><td align="left">211</td><td align="left">0,27</td></tr><tr><td align="left">DM - 5U</td><td align="left">4</td><td align="left">13</td><td align="left">0,30</td></tr><tr><td align="left">Y - WG</td><td align="left">36</td><td align="left">124</td><td align="left">0.29</td></tr><tr><td align="left">Y - PM</td><td align="left">24</td><td align="left">88</td><td align="left">0.27</td></tr></tbody></table><table-wrap-foot><p>Comparison between CORENup and LeNup time for epochs, all the time are expressed in seconds. The Overhead column reports the ratio between CORENup and LeNup times</p></table-wrap-foot></table-wrap><table-wrap id="Tab6"><label>Table 6</label><caption><p>Training time</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">CORENup</th><th align="left">LeNup</th><th align="left">Overhead</th></tr></thead><tbody><tr><td align="left">HS</td><td align="left">166,79</td><td align="left">305</td><td align="left">0,55</td></tr><tr><td align="left">DM</td><td align="left">219,14</td><td align="left">382,75</td><td align="left">0,72</td></tr><tr><td align="left">E</td><td align="left">207,40</td><td align="left">312,35</td><td align="left">0,66</td></tr><tr><td align="left">Y</td><td align="left">287,46</td><td align="left">186,5</td><td align="left">1,54</td></tr><tr><td align="left">HS - LC</td><td align="left">2544,14</td><td align="left">1426</td><td align="left">1,78</td></tr><tr><td align="left">HS - PM</td><td align="left">1558,92</td><td align="left">1806</td><td align="left">0,86</td></tr><tr><td align="left">HS - 5U</td><td align="left">316,08</td><td align="left">750</td><td align="left">0,42</td></tr><tr><td align="left">DM - LC</td><td align="left">2025.52</td><td align="left">4435</td><td align="left">0.46</td></tr><tr><td align="left">DM - PM</td><td align="left">1823.39</td><td align="left">3040</td><td align="left">0.60</td></tr><tr><td align="left">DM - 5U</td><td align="left">121.78</td><td align="left">171</td><td align="left">0.71</td></tr><tr><td align="left">Y - WG</td><td align="left">791.37</td><td align="left">1294</td><td align="left">0.61</td></tr><tr><td align="left">Y - PM</td><td align="left">500.37</td><td align="left">1311</td><td align="left">0.38</td></tr></tbody></table><table-wrap-foot><p>Comparison between CORENup and LeNup Training time, all the time are expressed in seconds. The Overhead column reports the ratio between CORENup and LeNup times</p></table-wrap-foot></table-wrap></p>
    <p>The other eight datasets (HS-LC, HS-PM, HS-5U, DM-LC, DM-PM, DM-5U, Y-WG and Y-PM) are much more difficult because there is much more noise in the sequences. In this trial, the CORENup gives better results on 5 datasets out of 8, as reported in Table <xref rid="Tab7" ref-type="table">7</xref>, still maintaining the same, very fast, training time. Notice that the AUC in dataset HS-PM is only 1∗10<sup>−3</sup> below the LeNup value and that the only significant differences are on the HS-LC and Y-PM datasets. The training times are reported in the last eight rows of Tables <xref rid="Tab5" ref-type="table">5</xref> and <xref rid="Tab6" ref-type="table">6</xref>, again it is possible to see that the time for epoch is one-third of the LeNup training time and that the whole training time is still lower, except for the HS-LC dataset.
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Experiments result for the second group of data-sets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">CORENup</th><th align="left">LeNup</th><th align="left">Best for [<xref ref-type="bibr" rid="CR35">35</xref>]</th></tr></thead><tbody><tr><td align="left">HS - LC</td><td align="left">0,912</td><td align="left"><bold>0,926</bold></td><td align="left">0,65</td></tr><tr><td align="left">HS - PM</td><td align="left">0,875</td><td align="left"><bold>0,876</bold></td><td align="left">0,67</td></tr><tr><td align="left">HS - 5U</td><td align="left"><bold>0,758</bold></td><td align="left">0,732</td><td align="left">∼0,7</td></tr><tr><td align="left">DM - LC</td><td align="left"><bold>0,734</bold></td><td align="left">0,724</td><td align="left">∼0,7</td></tr><tr><td align="left">DM - PM</td><td align="left"><bold>0,738</bold></td><td align="left">0,734</td><td align="left">∼0,7</td></tr><tr><td align="left">DM - 5U</td><td align="left"><bold>0,746</bold></td><td align="left">0,695</td><td align="left">∼0,7</td></tr><tr><td align="left">Y - WG</td><td align="left"><bold>0,968</bold></td><td align="left">0,939</td><td align="left">0,77</td></tr><tr><td align="left">Y - PM</td><td align="left">0,909</td><td align="left"><bold>0,933</bold></td><td align="left">0,79</td></tr></tbody></table><table-wrap-foot><p>The AUC values are calculated as explained in the work [<xref ref-type="bibr" rid="CR35">35</xref>] where the data-sets were originally proposed. The last column reports the results of the best performer among the 8 methods compared in the original paper. Best values are shown in boldface</p></table-wrap-foot></table-wrap></p>
    <p>The third column in Table <xref rid="Tab7" ref-type="table">7</xref> shows the AUC values of the best performer among the 8 methods compared in the paper by Liu <italic>et al</italic> [<xref ref-type="bibr" rid="CR35">35</xref>]. The symbol ’ ∼’ is used to indicate a ’very close to’ value, and this approximation is mandatory since the paper reports no more than a bar plot for the AUC values. The comparison shows the improvements obtained by using the two deep neural networks.</p>
    <p>All the experiments reported ran on Intel i7 CPU with 32GB of memory RAM and an NVIDIA TITAN V with 12GB of GPU dedicated memory.</p>
  </sec>
  <sec id="Sec11" sec-type="conclusion">
    <title>Conclusions</title>
    <p>Deep neural networks are suitable for sequence classification because they can automatically extract the useful features from sequence and can use them for classification. In deep neural networks like GoogLeNet [<xref ref-type="bibr" rid="CR33">33</xref>] or LeNup network [<xref ref-type="bibr" rid="CR25">25</xref>], these features are processed by a sequence of layers that groups them and extract from this composition new high-order more complex features.</p>
    <p>The same principle of feature composition can be exploited using features of different nature, such as the one extracted by convolutional networks and recurrent networks. The convolutional layers can extract and process features that are related to the presence of “static” patterns in the sequences, such as few letters words or other patterns. The recurrent layer can complete the extraction of the feature related to the periodic characteristics of the sequences probably related to word repetition. Using this approach, the input signal follows different processing paths aimed to extract different information from the input. The resulting network is more "wide" than deep, with many layers that work in parallel on the same input. In this paper, we have proposed a deep neural network, called CORENup, which follows this parallel layer composition. CORENup has shown to be a top performer with a smaller training time with respect to the state of the art. This kind of architecture shows that the features extraction and composition process can be obtained not only in deep stacks of convolutional layers but also in shallow modules that process the signal in different ways and the collecting the results in a single representing vector.</p>
    <p>Future efforts will be focused to increase the information extracted from the input and to mix them to obtain a more rich input signal for the fully connected layers.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <p>We gratefully acknowledge the support of NVIDIA Corporation for this research with the donation of a Titan Xp that was used for the preliminary studies, and a Titan V GPUs used for the training of the model on the complete datasets. This research is funded in part by MIUR Project of National Relevance 2017WR7SHH ”Multicriteria Data Structures and Algorithms: from compressed to learned indexes, and beyond”.</p>
    <sec id="d30e3392">
      <title>About this supplement</title>
      <p>This article has been published as part of Volume 21, Supplement 8 2020: Italian Society of Bioinformatics (BITS): Annual Meeting 2019. The full contents of the supplement are available at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-21-supplement-8">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-21-supplement-8</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>GLB and RR conceived the study. GLB, RR, AD designed the methodology. RR and DA implemented the methodology. AD prepared the data and ran the experiments. GLB and RR analyzed and interpreted the data. The authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Publication costs are funded by the I.E.ME.S.T. (Euromediterranean Institute of Science and Technology).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The source code of CORENup is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLearningForSequence/CORENup-A-Combination-of-Convolutional-and-Recurrent-Deep-Neural-Networks-for-NucleosomePositioning">https://github.com/DeepLearningForSequence/ CORENup-A-Combination-of-Convolutional-and-Recurrent-Deep-Neural-Networks-for-NucleosomePositioning</ext-link>. The data used for the experiments are downloadable at <ext-link ext-link-type="uri" xlink:href="https://github.com/DeepLearningForSequence/CORENup-Datasets/tree/master/Datasets">https://github.com/DeepLearningForSequence/CORENup-Datasets/tree/master/Datasets</ext-link>.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ridgway</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Almouzni</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Chromatin assembly and organization</article-title>
        <source>J Cell Sci</source>
        <year>2001</year>
        <volume>114</volume>
        <issue>15</issue>
        <fpage>2711</fpage>
        <lpage>2</lpage>
        <?supplied-pmid 11683405?>
        <pub-id pub-id-type="pmid">11683405</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weiner</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hughes</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yassour</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rando</surname>
            <given-names>OJ</given-names>
          </name>
          <name>
            <surname>Friedman</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>High-resolution nucleosome mapping reveals transcription-dependent promoter packaging</article-title>
        <source>Genome Res</source>
        <year>2010</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>90</fpage>
        <lpage>100</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.098509.109</pub-id>
        <pub-id pub-id-type="pmid">19846608</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Whitehouse</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Tsukiyama</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Antagonistic forces that position nucleosomes in vivo</article-title>
        <source>Nat Struct Mol Biol</source>
        <year>2006</year>
        <volume>13</volume>
        <issue>7</issue>
        <fpage>633</fpage>
        <pub-id pub-id-type="doi">10.1038/nsmb1111</pub-id>
        <pub-id pub-id-type="pmid">16819518</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cairns</surname>
            <given-names>BR</given-names>
          </name>
        </person-group>
        <article-title>Chromatin remodeling complexes: strength in diversity, precision through specialization</article-title>
        <source>Curr Opin Genet Dev</source>
        <year>2005</year>
        <volume>15</volume>
        <issue>2</issue>
        <fpage>185</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/j.gde.2005.01.003</pub-id>
        <pub-id pub-id-type="pmid">15797201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sala</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Toto</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pinello</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Gabriele</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Di Benedetto</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ingrassia</surname>
            <given-names>AMR</given-names>
          </name>
          <name>
            <surname>Lo Bosco</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Di Gesù</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Giancarlo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Corona</surname>
            <given-names>DFV</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide characterization of chromatin binding and nucleosome spacing activity of the nucleosome remodelling atpase iswi</article-title>
        <source>EMBO J</source>
        <year>2011</year>
        <volume>30</volume>
        <issue>9</issue>
        <fpage>1766</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1038/emboj.2011.98</pub-id>
        <pub-id pub-id-type="pmid">21448136</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schnitzler</surname>
            <given-names>GR</given-names>
          </name>
        </person-group>
        <article-title>Control of nucleosome positions by dna sequence and remodeling machines</article-title>
        <source>Cell Biochem Biophys</source>
        <year>2008</year>
        <volume>51</volume>
        <issue>2-3</issue>
        <fpage>67</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1007/s12013-008-9015-6</pub-id>
        <pub-id pub-id-type="pmid">18543113</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shahbazian</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Grunstein</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Functions of site-specific histone acetylation and deacetylation</article-title>
        <source>Annu Rev Biochem</source>
        <year>2007</year>
        <volume>76</volume>
        <fpage>75</fpage>
        <lpage>100</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev.biochem.76.052705.162114</pub-id>
        <pub-id pub-id-type="pmid">17362198</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <mixed-citation publication-type="other">Nucleosome positioning In: Ranganathan S, Gribskov M, Nakai K, Schönbach C, editors. Encyclopedia of Bioinformatics and Computational Biology. Oxford: Academic Press: 2019. p. 308–17.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Wallrath</surname>
            <given-names>LL</given-names>
          </name>
          <name>
            <surname>Elgin</surname>
            <given-names>SC</given-names>
          </name>
        </person-group>
        <article-title>Nucleosome positioning and gene regulation</article-title>
        <source>J Cell Biochem</source>
        <year>1994</year>
        <volume>55</volume>
        <issue>1</issue>
        <fpage>83</fpage>
        <lpage>92</lpage>
        <pub-id pub-id-type="doi">10.1002/jcb.240550110</pub-id>
        <pub-id pub-id-type="pmid">8083303</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Svaren</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Horz</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Transcription factors vs. nucleosomes: Regulation of the pho5 promoter in yeast</article-title>
        <source>Trends Biochem Sci</source>
        <year>1997</year>
        <volume>22</volume>
        <fpage>93</fpage>
        <lpage>97</lpage>
        <pub-id pub-id-type="doi">10.1016/S0968-0004(97)01001-3</pub-id>
        <pub-id pub-id-type="pmid">9066259</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>M-J</given-names>
          </name>
          <name>
            <surname>Seddon</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>ZT-Y</given-names>
          </name>
          <name>
            <surname>Major</surname>
            <given-names>IT</given-names>
          </name>
          <name>
            <surname>Floer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Howe</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Shiu</surname>
            <given-names>S-H</given-names>
          </name>
        </person-group>
        <article-title>Determinants of nucleosome positioning and their influence on plant gene expression</article-title>
        <source>Genome Res</source>
        <year>2015</year>
        <volume>25</volume>
        <issue>8</issue>
        <fpage>1182</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.188680.114</pub-id>
        <pub-id pub-id-type="pmid">26063739</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pulivarthy</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Lion</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kuzu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Matthews</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Borowsky</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Morris</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kingston</surname>
            <given-names>RE</given-names>
          </name>
          <name>
            <surname>Dennis</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Tolstorukov</surname>
            <given-names>MY</given-names>
          </name>
          <name>
            <surname>Oettinger</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Regulated large-scale nucleosome density patterns and precise nucleosome positioning correlate with v (d) j recombination</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2016</year>
        <volume>113</volume>
        <issue>42</issue>
        <fpage>6427</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1605543113</pub-id>
        <pub-id pub-id-type="pmid">27162339</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Satchwell</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Drew</surname>
            <given-names>HR</given-names>
          </name>
          <name>
            <surname>Travers</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <article-title>Sequence periodicities in chicken nucleosome core dna</article-title>
        <source>J Mol Biol</source>
        <year>1986</year>
        <volume>191</volume>
        <issue>4</issue>
        <fpage>659</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-2836(86)90452-3</pub-id>
        <pub-id pub-id-type="pmid">3806678</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Drew</surname>
            <given-names>HR</given-names>
          </name>
          <name>
            <surname>Travers</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <article-title>Dna bending and its relation to nucleosome positioning</article-title>
        <source>J Mol Biol</source>
        <year>1985</year>
        <volume>186</volume>
        <issue>4</issue>
        <fpage>773</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-2836(85)90396-1</pub-id>
        <pub-id pub-id-type="pmid">3912515</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <mixed-citation publication-type="other">Lowman H, Bina M. Correlation between dinucleotide periodicities and nucleosome positioning on mouse satellite dna. Biopolymers. 1990; 30(9–10):861–76. https://doi.org/10.1002/bip.360300902. http://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/bip.360300902.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giancarlo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rombo</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Utro</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>In vitro versus in vivo compositional landscapes of histone sequence preferences in eucaryotic genomes</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>20</issue>
        <fpage>3454</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty799</pub-id>
        <pub-id pub-id-type="pmid">30204840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kaplan</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>K Moore</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Mittendorf</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>J Gossett</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tillo</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Field</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>M LeProust</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>R Hughes</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lieb</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Widom</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Segal</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>The dna-encoded nucleosome organization of a eukaryotic genome</article-title>
        <source>Nature</source>
        <year>2009</year>
        <volume>458</volume>
        <fpage>362</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1038/nature07667</pub-id>
        <pub-id pub-id-type="pmid">19092803</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lo Bosco</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Angelini</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Rancoita</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Rovetta</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Alignment free dissimilarities for nucleosome classification</article-title>
        <source>Computational Intelligence Methods for Bioinformatics and Biostatistics</source>
        <year>2016</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Utro</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Di Benedetto</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Corona</surname>
            <given-names>DFV</given-names>
          </name>
          <name>
            <surname>Giancarlo</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>The intrinsic combinatorial organization and information theoretic content of a sequence are correlated to the DNA encoded nucleosome organization of eukaryotic genomes</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>32</volume>
        <issue>6</issue>
        <fpage>835</fpage>
        <lpage>42</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv679</pub-id>
        <pub-id pub-id-type="pmid">26576651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Giancarlo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Rombo</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Utro</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Dna combinatorial messages and epigenomics: The case of chromatin organization and nucleosome occupancy in eukaryotic genomes</article-title>
        <source>Theor Comput Sci</source>
        <year>2019</year>
        <volume>792</volume>
        <fpage>117</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tcs.2018.06.047</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chereji</surname>
            <given-names>RV</given-names>
          </name>
          <name>
            <surname>Clark</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Major determinants of nucleosome positioning</article-title>
        <source>Biophys J</source>
        <year>2018</year>
        <volume>114</volume>
        <issue>10</issue>
        <fpage>2279</fpage>
        <lpage>89</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bpj.2018.03.015</pub-id>
        <pub-id pub-id-type="pmid">29628211</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Bao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Asmann</surname>
            <given-names>YW</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>NucMap: a database of genome-wide nucleosome positioning map across species</article-title>
        <source>Nucleic Acids Res</source>
        <year>2018</year>
        <volume>47</volume>
        <issue>D1</issue>
        <fpage>163</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gky980</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>S-H</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>E-Z</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>L-Q</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>inuc-pseknc: a sequence-based predictor for predicting nucleosome positioning in genomes with pseudo k-tuple nucleotide composition</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>11</issue>
        <fpage>1522</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu083</pub-id>
        <pub-id pub-id-type="pmid">24504871</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tahir</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hayat</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>inuc-stnc: a sequence-based predictor for identification of nucleosome positioning in genomes by extending the concept of saac and chou’s pseaac</article-title>
        <source>Mol BioSyst</source>
        <year>2016</year>
        <volume>12</volume>
        <fpage>2587</fpage>
        <lpage>93</lpage>
        <pub-id pub-id-type="doi">10.1039/C6MB00221H</pub-id>
        <pub-id pub-id-type="pmid">27271822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Lenup: learning nucleosome positioning from dna sequences with improved convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>10</issue>
        <fpage>1705</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty003</pub-id>
        <pub-id pub-id-type="pmid">29329398</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Di Gangi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lo Bosco</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rizzo</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Deep learning architectures for prediction of nucleosome positioning from sequences data</article-title>
        <source>BMC Bioinformatics</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>14</issue>
        <fpage>418</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-018-2386-9</pub-id>
        <pub-id pub-id-type="pmid">30453896</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Lo Bosco G, Rizzo R, Fiannaca A, La Rosa M, Urso A. A deep learning model for epigenomic studies. In: 12th International Conference on Signal-Image Technology Internet-Based Systems (SITIS). IEEE: 2016. p. 688–92. 10.1109/sitis.2016.115.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Di Gangi</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Gaglio</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>La Bua</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lo Bosco</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rizzo</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Rojas</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Ortuño</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A deep learning network for exploiting positional information in nucleosome related sequences</article-title>
        <source>Bioinformatics and Biomedical Engineering</source>
        <year>2017</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fiannaca</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>La Paglia</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>La Rosa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Renda</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rizzo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gaglio</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Urso</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning models for bacteria taxonomic classification of metagenomic data</article-title>
        <source>BMC Bioinformatics</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>7</issue>
        <fpage>198</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-018-2182-6</pub-id>
        <pub-id pub-id-type="pmid">30066629</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <mixed-citation publication-type="other">Amato D, Di Gangi MA, Lo Bosco G, Rizzo R. Recurrent deep neural networks for nucleosome classification In: Raposo M, Ribeiro P, Sério S, Staiano A, Ciaramella A, editors. Computational Intelligence Methods for Bioinformatics and Biostatistics. Cham: Springer: 2020. p. 118–27.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Di Gesù</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Lo Bosco</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pinello</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>G-C</given-names>
          </name>
          <name>
            <surname>Corona</surname>
            <given-names>DFV</given-names>
          </name>
        </person-group>
        <article-title>A multi-layer method to study genome-scale positions of nucleosomes</article-title>
        <source>Genomics</source>
        <year>2009</year>
        <volume>93</volume>
        <issue>2</issue>
        <fpage>140</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ygeno.2008.09.012</pub-id>
        <pub-id pub-id-type="pmid">18951969</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pinello</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lo Bosco</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>G-C</given-names>
          </name>
        </person-group>
        <article-title>Applications of alignment-free methods in epigenomics</article-title>
        <source>Brief Bioinformatics</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>3</issue>
        <fpage>419</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbt078</pub-id>
        <pub-id pub-id-type="pmid">24197932</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <mixed-citation publication-type="other">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with convolutions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition: 2015. p. 1–9. 10.1109/cvpr.2015.7298594.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>Using deformation energy to analyze nucleosome positioning in genomes</article-title>
        <source>Genomics</source>
        <year>2016</year>
        <volume>107</volume>
        <issue>2</issue>
        <fpage>69</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ygeno.2015.12.005</pub-id>
        <pub-id pub-id-type="pmid">26724497</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <mixed-citation publication-type="other">Liu H, Zhang R, Xiong W, Guan J, Zhuang Z, Zhou S. A comparative evaluation on prediction methods of nucleosome positioning. Brief Bioinforma. 2013; 15. 10.1093/bib/bbt062.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <mixed-citation publication-type="other">Tensorflow. <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/install">https://www.tensorflow.org/install</ext-link>. Accessed 07 April 2020.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A method for stochastic optimization. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings: 2015. http://arxiv.org/abs/1412.6980.</mixed-citation>
    </ref>
  </ref-list>
</back>
