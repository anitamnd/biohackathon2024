<?DTDIdentifier.IdentifierValue article.dtd?>
<?DTDIdentifier.IdentifierType system?>
<?SourceDTD.DTDName article.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName bmc2nlmx2.xsl?>
<?ConverterInfo.Version 2?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">2848649</article-id>
    <article-id pub-id-type="publisher-id">1471-2105-11-126</article-id>
    <article-id pub-id-type="pmid">20226024</article-id>
    <article-id pub-id-type="doi">10.1186/1471-2105-11-126</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Mocapy++ - A toolkit for inference and learning in dynamic Bayesian networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" id="A1">
        <name>
          <surname>Paluszewski</surname>
          <given-names>Martin</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>palu@binf.ku.dk</email>
      </contrib>
      <contrib contrib-type="author" id="A2">
        <name>
          <surname>Hamelryck</surname>
          <given-names>Thomas</given-names>
        </name>
        <xref ref-type="aff" rid="I1">1</xref>
        <email>thamelry@binf.ku.dk</email>
      </contrib>
    </contrib-group>
    <aff id="I1"><label>1</label>Bioinformatics Centre, University of Copenhagen, Denmark</aff>
    <pub-date pub-type="collection">
      <year>2010</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>3</month>
      <year>2010</year>
    </pub-date>
    <volume>11</volume>
    <fpage>126</fpage>
    <lpage>126</lpage>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>10</month>
        <year>2009</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>3</month>
        <year>2010</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright Â©2010 Paluszewski and Hamelryck; licensee BioMed Central Ltd.</copyright-statement>
      <copyright-year>2010</copyright-year>
      <copyright-holder>Paluszewski and Hamelryck; licensee BioMed Central Ltd.</copyright-holder>
      <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="http://www.biomedcentral.com/1471-2105/11/126"/>
    <abstract>
      <sec>
        <title>Background</title>
        <p>Mocapy++ is a toolkit for parameter learning and inference in <italic>dynamic Bayesian networks </italic>(DBNs). It supports a wide range of DBN architectures and probability distributions, including distributions from directional statistics (the statistics of angles, directions and orientations).</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>The program package is freely available under the <italic>GNU General Public Licence </italic>(GPL) from SourceForge <ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/mocapy">http://sourceforge.net/projects/mocapy</ext-link>. The package contains the source for building the Mocapy++ library, several usage examples and the user manual.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>Mocapy++ is especially suitable for constructing probabilistic models of biomolecular structure, due to its support for directional statistics. In particular, it supports the Kent distribution on the sphere and the bivariate von Mises distribution on the torus. These distributions have proven useful to formulate probabilistic models of protein and RNA structure in atomic detail.</p>
      </sec>
    </abstract>
  </article-meta>
</front>
<body>
  <sec>
    <title>Background</title>
    <p>A <italic>Bayesian network </italic>(BN) represents a set of variables and their joint probability distribution using a directed acyclic graph [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>]. A <italic>dynamic Bayesian network </italic>(DBN) is a BN that represents sequences, such as time-series from speech data or biological sequences [<xref ref-type="bibr" rid="B3">3</xref>]. One of the simplest examples of a DBN is the well known <italic>hidden Markov model </italic>(HMM) [<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B5">5</xref>]. DBNs have been applied with great success to a large number of problems in various fields. In bioinformatics, DBNs are especially relevant because of the sequential nature of biological molecules, and have therefore proven suitable for tackling a large number of problems. Examples are protein homologue detection [<xref ref-type="bibr" rid="B6">6</xref>], protein secondary structure prediction [<xref ref-type="bibr" rid="B7">7</xref>,<xref ref-type="bibr" rid="B8">8</xref>], gene finding [<xref ref-type="bibr" rid="B5">5</xref>], multiple sequence alignment [<xref ref-type="bibr" rid="B5">5</xref>] and sampling of protein conformations [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>].</p>
    <p>Here, we present a general, open source toolkit, called Mocapy++, for inference and learning in BNs and especially DBNs. The main purpose of Mocapy++ is to allow the user to concentrate on the probabilistic model itself, without having to implement customized algorithms. The name Mocapy stands for <italic>Markov chain </italic><bold><italic>Mo</italic></bold><italic>nte </italic><bold><italic>Ca</italic></bold><italic>rlo and </italic><bold><italic>Py</italic></bold><italic>thon</italic>: the key ingredients in the original implementation of Mocapy (T. Hamelryck, University of Copenhagen, 2004, unpublished). Today, Mocapy has been re-implemented in C++ but the name is kept for historical reasons. Mocapy supports a large range of architectures and probability distributions, and has proven its value in several published applications [<xref ref-type="bibr" rid="B9">9</xref>-<xref ref-type="bibr" rid="B13">13</xref>]. This article serves as the main single reference for both Mocapy and Mocapy++.</p>
    <sec>
      <title>Existing Packages</title>
      <p>Kevin Murphy maintains a list of software packages for inference in BNs [<xref ref-type="bibr" rid="B14">14</xref>]. Currently, this list contains 54 packages. A small subset of these packages share some key features with Mocapy++ (see Table <xref ref-type="table" rid="T1">1</xref>). These packages have an <italic>application programming interface </italic>(API), perform parameter estimation and are free of charge (at least for academic use). Mocapy++ is mainly intended for use in scientific research, where reproducibility and openness of scientific results are important. Commercial closed source packages are therefore not included in this discussion.</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Some popular Free BN packages with an API. Extracted from Murphy [<xref ref-type="bibr" rid="B14">14</xref>].</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left">Name</th>
              <th align="left">Authors</th>
              <th align="left">Source</th>
              <th align="left">Inference</th>
              <th align="left">Learning</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">Bayes Blocks</td>
              <td align="left">Harva et al. [<xref ref-type="bibr" rid="B30">30</xref>]</td>
              <td align="left">Python/C++</td>
              <td align="left">Ensemble learning</td>
              <td align="left">VB</td>
            </tr>
            <tr>
              <td align="left">BNT</td>
              <td align="left">Murphy [<xref ref-type="bibr" rid="B31">31</xref>]</td>
              <td align="left">Matlab/C</td>
              <td align="left">JTI, MCMC</td>
              <td align="left">EM</td>
            </tr>
            <tr>
              <td align="left">BUGS</td>
              <td align="left">Lunn et al. [<xref ref-type="bibr" rid="B32">32</xref>]</td>
              <td align="left">N</td>
              <td align="left">Gibbs</td>
              <td align="left">Gibbs</td>
            </tr>
            <tr>
              <td align="left">Elvira</td>
              <td align="left">Elvira Consortium [<xref ref-type="bibr" rid="B33">33</xref>]</td>
              <td align="left">Java</td>
              <td align="left">JTI, IS</td>
              <td align="left">EM</td>
            </tr>
            <tr>
              <td align="left">Genie</td>
              <td align="left">U. Pittsburgh [<xref ref-type="bibr" rid="B34">34</xref>]</td>
              <td align="left">N</td>
              <td align="left">JTI</td>
              <td align="left">EM</td>
            </tr>
            <tr>
              <td align="left">GMTk</td>
              <td align="left">Blimes, Zweig [<xref ref-type="bibr" rid="B35">35</xref>]</td>
              <td align="left">N</td>
              <td align="left">JTI</td>
              <td align="left">EM</td>
            </tr>
            <tr>
              <td align="left">Infer.NET</td>
              <td align="left">Winn and Minka [<xref ref-type="bibr" rid="B36">36</xref>]</td>
              <td align="left">C#</td>
              <td align="left">BP, EP, Gibbs, VB</td>
              <td align="left">EP</td>
            </tr>
            <tr>
              <td align="left">JAGS</td>
              <td align="left">Plummer</td>
              <td align="left">C++</td>
              <td align="left">Gibbs</td>
              <td align="left">Gibbs</td>
            </tr>
            <tr>
              <td align="left">Mocapy++</td>
              <td align="left">Paluszewski and Hamelryck</td>
              <td align="left">C++</td>
              <td align="left">Gibbs</td>
              <td align="left">S-EM, MC-EM</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>The abbreviations are N: source code is not freely available, BP: belief propagation, EP: expectation propagation, JTI: junction tree inference, Gibbs: Gibbs sampling, MCMC: Markov chain Monte Carlo, VB: variational Bayes, IS: importance sampling. JAGS is available online from <ext-link ext-link-type="uri" xlink:href="http://www-fis.iarc.fr/~martyn/software/jags/">http://www-fis.iarc.fr/~martyn/software/jags/</ext-link></p>
        </table-wrap-foot>
      </table-wrap>
      <p>In bioinformatics, models are typically trained using large datasets. Some packages in Table <xref ref-type="table" rid="T1">1</xref> only provide exact inference algorithms that are often not suitable for training models with large datasets. Other packages have no or little support for DBNs, which is important for modelling biomolecular structure. To our knowledge none of the publically available open source toolkits support directional statistics, which has recently become of crucial importance for applications in structural bioinformatics such as modelling protein and RNA structure in 3D detail [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>,<xref ref-type="bibr" rid="B12">12</xref>,<xref ref-type="bibr" rid="B15">15</xref>]. Furthermore, Mocapy++ is the only package that uses the stochastic EM [<xref ref-type="bibr" rid="B16">16</xref>-<xref ref-type="bibr" rid="B18">18</xref>] algorithm for parameter learning (see the Materials and Methods section). These features make Mocapy++ an excellent choice for many tasks in bioinformatics and especially structural bioinformatics.</p>
    </sec>
  </sec>
  <sec>
    <title>Implementation</title>
    <p>Mocapy++ is implemented as a program library in C++. The library is highly modular and new node types can be added easily. For object serialization and special functions the Boost C++ library [<xref ref-type="bibr" rid="B19">19</xref>] is used. All relevant objects are serializable, meaning that Mocapy++ can be suspended and later resumed at any state during training or sampling. The LAPACK library [<xref ref-type="bibr" rid="B20">20</xref>] is used for linear algebra routines.</p>
    <p>Mocapy++ uses CMake [<xref ref-type="bibr" rid="B21">21</xref>] to locate packages and configure the build system and can be used either as a static or shared library. The package includes a Doxygen configuration file for HTML formatted documentation of the source code. An example of a Python interface file for SWIG <ext-link ext-link-type="uri" xlink:href="http://www.swig.org">http://www.swig.org</ext-link> is also included in the package.</p>
    <sec>
      <title>Data structures</title>
      <p>Most of the internal data is stored in simple <italic>Standard Template Library </italic>(STL) [<xref ref-type="bibr" rid="B22">22</xref>] data structures. However, STL or other public libraries offer little support for multidimensional arrays when the dimension needs to be set at run-time. In Mocapy++ such a multidimensional array is for example needed to store the <italic>conditional probability table </italic>(CPT) of the discrete nodes. The CPT is a matrix that holds the probabilities of each combination of node and parent values. For example, a discrete node of size 2 with two parents of sizes 3 and 4, respectively, will have a 3 Ã 4 Ã 2 matrix as its CPT. Mocapy++ therefore has its own implementation of a multidimensional array, called MDArray. The MDArray class features dynamic allocation of dimensions and provides various slicing operations. The MDArray is also used for storing the training data and other internal data.</p>
    </sec>
    <sec>
      <title>Specifying a DBN in Mocapy++</title>
      <p>Consider a sequence of observations. Each position in the sequence is characterized by <italic>n </italic>random variables (called a <italic>slice</italic>, see Figure <xref ref-type="fig" rid="F1">1</xref>). Each slice in the sequence can be represented by an ordinary BN, which is duplicated along the sequence as necessary. The sequential dependencies are in turn represented by edges between the consecutive slices. Hence, a DBN in Mocapy++ is defined by three components: a set of nodes that represent all variables for a given slice, the edges between the nodes within a slice (the <italic>intra edges</italic>) and the edges that connect nodes in two consecutive slices (the <italic>inter edges</italic>).</p>
      <fig id="F1" position="float">
        <label>Figure 1</label>
        <caption>
          <p><bold>BARNACLE: a probabilistic model of RNA structure</bold>. A DBN with nine slices is shown, of which one slice is boxed. Nodes <italic>D </italic>and <italic>H </italic>are discrete nodes, while node <italic>A </italic>is a univariate von Mises node. The dihedral angles within one nucleotide <italic>i </italic>are labelled <italic>Î±</italic><sup><italic>i </italic></sup>to <italic>Î¶</italic><sup><italic>i</italic></sup>. BARNACLE is a probabilistic model of the dihedral angles in a stretch of RNA [<xref ref-type="bibr" rid="B12">12</xref>].</p>
        </caption>
        <graphic xlink:href="1471-2105-11-126-1"/>
      </fig>
    </sec>
    <sec>
      <title>Node Types</title>
      <p>Mocapy++ supports several node types, each corresponding to a specific probability distribution. The categorical distribution (discrete node), multinomial (for vectors of counts), Gaussian (uni- and multivariate), von Mises (uni- and bivariate; for data on the circle or the torus, respectively) [<xref ref-type="bibr" rid="B23">23</xref>], Kent (5-parameter Fisher-Bingham; for data on the sphere) [<xref ref-type="bibr" rid="B24">24</xref>] and Poisson distributions are supported. Some node types, such as the bivariate von Mises and Kent nodes, are to our knowledge only available in Mocapy++. The bivariate von Mises and Kent distributions are briefly described here. These distributions belong to the realm of directional statistics, which is concerned with probability distributions on manifolds such as the circle, the sphere or the torus [<xref ref-type="bibr" rid="B23">23</xref>,<xref ref-type="bibr" rid="B25">25</xref>].</p>
      <sec>
        <title>Kent Distribution</title>
        <p>The Kent distribution [<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B24">24</xref>,<xref ref-type="bibr" rid="B26">26</xref>-<xref ref-type="bibr" rid="B29">29</xref>], also known as the 5-parameter Fisher-Bingham distribution, is a distribution on the 2D sphere (the surface of the 3D ball). It is the 2D member of a larger class of <italic>N</italic>-dimensional distributions called the Fisher-Bingham distributions. The density function of the Kent distribution is:<disp-formula><graphic xlink:href="1471-2105-11-126-i1.gif"/></disp-formula></p>
        <p>where <bold>x </bold>is a random 3D unit vector that specifies a point on the 2D sphere.</p>
        <p>The various parameters can be interpreted as follows:</p>
        <p>â¢ <italic>Îº</italic>: a concentration parameter. The concentration of the density increases with <italic>Îº</italic>.</p>
        <p>â¢ <italic>Î²</italic>: determines the ellipticity of the equal probability contours of the distribution. The ellipticity increases with <italic>Î²</italic>. If <italic>Î² </italic>= 0, the Kent distribution becomes the von Mises-Fisher distribution on the 2D sphere.</p>
        <p>â¢ <italic>Î³</italic><sub>1</sub>: the mean direction.</p>
        <p>â¢ <italic>Î³</italic><sub>2</sub>: the main axis of the elliptical equal probability contours.</p>
        <p>â¢ <italic>Î³</italic><sub>3</sub>: the secondary axis of the elliptical equal probability contours.</p>
        <p>The normalizing factor <italic>C</italic>(<italic>Îº</italic>, <italic>Î²</italic>) is approximately given by:<disp-formula><graphic xlink:href="1471-2105-11-126-i2.gif"/></disp-formula></p>
        <p>The Kent distribution can be fully characterized by 5 independent parameters. The concentration and the shape of the equal probability contours are characterized by the <italic>Îº </italic>and <italic>Î² </italic>parameters, respectively. Two angles are sufficient to specify the mean direction on the sphere, and one additional angle fixes the orientation of the elliptical equal probability contours. The latter three angles are in practice specified by the three orthonormal <italic>Î³ </italic>vectors, which form a 3 Ã 3 orthogonal matrix.</p>
        <p>The advantage of the Kent distribution over the von Mises-Fisher distribution on the 2D sphere is that the equal probability contours of the density are not restricted to be circular: they can be elliptical as well. The Kent distribution is equivalent to a Gaussian distribution with unrestricted covariance. Hence, for 2D directional data the Kent distribution is richer than the corresponding von Mises-Fisher distribution, i.e. it is more suited if the data contains non-circular clusters. The Kent distribution is illustrated in Figure <xref ref-type="fig" rid="F2">2</xref>. This distribution was used to formulate FB5HMM [<xref ref-type="bibr" rid="B9">9</xref>], which is a probabilistic model of the local structure of proteins in terms of the <italic>CÎ± </italic>positions.</p>
        <fig id="F2" position="float">
          <label>Figure 2</label>
          <caption>
            <p><bold>Samples from three Kent distributions on the sphere</bold>. The red points were sampled from a distribution with high concentration and high correlation (<italic>Îº </italic>= 1000, <italic>Î² </italic>= 499), the green points were sampled from a distribution with low concentration and no correlation (<italic>Îº </italic>= 10, <italic>Î² </italic>= 0), and the blue points were sampled from a distribution with medium concentration and medium correlation (<italic>Îº </italic>= 200, <italic>Î² </italic>= 50). The distributions underlying the red and green points have the same mean direction and axes and illustrate the effect of <italic>Îº </italic>and <italic>Î²</italic>. For each distribution, 5000 points are sampled.</p>
          </caption>
          <graphic xlink:href="1471-2105-11-126-2"/>
        </fig>
      </sec>
      <sec>
        <title>Bivariate von Mises Distribution</title>
        <p>Another distribution from directional statistics is the bivariate von Mises distribution on the torus [<xref ref-type="bibr" rid="B23">23</xref>]. This distribution can be used to model bivariate angular data. The density function of the bivariate von Mises (cosine variant) distribution is:<disp-formula><graphic xlink:href="1471-2105-11-126-i3.gif"/></disp-formula></p>
        <p>where <italic>C</italic>(<italic>Îº</italic><sub>1</sub>, <italic>Îº</italic><sub>2</sub>, <italic>Îº</italic><sub>3</sub>) is the normalizing factor and <italic>Ï</italic>, <italic>Ï </italic>are random angles in [0, 2<italic>Ï</italic>[. Such an angle pair defines a point on the torus.</p>
        <p>The distribution has 5 parameters:</p>
        <p>â¢ <italic>Î¼ </italic>and <italic>Î½ </italic>are the means for <italic>Ï </italic>and <italic>Ï </italic>respectively.</p>
        <p>â¢ <italic>Îº</italic><sub>1 </sub>and <italic>Îº</italic><sub>2 </sub>are the concentration of <italic>Ï </italic>and <italic>Ï </italic>respectively.</p>
        <p>â¢ <italic>Îº</italic><sub>3 </sub>is related to their correlation.</p>
        <p>This distribution is illustrated in Figure <xref ref-type="fig" rid="F3">3</xref> and described in greater detail in Mardia <italic>et al. </italic>[<xref ref-type="bibr" rid="B23">23</xref>]. The distribution was used by Boomsma <italic>et al. </italic>[<xref ref-type="bibr" rid="B10">10</xref>] to formulate a probabilistic model of the local structure of proteins in atomic detail.</p>
        <fig id="F3" position="float">
          <label>Figure 3</label>
          <caption>
            <p><bold>Samples from three bivariate von Mises distributions on the torus</bold>. The green points were sampled from a distribution with high concentration and no correlation (<italic>Îº</italic><sub>1 </sub>= <italic>Îº</italic><sub>2 </sub>= 100, <italic>Îº</italic><sub>3 </sub>= 0), the blue points were sampled from a distribution with high concentration and negative correlation (<italic>Îº</italic><sub>1 </sub>= <italic>Îº</italic><sub>2 </sub>= 100, <italic>Îº</italic><sub>3 </sub>= 49), and the red points were sampled from a distribution with low concentration and no correlation (<italic>Îº</italic><sub>1 </sub>= <italic>Îº</italic><sub>2 </sub>= 10, <italic>Îº</italic><sub>3 </sub>= 0). For each distribution, 10000 points are sampled.</p>
          </caption>
          <graphic xlink:href="1471-2105-11-126-3"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>Inference and Learning</title>
      <p>Mocapy++ uses a <italic>Markov chain Monte Carlo </italic>(MCMC) technique called Gibbs sampling [<xref ref-type="bibr" rid="B1">1</xref>] to perform inference, i.e. to approximate the probability distribution over the values of the hidden nodes. Sampling methods such as Gibbs sampling are attractive because they allow complicated network architectures and a wide range of probability distributions.</p>
      <p>Parameter learning of a DBN with hidden nodes is done using the <italic>expectation maximization </italic>(EM) method, which provides a maximum likelihood point estimate of the parameters. In the E-step, the values of the hidden nodes are inferred using the current DBN parameters. In the subsequent M-step, the inferred values of the hidden nodes are used to update the parameters of the DBN using maximum likelihood estimation. The E- and M-step cycle is repeated until convergence. Parameter learning using the EM algorithm requires a method to perform inference over the possible hidden node values. If one uses a stochastic procedure to perform the E-step (as in Mocapy++), a stochastic version of the EM algorithm is obtained. There are two reasons to use a stochastic E-step. First, deterministic inference might be intractable. Second, certain stochastic versions of the EM algorithm are more robust than the classic version of EM [<xref ref-type="bibr" rid="B16">16</xref>]. EM algorithms with a stochastic E-step come in two flavors [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B17">17</xref>]. In <italic>Monte Carlo EM </italic>(MC-EM), a large number of samples is generated in the EM step. In <italic>Stochastic EM </italic>(S-EM) [<xref ref-type="bibr" rid="B16">16</xref>-<xref ref-type="bibr" rid="B18">18</xref>] only one sample is generated for each hidden node, and a 'completed' dataset is obtained. In contrast to MC-EM, S-EM has some clear advantages over deterministic EM algorithms: S-EM is less dependent on starting conditions, and has a lower tendency to get stuck at saddle points, or insignificant local maxima. Because only one value needs to be sampled for each hidden node in the E-step, S-EM can also be considerably faster than MC-EM. S-EM is especially suited for large datasets, while for small datasets MC-EM is a better choice. Mocapy++ supports both forms of EM.</p>
    </sec>
  </sec>
  <sec>
    <title>Results and Discussion</title>
    <p>Hamelryck <italic>et al. </italic>[<xref ref-type="bibr" rid="B9">9</xref>] sample realistic protein <italic>CÎ±</italic>-traces using an HMM with a Kent output node. Boomsma <italic>et al. </italic>[<xref ref-type="bibr" rid="B10">10</xref>] extend this model to full atomic detail using the bivariate von Mises distribution [<xref ref-type="bibr" rid="B23">23</xref>]. In both applications, Mocapy was used for parameter estimation and sampling. Zhao <italic>et al. </italic>[<xref ref-type="bibr" rid="B11">11</xref>] used Mocapy for related work. Mocapy has also been used to formulate a probabilistic model of RNA structure [<xref ref-type="bibr" rid="B12">12</xref>] (Figure <xref ref-type="fig" rid="F1">1</xref>) and to infer functional interactions in a biomolecular network [<xref ref-type="bibr" rid="B13">13</xref>].</p>
    <p>To illustrate the speed of Mocapy++, we use three parameter estimation benchmarks and report the execution time on a standard PC (Intel Core 2 Duo, 2.33 GHz). The first benchmark is an HMM with 50 slices and two discrete nodes in each slice (one hidden node and one output node). All nodes have 5 states. The second benchmark is similar, but with a 4-dimensional Gaussian output node and a 10 state hidden node. The third benchmark is more complex and is shown in Figure <xref ref-type="fig" rid="F4">4</xref>.</p>
    <fig id="F4" position="float">
      <label>Figure 4</label>
      <caption>
        <p><bold>The model used in the third benchmark</bold>. Each slice contains two hidden nodes (<italic>H </italic>and <italic>I</italic>). They are parents to a multivariate four-dimensional Gaussian node (<italic>G</italic>) and a bivariate von Mises node (<italic>V</italic>), respectively. The sizes of <italic>H </italic>and <italic>I </italic>are five and three, respectively. The length of the BN is 50 slices.</p>
      </caption>
      <graphic xlink:href="1471-2105-11-126-4"/>
    </fig>
    <p>Using a training set consisting of 200 sequences, 100 iterations of S-EM take 14 seconds for the discrete HMM, 41 seconds for the Gaussian HMM and 195 seconds for the complex BN. The evolution of the log-likelihood during training is shown in Figure <xref ref-type="fig" rid="F5">5</xref>.</p>
    <fig id="F5" position="float">
      <label>Figure 5</label>
      <caption>
        <p><bold>Log-likelihood evolution during S-EM training</bold>. Each column shows the evolution of the log-likelihood for one of the three benchmarks described in the results section. The training procedure was started from two different random seeds (indicated by a solid and a dashed line). The log-likelihood values, log <italic>P </italic>(<italic>D</italic>|<italic>H</italic><sub><italic>n</italic></sub>, <italic>Î¸</italic><sub><italic>n</italic></sub>), used in the upper figures are conditional on the states of the sampled hidden nodes (<italic>Î¸</italic><sub><italic>n </italic></sub>are the parameter values at iteration <italic>n</italic>, <italic>H</italic><sub><italic>n </italic></sub>are the hidden node values at iteration <italic>n </italic>and <italic>D </italic>is the observed data). The log-likelihood values in the lower figures, log <italic>P </italic>(<italic>D</italic>|<italic>Î¸</italic><sub><italic>n</italic></sub>), are computed by summing over all hidden node sequences using the forward algorithm [<xref ref-type="bibr" rid="B5">5</xref>]. Note that the forward algorithm can only be used on HMMs and is therefore not applied on the complex benchmark.</p>
      </caption>
      <graphic xlink:href="1471-2105-11-126-5"/>
    </fig>
    <p>In practice, the most time consuming step in parameter learning is Gibbs sampling of the hidden nodes. The running time for one sweep of Gibbs sampling for a hidden discrete node is <italic>O</italic>(<italic>l </italic>Ã <italic>s</italic>) where <italic>l </italic>is the total number of slices in the data and <italic>s </italic>is the size of the node. The largest model that, to our knowledge, has been successfully trained with Mocapy++ is an extension of TorusDBN [<xref ref-type="bibr" rid="B10">10</xref>]. The dataset consisted of 9059 sequences with a total of more than 1.5 million slices. The model has 11897 parameters and one EM-iteration takes 860 seconds. The number of S-EM iterations needed for likelihood convergence is around 100.</p>
    <p>Toolkits for inference and learning in Bayesian networks use many different algorithms and are implemented in a variety of computer languages (Matlab, R, Java,...); comparisons are thus necessarily unfair or even irrelevant. Therefore, we feel it suffices to point out that Mocapy++ has some unique features (such as the support for directional statistics), and that the benchmarks clearly show that its performance is more than satisfactory for real life problems - both with respect to speed and data set size.</p>
    <sec>
      <title>Future Directions of Mocapy++</title>
      <p>The core of Mocapy++ described here is not expected to change much in future versions of Mocapy++. However, Mocapy++ is an evolving project with room for new features and additions. We therefore encourage people to propose their ideas for improvements and to participate in the development of Mocapy++. Potential directions include:</p>
      <p>â¢ Additional probability distributions</p>
      <p>â¢ Structure learning</p>
      <p>â¢ Graphical user interface</p>
      <p>â¢ Plugins for reading data in various formats</p>
    </sec>
  </sec>
  <sec>
    <title>Conclusions</title>
    <p>Mocapy++ has a number of attractive features that are not found together in other toolkits [<xref ref-type="bibr" rid="B14">14</xref>]: it is open source, implemented in C++ for optimal speed efficiency and supports directional statistics. This branch of statistics deals with data on unusual manifolds such as the sphere or the torus [<xref ref-type="bibr" rid="B25">25</xref>], which is particularly useful to formulate probabilistic models of biomolecular structure in atomic detail [<xref ref-type="bibr" rid="B9">9</xref>-<xref ref-type="bibr" rid="B12">12</xref>]. Finally, the use of S-EM for parameter estimation avoids problems with convergence [<xref ref-type="bibr" rid="B16">16</xref>,<xref ref-type="bibr" rid="B17">17</xref>] and allows for the use of large datasets, which are particularly common in bioinformatics. In conclusion, Mocapy++ provides a powerful machine learning tool to tackle a large range of problems in bioinformatics.</p>
  </sec>
  <sec>
    <title>Availability and Requirements</title>
    <p>â¢ Project name: Mocapy++</p>
    <p>â¢ Project home page: <ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/mocapy">http://sourceforge.net/projects/mocapy</ext-link></p>
    <p>â¢ Operating system(s): Linux, Unix, Mac OS X, Windows with Cygwin</p>
    <p>â¢ Programming language: C++</p>
    <p>â¢ Other requirements: Boost, CMake and LAPACK, GNU Fortran</p>
    <p>â¢ License: GNU GPL</p>
  </sec>
  <sec>
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec>
    <title>Authors' contributions</title>
    <p>TH designed and implemented Mocapy in Python. MP designed and implemented Mocapy++. MP drafted the manuscript and TH revised the manuscript. Both authors read and approved the final manuscript.</p>
  </sec>
</body>
<back>
  <sec>
    <title>Acknowledgements</title>
    <p>The authors thank the colleagues at the Bioinformatics centre who have helped in the development of Mocapy++: Christian Andreetta, Wouter Boomsma, Mikael Borg, Jes Frellsen, Tim Harder and Kasper Stovgaard. We also thank John T. Kent and Kanti Mardia, University of Leeds, UK and Jesper Ferkinghoff-Borg, Technical University of Denmark for helpful discussions. We acknowledge funding from the Danish Council for Strategic Research <italic>(Program Commission on Nanoscience, Biotechnology and IT, Project: simulating proteins on a millisecond time scale, 2106-06-0009)</italic>.</p>
  </sec>
  <ref-list>
    <ref id="B1">
      <mixed-citation publication-type="book">
        <name>
          <surname>Bishop</surname>
          <given-names>CM</given-names>
        </name>
        <source>Pattern recognition and machine learning</source>
        <year>2006</year>
        <publisher-name>Springer</publisher-name>
      </mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="book">
        <name>
          <surname>Pearl</surname>
          <given-names>J</given-names>
        </name>
        <source>Probabilistic reasoning in intelligent systems: networks of plausible inference</source>
        <year>1997</year>
        <publisher-name>Morgan Kaufmann</publisher-name>
      </mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Ghahramani</surname>
          <given-names>Z</given-names>
        </name>
        <article-title>Learning dynamic Bayesian networks</article-title>
        <source>Lect Notes Comp Sci</source>
        <year>1998</year>
        <volume>1387</volume>
        <fpage>168</fpage>
        <lpage>197</lpage>
        <comment>full_text</comment>
      </mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Rabiner</surname>
          <given-names>LR</given-names>
        </name>
        <article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title>
        <source>Proc IEEE</source>
        <year>1989</year>
        <volume>77</volume>
        <issue>2</issue>
        <fpage>257</fpage>
        <lpage>286</lpage>
        <pub-id pub-id-type="doi">10.1109/5.18626</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book">
        <name>
          <surname>Durbin</surname>
          <given-names>R</given-names>
        </name>
        <name>
          <surname>Eddy</surname>
          <given-names>SR</given-names>
        </name>
        <name>
          <surname>Krogh</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Mitchison</surname>
          <given-names>G</given-names>
        </name>
        <source>Biological sequence analysis</source>
        <year>1999</year>
        <publisher-name>Cambridge University Press</publisher-name>
      </mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Raval</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Ghahramani</surname>
          <given-names>Z</given-names>
        </name>
        <name>
          <surname>Wild</surname>
          <given-names>DL</given-names>
        </name>
        <article-title>A Bayesian network model for protein fold and remote homologue recognition</article-title>
        <source>Bioinformatics</source>
        <year>2002</year>
        <volume>18</volume>
        <issue>6</issue>
        <fpage>788</fpage>
        <lpage>801</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/18.6.788</pub-id>
        <pub-id pub-id-type="pmid">12075014</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Schmidler</surname>
          <given-names>SC</given-names>
        </name>
        <name>
          <surname>Liu</surname>
          <given-names>JS</given-names>
        </name>
        <name>
          <surname>Brutlag</surname>
          <given-names>DL</given-names>
        </name>
        <article-title>Bayesian segmentation of protein secondary structure</article-title>
        <source>J Comp Biol</source>
        <year>2000</year>
        <volume>7</volume>
        <issue>1-2</issue>
        <fpage>233</fpage>
        <lpage>248</lpage>
        <pub-id pub-id-type="doi">10.1089/10665270050081496</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Chu</surname>
          <given-names>W</given-names>
        </name>
        <name>
          <surname>Ghahramani</surname>
          <given-names>Z</given-names>
        </name>
        <name>
          <surname>Podtelezhnikov</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Wild</surname>
          <given-names>DL</given-names>
        </name>
        <article-title>Bayesian segmental models with multiple sequence alignment profiles for protein secondary structure and contact map prediction</article-title>
        <source>IEEE/ACM Trans Comp Biol Bioinf</source>
        <year>2006</year>
        <volume>3</volume>
        <issue>2</issue>
        <fpage>98</fpage>
        <lpage>113</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2006.17</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Hamelryck</surname>
          <given-names>T</given-names>
        </name>
        <name>
          <surname>Kent</surname>
          <given-names>JT</given-names>
        </name>
        <name>
          <surname>Krogh</surname>
          <given-names>A</given-names>
        </name>
        <article-title>Sampling realistic protein conformations using local structural bias</article-title>
        <source>PLoS Comp Biol</source>
        <year>2006</year>
        <volume>2</volume>
        <issue>9</issue>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.0020131</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Boomsma</surname>
          <given-names>W</given-names>
        </name>
        <name>
          <surname>Mardia</surname>
          <given-names>KV</given-names>
        </name>
        <name>
          <surname>Taylor</surname>
          <given-names>CC</given-names>
        </name>
        <name>
          <surname>Ferkinghoff-Borg</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Krogh</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Hamelryck</surname>
          <given-names>T</given-names>
        </name>
        <article-title>A generative, probabilistic model of local protein structure</article-title>
        <source>Proc Natl Acad Sci USA</source>
        <year>2008</year>
        <volume>105</volume>
        <issue>26</issue>
        <fpage>8932</fpage>
        <lpage>8937</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0801715105</pub-id>
        <pub-id pub-id-type="pmid">18579771</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Zhao</surname>
          <given-names>F</given-names>
        </name>
        <name>
          <surname>Li</surname>
          <given-names>S</given-names>
        </name>
        <name>
          <surname>Sterner</surname>
          <given-names>BW</given-names>
        </name>
        <name>
          <surname>Xu</surname>
          <given-names>J</given-names>
        </name>
        <article-title>Discriminative learning for protein conformation sampling</article-title>
        <source>Prot Struct Func Bioinf</source>
        <year>2008</year>
        <volume>73</volume>
        <fpage>228</fpage>
        <lpage>240</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.22057</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Frellsen</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Moltke</surname>
          <given-names>I</given-names>
        </name>
        <name>
          <surname>Thiim</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Mardia</surname>
          <given-names>KV</given-names>
        </name>
        <name>
          <surname>Ferkinghoff-Borg</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Hamelryck</surname>
          <given-names>T</given-names>
        </name>
        <article-title>A probabilistic model of RNA conformational space</article-title>
        <source>PLoS Comp Biol</source>
        <year>2009</year>
        <volume>5</volume>
        <issue>6</issue>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000406</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Kwang-Hyun</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>Hyung-Seok</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>Sang-Mok</surname>
          <given-names>C</given-names>
        </name>
        <article-title>Unraveling the functional interaction structure of a biomolecular network through alternate perturbation of initial conditions</article-title>
        <source>J Biochem Biophys Met</source>
        <year>2007</year>
        <volume>70</volume>
        <issue>4</issue>
        <fpage>701</fpage>
        <lpage>707</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbbm.2007.01.008</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Murphy</surname>
          <given-names>KP</given-names>
        </name>
        <article-title>Software for graphical models: A review</article-title>
        <source>Int Soc Bayesian Anal Bull</source>
        <year>2007</year>
        <volume>14</volume>
        <issue>4</issue>
        <fpage>13</fpage>
        <lpage>15</lpage>
      </mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Hamelryck</surname>
          <given-names>T</given-names>
        </name>
        <article-title>Probabilistic models and machine learning in structural bioinformatics</article-title>
        <source>Stat Met Med Res</source>
        <year>2009</year>
        <volume>18</volume>
        <issue>5</issue>
        <fpage>505</fpage>
        <lpage>526</lpage>
        <pub-id pub-id-type="doi">10.1177/0962280208099492</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Nielsen</surname>
          <given-names>SF</given-names>
        </name>
        <article-title>The stochastic EM algorithm: estimation and asymptotic results</article-title>
        <source>Bernoulli</source>
        <year>2000</year>
        <volume>6</volume>
        <issue>3</issue>
        <fpage>457</fpage>
        <lpage>489</lpage>
        <pub-id pub-id-type="doi">10.2307/3318671</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book">
        <name>
          <surname>Gilks</surname>
          <given-names>WR</given-names>
        </name>
        <name>
          <surname>Richardson</surname>
          <given-names>S</given-names>
        </name>
        <name>
          <surname>Spiegelhalter</surname>
          <given-names>D</given-names>
        </name>
        <source>Markov chain Monte Carlo in practice</source>
        <year>1995</year>
        <publisher-name>Chapman &amp; Hall/CRC</publisher-name>
      </mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Celeux</surname>
          <given-names>G</given-names>
        </name>
        <name>
          <surname>Diebolt</surname>
          <given-names>J</given-names>
        </name>
        <article-title>The SEM algorithm: a probabilistic teacher algorithm derived from the EM algorithm for the mixture problem</article-title>
        <source>Comp Stat Quart</source>
        <year>1985</year>
        <volume>2</volume>
        <fpage>73</fpage>
        <lpage>92</lpage>
      </mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book">
        <name>
          <surname>Abrahams</surname>
          <given-names>D</given-names>
        </name>
        <name>
          <surname>Gurtovoy</surname>
          <given-names>A</given-names>
        </name>
        <source>C++ template metaprogramming: concepts, tools, and techniques from Boost and beyond</source>
        <year>2004</year>
        <publisher-name>Addison-Wesley Professional</publisher-name>
      </mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="other">
        <name>
          <surname>Angerson</surname>
          <given-names>E</given-names>
        </name>
        <name>
          <surname>Bai</surname>
          <given-names>Z</given-names>
        </name>
        <name>
          <surname>Dongarra</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Greenbaum</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Mckenney</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Du Croz</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Hammarling</surname>
          <given-names>S</given-names>
        </name>
        <name>
          <surname>Demmel</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Bischof</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>Sorensen</surname>
          <given-names>D</given-names>
        </name>
        <article-title>LAPACK: A portable linear algebra library for high-performance computers</article-title>
        <source>Proc Supercomp '90</source>
        <year>1990</year>
        <fpage>2</fpage>
        <lpage>11</lpage>
        <comment>full_text</comment>
      </mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="other">
        <name>
          <surname>Wojtczyk</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Knoll</surname>
          <given-names>A</given-names>
        </name>
        <article-title>A cross platform development workflow for C/C++ applications</article-title>
        <source>ICSEA '08</source>
        <year>2008</year>
        <fpage>224</fpage>
        <lpage>229</lpage>
      </mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="book">
        <name>
          <surname>Musser</surname>
          <given-names>DR</given-names>
        </name>
        <name>
          <surname>Saini</surname>
          <given-names>A</given-names>
        </name>
        <source>STL: Tutorial and Reference Guide: C++ Programming with the Standard Template Library</source>
        <year>1996</year>
        <publisher-name>Addison-Wesley Professional Computing Series</publisher-name>
      </mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Mardia</surname>
          <given-names>KV</given-names>
        </name>
        <name>
          <surname>Taylor</surname>
          <given-names>CC</given-names>
        </name>
        <name>
          <surname>Subramaniam</surname>
          <given-names>G</given-names>
        </name>
        <article-title>Protein bioinformatics and mixtures of bivariate von Mises distributions for angular data</article-title>
        <source>Biometrics</source>
        <year>2007</year>
        <volume>63</volume>
        <issue>2</issue>
        <fpage>505</fpage>
        <lpage>512</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1541-0420.2006.00682.x</pub-id>
        <pub-id pub-id-type="pmid">17688502</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Kent</surname>
          <given-names>JT</given-names>
        </name>
        <article-title>The Fisher-Bingham distribution on the sphere</article-title>
        <source>J Roy Stat Soc</source>
        <year>1982</year>
        <volume>44</volume>
        <fpage>71</fpage>
        <lpage>80</lpage>
      </mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="book">
        <name>
          <surname>Mardia</surname>
          <given-names>KV</given-names>
        </name>
        <name>
          <surname>Jupp</surname>
          <given-names>PE</given-names>
        </name>
        <source>Directional statistics</source>
        <year>2000</year>
        <publisher-name>Wiley</publisher-name>
      </mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Leong</surname>
          <given-names>P</given-names>
        </name>
        <name>
          <surname>Carlile</surname>
          <given-names>S</given-names>
        </name>
        <article-title>Methods for spherical data analysis and visualization</article-title>
        <source>J Neurosci Met</source>
        <year>1998</year>
        <volume>80</volume>
        <fpage>191</fpage>
        <lpage>200</lpage>
        <pub-id pub-id-type="doi">10.1016/S0165-0270(97)00201-X</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Peel</surname>
          <given-names>D</given-names>
        </name>
        <name>
          <surname>Whiten</surname>
          <given-names>W</given-names>
        </name>
        <name>
          <surname>McLachlan</surname>
          <given-names>G</given-names>
        </name>
        <article-title>Fitting mixtures of Kent distributions to aid in joint set identification</article-title>
        <source>J Am Stat Ass</source>
        <year>2001</year>
        <volume>96</volume>
        <fpage>56</fpage>
        <lpage>63</lpage>
        <pub-id pub-id-type="doi">10.1198/016214501750332974</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="book">
        <name>
          <surname>Kent</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Hamelryck</surname>
          <given-names>T</given-names>
        </name>
        <person-group person-group-type="editor">Barber S, Baxter P, Mardia K, Walls R</person-group>
        <article-title>Using the Fisher-Bingham distribution in stochastic models for protein structure</article-title>
        <source>Quantitative Biology, Shape Analysis, and Wavelets</source>
        <year>2005</year>
        <volume>24</volume>
        <publisher-name>Leeds University Press</publisher-name>
        <fpage>57</fpage>
        <lpage>60</lpage>
      </mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="book">
        <name>
          <surname>Boomsma</surname>
          <given-names>W</given-names>
        </name>
        <name>
          <surname>Kent</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Mardia</surname>
          <given-names>K</given-names>
        </name>
        <name>
          <surname>Taylor</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>Hamelryck</surname>
          <given-names>T</given-names>
        </name>
        <person-group person-group-type="editor">Barber S, Baxter P, Mardia K, Walls R</person-group>
        <article-title>Graphical models and directional statistics capture protein structure</article-title>
        <source>Interdisciplinary Statistics and Bioinformatics</source>
        <year>2006</year>
        <volume>25</volume>
        <publisher-name>Leeds University Press</publisher-name>
        <fpage>91</fpage>
        <lpage>94</lpage>
      </mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Raiko</surname>
          <given-names>T</given-names>
        </name>
        <name>
          <surname>Valpola</surname>
          <given-names>H</given-names>
        </name>
        <name>
          <surname>Harva</surname>
          <given-names>M</given-names>
        </name>
        <name>
          <surname>Karhunen</surname>
          <given-names>J</given-names>
        </name>
        <article-title>Building blocks for variational Bayesian learning of latent variable models</article-title>
        <source>J Mach Learn Res</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>155</fpage>
        <lpage>201</lpage>
      </mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Murphy</surname>
          <given-names>KP</given-names>
        </name>
        <article-title>The Bayes net toolbox for MATLAB</article-title>
        <source>Comp Sci Stat</source>
        <year>2001</year>
        <volume>33</volume>
        <fpage>2001</fpage>
      </mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Lunn</surname>
          <given-names>DJ</given-names>
        </name>
        <name>
          <surname>Thomas</surname>
          <given-names>A</given-names>
        </name>
        <name>
          <surname>Best</surname>
          <given-names>N</given-names>
        </name>
        <name>
          <surname>Spiegelhalter</surname>
          <given-names>D</given-names>
        </name>
        <article-title>WinBUGS - A Bayesian modelling framework: Concepts, structure, and extensibility</article-title>
        <source>Stat Comp</source>
        <year>2000</year>
        <volume>10</volume>
        <issue>4</issue>
        <fpage>325</fpage>
        <lpage>337</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1008929526011</pub-id>
      </mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="book">
        <name>
          <surname>Lacave</surname>
          <given-names>C</given-names>
        </name>
        <name>
          <surname>Atienza</surname>
          <given-names>R</given-names>
        </name>
        <name>
          <surname>DÃ­ez</surname>
          <given-names>FJ</given-names>
        </name>
        <article-title>Graphical explanation in Bayesian networks</article-title>
        <source>ISMDA '00: Proceedings of the First International Symposium on Medical Data Analysis</source>
        <year>2000</year>
        <publisher-name>London, UK: Springer-Verlag</publisher-name>
        <fpage>122</fpage>
        <lpage>129</lpage>
      </mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="book">
        <name>
          <surname>Druzdzel</surname>
          <given-names>MJ</given-names>
        </name>
        <source>SMILE: Structural modeling, inference, and learning engine and GeNIE: A development environment for graphical decision-theoretic models</source>
        <year>1999</year>
        <publisher-name>AAAI/IAAI</publisher-name>
        <fpage>902</fpage>
        <lpage>903</lpage>
      </mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal">
        <name>
          <surname>Bilmes</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Zweig</surname>
          <given-names>G</given-names>
        </name>
        <article-title>The graphical models toolkit: An open source software system for speech and time-series processing</article-title>
        <source>IEEE ICASSP</source>
        <year>2002</year>
        <volume>4</volume>
        <fpage>3916</fpage>
        <lpage>3919</lpage>
      </mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book">
        <name>
          <surname>Minka</surname>
          <given-names>T</given-names>
        </name>
        <name>
          <surname>Winn</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Guiver</surname>
          <given-names>J</given-names>
        </name>
        <name>
          <surname>Kannan</surname>
          <given-names>A</given-names>
        </name>
        <source>Infer.NET 2.2</source>
        <year>1999</year>
        <publisher-name>Microsoft Research Cambridge</publisher-name>
        <ext-link ext-link-type="uri" xlink:href="http://research.microsoft.com/infernet">http://research.microsoft.com/infernet</ext-link>
      </mixed-citation>
    </ref>
  </ref-list>
</back>
