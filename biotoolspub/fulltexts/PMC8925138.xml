<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8925138</article-id>
    <article-id pub-id-type="publisher-id">4628</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-04628-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A-Prot: protein structure modeling using MSA transformer</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Hong</surname>
          <given-names>Yiyu</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Lee</surname>
          <given-names>Juyong</given-names>
        </name>
        <address>
          <email>juyong.lee@kangwon.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ko</surname>
          <given-names>Junsu</given-names>
        </name>
        <address>
          <email>junsuko@arontier.co</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Arontier Co, Seoul, Republic of Korea </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.412010.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0707 9039</institution-id><institution>Department of Chemistry, Division of Chemistry and Biochemistry, </institution><institution>Kangwon National University, </institution></institution-wrap>Chuncheon, Republic of Korea </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>93</elocation-id>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>3</day>
        <month>3</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The accuracy of protein 3D structure prediction has been dramatically improved with the help of advances in deep learning. In the recent CASP14, Deepmind demonstrated that their new version of AlphaFold (AF) produces highly accurate 3D models almost close to experimental structures. The success of AF shows that the multiple sequence alignment of a sequence contains rich evolutionary information, leading to accurate 3D models. Despite the success of AF, only the prediction code is open, and training a similar model requires a vast amount of computational resources. Thus, developing a lighter prediction model is still necessary.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this study, we propose a new protein 3D structure modeling method, A-Prot, using MSA Transformer, one of the state-of-the-art protein language models. An MSA feature tensor and row attention maps are extracted and converted into 2D residue-residue distance and dihedral angle predictions for a given MSA. We demonstrated that A-Prot predicts long-range contacts better than the existing methods. Additionally, we modeled the 3D structures of the free modeling and hard template-based modeling targets of CASP14. The assessment shows that the A-Prot models are more accurate than most top server groups of CASP14.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">These results imply that A-Prot accurately captures the evolutionary and structural information of proteins with relatively low computational cost. Thus, A-Prot can provide a clue for the development of other protein property prediction methods.</p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-022-04628-8.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Protein structure prediction</kwd>
      <kwd>Multiple sequence alignment</kwd>
      <kwd>Protein language model</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003725</institution-id>
            <institution>National Research Foundation of Korea</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2018R1C1B600543513</award-id>
        <award-id>2019M3E5D4066898</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lee</surname>
            <given-names>Juyong</given-names>
          </name>
          <name>
            <surname>Ko</surname>
            <given-names>Junsu</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par8">Modeling the 3D structure of a protein from its sequence has been one of the most critical problems in biophysics and biochemistry [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. The knowledge of the 3D structure of a protein facilitates the discovery of novel ligands, function annotation, and protein engineering. Due to its importance, the community of computational protein scientists have been developing various prediction methods and assessed their performance in the large-scale blind tests, CASPs, which have continued for over two decades [<xref ref-type="bibr" rid="CR2">2</xref>]. In CASP14, Deepmind demonstrated that their deep learning-based model, AlphaFold2 (AF2), predicts the 3D structures of proteins from their sequences with extremely high accuracy, comparable to experimental accuracy [<xref ref-type="bibr" rid="CR3">3</xref>]. The source code of AF2 became publicly available recently, and many model structures of genes of biologically important organisms have been released [<xref ref-type="bibr" rid="CR3">3</xref>].</p>
    <p id="Par9">The success of AF2 can be attributed to the accurate extraction of coevolutionary information from multiple sequence alignments (MSA). The idea of using coevolutionary information for contact prediction or structure modeling has been widely used [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR12">12</xref>]. However, previous attempts were not successful enough because coevolutionary signals in MSAs were not strong enough or too noisy. Various statistical mechanics-based models were proposed, but their accuracy, discriminating actual contacts from false contacts, was limited. In CASP13, AlphaFold and RaptorX used coevolutionary information from MSAs as input and predicted inter-residue distances [<xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR15">15</xref>]. They achieved significant improvements than previous CASPs, but their predictions were not accurate enough to obtain model structures comparable to experiments. Finally, in CASP14, with the help of the attention algorithm, truly accurate extraction of actual residue-residue contact signals from MSAs becomes available [<xref ref-type="bibr" rid="CR3">3</xref>].</p>
    <p id="Par10">Despite this remarkable achievement of AF2, certain limitations remain for this technology to be widely accessible and used for further development. First, the source code for training AF2 is not open-sourced yet. In the first release of AF2, only the production, model generation, part of AF2, and the model parameters are open. Second, training the AF2 architecture is computationally expensive. It is reported that Deepmind used 128 TPUv3 cores for approximately one week and four more days to train AF2. This amount of computational resources are not readily accessible for most academic groups. Thus, developing lighter and general models is still necessary.</p>
    <p id="Par11">To extract evolutionary information from MSAs, various protein language models have been proposed [<xref ref-type="bibr" rid="CR16">16</xref>–<xref ref-type="bibr" rid="CR22">22</xref>]. Rao et al. proposed the MSA transformer model, an unsupervised protein language model using the MSA of a query sequence instead of a single query sequence. The model uses row and column attention of input MSAs and masked language modeling objectives. It is demonstrated that the model successfully predicted long-range contacts between residues. In addition, the model predicted other properties of proteins, such as secondary structure prediction and mutational effects, with high accuracy [<xref ref-type="bibr" rid="CR20">20</xref>]. These results indicate that the MSA Transformer model extracts the characteristics of proteins from their MSA profiles efficiently.</p>
    <p id="Par12">This study developed a new protein 3D structure prediction method, A-Prot, using MSA Transformer [<xref ref-type="bibr" rid="CR22">22</xref>]. For a given MSA, we extracted evolutionary information with MSA Transformer. The extracted row attention map and input features were converted to a 2D residue-residue distance map and dihedral angle predictions. We benchmarked the 3D protein structure modeling performance using the FM/TBM-hard targets of CASP13 and 14 [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>]. The results show that A-Prot outperforms most top server groups of CASP13 and 14 in terms of long-range contact predictions and 3D protein structure modeling.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Overview</title>
      <p id="Par13">The overall pipeline of the proposed protein 3D structure prediction method is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. We mainly combine the works of the MSA Transformer [<xref ref-type="bibr" rid="CR22">22</xref>] and the trRosetta [<xref ref-type="bibr" rid="CR25">25</xref>]. Given an MSA, it will input to the MSA Transformer to output MSA features and row attention maps. After a series of transformations like dimension reduction and concatenation etc., these two kinds of features will be transformed and combined into 2D feature maps, which is suitable for input to the trRosetta to finally output protein 3D structure.<fig id="Fig1"><label>Fig. 1</label><caption><p>Pipeline of the proposed protein 3D structure prediction method. Input an MSA to the MSA Transformer to extract MSA Features and row attention maps. Then, the MSA features corresponding to the query sequence and the row attention maps are combined to a 2D feature maps by a set of transformations. Next, the 2D feature maps are input to a Dilated ResNet after dimension reduction to output inter-residue geometries, which further input to the trRosetta Protein Structure Modeling to output a predicted protein 3D structure</p></caption><graphic xlink:href="12859_2022_4628_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par14">The MSA Transformer used in this paper was an already pre-trained version that was learned from 26 million MSAs. It plays a role as a feature extractor that inputs an MSA and outputs its relative features.</p>
      <p id="Par15">trRosetta consists of a deep neural network part and protein structure modeling part. The deep neural network is modified to receive the MSA features from the MSA Transformer instead of the original manually engineered features generated by the statistic approach. Nevertheless, the protein structure modeling part remains the same.</p>
    </sec>
    <sec id="Sec4">
      <title>Dataset and MSA generation</title>
      <p id="Par16">We used the procedure with MSA transformer to generate the MSAs of target sequences (Rao, Liu, et al., 2021). The MSA of a query sequence was generated using Jackhammer [<xref ref-type="bibr" rid="CR26">26</xref>], HHblits ver. 3.3.0, and HHsearch [<xref ref-type="bibr" rid="CR27">27</xref>] together with the unicluster30_2017_10 [<xref ref-type="bibr" rid="CR28">28</xref>] and BFD [<xref ref-type="bibr" rid="CR29">29</xref>] databases. If not specified, all training and test MSAs are used BFD. If the number of detected homologous sequences exceeds 256, up to 256 sequences were selected through diversity minimization [<xref ref-type="bibr" rid="CR22">22</xref>]. The upper limit of the number of sequences was determined by the maximum size of GPU memory of the NVIDIA Quadro RTX 8000 GPU (48 GB) card.</p>
      <p id="Par17">To perform contact predictions, a customized non-redundant protein structure database is compiled, and we named it PDB30. The PDB30 dataset consists of protein structures deposited in PDB before Apr-30–2018 whose resolution is higher than 2.5 A and sequence length is longer than 40 amino acids. Using 102,300 sequences that satisfy the condition, clustering analysis was performed using MMSeq2 [<xref ref-type="bibr" rid="CR30">30</xref>] with a threshold of sequence identity of 30%, leading to 16,612 non-redundant sequences.</p>
    </sec>
    <sec id="Sec5">
      <title>Network architecture</title>
      <p id="Par18">Let us define an input MSA as an <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r \times c$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq1.gif"/></alternatives></inline-formula> character matrix, where <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r$$\end{document}</tex-math><mml:math id="M4"><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq2.gif"/></alternatives></inline-formula> and <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c$$\end{document}</tex-math><mml:math id="M6"><mml:mi>c</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq3.gif"/></alternatives></inline-formula> are rows (number of sequences) and columns (sequence length) in the MSA, respectively. Through the token and position embedding of the MSA Transformer, the input MSA is embedded into a <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r \times c \times 768$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq4.gif"/></alternatives></inline-formula> tensor which is the input and output of each attention block. The attention block is composed in the order of row attention layer, column attention layer, and feed-forward layer. A layer normalization operation is followed by each layer. And each attention layer has 12 attention heads. The MSA Transformer is a stack of 12 such attention blocks.</p>
      <p id="Par19">Two kinds of features were extracted from the MSA Transformer to construct 2D feature maps by a series of transformations. (1) One is the last attention block’s output, a <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r \times c \times 768$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq5.gif"/></alternatives></inline-formula> tensor; we named it MSA features (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Only features corresponding to the query sequence are selected, which is a <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times c \times 768$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mn>768</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq6.gif"/></alternatives></inline-formula> tensor. Then, the dimension of this feature is reduced to 128 by an MLP (multi-layer perceptron) consist of 3 linear layers with neuron sizes 384, 192, 128. The dimension reduced 1D feature is then outer concatenated (redundantly expanding horizontally and vertically and then stacked together) to form a query sequence feature of a <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c \times c \times 256$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq7.gif"/></alternatives></inline-formula> tensor. (2) The other one is row attention maps that are derived from each attention head of each row attention layer, totally <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$12 \times 12 = 144$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mn>12</mml:mn><mml:mo>×</mml:mo><mml:mn>12</mml:mn><mml:mo>=</mml:mo><mml:mn>144</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq8.gif"/></alternatives></inline-formula> attention maps stacked to shape in <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c \times c \times 144$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mn>144</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq9.gif"/></alternatives></inline-formula>. Then it is symmetrized by adding it to its transposed tensor to yield symmetrized row attention maps. The query sequence feature and the symmetrized row attention maps are concatenated in the feature map dimension to form a 2D feature map that is a <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c \times c \times 400$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mn>400</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq10.gif"/></alternatives></inline-formula> tensor.</p>
      <p id="Par20">The dimension of the 2D feature maps afterward reduced to 64 by three convolutional layers that consist of 256, 128, 64 kernels of size <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 1$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq11.gif"/></alternatives></inline-formula> [<xref ref-type="bibr" rid="CR31">31</xref>], where each convolutional layer is followed by an instance normalization and a ReLU activation. Then this <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c \times c \times 64$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mo>×</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq12.gif"/></alternatives></inline-formula> tensor is input to the Dilated ResNet consist of 28 residual blocks [<xref ref-type="bibr" rid="CR32">32</xref>], each having one instance normalization, two ReLU activations, and two convolutional layers each with 64 kernels of size <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3 \times 3$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq13.gif"/></alternatives></inline-formula>. Dilation is applied to the convolutional layers cycle through the residual blocks with rates of 1, 2, 4. After the last residual block, there are four independent convolutional layers with 25, 13, 25, 37 kernels of size <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1 \times 1$$\end{document}</tex-math><mml:math id="M28"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq14.gif"/></alternatives></inline-formula>, each for predicting inter-residue geometries of <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta ,{ }\varphi ,{ }\omega ,{\text{ d}}$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mrow/><mml:mi>φ</mml:mi><mml:mo>,</mml:mo><mml:mrow/><mml:mi>ω</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mspace width="0.333333em"/><mml:mtext>d</mml:mtext></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq15.gif"/></alternatives></inline-formula>, respectively. Please refer to the reference [<xref ref-type="bibr" rid="CR25">25</xref>] for more details about the inter-residue geometries as we used the same settings. Finally, the trRosetta Protein Structure Modeling module will predict and modeling the protein 3D structure based on the inter-residue geometries information.</p>
    </sec>
    <sec id="Sec6">
      <title>Training and inference</title>
      <p id="Par21">At the training stage, we fixed the parameters in the MSA Transformer. In contrast, parameters in the other deep neural networks were trained with a batch size of 16 with gradient accumulation steps, a learning rate of 1e − 3, using the RAdam optimizer [<xref ref-type="bibr" rid="CR33">33</xref>], the categorical cross-entropy loss was calculated with equal weight for the four inter-residue geometry objectives [<xref ref-type="bibr" rid="CR25">25</xref>]. The ground truth values for the inter-residue geometries are all discretized into bins which have same number as corresponding convolutional kernels (<inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta$$\end{document}</tex-math><mml:math id="M32"><mml:mi>θ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq16.gif"/></alternatives></inline-formula> for 25 bins, <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi$$\end{document}</tex-math><mml:math id="M34"><mml:mi>φ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq17.gif"/></alternatives></inline-formula> for 13 bins, <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega$$\end{document}</tex-math><mml:math id="M36"><mml:mi>ω</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq18.gif"/></alternatives></inline-formula> for 25 bins, <inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{d}}$$\end{document}</tex-math><mml:math id="M38"><mml:mtext>d</mml:mtext></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq19.gif"/></alternatives></inline-formula> for 37 bins); each bin is treated as a classification label. The total model was trained end-to-end on an NVIDIA Quadro RTX 8000 GPU (48 GB) for around 40 epochs which took about five days.</p>
      <p id="Par22">An MSA subsampling strategy is applied during training, not only for regarding as data augmentation to train a robust model, but also for preventing the GPU from running out of memory when filled with large MSA. We randomly select MSA rows, up to a maximum of <inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2^{14} /c$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>14</mml:mn></mml:msup><mml:mo stretchy="false">/</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq20.gif"/></alternatives></inline-formula>, and down to a minimum of 16, though always including the query sequences in the first row. Large proteins of more than 1023 residues long were discarded during training. We subsampled MSA with 256 sequences at the inference stage by adding the sequence with the lowest average hamming distance. We performed trRosetta protein structure modeling five times with the same input and selected the structure with the lowest energy for protein structure similarity measurement.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Results and discussion</title>
    <sec id="Sec8">
      <title>Benchmarking contact prediction</title>
      <p id="Par23">First, we benchmarked the long-range contact prediction performance of A-Prot using the FM and FM/TBM targets of CASP13 [<xref ref-type="bibr" rid="CR24">24</xref>]. The benchmark results show that the performance of our model outperforms that of the existing methods (Table <xref rid="Tab1" ref-type="table">1</xref>). We compared the precision of our model's top L/5, L/2, long-range contact predictions (long-range: sequence separation of the residue pair ≥ 24) and the other existing methods. The performance measures of the other methods are adopted from the reference [<xref ref-type="bibr" rid="CR34">34</xref>]. The top L/5, Top L/2, and L contact precisions of our model are 0.812, 0.710, and 0.562, which are higher than those of the other methods. For example, DeepDist, one of the state-of-the-art methods, predicted the top L/5, L/2, and L with precisions of 0.793, 0.661, and 0.517. Also, compared with AlphaFold predictions of CASP13, A-Prot predicted more accurately in all three measures by 7–9%.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Contact Precision on CASP13 FM and FM/TBM targets corresponding to 43 domains (results are </p><p>adapted from DeepDist [<xref ref-type="bibr" rid="CR34">34</xref>])</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Group</th><th align="left">Top L</th><th align="left">Top L/2</th><th align="left">Top L/5</th></tr></thead><tbody><tr><td align="left">TripletRes</td><td char="." align="char">0.451</td><td char="." align="char">0.587</td><td char="." align="char">0.700</td></tr><tr><td align="left">AlphaFold</td><td char="." align="char">0.497</td><td char="." align="char">0.629</td><td char="." align="char">0.742</td></tr><tr><td align="left">RaptorX-Contact</td><td char="." align="char">0.481</td><td char="." align="char">0.612</td><td char="." align="char">0.744</td></tr><tr><td align="left">trRosetta</td><td char="." align="char">0.506</td><td char="." align="char">0.652</td><td char="." align="char">0.751</td></tr><tr><td align="left">DeepDist</td><td char="." align="char">0.517</td><td char="." align="char">0.661</td><td char="." align="char">0.793</td></tr><tr><td align="left">A-Prot (w BFD)</td><td char="." align="char"><bold>0.562</bold></td><td char="." align="char"><bold>0.710</bold></td><td char="." align="char"><bold>0.812</bold></td></tr><tr><td align="left">A-Prot (w/o BFD)</td><td char="." align="char">0.540</td><td char="." align="char">0.681</td><td char="." align="char">0.780</td></tr></tbody></table><table-wrap-foot><p>The highest score of each column is highlighted in bold</p></table-wrap-foot></table-wrap></p>
      <p id="Par24">We also compared the contact prediction accuracy of A-Prot with MSA-Transformer, which is the baseline of A-Prot. The performance of long-range Top L and Top L/5 for supervised contact prediction on CASP13 FM targets were 0.546 and 0.775 respectively (Rao, Meier, et al., 2021). The contact precision of Top L and Top L/5 for A-prot were 0.539 and 0.785, which is comparable to those of MSA Transformer (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S2). We also investigated the effect of a MSA to contact prediction by predicting structures with the MSAs obtained without BFD and obtained with DeepMSA [<xref ref-type="bibr" rid="CR35">35</xref>]. The assessment shows that not using BFD deteriorates the quality of models. The accuracies of Top L and Top L/5 contacts decreased by 2.5% and 4.0%, respectively. When DeepMSA was used, the Top L and Top L/5 accuracies dropped by 3.9% and 2.0%, respectively. These results show that considering a gigantic meta-genome DB helps improve prediction quality, but not significantly.</p>
      <p id="Par25">To investigate how the number of residual blocks affects the prediction quality, ablation tests were performed by changing the number of the residual blocks (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S4). We performed contact and structure prediction with 4, 16, 28, and 40 residual blocks. The contact prediction results show that A-Prot calculations with 28 blocks resulted in the best contact prediction. Interestingly, using more blocks, 40, decreased contact prediction accuracy. Fewer residual blocks, 4 and 16, led to significantly worse contact prediction results. In terms of model accuracy, A-Prot with 40 blocks resulted in the highest TMscore although contact predictions were most accurate with 28 blocks. These results show that A-Prot with 28 blocks is close to the optimal model considering both the prediction accuracy and computational cost.</p>
    </sec>
    <sec id="Sec9">
      <title>Benchmark on model accuracy</title>
      <p id="Par26">In addition to contact prediction, we also compared the quality of protein models predicted by A-Prot with those submitted by the top-performing server groups of CASP14 (Table <xref rid="Tab2" ref-type="table">2</xref>). The highest score of each column is highlighted in bold. 
First, we modeled the structures of 25 FM/TBM and TBM-hard targets of CASP14. The average TM-score and lDDT score of the models were compared with those of the following server groups: FEIG-S [<xref ref-type="bibr" rid="CR36">36</xref>], BAKER-ROSETTASERVER [<xref ref-type="bibr" rid="CR37">37</xref>], Zhang-Server, and QUARK [<xref ref-type="bibr" rid="CR38">38</xref>]. The model structures of the other groups were downloaded from the archive of the CASP14 website, and TM-score and lDDT scores were recalculated with the crystal structures and domain information for a fair comparison.<table-wrap id="Tab2"><label>Table 2</label><caption><p>The average TM-score and lDDT of the model structures of 25 CASP14 FM, FM/TBM, and TBM-hard domains</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Server group</th><th align="left">TM-score</th><th align="left"><italic>P</italic>-value (TM-score)</th><th align="left">lDDT</th><th align="left"><italic>P</italic>-value (lDDT)</th></tr></thead><tbody><tr><td align="left">FEIG-S</td><td char="." align="char">0.461</td><td align="left">0.0007</td><td char="." align="char">0.413</td><td char="." align="left">0.0172</td></tr><tr><td align="left">BAKER-ROSETTASERVER</td><td char="." align="char">0.517</td><td char="." align="left">0.0391</td><td char="." align="char">0.455</td><td align="left">0.2636</td></tr><tr><td align="left">Zhang-Server</td><td char="." align="char"><bold>0.595</bold></td><td char="." align="left">0.6684</td><td char="." align="char">0.489</td><td char="." align="left">0.8510</td></tr><tr><td align="left">QUARK</td><td char="." align="char">0.588</td><td char="." align="left">0.6208</td><td char="." align="char">0.484</td><td align="left">0.7408</td></tr><tr><td align="left">A-Prot</td><td char="." align="char">0.576</td><td char="." align="left"><bold>–</bold></td><td char="." align="char"><bold>0.499</bold></td><td align="left"><bold>–</bold></td></tr></tbody></table></table-wrap></p>
      <p id="Par27">A-Prot outperforms the other top server groups in terms of lDDT. Compared with BAKER- ROSETTASERVER and FEIG-S, A-Prot models are consistently more accurate in both measures. The P-values show that A-Prot outperforms ROSETTASERVER and FEIG-S statistically significantly. These results show that the accuracy of A-Prot is comparable to or better than the top-performing server groups that participated in CASP14 [<xref ref-type="bibr" rid="CR23">23</xref>]. In terms of TM-score, A-Prot is showing slightly worse results, 0.576, than Zhang-Server and QUARK, whose TM-scores are 0.595 and 0.588. However, the P-values show that the A-Prot results are not meaningfully different.</p>
      <p id="Par28">We also compared the performance of A-Prot with trRosetta [<xref ref-type="bibr" rid="CR25">25</xref>] by modeling structures using trRosetta with the identical MSAs that we used for A-Prot (Table <xref rid="Tab3" ref-type="table">3</xref>). The results show that A-Prot significantly outperforms trRosetta using the identical MSAs. In terms of dihedral angle predictions, A-Prot improved the correlation coefficient between ground truth and predictions by 0.071 in average. In addition, A-Prot generated better models with higher TMscore and lDDT values than the trRosetta results. The average TMscore and lDDT enhanced by 0.048 and 0.042 respectively.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance of A-Prot and trRosetta using same MSA of ours on 25 CASP14 FM, FM/TBM, TBM-Hard domains. (Inter-residue distance and angles are measured using Pearson correlation between predicted bin indexes of max probability and ground truth, Top L for long range contact precision)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left"><inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta$$\end{document}</tex-math><mml:math id="M42"><mml:mi>θ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq21.gif"/></alternatives></inline-formula></th><th align="left"><inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi$$\end{document}</tex-math><mml:math id="M44"><mml:mi>φ</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq22.gif"/></alternatives></inline-formula></th><th align="left"><inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega$$\end{document}</tex-math><mml:math id="M46"><mml:mi>ω</mml:mi></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq23.gif"/></alternatives></inline-formula></th><th align="left"><inline-formula id="IEq24"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{d}}$$\end{document}</tex-math><mml:math id="M48"><mml:mtext>d</mml:mtext></mml:math><inline-graphic xlink:href="12859_2022_4628_Article_IEq24.gif"/></alternatives></inline-formula></th><th align="left">Top L</th><th align="left">TMS</th><th align="left">lDDT</th></tr></thead><tbody><tr><td align="left">trRosetta</td><td char="." align="char">0.539</td><td char="." align="char">0.498</td><td char="." align="char">0.459</td><td char="." align="char">0.362</td><td char="." align="char">0.363</td><td char="." align="char">0.524</td><td char="." align="char">0.457</td></tr><tr><td align="left">A-Prot</td><td char="." align="char">0.604</td><td char="." align="char">0.578</td><td char="." align="char">0.528</td><td char="." align="char">0.464</td><td char="." align="char">0.424</td><td char="." align="char">0.576</td><td char="." align="char">0.499</td></tr></tbody></table></table-wrap></p>
      <p id="Par29">In contrast to MSA-Transformer or ROSETTA, A-Prot used the subsampling strategy that minimizes the diversity of sequences to reduce the sizes of MSAs. For protein language models, four subsampling strategies have been tested: random, diversity maximization, diversity minimization, and HHFilter to reduce the size of input MSAs while preserving prediction accuracies. It was shown that diversity maximizing performed best for the supervised contact prediction [<xref ref-type="bibr" rid="CR22">22</xref>]. On the contrary, trRosetta generates a MSA in a similar manner to the diversity minimizing approach [<xref ref-type="bibr" rid="CR25">25</xref>]. During the development of A-Prot, we tried both diversity minimization and maximization approaches. The prediction results show that diversity minimizing yields more accurate models than diversity maximization (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S3). We predicted the structures of 43 domains of CASP13 FM, FM/TBM and 25 domains of CASP14 FM, FM/TBM and TBM-hard targets using MSAs subsampled with diversity minimization and maximization. In average, the models generated with the MSAs obtained with diversity minimization have higher TM-scores, 0.658 and 0.555 for the CASP13 and CASP14 targets, than those with diversity maximization, 0.603 and 0.532. Thus, we employed the diversity minimization approach for A-Prot.</p>
    </sec>
    <sec id="Sec10">
      <title>A head-to-head comparison with ROSETTASERVER</title>
      <p id="Par30">Because A-Prot uses trRosetta for modeling structure at the final stage, we performed a head-to-head comparison of A-Prot models with the ROSETTASERVER models to identify improvement in residue-residue distance predictions (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). In terms of TM-score, many predictions made by A-Prot are significantly better than BAKER-ROSETTASERVER. For instance, the model qualities of five targets that were predicted to have TM-score less than 0.4 by ROSETTASERVER were improved higher than 0.4, corresponding to a correct fold prediction. Four highly accurate models are depicted in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig2"><label>Fig. 2</label><caption><p>TM-score and lDDT on CASP14 FM, FM/TBM and TBM-hard 25 domains compared with BAKER-ROSETTASERVER</p></caption><graphic xlink:href="12859_2022_4628_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>Model comparison of four high-quality CASP14 models generated from our method versus their native structures. Brown: native structure; Blue: model</p></caption><graphic xlink:href="12859_2022_4628_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par31">Similarly, in terms of the lDDT measure, prediction results of eight targets were improved significantly. On the contrary, only two targets deteriorated more than 0.05. Therefore, A-Prot predicts residue-residue distances and dihedral angles better than most server groups participated in CASP14 [<xref ref-type="bibr" rid="CR2">2</xref>].</p>
      <p id="Par32">On the other hand, nearly all worse predictions by A-Prot deteriorated only by a small margin except the T1026-D1 target. For T1026-D1, A-Prot did not predict its correct fold. This incorrect prediction is attributed to an incomplete MSA. T1026-D1 is a protein consisting of a virus capsid (PDB ID: 6S44). Our sequence search procedure found only less than 30 sequences, which appear to be not enough to extract correct evolutionary information. When T1026-D1 is modelled with the MSA of trRosetta containing more than 100 sequences, T1026-D1, a model with a similar TM-score is obtained. Thus, the failure of T1026-D1 suggests that having enough homologous sequences is critical in accurate 3D structure modeling using MSA-Transformer. In other words, a more extensive sequence search may improve the model accuracy of A-Pro. Except for T1026-D1, the deviations of all worse predictions than ROSETTASERVER were less than 0.1 TM-score, much smaller than the improvements.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Conclusion</title>
    <p id="Par33">In this study, we introduced a new protein structure prediction method, A-Prot, using MSA Transformer. Our benchmark results on the CASP13 TBM/FM and FM targets show that A-Prot predicts long-range residue-residue contacts more accurately than the existing methods. We also assessed the quality of protein structure models based on the predicted residue-residue distance information. The model generated by A-Prot is more accurate than most of the server groups that participated in CASP14. The average lDDT of A-Prot models is higher than that of all server group models. In terms of TM-score, our model is slightly worse than QUARK and Zhang-Server. These results show that our approach yields highly accurate residue-residue distance predictions.</p>
    <p id="Par34">A-Prot requires less computational resources than the other state-of-the-art protein structure prediction models [<xref ref-type="bibr" rid="CR2">2</xref>]. The source code of AlphaFold2 is only partially open [<xref ref-type="bibr" rid="CR3">3</xref>]. Its model parameters are fixed, and only the structure modeling part is open. Thus, it is hard to tune AlphaFold2 for bespoken purposes. In addition, training the AlphaFold2 architecture requires a significant amount of computational resources, which is not assessable for most academic groups. On the other hand, A-Prot can be trained with a single GPU card. In summary, A-Prot will open new possibilities for training novel deep-learning-based models to predict various properties of proteins only using sequence information.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec12">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2022_4628_MOESM1_ESM.docx">
            <caption>
              <p><bold>Additional file 1</bold>. The additional analysis on the modeling results are provided.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>MSA</term>
        <def>
          <p id="Par4">Multiple sequence alignment</p>
        </def>
      </def-item>
      <def-item>
        <term>FM</term>
        <def>
          <p id="Par5">Free modeling</p>
        </def>
      </def-item>
      <def-item>
        <term>TBM</term>
        <def>
          <p id="Par6">Template-based modeling</p>
        </def>
      </def-item>
      <def-item>
        <term>RMSD</term>
        <def>
          <p id="Par7">Root mean square deviation</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We wish to thank CASP organizers and predictors for sharing the data used in this work.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>YH, JL and JK conceived the research. YH wrote the code, performed simulations. YH, JL, and JK analyzed the results and wrote the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>JL was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (Nos. 2018R1C1B600543513, 2019M3E5D4066898 and 2020M3A9G710393).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>All predicted model structures and the multiple sequences alignments used as the inputs for the A-Prot models are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/arontier/A_Prot_Paper">https://github.com/arontier/A_Prot_Paper</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par35">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par36">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par37">The authors declare no conflict of interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kwon</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Won</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kryshtafovych</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Seok</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Assessment of protein model structure accuracy estimation in CASP14: old and new challenges</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2021</year>
        <volume>89</volume>
        <fpage>1940</fpage>
        <lpage>1948</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.26192</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Simpkin</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Hartmann</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Rigden</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Keegan</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Lupas</surname>
            <given-names>AN</given-names>
          </name>
        </person-group>
        <article-title>High-accuracy protein structure prediction in CASP14</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2021</year>
        <volume>89</volume>
        <fpage>1687</fpage>
        <lpage>1699</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.26171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jumper</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pritzel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Green</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Figurnov</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>
        <source>Nature</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id>
        <?supplied-pmid 34293799?>
        <pub-id pub-id-type="pmid">34293799</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tillier</surname>
            <given-names>ERM</given-names>
          </name>
          <name>
            <surname>Charlebois</surname>
            <given-names>RL</given-names>
          </name>
        </person-group>
        <article-title>The human protein coevolution network</article-title>
        <source>Genome Res</source>
        <year>2009</year>
        <volume>19</volume>
        <fpage>1861</fpage>
        <lpage>1871</lpage>
        <pub-id pub-id-type="doi">10.1101/gr.092452.109</pub-id>
        <pub-id pub-id-type="pmid">19696150</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weigt</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Szurmant</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hoch</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Hwa</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Identification of direct residue contacts in protein–protein interaction by message passing</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2009</year>
        <volume>106</volume>
        <fpage>67</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0805923106</pub-id>
        <pub-id pub-id-type="pmid">19116270</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Lunt B, Szurmant H, Procaccini A, Hoch JA, Hwa T, Weigt M. Inference of direct residue contacts in two-component signaling. In: Methods in enzymology. 2010. pp. 17–41.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ovchinnikov</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kamisetty</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Robust and accurate prediction of residue-residue interactions across protein interfaces using evolutionary information</article-title>
        <source>eLife</source>
        <year>2014</year>
        <volume>2014</volume>
        <fpage>1</fpage>
        <lpage>21</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Juan</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pazos</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Valencia</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Emerging methods in protein co-evolution</article-title>
        <source>Nat Rev Genet</source>
        <year>2013</year>
        <volume>14</volume>
        <fpage>249</fpage>
        <lpage>261</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg3414</pub-id>
        <pub-id pub-id-type="pmid">23458856</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marks</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Hopf</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Protein structure prediction from sequence variation</article-title>
        <source>Nat Publ Group</source>
        <year>2012</year>
        <pub-id pub-id-type="doi">10.1038/2419</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seemayer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gruber</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>CCMpred: fast and precise prediction of protein residue-residue contacts from correlated mutations</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <fpage>3128</fpage>
        <lpage>3130</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu500</pub-id>
        <pub-id pub-id-type="pmid">25064567</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kosciolek</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tetchner</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <fpage>999</fpage>
        <lpage>1006</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu791</pub-id>
        <pub-id pub-id-type="pmid">25431331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Hopf TA, Schärfe CPI, Rodrigues JPGLM, Green AG, Kohlbacher O, Sander C, et al. Sequence co-evolution gives 3D contacts and structures of protein complexes. eLife. 2014; 3.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Senior</surname>
            <given-names>AW</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Jumper</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kirkpatrick</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sifre</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Green</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improved protein structure prediction using potentials from deep learning</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>577</volume>
        <fpage>706</fpage>
        <lpage>710</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-019-1923-7</pub-id>
        <pub-id pub-id-type="pmid">31942072</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Analysis of distance-based protein structure prediction by deep learning in CASP13</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2019</year>
        <volume>87</volume>
        <fpage>1069</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25810</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Distance-based protein folding powered by deep learning</article-title>
        <source>Proc Natl Acad Sci USA</source>
        <year>2019</year>
        <volume>116</volume>
        <fpage>16856</fpage>
        <lpage>16865</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1821309116</pub-id>
        <pub-id pub-id-type="pmid">31399549</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bepler</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Learning the protein language: evolution, structure, and function</article-title>
        <source>Cell Syst</source>
        <year>2021</year>
        <volume>12</volume>
        <fpage>654</fpage>
        <lpage>669.e3</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2021.05.017</pub-id>
        <pub-id pub-id-type="pmid">34139171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Strodthoff</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wagner</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wenzel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Samek</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>UDSMProt: Universal deep sequence models for protein classification</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>2401</fpage>
        <lpage>2409</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa003</pub-id>
        <pub-id pub-id-type="pmid">31913448</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Vig J, Madani A, Varshney LR, Xiong C, Socher R, Rajani NF. BERTology meets biology: interpreting attention in protein language models. 2020.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Madani A, McCann B, Naik N, Keskar NS, Anand N, Eguchi RR, et al. ProGen: language modeling for protein generation. 2020. 10.1101/2020.03.07.982272.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rives</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Meier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sercu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Goyal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2021</year>
        <volume>118</volume>
        <fpage>e2016239118</fpage>
        <pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id>
        <pub-id pub-id-type="pmid">33876751</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Rao R, Meier J, Sercu T, Ovchinnikov S, Rives A. Transformer protein language models are unsupervised structure learners. In: ICLR 2021 conference. 2021.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Rao R, Liu J, Verkuil R, Meier J, Canny JF, Abbeel P, et al. MSA transformer. 2021.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kinch</surname>
            <given-names>LN</given-names>
          </name>
          <name>
            <surname>Schaeffer</surname>
            <given-names>RD</given-names>
          </name>
          <name>
            <surname>Kryshtafovych</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Grishin</surname>
            <given-names>NV</given-names>
          </name>
        </person-group>
        <article-title>Target classification in the 14th round of the critical assessment of protein structure prediction (CASP14)</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2021</year>
        <volume>89</volume>
        <fpage>1618</fpage>
        <lpage>1632</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.26202</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kinch</surname>
            <given-names>LN</given-names>
          </name>
          <name>
            <surname>Kryshtafovych</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Monastyrskyy</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grishin</surname>
            <given-names>NV</given-names>
          </name>
        </person-group>
        <article-title>CASP13 target classification into tertiary structure prediction categories</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2019</year>
        <volume>87</volume>
        <fpage>1021</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25775</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Yang J, Anishchenko I, Park H, Peng Z, Ovchinnikov S, Baker D. Improved protein structure prediction using predicted interresidue orientations. Proc Natl Acad Sci USA 2020;117.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eddy</surname>
            <given-names>SR</given-names>
          </name>
        </person-group>
        <article-title>Accelerated profile HMM searches</article-title>
        <source>PLOS Comput Biol</source>
        <year>2011</year>
        <volume>7</volume>
        <fpage>1002195</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1002195</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Steinegger M, Meier M, Mirdita M, Vöhringer H, Haunsberger SJ, Söding J. HH-suite3 for fast remote homology detection and deep protein annotation. BMC Bioinform. 2019; 20.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mirdita</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>von den Driesch</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Galiez</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Steinegger</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Uniclust databases of clustered and deeply annotated protein sequences and alignments</article-title>
        <source>Nucleic Acids Res</source>
        <year>2017</year>
        <volume>45</volume>
        <fpage>D170</fpage>
        <lpage>D176</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw1081</pub-id>
        <pub-id pub-id-type="pmid">27899574</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Steinegger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mirdita</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>603</fpage>
        <lpage>606</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0437-4</pub-id>
        <pub-id pub-id-type="pmid">31235882</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Steinegger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title>
        <source>Nat Biotechnol</source>
        <year>2017</year>
        <volume>35</volume>
        <fpage>1026</fpage>
        <lpage>1028</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.3988</pub-id>
        <pub-id pub-id-type="pmid">29035372</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Lin M, Chen Q, Yan S. Network in network. 2013; arXiv:1312.4400.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Identity mappings in deep residual networks. In: Leibe Bastian and Matas J and SN and WM, editor. Computer vision—ECCV 2016. Cham: Springer International Publishing; 2016. p. 630–45.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Liu L, Jiang H, He P, Chen W, Liu X, Gao J, et al. On the variance of the adaptive learning rate and beyond. In: Eighth international conference on learning representations (ICLR). 2020.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Wu T, Guo Z, Hou J, Cheng J. DeepDist: real-value inter-residue distance prediction with deep residual convolutional network. BMC Bioinform. 2021; 22.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Mortuza</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>DeepMSA: Constructing deep multiple sequence alignment to improve contact prediction and fold-recognition for distant-homology proteins</article-title>
        <source>Bioinformatics</source>
        <year>2020</year>
        <volume>36</volume>
        <fpage>2105</fpage>
        <lpage>2112</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz863</pub-id>
        <pub-id pub-id-type="pmid">31738385</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heo</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Janson</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Feig</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Physics-based protein structure refinement in the era of artificial intelligence</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2021</year>
        <volume>89</volume>
        <fpage>1870</fpage>
        <lpage>1887</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.26161</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Anishchenko</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Baek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hiranuma</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Dauparas</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein tertiary structure prediction and refinement using deep learning and Rosetta in CASP14</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2021</year>
        <volume>89</volume>
        <fpage>1722</fpage>
        <lpage>1733</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.26194</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Pearce</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bell</surname>
            <given-names>EW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein structure prediction using deep learning distance and hydrogen-bonding restraints in CASP14</article-title>
        <source>Proteins Struct Funct Bioinform</source>
        <year>2021</year>
        <volume>89</volume>
        <fpage>1734</fpage>
        <lpage>1751</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.26193</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
