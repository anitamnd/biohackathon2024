<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Vision (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Vision (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">vision</journal-id>
    <journal-title-group>
      <journal-title>Vision</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2411-5150</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7355841</article-id>
    <article-id pub-id-type="doi">10.3390/vision4020025</article-id>
    <article-id pub-id-type="publisher-id">vision-04-00025</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MLGaze: Machine Learning-Based Analysis of Gaze Error Patterns in Consumer Eye Tracking Systems</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2543-1697</contrib-id>
        <name>
          <surname>Kar</surname>
          <given-names>Anuradha</given-names>
        </name>
      </contrib>
    </contrib-group>
    <aff id="af1-vision-04-00025">École Normale Supérieure de Lyon, 46 Allée d’Italie, 69007 Lyon, France; <email>anuradha.kar@ens-lyon.fr</email> or <email>anuradha.kar49@gmail.com</email>; Tel.: +33-6238-20074</aff>
    <pub-date pub-type="epub">
      <day>07</day>
      <month>5</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <volume>4</volume>
    <issue>2</issue>
    <elocation-id>25</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>1</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>4</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 by the author.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="open-access">
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Analyzing the gaze accuracy characteristics of an eye tracker is a critical task as its gaze data is frequently affected by non-ideal operating conditions in various consumer eye tracking applications. In previous research on pattern analysis of gaze data, efforts were made to model human visual behaviors and cognitive processes. What remains relatively unexplored are questions related to identifying gaze error sources as well as quantifying and modeling their impacts on the data quality of eye trackers. In this study, gaze error patterns produced by a commercial eye tracking device were studied with the help of machine learning algorithms, such as classifiers and regression models. Gaze data were collected from a group of participants under multiple conditions that commonly affect eye trackers operating on desktop and handheld platforms. These conditions (referred here as error sources) include user distance, head pose, and eye-tracker pose variations, and the collected gaze data were used to train the classifier and regression models. It was seen that while the impact of the different error sources on gaze data characteristics were nearly impossible to distinguish by visual inspection or from data statistics, machine learning models were successful in identifying the impact of the different error sources and predicting the variability in gaze error levels due to these conditions. The objective of this study was to investigate the efficacy of machine learning methods towards the detection and prediction of gaze error patterns, which would enable an in-depth understanding of the data quality and reliability of eye trackers under unconstrained operating conditions. Coding resources for all the machine learning methods adopted in this study were included in an open repository named MLGaze to allow researchers to replicate the principles presented here using data from their own eye trackers.</p>
    </abstract>
    <kwd-group>
      <kwd>eye gaze</kwd>
      <kwd>gaze data</kwd>
      <kwd>pattern recognition</kwd>
      <kwd>modelling</kwd>
      <kwd>machine learning</kwd>
      <kwd>neural networks</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-vision-04-00025">
    <title>1. Introduction</title>
    <sec id="sec1dot1-vision-04-00025">
      <title>1.1. Research Questions and Motivation</title>
      <p>Gaze data obtained from eye trackers operating on various consumer platforms is frequently affected by a multitude of factors (or error sources), such as the head pose, user distance, display properties of the setup, illumination variations, and occlusions. The impact of these factors on gaze data are manifested in the form of gaze estimation errors whose characteristics or distributions have not been explored adequately in contemporary gaze research [<xref rid="B1-vision-04-00025" ref-type="bibr">1</xref>]. Conventionally, researchers attempt to improve the accuracy of eye trackers or calibration methods, while gaze error patterns are rarely analyzed and thus there remain many questions regarding the nature of gaze estimation errors. For example, it is not known whether the above error sources produce any particular pattern of errors or if the nature of gaze errors follows any statistical distribution or if they are simply random. These aspects cannot be understood by looking at raw gaze data, which is almost always corrupted with noise and outliers, or even by studying mean gaze error values.</p>
      <p>With respect to gaze error analysis, several questions arise: These include: (1) how can gaze errors caused by one error source be distinguished from those caused by another? (2) How can the presence of different error sources in a certain gaze dataset be detected without prior knowledge? (3) Is it possible to predict the level of gaze errors that have been caused by different error sources? (4) Can suitable features be extracted from gaze datasets to identify the different error sources. These questions form the main topics that are addressed in this paper.</p>
      <p>This paper focuses on defining methods for detailed analysis of eye tracking data obtained from a generic commercial eye tracker for the detection, identification, and prediction of gaze errors. The aim of this study was to observe gaze error patterns produced by three error sources that commonly affect eye trackers in consumer platforms like desktop and tablets. These include head movement, user distance, and platform orientation. It was found that there currently exists no publicly available gaze dataset that contains raw gaze and ground truth (gaze target location) data along with signatures of these error sources. Therefore, a new eye tracking dataset was built by collecting gaze data from 20 participants using a commercial eye tracker on both a desktop and a tablet platform. This dataset was published in an open repository for use by the gaze research community (link to the dataset webpage is in <xref ref-type="sec" rid="sec2dot5dot5-vision-04-00025">Section 2.5.5</xref>). During data collection, variations in head pose, user distance, and platform orientations were introduced sequentially and in a calibrated manner so that the gaze data contains the influence of one known condition (or error source) at a time. Reference data, which does not have the influence of any of these conditions, was collected as well.</p>
      <p>For the detection of gaze error patterns caused by the different error sources, the collected data was used for training machine learning (ML) algorithms by creating training features from the datasets. As the operating conditions change, the feature variables in the training data get affected and as a result the ML models learn to differentiate between error patterns caused by different sources. In this way, anomalous gaze data may be distinguished using classifiers, which were trained using both affected and reference gaze datasets. Finally, regression algorithms were built to model and predict gaze errors that may be produced by the above three error sources.</p>
    </sec>
    <sec id="sec1dot2-vision-04-00025">
      <title>1.2. Background and Scope</title>
      <p>Pattern classification and modelling approaches have been applied on eye tracking data for various purposes. In [<xref rid="B2-vision-04-00025" ref-type="bibr">2</xref>], the effect of the video frame rate on the viewing behavior and visual perception of users was studied by comparing gaze data patterns collected for high and low video frame rates. In [<xref rid="B3-vision-04-00025" ref-type="bibr">3</xref>], a new hybrid fuzzy approach was introduced to distinguish gaze patterns when users performed face and text scanning. In [<xref rid="B4-vision-04-00025" ref-type="bibr">4</xref>], the dominant gaze characteristics of experienced and inexperienced train drivers were classified using the Markov cluster (MCL) algorithm, and marked differences were observed between gaze patterns of the two driver classes. The authors in [<xref rid="B5-vision-04-00025" ref-type="bibr">5</xref>] used a Bayesian mixture model to learn the gaze behavior of drivers who perform a variety of tasks during conditionally automated driving, by classifying the driver’s fixations and saccades. Gaze patterns were used with clustering and classification algorithms to predict the user’s intention to perform a set of tasks in [<xref rid="B6-vision-04-00025" ref-type="bibr">6</xref>].</p>
      <p>A special de-noising and segmentation algorithm based on naive segmented linear regression and hidden Markov models was developed in [<xref rid="B7-vision-04-00025" ref-type="bibr">7</xref>] to classify gaze features into fixations, smooth pursuits, saccades, and post-saccadic oscillations using good-quality as well as noisy data. Automated classification of fixations and saccades was achieved using random forest classifiers on high-quality as well as noisy gaze datasets in [<xref rid="B8-vision-04-00025" ref-type="bibr">8</xref>].</p>
      <p>It may be observed that gaze or eye movement pattern modelling have mostly been applied towards either cognitive studies, i.e., to interpret viewing patterns or distinguish between oculomotor event types, e.g., saccades, fixations, and smooth pursuits. With respect to studying gaze error patterns, however, only a few works were reported. Examples include the work in [<xref rid="B9-vision-04-00025" ref-type="bibr">9</xref>], which estimated the two-dimensional gaze error distribution and used a predictive model to shift the gaze according to a directional error estimate. This is based on a previous study [<xref rid="B10-vision-04-00025" ref-type="bibr">10</xref>], which proposed a gaze estimation error model that can predict gaze error by taking into consideration factors, such as the mapping of pupil positions to scene camera coordinates, marker-based display detection, and mapping from a scene camera to on-screen gaze coordinates. The major difference between the approaches taken in [<xref rid="B9-vision-04-00025" ref-type="bibr">9</xref>,<xref rid="B10-vision-04-00025" ref-type="bibr">10</xref>] and the current study is that the authors in those papers modelled the gaze error based on the contributions from the components of the gaze estimation process, e.g., mapping of the pupil to the scene camera coordinates and that from the scene camera to the on-screen coordinates. However, in this study, the gaze errors were studied as impacts of operating conditions that are external to the gaze estimation system. The reason for adopting this approach in this study was that for most consumer eye trackers, the gaze estimation principle and components of the eye trackers are not accessible for inspection to users; however, the influence of the operating conditions is manifested via variations in the gaze data characteristics. Therefore, it is feasible to model the gaze errors of eye trackers by studying their output data characteristics in response to different operating conditions. It was also found that no research works till now have covered the aspect of the classification of gaze error patterns induced by different operating conditions and prediction of error levels under their influence. These form the basis of the concepts that are presented in this paper.</p>
      <p>The identification of unusual or anomalous data patterns has motivated research in areas, such as computer networks, business intelligence, and data mining [<xref rid="B11-vision-04-00025" ref-type="bibr">11</xref>]. This study was inspired by research in these fields and applied machine learning principles, which have been used successfully in anomaly detection algorithms. Automatic detection of anomalous data has manifold advantages, for example, in identifying the presence of defects, and getting insights into system behavior while also helping in reducing the number of system failures and improving the chances of recovery [<xref rid="B12-vision-04-00025" ref-type="bibr">12</xref>]. Similar to the above research areas, eye tracking devices also generate bulk amounts of data and the data quality is critical for the use of the collected datasets. It is therefore imperative for researchers to ensure that over the periods of experiments, the gaze data follows consistent accuracy patterns. However, several factors, as described in this paper, arising from the user or experimental setup conditions can result in altered data characteristics, which must be detected and corrected to maintain the consistency of gaze research results. This forms the motivation behind the gaze error pattern identification and modelling work presented in this paper.</p>
      <p>In this study, the term gaze error is frequently used, which is the angular difference between the gaze locations estimated by an eye tracker and the actual locations of the visual targets (also called ground truth), typically appearing on a display screen or viewing area. The “gaze error patterns” described here signify the magnitude distributions of the gaze error values over a given number of samples or display area. Error sources refer to non-ideal operating conditions, such as a high degree of head pose variation with respect to the frontal pose, too long or too short user distances, or conditions where an eye tracking platform is tilted to different angles as opposed to their neutral positions. Such conditions were considered in this study because they occur frequently during real world operations of eye trackers. However, it is not known to what extent these conditions affect the gaze data characteristics and impact the gaze error patterns.</p>
      <p>The impact of these error sources on the gaze error patterns produced by a given eye tracker was studied in this paper. The scope of the paper was limited to studying one error source or operating condition at a time (i.e., either the head pose or user distance or platform pose was varied at a time). However, eye trackers could face more complex scenarios, for example, two or more error sources could occur together and affect an eye tracker’s data. Such analysis was not covered in this study as firstly, relevant data was not available, and secondly, the occurrences of such complex circumstances are rarer than the ones considered here. It is likely that at a time there is only one error source affecting an eye tracker with a major influence compared to other sources, which may or may not be present. For example, the user distance is fixed in a desktop or automotive-based eye tracking systems and head motion is the major source of gaze tracking error in these platforms. The analysis in this paper was based on data collected from static remote eye tracker setups on desktop and tablet platforms.</p>
    </sec>
    <sec id="sec1dot3-vision-04-00025">
      <title>1.3. Organization of Contents</title>
      <p>The paper is organized as follows. <xref ref-type="sec" rid="sec2-vision-04-00025">Section 2</xref> describes the gaze data collection setup and procedure for data pre-processing and exploration. <xref ref-type="sec" rid="sec3-vision-04-00025">Section 3</xref> presents the gaze error detection using machine learning models. <xref ref-type="sec" rid="sec4-vision-04-00025">Section 4</xref> presents the regression algorithms for the modelling of gaze errors. <xref ref-type="sec" rid="sec5-vision-04-00025">Section 5</xref> describes the code repository for the implementation of the methods described in this paper.</p>
    </sec>
  </sec>
  <sec sec-type="methods" id="sec2-vision-04-00025">
    <title>2. Experimental Methodology and Data Exploration</title>
    <p>The concepts of this study were implemented in several phases. Eye tracking data was collected through special experiments using two consumer platforms, a desktop and a tablet, under different operating conditions. The collected data went through the processing pipeline (<xref ref-type="fig" rid="vision-04-00025-f001">Figure 1</xref>) with an initial investigation using statistical methods and visualizations before being fed to the machine learning models. The data collection experiments, experimental setup, and steps of the data analysis workflow are described below. All subjects gave their informed consent for inclusion before they participated in the study. The study was conducted in accordance with the Declaration of Helsinki, and the protocol was approved by the Ethics Committee of SFI Project ID: 13/SPP/I2868.</p>
    <sec id="sec2dot1-vision-04-00025">
      <title>2.1. Eye Tracking Data Collection</title>
      <p>A detailed description of the gaze data collection process and setup using a commercial eye tracker was provided in [<xref rid="B1-vision-04-00025" ref-type="bibr">1</xref>] and also in the sections below. Gaze data was collected from a group of 20 participants and for four user distances from both the desktop and tablet platforms.</p>
    </sec>
    <sec id="sec2dot2-vision-04-00025">
      <title>2.2. Eye Tracking UI and Device</title>
      <p>For eye tracking data collection, an eye tracker coupled with a visual stimulus interface (also called UI) was used. The trackers used were a Tobii EyeX 4C and an Eye tribe remote eye tracking device, which come with their own calibration routines. During an experiment session, participants were seated in front of the tracker with a chin rest and the UI ran simultaneously with the eye tracker. The UI shows a moving dot, which sequentially traces a grid of (5 × 3) known locations (also called the area of interest or AOI) over the desktop or tablet’s display. The dot radius is 10 pixels and it stops at each AOI for 3 s before moving to the next, and gaze data was collected while participants looked at the dot. Gaze data comprised of a participant’s gaze coordinates (x, y positions in pixels) on the (desktop/tablet) display and corresponding time stamps as estimated by the eye tracker. The known on-screen dot locations form the ground truth data and were used for accuracy calculations. The chin rest was used for stabilizing the participant’s head for all experiments (<xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>g). A nine-point eye calibration was performed for all participants before each experiment session. Photos from the desktop and tablet experimental setup are shown in <xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>a,b, respectively.</p>
    </sec>
    <sec id="sec2dot3-vision-04-00025">
      <title>2.3. Setup and Experiment Details</title>
      <sec id="sec2dot3dot1-vision-04-00025">
        <title>2.3.1. Setup Description</title>
        <p>The desktop setup consisted of the eye tracker mounted on the screen of a desktop computer (<xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>a). The screen diagonal was 22 inches (model: Asus VW22ATL–LCD), with a pixel resolution of 1680 × 1050. Two experiments were done with this setup. These were: (a) User distance experiments: In these, gaze data was collected at user-eye tracker distances of 50, 60, 70, and 80 cm; and (b) head-pose variability experiments: Head pose here refers to the position of a user’s head in 3-D space in terms of the roll, pitch, and yaw (RPY) angles. During the experiments, a user was seated at a fixed distance (60 cm) from the tracker and was asked to vary their head position to distinct pose (RPY) angles each time, with respect to the frontal position (RPY = 0) while looking at the UI on the display. Their gaze was tracked on the UI and their head position was tracked simultaneously using an active appearance model [<xref rid="B13-vision-04-00025" ref-type="bibr">13</xref>], or AAM, which is a computer vision-based software that measures head pose RPY angles with 1 degree accuracy. To run the AAM, a Logitech HD Webcam C270 (1280 × 720px) was used to capture the facial video of users from which the head pose was estimated by the model. Sample images from the AAM showing various head pose angles are in <xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>c. A sample video showing head pose estimation using AAM is in the <xref ref-type="app" rid="app1-vision-04-00025">Supplementary Materials</xref> with this paper. Neutral pose data is the same as the user distance data at 60 cm. </p>
        <p>The tablet model was a Lenovo MIIX 310 with a screen diagonal size of 10.1 inches and pixel resolution of 1920 × 800. For the tablet experiments, the tablet was mounted on a gimbal tripod as shown in <xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>d and the orientations were measured using the readings from the inbuilt inertial sensors of the tablet. For the tablet setup, two sets of experiments were dome with the same test UI as used for the desktop. The first were the user distance experiments done in the same way as for the desktop platform. The other was done by studying the impact of a variable platform orientation on the eye tracker data. For this, the orientation of the combined tablet-eye tracker setup was varied to known platform roll, pitch, and yaw angles (20 degree in each of roll, pitch yaw directions) as shown in <xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>d. The user’s head and distance from the tablet-tracker setup were kept fixed at 60 cm. Gaze data was collected for each tablet orientation. Data for the neutral orientation was the same as gaze data at 60 cm from the tablet. The full list of experiments is shown in <xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>e.</p>
        <p>The maximum head and platform pose allowed by the eye tracker was 20 degrees and no tracker data could be obtained at distances below 50 cm or above 80 cm. Data from the user distance experiments were called UD50, UD60, UD70, and UD80 for distances of 50, 60, 70, and 80 cm, respectively. “HP” was used to denote head pose experiments. The process flow of experiments (<xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>h) comprised of positioning the user in front of the desktop or tablet setup and calibrating their eyes with the tracker calibration software. Next, the UI was run with its data-logging routine to record gaze data in comma-separated values or CSV files, along with millisecond timestamps.</p>
        <p>To position a user on the desktop and tablet setup, the center of the user’s head was kept aligned with the screen center line as shown in <xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>i. The height of the chin rest was adjusted according to the height of each participant to keep the user eyes level with the center of the display screen. To change the user distance, the user was made to sit at increasing distances along the center line while keeping their head aligned with the center of the display.</p>
      </sec>
      <sec id="sec2dot3dot2-vision-04-00025" sec-type="subjects">
        <title>2.3.2. Participants and Demography Information</title>
        <p>For both the desktop and tablet experiments, the same group of 20 participants (15 males, 5 females) were involved in the data collection for all experiments. The age range of all the users was between 30 and 45 years with a median age of 38 years. This study was done with participants not wearing glasses, in an indoor space and under uniform illumination levels, to rule out the impact of occlusion and illumination changes on the gaze data. The users were made familiar with the eye tracking setup through instructions and pilot experiments before the main data collection process was started.</p>
      </sec>
    </sec>
    <sec id="sec2dot4-vision-04-00025">
      <title>2.4. Data Preparation</title>
      <p><xref ref-type="fig" rid="vision-04-00025-f003">Figure 3</xref> below shows the raw gaze data overlaid on the ground truth (or target locations), as obtained from the different experiments described above. It can be seen that it is impossible to decipher error patterns from gaze data by simply looking at it in raw form, and data from very different operating conditions often look similar and vice versa. However, as will be shown next, the eye tracking data obtained under different operating conditions do have diverse error distributions and statistical properties.</p>
      <p>The first step to prepare the gaze data for the learning tasks was to convert the raw gaze data coordinates (in pixels) into frontal gaze angles and gaze yaw and pitch angles. The ground truth data comprised of the screen locations (x, y coordinates in pixels) at which the black dot in the stimuli UI stopped during the gaze data collection (<xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>f). The raw gaze x,y pixel coordinates of the left and right eye (<italic>X<sub>left</sub></italic>, <italic>Y<sub>left</sub></italic>, and <italic>X<sub>right</sub></italic>, <italic>Y<sub>right</sub></italic> respectively) obtained from the tracker were used to estimate the gaze angle and gaze yaw and pitch angles as follows [<xref rid="B1-vision-04-00025" ref-type="bibr">1</xref>]:<disp-formula id="FD1-vision-04-00025"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mrow><mml:mi>GazeX</mml:mi><mml:mo>=</mml:mo><mml:mi>mean</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">X</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>left</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">X</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>right</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mrow><mml:mi>GazeY</mml:mi><mml:mo>=</mml:mo><mml:mi>mean</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>left</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Y</mml:mi><mml:mrow><mml:mi>right</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The on-screen distance (OSD) of a user’s gaze point was the distance between the origin and a certain gaze point with the coordinates (<italic>GazeX</italic>, <italic>GazeY</italic>). In our case, the tracker was attached directly below the screens and the origin was the center of the screen. Therefore:<disp-formula id="FD2-vision-04-00025"><label>(2)</label><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mrow><mml:mi>OSD</mml:mi><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mi>mm</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>µ</mml:mi></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mi>X</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mi>Y</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>µ</italic> is the pixel pitch of the display where the gaze was tracked in units of mm/pixel.</p>
      <p>The gaze angle of a point on the screen relative to a user’s eyes was calculated as:<disp-formula id="FD3-vision-04-00025"><label>(3)</label><mml:math id="mm3"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi>Gaze</mml:mi><mml:mo> </mml:mo><mml:mi>angle</mml:mi><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="sans-serif">θ</mml:mi><mml:mi>gaze</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>tan</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>OSD</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The ground truth (<italic>θgt</italic>) gaze angles for the AOI locations with coordinates (AOI_X, AOI_Y) were:<disp-formula id="FD4-vision-04-00025"><label>(4)</label><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>OSD</mml:mi><mml:mo>_</mml:mo><mml:mi>GT</mml:mi><mml:mo>=</mml:mo><mml:mi>µ</mml:mi></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>AOI</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">X</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mi>AOI</mml:mi><mml:mo>_</mml:mo><mml:mi mathvariant="normal">Y</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mspace linebreak="newline"/><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi>GT</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="sans-serif">θ</mml:mi><mml:mi>gt</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>tan</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>OSD</mml:mi><mml:mo>_</mml:mo><mml:mi>GT</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The gaze yaw and pitch angles were derived as follows:<disp-formula id="FD6-vision-04-00025"><label>(5)</label><mml:math id="mm5"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi>Gaze</mml:mi><mml:mo> </mml:mo><mml:mi>pitch</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="sans-serif">θ</mml:mi><mml:mi>pitch</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>tan</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>GazeY</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Z</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>Gaze</mml:mi><mml:mo> </mml:mo><mml:mi>yaw</mml:mi><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="sans-serif">θ</mml:mi><mml:mi>yaw</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>tan</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>GazeX</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The ground truth pitch and yaw angular values for each AOI dot with screen coordinates (<italic>AOI_X, AOI_Y</italic>) were given by:<disp-formula id="FD7-vision-04-00025"><label>(6)</label><mml:math id="mm6"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mi>AOI</mml:mi><mml:mo> </mml:mo><mml:mi>pitch</mml:mi><mml:mo>=</mml:mo><mml:mi>tan</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>AOIy</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Z</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>AOI</mml:mi><mml:mo> </mml:mo><mml:mi>yaw</mml:mi><mml:mo>=</mml:mo><mml:mi>tan</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>AOIx</mml:mi><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>These gaze variables along with their statistics were later used to construct the feature vectors for the learning algorithms. The plots of the gaze frontal and rotational angles are shown below in <xref ref-type="fig" rid="vision-04-00025-f004">Figure 4</xref>. Throughout this paper, the terminologies gaze angle, gaze yaw, and gaze pitch angle will be used to indicate the variables defined by Equations (3)–(6) above. Readers must not be confused between the terms gaze yaw/pitch and user head roll/yaw/pitch and platform pose roll/yaw/pitch, which are also used in this paper.</p>
      <p>The common problems associated with analyzing the collected gaze dataset were: (a) The presence of outliers, (b) missing or null values, and (c) unequal data row lengths. Therefore, before using the data to train the models and yield meaningful results, it was essential that the data went through certain pre-processing steps. To fill the missing values, a mean substitution was used. For outlier removal, the following methods were tested: (i) 1-D Median filtering: This method can detect isolated out-of-range values from legitimate data features. In this method, the value of a data point is replaced by that of the median of all data points in a neighborhood w [<xref rid="B14-vision-04-00025" ref-type="bibr">14</xref>], such that:
<disp-formula>y[m,n] = median {x[i,j], (i,j) ϵ w}<label>(7)</label></disp-formula>(ii) Median absolute deviation (MAD): This is calculated by taking the absolute difference between each point and the median, and then calculating the median of those differences. This is more robust than using the standard deviation for outlier detection as the standard deviation is itself affected by the presence of outliers [<xref rid="B15-vision-04-00025" ref-type="bibr">15</xref>]. (d) Inter-quartile range(IQR): The concepts of using the Z-score and inter-quartile range (IQR) to study outliers were discussed in our previous study [<xref rid="B1-vision-04-00025" ref-type="bibr">1</xref>]. A data point is denoted as an outlier if the value for the point is 1.5⋅IQR above the third quartile or below the first quartile of the data. The results from the three outlier removal methods are shown in <xref ref-type="fig" rid="vision-04-00025-f005">Figure 5</xref>a–c and it is seen from the figures that median filtering (with a kernel size =41, which is the mean number of data points around each AOI) worked best for all of the datasets.</p>
    </sec>
    <sec id="sec2dot5-vision-04-00025">
      <title>2.5. Exploratory data analysis and visualizations</title>
      <p>Since the eye tracking datasets were collected under unconstrained conditions and the nature of such data or its distributions were unknown, data exploration was essential to observe any underlying patterns before proceeding to the machine learning step. This was done after the data cleaning steps described above as the presence of outliers and null values made it impossible to study any meaningful patterns prior to data preprocessing. After null and outlier removal, gaze angular errors for the frontal and rotational components were computed in absolute units (degrees) as the angular deviation between ground truth and estimated gaze angular values using Equations (3)–(6), as:<disp-formula>(Gaze frontal angular error)<sub>i</sub> = (θgaze)<sub>i</sub> − (θgt)<sub>i</sub>,<label>(8)</label></disp-formula>
<disp-formula>(Gaze yaw angle error)<sub>i</sub> = (θyaw)<sub>i</sub> − (AOI_Yaw)<sub>i</sub>,<label>(9)</label></disp-formula>
<disp-formula>(Gaze pitch angle error)<sub>i</sub> = (θgaze )<sub>i</sub> − (AOI_Pitch)<sub>i</sub>.<label>(10)</label></disp-formula></p>
      <p>The gaze frontal angular error and gaze yaw and pitch errors are three categories derived from the same gaze data sample. This aspect was used to generate the training sample set from the collected gaze datasets.</p>
      <sec id="sec2dot5dot1-vision-04-00025">
        <title>2.5.1. Studying Gaze Data Statistics for Desktop and Tablets</title>
        <p>The first step in studying the error characteristics of gaze data obtained from the different experiments was to look into the error statistical parameters, such as the mean, median absolute deviation, inter-quartile range, and 95% confidence interval. These parameters also form parts of the feature vector for the classification studies described later in this paper. <xref rid="vision-04-00025-t001" ref-type="table">Table 1</xref> shows the statistical properties of the gaze errors for different desktop experiments. It was seen that the gaze error is higher at low user distances and the error reduces as the user-tracker distance increases. This was primarily due to a reduction in the visual angle and eccentricity with increasing distance as discussed in [<xref rid="B1-vision-04-00025" ref-type="bibr">1</xref>]. Error due to head yaw was seen to have the highest magnitude, although errors due to head pitch had the highest inter-quartile range or highest variability in error values. Additionally, the error levels due to various head poses were quite high compared to when the head pose was neutral (UD60 values in <xref rid="vision-04-00025-t001" ref-type="table">Table 1</xref>). All values in the tables units of degrees of angular resolution.</p>
        <p><xref rid="vision-04-00025-t002" ref-type="table">Table 2</xref> shows the statistical properties of the gaze errors for different experiments on the tablet platform, including variations in the user distance and tablet pose (here the roll, pitch, and yaw represent tablet poses). The magnitudes of errors due to tablet pose changes were high and the highest error was caused due to platform roll variations. It was also seen that the error characteristics of tablet data are quite different than those from the desktop platform. Compared to desktop data, the error magnitudes were lower for the tablet for all user distances. However, the magnitude of errors produced due to different platform poses was higher compared to errors induced due to the head pose.</p>
      </sec>
      <sec id="sec2dot5dot2-vision-04-00025">
        <title>2.5.2. Studying Gaze Error Distributions for Desktop and Tablet Data</title>
        <p>The one-dimensional distributions of the angular error values for different gaze datasets were studied using the kernel density estimate (KDE) [<xref rid="B16-vision-04-00025" ref-type="bibr">16</xref>]. Since the exact distributions of the gaze errors were unknown, the kernel density estimation was useful since it is a non-parametric way to approximate the probability density function of the data, compared to parametric estimation, where a fixed functional form and its parameters are required to fit the data. For data with samples x(i), using a kernel function (<italic>K</italic>) and bandwidth h, the probability density at a point x is:<disp-formula id="FD8-vision-04-00025"><label>(11)</label><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mi>KDE</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">N</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="normal">h</mml:mi></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p>The Gaussian kernel K is given by: <disp-formula id="FD9-vision-04-00025"><label>(12)</label><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mfrac bevelled="true"><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">u</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="sans-serif">π</mml:mi></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mi mathvariant="normal">e</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>.</mml:mo><mml:mn>5</mml:mn><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p>Using the data collected from the desktop and tablet experiments, the KDE plots of gaze errors for three different user distances were plotted (bandwidth = 0.2) in <xref ref-type="fig" rid="vision-04-00025-f006">Figure 6</xref> and <xref ref-type="fig" rid="vision-04-00025-f007">Figure 7</xref> above. It was seen that different user distances have individual impacts on the gaze error distribution. These error patterns were difficult to decipher when looking at raw gaze data or simple error magnitudes. Further, the KDE plots were found to be non-Gaussian or resembling any known statistical distribution, which makes it difficult to predict nature of the gaze errors. These form the background for studying gaze error characteristics and implementing the learning tasks described in this paper.</p>
      </sec>
      <sec id="sec2dot5dot3-vision-04-00025">
        <title>2.5.3. Studying Spatial Error Distribution Properties</title>
        <p>The two-dimensional spatial distribution of gaze error values over the display screen area, as a function of the corresponding visual angles for different datasets, were computed and are shown in <xref ref-type="fig" rid="vision-04-00025-f008">Figure 8</xref>a–h. These plots show that factors like head pose angles have distinct impacts on the gaze error spatial patterns, with minimum errors obtained for neutral head positions. Similar features were seen from the tablet data plots, for different platform poses.</p>
      </sec>
      <sec id="sec2dot5dot4-vision-04-00025">
        <title>2.5.4. Studying Data Correlations</title>
        <p>Desktop data from user distance experiments at 50, 60, 70, and 80 cm (UD 50, UD60, UD70, UD80) and head pose experiments at head roll, pitch, and yaw angles of 20 degrees, (R20, Y20, P20) were used to compute the desktop data correlation matrix of <xref ref-type="fig" rid="vision-04-00025-f009">Figure 9</xref>a. Similarly, data from the tablet experiments for four user distances and three platform poses were used to compute the tablet data correlation matrix of <xref ref-type="fig" rid="vision-04-00025-f009">Figure 9</xref>b. As can be observed, the gaze data collected under different operating conditions from the same platform and eye tracker do not have any correlations between their characteristics.</p>
      </sec>
      <sec id="sec2dot5dot5-vision-04-00025">
        <title>2.5.5. Discussions</title>
        <p>In the above sections, various parts of the gaze data processing pipeline were described, and detailed visual and numerical exploration of the eye tracking data was done. With respect to the data demographics, no age bias could be found, and the effect of glasses were ruled out by having no participants wearing glasses during the experiments. There was also no gender bias found in the data. The chin rest had to be adjusted according to individual participants’ heights to make the participants head level with the center of the display screen and so that their eyes could be clearly visible to the eye tracker’s camera. Participants were made familiar with the experiments through instructions and pilot experiments.</p>
        <p>The gaze data showed varied levels of in-homogeneities, and only after outlier removal could distinct gaze error patterns be observed. One significant aspect noted is that the gaze errors from the tablet are much lower than the errors from the desktop for the same user distances. This could indicate that the distinguishing aspect between the two platforms, which is the display size and resolution, could be a factor determining the error levels for an eye tracker. However, further comparison of the data from the two platforms was not done in this study. Other takeaways from this section include the identification of robust outlier removal methods for gaze data and observations of the gaze error distributions, which were heavily affected by operating conditions but were not observable in the raw gaze data plots. Additionally, studies on the correlation of different eye tracking datasets (<xref ref-type="fig" rid="vision-04-00025-f009">Figure 9</xref>) revealed an important aspect: Under different operating conditions, an eye tracker’s data may behave in totally independent ways, which are not related to the tracker’s data characteristics under stable conditions.</p>
        <p>In order for other researchers to compare their eye trackers’ data characteristics with the data collected during this study, the full set of data collected for the different setups and operating conditions was provided in the Mendeley open data repository and can be accessed in the link here: <uri xlink:href="https://data.mendeley.com/datasets/cfm4d9y7bh/1">https://data.mendeley.com/datasets/cfm4d9y7bh/1</uri> (See <xref ref-type="app" rid="app1-vision-04-00025">Supplementary Materials</xref> section at the end of this paper).</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec3-vision-04-00025">
    <title>3. Identification of Error Patterns in Eye Gaze Data</title>
    <sec id="sec3dot1-vision-04-00025">
      <title>3.1. Objectives and Task Definition for Classification of Desktop and Tablet Data</title>
      <p>In this section, the main goal was to identify a certain error source or operating condition solely from the output data from an eye tracker, which was influenced by the condition. For this purpose, multiple classifier models were trained using data collected from the different eye tracking experiments, which were seen to produce different error patterns. The objective was to see if machine learning models [<xref rid="B17-vision-04-00025" ref-type="bibr">17</xref>,<xref rid="B18-vision-04-00025" ref-type="bibr">18</xref>] could learn to distinguish between these gaze error patterns as they appear among a mix of data captured under different operating conditions.</p>
      <p>The following classification tasks were performed using the desktop datasets: (1) Classification of errors for different user distances (i.e., between four classes of data from a user distance of 50-, 60-, 70-, 80-cm datasets), (2) classification of errors for different head poses (i.e., between four classes: neutral pose, roll (20 degrees), pitch (20 degrees), and yaw (20 degrees) datasets), and (3) classification between the head pose and user distance errors patterns (i.e., between four user distance classes, and three head pose classes, with a total of seven classes). With data from the tablet experiments, the following classification tasks were implemented: (1) Classification of errors for different user distances (i.e., between four classes of user distances data), (2) error classification for different tablet orientation poses (i.e., between four classes of data from the neutral tablet pose, roll (20 degrees), pitch (20 degrees), and yaw (20 degrees) datasets), and (3) classification between the tablet orientation and user distance error patterns (i.e., between four user distance classes, and three tablet pose classes, with a total of seven classes).</p>
      <p>Before implementing the classification algorithms on both the desktop or tablet data, the user distance, head-pose, and tablet orientation datasets were augmented to increase the number of training samples [<xref rid="B19-vision-04-00025" ref-type="bibr">19</xref>,<xref rid="B20-vision-04-00025" ref-type="bibr">20</xref>]. After this, training features were constructed and formatted before being input into the models. Same data augmentation strategies were used for both the desktop and tablet data. For the training and testing, the gaze angle, yaw, and pitch feature datasets were created and used. The classification results from the desktop data are provided in <xref ref-type="sec" rid="sec3dot3-vision-04-00025">Section 3.3</xref> while that from the the tablet datasets are in <xref ref-type="sec" rid="sec3dot4-vision-04-00025">Section 3.4</xref>.</p>
    </sec>
    <sec id="sec3dot2-vision-04-00025">
      <title>3.2. Data Augmentation Strategies</title>
      <p>With our dataset size (20 persons × 4 operating conditions each for the user distance and head pose), in order to use a sufficient number of features for classification without facing the overfitting problem, augmentation of the dataset was essential. In this study, 10-fold augmentation strategies were used on the gaze angle, yaw, and pitch error datasets estimated from raw data using Equations (8)–(10). The methods used for data augmentation were as follows: (1) The addition of Gaussian noise: Gaze error magnitudes at all data points were perturbed with Gaussian noise with 0 mean and 0.2 sigma [<xref rid="B21-vision-04-00025" ref-type="bibr">21</xref>]; (2) addition of jitter: Human eye jitter was modelled as pink noise [<xref rid="B22-vision-04-00025" ref-type="bibr">22</xref>,<xref rid="B23-vision-04-00025" ref-type="bibr">23</xref>], in which the power spectral density is inversely proportional to the frequency of the signal, given by PSD = 1/f<sup>α</sup>. The jitter signal was simulated by first generating white noise with the mean, sigma (0, 0.2), and successively applying a pink noise filter with the parameters α = 0.8 at a frequency of 2Hz and added to the error datasets; (3) interpolation: Linear interpolation on the input data points was used to produce variants of the original gaze error samples [<xref rid="B24-vision-04-00025" ref-type="bibr">24</xref>]; (4) convolution: The error signals were convolved with a raised cosine kernel with a window size of N = 30 with the form of Equation (13) below to produce a smoothed variant of the original data [<xref rid="B25-vision-04-00025" ref-type="bibr">25</xref>]:<disp-formula id="FD10-vision-04-00025"><label>(13)</label><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo> </mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>cos</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="sans-serif">π</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The convolution operation was given by:<disp-formula id="FD11-vision-04-00025"><label>(14)</label><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mo>∗</mml:mo><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mo>∞</mml:mo></mml:mrow><mml:mo>∞</mml:mo></mml:msubsup><mml:mi mathvariant="normal">f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="sans-serif">τ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="sans-serif">τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>(5) time shifting: The gaze data were shifted by 10 samples to estimate new variants in error values from the shifted datasets [<xref rid="B26-vision-04-00025" ref-type="bibr">26</xref>]; (6) combinations: Combinations of the addition of different noise patterns to interpolated signal were used to augment the dataset; and (7) flipping: Horizontal and vertical flipping of error magnitudes at the different AOIs were used to augment the dataset as well as to remove any viewer bias towards the top, bottom, and side locations of the screen. In horizontal flipping, the error magnitudes of the top AOI-1 to AOI 5 (<xref ref-type="fig" rid="vision-04-00025-f002">Figure 2</xref>b) were replaced by the values of the bottom AOIs (AOI-11 to AOI 15). In vertical flipping, the error magnitudes of the left AOIs (AOI No. 1,6,11) were swapped with the right AOIs, (No. 5,10,15). <xref ref-type="fig" rid="vision-04-00025-f010">Figure 10</xref> shows how the samples from a dataset were modified by these augmentation methods. <xref rid="vision-04-00025-t003" ref-type="table">Table 3</xref> shows how training samples were created from the collected dataset.</p>
    </sec>
    <sec id="sec3dot3-vision-04-00025">
      <title>3.3. Feature Engineering, Exploration, and Selection</title>
      <p>The original data sample set in this study had 20 participants and for each participant, there were three categories, i.e., gaze angular error, yaw error, and pitch error (Equations (8)–(10)) from which training features were computed [<xref rid="B27-vision-04-00025" ref-type="bibr">27</xref>,<xref rid="B28-vision-04-00025" ref-type="bibr">28</xref>,<xref rid="B29-vision-04-00025" ref-type="bibr">29</xref>,<xref rid="B30-vision-04-00025" ref-type="bibr">30</xref>]. The training feature set was constructed by estimating the error magnitudes at 15 AOI locations distributed all over the display screen (to account for the spatial distribution of errors), and statistical values for each sample, i.e., mean error (µ), error standard deviation (σ), interquartile range (IQR), and upper and lower bounds of the 95% confidence interval of the sample. Thus, each training sample had 20 features as follows:<disp-formula>[Error_AOI-1, Error_AOI-2, Error_AOI-15, µ, σ, IQR, 95% conf upper, 95% conf lower]<sub>sample</sub>.<label>(15)</label></disp-formula></p>
      <p>The above feature set was calculated from the gaze angle as well as the yaw and pitch angle data for each sample. To train the machine learning models, the features from all datasets were standardized so that they had a zero mean and unit variance and were shuffled randomly before splitting the datasets into the train and test sections. The proportion of train vs. test samples was varied between 0.4 to 0.25. <xref rid="vision-04-00025-t003" ref-type="table">Table 3</xref> below describes the contents of all the training and test datasets used in this study.</p>
      <p>For visualization of the high-dimensional feature set, the t-distributed stochastic neighbor embedding (t-SNE) method [<xref rid="B31-vision-04-00025" ref-type="bibr">31</xref>] was used, which maps data points x<sub>i</sub> in the high-dimensional feature space R<sup>D</sup> (D is the dimension of the feature set, D = 20 here) to points y<sub>i</sub> in a lower d-dimensional space (R<sup>d</sup>, here d= 2) by finding similarities (p<sub>ji</sub>) between the data and learning the corresponding low dimensional mapping points y<sub>1</sub>, …, y<sub>N</sub> (with y<sub>i</sub> ϵR<sup>d</sup> {\displaystyle \mathbf {y} _{1},\dots, \mathbf {y} _{N}} that reflects these similarities {\displaystyle p_{ij}} as best as possible [<xref rid="B32-vision-04-00025" ref-type="bibr">32</xref>,<xref rid="B33-vision-04-00025" ref-type="bibr">33</xref>,<xref rid="B34-vision-04-00025" ref-type="bibr">34</xref>]. The pairwise similarity (p<sub>ji</sub>) between points x<sub>i</sub> and x<sub>j</sub> is:<disp-formula id="FD12-vision-04-00025"><label>(16)</label><mml:math id="mm11"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo>‖</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mo>‖</mml:mo></mml:mrow><mml:msup><mml:mo> </mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo>‖</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mo>‖</mml:mo></mml:mrow><mml:msup><mml:mo> </mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>To observe the training data in the feature space, the t-SNE algorithm (with n_components = 2, perplexity = 80) was applied on the feature sets computed from the user distance, head pose, and platform pose datasets, as plotted in <xref ref-type="fig" rid="vision-04-00025-f011">Figure 11</xref>a,b for the desktop and tablet data, respectively. On the datasets from both the desktop and tablet, the t-SNE points look scattered with no structure. For the head pose data and platform pose data and mixed datasets, the t-SNE plots show several clusters, but within them, the class labels were found to be highly mixed. This reflects the high degree of complexity of the datasets used in this study since the different classes within them do not form any observable clusters; they are neither symmetrically distributed nor have any clear separation between them. This is close to real world gaze datasets in which normal gaze data is most often mixed with anomalous data due to unpredictable operating conditions.</p>
      <p>For feature selection, the relative importance of features (Equation (15)) were estimated using a random forest classifier method [<xref rid="B35-vision-04-00025" ref-type="bibr">35</xref>,<xref rid="B36-vision-04-00025" ref-type="bibr">36</xref>,<xref rid="B37-vision-04-00025" ref-type="bibr">37</xref>]. This model comprises of a number of decision trees in which it can be computed how much each feature reduces the weighted impurity [<xref rid="B38-vision-04-00025" ref-type="bibr">38</xref>] in the tree. The total impurity decrease due to each feature was averaged to rank the feature’s importance.</p>
      <p>Features computed on desktop and tablet datasets were fed to a random tree classifier model with hyperparameters (n_estimators = 200, max_depth = 8). The rankings of features for the desktop and tablet datasets are shown in <xref ref-type="fig" rid="vision-04-00025-f012">Figure 12</xref>a,b. In all the datasets, the mean, standard deviation, IQR, and confidence intervals (feature numbers 16–20) emerged as the most significant features. Based on this, the following reduced feature set was computed: [µ, σ, 95%Conf_up, 95%Conf_down, IQR]. This reduced feature set was used with SVM and KNN for classification. However, the neural network models required the full feature set to reduce the training error and prevent under-fitting.</p>
    </sec>
    <sec id="sec3dot4-vision-04-00025">
      <title>3.4. Classification Models: k-NN, SVM, and ANN</title>
      <p>After data augmentation and exploration of the feature set and labelling, the datasets were used with three different machine learning models from the Python Scikit-Learn libraries, run on a Windows 7 computer with core i7 2.6 GHz processor. A brief description of the models is given below.</p>
      <p>(a) K-nearest neighbors (k-NN): In this, the underlying assumption is that samples of the same class will be the nearest neighbors of each other, i.e., the distance between them will be small since they are related [<xref rid="B39-vision-04-00025" ref-type="bibr">39</xref>,<xref rid="B40-vision-04-00025" ref-type="bibr">40</xref>]. While using this method, the training samples form vectors in a multidimensional feature space, each with its class label. In the training phase of the algorithm, the feature vectors are stored along with class labels of the training samples. In the classification phase, the distance between an unlabelled input vector and the training dataset is calculated to get the k (a user defined hyperparameter number) nearest points of the input sample. Then, the input sample is categorized into the class of the majority in the k nearest points. Thus, if the training data is {(x<sub>1</sub>,y<sub>1</sub>), (x<sub>2</sub>,y<sub>2</sub>), …, (x<sub>N</sub>,y<sub>N</sub>)} and x is the feature vector of an input sample, the KNN finds the class of the input sample x using a distance metric [<xref rid="B41-vision-04-00025" ref-type="bibr">41</xref>,<xref rid="B42-vision-04-00025" ref-type="bibr">42</xref>], which in this study is the Euclidean distance given by:<disp-formula id="FD13-vision-04-00025"><label>(17)</label><mml:math id="mm12"><mml:mrow><mml:mrow><mml:msub><mml:mi>Dist</mml:mi><mml:mo> </mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>(b) Support vector machines (SVMs): It is a supervised learning method [<xref rid="B43-vision-04-00025" ref-type="bibr">43</xref>,<xref rid="B44-vision-04-00025" ref-type="bibr">44</xref>] that works on the principle of transforming the input feature space by a nonlinear transformation to a high-dimensional feature space, and searching for an optimal separating hyperplane for the input classes in this new high-dimensional space [<xref rid="B45-vision-04-00025" ref-type="bibr">45</xref>,<xref rid="B46-vision-04-00025" ref-type="bibr">46</xref>]. With this separating hyperplane, the training data xi with labels yi can be classified so that the minimal distance of each point from the hyperplane is maximized. The training data are depicted as an instance-label pair (xi, yi), i = 1,...,m, where xi ϵ Rn represents the input vector and yi ϵ (−1,1) is the corresponding output label of xi. The objective function for SVMs can be defined as:<disp-formula id="FD14-vision-04-00025"><label>(18)</label><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="normal">b</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="sans-serif">ξ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>‖</mml:mo><mml:mi>w</mml:mi><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">C</mml:mi><mml:mo> </mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msup><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>
with the condition:<disp-formula>y<sub>i</sub>(w<sup>T</sup>x +b)≥ 1 − ξ<sup>2</sup> (i = 1, … N)<label>(19)</label></disp-formula>
where C &gt; 0 is the regularization parameter and ξ<sub>i</sub> is the slack variable, w is the weight vector, and b is the offset. To classify unlabelled examples x<sub>k</sub> according to labels y<sub>k</sub> using the kernel function K(x<sub>i</sub>, x<sub>k</sub>), the optimal separating hyperplane equation becomes:<disp-formula id="FD17-vision-04-00025"><label>(20)</label><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:msub><mml:mi mathvariant="normal">k</mml:mi><mml:mo> </mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mi>sign</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub><mml:mi>ϵ</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo> </mml:mo><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where S is the set of support vectors xi, and ai are Lagrange multipliers (used for solving the optimization problem). In this study, a Gaussian radial basis function or RBF kernel is used, which is given by:<disp-formula id="FD18-vision-04-00025"><label>(21)</label><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="sans-serif">γ</mml:mi><mml:mrow><mml:mo>‖</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:mo>‖</mml:mo></mml:mrow><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The hyperparameters C and γ are chosen for optimal fitting and discussed in the next section.</p>
      <p>(c) Multilayer perceptrons (MLP) or neural networks: There are supervised learning algorithms [<xref rid="B47-vision-04-00025" ref-type="bibr">47</xref>,<xref rid="B48-vision-04-00025" ref-type="bibr">48</xref>,<xref rid="B49-vision-04-00025" ref-type="bibr">49</xref>,<xref rid="B50-vision-04-00025" ref-type="bibr">50</xref>], which takes in labelled training examples and solves the complex non-linear hypothesis by a network of computing units called “neurons”. Learning in an MLP takes place by updating the connection weights of each neuron after passing a batch of data samples from input to output, depending on the amount of error in the output compared to the target. The general update rule for weights (Δw<sub>ji</sub>) in an MLP based on backpropagation and the gradient descent is:<disp-formula id="FD19-vision-04-00025"><label>(22)</label><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">Δ</mml:mi><mml:msub><mml:mi mathvariant="normal">w</mml:mi><mml:mi>ji</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo> </mml:mo><mml:mi mathvariant="sans-serif">η</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="normal">E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi mathvariant="normal">w</mml:mi><mml:mrow><mml:mrow><mml:mi>ij</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where y<sub>i</sub>(n) is the output of the previous neuron and η {\displaystyle \eta} is the learning rate, and E is the error in the nth node. In this study, neural networks with ReLU activation, Adam optimizer, and a constant learning rate of 0.001 were used.</p>
    </sec>
    <sec id="sec3dot5-vision-04-00025">
      <title>3.5. KNN, SVM, and MLP-Based Classification Results on Desktop Data</title>
      <p>In the sub-sections below, the results from training the three ML models with the dataset described in <xref rid="vision-04-00025-t003" ref-type="table">Table 3</xref> are presented, along with statistics of the learning process. The models used here were optimized through experimentation to select the set of hyperparameters that best improve their cross-validation scores. In some cases, the results show underfitting, especially in seven class classification problems, which are inherently complex. For this kind of problem with little separation between classes, model underfitting is a commonly observed issue.</p>
      <sec id="sec3dot5dot1-vision-04-00025">
        <title>3.5.1. Results, Task I: Classification of Desktop Data for Different User Distances</title>
        <p>The KNN, SVM, and MLP models were used for the classification of gaze datasets corresponding to four different classes of user distances (50, 60, 70, 80 cm) and the results are given in <xref rid="vision-04-00025-t004" ref-type="table">Table 4</xref>. In the table, only the cross-validation accuracies are mentioned. A training-test sample proportion of 30% and 10-fold cross validation was used in all cases [<xref rid="B51-vision-04-00025" ref-type="bibr">51</xref>]. For all models, grid search [<xref rid="B52-vision-04-00025" ref-type="bibr">52</xref>] with cross-validation was used to determine the optimal hyperparameters. For K-NN, the optimal number of neighbors was found to be 3. For SVM, the RBF kernel was used [<xref rid="B53-vision-04-00025" ref-type="bibr">53</xref>], with the soft margin cost function parameter C set to 10 and gamma set to 1.0. C defines the trade-off between misclassification and the simplicity of the decision surface. Gamma is the RBF kernel parameter (Equation (21)). For MLP, the grid search yielded a best hidden layer size of 3 with neuron configurations of (50, 100, 50) units in the layers, with a full training set of 20 features being used.</p>
      </sec>
      <sec>
        <title>3.5.2. Results, Task II: Classification of Desktop Data for Different Head Poses</title>
        <p>In this task, the gaze datasets collected from the desktop setup for different user head poses were used. The head pose datasets include the gaze error values for the head roll (20 degrees), pitch (20 degrees), yaw (20 degrees), and neutral (roll, pitch, yaw = 0 degrees). The KNN, SVM, and MLP models were used and the classification accuracies for the different classifiers are tabulated in <xref rid="vision-04-00025-t005" ref-type="table">Table 5</xref>.</p>
        <p>Using the grid search, for KNN, the best number of neighbors was found to be 3 and a reduced feature set was used. For SVM, a C value of 10 and gamma of 1.25 was used with the reduced feature set. For the MLP model, a three-layer network was used, and its architecture was tuned by varying the number of units in each layer between 50 and 100 and varying the regularization parameter values between 0.001 to 0.5 to control overfitting. Additionally, the full feature set was used for training.</p>
      </sec>
      <sec>
        <title>3.5.3. Results, Task III: Classification on Merged User Distance and Head Pose Datasets from Desktop</title>
        <p>In this task, the head-pose and user distance datasets were merged, and classifiers were applied to do a seven-class classification on this mixed dataset. The classification results are presented in <xref rid="vision-04-00025-t004" ref-type="table">Table 4</xref> and the confusion matrix for the MLP model is shown in <xref ref-type="fig" rid="vision-04-00025-f013">Figure 13</xref>. Since the head pose data was collected at a user distance of 60 cm, the neutral head pose data and the UD60 data are the same, and therefore only one of these two datasets were used to avoid a class imbalance in the mixed dataset. As above, the three classifiers were trained and tested on this dataset. For the KNN, the best number of neighbors was chosen to be 3 as shown in <xref ref-type="fig" rid="vision-04-00025-f014">Figure 14</xref>a below, which shows the dependence of the train, test, and cross-validation error on the number of neighbors l. For the SVM model, the parameters were the same for the above two tasks. For the MLP, 2 hidden layers with 100 units each and a regularization value of 0.001 were used with the full set of features. <xref rid="vision-04-00025-t005" ref-type="table">Table 5</xref> shows the performance of the three classifiers on different datasets, with true and false detection rates and precision values. <xref ref-type="fig" rid="vision-04-00025-f014">Figure 14</xref>b−d show the dependence of MLP parameters on model accuracy. The KNN and MLP classification performances are close, with KNN performing well for all the datasets.</p>
      </sec>
    </sec>
    <sec id="sec3dot6-vision-04-00025">
      <title>3.6. KNN, SVM, and MLP-Based Classification Results on Tablet Data</title>
      <p>Here the results from using the tablet data with various classification models are presented. These include datasets for different user-tablet distances (50, 60, 70, 80 cm) and tablet poses of the neutral, platform roll, pitch, and yaw of 20 degrees. A mixed dataset was created by merging the datasets for the user distance and platform pose, and classifier models were trained to do seven class classifications, i.e., distinguish between four user distance and three tablet pose classes.</p>
      <sec id="sec3dot6dot1-vision-04-00025">
        <title>3.6.1. Results, Task IV: Classification of Tablet Data for Different User Distances</title>
        <p>For the user distance dataset from the tablet platform, the KNN classifier with three neighbors was used. The SVM was used with the RBF kernel, C = 10 and gamma of 1, and the MLP classifier was used with 3 layers, 200 units each, and with a regularization value of 0.0001. The best performance was by MLP and the classification results are shown in <xref rid="vision-04-00025-t006" ref-type="table">Table 6</xref>.</p>
      </sec>
      <sec id="sec3dot6dot2-vision-04-00025">
        <title>3.6.2. Results, Task V: Classification of Tablet Data for Different Tablet Poses</title>
        <p>For the tablet pose dataset, the KNN classifier with three neighbors was used. The SVM was used with RBF kernel, C = 5, and gamma of 0.5. The MLP model was used with 2 layers, 100 units each, and a regularization value of 0.0001. The best classification results were by KNN and are shwon in <xref rid="vision-04-00025-t006" ref-type="table">Table 6</xref>.</p>
      </sec>
      <sec id="sec3dot6dot3-vision-04-00025">
        <title>3.6.3. Results, Task VI: Classification of Tablet Data for Mixed User Distance and Tablet Pose Datasets</title>
        <p>For the mixed user distance and platform pose datasets, the KNN classifier with three neighbors was used. The SVM was used with RBF kernel, C = 5, and gamma of 0.5 and the MLP classifier was used with 2 layers, 100 units each, and with a regularization value of 0.0001. Both SVM and MLP performed well and the classification results are shown in <xref rid="vision-04-00025-t006" ref-type="table">Table 6</xref>.</p>
      </sec>
    </sec>
    <sec id="sec3dot7-vision-04-00025">
      <title>3.7. Discussion</title>
      <p>The results demonstrate that ML models can distinguish between gaze data collected under normal and varying operating conditions. Thus, when using these models, anomalies present in gaze datasets may be detected. This is similar to the anomaly detection approaches [<xref rid="B54-vision-04-00025" ref-type="bibr">54</xref>,<xref rid="B55-vision-04-00025" ref-type="bibr">55</xref>,<xref rid="B56-vision-04-00025" ref-type="bibr">56</xref>] used in fields, such as cyber intrusion and video surveillance, where training sets of normal and nominal examples are used to design a decision rule such that occurrences of erroneous data samples are detected.</p>
      <p>For the classification tasks, gaze datasets were constructed such that they contained signatures of a single or multiple error sources. It was found through the decision tree-based feature selection technique that statistical attributes, such as gaze error confidence levels and interquartile ranges, are significant parameters that can be used to distinguish gaze error sources. In the classification tasks, the KNNs and MLP classifiers performed the best for all datasets, although the MLP models take more time to converge. Additionally, it is seen from <xref rid="vision-04-00025-t004" ref-type="table">Table 4</xref> and <xref rid="vision-04-00025-t006" ref-type="table">Table 6</xref> that using the gaze yaw and pitch features result in better detection accuracies than the frontal gaze datasets. From <xref rid="vision-04-00025-t005" ref-type="table">Table 5</xref> and <xref rid="vision-04-00025-t007" ref-type="table">Table 7</xref>, it is seen that the rate of false detections is also low for all the models. All the datasets, especially the ones created with seven classes, were found to be quite complex as no class separation could be detected from the t-SNE results. Despite this, the classification models achieved a cross-validation score of 85–90% on most datasets. This shows the feasibility of the ML models in detecting gaze error sources even from complex gaze datasets where more than one source is present.</p>
    </sec>
  </sec>
  <sec id="sec4-vision-04-00025">
    <title>4. Modelling and Prediction of Gaze Errors</title>
    <p>Using the collected desktop and tablet datasets, regression models [<xref rid="B57-vision-04-00025" ref-type="bibr">57</xref>,<xref rid="B58-vision-04-00025" ref-type="bibr">58</xref>] were trained to predict gaze estimation errors using the gaze angle, yaw, and pitch angle values as input features. Two different models were built, which may be used to predict the gaze errors of an eye tracker under two different operating conditions. These include the head-pose error model, which can be used to predict gaze errors occurring due to various user head poses. The other is the platform pose error model, for predicting gaze error corresponding to different poses of the eye tracker. The gaze errors were predicted as a function of a user’s gaze angle and gaze yaw/pitch angles (Equation (28)). The results from the various gaze error models are presented in sub-<xref ref-type="sec" rid="sec4dot1-vision-04-00025">Section 4.1</xref> and <xref ref-type="sec" rid="sec4dot2-vision-04-00025">Section 4.2</xref> below.</p>
    <p>A regression model describes a dependent variable Y as a function of an independent variable x with the generic relation: Y = a + b1X1 + b2X2 + b3X3 + … + btXt + u, where X1, X2... Xt are the independent variables, a is the intercept, b is the slope, and u is the regression residual [<xref rid="B59-vision-04-00025" ref-type="bibr">59</xref>]. The cost function used for the evaluation of the model fit is given by the root mean squared error or RMSE:<disp-formula id="FD20-vision-04-00025"><label>(23)</label><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mi>RMSE</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">n</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="normal">y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mrow></mml:math></disp-formula>
where n is the number of observations, yi is the true value of target to predict, and <inline-formula><mml:math id="mm1111"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="normal">y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the model’s predicted result. The optimization function of a standard linear regression can be expressed as:<disp-formula>min || Xw − y ||<sup>2</sup>,<label>(24)</label></disp-formula>
where <italic>X</italic> is the set of feature variables, <italic>w</italic> represents the weights, and <italic>y</italic> comprises of the ground truth data points. In this study, several regression models were used, including Ridge, Lasso, ElasticNet [<xref rid="B60-vision-04-00025" ref-type="bibr">60</xref>,<xref rid="B61-vision-04-00025" ref-type="bibr">61</xref>], and neural network-based models [<xref rid="B62-vision-04-00025" ref-type="bibr">62</xref>]. In Ridge regression, the issue of high variance is mitigated through the addition of a squared bias factor as regularization in the form:<disp-formula>min || Xw—y ||<sup>2</sup> + z|| w ||<sup>2</sup>.<label>(25)</label></disp-formula></p>
    <p>Whereas in Lasso regression, an absolute value bias of the form below is used:<disp-formula>min || Xw—y ||<sup>2</sup> + z|| w ||.<label>(26)</label></disp-formula></p>
    <p>The ElasticNet regression uses the regularization factors of both the above techniques:<disp-formula>min || Xw—y ||<sup>2</sup> + z_1|| w || + z_2|| w ||<sup>2</sup>.<label>(27)</label></disp-formula></p>
    <p>In this study, the regression models were used to map the gaze frontal and yaw, and pitch angle values to gaze errors produced under different operating conditions. The task of the regression algorithms is not only to find the mapping between input and output variables but also take into consideration the interactions between the input variables. The input features (X1, X2, X3) were:<disp-formula>[Gaze_Angle, Gaze_Yaw, Gaze_Pitch] → Gaze error.<label>(28)</label></disp-formula></p>
    <p>For the regression tasks, the input features were standardized, such that their distributions had a mean value of 0 and standard deviation of 1. Six different regression models for predicting the gaze errors were trained on the features created using Equation (28) on the head pose (desktop) and platform pose (tablet) datasets. For each model, the fit estimates in RMSE values are summarized in <xref rid="vision-04-00025-t008" ref-type="table">Table 8</xref>. The coefficients of the best performing models are listed in <xref rid="vision-04-00025-t009" ref-type="table">Table 9</xref>.</p>
    <sec id="sec4dot1-vision-04-00025">
      <title>4.1. Head Pose Error Model</title>
      <p>The plots for the actual gaze error values due to the head pose (obtained from desktop setup) and the error values predicted after training by the different models are shown in <xref ref-type="fig" rid="vision-04-00025-f015">Figure 15</xref>a–d.</p>
      <p><xref rid="vision-04-00025-t008" ref-type="table">Table 8</xref> shows that ElasticNet (with penalty parameter = 0.5) has the lowest prediction errors for all head pose datasets. The regularization used for the Ridge and Lasso models is 0.001. The MLP model is used with 1 hidden layer with 100 units, ReLU activation, and a regularization value of 0.001.</p>
    </sec>
    <sec id="sec4dot2-vision-04-00025">
      <title>4.2. Platform Pose Error Model</title>
      <p>For the tablet pose dataset, the ElasticNet performs well (<xref ref-type="fig" rid="vision-04-00025-f016">Figure 16</xref>a–d), with a penalty parameter value of 0.5. The MLP and polynomial regression models were seen to overestimate the error values. It was seen that outliers and noise strongly affect all the models and therefore outlier removal and data standardization methods to convert the input features to have a normal distribution were essential.</p>
    </sec>
    <sec id="sec4dot3-vision-04-00025">
      <title>4.3. Establishment of the Error Models</title>
      <p>As described above, a regression model with three predictor variables can be expressed as:<disp-formula>Y = B<sub>0</sub> + B<sub>1</sub>*X<sub>1</sub> + B<sub>2</sub>*X<sub>2</sub> + B<sub>3</sub>*X<sub>3</sub>,<label>(29)</label></disp-formula>
where B<sub>0</sub> is the intercept; B<sub>1</sub>, B<sub>2</sub>, and B<sub>3</sub> are coefficients; and X<sub>1</sub>, X<sub>2</sub>, and X<sub>3</sub> are input features (gaze, yaw, pitch angles). Since the ElasticNet model had the lowest RMSE for predicting the head and tablet pose errors, the B<sub>0</sub>, B<sub>1</sub>, B<sub>2</sub>, and B<sub>3</sub> parameters of this model were computed for the head pose and tablet pose datasets and are presented in <xref rid="vision-04-00025-t009" ref-type="table">Table 9</xref>. With these parameters, the head and platform pose error models may be constructed for error prediction using gaze angular variables as the input.</p>
    </sec>
  </sec>
  <sec id="sec5-vision-04-00025">
    <title>5. MLGaze: An Open Machine Learning Repository for Gaze Error Pattern Recognition</title>
    <p>In this study, the major aim was to present the research direction on the pattern recognition of gaze errors and to show, as a proof of concept, that machine learning algorithms can be used to distinguish gaze error patterns created due to different operating conditions of an eye tracker. To test and replicate the principles, the full set of gaze data collected for this study was provided in the Mendeley data repository as mentioned in <xref ref-type="sec" rid="sec2dot5dot5-vision-04-00025">Section 2.5.5</xref> above.</p>
    <p>For any eye tracker other than the Eyetribe, the gaze error characteristics will be different based on the tracking principle and tracker hardware. Therefore, for other eye trackers, the classifier models described in this study have to be re-trained using data from the respective eye trackers if their characteristics vary widely from Eyetribe (which can be determined by comparing the respective eye tracker’s data with the data provided in the Mendeley data repository). To facilitate this and make it possible for researchers to replicate the principles of this study on their own eye tracker’s data, the codes for implementing the full data processing pipeline as described in this paper and in <xref ref-type="fig" rid="vision-04-00025-f001">Figure 1</xref> are provided in an open repository called MLGaze, hosted on GitHub ((<xref ref-type="fig" rid="vision-04-00025-f017">Figure 17</xref>a). The link to the repository may be found here: <uri xlink:href="https://github.com/anuradhakar49/MLGaze">https://github.com/anuradhakar49/MLGaze</uri>.</p>
    <p>There are eight python scripts in the repository (<xref ref-type="fig" rid="vision-04-00025-f017">Figure 17</xref>b), and these are fully documented. The codes in data_augmentation.py and outlier_removal.py may be used for data augmentation and outlier removal as described in <xref ref-type="sec" rid="sec3dot2-vision-04-00025">Section 3.2</xref> and <xref ref-type="sec" rid="sec2dot4-vision-04-00025">Section 2.4</xref>, respectively. Code create_features.py is for generating training features, whose output may be used with the codes train_SVM.py, train_KNN.py, and train_MLP.py to train KNN, SVM, and MLP models, respectively, as described in <xref ref-type="sec" rid="sec3dot4-vision-04-00025">Section 3.4</xref> and <xref ref-type="sec" rid="sec3dot5-vision-04-00025">Section 3.5</xref>. Code regression_models.py is for running all the regression models described in <xref ref-type="sec" rid="sec4-vision-04-00025">Section 4</xref>. The codes for ML models also produce the confusion matrices and detection scores as well as learning curves as described in <xref ref-type="sec" rid="sec3-vision-04-00025">Section 3</xref> and <xref ref-type="app" rid="app3-vision-04-00025">Appendix B</xref>. Apart from this, sample CSV files are also included, which can be used to test the coding resources. The repository was released under the GNU GPLv3 license.</p>
  </sec>
  <sec sec-type="conclusions" id="sec6-vision-04-00025">
    <title>6. Conclusions and Future Work</title>
    <p>In contemporary eye gaze research, there is a scarcity of analytical methods for studying the variability of gaze accuracy in eye tracking data acquired under unconstrained conditions. There is also a lack of datasets that provide gaze and ground truth information for different operating conditions, and gaze datasets labelled with different experimental scenarios are absent. Therefore, for this study, a new and diverse gaze dataset was collected and labelled with different operating conditions. The dataset named NUIG_EyeGaze01(Labelled eye gaze dataset) comprises of fixation data from 20 participants captured from different user platforms and operating conditions described in this paper. This dataset was then analyzed to identify the presence and to model the impacts of the above conditions on the gaze error levels.</p>
    <p>Several new approaches with respect to gaze error pattern analysis were implemented in this study. This includes the use of multiple strategies for outlier removal, and it was found that the median filtering approach, among others, was successful in de-noising gaze data from all the collected datasets. Next, various gaze data augmentation methods were applied, which helped to increase the size of the collected gaze datasets by an order of magnitude and introduce a lot of variabilities within the datasets. The choice of gaze error features and use of a random forest-based feature selection method helped to reveal insights about the gaze error characteristics, e.g., that the interquartile range and confidence intervals are significant indicators for distinguishing gaze error patterns.</p>
    <p>The t-SNE algorithm is a relatively new concept in machine learning, which was used in this study to visualize the distribution of classes within the gaze datasets, when gaze data influenced by different error sources were mixed together. This technique demonstrated the complexities of the gaze datasets used in this study, which are close to what might be expected in gaze data from unconstrained practical applications. Finally, the use of machine learning for classification and prediction of gaze error patterns in artificially created heterogeneous gaze datasets as done in this study is a new concept, which has been sparsely explored before.</p>
    <p>The results from the training of the ML models reveal that these could be highly robust in identifying gaze error patterns even in complex gaze datasets. It was seen that classifier models can successfully distinguish between eye tracking data collected under normal conditions and data collected under varying operating conditions, such as a high degree of head pose variations, different user distances, and platform poses. With these classifiers, familiar error patterns in eye gaze datasets may be detected, and gaze error patterns that do not match the normal gaze behavior may be identified and recognized as new error types.</p>
    <p>The various concepts developed in this study, including the application of classifier and regression models to gaze error data, may be used to build improved eye tracking systems and algorithms and also to obtain better results from them. The main benefit of training the classifier models is that they can distinguish anomalous gaze data present in realistic and complex gaze datasets as used in this study and possibly recognize what caused them (e.g., impact of user distance or head pose). This can help eye gaze application developers, researchers, or engineers to deploy appropriate prevention methods, compensation strategies, or setup improvements to reduce the impacts of the error sources affecting their data.</p>
    <p>The regression models may be used predict how an eye tracker might behave under practical operating conditions like a high degree of head pose or platform pose variations and forecast the possible levels of error caused by different error sources. This can help eye gaze researchers or engineers to quantitatively specify the limits of their systems while operating under these challenging conditions and also develop suitable error correction methods.</p>
    <p>Additionally, the outlier detection techniques described in this study could be useful for applications to any gaze dataset prior to analyzing them, to observe underlying data patterns. Overall, the gaze data analysis pipeline as implemented in this study could be of use to any eye gaze researcher or engineer who wishes to gain deeper insights into their collected gaze data and have an estimate about the impacts of various non-ideal operating conditions on their eye tracking system.</p>
    <p>There are also several ways to extend the study presented here since extracting and analyzing error patterns from gaze data is a new and unexplored field in gaze research. For example, deep neural networks may be used for the detection of anomalous gaze data collected from dynamic eye tracking applications under unconstrained operating conditions. As mentioned in <xref ref-type="sec" rid="sec1dot2-vision-04-00025">Section 1.2</xref>, in this study, the influence of one operating was considered at a time while collecting gaze data. However, complex scenarios may arise, where gaze data may be affected by more than one error source, such as, for example, in automotive use cases. The problem of a multi-factor influence on gaze data could be an interesting and complex research question but can only be answered when relevant and labelled data is available. Therefore, the collection of relevant gaze datasets from complex operational scenarios using desktop, handheld, head mounted, or automotive setups to understand a diverse range of gaze error patterns could be another potential future study in this domain.</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="app1-vision-04-00025">
      <title>Supplementary Materials</title>
      <p>The following are available online at <uri xlink:href="https://www.mdpi.com/2411-5150/4/2/25/s1">https://www.mdpi.com/2411-5150/4/2/25/s1</uri>. The eye gaze dataset collected from desktop and tablet platforms and used in this study is available online at: <uri xlink:href="https://data.mendeley.com/datasets/cfm4d9y7bh/1">https://data.mendeley.com/datasets/cfm4d9y7bh/1</uri>. A GitHub repository containing the Python implementations of the gaze error pattern recognition pipeline is made available online at: <uri xlink:href="https://github.com/anuradhakar49/MLGaze">https://github.com/anuradhakar49/MLGaze</uri>. A supplementary video (AK_face_pose.mp4) showing the application of the head pose model used in this study on video frames in real time is available with this manuscript.</p>
      <supplementary-material content-type="local-data" id="vision-04-00025-s001">
        <media xlink:href="vision-04-00025-s001.zip">
          <caption>
            <p>Click here for additional data file.</p>
          </caption>
        </media>
      </supplementary-material>
    </app>
  </app-group>
  <notes>
    <title>Funding</title>
    <p>The research work presented here was funded under the Strategic Partnership Program of Science Foundation Ireland (SFI) and co-funded by FotoNation Ltd. Project ID: 13/SPP/I2868 on “Next Generation Imaging for Smartphone and Embedded Platforms”.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The author declares no conflict of interest.</p>
  </notes>
  <app-group>
    <app id="app2-vision-04-00025">
      <title>Appendix A</title>
      <p>The tablet was mounted on a gimbal holder as shown below in <xref ref-type="fig" rid="vision-04-00025-f0A1">Figure A1</xref>a. To measure the device angles, the data from the inertial measurement sensor of the tablet were used. These are highly accurate sensors found in most tablet/smartphones to estimate the orientation of the device with 0.5-1 degree of precision. There are apps like Accelerometer analyser for Windows tablets and smartphones, which can provide device orientation angles (in 3-D). Apart from this, the tablet orientation was verified by aligning a device (e.g., Android phone) with the tablet and running an Accelerometer app on the device to see if the two orientation results were similar. A screenshot from such an app (Tiltmeter, available on Google Play for Android) is shown in <xref ref-type="fig" rid="vision-04-00025-f0A1">Figure A1</xref>b. Mechanical protractors were occasionally used to manually verify the angles. Readings for one orientation were taken for all participants in consequent sessions to allow minimum tablet setup perturbation between the experiments.</p>
      <fig id="vision-04-00025-f0A1" orientation="portrait" position="anchor">
        <label>Figure A1</label>
        <caption>
          <p>(<bold>a</bold>) Gimbal tripod mount for the tablet; (<bold>b</bold>) 3-D position reading from inertial sensors using an smartphone/tablet app.</p>
        </caption>
        <graphic xlink:href="vision-04-00025-g0A1"/>
      </fig>
    </app>
    <app id="app3-vision-04-00025">
      <title>Appendix B</title>
      <p>For this study, the selection of the participant number was based on the criteria about the number of data samples required compared to the size of the feature set used for training the machine learning (ML) algorithms. For the ML models like SVM, KNN, and MLP, there is an empirical rule that the training dataset size should be at least 10 times the size of the feature set. This is also termed as the “rule of 10” [<xref rid="B63-vision-04-00025" ref-type="bibr">63</xref>]. Going by this rule, it may be seen that in this paper, the maximum size of the feature set is 20 (Equation (15)) for the classification problem. Therefore, data from 20 persons with 10-fold augmentation were used. Additionally, the gaze yaw and pitch features were included in the training set, so the total training data size was (20 × 3 × 10) = 600 samples per class. As an example, for the classification of the user distance data from the desktop or tablet, there were four classes and all three categories of gaze data were collected for each of them. So, the total number of training samples was 600 × 4 = 2400 samples for the 4-class classification problem (this is shown in <xref rid="vision-04-00025-t003" ref-type="table">Table 3</xref> above). This feature size was deemed to be useful while training and is supported by the learning curves for the different ML models whose plots are shown in <xref ref-type="fig" rid="vision-04-00025-f0A2">Figure A2</xref>a–d below for the KNN, SVM, and MLP classifiers and the ElasticNet regression model, respectively. The training set size in the figures indicates the number of data samples used for training the respective model.</p>
      <p>From the figures, it is seen that the training/cross validation accuracy for the different machine learning models converges and stabilizes into an asymptote within the training sample size for the classification and regression tasks and therefore it can be said that the training set size chosen is suitable for the machine learning model. Once the training and validation curves converge, it is unlikely that increasing the training data size will increase the model performance. While it is always possible to gather more data for studying the gaze error patterns, based on the trade-off between the available number of participants, the data collection time required, and the training/validation accuracy, the number of participants chosen in this study was found to be optimal.</p>
      <fig id="vision-04-00025-f0A2" orientation="portrait" position="anchor">
        <label>Figure A2</label>
        <caption>
          <p>Learning curves for (<bold>a</bold>) KNN, (<bold>b</bold>) SVM, and (<bold>c</bold>) MLP classifiers, and the (<bold>d</bold>) Elastic-Net model.</p>
        </caption>
        <graphic xlink:href="vision-04-00025-g0A2"/>
      </fig>
    </app>
  </app-group>
  <ref-list>
    <title>References</title>
    <ref id="B1-vision-04-00025">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Corcoran</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Performance Evaluation Strategies for Eye Gaze Estimation Systems with Quantitative Metrics and Visualizations</article-title>
        <source>Sensors</source>
        <year>2018</year>
        <volume>18</volume>
        <elocation-id>3151</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s18093151</pub-id>
        <?supplied-pmid 30231547?>
        <pub-id pub-id-type="pmid">30231547</pub-id>
      </element-citation>
    </ref>
    <ref id="B2-vision-04-00025">
      <label>2.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Cheon</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Gaze pattern analysis for video contents with different frame rates</article-title>
        <source>Proceedings of the 2013 Visual Communications and Image Processing (VCIP)</source>
        <conf-loc>Kuching, Malaysia</conf-loc>
        <conf-date>17–20 November 2013</conf-date>
        <fpage>1</fpage>
        <lpage>5</lpage>
      </element-citation>
    </ref>
    <ref id="B3-vision-04-00025">
      <label>3.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Mendis</surname>
            <given-names>B.S.U.</given-names>
          </name>
          <name>
            <surname>Gedeon</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Asthana</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Goecke</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>A Hybrid Fuzzy Approach for Human Eye Gaze Pattern Recognition</article-title>
        <source>ICONIP 2008: Advances in Neuro-Information Processing</source>
        <series>Lecture Notes in Computer Science</series>
        <person-group person-group-type="editor">
          <name>
            <surname>Köppen</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kasabov</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Coghill</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <year>2009</year>
        <volume>Volume 5507</volume>
      </element-citation>
    </ref>
    <ref id="B4-vision-04-00025">
      <label>4.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Horiguchi</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Suzuki</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sawaragi</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Nakanishi</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Takimoto</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Extraction and Investigation of Dominant Eye-Gaze Pattern in Train Driver’s Visual Behavior Using Markov Cluster Algorithm</article-title>
        <source>Proceedings of the 2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)</source>
        <conf-loc>Sapporo, Japan</conf-loc>
        <conf-date>25–28 August 2016</conf-date>
        <fpage>578</fpage>
        <lpage>581</lpage>
      </element-citation>
    </ref>
    <ref id="B5-vision-04-00025">
      <label>5.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Braunagel</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Geisler</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Stolzmann</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Rosenstiel</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Kasneci</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>On the necessity of adaptive eye movement classification in conditionally automated driving scenarios</article-title>
        <source>Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications (ETRA 16)</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2016</year>
        <fpage>19</fpage>
        <lpage>26</lpage>
      </element-citation>
    </ref>
    <ref id="B6-vision-04-00025">
      <label>6.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Koochaki</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Najafizadeh</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Predicting Intention Through Eye Gaze Patterns</article-title>
        <source>Proceedings of the 2018 IEEE Biomedical Circuits and Systems Conference (BioCAS)</source>
        <conf-loc>Cleveland, OH, USA</conf-loc>
        <conf-date>17–19 October 2018</conf-date>
        <fpage>1</fpage>
        <lpage>4</lpage>
      </element-citation>
    </ref>
    <ref id="B7-vision-04-00025">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pekkanen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lappi</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>A new and general approach to signal denoising and eye movement classification based on segmented linear regression</article-title>
        <source>Sci. Rep.</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-17983-x</pub-id>
        <?supplied-pmid 29255207?>
        <pub-id pub-id-type="pmid">28127051</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-vision-04-00025">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zemblys</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Niehorster</surname>
            <given-names>D.C.</given-names>
          </name>
          <name>
            <surname>Komogortsev</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Holmqvist</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Using machine learning to detect events in eye-tracking data</article-title>
        <source>Behav. Res. Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <fpage>160</fpage>
        <lpage>181</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-017-0860-3</pub-id>
        <pub-id pub-id-type="pmid">28233250</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-vision-04-00025">
      <label>9.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Barz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Daiber</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Sonntag</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Error-aware gaze-based interfaces for robust mobile gaze interaction</article-title>
        <source>Proceedings of the 2018 ACM Symposium on Eye Tracking Research &amp; Applications (ETRA’18)</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2018</year>
        <comment>Article 24</comment>
        <size units="pages">10p</size>
      </element-citation>
    </ref>
    <ref id="B10-vision-04-00025">
      <label>10.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Barz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Daiber</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Sonntag</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of gaze estimation error for error-aware gaze-based interfaces</article-title>
        <source>Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications (ETRA’16)</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2016</year>
        <fpage>275</fpage>
        <lpage>278</lpage>
      </element-citation>
    </ref>
    <ref id="B11-vision-04-00025">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chandola</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Banerjee</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>Anomaly Detection: A Survey</article-title>
        <source>ACM Comput. Surv. (CSUR)</source>
        <year>2009</year>
        <volume>41</volume>
        <fpage>15:1</fpage>
        <lpage>15:58</lpage>
        <pub-id pub-id-type="doi">10.1145/1541880.1541882</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-vision-04-00025">
      <label>12.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Diao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Read</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Stiegler</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bifet</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>EXAD: A System for Explainable Anomaly Detection on Big Data Traces</article-title>
        <source>Proceedings of the 2018 IEEE International Conference on Data Mining Workshops</source>
        <conf-loc>Singapore</conf-loc>
        <conf-date>17–20 November 2016</conf-date>
        <year>2018</year>
        <fpage>1435</fpage>
        <lpage>1440</lpage>
      </element-citation>
    </ref>
    <ref id="B13-vision-04-00025">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ishikawa</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Matthews</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Kanade</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Passive Driver Gaze Tracking with Active Appearance Models</article-title>
        <source>Proc. World Congr. Intell. Transp. Syst.</source>
        <year>2004</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1184/R1/6557315.v1</pub-id>
      </element-citation>
    </ref>
    <ref id="B14-vision-04-00025">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>L.H.</given-names>
          </name>
        </person-group>
        <article-title>Tri-state median filter for image denoising</article-title>
        <source>IEEE Trans. Image Process.</source>
        <year>1999</year>
        <volume>8</volume>
        <fpage>1834</fpage>
        <lpage>1838</lpage>
        <pub-id pub-id-type="doi">10.1109/83.806630</pub-id>
        <pub-id pub-id-type="pmid">18267461</pub-id>
      </element-citation>
    </ref>
    <ref id="B15-vision-04-00025">
      <label>15.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Khalil</surname>
            <given-names>H.H.</given-names>
          </name>
          <name>
            <surname>Rahmat</surname>
            <given-names>R.O.K.</given-names>
          </name>
          <name>
            <surname>Mahmoud</surname>
            <given-names>W.A.</given-names>
          </name>
        </person-group>
        <article-title>Chapter 15: Estimation of Noise in Gray-Scale and Colored Images Using Median Absolute Deviation (MAD)</article-title>
        <source>Proceedings of the 3rd International Conference on Geometric Modeling and Imaging</source>
        <conf-loc>London, UK</conf-loc>
        <conf-date>9–11 July 2008</conf-date>
        <fpage>92</fpage>
        <lpage>97</lpage>
      </element-citation>
    </ref>
    <ref id="B16-vision-04-00025">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Y.C.</given-names>
          </name>
        </person-group>
        <article-title>A tutorial on kernel density estimation and recent advances</article-title>
        <source>Biostat. Epidemiol.</source>
        <year>2017</year>
        <volume>1</volume>
        <fpage>161</fpage>
        <lpage>187</lpage>
        <pub-id pub-id-type="doi">10.1080/24709360.2017.1396742</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-vision-04-00025">
      <label>17.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Koydemir</surname>
            <given-names>H.C.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Nadkarni</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tseng</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Benien</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Ozcan</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>A survey of supervised machine learning models for mobile-phone based pathogen identification and classification</article-title>
        <source>Proceedings of the SPIE 10055, Optics and Bio-photonics in Low-Resource Settings III, 100550A (7 March 2017)</source>
        <publisher-name>International Society for Optics and Photonics</publisher-name>
        <publisher-loc>Washington, DC, USA</publisher-loc>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="B18-vision-04-00025">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qiu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A survey of machine learning for big data processing</article-title>
        <source>EURASIP J. Adv. Sig. Process.</source>
        <year>2016</year>
        <volume>2016</volume>
        <fpage>67</fpage>
        <pub-id pub-id-type="doi">10.1186/s13634-016-0355-x</pub-id>
      </element-citation>
    </ref>
    <ref id="B19-vision-04-00025">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bjerrum</surname>
            <given-names>E.J.</given-names>
          </name>
          <name>
            <surname>Glahder</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Skov</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Data Augmentation of Spectral Data for Convolutional Neural Network (CNN) Based Deep Chemometrics</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <pub-id pub-id-type="arxiv">1710.01927</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-vision-04-00025">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Polson</surname>
            <given-names>N.G.</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>S.L.</given-names>
          </name>
        </person-group>
        <article-title>Data augmentation for support vector machines</article-title>
        <source>Bayesian Anal.</source>
        <year>2011</year>
        <volume>6</volume>
        <fpage>23</fpage>
        <pub-id pub-id-type="doi">10.1214/11-BA601</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-vision-04-00025">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sáiz-Abajo</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Mevik</surname>
            <given-names>B.H.</given-names>
          </name>
          <name>
            <surname>Segtnan</surname>
            <given-names>V.H.</given-names>
          </name>
          <name>
            <surname>Næs</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Ensemble methods and data augmentation by noise addition applied to the analysis of spectroscopic data</article-title>
        <source>Anal. Chim. Acta</source>
        <year>2005</year>
        <volume>533</volume>
        <fpage>147</fpage>
        <lpage>159</lpage>
        <comment>ISSN 0003-2670</comment>
      </element-citation>
    </ref>
    <ref id="B22-vision-04-00025">
      <label>22.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Duchowski</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Jörg</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Allen</surname>
            <given-names>T.N.</given-names>
          </name>
          <name>
            <surname>Giannopoulos</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Krejtz</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Eye movement synthesis</article-title>
        <source>Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications (ETRA ’16)</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2016</year>
        <fpage>147</fpage>
        <lpage>154</lpage>
      </element-citation>
    </ref>
    <ref id="B23-vision-04-00025">
      <label>23.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Duchowski</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Jörg</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lawson</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bolte</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Świrski</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Krejtz</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Eye movement synthesis with 1/f pink noise</article-title>
        <source>Proceedings of the 8th ACM SIGGRAPH Conference on Motion in Games (MIG’15)</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2015</year>
        <fpage>47</fpage>
        <lpage>56</lpage>
      </element-citation>
    </ref>
    <ref id="B24-vision-04-00025">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Devries</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>G.W.</given-names>
          </name>
        </person-group>
        <article-title>Dataset Augmentation in Feature Space</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <pub-id pub-id-type="arxiv">1702.05538</pub-id>
      </element-citation>
    </ref>
    <ref id="B25-vision-04-00025">
      <label>25.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Um</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Pfister</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Pichler</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Endo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hirche</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Fietzek</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Kulić</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Data augmentation of wearable sensor data for parkinson’s disease monitoring using convolutional neural networks</article-title>
        <source>Proceedings of the 19th ACM International Conference on Multimodal Interaction (ICMI’17)</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2017</year>
        <fpage>216</fpage>
        <lpage>220</lpage>
      </element-citation>
    </ref>
    <ref id="B26-vision-04-00025">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Salamon</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bello</surname>
            <given-names>J.P.</given-names>
          </name>
        </person-group>
        <article-title>Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification</article-title>
        <source>IEEE Signal Process. Lett.</source>
        <year>2017</year>
        <volume>24</volume>
        <fpage>279</fpage>
        <lpage>283</lpage>
        <pub-id pub-id-type="doi">10.1109/LSP.2017.2657381</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-vision-04-00025">
      <label>27.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Research on machine learning algorithms and feature extraction for time series</article-title>
        <source>Proceedings of the 2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)</source>
        <conf-loc>Montreal, QC, Canada</conf-loc>
        <conf-date>8–13 October 2017</conf-date>
        <fpage>1</fpage>
        <lpage>5</lpage>
      </element-citation>
    </ref>
    <ref id="B28-vision-04-00025">
      <label>28.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Popescu</surname>
            <given-names>M.C.</given-names>
          </name>
          <name>
            <surname>Sasu</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>Feature extraction, feature selection and machine learning for image classification: A case study</article-title>
        <source>Proceedings of the 2014 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)</source>
        <conf-loc>Bran, Romania</conf-loc>
        <conf-date>22–24 May 2014</conf-date>
        <fpage>968</fpage>
        <lpage>973</lpage>
      </element-citation>
    </ref>
    <ref id="B29-vision-04-00025">
      <label>29.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Khalid</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Khalil</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Nasreen</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A survey of feature selection and feature extraction techniques in machine learning</article-title>
        <source>Proceedings of the 2014 Science and Information Conference</source>
        <conf-loc>London, UK</conf-loc>
        <conf-date>27–29 August 2014</conf-date>
        <fpage>372</fpage>
        <lpage>378</lpage>
      </element-citation>
    </ref>
    <ref id="B30-vision-04-00025">
      <label>30.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Oravec</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Feature extraction and classification by machine learning methods for biometric recognition of face and iris</article-title>
        <source>Proceedings of the ELMAR-2014</source>
        <conf-loc>Zadar, Croatia</conf-loc>
        <conf-date>10–12 September 2014</conf-date>
        <fpage>1</fpage>
        <lpage>4</lpage>
      </element-citation>
    </ref>
    <ref id="B31-vision-04-00025">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van der Maaten</surname>
            <given-names>L.J.P.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.E.</given-names>
          </name>
        </person-group>
        <article-title>Visualizing High-Dimensional Data Using t-SNE</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>2579</fpage>
        <lpage>2605</lpage>
      </element-citation>
    </ref>
    <ref id="B32-vision-04-00025">
      <label>32.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Rogovschi</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Kitazono</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Grozavu</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Omori</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ozawa</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>t-Distributed stochastic neighbor embedding spectral clustering</article-title>
        <source>Proceedings of the 2017 International Joint Conference on Neural Networks (IJCNN)</source>
        <conf-loc>Anchorage, AK, USA</conf-loc>
        <conf-date>14–19 May 2017</conf-date>
        <fpage>1628</fpage>
        <lpage>1632</lpage>
      </element-citation>
    </ref>
    <ref id="B33-vision-04-00025">
      <label>33.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Mounce</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Visualizing Smart Water Meter Dataset Clustering With Parametric T-distributed Stochastic Neighbour Embedding</article-title>
        <source>Proceedings of the 2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)</source>
        <conf-loc>Guilin, China</conf-loc>
        <conf-date>29–31 July 2017</conf-date>
        <fpage>1940</fpage>
        <lpage>1945</lpage>
      </element-citation>
    </ref>
    <ref id="B34-vision-04-00025">
      <label>34.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Retsinas</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Stamatopoulos</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Louloudis</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Sfikas</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Gatos</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Nonlinear Manifold Embedding on Keyword Spotting Using t-SNE</article-title>
        <source>Proceedings of the 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</source>
        <conf-loc>Kyoto, Japan</conf-loc>
        <conf-date>9–15 November 2017</conf-date>
        <fpage>487</fpage>
        <lpage>492</lpage>
      </element-citation>
    </ref>
    <ref id="B35-vision-04-00025">
      <label>35.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pancerz</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Paja</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Gomuła</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Random forest feature selection for data coming from evaluation sheets of subjects with ASDs</article-title>
        <source>Proceedings of the 2016 Federated Conference on Computer Science and Information Systems (FedCSIS)</source>
        <conf-loc>Gdansk, Poland</conf-loc>
        <conf-date>11–14 September 2016</conf-date>
        <fpage>299</fpage>
        <lpage>302</lpage>
      </element-citation>
    </ref>
    <ref id="B36-vision-04-00025">
      <label>36.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Speaker-independent speech emotion recognition based on random forest feature selection algorithm</article-title>
        <source>Proceedings of the 2017 36th Chinese Control Conference (CCC)</source>
        <conf-loc>Dalian, China</conf-loc>
        <conf-date>26–28 July 2017</conf-date>
        <fpage>10995</fpage>
        <lpage>10998</lpage>
      </element-citation>
    </ref>
    <ref id="B37-vision-04-00025">
      <label>37.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gomes</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ahsan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Denton</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Random Forest Classifier in SDN Framework for User-Based Indoor Localization</article-title>
        <source>Proceedings of the 2018 IEEE International Conference on Electro/Information Technology (EIT)</source>
        <conf-loc>Rochester, MI, USA</conf-loc>
        <conf-date>3–5 May 2018</conf-date>
        <fpage>0537</fpage>
        <lpage>0542</lpage>
      </element-citation>
    </ref>
    <ref id="B38-vision-04-00025">
      <label>38.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>The random forest classifier applied in droplet fingerprint recognition</article-title>
        <source>Proceedings of the 2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)</source>
        <conf-loc>Zhangjiajie, China</conf-loc>
        <conf-date>15–17 August 2015</conf-date>
        <fpage>722</fpage>
        <lpage>726</lpage>
      </element-citation>
    </ref>
    <ref id="B39-vision-04-00025">
      <label>39.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Okfalisa</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Gazalba</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Reza</surname>
            <given-names>N.G.I.</given-names>
          </name>
        </person-group>
        <article-title>Comparative analysis of k-nearest neighbor and modified k-nearest neighbor algorithm for data classification</article-title>
        <source>Proceedings of the 2017 2nd International conferences on Information Technology, Information Systems and Electrical Engineering (ICITISEE)</source>
        <conf-loc>Yogyakarta, Indonesia</conf-loc>
        <conf-date>1–2 November 2017</conf-date>
        <fpage>294</fpage>
        <lpage>298</lpage>
      </element-citation>
    </ref>
    <ref id="B40-vision-04-00025">
      <label>40.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Guan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>A Method of False Alarm Recognition Based on k-Nearest Neighbor</article-title>
        <source>Proceedings of the 2017 International Conference on Dependable Systems and Their Applications (DSA)</source>
        <conf-loc>Beijing, China</conf-loc>
        <conf-date>31 October–2 November 2017</conf-date>
        <fpage>8</fpage>
        <lpage>12</lpage>
      </element-citation>
    </ref>
    <ref id="B41-vision-04-00025">
      <label>41.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Cai</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>A K-nearest neighbor locally search regression algorithm for short-term traffic flow forecasting</article-title>
        <source>Proceedings of the 2017 9th International Conference on Modelling, Identification and Control (ICMIC)</source>
        <conf-loc>Kunming, China</conf-loc>
        <conf-date>10–12 July 2017</conf-date>
        <fpage>624</fpage>
        <lpage>629</lpage>
      </element-citation>
    </ref>
    <ref id="B42-vision-04-00025">
      <label>42.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>A k-nearest neighbor text classification algorithm based on fuzzy integral</article-title>
        <source>Proceedings of the 2010 Sixth International Conference on Natural Computation</source>
        <conf-loc>Yantai, China</conf-loc>
        <conf-date>10–12 August 2010</conf-date>
        <fpage>2228</fpage>
        <lpage>2231</lpage>
      </element-citation>
    </ref>
    <ref id="B43-vision-04-00025">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waske</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Benediktsson</surname>
            <given-names>J.A.</given-names>
          </name>
        </person-group>
        <article-title>Fusion of Support Vector Machines for Classification of Multisensor Data</article-title>
        <source>IEEE Trans. Geosci. Remote Sens.</source>
        <year>2007</year>
        <volume>45</volume>
        <fpage>3858</fpage>
        <lpage>3866</lpage>
        <pub-id pub-id-type="doi">10.1109/TGRS.2007.898446</pub-id>
      </element-citation>
    </ref>
    <ref id="B44-vision-04-00025">
      <label>44.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Sheng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Mengjun</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Lanyong</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Research on information fusion of infrared and radar sensor based on SVM</article-title>
        <source>Proceedings of the 2012 International Conference on Measurement, Information and Control</source>
        <conf-loc>Harbin, China</conf-loc>
        <conf-date>18–20 May 2012</conf-date>
        <fpage>98</fpage>
        <lpage>101</lpage>
      </element-citation>
    </ref>
    <ref id="B45-vision-04-00025">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nakano</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Nukala</surname>
            <given-names>B.T.</given-names>
          </name>
          <name>
            <surname>Tsay</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zupancic</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Lie</surname>
            <given-names>D.Y.C.</given-names>
          </name>
          <name>
            <surname>Lopez</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>T.Q.</given-names>
          </name>
        </person-group>
        <article-title>Gaits Classification of Normal vs Patients by Wireless Gait Sensor and Support Vector Machine (SVM) Classifier</article-title>
        <source>Int. J. Softw. Innov. (IJSI)</source>
        <year>2017</year>
        <volume>5</volume>
        <fpage>17</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.4018/IJSI.2017010102</pub-id>
      </element-citation>
    </ref>
    <ref id="B46-vision-04-00025">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jeong</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Truong</surname>
            <given-names>P.H.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Classification of Three Types of Walking Activities Regarding</article-title>
        <source>IEEE Sens. J.</source>
        <year>2017</year>
        <volume>17</volume>
        <fpage>2638</fpage>
        <lpage>2639</lpage>
        <pub-id pub-id-type="doi">10.1109/JSEN.2017.2682322</pub-id>
      </element-citation>
    </ref>
    <ref id="B47-vision-04-00025">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Vincent</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Representation Learning: A Review and New Perspectives</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2013</year>
        <volume>35</volume>
        <fpage>1798</fpage>
        <lpage>1828</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2013.50</pub-id>
        <pub-id pub-id-type="pmid">23787338</pub-id>
      </element-citation>
    </ref>
    <ref id="B48-vision-04-00025">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koldowski</surname>
            <given-names>U.M.M.</given-names>
          </name>
        </person-group>
        <article-title>Spiking neural network vs multilayer perceptron: Who is the winner in the racing car computer game</article-title>
        <source>Soft Comput.</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>3465</fpage>
        <lpage>3478</lpage>
      </element-citation>
    </ref>
    <ref id="B49-vision-04-00025">
      <label>49.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bieniasz</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Rawski</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Skowron</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Trzepiński</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of multilayer perceptron algorithms for an analysis of network flow data</article-title>
        <source>Photonics Applications in Astronomy, Communications, Industry, and High-Energy Physics Experiments 2016</source>
        <publisher-name>International Society for Optics and Photonics</publisher-name>
        <publisher-loc>Washington, DC, USA</publisher-loc>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="B50-vision-04-00025">
      <label>50.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lang</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Monotonic Multi-layer Perceptron Networks as Universal Approximators</article-title>
        <source>Artificial Neural Networks: Formal Models and Their Applications–ICANN 2005</source>
        <series>Lecture Notes in Computer Science</series>
        <person-group person-group-type="editor">
          <name>
            <surname>Duch</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Kacprzyk</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Oja</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Zadrożny</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <year>2005</year>
        <volume>Volume 3697</volume>
      </element-citation>
    </ref>
    <ref id="B51-vision-04-00025">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Astorino</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Fuduli</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>The Proximal Trajectory Algorithm in SVM Cross Validation</article-title>
        <source>IEEE Trans. Neural Networks Learn. Syst.</source>
        <year>2016</year>
        <volume>27</volume>
        <fpage>966</fpage>
        <lpage>977</lpage>
        <pub-id pub-id-type="doi">10.1109/TNNLS.2015.2430935</pub-id>
      </element-citation>
    </ref>
    <ref id="B52-vision-04-00025">
      <label>52.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Mao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>An Improved Grid Search Algorithm of SVR Parameters Optimization</article-title>
        <source>Proceedings of the 2012 IEEE 14th International Conference on Communication Technology</source>
        <conf-loc>Chengdu, China</conf-loc>
        <conf-date>9–11 November 2012</conf-date>
        <fpage>1022</fpage>
        <lpage>1026</lpage>
      </element-citation>
    </ref>
    <ref id="B53-vision-04-00025">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Eye detection using discriminatory Haar features and a new efficient SVM</article-title>
        <source>Image Vis. Comput.</source>
        <year>2015</year>
        <volume>33</volume>
        <fpage>68</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1016/j.imavis.2014.10.007</pub-id>
      </element-citation>
    </ref>
    <ref id="B54-vision-04-00025">
      <label>54.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Shang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Research on Industrial Control Anomaly Detection Based on FCM and SVM</article-title>
        <source>Proceedings of the 12th IEEE International Conference on Big Data Science and Engineering, TrustCom/BigDataSE</source>
        <conf-loc>New York, NY, USA</conf-loc>
        <conf-date>1–3 August 2018</conf-date>
        <fpage>218</fpage>
        <lpage>222</lpage>
      </element-citation>
    </ref>
    <ref id="B55-vision-04-00025">
      <label>55.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>An intelligent anomaly analysis for intrusion detection based on SVM</article-title>
        <source>Proceedings of the 2012 International Conference on Computer Science and Information Processing (CSIP)</source>
        <conf-loc>Xi’an, China</conf-loc>
        <conf-date>24–26 August 2012</conf-date>
        <fpage>739</fpage>
        <lpage>742</lpage>
      </element-citation>
    </ref>
    <ref id="B56-vision-04-00025">
      <label>56.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Guang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Min</surname>
            <given-names>N.I.E.</given-names>
          </name>
        </person-group>
        <article-title>Anomaly Intrusion Detection Based on Wavelet Kernel LS-SVM</article-title>
        <source>Proceedings of the 2013 3rd International Conference on Computer Science and Network Technology</source>
        <conf-loc>Dalian, China</conf-loc>
        <conf-date>12–13 October 2013</conf-date>
        <fpage>434</fpage>
        <lpage>437</lpage>
      </element-citation>
    </ref>
    <ref id="B57-vision-04-00025">
      <label>57.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Eswaran</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Logeswaran</surname>
            <given-names>R.A.</given-names>
          </name>
        </person-group>
        <article-title>Comparison of ARIMA, Neural Network and Linear Regression Models for the Prediction of Infant Mortality Rate</article-title>
        <source>Proceedings of the 2010 Fourth Asia International Conference on Mathematical/Analytical Modelling and Computer Simulation</source>
        <conf-loc>Bornea, Malaysia</conf-loc>
        <conf-date>26–28 May 2010</conf-date>
        <publisher-name>IEEE</publisher-name>
        <publisher-loc>Piscataway, NJ, USA</publisher-loc>
        <year>2010</year>
        <fpage>34</fpage>
        <lpage>39</lpage>
      </element-citation>
    </ref>
    <ref id="B58-vision-04-00025">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Regularization Paths for Generalized Linear Models via Coordinate Descent</article-title>
        <source>J. Stat. Softw.</source>
        <year>2010</year>
        <volume>33</volume>
        <fpage>1</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.18637/jss.v033.i01</pub-id>
        <pub-id pub-id-type="pmid">20808728</pub-id>
      </element-citation>
    </ref>
    <ref id="B59-vision-04-00025">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>L.K.</given-names>
          </name>
        </person-group>
        <article-title>Local Minimax Learning of Functions with Best Finite Sample Estimation Error Bounds: Applications to Ridge and Lasso Regression, Boosting, Tree Learning, Kernel Machines, and Inverse Problems</article-title>
        <source>IEEE Trans. Inf. Theory</source>
        <year>2009</year>
        <volume>55</volume>
        <fpage>5700</fpage>
        <lpage>5727</lpage>
        <pub-id pub-id-type="doi">10.1109/TIT.2009.2027479</pub-id>
      </element-citation>
    </ref>
    <ref id="B60-vision-04-00025">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kirpich</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ainsworth</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Wedow</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Newman</surname>
            <given-names>J.R.B.</given-names>
          </name>
          <name>
            <surname>Michailidis</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>McIntyre</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>M Variable selection in omics data: A practical evaluation of small sample sizes</article-title>
        <source>PLoS ONE</source>
        <year>2018</year>
        <volume>13</volume>
        <elocation-id>e0197910</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0197910</pub-id>
        <?supplied-pmid 29927942?>
        <pub-id pub-id-type="pmid">29927942</pub-id>
      </element-citation>
    </ref>
    <ref id="B61-vision-04-00025">
      <label>61.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bayindir</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Gok</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kabalci</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Kaplan</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>An Intelligent Power Factor Correction Approach Based on Linear Regression and Ridge Regression Methods</article-title>
        <source>Proceedings of the 2011 10th International Conference on Machine Learning and Applications and Workshops</source>
        <conf-loc>Honolulu, HI, USA</conf-loc>
        <conf-date>18–21 December 2011</conf-date>
        <fpage>313</fpage>
        <lpage>315</lpage>
      </element-citation>
    </ref>
    <ref id="B62-vision-04-00025">
      <label>62.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Verma</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tiwana</surname>
            <given-names>A.P.S.</given-names>
          </name>
          <name>
            <surname>Reddy</surname>
            <given-names>C.C.</given-names>
          </name>
        </person-group>
        <article-title>Data Analysis to Generate Models Based on Neural Network and Regression for Solar Power Generation Forecasting</article-title>
        <source>Proceedings of the 2016 7th International Conference on Intelligent Systems, Modelling and Simulation (ISMS)</source>
        <conf-loc>Bangkok, Thailand</conf-loc>
        <conf-date>25–27 January 2016</conf-date>
        <fpage>97</fpage>
        <lpage>100</lpage>
      </element-citation>
    </ref>
    <ref id="B63-vision-04-00025">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vabalas</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gowen</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Poliakoff</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Casson</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Machine learning algorithm validation with a limited sample size</article-title>
        <source>PLoS ONE</source>
        <year>2019</year>
        <volume>14</volume>
        <elocation-id>e0224365</elocation-id>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0224365</pub-id>
        <?supplied-pmid 31697686?>
        <pub-id pub-id-type="pmid">31697686</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="vision-04-00025-f001" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <p>Gaze data processing and analysis pipeline followed in this study.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g001"/>
  </fig>
  <fig id="vision-04-00025-f002" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <p>(<bold>a</bold>) Desktop setup and (<bold>b</bold>) tablet setup for gaze data collection using eye tracker. (<bold>c</bold>) Desktop setup and (<bold>d</bold>) variation of the tablet orientation for the experiments. (<bold>e</bold>) List of the experiments performed in this study. (<bold>f</bold>) Layout of the user interface (UI) for gaze data collection. (<bold>g</bold>) The user-eye tracker setup. (<bold>h</bold>) Process flow for data collection and analysis. (<bold>i</bold>) Schematic diagram showing the positioning of users during the experiments.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g002a"/>
    <graphic xlink:href="vision-04-00025-g002b"/>
  </fig>
  <fig id="vision-04-00025-f003" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <p>Raw gaze data (blue points) overlaid on ground truth data (black lines) for (<bold>a</bold>) neutral head pose (<bold>b</bold>) head pose with roll of 20 degrees (<bold>c</bold>), head pitch of 20 degrees, and (<bold>d</bold>) head yaw 20 degrees. It can be seen that it is very hard to visually distinguish between data from different experiments done under different conditions, unless special pre-processing steps and learning algorithms are employed to identify the error source affecting the eye tracker.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g003"/>
  </fig>
  <fig id="vision-04-00025-f004" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <p>Time series of (<bold>a</bold>) gaze angles, (<bold>b</bold>) gaze yaw, and (<bold>c</bold>) gaze pitch angles (black lines with ground truth in blue lines) for one person during one experimental session for a neutral pose captured on the desktop setup.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g004"/>
  </fig>
  <fig id="vision-04-00025-f005" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <p>Outlier detection and removal with (<bold>a</bold>) median filtering, (<bold>b</bold>) MAD, and (<bold>c</bold>) IQR methods. Median filtering (<bold>a</bold>) is seen to remove nearly all outliers.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g005"/>
  </fig>
  <fig id="vision-04-00025-f006" orientation="portrait" position="float">
    <label>Figure 6</label>
    <caption>
      <p>Kernel density plots over histograms of gaze error for user distances of (<bold>a</bold>) 50 cm, (<bold>b</bold>) 60 cm, and (<bold>c</bold>) 70 cm for desktop data.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g006"/>
  </fig>
  <fig id="vision-04-00025-f007" orientation="portrait" position="float">
    <label>Figure 7</label>
    <caption>
      <p>KDE plots over histograms of gaze error for user distances (<bold>a</bold>) 50 cm, (<bold>b</bold>) 60 cm, and (<bold>c</bold>) 70 cm for tablet data.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g007"/>
  </fig>
  <fig id="vision-04-00025-f008" orientation="portrait" position="float">
    <label>Figure 8</label>
    <caption>
      <p>Gaze error spatial distribution as a function of the visual angles (x axis= yaw angle, y axis= pitch angle) over the display area. <xref ref-type="fig" rid="vision-04-00025-f008">Figure 8</xref> (<bold>a</bold>–<bold>d</bold>) show the spatial error variation due to the head pose (desktop). <xref ref-type="fig" rid="vision-04-00025-f008">Figure 8</xref> (<bold>e</bold>–<bold>h</bold>) show the error distributions due to the tablet poses.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g008"/>
  </fig>
  <fig id="vision-04-00025-f009" orientation="portrait" position="float">
    <label>Figure 9</label>
    <caption>
      <p>Correlation between data from (<bold>a</bold>) desktop and (<bold>b</bold>) tablet experiments.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g009"/>
  </fig>
  <fig id="vision-04-00025-f010" orientation="portrait" position="float">
    <label>Figure 10</label>
    <caption>
      <p>Shows a sample from a dataset after applying each augmentation strategy. The blue lines represent samples from the original data, while the red lines are from the augmented dataset after applying each strategy.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g010"/>
  </fig>
  <fig id="vision-04-00025-f011" orientation="portrait" position="float">
    <label>Figure 11</label>
    <caption>
      <p>(<bold>a</bold>). t-SNE plots for (i) user distance (ii) head pose (iii) merged user-distance and head pose for desktop datasets. Legends 1, 2, 3, and 4 are user distance classes UD50-UD80, 5, 6, 7, and 8 are head pose classes the neutral, roll, pitch, and yaw. (<bold>b</bold>). t-SNE plots for (i) the user distance, (ii) tablet pose, and (iii) merged user-distance and tablet pose for tablet datasets. Legends 1, 2, 3, and 4 are user-tablet distance classes UD50-UD80, 5, 6, 7, and 8 are tablet pose classes neutral, roll, pitch, and yaw.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g011"/>
  </fig>
  <fig id="vision-04-00025-f012" orientation="portrait" position="float">
    <label>Figure 12</label>
    <caption>
      <p>(<bold>a</bold>): Relative importance of features for different desktop datasets. (<bold>b</bold>): Relative importance of features for different tablet datasets.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g012"/>
  </fig>
  <fig id="vision-04-00025-f013" orientation="portrait" position="float">
    <label>Figure 13</label>
    <caption>
      <p>Confusion matrix using K-neared neighbor on the mixed head pose and user distance dataset for the desktop.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g013"/>
  </fig>
  <fig id="vision-04-00025-f014" orientation="portrait" position="float">
    <label>Figure 14</label>
    <caption>
      <p>Effect of varying the model hyperparameters for KNN (<bold>a</bold>) and (<bold>b</bold>,<bold>c</bold>) Multilayer perceptron. (<bold>d</bold>) Confusion matrix using K-nearest neighbour on the mixed head pose and user distance dataset for the tablet.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g014"/>
  </fig>
  <fig id="vision-04-00025-f015" orientation="portrait" position="float">
    <label>Figure 15</label>
    <caption>
      <p>Actual (red) and predicted gaze errors for the head pose neutral, roll, pitch, and yaw (<bold>a</bold>–<bold>d</bold>) datasets.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g015"/>
  </fig>
  <fig id="vision-04-00025-f016" orientation="portrait" position="float">
    <label>Figure 16</label>
    <caption>
      <p>Actual gaze errors (red) and predicted gaze errors for the tablet pose neutral, roll, pitch, and yaw (<bold>a</bold>–<bold>d</bold>) datasets.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g016"/>
  </fig>
  <fig id="vision-04-00025-f017" orientation="portrait" position="float">
    <label>Figure 17</label>
    <caption>
      <p>(<bold>a</bold>). The MLGaze repository hosted on GitHub, which contains Python coding resources for implementation of the methods described above. (<bold>b</bold>). Contents of the MLGaze repository.</p>
    </caption>
    <graphic xlink:href="vision-04-00025-g017a"/>
    <graphic xlink:href="vision-04-00025-g017b"/>
  </fig>
  <table-wrap id="vision-04-00025-t001" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Gaze error statistics from the desktop experiments.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Desktop</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD50</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD60</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD70</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD80</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Roll 20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Yaw 20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pitch 20</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Mean</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.37</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.04</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.21</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.02</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.7</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8.51</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.15</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">MAD</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.49</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.77</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.82</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.66</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.63</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.0</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.90</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">IQR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.13</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.77</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.76</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.79</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.21</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.49</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.59</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95% interval</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.15–3.59</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.90–2.18</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.15–1.26</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.16–1.24</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.30–4.09</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.60–9.43</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.83–3.47</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-04-00025-t002" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>Gaze error statistics from the tablet experiments.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Tablet</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD50</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD60</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD70</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">UD80</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Roll 20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Yaw 20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Pitch 20</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Mean</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.68</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.46</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.59</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.55</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">7.74</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.25</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.45</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">MAD</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.38</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.42</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.29</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.24</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.77</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.60</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.46</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">IQR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.39</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.54</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.33</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.22</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.75</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.53</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.23</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">95% interval</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.65–2.71</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.43–2.48</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.57–0.61</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.53–1.57</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">7.69–7.80</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.22–4.29</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">2.41–2.49</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-04-00025-t003" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t003_Table 3</object-id>
    <label>Table 3</label>
    <caption>
      <p>Training and test dataset details.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Samples for Train, Test</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Original Feature Set<break/>(20 Features)</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Augmentation Strategies</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Samples Per Person</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Samples and Classes</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td rowspan="6" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">Total participants: 20<break/>12–16 participants for training, <break/>8-5 for test <break/>Data labelled and randomly shuffled <break/></td>
          <td rowspan="6" align="left" valign="middle" style="border-bottom:solid thin" colspan="1"><list list-type="simple"><list-item><label>(1)</label><p>Gaze error values at 15 AOIs, Mean, SD, IQR, 0.95 interval bounds of gaze error</p></list-item><list-item><label>(2)</label><p>Yaw error values at 15 AOIs, Mean, SD, IQR, 0.95 interval bounds of yaw error</p></list-item><list-item><label>(3)</label><p>Pitch error values at 15 AOIs, Mean, SD, IQR, 0.95 interval limits of pitch error</p></list-item></list>Reduced feature set (5 features)<break/>Mean, SD, IQR, 0.95 interval bounds for each sample.</td>
          <td rowspan="6" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">
            <list list-type="simple">
              <list-item>
                <label>(1)</label>
                <p>Gaussian noise</p>
              </list-item>
              <list-item>
                <label>(2)</label>
                <p>Jitter or pink noise</p>
              </list-item>
              <list-item>
                <label>(3)</label>
                <p>Horizontal data flipping</p>
              </list-item>
              <list-item>
                <label>(4)</label>
                <p>Vertical data flipping</p>
              </list-item>
              <list-item>
                <label>(5)</label>
                <p>Magnitude warping</p>
              </list-item>
              <list-item>
                <label>(6)</label>
                <p>Time warp</p>
              </list-item>
              <list-item>
                <label>(7)</label>
                <p>Interpolation and combinations</p>
              </list-item>
            </list>
          </td>
          <td rowspan="6" align="left" valign="middle" style="border-bottom:solid thin" colspan="1">10 × gaze error<break/>10 × yaw error<break/>10 × pitch error<break/>Merged dataset<break/>30 samples </td>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <list list-type="simple">
              <list-item>
                <label>(1)</label>
                <p>Desktop user dist, 2400 samples, 4 classes</p>
              </list-item>
            </list>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <list list-type="simple">
              <list-item>
                <label>(2)</label>
                <p>Head pose, 2400 samples, 4 classes</p>
              </list-item>
            </list>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <list list-type="simple">
              <list-item>
                <label>(3)</label>
                <p>Desktop mixed: 4200 samples, 7 classes</p>
              </list-item>
            </list>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <list list-type="simple">
              <list-item>
                <label>(5)</label>
                <p>Tablet user dist, 2400 samples, 4 classes</p>
              </list-item>
            </list>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">
            <list list-type="simple">
              <list-item>
                <label>(6)</label>
                <p>Tablet pose, 2400 samples, 4 classes</p>
              </list-item>
            </list>
          </td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <list list-type="simple">
              <list-item>
                <label>(7)</label>
                <p>Tablet mixed: 4200 samples, 7 classes</p>
              </list-item>
            </list>
          </td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-04-00025-t004" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t004_Table 4</object-id>
    <label>Table 4</label>
    <caption>
      <p>Classifier accuracies for different datasets and machine learning models.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">For User Distance</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">For Head Pose</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">On Mixed Datasets</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">k-NN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">k-NN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">k-NN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Only gaze angle features</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">89%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">69%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">86%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">91%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">83%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">90%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">87%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">74%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">83%</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Only gaze yaw features</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">83%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">64%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">88%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">78%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">86%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">81%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">66%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">80%</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Only gaze pitch features</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">96%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">89%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">73%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">95%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">96%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">83%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">92%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">91%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">79%</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Merged (gaze, yaw, pitch) feature dataset</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">67%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">81%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">89%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">80%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">70%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">77%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-04-00025-t005" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t005_Table 5</object-id>
    <label>Table 5</label>
    <caption>
      <p>True and false detection rates from classifiers of the desktop (user distance, head pose, and mixed datasets).</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">For User Distance Dataset</th>
          <th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">For Head Pose Dataset</th>
          <th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">On Mixed Dataset</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KNN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KNN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KNN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">TPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.96</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.95</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.85</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.97</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.91</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.95</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.83</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.93</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.83</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">FPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.04</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.02</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.02</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.02</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">TNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.98</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.98</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.95</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.99</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.97</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.98</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.97</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.98</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.97</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">FNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.04</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.04</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.14</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.03</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.08</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.04</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.06</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.16</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.98</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.93</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.97</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.97</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.95</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Precision</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.93</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-04-00025-t006" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t006_Table 6</object-id>
    <label>Table 6</label>
    <caption>
      <p>Classifier performance for different datasets.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">For User-Tablet Distance</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">For Tablet Pose</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">On Mixed Datasets</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Dataset</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">k-NN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">k-NN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">k-NN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <bold>Only gaze angle features</bold>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">81%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">82%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">81%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">86%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">88%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">75%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">80%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">80%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">78%</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <bold>Only gaze yaw features</bold>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">86%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">78%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">93%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">92%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">85%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">70%</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <bold>Only gaze pitch features</bold>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">90%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">81%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">71%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">94%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">91%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">89%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">79%</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>Merged (gaze, yaw, pitch) feature dataset</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">84%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">85%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">82%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">83%</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">75%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-04-00025-t007" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t007_Table 7</object-id>
    <label>Table 7</label>
    <caption>
      <p>True and false detection results from classifiers of the tablet (user distance, tablet pose, and mixed datasets). * Prec refers to the precision of classification.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">User-Tablet Distance Dataset</th>
          <th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Tablet Pose Dataset</th>
          <th colspan="4" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Mixed Dataset</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">UD</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KNN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Pose</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KNN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Mixed</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KNN</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">SVM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">MLP</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">TPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.87</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.85</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.71</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.90</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.83</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.79</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.80</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.83</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.70</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">FPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.04</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.04</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.09</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.03</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.27</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.06</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FPR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.03</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.02</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.04</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">TNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.95</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.95</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.90</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.96</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.97</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.93</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">TNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.96</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.97</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.95</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">FNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.12</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.14</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.28</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.09</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.20</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">FNR</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.19</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.29</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prec.*</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.88</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.72</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prec.*</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.91</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.86</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.79</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Prec.*</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.82</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.88</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.72</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-04-00025-t008" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t008_Table 8</object-id>
    <label>Table 8</label>
    <caption>
      <p>Prediction errors (RMSE) for models of the desktop (left, red columns) and tablet (right, blue columns).</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;background:#F2DBDB" rowspan="1" colspan="1">Neutral</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;background:#F2DBDB" rowspan="1" colspan="1">R20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;background:#F2DBDB" rowspan="1" colspan="1">P20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;background:#F2DBDB" rowspan="1" colspan="1">Y20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">Neutral</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">R20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">P20</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">Y20</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Linear</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.24</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.36</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.45</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.71</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.73</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">1.79</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.58</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.72</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Polynomial</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">7.48</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.82</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">7.88</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.88</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">2.61</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">1.99</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">5.24</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.81</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Ridge</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.24</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.36</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.44</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.70</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.73</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">1.79</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.58</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.72</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Lasso</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.24</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.15</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.31</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.70</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.73</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">1.73</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.58</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.71</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">ElasticNet</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.25</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.07</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">1.29</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">2.69</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.73</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">1.71</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.50</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">0.70</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Neural network</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#F2DBDB" rowspan="1" colspan="1">4.01</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#F2DBDB" rowspan="1" colspan="1">1.21</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#F2DBDB" rowspan="1" colspan="1">2.09</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#F2DBDB" rowspan="1" colspan="1">2.75</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">1.75</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">1.78</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">2.39</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">1.18</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="vision-04-00025-t009" orientation="portrait" position="float">
    <object-id pub-id-type="pii">vision-04-00025-t009_Table 9</object-id>
    <label>Table 9</label>
    <caption>
      <p>Coefficients and intercept of the best model for the head pose and platform pose.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Platform/Condition</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">ElasticNet Coefficients B<sub>1</sub>, B<sub>2</sub> and B<sub>3</sub></th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Intercept B<sub>0</sub></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">Desktop: Head pose neutral</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">[0.09336917, 0.19406989, −0.00279198]</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">−1.99912371 × 10<sup>−16</sup></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">Desktop: Head Roll 20</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">[0.0, 0.65189252, 0.07303053]</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">3.23942926 × 10<sup>−16</sup></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">Desktop: Head Pitch 20</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">[0.22606558, 0.11028886, 0.05731872]</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">−9.55229176 × 10<sup>−17</sup></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">Desktop: Head Yaw 20</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">[0.0, 0.51352565, 0.08149052]</td>
          <td align="center" valign="middle" style="background:#F2DBDB" rowspan="1" colspan="1">8.94037155 × 10<sup>−17</sup></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">Tablet: Platform pose neutral</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">[0.07333954, 0.0, −0.17956056]</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">1.76076146 × 10<sup>−16</sup></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">Tablet: Platform Roll 20</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">[0.0, −0.31460996, −0.23620848]</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">−2.87637414 × 10<sup>−16</sup></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">Tablet: Platform Pitch 20</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">[0.0, −0.05682588, −0.20804325]</td>
          <td align="center" valign="middle" style="background:#DBE5F1" rowspan="1" colspan="1">2.34007877 × 10<sup>−16</sup></td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">Tablet: Platform Yaw20</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">[0.0, −0.01596391, −0.06346607]</td>
          <td align="center" valign="middle" style="border-bottom:solid thin;background:#DBE5F1" rowspan="1" colspan="1">−2.41027682 × 10<sup>−17</sup></td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
