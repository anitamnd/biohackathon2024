<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6710256</article-id>
    <article-id pub-id-type="publisher-id">48786</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-019-48786-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deeper Profiles and Cascaded Recurrent and Convolutional Neural Networks for state-of-the-art Protein Secondary Structure Prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3016-3655</contrib-id>
        <name>
          <surname>Torrisi</surname>
          <given-names>Mirko</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kaleel</surname>
          <given-names>Manaz</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Pollastri</surname>
          <given-names>Gianluca</given-names>
        </name>
        <address>
          <email>gianluca.pollastri@ucd.ie</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0768 2743</institution-id><institution-id institution-id-type="GRID">grid.7886.1</institution-id><institution>School of Computer Science, </institution><institution>University College Dublin, </institution></institution-wrap>Belfield, Dublin 4 Ireland </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>26</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>26</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>12374</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>3</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>8</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Protein Secondary Structure prediction has been a central topic of research in Bioinformatics for decades. In spite of this, even the most sophisticated ab initio SS predictors are not able to reach the theoretical limit of three-state prediction accuracy (88–90%), while only a few predict more than the 3 traditional Helix, Strand and Coil classes. In this study we present tests on different models trained both on single sequence and evolutionary profile-based inputs and develop a new state-of-the-art system with Porter 5. Porter 5 is composed of ensembles of cascaded Bidirectional Recurrent Neural Networks and Convolutional Neural Networks, incorporates new input encoding techniques and is trained on a large set of protein structures. Porter 5 achieves 84% accuracy (81% SOV) when tested on 3 classes and 73% accuracy (70% SOV) on 8 classes on a large independent set. In our tests Porter 5 is 2% more accurate than its previous version and outperforms or matches the most recent predictors of secondary structure we tested. When Porter 5 is retrained on SCOPe based sets that eliminate homology between training/testing samples we obtain similar results. Porter is available as a web server and standalone program at <ext-link ext-link-type="uri" xlink:href="http://distilldeep.ucd.ie/porter/">http://distilldeep.ucd.ie/porter/</ext-link> alongside all the datasets and alignments.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Protein sequence analyses</kwd>
      <kwd>Protein structure predictions</kwd>
      <kwd>Proteome informatics</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100002081</institution-id>
            <institution>Irish Research Council (An Chomhairle um Thaighde in &amp;amp;#x00C9;irinn)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>GOIPG/2015/3717</award-id>
        <award-id>GOIPG/2014/603</award-id>
        <principal-award-recipient>
          <name>
            <surname>Torrisi</surname>
            <given-names>Mirko</given-names>
          </name>
          <name>
            <surname>Kaleel</surname>
            <given-names>Manaz</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">From DNA repair to enzyme catalysis, proteins are the chief actors within the cell. While we discovered more than 325 million protein sequences<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, we still lack a feasible method to experimentally fully characterize them at a large scale. Nonetheless, nearly 150,000 protein structures are now freely available, growing by roughly 10,000 per year, making proteins a central research topic in Bioinformatics<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. One of the most enduring open problems in Bioinformatics is Secondary Structure (SS) prediction<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>. It was inaugurated by Pauling and Corey in 1951, when they predicted the existence of the two most common SS conformations – <italic>α</italic>-helix and <italic>β</italic>-sheet – before the first protein structure was fully determined<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. What followed was the first generation of SS predictors, all based on exploiting statistical propensities of single AA towards specific SS conformations<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR9">9</xref></sup>. The second generation of SS predictors was developed relying on information contained within segments of multiple adjacent amino acids (AA)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, physicochemical properties<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, and algorithmic developments such as Neural Networks (NN)<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref></sup>, graph theory<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, and nearest-neighbor methods<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Finally, the encoding of richer input including evolutionary information characterises the third generation of SS predictors, where profile-based inputs extracted from alignments of multiple homologous sequences led to accuracies exceeding 70% for the first time<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Notably, each generation of SS predictors has taken great advantage of the constantly growing availability of computational resources and data to exploit deeper information through more advanced methods<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>. Moreover, since the 90 s, NN have become the de facto standard technique to predict SS<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR26">26</xref></sup>, and maintain a central role at the two most important academic assessments of protein structure predictors: CASP and CAMEO<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>.</p>
    <p id="Par3">Six decades of efforts towards more accurate protein SS predictions have passed<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. Nonetheless, the theoretical limits of prediction – set at 88–90% accuracy per AA, mainly due to the intrinsic dynamic nature of protein structure<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> and ambiguity of SS class assignment – have not been reached yet, and the importance of accurate SS prediction as an intermediate step towards more complex protein features, such as tertiary, or quaternary structure, has not diminished<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. We start this study assessing the potential and limits of SS prediction without evolutionary information, reaching roughly 70% accuracy, similar to that of early profile-based methods<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. We then assess different NN architectures, focusing on classic window-based Feed Forward NN (FFNN) and cascaded Bidirectional Recurrent Neural Networks and Convolutional Neural Networks (CBRCNN) to gauge the relative strengths of each architecture. We investigate different pipelines to harness evolutionary information extracted with two of the most common tools – PSI-BLAST<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and HHblits<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> – and benchmark different techniques to encode evolutionary information in the form of profiles. We develop a novel input encoding which is able to represent both evolutionary information and the identity of the query sequence. Finally, we implement the best methods into Porter 5, a state-of-the-art three- and eight-state SS predictor. Porter 5 is available as a light standalone program and a simple web server, alongside training and test datasets used for this study.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <p id="Par4">We trained profile-less and profile-based models with profiles encoded in a number of different ways. We identified the most successful predictors in 5-fold cross validation experiments on the training set. We ensembled some of these models in our final predictor Porter 5, which we tested on multiple independent sets alongside a number of the most recent SS predictors.</p>
    <sec id="Sec3">
      <title>Alignment-free predictions</title>
      <p id="Par5">Evolutionary information, in the form of aligned sequences, was first used to significantly improve the prediction of SS in the early 90 s<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. The training sets used at the time contained only a few hundred proteins. For this study we were able to build a training dataset of almost 16,000 proteins (4 million AA). Given this massive growth in sample size, we tried to gauge whether it is now possible to produce reliable predictions without the use of alignments.</p>
      <p id="Par6">In 1993, an ensemble of 2 cascaded FFNN was adopted to reach a Q3 accuracy above 70% (see Methods: Measuring performances)<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. We assessed window-based FFNN adopting an incremental training approach (described in Methods:FFNN) that allowed us to reach 69.7% Q3 accuracy with no profiles and just one hidden layer. We slightly improved the same FFNN up to 69.8% and 69.9% Q3 accuracy adding 1 or 2 hidden layers, respectively, our best results with a single FFNN without evolutionary information. It should be noted that a baseline predictor that classifies each residue into the most frequent secondary structure for its type results in a Q3 of 45.2%, 5.4% better than classifying all AA as the most common class (coils), see Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Performances of single models of different NN architectures on the validation set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>profile-less</th><th>plain profiles</th><th>deep profiles</th></tr></thead><tbody><tr><td>Baseline</td><td>45.19%</td><td>60.9%</td><td>61.33%</td></tr><tr><td>FFNN</td><td>69.85%</td><td>80.02%</td><td>80.45%</td></tr><tr><td>CBRCNN</td><td>71.33%</td><td>82.32%</td><td>83.1%</td></tr></tbody></table></table-wrap></p>
      <p id="Par7">To summarize, adopting a considerably larger training set but without evolutionary information, we reached comparable results to the 1993 state-of-the-art<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Using CBRCNN (see Methods:CBRCNN) instead of FFNN we observed a further increase in accuracy, up to 71.3% on the same sets. While there might be advantages to alignment-less predictions, as they require considerably less computational time with respect to profile-based solutions (fractions of seconds per protein instead of minutes), their accuracy, at ~71%, is far from the state-of-the-art predictors including evolutionary information, estimated at ~82–83%<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>.</p>
    </sec>
    <sec id="Sec4">
      <title>Profile-based predictions</title>
      <p id="Par8">In a second phase of this study we tested different ways to encode alignments in order to maximise input information to a predictor.</p>
      <p id="Par9">We generated alignments with both PSI-BLAST<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and HHblits<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. We did not limit the number of hits of PSI-BLAST or HHblits, resulting in alignments with an average of ~14,000 and ~1,300 proteins, respectively (see also Methods:Evolutionary Information). We encoded evolutionary information into 22 inputs using plain profiles, as described in Methods:Input Encoding.</p>
      <p id="Par10">We trained a three hidden layer FFNN constructed similarly to the best FFNN based on single-sequence inputs. We obtained 79.9% accuracy, a 10% improvement over the alignment-less case. Fine-tuning the FFNN hyperparameters did not substantially change the results, with only slight improvements for networks with larger hidden layers, confirming that there is more information embedded in a profile than in a single sequence.</p>
      <p id="Par11">It should also be noted that, when using profiles, we obtain a Q3 accuracy of 60.9% completely disregarding the context surrounding an AA (training a FFNN with window of size 1), see Table <xref rid="Tab1" ref-type="table">1</xref>.</p>
      <p id="Par12">CBRCNN performed significantly better on the same data and encoding, up to 82.3% for a single model, matching the performance of a fully tuned ensemble trained on a smaller set<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p>
      <sec id="Sec5">
        <title>Deeper profiles</title>
        <p id="Par13">We found beneficial to employ, at encoding time, a weighting scheme that aims to maximize the entropy of the profiles (see Methods:Input Encoding), i.e. that weighs more those sequences that are more informative (more different from the plain profile). This step improved the Q3 accuracy of our best FFNN and CBRCNN by 0.4% and 0.3%, respectively, while maintaining an encoding composed of 22 input numbers per AA.</p>
        <p id="Par14">We trained both FFNN and CBRCNN on several more encoding schemes (not reported), testing various options of PSI-BLAST and concatenating additional features such as protein length or the encoded sequence without a profile from alignments. We obtained the best results by adopting a simple, novel “clipping” technique (see Methods:Input Encoding) that is capable of presenting both the weighted profile from the aligned sequences and the identity of the AA in the protein itself, while keeping the encoding size unchanged. Combining this clipping scheme and the alignment profile of maximal entropy, a single CBRCNN reached 83.1% Q3 accuracy, as reported in Table <xref rid="Tab1" ref-type="table">1</xref>.</p>
      </sec>
      <sec id="Sec6">
        <title>HHblits</title>
        <p id="Par15">As a final step to exploit evolutionary information, we adopted alignments generated by HHblits<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, and compared it to PSI-BLAST<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>.</p>
        <p id="Par16">Although HHblits aligns considerably fewer sequences in our experimental settings (roughly a tenth of PSI-BLAST, that is ~1,300 proteins in our case), the set of hyperparameters selected for the CBRCNN trained on PSI-BLAST also worked close to optimally for training on HHblits inputs. In particular, after some tuning of the HHblits options (see Methods:Evolutionary Information), we observed a Q3 accuracy of 83.15% training CBRCNN on HHblits inputs, directly comparable with the 83.1% obtained on PSI-BLAST inputs. Refining on HHblits profiles models previously trained on PSI-BLAST gave a Q3 accuracy of 83.41%. Training a single CBRCNN on the average of PSI-BLAST and HHblits inputs improved the accuracy further, to 83.79%. We found less beneficial to train on inputs encoded from the union (83.41%) or the intersection (82.81%) of the two sets of alignments. Finally, we obtained 83.77% Q3 accuracy training on the concatenation of PSI-BLAST and HHblits profiles (44 inputs rather than 22). See Table <xref rid="Tab2" ref-type="table">2</xref> for a summary.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performances of single CBRCNN trained with different approaches relaying on both PSI-BLAST and HHblits.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Training</th><th><italic>From scratch</italic></th><th>Refining</th><th>Average</th><th>Union</th><th>Intersection</th><th>Concatenation</th></tr></thead><tbody><tr><td>Q3 Accuracy</td><td><italic>83.15</italic>%</td><td>83.41%</td><td>83.79%</td><td>83.41%</td><td>82.81%</td><td>83.77%</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>Towards state-of-the-art predictor</title>
      <p id="Par17">Finally, we built an ensemble of predictors based on the most successful individual models. All the experiments were run on five-fold cross-validation to gauge generalization performances of the ensemble<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
      <sec id="Sec8">
        <title>Ensembling</title>
        <p id="Par18">Bayesian model averaging is a classic ensembling approach which we exploited since the first version of Porter<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>: the outputs of individual models (outputs of a softmax function in our case) are simply averaged component by component. We ran preliminary testing by splitting our set into 1/5 for testing and 4/5 for training. An ensemble of the best 17 CBRCNN, with different hyperparameters but all trained on PSI-BLAST, achieved an accuracy of 84% (Table <xref rid="Tab2" ref-type="table">2</xref>). Ensembles of decreasing sizes record modest reductions in performances, down to 83.82% with just 3 CBRCNN. Adding 3 structurally identical CBRCNN trained on HHblits inputs we observed a Q3 accuracy of 84.63%. We could not significantly improve on this by adding any further model trained on either HHblits or PSI-BLAST. Adding to the ensemble the single best performing CBRCNN trained on the concatenation of PSI-BLAST and HHblits inputs led to a further small increase in performances, up to 84.7% Q3.</p>
        <p id="Par19">We then tested this same ensemble of 7 models in 5-fold cross-validation, without any further tuning of hyperparameters or any change in the models selected. We obtained very similar results to our preliminary testing. The overall ensemble accuracy, averaged over the 5 folds, was 84.85%.</p>
        <p id="Par20">Finally, we trained from scratch the 7 best performing CBRCNN (selected by cross-validation, as described above) on the full training set rather than on individual training folds of the cross-validation. We then tested an ensemble of these 7 models on a completely independent set (see “2017_test” in Methods:Datasets) containing over 3,000 proteins. We compared the accuracy of this ensemble against the ensemble of all 35 models resulting from the 5-fold cross-validation training (3 PSI-BLAST CBRCNN, 3 HHblits CBRCNN and 1 PSI-BLAST + HHblits CBRCNN for each of the 5 folds). As reported in Table <xref rid="Tab3" ref-type="table">3</xref>, while there were some differences between the accuracies of individual components of these two solutions, the overall ensembles performed almost identically, hence the retrained ensemble of 7 models is preferable for the final predictor as it is computationally more compact than the ensemble of 35 models.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Assessment on the 2017_test set of three-state ensembles trained on either five-fold cross-validation or full set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Training strategy</th><th>PSI-BLAST</th><th>HHblits</th><th>Concatenation</th><th>PSI-BLAST and HHblits</th><th>All the previous</th></tr></thead><tbody><tr><td>Five-fold cross-validation</td><td>83.55%</td><td>83.55%</td><td>83.98%</td><td>84.18%</td><td>84.19%</td></tr><tr><td>Full set (Porter 5)</td><td>83.42%</td><td>83.39%</td><td>83.49%</td><td>84.13%</td><td>84.19%</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="Sec9">
        <title>Stacking and further results</title>
        <p id="Par21">We tried many other architectural solutions during preliminary testing, including deep FFNN architectures, and structures akin to Residual Neural Networks<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, in which the global inputs to the model (the profile of residue frequencies) is presented to downstream stages through shortcut connections alongside the predictions of previous stages<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. While we observed small improvements when modestly increasing the number of hidden layers (up to 3–4, depending on the precise configuration), the results we obtained were generally poorer than those we observed with CBRCNN - typically around 1.5% worse than individual CBRCNN of similar size, and approximately 2% worse than those of a stack of 2 CBRCNN, which is what we used in our final predictors. While it is not entirely clear why, it appears that the recurrent stages in the CBRCNN are more efficient at capturing the sequential dynamics of our inputs than those of feed-forward networks alone, possibly because of their unrestricted input size. We did observe only marginal improvements in performances (roughly +0.1%) when stacking more than 2 CBRCNN stages with shortcut connections, and decided against including these more complex models into our final testing and predictor.</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Eight-state prediction</title>
      <p id="Par22">We applied the same pipeline described in Ensembling (section above) to the prediction of the full DSSP 8-class definition of SS<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. It should be noted that this slightly increases the total number of tunable parameters of the CBRCNN with respect to three-state SS prediction. In particular, we applied Bayesian model averaging on an equal number of CBRCNN trained on either PSI-BLAST or HHblits inputs, and some trained on concatenated inputs (as in Table <xref rid="Tab2" ref-type="table">2</xref>). We obtained 71.76%, 71.66% and 72.29% Q8 accuracy training single CBRCNN on 4/5 of the training set on PSI-BLAST, HHblits and concatenated inputs, respectively. An ensemble of 3 CBRCNN trained on PSI-BLAST inputs yields 72.47% Q8 accuracy on the same fold. When we add to the input of these networks the output of the ensemble of the 3 corresponding (PSI-BLAST) models trained on the 3-class problem we record a further improvement, to 72.79% Q8 (+0.3%), without dramatically increasing the encoding size (total of 25 inputs). We extended this approach to the 3 HHblits-trained models and to the one trained on concatenated PSI-BLAST and HHblits profiles.</p>
      <p id="Par23">The overall ensemble of 7 models trained on the full training set achieves 73.02% Q8 accuracy on the 2017_test set described in Methods:Dataset (see also Table <xref rid="Tab4" ref-type="table">4</xref>). An ensemble of the same 7 CBRCNN without the three-state predictions as inputs has an accuracy of 72.11%, confirming that including these predictions is beneficial.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Q3/Q8 accuracy and SOV score per AA on the full test set.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Q3</th><th>SOV’99</th><th>SOV_refine</th><th>Q8</th><th>SOV8’99</th><th>SOV8_refine</th></tr></thead><tbody><tr><td><bold>Porter 5</bold></td><td><bold>84.19%</bold></td><td><bold>81.19%</bold></td><td><bold>76.72%</bold></td><td><bold>73.02%</bold></td><td><bold>69.91%</bold></td><td><bold>72.09%</bold></td></tr><tr><td>Porter 5 (HHblits and PSI-BLAST)</td><td>83.49%</td><td>80.17%</td><td>75.64%</td><td>71.94%</td><td>69.03%</td><td>71.45%</td></tr><tr><td>Porter 5 (PSI-BLAST only)</td><td>83.42%</td><td>80.41%</td><td>75.8%</td><td>72.11%</td><td>69.28%</td><td>71.56%</td></tr><tr><td>Porter 5 (HHblits only)</td><td>83.39%</td><td>80.19%</td><td>75.59%</td><td>71.8%</td><td>68.87%</td><td>71.16%</td></tr><tr><td>SSpro 5.1 with templates</td><td>82.62%</td><td>79%</td><td>74.58%</td><td>71.91%</td><td>68.68%</td><td>70.72%</td></tr><tr><td>PSIPRED 4.01</td><td>82.06%</td><td>77.83%</td><td>72.95%</td><td>N.A.</td><td>N.A.</td><td>N.A.</td></tr><tr><td>RaptorX-Property</td><td>82.04%</td><td>78.57%</td><td>73.66%</td><td>70.74%</td><td>67.59%</td><td>69.65%</td></tr><tr><td>Porter 4</td><td>82%</td><td>78.85%</td><td>73.89%</td><td>N.A.</td><td>N.A.</td><td>N.A.</td></tr><tr><td>DeepCNF</td><td>81%</td><td>76.96%</td><td>71.84%</td><td>69.76%</td><td>66.42%</td><td>68.5%</td></tr><tr><td>SSpro 5.1 ab initio</td><td>80.7%</td><td>76.85%</td><td>72%</td><td>68.85%</td><td>65.33%</td><td>67.54%</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec11">
      <title>Assessment of multiple predictors on independent test set</title>
      <p id="Par24">Porter 5 is an ensemble of 7 CBRCNN (see Ensembling, above): 3 trained on PSI-BLAST, 3 trained on HHblits and 1 trained on both (44 inputs rather than 22). Porter 5 relies on 7 more CBRCNN to predict eight-state SS (see Eight-state prediction, above). We tested Porter 5 against Porter 4<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, Spider3<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, SSpro 5.1<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, PSIPRED 4.01<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, RaptorX-Property<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> and DeepCNF<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> on the 2017_test set we created, containing 3,154 proteins. Spider3 rejects proteins containing undetermined (X) amino acids (562 overall) and, when we use the parameters required by Spider3, either PSI-BLAST or HHblits do not return a valid result for 129 proteins. Because of this we report results on two sets: one where we exclude the proteins on which we could not obtain a valid response from Spider3 (2,463 entries composed by 497,142AA) (Table <xref rid="Tab5" ref-type="table">5</xref>); the full set of 3,154 proteins (Table <xref rid="Tab4" ref-type="table">4</xref>) on which Spider3 is not assessed.<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performances on the smaller 2017_test set for which Spider3 generates predictions, sorted by Q3 accuracy.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Q3 per AA</th><th>SOV’99 per AA</th><th>SOV_refine per AA</th><th>Q3 per protein</th><th>SOV’99 per protein</th><th>SOV_refine per protein</th></tr></thead><tbody><tr><td><bold>Porter 5</bold></td><td><bold>83.81%</bold></td><td><bold>80.41%</bold></td><td><bold>75.73%</bold></td><td><bold>84.32%</bold></td><td><bold>81.05%</bold></td><td><bold>76.45%</bold></td></tr><tr><td>Spider3</td><td>83.15%</td><td>79.43%</td><td>74.68%</td><td>83.42%</td><td>79.79%</td><td>75.07%</td></tr><tr><td>Porter 5 (HHblits only)</td><td>83.06%</td><td>79.49%</td><td>74.71%</td><td>83.68%</td><td>80.26%</td><td>75.58%</td></tr><tr><td>SSpro 5.1 with templates</td><td>82.58%</td><td>78.54%</td><td>74.02%</td><td>83.94%</td><td>80.29%</td><td>76.15%</td></tr><tr><td>PSIPRED 4.01</td><td>81.88%</td><td>77.36%</td><td>72.33%</td><td>82.48%</td><td>78.22%</td><td>73.31%</td></tr><tr><td>RaptorX-Property</td><td>81.86%</td><td>78.08%</td><td>72.99%</td><td>82.57%</td><td>78.99%</td><td>74.03%</td></tr><tr><td>Porter 4</td><td>81.66%</td><td>78.05%</td><td>72.89%</td><td>82.29%</td><td>78.61%</td><td>73.55%</td></tr><tr><td>SSpro 5.1 ab initio</td><td>81.17%</td><td>76.87%</td><td>72.03%</td><td>81.1%</td><td>76.92%</td><td>72.12%</td></tr><tr><td>DeepCNF</td><td>81.04%</td><td>76.74%</td><td>71.47%</td><td>81.16%</td><td>76.99%</td><td>71.7%</td></tr></tbody></table></table-wrap></p>
      <p id="Par25">Porter 5 is the most accurate 3-state and 8-state predictor in our tests on the 2017_test set with 3-class accuracy of 83.8% on the smaller version of the set and 84.2% on the larger one, 0.7% better than Spider3, 1.2–1.6% better than SSpro 5.1 with templates, and at least 2% more accurate than all the other predictors.</p>
      <p id="Par26">Performances of the servers show very similar deviations and differences greater than approximately 0.12% in Table <xref rid="Tab4" ref-type="table">4</xref> and 0.14% in Table <xref rid="Tab5" ref-type="table">5</xref> are significant at p = 0.05. Porter 5 is also very fast given the small size of its models (on average 39k parameters for the 3-class networks, 58k for 8 classes). Once the alignments by PSI-BLAST and HHblits are present, Porter 5 runs 2 orders of magnitude faster than Spider3.</p>
      <p id="Par27">We also measured the SOV’99<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> and the SOV_refine<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> (see Methods:Measuring performances) of every SS predictor on both versions of the 2017_test set. Porter 5 is consistently the best-performing 3-state and 8-state SS predictor, with both SOV scores at least 1% and 2% better than any other SS predictor on small and large versions of the set, respectively. Porter 5 is also 1.2% better that any other predictor considering the 8-state SOV’99 and the SOV_refine scores.</p>
      <p id="Par28">Finally, we measured Porter 5 performances on the CASP13<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> set, and on 6 months of proteins (December 28 2018 to June 22 2019) released by CAMEO<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. The results are reported in Table <xref rid="Tab6" ref-type="table">6</xref> and roughly confirm the Porter 5 results we obtained on the 2017_test set.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Assessment of Porter 5 on CASP13, i.e. 43 targets, and on the last 6 months of CAMEO, i.e. 463 proteins released from Dec 28, 2018 to Jun 22, 2019.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Q3</th><th>SOV’99</th><th>SOV_refine</th><th>Q8</th><th>SOV8’99</th><th>SOV8_refine</th></tr></thead><tbody><tr><td>CAMEO</td><td>85.48%</td><td>82.08%</td><td>78.08%</td><td>74.99%</td><td>72.36%</td><td>74.81%</td></tr><tr><td>CASP13</td><td>82.99%</td><td>78.36%</td><td>73.39%</td><td>71.08%</td><td>66.95%</td><td>69.27%</td></tr></tbody></table></table-wrap></p>
      <sec id="Sec12">
        <title>Nuclear magnetic resonance</title>
        <p id="Par29">We also analyzed the performance of Porter 5 separately on proteins resolved by Nuclear Magnetic Resonance (NMR) and by X-ray crystallography. NMR proteins are predicted at a significantly lower Q3 accuracy (81.6%, <italic>σ</italic> = 0.12%), possibly because of their different statistics (e.g. average length and composition) or less certain determination of SS. The X-ray only section of the 2017_test set, which is roughly 90% of the total, is predicted at an average Q3 of 84.65% (Table <xref rid="Tab7" ref-type="table">7</xref>).<table-wrap id="Tab7"><label>Table 7</label><caption><p>Porter 5 on NMR vs X-ray crystallography proteins.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Q3</th><th>SOV’99</th><th>SOV_refine</th><th>Q8</th><th>SOV’99</th><th>SOV_refine</th></tr></thead><tbody><tr><td>NMR</td><td>81.61%</td><td>76.64%</td><td>70.55%</td><td>67.52%</td><td>62.41%</td><td>63.86%</td></tr><tr><td><bold>X-ray</bold></td><td><bold>84.65%</bold></td><td><bold>81.99%</bold></td><td><bold>77.81%</bold></td><td><bold>74%</bold></td><td><bold>71.24%</bold></td><td><bold>73.54%</bold></td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="Sec13">
        <title>Porter 5 with SCOP based redundancy reduction protocol</title>
        <p id="Par30">While redundancy reduction protocols similar to the one we adopted to build our sets are widely used<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR42">42</xref>–<xref ref-type="bibr" rid="CR44">44</xref></sup>, this type of redundancy reduction does not fully eliminate the occurrence of proteins with similar 3D structures (hence similar SS) in the training and testing sets<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>. Normally, the only way to genuinely control for this and produce sets that are completely devoid of structurally homologous examples is to resort to classifications of protein structures such as SCOPe/ASTRAL<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> and use information gleaned from these to guide the construction of the data sets, e.g. by selecting only one representative per superfamily or family of proteins. The drawback of this procedure is that the resulting sets will be smaller, and it has been shown in different occasions (e.g.<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>) that, all other factors being the same (e.g. same algorithms, same redundancy reduction protocols) larger data sets lead to improvements in performances. In order to gauge the effect of stricter redundancy reduction criteria on the methods presented here, we retrained 2 separate versions of Porter 5 using the JPred4 sets<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. In these sets, only one representative for each of the 1,358 SCOPe/ASTRAL v.2.04<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> superfamily domain sequences is selected for the training set, while a further 150 proteins from superfamilies not included in the training set are used as a blind test set. The first version we retrained uses the exact same protocol and ensemble as Porter 5, including recent versions of the UniRef database for the creation of MSA and a combination of alignments by PSI-BLAST and HHblits, but it is trained on the 1,348 JPred4 set. In the second version we also adopted the same alignments used by JPred4, based on release 2014_7 of UniRef90 and obtained using PSI-BLAST, which are available from the JPred4 web site. It should be noted that in this case, given that we do not use HHblits alignments, all the models in the ensemble are trained solely on PSI-BLAST profiles. The first version, adopting recent alignments, achieves 84.62% correct prediction on the JPred4 blind set. The second version, which relies on the exact same training and testing data as JPred4, achieves 83.62% correct prediction. JPred4, which is based on a standard feed-forward neural network architecture, has a 82.29% Q3 per amino acid on the same sets. While the testing set is small (150 proteins), these results suggest that more sophisticated machine learning algorithms (and, indeed, more up to date alignment sets and treatment thereof) may be beneficial to predictive performances. These results also roughly match what we found on our larger data sets, although it should be noted that the class definition in these sets is slightly different in that DSSP class ‘G’ is assigned to Coil rather than to Helix. This different assignment has been shown in the past to lead to somewhat higher Q3 values, e.g. in<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>, which might explain how Porter achieves a similar Q3 when trained on a set which is an order of magnitude smaller than its original training set.</p>
        <p id="Par31">We also assessed Jpred4 on the 2019_test set (see Table <xref rid="Tab8" ref-type="table">8</xref> and description of the set in the following section) with classes recast to match the Jpred4 class assignment. In this case the more modern predictors including Porter 5 show Q3 3.7–4.7% higher than Jpred4 and similar improvements in SOV, suggesting that larger training sets, alongside larger alignment sets and more sophisticated algorithms, may be beneficial.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Most recent predictors and Jpred4 assessed on 2019_test set of 618 proteins.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Q3</th><th>SOV’99</th><th>SOV_refine</th><th>Q8</th><th>SOV8’99</th><th>SOV8_refine</th></tr></thead><tbody><tr><td>SPOT-1D</td><td>82.13%</td><td>76.65%</td><td>71.37%</td><td>69.69%</td><td>65.52%</td><td>67.18%</td></tr><tr><td><bold>Porter 5</bold></td><td><bold>81.74%</bold></td><td><bold>76.67%</bold></td><td><bold>71.03%</bold></td><td><bold>68.25%</bold></td><td><bold>63.24%</bold></td><td><bold>64.87%</bold></td></tr><tr><td>NetSurfP-2.0</td><td>81.3%</td><td>75.64%</td><td>70.3%</td><td>67.93%</td><td>62.77%</td><td>64.66%</td></tr><tr><td>MUFOLD-SS</td><td>81.09%</td><td>75.28%</td><td>69.87%</td><td>68.21%</td><td>64.3%</td><td>66.33%</td></tr><tr><td><italic>Jpred4</italic></td><td><italic>77.38%</italic></td><td><italic>72.29%</italic></td><td><italic>64.96%</italic></td><td><italic>N.A</italic>.</td><td><italic>N.A</italic>.</td><td><italic>N.A</italic>.</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="Sec14">
      <title>Assessment of latest SS predictors</title>
      <p id="Par32">In a separate test set we assessed some very recent predictors which have been trained on sets more recent than our 2017_test set, i.e. MUFOLD-SS<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, NetSurfP-2.0<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> and SPOT-1D<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. Because of this, we generated a second independent test set (also see Methods:Datasets) starting from June 24 2019 PDB proteins, 25% redundancy reduced against the Porter 5, NetSuft-2.0 and SPOT-1D training sets (we could not access the MUFOLD-SS training set). As either MUFOLD-SS or SPOT-1D or both did not produce a valid prediction for 243 out of the 861 original proteins in the set, Table <xref rid="Tab8" ref-type="table">8</xref> shows the performances observed on the 618 proteins successfully predicted by all predictors of this group. The 3 predictors have similar performances, with Porter 5 slightly outperforming both MUFOLD-SS and NetSurfP-2.0, but being slightly outperformed by SPOT-1D. Differences of 0.28% in Q3 and 0.37% in Q8 in the table are significant at p = 0.05. It should be noted that SPOT-1D relies on the predictions of SPOT-Contact<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, i.e. a Contact Map predictor<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, which in turn requires Spider3<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, CCMpred<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, and DCA<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. This results in a far more computationally intense pipeline which in essence derives the SS from a guess of a protein’s 3D structure through its contact map<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>.</p>
    </sec>
  </sec>
  <sec id="Sec15" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par33">In this study we describe the development of a new, state-of-the-art SS predictor, Porter 5<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. We trained window-based feed-forward neural networks with different hyperparameters and input encoding (see Table <xref rid="Tab1" ref-type="table">1</xref> to define our baselines and assess the quality of our large training set (see Table <xref rid="Tab9" ref-type="table">9</xref>). We developed both a state-of-the-art model, and a novel encoding technique, i.e. “clipping”. We assembled the final predictor Porter 5 as a simple ensemble of models trained on different inputs, i.e. either PSI-BLAST<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> or HHblits<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> or a concatenation of both. We applied a very similar approach to the harder eight-state SS prediction problem to develop the eight-state version of Porter 5 which represents, analogously to the three-state Porter 5, the state-of-the-art for this task (see Table <xref rid="Tab4" ref-type="table">4</xref>). Porter 4, the previous release of Porter, was trained on 7,522 proteins<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Thanks to the constant growth of the PDB<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, we performed all the experiments on a training set twice as large as the one adopted for Porter 4, i.e. 15,753 proteins (see Methods:Datasets). The results we present in this study confirm the continuing positive contribution of a larger, well-distributed training set.<table-wrap id="Tab9"><label>Table 9</label><caption><p>Overview of AA composition of Training, 2017_test and 2019_test.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>3-states</th><th>8-states</th><th colspan="2">Training Set</th><th colspan="2">2017_test Set</th><th colspan="2">2019_test Set</th></tr></thead><tbody><tr><td rowspan="3">Helices</td><td>G</td><td rowspan="3">38%</td><td>135,498</td><td rowspan="3">39.22%</td><td>21,404</td><td rowspan="3">37.41%</td><td>2,300</td></tr><tr><td>H</td><td>1,306,610</td><td>233,961</td><td>31,854</td></tr><tr><td>I</td><td>714</td><td>177</td><td>33</td></tr><tr><td rowspan="2">Sheets</td><td>E</td><td rowspan="2">22.15%</td><td>800,297</td><td rowspan="2">20.9%</td><td>129,425</td><td rowspan="2">17.87%</td><td>15,411</td></tr><tr><td>B</td><td>41,026</td><td>6,793</td><td>916</td></tr><tr><td rowspan="3">Coils</td><td>C</td><td rowspan="3">39.85%</td><td>764,391</td><td rowspan="3">39.88%</td><td>133,183</td><td rowspan="3">44.72%</td><td>21,605</td></tr><tr><td>S</td><td>331,075</td><td>57,678</td><td>9,804</td></tr><tr><td>T</td><td>417,815</td><td>68,973</td><td>9,452</td></tr></tbody></table></table-wrap></p>
    <p id="Par34">For this study, we exploited evolutionary information through different encodings and gauged their importance with respect to an encoding containing only the plain protein sequence. While we can now predict SS using plain protein sequences at an accuracy that would have represented the state-of-the-art including evolutionary information 25 years ago, we observed that evolutionary information is as important as ever, boosting prediction accuracies by 10% or more. In particular we used evolutionary information mined by both PSI-BLAST<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and HHblits<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and observed that while they lead to broadly similar predictive accuracies when used individually, their combination is clearly beneficial. To the best of our knowledge, Porter 5 is the first SS predictor to ensemble models trained on PSI-BLAST or HHblits, which (empirically) appears to be the most effective way to exploit both algorithms at the same time (see Table <xref rid="Tab2" ref-type="table">2</xref>).</p>
    <p id="Par35">We have also studied a number of different models, confirming that recurrent neural network architectures are particularly effective at SS prediction, with a combination of bidirectional recurrent networks and dense convolutional layers being the best performing model. While a modest increase in the number of stages adopted worked well for us, we did not observe improvements in performances beyond 3–4 internal layers for feed-forward networks and 2 stages of BRNN-CNN stacks. This seems to suggest that, at least given the current sizes of training sets, recurrent neural network stages capture all the long-range information that can be exploited effectively.</p>
    <p id="Par36">Unlike many other modern “deep” predictors, Porter 5’s models are individually tuned to have roughly correct individual expressive power rather than being oversized in the first place and kept to the right capacity by regularization techniques or dropout. Individual models within Porter 5 have 40,000–60,000 free parameters. This is significantly less than the average 500,000 parameters of DeepCNF<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, i.e. the PSI-BLAST version of RaptorX-Property<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, or the well over one-million of Spider3<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, although it is still a 2–3 fold increase with respect to the 13,000–18,000 free parameters of Porter 4<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. The relative small size of Porter 5 also means that, once alignments are available, individual predictions are extremely fast to run.</p>
    <p id="Par37">We assessed Porter 5 on a first independent test set (2017_test), along with some of the SS predictors trained up to 2017: DeepCNF<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, Porter 4<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, PSIPRED 4.01<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, RaptorX-Property<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, Spider3<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> and both versions of SSpro 5.1<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, i.e. profile-based and template-based. In all our tests Porter 5 outperformed the other methods, often by large margins with an accuracy of approximately 84% for 3-class SS prediction and 73% for 8-class prediction. It should also be noted that our assessment might be somewhat optimistic for some of the competing predictors since we did not perform any redundancy reduction of our final test set against their training sets<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
    <p id="Par38">Finally, we assessed Porter 5 against some of the most recent predictors which have also been trained on very recent and large training sets, i.e. MUFOLD-SS<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, NetSurfP-2.0<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> and SPOT-1D<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. As these predictors’ training sets overlapped with our original test set, we generated a second smaller independent test set (2019_test) based on PDB sequences uploaded up to June 24th. In this case we observed results which are broadly similar between these newer predictors, with Porter 5 slightly outperforming both MUFOLD-SS and NetSurfP-2.0, but slightly outperformed by SPOT-1D which, however, is built on a more complex and computationally intensive (though highly effective) pipeline in which the SS is predicted through a protein’s contact map.</p>
  </sec>
  <sec id="Sec16">
    <title>Methods</title>
    <sec id="Sec17">
      <title>Datasets</title>
      <p id="Par39">The selection and preparation of datasets to adopt has a central role in any machine learning method<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. We built our datasets from the Protein Data Bank (PDB)<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, the public repository of all the freely and publicly known protein structures. We assembled our final datasets only with proteins sharing up to 25% sequence identity<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Specifically, we built our training set from the PDB released on Dec 11 2014, internally redundancy-reduced at a 25% identity threshold. We also built an independent test set (2017_test) from the PDB released after Dec 11 2014 and up to Jun 14, 2017. We redundancy-reduced this set at a 25% identity threshold against the training set. Further, we internally redundancy reduced the resulting set at a 25% identity threshold. Finally, we removed all proteins with at least 10 consecutive undetermined AA from both sets. The training set contains 15,753 proteins (3,797,426 AA) and 2017_test 3,154 proteins (651,594 AA), among the largest ever used to build a SS predictor. The SS states were assigned according to the Dictionary of Protein SS (DSSP)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> and their distribution is highlighted in Table <xref rid="Tab9" ref-type="table">9</xref>. In different tests the training set is used as a whole for training purposes or split into 5 randomly distributed folds in cross-validation for hyperparameter optimization<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. The 2017_test set is only used in the final part of this study to evaluate our final solutions and other solutions previously published. The training and the test sets are available at <ext-link ext-link-type="uri" xlink:href="http://distilldeep.ucd.ie/porter/">http://distilldeep.ucd.ie/porter/</ext-link>.</p>
      <p id="Par40">We also curated an additional independent test set (2019_test) to fairly compare Porter 5 against some of the most recent SS predictors, i.e. MUFOLD-SS<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, NetSurfP-2.0<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> and SPOT-1D<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, which have been trained on sets overlapping with our 2017_test set. We removed any protein shorter than 30 AA or containing more than 10% of undetermined AA from the PDB proteins deposited up to Jun 24 2019. We then redundancy-reduced this set against the training sets of SPOT-1D, NetSurfP-2.0 and our training set at 25% identity threshold. Finally, we reduced the internal redundancy of this set at a 25% sequence identity threshold and obtained 861 proteins. As MUFOLD-SS or SPOT-1D or both do not return a valid answer for 243 of these proteins, we report results on 618 proteins, comprising 91,375 amino acids (2019_test).</p>
    </sec>
    <sec id="Sec18">
      <title>Evolutionary information</title>
      <p id="Par41">A key aspect of any modern SS predictor is harnessing evolutionary information<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. PSI-BLAST<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and more recently HHblits<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> are methods widely used for the purpose – i.e. gathering known protein sequences which are likely to be evolutionarily related to the protein of interest<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. We relied on both, finding the best results with the default settings and iterating them 3 times with an e-value of 0.001<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> without limiting the number of sequence hits. PSI-BLAST is run on the May, 2016 version of UniRef90<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, containing almost forty-two millions clusters. HHblits is run on the February, 2016 version of UniProt20, containing over eight millions clusters. Our experiments show similar results when a model is trained with either PSI-BLAST or HHblits, but significant improvements when both are used (see Results:HHblits).</p>
      <sec id="Sec19">
        <title>Input encoding</title>
        <p id="Par42">Among the several encoding schemes assessed, we focused on three approaches: alignment-free, plain profiles and weighted profiles.</p>
        <p id="Par43">For the alignment-free case, when no evolutionary information is employed, we adopted a simple one-hot encoding of 20 positions - one for each standard AA - and a zero vector for non standard AA, i.e. “B”, “J”, “O”, “U”, “Z”, and “X”.</p>
        <p id="Par44">For the plain profiles case, our baseline for employing evolutionary information, we adopted arrays of 22 positions composed of 20 frequencies for standard AA, 1 for unknown or non-standard and the last position for gaps. The first 21 numbers are normalized to add up to 1 without considering gaps, while the 22nd number represents the total frequency of gaps in a column of the alignment.</p>
        <p id="Par45">For the weighted profiles case, we maximized the entropy deriving from the evolutionary information applying a weighting scheme to the plain profiles<sup><xref ref-type="bibr" rid="CR56">56</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>. In particular, we calculated the weight of each sequence in the alignment as:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}_{seq}=\mathop{\sum }\limits_{n=1}^{length}\,-\,\mathrm{log}\,f[a{a}_{seq}(n)]$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:munderover><mml:mspace width=".25em"/><mml:mo>−</mml:mo><mml:mspace width="-.25em"/><mml:mi>log</mml:mi><mml:mspace width=".10em"/><mml:mi>f</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:math><graphic xlink:href="41598_2019_48786_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>where <italic>f</italic>[<italic>aa</italic><sub><italic>seq</italic></sub>(<italic>n</italic>)] is the relative frequency of the n-th AA of sequence <italic>seq</italic> within column <italic>n</italic> of the alignment. We then weighted every sequence <italic>seq</italic> in the alignment by W<sub><italic>seq</italic></sub> and, finally, normalized as for the plain profiles case, i.e. the first 21 components add up to 1 and the 22nd is normalized independently. Differently from plain profiles, we did not consider external gaps when calculating the gap frequency.</p>
        <p id="Par46">Clipping is the novel encoding method we introduce in this study. The simple idea is to set to 1 the position in the profile vector associated to the AA in the query sequence, regardless of its frequency in the alignment. This approach can be seen as a merging technique between one-hot encoding – adopted when evolutionary information is lacking – and any method to represent evolutionary information. It should also be noted that no information in the profile is lost when adopting clipping, as any one of the 21 numbers in the profile is equal to 1 minus the sum of the others.</p>
        <p id="Par47">We also build a version of Porter 5 which predicts the eight-state SS classes by the DSSP program<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. In this case the output of the three-state Porter 5 is concatenated to the input – i.e. 25 inputs rather than 22, as in the three-state Porter 5.</p>
      </sec>
    </sec>
    <sec id="Sec20">
      <title>Feedforward neural networks</title>
      <p id="Par48">We defined our baselines implementing window-based FFNN of up to 7 hidden-layers. The symmetric input-window allows segments of AA composed by an odd number of AA, centered on the current n-th position. More in detail, the input at n-th time step is defined as <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I(n)={v}_{n-l},{v}_{n-l+1},{v}_{n-l+2},\,\mathrm{..}\,{v}_{n+l}$$\end{document}</tex-math><mml:math id="M4"><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width=".25em"/><mml:mn>..</mml:mn><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41598_2019_48786_Article_IEq1.gif"/></alternatives></inline-formula> where <italic>ν</italic><sub><italic>n</italic></sub> is the n-th encoded input and <italic>l</italic> is the number of right- and left-adjacent AA considered as additional contextual information, i.e. the input-window contains <italic>l</italic> * 2 + 1AA at any position.</p>
      <p id="Par49">We trained one-hidden-layer FFNN increasing the number of hidden units – to verify whether we had sufficient data to approximate the mapping function (from AA to SS)<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> – and then trained deeper solutions – i.e. increasing the number of hidden layers. To reduce the computational costs of the hyperparameter search, we adopted an incremental training technique. More in detail, we continued the trainings until completion, then substituted the top layer, i.e. the softmax layer, with untrained hidden-layer + softmax layers and trained these alone, leaving all the weights upstream of them untouched. Finally, we briefly refined the whole FFNN, training every hidden-layer – i.e. end to end, and iterated the process.</p>
    </sec>
    <sec id="Sec21">
      <title>Cascaded bidirectional recurrent and convolutional neural networks</title>
      <p id="Par50">The CBRCNN, assessed in this study and at the core of Porter 5, is an additional refinement of the two-stage bidirectional recurrent neural network (BRNN) initially implemented for the first release of Porter<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> and successively exploited to predict several more protein structure annotations – e.g. relative solvent accessibility, torsion angles and contact density<sup><xref ref-type="bibr" rid="CR59">59</xref>–<xref ref-type="bibr" rid="CR61">61</xref></sup>. The CBRCNN preserves two cascaded stages, both containing a BRNN layer with two-layered recurrent cells, and introduces convolutional layers downstream of the BRNN to process windows of both forward and backward chain memories. Differently from the window-based FFNN, the CBRCNN fetches one input/AA at time but then elaborates the entire protein into two Markovian chains (of the BRNN), before processing windows of them through the convolutional layers (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>).<fig id="Fig1"><label>Figure 1</label><caption><p>Diagram of the BRCNN. The input sequence is processed by three stages, i.e. one BRNN and two CNN stages, in order to predict the SS. The final architecture of Porter 5 - the CBRCNN - is the (two) cascaded version of the above.</p></caption><graphic xlink:href="41598_2019_48786_Fig1_HTML" id="d29e2424"/></fig></p>
      <p id="Par51">In particular, a BRNN (with independent weights and one hidden layer) is followed by a 1D convolutional layer with kernel size greater than one – i.e. able to look at different time steps of the two preceding chain states –, then by a further convolutional layer of kernel size one with softmax outputs. Equivalently, the two convolutional stages can be thought of as a single map implemented by a two-layered network. The output of this overall network is then fed to a similar network for the second stage. The main differences between the first and the second stage are the network size and input: all the layer sizes in the second stage are half the size of those in the first stage, and the output of the first stage network is averaged in different segments to feed the second stage. In other words, the second stage CBRCNN learns to associate every target with a given number of segments, which are built averaging the output of the first stage CBRCNN.</p>
      <p id="Par52">We fixed the number of time steps seen by the first convolutional layer (i.e. the kernel size) to 7 – i.e. 3 adjacent steps per side plus the one at a given position –, and the number of segments of the second stage and their size to 15 and 21, respectively. Therefore, the second stage processes 15 windows, each containing the average of the first stage predictions over 21 time steps, for a total of 315 adjacent steps processed per prediction.</p>
      <p id="Par53">The number of hyperparameters to set in a BRNN, and the more sophisticated internal dynamics, makes this architecture a more complex neural network to train and tune with respect to a FFNN. Step by step, the memory size for the recurrent networks (NF/B), the hidden layer sizes for the recurrent networks (NHF/B) and for the layer preceding the softmax (NHY), in addition to the number of time steps seen by the convolutional layer (CoF/B) and the number and size of the segments feeding the second stage (Cseg and Cwin), have to be determined. The values for these hyperparameters in the models used within Porter 5 are reported in Table <xref rid="Tab10" ref-type="table">10</xref>.<table-wrap id="Tab10"><label>Table 10</label><caption><p>The hyperparameters of the models employed for Porter 5.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Input</th><th colspan="4">3-state</th><th colspan="4">8-state</th></tr><tr><th colspan="3">PSI-BLAST or HHblits</th><th>Concatenated</th><th colspan="3">PSI-BLAST or HHblits</th><th>Concatenated</th></tr></thead><tbody><tr><td>NF/B</td><td>25</td><td>30</td><td>30</td><td>25</td><td>30</td><td>35</td><td>36</td><td>30</td></tr><tr><td>NHF/B</td><td>40</td><td>40</td><td>45</td><td>40</td><td>45</td><td>55</td><td>60</td><td>45</td></tr><tr><td>NHY</td><td>50</td><td>50</td><td>55</td><td>50</td><td>50</td><td>45</td><td>48</td><td>50</td></tr><tr><td>CoF/B</td><td>3</td><td>3</td><td>3</td><td>3</td><td>3</td><td>3</td><td>3</td><td>3</td></tr><tr><td>Cseg</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td><td>10</td></tr><tr><td>Cwin</td><td>7</td><td>7</td><td>7</td><td>7</td><td>7</td><td>7</td><td>7</td><td>7</td></tr></tbody></table><table-wrap-foot><p>A total of 7 models are ensembled in both 3- and 8-state component. The PSI-BLAST or HHblits only models share the same hyperparameters.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec22">
      <title>Ensembling</title>
      <p id="Par54">In all cases we ensembled models by simply taking the average of their class (softmax) outputs. In preliminary tests we briefly assessed more complex strategies, e.g. Bayesian Model Combination<sup><xref ref-type="bibr" rid="CR62">62</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup> in which model-specific weights are learned, but did not find evidence that they performed significantly better than the simple average.</p>
    </sec>
    <sec id="Sec23">
      <title>Measuring performances</title>
      <p id="Par55">The two most commonly used measures to assess SS predictors, accuracy and SOV, have been employed in this study. Accuracy is simply the fraction of AA whose predicted SS class is the same as the observed class, as determined by DSSP<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. For the 3-class problem (helix, sheet, and coil) we call this Q3 accuracy. For the 8-class problem (<italic>α</italic>-helix, 3<sub>10</sub>-helix, <italic>π</italic>-helix, <italic>β</italic>-sheet, extended strand, hydrogen bonded turn, bend, and other) we call this Q8 accuracy. The 3 classes in the 3-class problem are obtained by merging DSSP-assigned <italic>α</italic>-helix, 3<sub>10</sub>-helix and <italic>π</italic>-helix into class helix, <italic>β</italic>-sheet and extended strand into sheet, and the rest into coil.</p>
      <p id="Par56">We also measured the Segment Overlap (SOV) between the predicted SS and the true one. This latter measure is meant to evaluate the prediction from a more biological viewpoint considering segments rather than single AA as the relevant prediction units. We measured both SOV’99<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> and SOV_refine<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>.</p>
    </sec>
    <sec id="Sec24">
      <title>Optimization</title>
      <p id="Par57">We implemented momentum<sup><xref ref-type="bibr" rid="CR64">64</xref></sup> and a dynamic adaptive learning rate to optimize the training process, along with standard stochastic gradient descent (SGD)<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. We set momentum to 0.9 and divided by two the learning rate any time that the cross entropy error on training set had not decreased for 100 epochs. The training set is shuffled at the end of each epoch, while the size of a mini-batch is set to ~10 proteins, that is, the network weights are updated during training after estimating the gradient on ~10 proteins at a time.</p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The work of M.T., and M.K. is supported by the Irish Research Council [GOIPG/2015/3717, and GOIPG/2014/603]. The UCD computing cluster provided computational resources for this work.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author Contributions</title>
    <p>M.T. ran and designed most of the experiments, analysed the results and produced the first draft of the article, M.K. contributed to the analysis and to the design of the input encodings. G.P. contributed to the design of the study and the individual experiments, implemented much of the code, contributed to the analysis of the data and to the writing and revision of the manuscript. All authors reviewed the manuscript and agree with its contents.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability</title>
    <p>Porter 5 is available as a web server and light standalone program at <ext-link ext-link-type="uri" xlink:href="http://distilldeep.ucd.ie/porter/">http://distilldeep.ucd.ie/porter/</ext-link> alongside with all the datasets and alignments.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing Interests</title>
    <p id="Par58">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <collab>The UniProt Consortium</collab>
        </person-group>
        <article-title>UniProt: the universal protein knowledgebase</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2016</year>
        <volume>45</volume>
        <fpage>D158</fpage>
        <lpage>D169</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw1099</pub-id>
        <pub-id pub-id-type="pmid">27899622</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berman</surname>
            <given-names>HM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The Protein Data Bank</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2000</year>
        <volume>28</volume>
        <fpage>235</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id>
        <pub-id pub-id-type="pmid">10592235</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Review: Protein Secondary Structure Prediction Continues to Rise</article-title>
        <source>J. Struct. Biol.</source>
        <year>2001</year>
        <volume>134</volume>
        <fpage>204</fpage>
        <lpage>218</lpage>
        <pub-id pub-id-type="doi">10.1006/jsbi.2001.4336</pub-id>
        <?supplied-pmid 11551180?>
        <pub-id pub-id-type="pmid">11551180</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Torrisi, M. &amp; Pollastri, G. Protein Structure Annotations. In Shaik, N. A., Hakeem, K. R., Banaganapalli, B. &amp; Elango, R. (eds) <italic>Essentials of Bioinformatics, Volume I: Understanding Bioinformatics: Genes to Proteins</italic>, 201–234 (Springer International Publishing, Cham, 2019), 10.1007/978-3-030-02634-910.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pauling</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Corey</surname>
            <given-names>RB</given-names>
          </name>
        </person-group>
        <article-title>Configurations of Polypeptide Chains With Favored Orientations Around Single Bonds</article-title>
        <source>Proc. Natl. Acad. Sci. United States Am.</source>
        <year>1951</year>
        <volume>37</volume>
        <fpage>729</fpage>
        <lpage>740</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.37.11.729</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Szent-Gyorgyi</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Role of Proline in Polypeptide Chain Configuration of Proteins</article-title>
        <source>Science</source>
        <year>1957</year>
        <volume>126</volume>
        <fpage>697</fpage>
        <lpage>698</lpage>
        <pub-id pub-id-type="doi">10.1126/science.126.3276.697</pub-id>
        <?supplied-pmid 13467266?>
        <pub-id pub-id-type="pmid">13467266</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Davies</surname>
            <given-names>DR</given-names>
          </name>
        </person-group>
        <article-title>A correlation between amino acid composition and protein structure</article-title>
        <source>J. Mol. Biol.</source>
        <year>1964</year>
        <volume>9</volume>
        <fpage>605</fpage>
        <lpage>609</lpage>
        <pub-id pub-id-type="doi">10.1016/S0022-2836(64)80232-1</pub-id>
        <?supplied-pmid 14202291?>
        <pub-id pub-id-type="pmid">14202291</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lim</surname>
            <given-names>VI</given-names>
          </name>
        </person-group>
        <article-title>Structural principles of the globular organization of protein chains. A stereochemical theory of globular protein secondary structure</article-title>
        <source>J. Mol. Biol.</source>
        <year>1974</year>
        <volume>88</volume>
        <fpage>857</fpage>
        <lpage>872</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-2836(74)90404-5</pub-id>
        <?supplied-pmid 4427383?>
        <pub-id pub-id-type="pmid">4427383</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kabsch</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>How good are predictions of protein secondary structure?</article-title>
        <source>FEBS Lett.</source>
        <year>1983</year>
        <volume>155</volume>
        <fpage>179</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1016/0014-5793(82)80597-8</pub-id>
        <?supplied-pmid 6852232?>
        <pub-id pub-id-type="pmid">6852232</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Garnier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Osguthorpe</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Robson</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Analysis of the accuracy and implications of simple methods for predicting the secondary structure of globular proteins</article-title>
        <source>J. Mol. Biol.</source>
        <year>1978</year>
        <volume>120</volume>
        <fpage>97</fpage>
        <lpage>120</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-2836(78)90297-8</pub-id>
        <?supplied-pmid 642007?>
        <pub-id pub-id-type="pmid">642007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ptitsyn</surname>
            <given-names>OB</given-names>
          </name>
          <name>
            <surname>Finkelstein</surname>
            <given-names>AV</given-names>
          </name>
        </person-group>
        <article-title>Theory of protein secondary structure and algorithm of its prediction</article-title>
        <source>Biopolymers</source>
        <year>1983</year>
        <volume>22</volume>
        <fpage>15</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1002/bip.360220105</pub-id>
        <?supplied-pmid 6673754?>
        <pub-id pub-id-type="pmid">6673754</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qian</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sejnowski</surname>
            <given-names>TJ</given-names>
          </name>
        </person-group>
        <article-title>Predicting the secondary structure of globular proteins using neural network models</article-title>
        <source>J. Mol. Biol.</source>
        <year>1988</year>
        <volume>202</volume>
        <fpage>865</fpage>
        <lpage>884</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-2836(88)90564-5</pub-id>
        <?supplied-pmid 3172241?>
        <pub-id pub-id-type="pmid">3172241</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kneller</surname>
            <given-names>DG</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>FE</given-names>
          </name>
          <name>
            <surname>Langridge</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Improvements in protein secondary structure prediction by an enhanced neural network</article-title>
        <source>J. Mol. Biol.</source>
        <year>1990</year>
        <volume>214</volume>
        <fpage>171</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-2836(90)90154-E</pub-id>
        <?supplied-pmid 2370661?>
        <pub-id pub-id-type="pmid">2370661</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holley</surname>
            <given-names>LH</given-names>
          </name>
          <name>
            <surname>Karplus</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction with a neural network</article-title>
        <source>Proc. Natl. Acad. Sci. United States Am.</source>
        <year>1989</year>
        <volume>86</volume>
        <fpage>152</fpage>
        <lpage>156</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.86.1.152</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mitchell</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Artymiuk</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Rice</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Willett</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Use of techniques derived from graph theory to compare secondary structure motifs in proteins</article-title>
        <source>J. Mol. Biol.</source>
        <year>1990</year>
        <volume>212</volume>
        <fpage>151</fpage>
        <lpage>166</lpage>
        <pub-id pub-id-type="doi">10.1016/0022-2836(90)90312-A</pub-id>
        <?supplied-pmid 2319595?>
        <pub-id pub-id-type="pmid">2319595</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yi</surname>
            <given-names>TM</given-names>
          </name>
          <name>
            <surname>Lander</surname>
            <given-names>ES</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction using nearest-neighbor methods</article-title>
        <source>J. Mol. Biol.</source>
        <year>1993</year>
        <volume>232</volume>
        <fpage>1117</fpage>
        <lpage>1129</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1993.1464</pub-id>
        <?supplied-pmid 8371270?>
        <pub-id pub-id-type="pmid">8371270</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Prediction of Protein Secondary Structure at Better than 70% Accuracy</article-title>
        <source>J. Mol. Biol.</source>
        <year>1993</year>
        <volume>232</volume>
        <fpage>584</fpage>
        <lpage>599</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1993.1413</pub-id>
        <?supplied-pmid 8345525?>
        <pub-id pub-id-type="pmid">8345525</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Przybylski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Alignments grow, secondary structure prediction improves</article-title>
        <source>Proteins</source>
        <year>2002</year>
        <volume>46</volume>
        <fpage>197</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.10029</pub-id>
        <pub-id pub-id-type="pmid">11807948</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Redefining the goals of protein secondary structure prediction</article-title>
        <source>J. Mol. Biol.</source>
        <year>1994</year>
        <volume>235</volume>
        <fpage>13</fpage>
        <lpage>26</lpage>
        <pub-id pub-id-type="doi">10.1016/S0022-2836(05)80007-5</pub-id>
        <pub-id pub-id-type="pmid">8289237</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Protein secondary structure prediction based on position-specific scoring matrices11edited by Von Heijne, G</article-title>
        <source>J. Mol. Biol.</source>
        <year>1999</year>
        <volume>292</volume>
        <fpage>195</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="doi">10.1006/jmbi.1999.3091</pub-id>
        <?supplied-pmid 10493868?>
        <pub-id pub-id-type="pmid">10493868</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brunak</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Frasconi</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Soda</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Exploiting the past and the future in protein secondary structure prediction</article-title>
        <source>Bioinformatics</source>
        <year>1999</year>
        <volume>15</volume>
        <fpage>937</fpage>
        <lpage>946</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/15.11.937</pub-id>
        <?supplied-pmid 10743560?>
        <pub-id pub-id-type="pmid">10743560</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>McLysaght</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Porter: a new, accurate server for protein secondary structure prediction</article-title>
        <source>Bioinformatics</source>
        <year>2005</year>
        <volume>21</volume>
        <fpage>1719</fpage>
        <lpage>1720</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bti203</pub-id>
        <?supplied-pmid 15585524?>
        <pub-id pub-id-type="pmid">15585524</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Buchan</surname>
            <given-names>DWA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein annotation and modelling servers at University College London</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2010</year>
        <volume>38</volume>
        <fpage>W563</fpage>
        <lpage>W568</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkq427</pub-id>
        <?supplied-pmid 20507913?>
        <pub-id pub-id-type="pmid">20507913</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Magnan</surname>
            <given-names>CN</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>SSpro/ACCpro 5: almost perfect prediction of protein secondary structure and relative solvent accessibility using profiles, machine learning and structural similarity</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <fpage>2592</fpage>
        <lpage>2597</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu352</pub-id>
        <?supplied-pmid 24860169?>
        <pub-id pub-id-type="pmid">24860169</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Wang, S., Peng, J., Ma, J. &amp; Xu, J. Protein Secondary Structure Prediction Using Deep Convolutional Neural Fields. <italic>Sci. Reports</italic><bold>6</bold>, 10.1038/srep18962 (2016).</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heffernan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Capturing non-local interactions by long short-term memory bidirectional recurrent neural networks for improving prediction of protein secondary structure, backbone angles, contact numbers and solvent accessibility</article-title>
        <source>Bioinforma. (Oxford, England)</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>2842</fpage>
        <lpage>2849</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx218</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haas</surname>
            <given-names>Jürgen</given-names>
          </name>
          <name>
            <surname>Barbato</surname>
            <given-names>Alessandro</given-names>
          </name>
          <name>
            <surname>Behringer</surname>
            <given-names>Dario</given-names>
          </name>
          <name>
            <surname>Studer</surname>
            <given-names>Gabriel</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>Steven</given-names>
          </name>
          <name>
            <surname>Bertoni</surname>
            <given-names>Martino</given-names>
          </name>
          <name>
            <surname>Mostaguir</surname>
            <given-names>Khaled</given-names>
          </name>
          <name>
            <surname>Gumienny</surname>
            <given-names>Rafal</given-names>
          </name>
          <name>
            <surname>Schwede</surname>
            <given-names>Torsten</given-names>
          </name>
        </person-group>
        <article-title>Continuous Automated Model EvaluatiOn (CAMEO) complementing the critical assessment of structure prediction in CASP12</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>2017</year>
        <volume>86</volume>
        <fpage>387</fpage>
        <lpage>398</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25431</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schaarschmidt</surname>
            <given-names>Joerg</given-names>
          </name>
          <name>
            <surname>Monastyrskyy</surname>
            <given-names>Bohdan</given-names>
          </name>
          <name>
            <surname>Kryshtafovych</surname>
            <given-names>Andriy</given-names>
          </name>
          <name>
            <surname>Bonvin</surname>
            <given-names>Alexandre M.J.J.</given-names>
          </name>
        </person-group>
        <article-title>Assessment of contact predictions in CASP12: Co-evolution and deep learning coming of age</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>2017</year>
        <volume>86</volume>
        <fpage>51</fpage>
        <lpage>66</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25407</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Yang, Y. <italic>et al</italic>. Sixty-five years of the long march in protein secondary structure prediction: the final stretch? <italic>Briefings Bioinforma</italic>, 10.1093/bib/bbw129 (2016).</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Martin</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein secondary structure assignment revisited: a detailed analysis of different assignment methods</article-title>
        <source>BMC Struct. Biol.</source>
        <year>2005</year>
        <volume>5</volume>
        <fpage>17</fpage>
        <pub-id pub-id-type="doi">10.1186/1472-6807-5-17</pub-id>
        <?supplied-pmid 16164759?>
        <pub-id pub-id-type="pmid">16164759</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Conservation and prediction of solvent accessibility in protein families</article-title>
        <source>Proteins: Struct. Funct. Bioinforma.</source>
        <year>1994</year>
        <volume>20</volume>
        <fpage>216</fpage>
        <lpage>226</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.340200303</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altschul</surname>
            <given-names>SF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>
        <source>Nucleic Acids Res.</source>
        <year>1997</year>
        <volume>25</volume>
        <fpage>3389</fpage>
        <lpage>3402</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id>
        <?supplied-pmid 9254694?>
        <pub-id pub-id-type="pmid">9254694</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Remmert</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Biegert</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hauser</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</article-title>
        <source>Nat. Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <fpage>173</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1818</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mirabello</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Porter, PaleAle 4.0: high-accuracy prediction of protein secondary structure and relative solvent accessibility</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <fpage>2056</fpage>
        <lpage>2058</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt344</pub-id>
        <?supplied-pmid 23772049?>
        <pub-id pub-id-type="pmid">23772049</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Walsh</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Tosatto</surname>
            <given-names>SCE</given-names>
          </name>
        </person-group>
        <article-title>Correct machine learning on protein sequences: a peer-reviewing perspective</article-title>
        <source>Briefings Bioinforma.</source>
        <year>2016</year>
        <volume>17</volume>
        <fpage>831</fpage>
        <lpage>840</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbv082</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S. &amp; Sun, J. Deep Residual Learning for Image Recognition. <italic>arXiv:1512.03385</italic> [<italic>cs</italic>] ArXiv: 1512.03385 (2015).</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Ripley, B. D. <italic>Pattern recognition and neural networks</italic> (Cambridge University press, 1996).</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kabsch</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded and geometrical features</article-title>
        <source>Biopolymers</source>
        <year>1983</year>
        <volume>22</volume>
        <fpage>2577</fpage>
        <lpage>2637</lpage>
        <pub-id pub-id-type="doi">10.1002/bip.360221211</pub-id>
        <?supplied-pmid 6667333?>
        <pub-id pub-id-type="pmid">6667333</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>RaptorX-Property: a web server for protein structure property prediction</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2016</year>
        <volume>44</volume>
        <fpage>W430</fpage>
        <lpage>W435</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw306</pub-id>
        <?supplied-pmid 27112573?>
        <pub-id pub-id-type="pmid">27112573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Zemla, A., Venclovas, C., Fidelis, K. &amp; Rost, B. A modified definition of Sov, a segment-based measure for protein secondary structure prediction assessment. <italic>Proteins: Struct. Funct. Bioinforma</italic>. <bold>34</bold>, 220–223, 10.1002/(SICI)1097-0134(19990201)34:2&lt;220::AID-PROT7&gt;3.0.CO;2-K (1999).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>SOV_refine: A further refined definition of segment overlap score and its significance for protein structure similarity</article-title>
        <source>Source Code for Biol. Medicine</source>
        <year>2018</year>
        <volume>13</volume>
        <fpage>1</fpage>
        <pub-id pub-id-type="doi">10.1186/s13029-018-0068-7</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Klausen, M. S. <italic>et al</italic>. NetSurfP-2.0: improved prediction of protein structural features by integrated deep learning. <italic>bioRxiv</italic> 311209, 10.1101/311209 (2018).</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>MUFOLD-SS: New deep inception-inside-inception networks for protein secondary structure prediction</article-title>
        <source>Proteins: Struct. Funct. Bioinforma.</source>
        <year>2018</year>
        <volume>86</volume>
        <fpage>592</fpage>
        <lpage>598</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25487</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanson</surname>
            <given-names>Jack</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>Kuldip</given-names>
          </name>
          <name>
            <surname>Litfin</surname>
            <given-names>Thomas</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Yuedong</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Yaoqi</given-names>
          </name>
        </person-group>
        <article-title>Improving prediction of protein secondary structure, backbone angles, solvent accessibility and contact numbers by using predicted contact maps and an ensemble of recurrent and residual convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <issue>14</issue>
        <fpage>2403</fpage>
        <lpage>2410</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty1006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Drozdetskiy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cole</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Procter</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Barton</surname>
            <given-names>GJ</given-names>
          </name>
        </person-group>
        <article-title>JPred4: a protein secondary structure prediction server</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2015</year>
        <volume>43</volume>
        <fpage>W389</fpage>
        <lpage>W394</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv332</pub-id>
        <?supplied-pmid 25883141?>
        <pub-id pub-id-type="pmid">25883141</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fox</surname>
            <given-names>NK</given-names>
          </name>
          <name>
            <surname>Brenner</surname>
            <given-names>SE</given-names>
          </name>
          <name>
            <surname>Chandonia</surname>
            <given-names>J-M</given-names>
          </name>
        </person-group>
        <article-title>SCOPe: Structural Classification of Proteins—extended, integrating SCOP and ASTRAL data and classification of new structures</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2014</year>
        <volume>42</volume>
        <fpage>D304</fpage>
        <lpage>D309</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkt1240</pub-id>
        <?supplied-pmid 24304899?>
        <pub-id pub-id-type="pmid">24304899</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Cuff, J. A. &amp; Barton, G. J. Evaluation and improvement of multiple sequence methods for protein secondary structure prediction. <italic>Proteins: Struct. Funct. Bioinforma</italic>. <bold>34</bold>, 508–519 10.1002/(SICI)1097-0134(19990301)34:4&lt;508::AIDPROT10&gt;3.0.CO;2-4 (1999).</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klausen</surname>
            <given-names>MS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</article-title>
        <source>Proteins: Struct. Funct. Bioinforma.</source>
        <year>2019</year>
        <volume>87</volume>
        <fpage>520</fpage>
        <lpage>527</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25674</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Litfin</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Accurate prediction of protein contact maps by coupling residual two-dimensional bidirectional long short-term memory with convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <fpage>4039</fpage>
        <lpage>4045</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty481</pub-id>
        <?supplied-pmid 29931279?>
        <pub-id pub-id-type="pmid">29931279</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seemayer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gruber</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>CCMpred—fast and precise prediction of protein residue–residue contacts from correlated mutations</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <fpage>3128</fpage>
        <lpage>3130</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu500</pub-id>
        <?supplied-pmid 25064567?>
        <pub-id pub-id-type="pmid">25064567</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Morcos</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Direct-coupling analysis of residue coevolution captures native contacts across many protein families</article-title>
        <source>Proc. Natl. Acad. Sci.</source>
        <year>2011</year>
        <volume>108</volume>
        <fpage>E1293</fpage>
        <lpage>E1301</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1111471108</pub-id>
        <?supplied-pmid 22106262?>
        <pub-id pub-id-type="pmid">22106262</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Torrisi, M., Kaleel, M. &amp; Pollastri, G. Porter 5: fast, state-of-the-art ab initio prediction of protein secondary structure in 3 and 8 classes. <italic>bioRxiv</italic> 289033, 10.1101/289033 (2018).</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Combining evolutionary information and neural networks to predict protein secondary structure</article-title>
        <source>Proteins: Struct. Funct. Bioinforma.</source>
        <year>1994</year>
        <volume>19</volume>
        <fpage>55</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.340190108</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Swindells</surname>
            <given-names>MB</given-names>
          </name>
        </person-group>
        <article-title>Getting the most from PSI–BLAST</article-title>
        <source>Trends Biochem. Sci.</source>
        <year>2002</year>
        <volume>27</volume>
        <fpage>161</fpage>
        <lpage>164</lpage>
        <pub-id pub-id-type="doi">10.1016/S0968-0004(01)02039-4</pub-id>
        <?supplied-pmid 11893514?>
        <pub-id pub-id-type="pmid">11893514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schäffer</surname>
            <given-names>AA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improving the accuracy of PSI-BLAST protein database searches with composition-based statistics and other refinements</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2001</year>
        <volume>29</volume>
        <fpage>2994</fpage>
        <lpage>3005</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/29.14.2994</pub-id>
        <pub-id pub-id-type="pmid">11452024</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krogh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mitchison</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Maximum entropy weighting of aligned sequences of proteins or DNA</article-title>
        <source>Proceedings. Int. Conf. on Intell. Syst. for Mol. Biol.</source>
        <year>1995</year>
        <volume>3</volume>
        <fpage>215</fpage>
        <lpage>221</lpage>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Przybylski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and profiles</article-title>
        <source>Proteins: Struct. Funct. Bioinforma.</source>
        <year>2002</year>
        <volume>47</volume>
        <fpage>228</fpage>
        <lpage>235</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.10082</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hornik</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Approximation capabilities of multilayer feedforward networks</article-title>
        <source>Neural Networks</source>
        <year>1991</year>
        <volume>4</volume>
        <fpage>251</fpage>
        <lpage>257</lpage>
        <pub-id pub-id-type="doi">10.1016/0893-6080(91)90009-T</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baú</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Distill: a suite of web servers for the prediction of one-, two- and three-dimensional structural features of proteins</article-title>
        <source>BMC Bioinforma.</source>
        <year>2006</year>
        <volume>7</volume>
        <fpage>402</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-7-402</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mooney</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Beyond the Twilight Zone: Automated prediction of structural properties of proteins by recursive neural networks and remote homology information</article-title>
        <source>Proteins: Struct. Funct. Bioinforma.</source>
        <year>2009</year>
        <volume>77</volume>
        <fpage>181</fpage>
        <lpage>190</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.22429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pollastri</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Mooney</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vullo</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Accurate prediction of protein secondary structure and solvent accessibility by consensus combiners of sequence and structure information</article-title>
        <source>BMC Bioinforma.</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>201</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-8-201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <mixed-citation publication-type="other">Monteith, K., Carroll, J. L., Seppi, K. &amp; Martinez, T. Turning Bayesian model averaging into Bayesian model combination. In <italic>The 2011 International Joint Conference on Neural Networks</italic>, 2657–2663, 10.1109/IJCNN.2011.6033566 (2011).</mixed-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z-H</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Ensembling neural networks: Many could be better than all</article-title>
        <source>Artif. Intell.</source>
        <year>2002</year>
        <volume>137</volume>
        <fpage>239</fpage>
        <lpage>263</lpage>
        <pub-id pub-id-type="doi">10.1016/S0004-3702(02)00190-X</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Polyak</surname>
            <given-names>BT</given-names>
          </name>
        </person-group>
        <article-title>Some methods of speeding up the convergence of iteration methods</article-title>
        <source>USSR Comput. Math. Math. Phys.</source>
        <year>1964</year>
        <volume>4</volume>
        <fpage>1</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1016/0041-5553(64)90137-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robbins</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Monro</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A Stochastic Approximation Method</article-title>
        <source>The Annals Math. Stat.</source>
        <year>1951</year>
        <volume>22</volume>
        <fpage>400</fpage>
        <lpage>407</lpage>
        <pub-id pub-id-type="doi">10.1214/aoms/1177729586</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
