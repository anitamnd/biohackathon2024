<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Bioinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2673-7647</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9580858</article-id>
    <article-id pub-id-type="publisher-id">740078</article-id>
    <article-id pub-id-type="doi">10.3389/fbinf.2022.740078</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Bioinformatics</subject>
        <subj-group>
          <subject>Brief Research Report</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>LEVERSC: Cross-Platform Scriptable Multichannel 3-D Visualization for Fluorescence Microscopy Images</article-title>
      <alt-title alt-title-type="left-running-head">Winter and Cohen</alt-title>
      <alt-title alt-title-type="right-running-head">LEVERSC: Cross-Platform 3-D Visualization</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Winter</surname>
          <given-names>Mark</given-names>
        </name>
        <uri xlink:href="https://loop.frontiersin.org/people/1446812/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Cohen</surname>
          <given-names>Andrew R.</given-names>
        </name>
        <xref rid="c001" ref-type="corresp">*</xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1053598/overview"/>
      </contrib>
    </contrib-group>
    <aff><institution>Department of Computer Engineering</institution>, <institution>Drexel University</institution>, <addr-line>Philadelphia</addr-line>, <addr-line>PA</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p><bold>Edited by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/626780/overview" ext-link-type="uri">Kevin Eliceiri</ext-link>, University of Wisconsin-Madison, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p><bold>Reviewed by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/1415646/overview" ext-link-type="uri">Benjamin Schmid</ext-link>, University of Erlangen Nuremberg, Germany</p>
        <p><ext-link xlink:href="https://loop.frontiersin.org/people/1106971/overview" ext-link-type="uri">Christoph Sommer</ext-link>, Institute of Science and Technology Austria (IST Austria), Austria</p>
      </fn>
      <corresp id="c001">*Correspondence: Andrew R. Cohen, <email>andrew.r.cohen@drexel.edu</email>
</corresp>
      <fn fn-type="other">
        <p>This article was submitted to Computational BioImaging, a section of the journal Frontiers in Bioinformatics</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>17</day>
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>2</volume>
    <elocation-id>740078</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>28</day>
        <month>1</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Winter and Cohen.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Winter and Cohen</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>We describe a new open-source program called LEVERSC to address the challenges of visualizing the multi-channel 3-D images prevalent in biological microscopy. LEVERSC uses a custom WebGL hardware-accelerated raycasting engine unique in its combination of rendering quality and performance, particularly for multi-channel data. Key features include platform independence, quantitative visualization through interactive voxel localization, and reproducible dynamic visualization <italic>via</italic> the scripting interface. LEVERSC is fully scriptable and interactive, and works with MATLAB, Python and Java/ImageJ.</p>
    </abstract>
    <kwd-group>
      <kwd>visualization</kwd>
      <kwd>microscopy</kwd>
      <kwd>light</kwd>
      <kwd>computational analysis</kwd>
      <kwd>biological microscopy</kwd>
      <kwd>scripted rendering</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>Human Frontier Science Program
</institution>
            <institution-id institution-id-type="doi">10.13039/100004412</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn002">RGP0043/2019</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Institute on Aging
</institution>
            <institution-id institution-id-type="doi">10.13039/100000049</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>Introduction</title>
    <p>Image analysis pipelines for 3-D fluorescence microscopy generally include image capture, image processing for object detection, tracking for time-lapse, object classification and ultimately statistical analysis of the extracted objects (<xref rid="B15" ref-type="bibr">Wait, 2014</xref>; <xref rid="B17" ref-type="bibr">Winter et al., 2016</xref>). Visualization at each of these stages is required to establish trust that the processing pipeline is capturing a true and meaningful summary of the data (<xref rid="B1" ref-type="bibr">Cohen, 2014</xref>). Three key requirements for effective visualization of 3-D multichannel images include 1) platform independence, 2) quantitative visualization, and 3) reproducible dynamic visualization. No existing image visualization tool satisfies this combination of requirements (<xref rid="B13" ref-type="bibr">Schroeder et al., 2006</xref>; <xref rid="B18" ref-type="bibr">Yushkevich et al., 2006</xref>; <xref rid="B8" ref-type="bibr">Meyer-Spradow et al., 2009</xref>; <xref rid="B2" ref-type="bibr">de Chaumont et al., 2012</xref>; <xref rid="B11" ref-type="bibr">Royer et al., 2015</xref>; <xref rid="B4" ref-type="bibr">Fantham and Kaminski, 2017</xref>; <xref rid="B6" ref-type="bibr">Gunther et al., 2019</xref>; <xref rid="B9" ref-type="bibr">O’Shaughnessy et al., 2019</xref>; <xref rid="B12" ref-type="bibr">Schmid et al., 2019</xref>; <xref rid="B7" ref-type="bibr">Jonsson et al., 2020</xref>; <xref rid="B10" ref-type="bibr">Pettersen et al., 2021</xref>; <xref rid="B14" ref-type="bibr">Napari Contributors, 2019</xref>). The LEVERSC visualization tool presented here was developed specifically to satisfy these requirements.</p>
    <p>Platform independence means supporting the operating systems and common processing tools used for image analysis. Operating systems include Mac OS, Windows and Linux. Operating system independence for 3-D visualization is complicated by platform-specific support for hardware acceleration. For example, Mac OS has recently deprecated the OpenGL and OpenCL libraries that are widely used for 3-D visualization (<xref rid="B13" ref-type="bibr">Schroeder et al., 2006</xref>; <xref rid="B11" ref-type="bibr">Royer et al., 2015</xref>; <xref rid="B14" ref-type="bibr">Napari Contributors, 2019</xref>). Widely used tools for working with multi-channel 3-D fluorescence microscopy images include ImageJ, Python, and MATLAB as well as Knime and Julia. The goal of LEVERSC is to provide simple integration with any extensible environment, and to be used as easily as existing 2-D visualization on each platform. Because it is integrated in standard tools, LEVERSC works alongside fast image processing libraries such as the Hydra image processing library to provide intuitive visual feedback when designing image processing pipelines (<xref rid="B16" ref-type="bibr">Wait et al., 2019</xref>). LEVERSC is built using WebGL for 3-D rendering and NodeJS for the backend, making it lightweight, portable, and future proof.</p>
    <p>Quantitative visualization means that the coordinates of any voxel on any channel can be precisely located. This is the location of the voxel within the raw image stack. High-quality 3-D visualization uses perspective projections that alter the spatial characteristics of the rectangular image volume to emulate the perspective vanishing point for 3-D structures on a 2-D screen. Quantitative visualization requires the ability to specify a 3-D location when viewing on a 2-D screen. Quantitative visualization is implemented in LEVERSC with the use of a view-aligned sampling plane. <xref rid="F1" ref-type="fig">Figure 1</xref> shows an example image showing nuclei of MCF10A cells labelled with H2B (histone 2B) in a 3-D spheroid (<xref rid="B3" ref-type="bibr">Ender et al., 2021</xref>; <xref rid="B5" ref-type="bibr">Gagliardi et al., 2021</xref>). The Laplacian of Gaussian filter is commonly used for nuclei detection (<xref rid="B16" ref-type="bibr">Wait et al., 2019</xref>). The positive and negative filter responses are extracted to separate image channels, and the volume is visualized (<xref rid="F1" ref-type="fig">Figure 1</xref>, left). The sampling plane in the left panel is visible as a yellow grid, with shading of the volume changing behind the plane. The sampling plane can be set at arbitrary depth relative to the current view orientation. The sampling plane can be shown within the full volume (<xref rid="F1" ref-type="fig">Figure 1</xref>, left), it can show only data behind it (clipped view), or it can show only data that intersects it (slice view), capturing a single slice through the volume (<xref rid="F1" ref-type="fig">Figure 1</xref>, middle). The sampling plane, in combination with the mouse location, is used to generate the 3-D coordinate of a voxel location in the raw image stack (<xref rid="F1" ref-type="fig">Figure 1</xref>, middle) that can then be viewed using a single channel 2-D image viewer from any processing environment (<xref rid="F1" ref-type="fig">Figure 1</xref>, right).</p>
    <fig position="float" id="F1">
      <label>FIGURE 1</label>
      <caption>
        <p>LEVERSC makes quantitative visualization at each step of the image processing pipeline as easy for 3-D multi-channel images as it is for 2-D images. The realistic rendering is enabled by a real-time anisotropic raycasting with perspective projection. The raw image and two computed images are visualized as a full stack (left), or as a slice at arbitrary orientation through the volume (center). Quantitative visualization recovers the precise 3-D voxel location, shown here with a 2-D single channel image view at <italic>Z</italic> location 25 (right).</p>
      </caption>
      <graphic xlink:href="fbinf-02-740078-g001" position="float"/>
    </fig>
    <p>Reproducible dynamic visualization means that all view parameters can be read into a scripting environment and written back into the visualization program. This allows view settings to be saved for exact reproducibility. This also allows the visualization to be animated. Image capture of the rendered image to the scripting environment is also critical so that animations can be saved as movies. <xref rid="s10" ref-type="sec">Supplementary Movie S1</xref> is an animation generated from the sample H2B image shown in <xref rid="F1" ref-type="fig">Figure 1</xref>, generated as described in the online methods section.</p>
    <p>Existing 3-D tools provide a variety of options for visualization of 3-D biological data. Visualization libraries such as the Visualization Toolkit (VTK) are fast and provide significant flexibility in visualization, but they require users to write a significant amount of custom code in <italic>each</italic> of their binding languages, such as Python or JAVA in order to provide interactive volumetric visualization (<xref rid="B13" ref-type="bibr">Schroeder et al., 2006</xref>). Tools such as Voreen and Inviwo are very flexible 3-D visualization prototyping tools, providing a platform for experimenting with different types of rendering and building volumetric renderers (<xref rid="B8" ref-type="bibr">Meyer-Spradow et al., 2009</xref>; <xref rid="B7" ref-type="bibr">Jonsson et al., 2020</xref>). These tools are excellent for technical users, but are not designed for quickly visualizing 3-D image data from multiple image processing environments. Other packages such as Icy, ChimeraX, itkSNAP and ImageTank, are built as standalone tools or tool ecosystems containing image-processing operations and visualization. However, each package limits users to only the operations available within the tool or requires that they write a tool extension or plugin for their purposes or export results for import into another tool for further processing or visualization (<xref rid="B18" ref-type="bibr">Yushkevich et al., 2006</xref>; <xref rid="B2" ref-type="bibr">de Chaumont et al., 2012</xref>; <xref rid="B9" ref-type="bibr">O’Shaughnessy et al., 2019</xref>; <xref rid="B10" ref-type="bibr">Pettersen et al., 2021</xref>). Powerful visualization tools such as ClearVolume, SciView, and FPBioimage, use related architecture components to LEVERSC, but focus largely on ImageJ/FIJI support (<xref rid="B11" ref-type="bibr">Royer et al., 2015</xref>; <xref rid="B4" ref-type="bibr">Fantham and Kaminski, 2017</xref>; <xref rid="B6" ref-type="bibr">Gunther et al., 2019</xref>). Similarly, 3DScript is an excellent ImageJ plugin for scripting movies using English commands, but does not support movie making in other image processing environments like Python or MATLAB (<xref rid="B12" ref-type="bibr">Schmid et al., 2019</xref>).</p>
    <p>LEVERSC was developed as part of a collection of software tools for analyzing live-cell microscopy images, called LEVERJS (short for Lineage Editing and Validation) (<xref rid="B17" ref-type="bibr">Winter et al., 2016</xref>). The design of LEVERSC was inspired by the imagesc command in MATLAB, which can be used to quickly display a 2-D array as an image, by automatically mapping the full range of the array to a color palette. The ease of use of this command during image processing tasks makes it a powerful debugging tool. LEVERSC strives to provide a similar ease of use and consistency, from multiple image processing environments, while also providing quantitative imaging and scripting controls. The LEVERSC architecture was also designed to be as simple as possible to integrate with a request application programming interface (API) that should be supported by any scripting language or processing environment.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>Methods</title>
    <sec id="s2-1">
      <title>Installation</title>
      <p>Both the leversc app and at least one client integration must be installed. First, install the LEVERSC app for the desired operating system. Next, follow the instructions for integrating LEVERSC into the client of choice (ImageJ, Python, or MATLAB).</p>
    </sec>
    <sec id="s2-2">
      <title>App Installation</title>
      <sec id="s2-2-1">
        <title>MacOS App Installation</title>
        <p>
          <list list-type="simple">
            <list-item>
              <p>• Download and run the MacOS installer using the link at <ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc#macos-app-install</ext-link>
</p>
            </list-item>
          </list>
        </p>
      </sec>
      <sec id="s2-2-2">
        <title>Windows App Installation</title>
        <p>
          <list list-type="simple">
            <list-item>
              <p>• Download and run the Windows installer using the link at <ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc#windows-app-install</ext-link>
</p>
            </list-item>
          </list>
        </p>
      </sec>
      <sec id="s2-2-3">
        <title>Linux App Manual Installation</title>
        <p>
          <list list-type="simple">
            <list-item>
              <p>• Download the Linux Appimage using the link at <ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc#linux-app-manual-install</ext-link>
</p>
            </list-item>
          </list>
        </p>
        <p>• Symlink the appimage file to a folder in $PATH (e.g., ∼/.local/bin).</p>
        <p>ln -fs /path/to/leverjs-*.AppImage ∼/.local/bin/leverjs</p>
      </sec>
    </sec>
    <sec id="s2-3">
      <title>Client Integration</title>
      <sec id="s2-3-1">
        <title>ImageJ Plugin</title>
        <p>
          <list list-type="simple">
            <list-item>
              <p>1. First, install the leversc app (see previous section).</p>
            </list-item>
            <list-item>
              <p>2. Download the ImageJ plugin using the link at <ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc#imagej-plugin</ext-link>.</p>
            </list-item>
            <list-item>
              <p>3. Copy the plugin jar file to the <italic>plugins/3D</italic> folder in your ImageJ executable directory.</p>
            </list-item>
            <list-item>
              <p>4. The plugin will appear in the ImageJ Plugins menu as <italic>Plugins-&gt;3D-&gt;Leversc Viewer</italic>
</p>
            </list-item>
          </list>
        </p>
        <p>Note: If using Fiji (highly recommended) then you may wish to create the 3D subfolder in plugins and place the jar file within, as it can make it easier to find. Alternatively, the jar file can be placed directly in the plugins folder and will be listed in the Fiji menu as Plugins-&gt;Leversc Viewer.</p>
        <p>Note: There is currently no scripting support available from within Fiji/ImageJ. In order to control the viewer programmatically you need to use MATLAB or Python as described below.</p>
      </sec>
      <sec id="s2-3-2">
        <title>Python Module–Requires Python 3.6 or Later</title>
        <p>
          <list list-type="simple">
            <list-item>
              <p>1. First, install the leversc app (see previous section).</p>
            </list-item>
            <list-item>
              <p>2. From the command line:</p>
              <list list-type="simple">
                <list-item>
                  <p>1. (Windows) ‘py -m pip install leversc’</p>
                </list-item>
                <list-item>
                  <p>2. (Mac/Linux) ‘python3 -m pip install leversc’</p>
                </list-item>
                <list-item>
                  <p>3. To validate install, start python, then:</p>
                </list-item>
                <list-item>
                  <p>&gt;&gt;&gt; import leversc; leversc.test_random()</p>
                </list-item>
              </list>
            </list-item>
            <list-item>
              <p>3. For the full sample code, download the LEVERSC source directory (<ext-link xlink:href="https://git-bioimage.coe.drexel.edu/opensource/leversc" ext-link-type="uri">https://git-bioimage.coe.drexel.edu/opensource/leversc</ext-link>).</p>
            </list-item>
          </list>
        </p>
      </sec>
      <sec id="s2-3-3">
        <title>MATLAB Class–Requires MATLAB 2019B or Later</title>
        <p>
          <list list-type="simple">
            <list-item>
              <p>1. First, install the leversc app (see previous section).</p>
            </list-item>
            <list-item>
              <p>2. Download the LEVERSC source directory (<ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc</ext-link>).</p>
            </list-item>
            <list-item>
              <p>3. Extract the downloaded source and support folders to a convenient location.</p>
            </list-item>
            <list-item>
              <p>4. Add the <italic>src/MATLAB</italic> subfolder to your MATLAB path, for example, by adding the statement <italic>addpath('path/to/extracted/folder')</italic> to your <italic>startup.m</italic> file.</p>
            </list-item>
          </list>
        </p>
        <p>Additional installation details can be found at: <ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc</ext-link>.</p>
      </sec>
    </sec>
    <sec id="s2-4">
      <title>Architecture</title>
      <p>The LEVERSC visualization tool is a NodeJS application for visualization of multichannel 3-D volumetric data using WebGL. LEVERSC uses a local HTTP server port binding to communicate with image processing tools. Currently LEVERSC has plugins for ImageJ, Python and MATLAB. Additional plugins for KNIME and Julia are planned.</p>
      <p>The LEVERSC HTTP request application programming interface (API) is designed to be simple to implement, so that image processing tools or scripting languages can be quickly extended to communicate with the LEVERSC viewer. The API can be implemented piecemeal, only requiring the image send request to be implemented in the simplest case. Additional API requests that control the visualization and allow scripted movie-making, can be implemented, but are not required for data visualization.</p>
      <p>This architecture is very flexible and supports fast, cross-platform communication between any environment that supports HTTP POST/GET requests. A detailed listing of all API requests is available in the online documentation linked from the main LEVERSC repository readme at: <ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc</ext-link>.</p>
    </sec>
    <sec id="s2-5">
      <title>Raycast Renderer</title>
      <p>LEVERSC uses a raycast sampler implemented in a WebGL fragment shader. For each pixel in the display a view ray is cast outward through the image volume. The image voxel values are sampled uniformly along the ray corresponding with the size of image voxels. At each sample a user-defined transfer function, detailed in the subsequent section, is applied to convert from normalized voxel values to emissive intensities and opacity. As sampling continues along each ray, color intensity and opacity (alpha) are accumulated using standard alpha blending calculations. Once all ray sampling is completed, the blended color and opacity are composited with the render background to draw the final rendered frame to the screen WebGL window.</p>
      <p>The LEVERSC visualization tool also supports three clip modes: off, front, and plane. These clipping modes control the rendering at and behind the user-controlled view-aligned clipping plane. With clipping turned off, the full depth of the volume is always rendered. Front clipping will render only the portion of the volume behind the clipping plane. Plane clipping will render only the image data that directly intersects the clipping plane, allowing users to view image slices at arbitrary orientation. The 3-D image coordinates of the mouse pointer projected onto the clipping plane is always displayed in the bottom left of the LEVERSC window so that users can easily determine exact voxel coordinates when using LEVERSC to design or debug image processing algorithms.</p>
    </sec>
    <sec id="s2-6">
      <title>Transfer Function</title>
      <p>The transfer function is a per-channel parameterized function that maps voxel values (normalized on [0,1]) to emissive intensity values (on [0,1]) for modeling light-transport in the ray caster. The function is a monotonic quadratic. In the user interface, the function is determined by 3 sliders, dark, medium, and bright. The “dark” slider defines the largest voxel value that is completely transparent, all values at or below the “dark” level are mapped to 0 and are fully transparent. The “bright” slider defines the smallest voxel value that is fully emissive and opaque, all values at “bright” or above are fully opaque and emissive. The “medium” slider is the output value in [0,1] of the midpoint value between “dark” and “bright”, in essence defining the curvature of the quadratic, if “medium” is larger than 0.5 the quadratic will be concave down, if “medium” is less than 0.5 the quadratic will be concave up, and if “medium” is 0.5 then the function will be linear. Below the transfer function equation and parameters is shown for a single channel, mapping from the input voxel intensity <inline-formula id="inf1"><mml:math id="m1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to output intensity <inline-formula id="inf2"><mml:math id="m2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>,<disp-formula id="equ1"><mml:math id="m3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>a</mml:mi><mml:msubsup><mml:mi>i</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>The parameters <inline-formula id="inf3"><mml:math id="m4" overflow="scroll"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> are chosen such that <inline-formula id="inf4"><mml:math id="m5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula id="inf5"><mml:math id="m6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="inf6"><mml:math id="m7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> when <inline-formula id="inf7"><mml:math id="m8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. The parameters are also constrained such that <inline-formula id="inf8"><mml:math id="m9" overflow="scroll"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> increases monotonically over the input range. The transfer function mapping, with per-channel parameters, is implemented in the WebGL raycasting fragment shader. This allows the user to adjust the transfer function settings and have the visualization update in real-time. Each channel also has a globally adjustable opacity (alpha). Once a ray is fully opaque or the ray has reached a far edge of the volume data, the total accumulated color is rendered to the screen, providing real-time volumetric rendering.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <p>In this section we will provide a detailed example of usage of the LEVERSC tool. This example shows the use of many of the LEVERSC application programming interface commands to create a high-quality rendered movie using the MATLAB scripting language. The full example script source code (<italic>sampleVolumeMovie.m</italic>) is available in the <italic>src/MATLAB</italic> directory of the LEVERSC repository. <xref rid="F2" ref-type="fig">Figure 2</xref> details the rendering interface used to control and indicate image coloring, alpha values and the intensity mapping transfer functions. The MP4 movie resulting from following this example script is included as <xref rid="s10" ref-type="sec">Supplementary Movie S1</xref>.</p>
    <fig position="float" id="F2">
      <label>FIGURE 2</label>
      <caption>
        <p>LEVERSC render parameter selection interface. Colors, alpha blending and intensity mapping transfer functions are all fully controllable <italic>via</italic> the user interface and the API.</p>
      </caption>
      <graphic xlink:href="fbinf-02-740078-g002" position="float"/>
    </fig>
    <p>We begin by loading some image data, in this case from the sample.LEVER file distributed along with the LEVERSC repository. Any format image readable by the scripting environment can be substituted.</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx1.jpg"/>
    </p>
    <p>A leversc class object must be initialized, here we initialize the leversc class with image data and metadata (metadata fields such as PixelPhysicalSize are important for correct data visualization).</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx2.jpg"/>
    </p>
    <p>A reproducible movie render should set the rendering parameters consistently at the beginning of rendering to properly visualize the data. In this case we use the LEVERSC tool interface to interactively identify good visualization values, then use the /renderParams API call to read the current settings. The rendering parameters are set interactively <italic>via</italic> the UI as shown in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p>
    <p>Once the rendering parameters are set, they are read back into MATLAB and hardcoded for subsequent reuse: <inline-graphic xlink:href="fbinf-02-740078-fx3.jpg"/>
</p>
    <p>We disable the display of most UI elements</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx4.jpg"/>
    </p>
    <p>We reset the view parameters to defaults for the start of the movie:</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx5.jpg"/>
    </p>
    <p>The first step in this movie is to apply a quick animated zoom to fill the display with the actual data in the volume, capturing frames for each zoom level. Since our movie will run at 10 frames per second (fps) we interpolate our zoom over 10 frames (a 1 s zoom):</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx6.jpg"/>
    </p>
    <p>Apply a 5 s (50 frame) rotation of 180 degrees about the y-axis:</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx7.jpg"/>
    </p>
    <p>Move the sampling plane to the edge of the volume and turn on planar clipping. Then we animate moving the plane to just a little back from the volume center. The plane animation is 2 s (20 frames) long:</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx8.jpg"/>
    </p>
    <p>Apply another 180-degree rotation to rotate the volume the rest of the way back to the starting view. This time we show matrix multiplication for the world rotation matrix by a delta rotation matrix:</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx9.jpg"/>
    </p>
    <p>As a final animation, we change the plane clipping mode to slice sampling, and animate moving out of the volume towards the camera:</p>
    <p>
      <inline-graphic xlink:href="fbinf-02-740078-fx10.jpg"/>
    </p>
    <p>Using the example shown here, as well as the full viewParams and renderParams API calls, complex camera and movie effects can be built through interpolation of multiple parameters at each frame. <xref rid="s10" ref-type="sec">Supplementary Movie S1</xref> shows the full results of running the code fragments above.</p>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>Effective analysis of complex anisotropic image data requires effective visualization at every step of the processing pipeline. Visualization needs to be available from any platform, operating system and processing environment. It should be as easy to use as inbuilt 2-D image visualization. Visualization also needs to be quantitative–able to precisely identify the image stack location of every voxel at any view orientation and scale. Finally, the visualization needs to be fully controllable from any scripting environment, allowing view settings to be recorded for subsequent reproducibility, and view settings to be played back for generating movies. The LEVERSC visualization tool described here addresses all these requirements.</p>
    <p>LEVERSC is designed to be useful to a broad audience of microscopists, biologists, and image analysists. However, the focus on compatibility and fast, interactive rendering does come with trade-offs. WebGL provides a highly compatible rendering interface with built-in support for fast 3-D image sampling. However, the use of a WebGL GPU texture sampler requires that the image volume be quantized to 8-bits per channel. This also requires that the entire image volume fit in GPU memory, and the LEVERSC tool cannot display images larger than the GPU memory available. However, users can still quickly crop a region of interest from a large volume to be visualized in LEVERSC, or they may downsample the volume in their image processing environment to view an approximation of the volume. We are also investigating multiresolution progressive rendering support in LEVERSC. This is already supported in the LEVERJS microscope analysis tool, but that rendering requires preprocessing and use of a specialized image format. While it would also be ideal to sample and display images at their full bit depth, we believe the speed of GPU textures is worth this sacrifice, particularly since most monitor displays generally have bit depth of 8–10-bits per color, and the current LEVERSC plugins use linear normalization to minimize the quantization error.</p>
    <p>The architecture of LEVERSC makes it highly platform independent and compatible with other tools since it runs as a separate NodeJS process using standard HTTP requests for communication. However, this does increase the memory requirements for visualization as the image volume must be sent (copied) from the originating process to the LEVERSC visualization process. We have not found this to be an issue with modern hardware and modern live-cell microscope images, but could certainly be a concern with large electron microscopy (EM) data.</p>
    <p>LEVERSC is a lightweight high-performance tool that provides a high-quality rendering <italic>via</italic> true raycasting using per voxel compositing across all image channels. It runs on all three major operating systems and is usable from ImageJ, Python and MATLAB. LEVERSC will support Knime using the ImageJ plugin architecture, with support for the Julia scripting language coming soon. The LEVERSC sampling plane enables quantitative visualization by projecting 2-D pointer location, together with the arbitrary plane location into image coordinates. LEVERSC is fully scriptable, providing image capture as well as programmatically controllable view, render and user interface settings. LEVERSC is available free and open source (MIT license). Source code for the scripting client and download links for the visualization executable can be found at <ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc</ext-link>.</p>
  </sec>
</body>
<back>
  <ack>
    <p>The authors thank Olivier Pertz and Pascal Ender for providing the sample image data.</p>
  </ack>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The sample data provided in this study is included in the source repository, found at: <ext-link xlink:href="https://leverjs.net/leversc" ext-link-type="uri">https://leverjs.net/leversc</ext-link>.</p>
  </sec>
  <sec id="s6">
    <title>Author Contributions</title>
    <p>MW and AC both contributed to the software development and to the writing of the manuscript.</p>
  </sec>
  <sec id="s7">
    <title>Funding</title>
    <p>Portions of this research were supported by the National Institute On Aging of the National Institutes of Health under award number R01AG041861 and by the Human Frontiers Science Program Grant RGP0043/2019.</p>
  </sec>
  <sec sec-type="COI-statement" id="s8">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s9">
    <title>Publisher’s Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec id="s10">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fbinf.2022.740078/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fbinf.2022.740078/full#supplementary-material</ext-link>
</p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Video1.MP4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>A. R.</given-names></name></person-group> (<year>2014</year>). <article-title>Extracting Meaning from Biological Imaging Data</article-title>. <source>Mol. Biol. Cel</source>
<volume>25</volume>, <fpage>3470</fpage>–<lpage>3473</lpage>. <pub-id pub-id-type="doi">10.1091/mbc.E14-04-0946</pub-id>
</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Chaumont</surname><given-names>F.</given-names></name><name><surname>Dallongeville</surname><given-names>S.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Chenouard</surname><given-names>N.</given-names></name><name><surname>Hervé</surname><given-names>N.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Pop</surname><given-names>S.</given-names></name></person-group>, <person-group person-group-type="author"><name><surname>Provoost</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2012</year>). <article-title>Icy: an Open Bioimage Informatics Platform for Extended Reproducible Research</article-title>. <source>Nat. Methods</source>
<volume>9</volume>, <fpage>690</fpage>–<lpage>696</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2075</pub-id>
<pub-id pub-id-type="pmid">22743774</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ender</surname><given-names>P.</given-names></name><name><surname>Gagliardi</surname><given-names>P. A.</given-names></name><name><surname>Dobrzyński</surname><given-names>M.</given-names></name><name><surname>Dessauges</surname><given-names>C.</given-names></name><name><surname>Höhener</surname><given-names>T.</given-names></name><name><surname>Jacques</surname><given-names>M.-A.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Spatio-temporal Control of ERK Pulse Frequency Coordinates Fate Decisions during Mammary Acinar Morphogenesis</article-title>. <source>In review and bioRxi</source>. <pub-id pub-id-type="doi">10.1101/2020.11.20.387167</pub-id>
</mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fantham</surname><given-names>M.</given-names></name><name><surname>Kaminski</surname><given-names>C. F.</given-names></name></person-group> (<year>2017</year>). <article-title>A New Online Tool for Visualization of Volumetric Data</article-title>. <source>Nat. Photon.</source>
<volume>11</volume>, <fpage>69</fpage>. <pub-id pub-id-type="doi">10.1038/nphoton.2016.273</pub-id>
</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagliardi</surname><given-names>P. A.</given-names></name><name><surname>Dobrzyński</surname><given-names>M.</given-names></name><name><surname>Jacques</surname><given-names>M.-A.</given-names></name><name><surname>Dessauges</surname><given-names>C.</given-names></name><name><surname>Ender</surname><given-names>P.</given-names></name><name><surname>Blum</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>Collective ERK/Akt Activity Waves Orchestrate Epithelial Homeostasis by Driving Apoptosis-Induced Survival</article-title>. <source>Dev. Cel.</source>
<volume>56</volume> (<issue>12</issue>), <fpage>1712</fpage>–<lpage>1726</lpage>. <pub-id pub-id-type="doi">10.1016/j.devcel.2021.05.007</pub-id>
<comment>In Press</comment>
</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gunther</surname><given-names>U.</given-names></name><name><surname>Pietzsch</surname><given-names>T.</given-names></name><name><surname>Gupta</surname><given-names>A.</given-names></name><name><surname>Harrington</surname><given-names>K. I. S.</given-names></name><name><surname>Tomancak</surname><given-names>P.</given-names></name><name><surname>Gumhold</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2019</year>). “<article-title>Scenery: Flexible Virtual Reality Visualization on the Java VM</article-title>”. In <conf-name>2019 IEEE Visualization Conference (VIS) 1–5</conf-name>, <conf-loc>Vancouver, BC, Canada</conf-loc>, <conf-date>20-25 Oct. 2019</conf-date>. <publisher-name>IEEE</publisher-name>. <pub-id pub-id-type="doi">10.1109/VISUAL.2019.8933605</pub-id>
</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jonsson</surname><given-names>D.</given-names></name><name><surname>Steneteg</surname><given-names>P.</given-names></name><name><surname>Sunden</surname><given-names>E.</given-names></name><name><surname>Englund</surname><given-names>R.</given-names></name><name><surname>Kottravel</surname><given-names>S.</given-names></name><name><surname>Falk</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Inviwo — A Visualization System with Usage Abstraction Levels</article-title>. <source>IEEE Trans. Vis. Comput. Graph.</source>
<volume>26</volume>, <fpage>3241</fpage>–<lpage>3254</lpage>. <pub-id pub-id-type="doi">10.1109/TVCG.2019.2920639</pub-id>
<pub-id pub-id-type="pmid">31180858</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer-Spradow</surname><given-names>J.</given-names></name><name><surname>Ropinski</surname><given-names>T.</given-names></name><name><surname>Mensmann</surname><given-names>J.</given-names></name><name><surname>Hinrichs</surname><given-names>K. V.</given-names></name></person-group> (<year>2009</year>). <article-title>A Rapid-Prototyping Environment for Ray-Casting-Based Volume Visualizations</article-title>. <source>IEEE Comput. Graph. Appl.</source>
<volume>29</volume>, <fpage>6</fpage>–<lpage>13</lpage>. <pub-id pub-id-type="doi">10.1109/MCG.2009.130</pub-id>
</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><collab>Napari Contributors</collab> (<year>2019</year>).<article-title> Napari: A Multi-Dimensional Image Viewer for Python</article-title>. <pub-id pub-id-type="doi">10.5281/zenodo.3555620</pub-id>
</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Shaughnessy</surname><given-names>E. C.</given-names></name><name><surname>Stone</surname><given-names>O. J.</given-names></name><name><surname>LaFosse</surname><given-names>P. K.</given-names></name><name><surname>Azoitei</surname><given-names>M. L.</given-names></name><name><surname>Tsygankov</surname><given-names>D.</given-names></name><name><surname>Heddleston</surname><given-names>J. M.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Software for Lattice Light-Sheet Imaging of FRET Biosensors, Illustrated with a New Rap1 Biosensor</article-title>. <source>J. Cel Biol.</source>
<volume>218</volume>, <fpage>3153</fpage>–<lpage>3160</lpage>. <pub-id pub-id-type="doi">10.1083/jcb.201903019</pub-id>
</mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pettersen</surname><given-names>E. F.</given-names></name><name><surname>Goddard</surname><given-names>T. D.</given-names></name><name><surname>Huang</surname><given-names>C. C.</given-names></name><name><surname>Meng</surname><given-names>E. C.</given-names></name><name><surname>Couch</surname><given-names>G. S.</given-names></name><name><surname>Croll</surname><given-names>T. I.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>UCSF ChimeraX: Structure Visualization for Researchers, Educators, and Developers</article-title>. <source>Protein Sci. Publ. Protein Soc.</source>
<volume>30</volume>, <fpage>70</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1002/pro.3943</pub-id>
</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royer</surname><given-names>L. A.</given-names></name><name><surname>Weigert</surname><given-names>M.</given-names></name><name><surname>Günther</surname><given-names>U.</given-names></name><name><surname>Maghelli</surname><given-names>N.</given-names></name><name><surname>Jug</surname><given-names>F.</given-names></name><name><surname>Sbalzarini</surname><given-names>I. F.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>ClearVolume: Open-Source Live 3D Visualization for Light-Sheet Microscopy</article-title>. <source>Nat. Methods</source>
<volume>12</volume>, <fpage>480</fpage>–<lpage>481</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3372</pub-id>
<pub-id pub-id-type="pmid">26020498</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmid</surname><given-names>B.</given-names></name><name><surname>Tripal</surname><given-names>P.</given-names></name><name><surname>Fraaß</surname><given-names>T.</given-names></name><name><surname>Kersten</surname><given-names>C.</given-names></name><name><surname>Ruder</surname><given-names>B.</given-names></name><name><surname>Grüneboom</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>3Dscript: Animating 3D/4D Microscopy Data Using a Natural-Language-Based Syntax</article-title>. <source>Nat. Methods</source>
<volume>16</volume>, <fpage>278</fpage>–<lpage>280</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0359-1</pub-id>
<pub-id pub-id-type="pmid">30886414</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>W.</given-names></name><name><surname>Martin</surname><given-names>K.</given-names></name><name><surname>Lorensen</surname><given-names>B.</given-names></name></person-group> (<year>2006</year>). <source>The Visualization Toolkit: an Object-Oriented Approach to 3D Graphics ; [visualize Data in 3D - Medical, Engineering or Scientific ; Build Your Own Applications with C++, Tcl, Java or Python ; Includes Source Code for VTK (Supports Unix, Windows and Mac)]</source>. <publisher-loc>Clifton Park, NY</publisher-loc>: <publisher-name>Kitware, Inc</publisher-name>. </mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wait</surname><given-names>E.</given-names></name></person-group> (<year>2014</year>). <article-title>Visualization and Correction of Automated Segmentation, Tracking and Lineaging from 5-D Stem Cell Image Sequences</article-title>. <source>BMC Bioinformatics</source>
<volume>15</volume>, <fpage>328</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-15-328</pub-id>
<pub-id pub-id-type="pmid">25281197</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wait</surname><given-names>E.</given-names></name><name><surname>Winter</surname><given-names>M.</given-names></name><name><surname>Cohen</surname><given-names>A. R.</given-names></name></person-group> (<year>2019</year>). <article-title>Hydra Image Processor: 5-D GPU Image Analysis Library with MATLAB and Python Wrappers</article-title>. <source>Bioinforma. Oxf. Engl.</source>
<volume>35</volume> (<issue>24</issue>), <fpage>5393</fpage>–<lpage>5395</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btz523</pub-id>
</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winter</surname><given-names>M.</given-names></name><name><surname>Mankowski</surname><given-names>W.</given-names></name><name><surname>Wait</surname><given-names>E.</given-names></name><name><surname>Temple</surname><given-names>S.</given-names></name><name><surname>Cohen</surname><given-names>A. R.</given-names></name></person-group> (<year>2016</year>). <article-title>LEVER: Software Tools for Segmentation, Tracking and Lineaging of Proliferating Cells</article-title>. <source>Bioinformatics</source>
<volume>32</volume>, <fpage>3530</fpage>–<lpage>3531</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btw406</pub-id>
<pub-id pub-id-type="pmid">27423896</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yushkevich</surname><given-names>P. A.</given-names></name><name><surname>Piven</surname><given-names>J.</given-names></name><name><surname>Hazlett</surname><given-names>H. C.</given-names></name><name><surname>Smith</surname><given-names>R. G.</given-names></name><name><surname>Ho</surname><given-names>S.</given-names></name><name><surname>Gee</surname><given-names>J. C.</given-names></name><etal/></person-group> (<year>2006</year>). <article-title>User-guided 3D Active Contour Segmentation of Anatomical Structures: Significantly Improved Efficiency and Reliability</article-title>. <source>NeuroImage</source>
<volume>31</volume>, <fpage>1116</fpage>–<lpage>1128</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.015</pub-id>
<pub-id pub-id-type="pmid">16545965</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
