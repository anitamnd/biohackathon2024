<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7414417</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giaa081</article-id>
    <article-id pub-id-type="publisher-id">giaa081</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
        <subject>AcademicSubjects/SCI02254</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Graph2GO: a multi-modal attributed network embedding method for inferring protein functions</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Fan</surname>
          <given-names>Kunjie</given-names>
        </name>
        <aff><institution>Department of Biomedical Informatics, College of Medicine, The Ohio State University</institution>, Columbus, OH 43210, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guan</surname>
          <given-names>Yuanfang</given-names>
        </name>
        <!--<email>gyuanfang@umich.edu</email>-->
        <aff><institution>Department of Computational Medicine and Bioinformatics, University of Michigan</institution>, Ann Arbor, MI 48109, <country country="US">USA</country></aff>
        <xref ref-type="corresp" rid="cor1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3357-5121</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Yan</given-names>
        </name>
        <!--<email>yanzhang.biomed@gmail.com</email>-->
        <aff><institution>Department of Biomedical Informatics, College of Medicine, The Ohio State University</institution>, Columbus, OH 43210, <country country="US">USA</country></aff>
        <aff><institution>The Ohio State University Comprehensive Cancer Center (OSUCCC - James)</institution>, Columbus, OH 43210, <country country="US">USA</country></aff>
        <xref ref-type="corresp" rid="cor2"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><bold>Correspondence address</bold>. Yuanfang Guan, Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor, MI 48109, USA. E-mail: <email>gyuanfang@umich.edu</email></corresp>
      <corresp id="cor2"><bold>Correspondence address</bold>. Yan Zhang, Department of Biomedical Informatics, College of Medicine, The Ohio State University, Columbus, OH 43210, USA. E-mail: <email>yanzhang.biomed@gmail.com</email></corresp>
    </author-notes>
    <pub-date pub-type="epub" iso-8601-date="2020-08-08">
      <day>08</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>9</volume>
    <issue>8</issue>
    <elocation-id>giaa081</elocation-id>
    <history>
      <date date-type="received">
        <day>14</day>
        <month>12</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>30</day>
        <month>4</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="giaa081.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>Identifying protein functions is important for many biological applications. Since experimental functional characterization of proteins is time-consuming and costly, accurate and efficient computational methods for predicting protein functions are in great demand for generating the testable hypotheses guiding large-scale experiments.“</p>
      </sec>
      <sec id="abs2">
        <title>Results</title>
        <p>Here, we propose Graph2GO, a multi-modal graph-based representation learning model that can integrate heterogeneous information, including multiple types of interaction networks (sequence similarity network and protein-protein interaction network) and protein features (amino acid sequence, subcellular location, and protein domains) to predict protein functions on gene ontology. Comparing Graph2GO to BLAST, as a baseline model, and to two popular protein function prediction methods (Mashup and deepNF), we demonstrated that our model can achieve state-of-the-art performance. We show the robustness of our model by testing on multiple species. We also provide a web server supporting function query and downstream analysis on-the-fly.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusions</title>
        <p>Graph2GO is the first model that has utilized attributed network representation learning methods to model both interaction networks and protein features for predicting protein functions, and achieved promising performance. Our model can be easily extended to include more protein features to further improve the performance. Besides, Graph2GO is also applicable to other application scenarios involving biological networks, and the learned latent representations can be used as feature inputs for machine learning tasks in various downstream analyses.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>protein function prediction</kwd>
      <kwd>graph neural network</kwd>
      <kwd>attributed network embedding</kwd>
      <kwd>representation learning</kwd>
      <kwd>multi-modal model</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Ohio State University</institution>
            <institution-id institution-id-type="DOI">10.13039/100006928</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="11"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1">
    <title>Introduction</title>
    <p>Knowledge of protein functions is of great importance to understanding life at a molecular level, studying disease mechanisms, and helping explore novel therapeutic targets. However, the experimental identification of protein functions is time-consuming and expensive, which is not suitable for large-scale applications. Therefore, high-throughput computational methods are required for discovering protein functions with reasonable quality and accuracy [<xref rid="bib1" ref-type="bibr">1</xref>], and providing testable hypotheses for targeted experimental validation.</p>
    <p>Most existing computational algorithms exploit homology inference to infer protein functions [<xref rid="bib2" ref-type="bibr">2–4</xref>], which are based on the assumption that proteins with similar sequences frequently carry out similar functions. A standard approach is simply to transfer annotations from the best-annotated BLAST hit [<xref rid="bib5" ref-type="bibr">5</xref>]. Some approaches involve the use of protein-protein interaction (PPI) networks, based on the fact that proteins closer in the network have a greater chance of sharing similar functions [<xref rid="bib6" ref-type="bibr">6</xref>, <xref rid="bib7" ref-type="bibr">7</xref>]. Some other methods utilize domain content in the sequence to assign functions [<xref rid="bib8" ref-type="bibr">8</xref>, <xref rid="bib9" ref-type="bibr">9</xref>]. Under the assumption that domains are protein structural architecture modules, it makes sense that protein function should be closely related to or determined by the composite domains. There are also other methods making use of other sources of information for protein function prediction, such as protein subcellular localization [<xref rid="bib10" ref-type="bibr">10</xref>, <xref rid="bib11" ref-type="bibr">11</xref>] and post-translational modifications [<xref rid="bib11" ref-type="bibr">11</xref>], as well as the literature [<xref rid="bib12" ref-type="bibr">12</xref>].</p>
    <p>Given the limited prediction capacity of a single source of information, many methods have been proposed to combine several kinds of information and take advantage of the power of machine learning techniques. For instance, INGA [<xref rid="bib13" ref-type="bibr">13</xref>] performs sequence similarity and domain architecture searches, and combines them with enrichment analyses on interaction networks to derive the consensus prediction. Similarly, COFACTOR [<xref rid="bib14" ref-type="bibr">14</xref>] consists of three individual pipelines for sequence-, structure-, and PPI-based predictions by querying the UniProt-GOA [<xref rid="bib15" ref-type="bibr">15</xref>], BioLip, and STRING [<xref rid="bib16" ref-type="bibr">16</xref>] databases, respectively, and it generates the consensus based on three confidence scores obtained from the three pipelines. DeepGO [<xref rid="bib17" ref-type="bibr">17</xref>] uses representation learning methods to learn features from sequence and interaction networks respectively, and then combines them to predict the function using a deep neuro-symbolic model. Although these methods could achieve reasonable prediction accuracy, a limitation is that they only treat multiple kinds of features separately and do not consider the relationships between proteins for each feature, which might result in the loss of information contained within the interactions. A possible solution to take into account the relationships is to use PPI networks to transfer features between proteins.</p>
    <p>Graphs, such as those representing social networks, molecular graph structures [<xref rid="bib18" ref-type="bibr">18</xref>], and biological PPI networks, occur naturally in various real-world scenarios and have important applications in modern machine learning. Modeling the interactions between entities as graphs has enabled researchers to understand the network system in a systematic manner. For example, in a social network application one might wish to predict the role of a person or recommend new friends to a user [<xref rid="bib19" ref-type="bibr">19</xref>]; in a clinical application, researchers want to predict new therapeutic applications of drug molecules [<xref rid="bib20" ref-type="bibr">20</xref>]; and basic scientists are also interested in classifying the roles of a protein in a biological interaction graph [<xref rid="bib21" ref-type="bibr">21</xref>]. Mashup [<xref rid="bib22" ref-type="bibr">22</xref>] is a framework for scalable and robust multiple network integration, and generates low-dimensional vectors for nodes by characterizing topological contexts in heterogeneous PPI networks based on the random walk with restart method. The key of Mashup is the use of the random walk with restart method to analyze the network structure. Researchers also proposed novel methods for dimension reduction and the integration of heterogeneous networks by solving optimization problems. DeepNF [<xref rid="bib23" ref-type="bibr">23</xref>] is a network fusion method for extracting protein features from multiple heterogeneous interaction networks based on multi-modal deep auto-encoders. DeepNF also utilizes the random walk with restart to extract information about network structures for individual PPI networks. DeepNF proposed a different integration method based on auto-encoders. Both of these two methods adopt a two-stage model: first generating informative embeddings based on network structures in an unsupervised manner, then building a supervised classification model to predict gene ontology (GO) terms with embeddings as the input. Although both Mashup and DeepNF can generate embeddings used for downstream protein function prediction, they only consider the topological information contained in multiple PPI networks, while ignoring informative protein attributes, such as protein sequence or protein domains; thus, they might lack important information.</p>
    <p>A crucial challenge in machine learning on graphs is finding a way to incorporate multiple types of information about the structure and attributes of the graph into the machine learning model. Traditional approaches usually use summary graph statistics [<xref rid="bib24" ref-type="bibr">24</xref>], kernel functions [<xref rid="bib25" ref-type="bibr">25</xref>], or handcrafted features to represent local neighborhood structures. Nowadays, people seek to learn representations that encode structural information in a data-driven way. The idea behind these representation learning methods is to learn a function that maps nodes in the graph to points in a low-dimensional vector space. By optimizing this mapping, the geometric relationships in the learned space can reflect the original structure of the graph, and the learned representation can be used as feature inputs for downstream machine learning applications [<xref rid="bib26" ref-type="bibr">26</xref>]. These network representation learning methods can also make use of node attributes to generate more informative embeddings, such as user profiles in a social network and protein signatures in a protein interaction network [<xref rid="bib21" ref-type="bibr">21</xref>].</p>
    <p>In this paper, we propose Graph2GO, a multi-modal graph-based architecture, that can make use of several kinds of data sources in a unified way to predict protein function. Unlike other consensus methods that we mentioned earlier, we first use PPI and sequence similarities to build two graphs separately and use protein sequence information, protein subcellular location, protein domains, or any other possible information as node attributes in both graphs. Given the attributed graph, we use the attributed network representation learning algorithm to obtain informative embeddings for each node in each graph. In the graph, a node represents a protein. In this way, we manage to learn representations by modeling both node attributes and network structures comprehensively and simultaneously. Then, we combine these two embeddings and use them to predict the protein functions with a feedforward neural network model.</p>
    <p>As far as we know, we are the first to use attributed network representation learning methods to model both an interaction network and a sequence similarity network (SSN) with node attributes and predict protein functions, and we have successfully achieved state-of-the-art performance on the benchmark data set. Besides, our model is extensible to other function-related information to further improve the performance. Graph2GO does not rely on any manually crafted features and is entirely data driven. Graph2GO is also applicable to other similar scenarios, such as predicting new therapeutic applications of existing drugs, since the learned embeddings can be used as feature inputs for various downstream machine learning tasks.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec2">
    <title>Materials and Methods</title>
    <p>Graph2GO consists of two parts: the first part is an unsupervised graph-based representation model that utilizes both network information (PPI, sequence similarities) and node attributes (protein sequence, subcellular location, and protein domains) to generate unique embedding vectors for each protein; the second part is a fully-connected deep neural network (DNN) classifier, which use embeddings as features and gene ontology (GO) [<xref rid="bib27" ref-type="bibr">27</xref>] as function labels. GO defines concepts that describe functions and classifies functions on three aspects: molecular function (MF), cellular component (CC), and biological process (BP) [<xref rid="bib27" ref-type="bibr">27</xref>]. We first describe how we obtain and encode different kinds of information, and then describe the detailed model specification. In Fig. <xref ref-type="fig" rid="fig1">1</xref>, we use the PTEN gene as an example to detail how we encode these information. The model architecture is shown in Fig. <xref ref-type="fig" rid="fig2">2</xref>.</p>
    <fig id="fig1" orientation="portrait" position="float">
      <label>Figure 1:</label>
      <caption>
        <p>Here we use PTEN as an example to show how we encode sequence, subcellular location, and protein domains features. For sequence encoding, we use the conjoint triad (CT) method. For subcellular location and protein domains, we use bag-of-words encoding.</p>
      </caption>
      <graphic xlink:href="giaa081fig1"/>
    </fig>
    <fig id="fig2" orientation="portrait" position="float">
      <label>Figure 2:</label>
      <caption>
        <p>(<bold>a</bold>) Architecture of the variational graph auto encoder (VGAE), the first part of Graph2GO. The inputs to VGAE include an adjacency matrix A, representing a protein-related network, and a node attribute matrix. VGAE is an encoder-decoder approach. The encoder is a two-layer graph convolutional network and the decoder is a dot product decoder. The mean vector, <inline-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{\mu }}_i$\end{document}</tex-math></inline-formula>, is the output embedding for classification. (<bold>b</bold>) Graph2GO pipeline consisting of two VGAE models for PPI networks and SSN respectively, and the final DNN classifier. The two embeddings generated from the PPI network and SSN are concatenated as the input for the DNN classifier, and the DNN classifier outputs the probabilities of the protein having each GO term annotation.</p>
      </caption>
      <graphic xlink:href="giaa081fig2"/>
    </fig>
    <sec id="sec2-1">
      <title>Data set</title>
      <p>We use reviewed and manually annotated proteins from SwissProt (release 2018_11) [<xref rid="bib15" ref-type="bibr">15</xref>]. The data set contains 20,412 human proteins. The data set provides GO annotations along with the experimental evidence code. We also obtained protein sequences, subcellular locations, and protein domains (Pfam) from SwissProt. We downloaded PPI networks from the STRING database (v10.5) [<xref rid="bib16" ref-type="bibr">16</xref>], filtered by the proteins we obtained from SwissProt.</p>
    </sec>
    <sec id="sec2-2">
      <title>Data encoding</title>
      <sec id="sec2-2-1">
        <title>Protein-protein interaction network</title>
        <p>From the network data in STRING, we use the “combined score” provided by STRING as the confidence score. As shown in <xref ref-type="supplementary-material" rid="sup10">Supplementary Figure S2</xref>, We perform cross-validation to choose the threshold of the combined score for building the PPI network, and we only use interactions that have a “combined score” greater than 300 to construct the adjacency matrix as the representation of the network. For interactions with low confidence and protein pairs with no interactions mentioned in STRING, we assign the corresponding element in the adjacency matrix as 0.</p>
      </sec>
      <sec id="sec2-2-2">
        <title>Sequence similarity network</title>
        <p>In order to build a SSN, we use the BLAST program to search similar sequences for each protein in our data set [<xref rid="bib28" ref-type="bibr">28</xref>]. We only select similarity pairs that have an “e-value” smaller than 1e-4 as candidate edges for the similarity network. Detailed cross-validation results on selecting the threshold are shown in <xref ref-type="supplementary-material" rid="sup10">Supplementary Figure S1</xref>.</p>
      </sec>
      <sec id="sec2-2-3">
        <title>Protein sequence</title>
        <p>We encode amino acid sequences following the conjoint triad (CT) method [<xref rid="bib29" ref-type="bibr">29</xref>], which has been widely used to represent sequences in related fields [<xref rid="bib30" ref-type="bibr">30–33</xref>]. The 20 kinds of amino acids are first clustered into 7 classes according to the dipoles and volumes of the side chains, since amino acids within the same class likely involve synonymous mutations. And then all the amino acids in the same class are considered as identical. The mapping between classes and amino acids is shown in <xref ref-type="supplementary-material" rid="sup10">Supplementary Table S1</xref>. Then, we consider any 3 continuous amino acids as a unit and count the triad frequencies by calculating the occurrence numbers within the protein sequence. Thus, the dimension of the CT encoding is 7 × 7 × 7 = 343. Using the CT method, we manage to convert amino acid sequences into fixed-dimension representation.</p>
      </sec>
      <sec id="sec2-2-4">
        <title>Subcellular location</title>
        <p>We obtained subcellular location information from the SwissProt data set. In total, there are 359 different subcellular locations. We use bag-of-words encoding to represent this information; that’s to say, the location is encoded as a binary vector of length 359 with each element indicating whether the protein is annotated with this location. Therefore, for a protein without any subcellular location annotation, it is represented as a vector of all 0s.</p>
      </sec>
      <sec id="sec2-2-5">
        <title>Protein domains</title>
        <p>In the data set we obtained from SwissProt, there are 5,817 unique protein domains. In order to avoid the curse of dimensionality and to decrease the complexity, we only use protein domain terms that appear more than 5 times in our data set, and we are left with 655 terms. The protein domain knowledge is also encoded by way of bag-of-words encoding.</p>
      </sec>
    </sec>
    <sec id="sec2-3">
      <title>VGAE model</title>
      <p>In the first part of Graph2GO, the core is a variational graph auto-encoder (VGAE) [<xref rid="bib34" ref-type="bibr">34</xref>], which can generate latent representations based on both network structure and node features, as shown in Fig. <xref ref-type="fig" rid="fig2">2a</xref>. We will discuss this model by problem formulation, followed by its inference part (encoder) and generative part (decoder). The purpose of VGAE is to learn interpretable embedding for each protein by training the encoder and decoder at the same time.</p>
      <sec id="sec2-3-1">
        <title>Problem formulation</title>
        <p>We are given an undirected graph, <inline-formula><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {G} = (\mathcal {V}, \mathcal {E})$\end{document}</tex-math></inline-formula>, with <inline-formula><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$N = | \mathcal {V} |$\end{document}</tex-math></inline-formula> nodes. Here, <italic>N</italic> is the number of proteins, and each vertex of <inline-formula><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {G}$\end{document}</tex-math></inline-formula> represents one protein, while each edge is one interaction in the PPI network or a similarity pair in the SSN. The adjacency matrix <inline-formula><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula> of <inline-formula><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\mathcal {G}$\end{document}</tex-math></inline-formula> and its degree matrix <inline-formula><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{D}}_{A}$\end{document}</tex-math></inline-formula> are derived from known network information. We enforce self-loops in the graph by simply adding the identity matrix to <inline-formula><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula>. The input features of each vertex are included in an <italic>N</italic> × <italic>R</italic> matrix <inline-formula><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{X}}$\end{document}</tex-math></inline-formula> which is the concatenation of protein sequence feature <inline-formula><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{S}}$\end{document}</tex-math></inline-formula>, subcellular location feature <inline-formula><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{L}}$\end{document}</tex-math></inline-formula>, and protein domains feature <inline-formula><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{D}}$\end{document}</tex-math></inline-formula>, as shown in Fig. <xref ref-type="fig" rid="fig2">2a</xref>. Here, <italic>R</italic> is the sum of feature dimensions of matrix <inline-formula><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{S}}$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{L}}$\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math id="M15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{D}}$\end{document}</tex-math></inline-formula>.</p>
        <p>The model depicted in Fig. <xref ref-type="fig" rid="fig2">2a</xref> is basically an encoder-decoder model. First the encoder maps each node <italic>v<sub>i</sub></italic> in the graph to a low-dimensional latent variable <inline-formula><tex-math id="M16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{z}}_i$\end{document}</tex-math></inline-formula>, based on the node’s position in the graph, its local neighborhood structure, and its attributes. Next, the decoder reconstructs the adjacency matrix term <italic>A<sub>ij</sub></italic> corresponding to the pair of nodes <italic>v<sub>i</sub></italic> and <italic>v<sub>j</sub></italic>. By jointly optimizing the encoder and decoder, the model learns to compress graph structure information and original node attributes into the low-dimensional latent space. In principle, the model learns to propagate features across the proteins based on the network structure, and the encoder-decoder architecture ensures that the learned representations in the latent space are meaningful and informative.</p>
      </sec>
      <sec id="sec2-3-2">
        <title>Encoder</title>
        <p>The inference module is a graph convolutional network (GCN) encoder [<xref rid="bib35" ref-type="bibr">35</xref>], which is a function with the goal of a mapping from the original features <inline-formula><tex-math id="M17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{X}}$\end{document}</tex-math></inline-formula> to the latent variable <inline-formula><tex-math id="M18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{Z}}$\end{document}</tex-math></inline-formula> with the network information <inline-formula><tex-math id="M19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula>. To be more specific, we want to learn a probability model <inline-formula><tex-math id="M20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$q(\boldsymbol {\mathrm{Z}} | \boldsymbol {\mathrm{X}}, \boldsymbol {\mathrm{A}})$\end{document}</tex-math></inline-formula>. Here we use GCN <italic>g</italic> to model this probability:
<disp-formula id="equ1"><label>(1)</label><tex-math id="M21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
[\boldsymbol {\mu };\log \boldsymbol {\sigma }] &amp;= g(\boldsymbol {\mathrm{X}},\boldsymbol {\mathrm{A}};\phi )
\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula id="equ2"><label>(2)</label><tex-math id="M22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
q(\boldsymbol {\mathrm{Z}} | \boldsymbol {\mathrm{X}},\boldsymbol {\mathrm{A}}) &amp;= \mathcal {N}(\boldsymbol {\mathrm{Z}}; \boldsymbol {\mu },\boldsymbol {\sigma }^{2}\boldsymbol {\mathrm{I}})
\end{eqnarray*}$$\end{document}</tex-math></disp-formula>where <italic>q</italic> is a function that encodes proteins into latent variables <inline-formula><tex-math id="M23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{Z}}$\end{document}</tex-math></inline-formula> based on network information <inline-formula><tex-math id="M24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula> and node attributes <inline-formula><tex-math id="M25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{X}}$\end{document}</tex-math></inline-formula>, ϕ is the parameter of GCN <italic>g</italic>, and <inline-formula><tex-math id="M26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{I}}$\end{document}</tex-math></inline-formula> is an identity matrix. <inline-formula><tex-math id="M27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mu }$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\sigma }$\end{document}</tex-math></inline-formula> are the mean and variance, respectively, of the Gaussian distribution corresponding to latent variable <inline-formula><tex-math id="M29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{Z}}$\end{document}</tex-math></inline-formula>, and are estimated using network <italic>g</italic> from data directly. Then <inline-formula><tex-math id="M30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{Z}}$\end{document}</tex-math></inline-formula> can be sampled from <inline-formula><tex-math id="M31">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$q(\boldsymbol {\mathrm{Z}} | \boldsymbol {\mathrm{X}},\boldsymbol {\mathrm{A}})$\end{document}</tex-math></inline-formula>. According to the reparameterization trick [<xref rid="bib36" ref-type="bibr">36</xref>], <inline-formula><tex-math id="M32">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {z_i}$\end{document}</tex-math></inline-formula> is obtained by:
<disp-formula id="equ3"><label>(3)</label><tex-math id="M33">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
\boldsymbol {\mathrm{z}}_i = \boldsymbol {\mu } + \boldsymbol {\sigma } \odot \epsilon _{i}
\end{eqnarray*}$$\end{document}</tex-math></disp-formula>where ⊙ is element-wise multiplication and <inline-formula><tex-math id="M34">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\epsilon _{i} \sim \mathcal {N}(0,\boldsymbol {\mathrm{I}})$\end{document}</tex-math></inline-formula>.</p>
        <p>Our GCN <italic>g</italic> is a two-layer network as defined:
<disp-formula id="equ4"><label>(4)</label><tex-math id="M35">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
g(\boldsymbol {\mathrm{X}}, \boldsymbol {\mathrm{A}};\phi ) = \tilde{\mathrm{\boldsymbol {A}}}\operatorname{ReLU}(\tilde{\mathrm{\boldsymbol {A}}}\mathrm{\boldsymbol {X}}\mathrm{\boldsymbol {W}}^{(0)})\boldsymbol {\mathrm{W}}^{(1)}
\end{eqnarray*}$$\end{document}</tex-math></disp-formula>where <inline-formula><tex-math id="M36">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{W}}^{(i)}$\end{document}</tex-math></inline-formula> are parameter matrices we need to train, <inline-formula><tex-math id="M37">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\operatorname{ReLU}(\cdot ) = max(0, \cdot )$\end{document}</tex-math></inline-formula> is the activation function, and <inline-formula><tex-math id="M38">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{\mathrm{\boldsymbol {A}}} = \boldsymbol {\mathrm{D}}_{A}^{-\frac{1}{2}}\boldsymbol {\mathrm{A}}\boldsymbol {\mathrm{D}}_{A}^{-\frac{1}{2}}$\end{document}</tex-math></inline-formula> is the symmetrically normalized adjacency matrix [<xref rid="bib35" ref-type="bibr">35</xref>].</p>
        <p>The intuition of GCN is as follows. The multiplication of feature matrix <inline-formula><tex-math id="M39">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{X}}$\end{document}</tex-math></inline-formula> and the adjacency matrix <inline-formula><tex-math id="M40">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula> means that, for every node, we sum up feature vectors of all its neighboring nodes and itself to update its feature representation. In this way, node attributes are propagated across the graph and the information associated with each protein is augmented, especially for proteins with little annotation. This graph convolution operation is similar to Laplacian smoothing, which makes features of nodes in the same cluster similar [<xref rid="bib37" ref-type="bibr">37</xref>]. In order to avoid changing the scale of the feature vectors, <inline-formula><tex-math id="M41">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula> is first normalized so that all rows sum to 1 by symmetric normalization. Multiplying <inline-formula><tex-math id="M42">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{X}}$\end{document}</tex-math></inline-formula> with <inline-formula><tex-math id="M43">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\tilde{\boldsymbol {\mathrm{A}}}$\end{document}</tex-math></inline-formula> means taking the average of neighboring nodes' features. In this way, GCN can effectively learn embeddings through integrating neighboring graph features.</p>
      </sec>
      <sec id="sec2-3-3">
        <title>Decoder</title>
        <p>As our latent embedding already contains both node attributes and structure information, the generative module we define here is a simple inner product decoder that aims to reconstruct adjacency matrix <inline-formula><tex-math id="M44">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula> using learned latent variables <inline-formula><tex-math id="M45">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{z}}_i$\end{document}</tex-math></inline-formula>:
<disp-formula id="equ5"><label>(5)</label><tex-math id="M46">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
p(\boldsymbol {\mathrm{A}} | \boldsymbol {\mathrm{Z}}) &amp;= \prod _{i=1}^{N}\prod _{j=1}^{N}p(A_{ij} | \boldsymbol {\mathrm{z}}_i, \boldsymbol {\mathrm{z}}_j)
\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula id="equ6"><label>(6)</label><tex-math id="M47">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
p(A_{ij} &amp;= 1 | \boldsymbol {\mathrm{z}}_i, \boldsymbol {\mathrm{z}}_j) = \sigma (\boldsymbol {\mathrm{z}}_i^\top \boldsymbol {\mathrm{z}}_j)
\end{eqnarray*}$$\end{document}</tex-math></disp-formula>where σ( · ) is the logistic function. We use the logistic function–transformed inner product of <inline-formula><tex-math id="M48">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{z}}_i$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M49">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{z}}_j$\end{document}</tex-math></inline-formula>, shown on the right-hand side of Equation (6), as the probability of these two proteins having interaction. As indicated in Fig.   <xref ref-type="fig" rid="fig2">2a</xref>, the output of the decoder <inline-formula><tex-math id="M50">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\hat{\mathrm{A}}}$\end{document}</tex-math></inline-formula> is the approximation of adjacency matrix <inline-formula><tex-math id="M51">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula>, and we optimize the model so as to make them as close as possible.</p>
      </sec>
      <sec id="sec2-3-4">
        <title>Cost function</title>
        <p>Similar to variational auto-encoder (VAE) [<xref rid="bib36" ref-type="bibr">36</xref>], the cost function is the reconstruction error with a regularizer:
<disp-formula id="equ7"><label>(7)</label><tex-math id="M52">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
\mathcal {L} = \mathbb {E}_{q(\boldsymbol {\mathrm{Z}} | \boldsymbol {\mathrm{X}}, \boldsymbol {\mathrm{A}})}[\log p(\boldsymbol {\mathrm{A}} | \boldsymbol {\mathrm{Z}})]- \operatorname{KL}[q(\boldsymbol {\mathrm{Z}} | \boldsymbol {\mathrm{X}}, \boldsymbol {\mathrm{A}}) \Vert p(\boldsymbol {\mathrm{Z}}) ]
\end{equation*}$$\end{document}</tex-math></disp-formula>where <italic>KL</italic> [<italic>q</italic>( · )‖<italic>p</italic>( · )] is the Kullback-Leibler divergence between <italic>q</italic>( · ) and <italic>p</italic>( · ). The first term is to minimize the reconstruction error of the adjacency matrix <inline-formula><tex-math id="M53">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mathrm{A}}$\end{document}</tex-math></inline-formula>. The second term is to minimize the difference between <inline-formula><tex-math id="M54">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$q(\boldsymbol {\mathrm{Z}} | \boldsymbol {\mathrm{X}}, \boldsymbol {\mathrm{A}})$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M55">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$p(\boldsymbol {\mathrm{Z}})$\end{document}</tex-math></inline-formula>. The cost function is the tradeoff between how accurately our model can reconstruct the input network and how closely the latent variables can match <inline-formula><tex-math id="M56">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$p(\boldsymbol {\mathrm{Z}})$\end{document}</tex-math></inline-formula>. As specified in VAE, we assume <inline-formula><tex-math id="M57">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$p(\boldsymbol {\mathrm{Z}}) \sim \mathcal {N}(0,\boldsymbol {\mathrm{I}})$\end{document}</tex-math></inline-formula>. We train the VGAE using stochastic gradient descent to optimize the cost function with respect to the parameters of the encoder [<xref rid="bib36" ref-type="bibr">36</xref>].</p>
      </sec>
    </sec>
    <sec id="sec2-4">
      <title>DNN classifier</title>
      <p>In the second part of Graph2GO, as shown in Fig. <xref ref-type="fig" rid="fig2">2b</xref>, we take out embeddings <inline-formula><tex-math id="M58">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mu }_i$\end{document}</tex-math></inline-formula> contained in the matrix <inline-formula><tex-math id="M59">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\boldsymbol {\mu }$\end{document}</tex-math></inline-formula> and train a fully-connected DNN as the final supervised classifier. DNN has become one of the most popular and powerful techniques for supervised machine learning [<xref rid="bib38" ref-type="bibr">38</xref>]. It consists of three types of layers: an input layer, hidden layers, and an output layer. The inputs to the classifier are embedding vectors for each protein, while the output layer represents GO terms that we aim to predict. Hidden layers are stacked between the input layer and output layer, aiming to learn meaningful abstract features for the task. To go from one layer to the next, a set of units (neurons) compute a weighted sum of their inputs from the previous layer and pass the result through a non-linear function (activation function). By stacking multiple hidden layers, DNN can learn an extremely intricate non-linear function mapping from the input to the output, which means it works best when the task is inherently non-linear. Gradient descent algorithm is commonly used for training neural networks and binary cross-entropy is used as the cost function in our multi-label classification task.</p>
      <p>We train three classifiers: one each for MF, BP, and CC respectively. For each classifier, it is a multi-class, multi-label model, and the dimension of the output space is the number of GO terms within each ontology. Each protein may be predicted with multiple GO terms simultaneously. The classifier predicts the probabilities of the protein having each GO term annotation. The performance of the classifier can be excellent without many complex neural network structures, since the embeddings already contain enough information and are highly representative in the learned low-dimensional vector space.</p>
    </sec>
    <sec id="sec2-5">
      <title>Graph2GO pipeline</title>
      <p>As shown in Fig. <xref ref-type="fig" rid="fig2">2b</xref>, the Graph2GO pipeline consists of two VGAE models for the PPI network and SSN, and the final DNN classifier for predicting protein functions. Instead of combining two networks first and training one VGAE to obtain overall embeddings, by training an independent VGAE model for each network and combining their embeddings, we try to avoid introducing noise and to keep as much information as possible. We compared the performance of these two ways for integrating two networks and the detailed results are shown in <xref ref-type="supplementary-material" rid="sup10">Supplementary Table S2</xref>.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec3">
    <title>Results</title>
    <sec id="sec3-1">
      <title>Experimental setup</title>
      <p>Graph2GO was implemented using Tensorflow in Python and took advantage of the powerful computing capacity of a graphics processing unit (GPU). All the simulations were carried out on the Owens cluster provided by the Ohio Supercomputer Center [] with 27 processors and 127 GB memory. The GPU we used was a NVIDIA Tesla P100 with 16 GB memory. Our source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/yanzhanglab/Graph2GO">https://github.com/yanzhanglab/Graph2GO</ext-link>.</p>
      <p>For our experiments, we use SwissProt’s reviewed and manually annotated human proteins (release 2018_11). We only use proteins that also exist in the STRING database (v10.5), where corresponding interaction information is available. In order to use the CT method to encode amino acid sequences, we delete proteins whose sequence contains ambiguous amino acids, including B, O, J, U, X, and Z. After filtering out 5,279 proteins, our final data set contains 15,133 proteins, along with 1,713,652 PPIs and 843,212 edges in the SSN. For GO annotations, we only consider the experimental evidence code among EXP, IDA, IPI, IMP, IGI, IEP, TAS, and IC [<xref rid="bib27" ref-type="bibr">27</xref>]. If a protein is annotated with a GO term, we additionally annotated it with all the ancestor terms.</p>
      <p>In the first part of our architecture, the encoder is a two-layer neural network structure with one layer having 400 neurons and the other having 800 neurons. Using two layers of graph convolutional operations is recommended in Kipf and Welling [<xref rid="bib35" ref-type="bibr">35</xref>] and Li et al. [<xref rid="bib37" ref-type="bibr">37</xref>], and we confirm the choice of two layers by performing cross-validation, as shown in <xref ref-type="supplementary-material" rid="sup10">Supplementary Figure S1</xref>. We initialize weights as described in Glorot and Bengio [<xref rid="bib40" ref-type="bibr">39</xref>], and train the model for 200 iterations using Adam algorithm with a learning rate of 0.001 [<xref rid="bib41" ref-type="bibr">40</xref>]. The final prediction classifier is a three-layer fully connected neural network with each layer having 1,024, 512, and 256 neurons respectively. A dropout layer and a batch normalization layer are used between every set of dense layers to avoid over-fitting and the dropout rate is set as 0.3. Adam is used to train the model for 100 iterations with a learning rate of 0.001.</p>
    </sec>
    <sec id="sec3-2">
      <title>Evaluation metrics</title>
      <p>We use metrics that are similar to those adopted by the Critical Assessment of protein Function Annotation algorithms (CAFA) challenge to evaluate our model and compare it with others [<xref rid="bib1" ref-type="bibr">1</xref>]. The first metric we use is the maximum F-measure (F-max) over all possible thresholds, defined as follows:
<disp-formula id="equ8"><label>(8)</label><tex-math id="M60">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
pr(t) &amp;= \displaystyle\frac{\sum _{i}\sum _{f}I(f \in P_{i}(t) \wedge f \in T_{i})}{\sum _{i}\sum _{f}I(f \in P_{i}(t))}
\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula id="equ9"><label>(9)</label><tex-math id="M61">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
{\textit rc(t)} &amp;= \displaystyle\frac{\sum _{i}\sum _{f}I(f \in P_{i}(t) \wedge f \in T_{i})}{\sum _{i}\sum _{f}I(f \in T_{i})}
\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula id="equ10"><label>(10)</label><tex-math id="M62">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
F_{\mathrm{max}} &amp;= \max \limits _{t}\left\lbrace \displaystyle\frac{2 \cdot pr(t) \cdot rc(t)}{pr(t)+rc(t)}\right\rbrace
\end{eqnarray*}$$\end{document}</tex-math></disp-formula>where <italic>pr</italic> means precision, <italic>rc</italic> means recall, <italic>I</italic> is the indicator function, f is a GO term, <italic>P<sub>i</sub></italic>(<italic>t</italic>) is a set of predicted GO terms for protein <italic>i</italic> using the threshold <italic>t</italic>, and <italic>T<sub>i</sub></italic> is a set of annotated GO terms for protein <italic>i</italic>.</p>
      <p>We also use similar term-centric metrics to evaluate the model, as suggested by CAFA. However, instead of using receiver operating characteristic (ROC) curves, we choose to consider precision-recall (PR) curves and calculate the area under the curve. As pointed out by Davis and Goadrich [<xref rid="bib42" ref-type="bibr">41</xref>], when dealing with highly imbalanced data sets, PR curves give a more informative picture of an algorithm’s performance than ROC curves. Since this is a multi-label task, we adopt two averaged measures of area under the precision-recall curve (AUPR) for all terms:
<disp-formula id="equ11"><label>(11)</label><tex-math id="M63">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
pr_{f}(t) &amp;= \displaystyle\frac{\sum _{i}I(f \in P_{i}(t) \wedge f \in T_{i})}{\sum _{i}I(f \in P_{i}(t))}
\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula id="equ12"><label>(12)</label><tex-math id="M64">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
rc_{f}(t) &amp;= \displaystyle\frac{\sum _{i}I(f \in P_{i}(t) \wedge f \in T_{i})}{\sum _{i}I(f \in T_{i})}
\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula id="equ13"><label>(13)</label><tex-math id="M65">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
\text{AUPR}_{f} &amp;= \sum _{t}(rc_{f}(t)-rc_{f}(t-1)) \cdot pr_{f}(t)
\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula id="equ14"><label>(14)</label><tex-math id="M66">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
\text{M-AUPR} &amp;= \displaystyle\frac{1}{N_f} \cdot \sum _{f}\text{AUPR}_{f}
\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula id="equ15"><label>(15)</label><tex-math id="M67">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}
\text{m-AUPR} &amp;= \sum _{t}(rc(t)-rc(t-1)) \cdot pr(t)
\end{eqnarray*}$$\end{document}</tex-math></disp-formula>Where <italic>pr<sub>f</sub></italic> and <italic>rc<sub>f</sub></italic> are precision and recall for a single GO term f, AUPR<sub><italic>f</italic></sub> is the area under the precision-recall curve (AUPR) for f, and <italic>N<sub>f</sub></italic> is the number of GO terms used for evaluation. The macro-averaged AUPR (M-AUPR) is defined as the unweighted mean of the AUPR for all labels, while the micro-averaged AUPR (m-AUPR) is calculated globally by considering each element of the label indicator matrix as a label.</p>
    </sec>
    <sec id="sec3-3">
      <title>Comparison between different types of features</title>
      <p>In this section, we compare the results of using different network information and node attributes. The purpose is to show which type of feature is most informative and how we can further improve the performance by combining multiple types of features. In Table <xref rid="tbl1" ref-type="table">1</xref>, we show the results when using different network types and node attributes. For network types, we test among only using the PPI network, only using the SSN, and using the combined network. As for node attributes, we compare the performance among different attributes (sequence, protein domains, and subcellular location) and especially compare with ALL attributes (using all three kinds of attributes together). We do not consider sequence information as node attributes when the SSN is used as the network to train the model, because of the redundancy.</p>
      <table-wrap id="tbl1" orientation="portrait" position="float">
        <label>Table 1:</label>
        <caption>
          <p>Performance comparison among using different network types (sequence similarity network, PPI network, and both) and node attributes (sequence, subcellular location, protein domains, and ALL)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th colspan="3" align="center" rowspan="1">CC</th>
              <th colspan="3" align="center" rowspan="1">MF</th>
              <th colspan="3" align="center" rowspan="1">BP</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Network</th>
              <th rowspan="1" colspan="1">Attribute</th>
              <th rowspan="1" colspan="1">M-AUPR</th>
              <th rowspan="1" colspan="1">m-AUPR</th>
              <th rowspan="1" colspan="1">F-max</th>
              <th rowspan="1" colspan="1">M-AUPR</th>
              <th rowspan="1" colspan="1">m-AUPR</th>
              <th rowspan="1" colspan="1">F-max</th>
              <th rowspan="1" colspan="1">M-AUPR</th>
              <th rowspan="1" colspan="1">m-AUPR</th>
              <th rowspan="1" colspan="1">F-max</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">location</td>
              <td rowspan="1" colspan="1">0.325</td>
              <td rowspan="1" colspan="1">0.686</td>
              <td rowspan="1" colspan="1">0.639</td>
              <td rowspan="1" colspan="1">0.411</td>
              <td rowspan="1" colspan="1">0.650</td>
              <td rowspan="1" colspan="1">0.624</td>
              <td rowspan="1" colspan="1">0.152</td>
              <td rowspan="1" colspan="1">0.336</td>
              <td rowspan="1" colspan="1">0.388</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Sequence</td>
              <td rowspan="1" colspan="1">domain</td>
              <td rowspan="1" colspan="1">0.217</td>
              <td rowspan="1" colspan="1">0.594</td>
              <td rowspan="1" colspan="1">0.568</td>
              <td rowspan="1" colspan="1">0.383</td>
              <td rowspan="1" colspan="1">0.669</td>
              <td rowspan="1" colspan="1">0.639</td>
              <td rowspan="1" colspan="1">0.144</td>
              <td rowspan="1" colspan="1">0.357</td>
              <td rowspan="1" colspan="1">0.384</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">ALL</td>
              <td rowspan="1" colspan="1">0.348</td>
              <td rowspan="1" colspan="1">0.694</td>
              <td rowspan="1" colspan="1">0.644</td>
              <td rowspan="1" colspan="1">0.465</td>
              <td rowspan="1" colspan="1">0.717</td>
              <td rowspan="1" colspan="1">0.671</td>
              <td rowspan="1" colspan="1">0.185</td>
              <td rowspan="1" colspan="1">0.390</td>
              <td rowspan="1" colspan="1">0.418</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">sequence</td>
              <td rowspan="1" colspan="1">0.471</td>
              <td rowspan="1" colspan="1">0.683</td>
              <td rowspan="1" colspan="1">0.639</td>
              <td rowspan="1" colspan="1">0.466</td>
              <td rowspan="1" colspan="1">0.657</td>
              <td rowspan="1" colspan="1">0.626</td>
              <td rowspan="1" colspan="1">0.268</td>
              <td rowspan="1" colspan="1">0.458</td>
              <td rowspan="1" colspan="1">0.472</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">PPI</td>
              <td rowspan="1" colspan="1">location</td>
              <td rowspan="1" colspan="1">0.448</td>
              <td rowspan="1" colspan="1">0.704</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.398</td>
              <td rowspan="1" colspan="1">0.596</td>
              <td rowspan="1" colspan="1">0.567</td>
              <td rowspan="1" colspan="1">0.233</td>
              <td rowspan="1" colspan="1">0.435</td>
              <td rowspan="1" colspan="1">0.450</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">domain</td>
              <td rowspan="1" colspan="1">0.443</td>
              <td rowspan="1" colspan="1">0.681</td>
              <td rowspan="1" colspan="1">0.634</td>
              <td rowspan="1" colspan="1">0.476</td>
              <td rowspan="1" colspan="1">0.678</td>
              <td rowspan="1" colspan="1">0.635</td>
              <td rowspan="1" colspan="1">0.258</td>
              <td rowspan="1" colspan="1">0.464</td>
              <td rowspan="1" colspan="1">0.470</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">ALL</td>
              <td rowspan="1" colspan="1">0.478</td>
              <td rowspan="1" colspan="1">0.716</td>
              <td rowspan="1" colspan="1">0.644</td>
              <td rowspan="1" colspan="1">0.478</td>
              <td rowspan="1" colspan="1">0.677</td>
              <td rowspan="1" colspan="1">0.623</td>
              <td rowspan="1" colspan="1">0.268</td>
              <td rowspan="1" colspan="1">0.471</td>
              <td rowspan="1" colspan="1">0.471</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">location</td>
              <td rowspan="1" colspan="1">0.465</td>
              <td rowspan="1" colspan="1">0.744</td>
              <td rowspan="1" colspan="1">0.686</td>
              <td rowspan="1" colspan="1">0.499</td>
              <td rowspan="1" colspan="1">0.715</td>
              <td rowspan="1" colspan="1">0.672</td>
              <td rowspan="1" colspan="1">0.239</td>
              <td rowspan="1" colspan="1">0.448</td>
              <td rowspan="1" colspan="1">0.463</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Combined</td>
              <td rowspan="1" colspan="1">domain</td>
              <td rowspan="1" colspan="1">0.426</td>
              <td rowspan="1" colspan="1">0.695</td>
              <td rowspan="1" colspan="1">0.643</td>
              <td rowspan="1" colspan="1">0.520</td>
              <td rowspan="1" colspan="1">0.751</td>
              <td rowspan="1" colspan="1">0.698</td>
              <td rowspan="1" colspan="1">0.253</td>
              <td rowspan="1" colspan="1">0.465</td>
              <td rowspan="1" colspan="1">0.473</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">ALL</td>
              <td rowspan="1" colspan="1">
                <bold>0.494</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.751</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.686</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.560</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.761</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.718</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.284</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.488</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.490</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="req-159527263516437360">
            <p>We use the M-AUPR, m-AUPR, and F-max as the evaluation metric. ALL means using the combination of all features as node attributes, and we do not include sequence as a node attribute when SSN is used to train the model.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Looking at the performance of using different network types, the combined network is always better than the other two single network types across all three ontologies, which indicates that the PPI network and the SSN can provide complementary information for function prediction. Comparing the effect of the PPI network and the SSN, the PPI network shows better performance than the SSN, except on MF ontology. We can conclude that the PPI network provides more function-related information than the SSN, and that MF ontology has a great correlation with sequence information.</p>
      <p>From the analysis of the effects of using different node attributes, we can conclude that the integration of all node attributes is better than using any single attribute across all three ontologies, no matter what kind of network type is used. We can see that using a sequence attribute can achieve reasonable performance for predicting functions in all three ontologies when combined with the PPI network, proving the importance of sequence information. It also shows that the protein domain attribute is the most informative feature for predicting MF functions, while the subcellular location attribute is important for CC function predictions. Overall, each type of node attribute contributes to the function prediction, and the combination of all features improves the performance and provides more robust predictions. Based on the results shown in Table <xref rid="tbl1" ref-type="table">1</xref>, our final model considers both the SSN and PPI network as our network information. As for node attributes, we include subcellular location and protein domain features as node attributes in the SSN, and include all three types of features as node attributes in the PPI network.</p>
    </sec>
    <sec id="sec3-4">
      <title>Comparison with BLAST</title>
      <p>We first compare our method with a baseline model, BLAST, which is a widely used homology-based method for annotating protein functions [<xref rid="bib5" ref-type="bibr">5</xref>]. We randomly split the data set into 80% for the training set and 20% for thre test set. We compare our method with BLAST under two conditions: (1) the full data set, using all of the 80% training set; and (2) the partial data set, removing those training samples with more than 50% identity with one of the test samples (removing potential homologs). The training set is used for constructing the database. Then all GO term annotations of the best hit in the database are assigned to the query protein in the test set as the predicted functions. Since BLAST cannot assign probabilities for each prediction, we use Precision, Recall, and F1 score as the evaluation metrics instead of the M-AUPR, m-AUPR, and F-max we used in the previous section. For Graph2GO, we use the same training and test sets and use 0.5 as the threshold to assign predicted labels.</p>
      <p>As shown in Table <xref rid="tbl2" ref-type="table">2</xref>, in terms of all metrics, Graph2GO outperforms BLAST by a large margin across all three ontologies under both comparison conditions, especially for CC and BP, proving the superiority of our model. We can see that Graph2GO can achieve significantly higher Precision than BLAST under the full data set condition. BLAST results in many false positives and low Precision because of predicting functions simply based on amino acid sequence information but ignoring other informative features. Besides, it is shown that BLAST can obtain reasonable performance in MF ontology, which demonstrates that amino acid sequence information is highly related to the functions in MF ontology again. After removing highly similar sequences of the test set in the training set, the performances of both methods drop accordingly, while Graph2GO shows a smaller drop and thus outperforms BLAST even more under the partial data set condition. For BLAST, the performance drops from the full data set condition to the partial data set condition in terms of F1 score are 27%, 32%, and 37% for CC, MF, and BP, respectively. As for Graph2GO, the F1 score drops to 9%, 25%, and 17% for CC, MF, and BP, respectively, which are significantly lower scores than BLAST, exhibiting a more robust performance.</p>
      <table-wrap id="tbl2" orientation="portrait" position="float">
        <label>Table 2:</label>
        <caption>
          <p>Performance comparison between BLAST and Graph2GO in terms of precision, recall, and F1 score under two conditions (full data set and partial data set after removing homologs)</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th colspan="3" align="center" rowspan="1">CC</th>
              <th colspan="3" align="center" rowspan="1">MF</th>
              <th colspan="3" align="center" rowspan="1">BP</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Method</th>
              <th rowspan="1" colspan="1">Precision</th>
              <th rowspan="1" colspan="1">Recall</th>
              <th rowspan="1" colspan="1">F1 Score</th>
              <th rowspan="1" colspan="1">Precision</th>
              <th rowspan="1" colspan="1">Recall</th>
              <th rowspan="1" colspan="1">F1 Score</th>
              <th rowspan="1" colspan="1">Precision</th>
              <th rowspan="1" colspan="1">Recall</th>
              <th rowspan="1" colspan="1">F1 Score</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">BLAST (full)</td>
              <td rowspan="1" colspan="1">0.540</td>
              <td rowspan="1" colspan="1">0.589</td>
              <td rowspan="1" colspan="1">0.564</td>
              <td rowspan="1" colspan="1">0.633</td>
              <td rowspan="1" colspan="1">0.712</td>
              <td rowspan="1" colspan="1">0.670</td>
              <td rowspan="1" colspan="1">0.373</td>
              <td rowspan="1" colspan="1">0.427</td>
              <td rowspan="1" colspan="1">0.398</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BLAST (partial)</td>
              <td rowspan="1" colspan="1">0.396</td>
              <td rowspan="1" colspan="1">0.429</td>
              <td rowspan="1" colspan="1">0.412</td>
              <td rowspan="1" colspan="1">0.421</td>
              <td rowspan="1" colspan="1">0.490</td>
              <td rowspan="1" colspan="1">0.453</td>
              <td rowspan="1" colspan="1">0.234</td>
              <td rowspan="1" colspan="1">0.271</td>
              <td rowspan="1" colspan="1">0.251</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Graph2GO (full)</td>
              <td rowspan="1" colspan="1">0.766</td>
              <td rowspan="1" colspan="1">0.624</td>
              <td rowspan="1" colspan="1">0.688</td>
              <td rowspan="1" colspan="1">0.816</td>
              <td rowspan="1" colspan="1">0.639</td>
              <td rowspan="1" colspan="1">0.717</td>
              <td rowspan="1" colspan="1">0.686</td>
              <td rowspan="1" colspan="1">0.354</td>
              <td rowspan="1" colspan="1">0.467</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Graph2GO (partial)</td>
              <td rowspan="1" colspan="1">0.731</td>
              <td rowspan="1" colspan="1">0.549</td>
              <td rowspan="1" colspan="1">0.627</td>
              <td rowspan="1" colspan="1">0.698</td>
              <td rowspan="1" colspan="1">0.448</td>
              <td rowspan="1" colspan="1">0.545</td>
              <td rowspan="1" colspan="1">0.595</td>
              <td rowspan="1" colspan="1">0.286</td>
              <td rowspan="1" colspan="1">0.387</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="req-159527268104837360">
            <p>We use 0.5 as the threshold to assign predicted labels for Graph2GO. It should be noted that the F1 score here is different from the F-max used previously, since the threshold is prespecified for Graph2GO and there is no threshold for BLAST.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="sec3-5">
      <title>Comparison with other network-based methods</title>
      <p>To evaluate the performance of our method, we compare our method with two popular protein function prediction methods: Mashup and deepNF, which are also based on the concept of embedding. In their initial implementations, Mashup and deepNF use support vector machine (SVM) as the classification model. In order to make the performance more comparable, we also use DNN that is used by Graph2GO as their classification models. Each method is evaluated using 5-fold cross-validation, repeated 10 times. We first compare the performance using all GO terms, as shown in Fig. <xref ref-type="fig" rid="fig3">3</xref>. We also group GO terms into three functional categories based on the sparsity levels for each ontology, each containing GO terms with 11-30, 31-100, and 101-300 proteins, and show the detailed comparisons in <xref ref-type="supplementary-material" rid="sup10">Supplementary Figures S2</xref>, <xref ref-type="supplementary-material" rid="sup10">S3</xref>, and <xref ref-type="supplementary-material" rid="sup10">S4</xref>.</p>
      <fig id="fig3" orientation="portrait" position="float">
        <label>Figure 3:</label>
        <caption>
          <p>Performance comparisons with other network-based methods. Graph2GO is compared with Mashup and deepNF in terms of three metrics: micro-AUPR, macro-AUPR, and F-max. The figure shows the comparison results on (<bold>a</bold>) CC, (<bold>b</bold>) MF, and (<bold>c</bold>) BP ontology. Each method is evaluated using 5-fold cross-validation, repeated 10 times to calculate the confidence interval.</p>
        </caption>
        <graphic xlink:href="giaa081fig3"/>
      </fig>
      <p>For both CC and MF ontology, our model outperforms deepNF and Mashup in terms of m-AUPR and F-max, and achieves comparable results with deepNF and Mashup in terms of M-AUPR. Our model is comparable to Mashup and outperforms deepNF in BP ontology in terms of m-AUPR and F-max and is slightly worse than Mashup in M-AUPR. Looking at the detailed comparison in three sparsity levels shown in <xref ref-type="supplementary-material" rid="sup10">Supplementary Figure S2</xref>, our model can achieve the best performance consistently for MF ontology, across all sparsity levels. As we discussed above, the MF ontology has a great correlation with sequence information. Unlike Mashup and deepNF, which only consider PPI networks, Graph2GO explicitly includes sequence information in the model (SSN and sequence in the node attribute), which improves the performance of our model. As for CC ontology, Graph2GO can achieve the best performance for the most general categories (i.e., annotating between 101 and 300 proteins). For GO terms annotated between 11 and 100 proteins, our method is not as good as Mashup and deepNF. For BP ontology, as the number of annotations per GO term increases, the performance of Graph2GO gradually improves and becomes comparable with other two methods.</p>
      <p>It can be observed that Graph2GO works best for GO terms with enough annotated proteins, and is not as good as Mashup for very specific GO terms with few annotations for CC and BP ontology. In summary, our model can achieve state-of-the-art performance in both CC and MF ontology, especially as the number of annotated proteins increases, and can obtain comparable results in BP ontology. Compared with Mashup and deepNF, we can conclude that both the inclusion of discriminating node attributes and the power of attribute propagation across two types of networks enable Graph2GO to obtain competing results.</p>
    </sec>
    <sec id="sec3-6">
      <title>Performance on other species</title>
      <p>In order to demonstrate the power and robustness of Graph2GO, we downloaded data from SwissProt and the STRING database for another 5 species (fruit fly, mouse, rat, <italic>Saccharomyces cerevisiae</italic>, and <italic>Bacillus subtilis</italic>), and tested the performance of Graph2GO on these species. For each species, we followed the same procedure as when we tested on a human data set, and the performance was evaluated in each species individually. Table <xref rid="tbl3" ref-type="table">3</xref> shows the results in terms of macro-AUPR, micro-AUPR, and F-max. We observe that Graph2GO can achieve consistently decent performance on all 5 species in terms of these evaluation metrics, which proves the robustness of our method. It should be noted that the performance on <italic>Saccharomyces cerevisiae</italic> and <italic>Bacillus subtilis</italic> is a lot better than on other species.</p>
      <table-wrap id="tbl3" orientation="portrait" position="float">
        <label>Table 3:</label>
        <caption>
          <p>Performance on other 5 species in terms of macro-AUPR, micro-AUPR, and F-max metrics</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th colspan="3" align="center" rowspan="1">CC</th>
              <th colspan="3" align="center" rowspan="1">MF</th>
              <th colspan="3" align="center" rowspan="1">BP</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Species</th>
              <th rowspan="1" colspan="1">M-AUPR</th>
              <th rowspan="1" colspan="1">m-AUPR</th>
              <th rowspan="1" colspan="1">F-max</th>
              <th rowspan="1" colspan="1">M-AUPR</th>
              <th rowspan="1" colspan="1">m-AUPR</th>
              <th rowspan="1" colspan="1">F-max</th>
              <th rowspan="1" colspan="1">M-AUPR</th>
              <th rowspan="1" colspan="1">m-AUPR</th>
              <th rowspan="1" colspan="1">F-max</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Human</td>
              <td rowspan="1" colspan="1">0.494</td>
              <td rowspan="1" colspan="1">0.751</td>
              <td rowspan="1" colspan="1">0.686</td>
              <td rowspan="1" colspan="1">0.560</td>
              <td rowspan="1" colspan="1">0.761</td>
              <td rowspan="1" colspan="1">0.718</td>
              <td rowspan="1" colspan="1">0.284</td>
              <td rowspan="1" colspan="1">0.488</td>
              <td rowspan="1" colspan="1">0.490</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Fruit fly</td>
              <td rowspan="1" colspan="1">0.570</td>
              <td rowspan="1" colspan="1">0.764</td>
              <td rowspan="1" colspan="1">0.729</td>
              <td rowspan="1" colspan="1">0.599</td>
              <td rowspan="1" colspan="1">0.710</td>
              <td rowspan="1" colspan="1">0.693</td>
              <td rowspan="1" colspan="1">0.388</td>
              <td rowspan="1" colspan="1">0.531</td>
              <td rowspan="1" colspan="1">0.530</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Mouse</td>
              <td rowspan="1" colspan="1">0.480</td>
              <td rowspan="1" colspan="1">0.743</td>
              <td rowspan="1" colspan="1">0.681</td>
              <td rowspan="1" colspan="1">0.583</td>
              <td rowspan="1" colspan="1">0.763</td>
              <td rowspan="1" colspan="1">0.710</td>
              <td rowspan="1" colspan="1">0.262</td>
              <td rowspan="1" colspan="1">0.482</td>
              <td rowspan="1" colspan="1">0.483</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Rat</td>
              <td rowspan="1" colspan="1">0.461</td>
              <td rowspan="1" colspan="1">0.701</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.577</td>
              <td rowspan="1" colspan="1">0.743</td>
              <td rowspan="1" colspan="1">0.699</td>
              <td rowspan="1" colspan="1">0.267</td>
              <td rowspan="1" colspan="1">0.429</td>
              <td rowspan="1" colspan="1">0.448</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>Saccharomyces cerevisiae</italic>
              </td>
              <td rowspan="1" colspan="1">0.677</td>
              <td rowspan="1" colspan="1">0.848</td>
              <td rowspan="1" colspan="1">0.781</td>
              <td rowspan="1" colspan="1">0.597</td>
              <td rowspan="1" colspan="1">0.740</td>
              <td rowspan="1" colspan="1">0.689</td>
              <td rowspan="1" colspan="1">0.481</td>
              <td rowspan="1" colspan="1">0.647</td>
              <td rowspan="1" colspan="1">0.620</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>Bacillus subtilis</italic>
              </td>
              <td rowspan="1" colspan="1">0.709</td>
              <td rowspan="1" colspan="1">0.860</td>
              <td rowspan="1" colspan="1">0.803</td>
              <td rowspan="1" colspan="1">0.620</td>
              <td rowspan="1" colspan="1">0.742</td>
              <td rowspan="1" colspan="1">0.693</td>
              <td rowspan="1" colspan="1">0.577</td>
              <td rowspan="1" colspan="1">0.665</td>
              <td rowspan="1" colspan="1">0.651</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec3-7">
      <title>Graph2GO web server</title>
      <p>We also developed a web server based on Shiny app in R that supports protein function query on-the-fly: <ext-link ext-link-type="uri" xlink:href="https://integrativeomics.shinyapps.io/graph2go/">https://integrativeomics.shinyapps.io/graph2go/</ext-link>. Currently, the web server supports 15,133 human proteins that are trained by our model. Network information (SSN and PPI network) and node attributes (subcellular location and protein domains) are displayed in our web server. The function prediction results are ranked by the probability score for each ontology separately, and users can specify the threshold to control the confidence of the results.</p>
      <p>A promising application of our model is to utilize the latent representations to perform downstream analyses, since the generated representations already summarize various kinds of informative features. We support downstream clustering analysis for the query protein, where similarities are measured based on latent representations. Most similar proteins can be detected for the query protein to constitute a network module. A GO enrichment analysis can be performed on all proteins in the network module to analyze functions of the module. In the future, we are going to extend the server to include more proteins from other species.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec4">
    <title>Discussion</title>
    <sec id="sec4-1">
      <title>Feature transfer for solving annotation bias</title>
      <p>A challenge in protein function prediction is the lack of annotated features for proteins, which are crucial for machine learning models. This is due to the bias in the experimental annotation of proteins and has impeded the development of biomedical research to some extent [<xref rid="bib43" ref-type="bibr">42</xref>, <xref rid="bib44" ref-type="bibr">43</xref>]. In our experiment on the human data set, among 15,133 proteins, 5,716 proteins lack annotation of protein domains, while 2,364 lack annotation of subcellular location. Even with lots of missing values, when only using protein domains or subcellular location as node attributes, Graph2GO can still achieve reasonable results, as shown in Table <xref rid="tbl1" ref-type="table">1</xref>. To demonstrate the power of Graph2GO to predict on these sparsely annotated proteins, we divide the test set into two groups: the sparsity group (without any subcellular location or protein domain annotation) and the non-sparsity group. We compare Graph2GO with a convolutional neural network (CNN) model. This CNN model uses the same node attributes as Graph2GO, but does not utilize networks to propagate protein attributes on these two groups separately. The detailed description of this CNN model is in the <xref ref-type="supplementary-material" rid="sup10">Supplementary Materials</xref>. The comparison results are shown in <xref ref-type="supplementary-material" rid="sup10">Supplementary Table S3</xref>. It is anticipated that the performance on the sparsity group should be worse than that of the non-sparsity group, due to the lack of features. We calculate the ratio between the performance of the sparsity and non-sparsity groups to measure how well Graph2GO and CNN handle the lack of features. The averaged ratios in terms of F-max for CNN are 74%, 44%, and 73% for CC, MF, and BP, respectively, while for Graph2GO, the averaged ratio are 101%, 84%, and 111%, respectively. We can see that when lacking features, the performance of CNN would drop significantly compared to the non-sparsity group. However, the performance of Graph2GO is much more stable when facing a lack of features. In some cases, the performance in the sparsity group is even better than the non-sparsity group. This is due to the feature transfer of Graph2GO, where node attributes are transferred between nodes in the graph to augment the information of each protein. Since proteins are connected in the interaction network, nodes without any attributes can still update their representations based on attributes of neighboring nodes, which solves the problem of missing values to some extent.</p>
      <p>The encoding of original features into the model is of high importance to Graph2GO, not only for preserving information as much as possible, but also for feature transfer. We represent protein domains and subcellular location using the bag-of-words encoding method, which best preserves the original information and supports the addition of features that the feature transfer relies on. As for protein sequence, the CT encoding method loses sequential information and is not well suited for the addition operation. To solve this, we also utilize the similarity network to represent the sequence information, and we discover that the inclusion of both types of sequence representations is better than either of them alone, which might indicate that they can provide some complementary information.</p>
    </sec>
    <sec id="sec4-2">
      <title>Extensibility and future work</title>
      <p>Compared to other methods that also use multiple protein features [<xref rid="bib13" ref-type="bibr">13</xref>, <xref rid="bib14" ref-type="bibr">14</xref>, <xref rid="bib17" ref-type="bibr">17</xref>], an advantage of our method is that it is convenient to incorporate other function-related information. Within the network architecture, all the features are regarded as node attributes and can be treated equally. In this paper, we found the integration of all these node features and the network information provides the best prediction performance. The model can be easily extended to take into account additional information, such as post-translational modifications, protein structure, or any other protein features. It’s also worth incorporating more networks and finding a better way to integrate them. For example, we may use the seven channels in the STRING database separately instead of using the combined PPI network, as adopted by Mashup.</p>
      <p>The architecture we proposed in this paper can not only be used to predict protein functions, but can also be applied to other tasks that involve biological networks, since the learned embeddings are informative and can be used as feature inputs for various downstream machine learning tasks. For example, one can predict the interactions between any two proteins, predict new therapeutic applications of an existing drug, or run a clustering algorithm to cluster genes into modules based on learned embeddings.</p>
      <p>In principle, predicting GO terms is a hierarchical classification problem, since GO terms are organized in a hierarchy and are interacting with each other [<xref rid="bib45" ref-type="bibr">44</xref>]. Currently, Graph2GO assumes the independence of all GO terms, which might cause inconsistencies between the prediction of leaf GO terms and the corresponding parent GO terms. In the future, we will consider adding some constraints on the output layer of our classification model to force the prediction to be consistent between leaf nodes and parent nodes. We will also try to incorporate the GO hierarchy into the model as 1 of the network structures to inform the training process.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec5">
    <title>Conclusion</title>
    <p>In this work, we present Graph2GO, a graph-based deep learning model that can make use of heterogeneous information in a unified way to predict protein functions. Our network representation learning method can take into account both network structure and protein attributes, to make predictions by organizing proteins into an attributed network architecture. Graph2GO achieves state-of-the-art performance on the benchmark data set and can easily be adapted to solve other tasks involving biological networks, such as link prediction, node classification, and sub-network discovery.</p>
  </sec>
  <sec id="sec6">
    <title>Availability of source code and requirements</title>
    <list list-type="bullet">
      <list-item>
        <p>Project name: Graph2GO</p>
      </list-item>
      <list-item>
        <p>Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/yanzhanglab/Graph2GO">https://github.com/yanzhanglab/Graph2GO</ext-link></p>
      </list-item>
      <list-item>
        <p>Operating system(s): Platform independent</p>
      </list-item>
      <list-item>
        <p>Programming language: Python, R</p>
      </list-item>
      <list-item>
        <p>Other requirements: not applicable</p>
      </list-item>
      <list-item>
        <p>License: MIT</p>
      </list-item>
      <list-item>
        <p>RRID: SCR_018726</p>
      </list-item>
      <list-item>
        <p>biotoolsID: biotools:graph2go</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="materials" id="sec7">
    <title>Availability of supporting data and materials</title>
    <p>All data are publicly available in our Github project home page. An archival copy of the code and supporting data are available via the <italic>GigaScience</italic> database, GigaDB [<xref rid="bib46" ref-type="bibr">45</xref>].</p>
  </sec>
  <sec id="sec9">
    <title>Abbreviations</title>
    <p>AUPR: area under the precision-recall curve; BLAST: Basic Local Alignment and Search Tool; BP: biological process; CAFA: Critical Assessment of protein Function Annotation algorithms; CC: cellular component; CNN: convolutional neural network; CT: conjoint triad; DNN: deep neural network; GCN: graph convolutional network; GO: gene ontology; GPU: graphics processing unit; MF: molecular function; M-AUPR: macro-averaged area under the precision-recall curve; m-AUPR: micro-averaged area under the precision-recall curve; PPI: protein-protein interaction; ROC: receiver operating characteristic; SSN: sequence similarity network; SVM: support vector machine; VAE: variational auto-encoder; VGAE: variational graph auto-encoder.</p>
  </sec>
  <sec id="sec11">
    <title>Competing Interests</title>
    <p>Y.G. serves as a consultant at Eli Lilly and Company and a consultant at Merck Group.</p>
  </sec>
  <sec id="sec12">
    <title>Funding</title>
    <p>This work was partially supported by The Ohio State University Startup Funds to Y.Z.</p>
  </sec>
  <sec id="sec13">
    <title>Authors' contributions</title>
    <p>Y.Z. and Y.G. conceived the project and supervised the study. K.F. performed the analysis and developed the tools. K.F. and Y.Z. drafted the manuscript. All authors read and approved the final manuscript.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>giaa081_GIGA-D-19-00430_Original_Submission</label>
      <media xlink:href="giaa081_giga-d-19-00430_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup2">
      <label>giaa081_GIGA-D-19-00430_Revision_1</label>
      <media xlink:href="giaa081_giga-d-19-00430_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup3">
      <label>giaa081_GIGA-D-19-00430_Revision_2</label>
      <media xlink:href="giaa081_giga-d-19-00430_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup4">
      <label>giaa081_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xlink:href="giaa081_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup5">
      <label>giaa081_Response_to_Reviewer_Comments_Revision_1</label>
      <media xlink:href="giaa081_response_to_reviewer_comments_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup6">
      <label>giaa081_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Iddo Friedberg -- 1/21/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa081_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup7">
      <label>giaa081_Reviewer_1_Report_Revision_1</label>
      <caption>
        <p>Iddo Friedberg -- 5/29/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa081_reviewer_1_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup8">
      <label>giaa081_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Karin Verspoor -- 1/24/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa081_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup9">
      <label>giaa081_Reviewer_2_Report_Revision_1</label>
      <caption>
        <p>Karin Verspoor -- 5/13/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa081_reviewer_2_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup10">
      <label>giaa081_Supplemental_File</label>
      <media xlink:href="giaa081_supplemental_file.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>We acknowledge the use of the compute resources of the Ohio Supercomputer Center.</p>
  </ack>
  <sec id="h1content1595272160967" sec-type="supplementary-material">
    <title>Supplementary Data</title>
    <p>There is an additional Supplementary Materials file containing additional information. It includes the following figures and tables:</p>
    <p><bold>Supplementary Figure 1</bold>. Cross-validation results of choosing different number of layers for the GCN model. The performances on all three ontologies are best when using two layers of graph convolutional operators. As the number of layers becomes larger the performance gets worse, because applying a graph convolutional operator repeatedly may mix node features from different clusters and make them indistinguishable.</p>
    <p><bold>Supplementary Figure 2</bold>. Performance comparison with other state-of-the-art methods in CC ontology for three different sparsity levels. Graph2GO is compared with Mashup and deepNF in terms of three metrics: micro-AUPR, macro-AUPR, and F-max. There, three figures show the results of 3 sparsity levels: [11-30], [31-100], and [101-300]. Each method is evaluated using 5-fold cross validation, repeated 10 times to calculate the confidence interval.</p>
    <p><bold>Supplementary Figure 3</bold>. Performance comparison with other state-of-the-art methods in MF ontology for three different sparsity levels. Graph2GO is compared with Mashup and deepNF in terms of three metrics: micro-AUPR, macro-AUPR, and F-max. There, three figures show the results of three sparsity levels: [11-30], [31-100], and [101-300]. Each method is evaluated using 5-fold cross validation, repeated 10 times to calculate the confidence interval.</p>
    <p><bold>Supplementary Figure 4</bold>. Performance comparison with other state-of-the-art methods in BP ontology for three different sparsity levels. Graph2GO is compared with Mashup and deepNF in terms of three metrics: micro-AUPR, macro-AUPR, and F-max. There, three figures show the results of three sparsity levels: [11-30], [31-100], and [101-300]. Each method is evaluated using 5-fold cross validation, repeated 10 times to calculate the confidence interval.</p>
    <p><bold>Supplementary Table 1</bold>. Classification of amino acids according to their dipoles and volumes of the side chains based on the CT method.</p>
    <p><bold>Supplementary Table 2</bold>. Performance between the model where we train an independent VGAE for each network and combine their embeddings (referred to as “individual”) and where we first combine the networks and train one VGAE to obtain the overall embedding (referred to as “combined”) in terms of M-AUPR, m-AUPR, and F-max. The experiment procedure is the same as other experiments performed in the “Results” section and is described there.</p>
    <p><bold>Supplementary Table 3</bold>. Performance of Graph2GO and CNN on sparsity group and non-sparsity group of the test set in terms of F1 score. Here, “sp” means the sparsity group, while “non” means the non-sparsity group.</p>
  </sec>
  <ref-list id="ref1">
    <title>REFERENCES</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Radivojac</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Clark</surname><given-names>WT</given-names></name>, <name name-style="western"><surname>Oron</surname><given-names>TR</given-names></name>, <etal>et al.</etal></person-group><article-title>A large-scale evaluation of computational protein function prediction</article-title>. <source>Nat Methods</source>. <year>2013</year>;<volume>10</volume>(<issue>3</issue>):<fpage>221</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">23353650</pub-id></mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Raimondo</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Redfern</surname><given-names>OC</given-names></name>, <etal>et al.</etal></person-group><article-title>Protein function annotation by homology-based inference</article-title>. <source>Genome Biol</source>. <year>2009</year>;<volume>10</volume>(<issue>2</issue>):<fpage>207</fpage>.<pub-id pub-id-type="pmid">19226439</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Piovesan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Luigi Martelli</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Fariselli</surname><given-names>P</given-names></name><etal>et al.</etal></person-group><article-title>BAR-PLUS: the Bologna Annotation Resource Plus for functional and structural annotation of protein sequences</article-title>. <source>Nucleic Acids Res</source>. <year>2011</year>;<volume>39</volume>(<issue>Suppl 2</issue>):<fpage>W197</fpage>–<lpage>202</lpage>.<pub-id pub-id-type="pmid">21622657</pub-id></mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chitale</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hawkins</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Park</surname><given-names>C</given-names></name>, <etal>et al.</etal></person-group><article-title>ESG: extended similarity group method for automated protein function prediction</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>25</volume>(<issue>14</issue>):<fpage>1739</fpage>–<lpage>45</lpage>.<pub-id pub-id-type="pmid">19435743</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jones</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Baumann</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>AL.</given-names></name></person-group><article-title>Automated methods of predicting the function of biological sequences using GO and BLAST</article-title>. <source>BMC Bioinformatics</source>. <year>2005</year>;<volume>6</volume>(<issue>1</issue>):<fpage>272</fpage>.<pub-id pub-id-type="pmid">16288652</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Ulitsky</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Shamir</surname><given-names>R.</given-names></name></person-group><article-title>Network-based prediction of protein function</article-title>. <source>Mol Syst Biol</source>. <year>2007</year>;<volume>3</volume>(<issue>1</issue>):<fpage>88</fpage>.<pub-id pub-id-type="pmid">17353930</pub-id></mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chua</surname><given-names>HN</given-names></name>, <name name-style="western"><surname>Sung</surname><given-names>WK</given-names></name>, <name name-style="western"><surname>Wong</surname><given-names>L.</given-names></name></person-group><article-title>Using indirect protein interactions for the prediction of gene ontology functions</article-title>. <source>BMC Bioinformatics</source>. <year>2007</year>;8(Suppl 4):<fpage>S8</fpage>.</mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Forslund</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Sonnhammer</surname><given-names>EL.</given-names></name></person-group><article-title>Predicting protein function from domain content</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>15</issue>):<fpage>1681</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">18591194</pub-id></mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Rentzsch</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Orengo</surname><given-names>CA.</given-names></name></person-group><article-title>Protein function prediction using domain families</article-title>. <source>BMC Bioinformatics</source>. <year>2013</year>;14(Suppl 3):<fpage>S5</fpage>.</mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Redfern</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Orengo</surname><given-names>C.</given-names></name></person-group><article-title>Predicting protein function from sequence and structure</article-title>. <source>Nat Rev Mol Cell Biol</source>. <year>2007</year>;<volume>8</volume>(<issue>12</issue>):<fpage>995</fpage>–<lpage>1005</lpage>.<pub-id pub-id-type="pmid">18037900</pub-id></mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jensen</surname><given-names>LJ</given-names></name>, <name name-style="western"><surname>Gupta</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Blom</surname><given-names>N</given-names></name><etal>et al.</etal></person-group><article-title>Prediction of human protein function from post-translational modifications and localization features</article-title>. <source>J Mol Biol</source>. <year>2002</year>;<volume>319</volume>(<issue>5</issue>):<fpage>1257</fpage>–<lpage>65</lpage>.<pub-id pub-id-type="pmid">12079362</pub-id></mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Verspoor</surname><given-names>KM.</given-names></name></person-group><article-title>Roles for text mining in protein function prediction</article-title>, <source>Methods Mol Biol</source>. <year>2014</year>;<volume>1159</volume>:<fpage>95</fpage>–<lpage>108</lpage>.<pub-id pub-id-type="pmid">24788263</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Piovesan</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Giollo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Leonardi</surname><given-names>E</given-names></name><etal>et al.</etal></person-group><article-title>INGA: protein function prediction combining interaction networks, domain assignments and sequence similarity</article-title>. <source>Nucleic Acids Res</source>. <year>2015</year>;<volume>43</volume>(<issue>W1</issue>):<fpage>W134</fpage>–<lpage>40</lpage>.<pub-id pub-id-type="pmid">26019177</pub-id></mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Freddolino</surname><given-names>PL</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><article-title>COFACTOR: improved protein function prediction by combining structure, sequence and protein–protein interaction information</article-title>. <source>Nucleic Acids Res</source>. <year>2017</year>;<volume>45</volume>(<issue>W1</issue>):<fpage>W291</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">28472402</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boutet</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Lieberherr</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tognolli</surname><given-names>M</given-names></name>, <etal>et al.</etal></person-group><article-title>UniProtKB/Swiss-Prot, the manually annotated section of the UniProt KnowledgeBase: how to use the entry view</article-title>, <source>Methods Mol Biol</source>, <year>2016</year>;<volume>1374</volume>:<fpage>23</fpage>–<lpage>54</lpage>.<pub-id pub-id-type="pmid">26519399</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Szklarczyk</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Morris</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Cook</surname><given-names>H</given-names></name>, <etal>et al.</etal></person-group><article-title>The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible</article-title>. <source>Nucleic Acids Res</source>. <year>2017</year>;<volume>45</volume>:<issue>(D1)</issue>:<fpage>D362</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">27924014</pub-id></mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kulmanov</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Khan</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Hoehndorf</surname><given-names>R.</given-names></name></person-group><article-title>DeepGO: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>4</issue>):<fpage>660</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">29028931</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kearnes</surname><given-names>S</given-names></name>, <name name-style="western"><surname>McCloskey</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Berndl</surname><given-names>M</given-names></name>, <etal>et al.</etal></person-group><article-title>Molecular graph convolutions: moving beyond fingerprints</article-title>. <source>J Comput Aided Mol Des</source>. <year>2016</year>;<volume>30</volume>(<issue>8</issue>):<fpage>595</fpage>–<lpage>608</lpage>.<pub-id pub-id-type="pmid">27558503</pub-id></mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Backstrom</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Leskovec</surname><given-names>J.</given-names></name></person-group><article-title>Supervised random walks: predicting and recommending links in social networks</article-title>, <comment>United States Conference: WSDM'11: Fourth ACM International Conference on Web Search and Data Mining Hong Kong China February</comment>, <year>2011</year> p.<fpage>635</fpage>–<lpage>44</lpage>., <publisher-name>Association for Computing Machinery</publisher-name>: <publisher-loc>New York, NY</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Duvenaud</surname><given-names>DK</given-names></name>, <name name-style="western"><surname>Maclaurin</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Iparraguirre</surname><given-names>J</given-names></name><etal>et al.</etal></person-group><article-title>Convolutional networks on graphs for learning molecular fingerprints</article-title>. <source>Adv Neural Inf Process Syst</source>28 (NIPS <year>2015</year>):<fpage>2224</fpage>–<lpage>32</lpage>.. <ext-link ext-link-type="uri" xlink:href="https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints">https://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hamilton</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Ying</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Leskovec</surname><given-names>J.</given-names></name></person-group><article-title>Inductive representation learning on large graphs</article-title>. <source>Adv Neural Inf Process Syst</source>30 (NIPS <year>2017</year>):<fpage>1024</fpage>–<lpage>34</lpage>.. <comment>https://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs</comment>.</mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cho</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Berger</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Peng</surname><given-names>J.</given-names></name></person-group><article-title>Compact integration of multi-network topology for functional analysis of genes</article-title>. <source>Cell Syst</source>. <year>2016</year>;<volume>3</volume>(<issue>6</issue>):<fpage>540</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">27889536</pub-id></mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gligorijević</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Barot</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bonneau</surname><given-names>R.</given-names></name></person-group><article-title>deepNF: deep network fusion for protein function prediction</article-title>. <source>Bioinformatics</source>. <year>2018</year>;<volume>34</volume>(<issue>22</issue>):<fpage>3873</fpage>–<lpage>81</lpage>.<pub-id pub-id-type="pmid">29868758</pub-id></mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bhagat</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Cormode</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Muthukrishnan</surname><given-names>S.</given-names></name></person-group><article-title>Node classification in social networks</article-title>. In: <source>Social network data analytics.</source><year>2011</year>;<fpage>115</fpage>–<lpage>48</lpage>., <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vishwanathan</surname><given-names>SVN</given-names></name>, <name name-style="western"><surname>Schraudolph</surname><given-names>NN</given-names></name>, <name name-style="western"><surname>Kondor</surname><given-names>R</given-names></name><etal>et al.</etal></person-group><article-title>Graph kernels</article-title>. <source>J Mach Learn Res</source>. <year>2010</year>;<volume>11</volume>(<issue>Apr</issue>):<fpage>1201</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamilton</surname><given-names>WL</given-names></name>, <name name-style="western"><surname>Ying</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Leskovec</surname><given-names>J.</given-names></name></person-group><article-title>Representation learning on graphs: methods and applications</article-title>, <source>IEEE Data Engineering Bulletin</source>. <year>2017</year>;<volume>40</volume>:<issue>(3</issue>):<fpage>52</fpage>–<lpage>74</lpage>.</mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ashburner</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ball</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Blake</surname><given-names>JA</given-names></name><etal>et al.</etal></person-group><article-title>Gene ontology: tool for the unification of biology</article-title>. <source>Nat Genet</source>. <year>2000</year>;<volume>25</volume>(<issue>1</issue>):<fpage>25</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">10802651</pub-id></mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Altschul</surname><given-names>SF</given-names></name>, <name name-style="western"><surname>Madden</surname><given-names>TL</given-names></name>, <name name-style="western"><surname>Schäffer</surname><given-names>AA</given-names></name>, <etal>et al.</etal></person-group><article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>. <source>Nucleic Acids Res</source>. <year>1997</year>;<volume>25</volume>(<issue>17</issue>):<fpage>3389</fpage>–<lpage>402</lpage>.<pub-id pub-id-type="pmid">9254694</pub-id></mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Luo</surname><given-names>X</given-names></name>, <etal>et al.</etal></person-group><article-title>Predicting protein–protein interactions based only on sequences information</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2007</year>;<volume>104</volume>(<issue>11</issue>):<fpage>4337</fpage>–<lpage>41</lpage>.<pub-id pub-id-type="pmid">17360525</pub-id></mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Muppirala</surname><given-names>UK</given-names></name>, <name name-style="western"><surname>Honavar</surname><given-names>VG</given-names></name>, <name name-style="western"><surname>Dobbs</surname><given-names>D.</given-names></name></person-group><article-title>Predicting RNA-protein interactions using only sequence information</article-title>. <source>BMC Bioinformatics</source>. <year>2011</year>;<volume>12</volume>(<issue>1</issue>):<fpage>489</fpage>.<pub-id pub-id-type="pmid">22192482</pub-id></mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>ZH</given-names></name>, <name name-style="western"><surname>Lei</surname><given-names>YK</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>L</given-names></name><etal>et al.</etal></person-group><article-title>Prediction of protein-protein interactions from amino acid sequences with ensemble extreme learning machines and principal component analysis</article-title>. <source>BMC Bioinformatics</source>. <year>2013</year>;14 (Suppl 8):<fpage>S10</fpage>.</mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>X.</given-names></name></person-group><article-title>Accurate prediction of nuclear receptors with conjoint triad feature</article-title>. <source>BMC Bioinformatics</source>. <year>2015</year>;<volume>16</volume>(<issue>1</issue>):<fpage>402</fpage>.<pub-id pub-id-type="pmid">26630876</pub-id></mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Jia</surname><given-names>L</given-names></name><etal>et al.</etal></person-group><article-title>Protein-protein interactions prediction using a novel local conjoint triad descriptor of amino acid sequences</article-title>. <source>Int J Mol Sci</source>. <year>2017</year>;<volume>18</volume>(<issue>11</issue>):<fpage>2373</fpage>.</mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kipf</surname><given-names>TN</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group><article-title>Variational graph auto-encoders</article-title>, <year>2016</year><comment>Bayesian Deep Learning Workshop (NIPS 2016), arXiv preprint (arXiv:161107308)</comment>.</mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kipf</surname><given-names>TN</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group><article-title>Semi-supervised classification with graph convolutional networks</article-title>, <year>2016</year><comment>arXiv preprint (arXiv:160902907)</comment>. ICLR 2017.</mixed-citation>
    </ref>
    <ref id="bib36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group><article-title>Auto-encoding variational Bayes</article-title>, <year>2013</year><comment>arXiv preprint (arXiv:13126114)</comment>.</mixed-citation>
    </ref>
    <ref id="bib37">
      <label>37.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Han</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>XM.</given-names></name></person-group><article-title>Deeper insights into graph convolutional networks for semi-supervised learning</article-title>. In: <source>Programs and Abstracts of the Thirty-Second AAAI Conference on Artificial Intelligence</source>; <year>2018</year> p. <fpage>3538</fpage>–<lpage>45</lpage>., <publisher-name>AAAI</publisher-name>, <publisher-loc>New Orleans, Louisiana, USA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="bib40">
      <label>39.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Glorot</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><article-title>Understanding the difficulty of training deep feedforward neural networks</article-title>, <comment>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 13-15 May</comment>, <year>2010</year> vol. <volume>9</volume> p. <fpage>249</fpage>–<lpage>56</lpage>., <publisher-name>Chia Laguna Resort</publisher-name>, <publisher-loc>Sardinia, Italy</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib41">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Ba</surname><given-names>J.</given-names></name></person-group><article-title>Adam: a method for stochastic optimization</article-title>, <comment>Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego</comment>, <year>2015</year><comment>arXiv preprint (arXiv:14126980)</comment>.</mixed-citation>
    </ref>
    <ref id="bib42">
      <label>41.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Davis</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Goadrich</surname><given-names>M.</given-names></name></person-group><article-title>The relationship between precision-recall and ROC curves</article-title>, <comment>ICML '06: Proceedings of the 23rd international conference on Machine learning June</comment>, <year>2006</year>, p. <fpage>233</fpage>–<lpage>40</lpage>., <publisher-name>Association for Computing Machinery</publisher-name>: <publisher-loc>Pittsburgh, Pennsylvania</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="bib43">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Haynes</surname><given-names>WA</given-names></name>, <name name-style="western"><surname>Tomczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Khatri</surname><given-names>P.</given-names></name></person-group><article-title>Gene annotation bias impedes biomedical research</article-title>. <source>Sci Rep</source>. <year>2018</year>;<volume>8</volume>(<issue>1</issue>):<fpage>1362</fpage>.<pub-id pub-id-type="pmid">29358745</pub-id></mixed-citation>
    </ref>
    <ref id="bib44">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schnoes</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Ream</surname><given-names>DC</given-names></name>, <name name-style="western"><surname>Thorman</surname><given-names>AW</given-names></name>, <etal>et al.</etal></person-group><article-title>Biases in the experimental annotations of protein function and their effect on our understanding of protein function space</article-title>. <source>PLOS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>5</issue>):<fpage>e1003063</fpage>.<pub-id pub-id-type="pmid">23737737</pub-id></mixed-citation>
    </ref>
    <ref id="bib45">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kahanda</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Funk</surname><given-names>CS</given-names></name>, <name name-style="western"><surname>Ullah</surname><given-names>F</given-names></name><etal>et al.</etal></person-group><article-title>A close look at protein function prediction evaluation protocols</article-title>. <source>GigaScience</source>. <year>2015</year>;<volume>14</volume>(<issue>4</issue>):<fpage>41</fpage>.</mixed-citation>
    </ref>
    <ref id="bib46">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Guan</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name></person-group><article-title>Supporting data for “Graph2GO: a multi-modal attributed network embedding method for inferring protein functions</article-title>", <year>2020</year><pub-id pub-id-type="doi">10.5524/100761</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
