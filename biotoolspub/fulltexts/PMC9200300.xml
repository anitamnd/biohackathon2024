<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9200300</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-21-37971</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0269449</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Fluorescence Imaging</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Thermodynamics</subject>
            <subj-group>
              <subject>Entropy</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Microscopy</subject>
          <subj-group>
            <subject>Light Microscopy</subject>
            <subj-group>
              <subject>Fluorescence Microscopy</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Earth Sciences</subject>
        <subj-group>
          <subject>Geology</subject>
          <subj-group>
            <subject>Petrology</subject>
            <subj-group>
              <subject>Sediment</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Earth Sciences</subject>
        <subj-group>
          <subject>Geology</subject>
          <subj-group>
            <subject>Sedimentary Geology</subject>
            <subj-group>
              <subject>Sediment</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Materials Science</subject>
          <subj-group>
            <subject>Materials</subject>
            <subj-group>
              <subject>Plastics</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MP-Net: Deep learning-based segmentation for fluorescence microscopy images of microplastics isolated from clams</article-title>
      <alt-title alt-title-type="running-head">MP-Net: Deep learning-based segmentation for microplastics</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9937-8617</contrib-id>
        <name>
          <surname>Park</surname>
          <given-names>Ho-min</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff006" ref-type="aff">
          <sup>6</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9854-0085</contrib-id>
        <name>
          <surname>Park</surname>
          <given-names>Sanghyeon</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="currentaff001" ref-type="author-notes">
          <sup>¤a</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5754-2041</contrib-id>
        <name>
          <surname>de Guzman</surname>
          <given-names>Maria Krishna</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Baek</surname>
          <given-names>Ji Yeon</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cirkovic Velickovic</surname>
          <given-names>Tanja</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="aff004" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="aff005" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8545-7437</contrib-id>
        <name>
          <surname>Van Messem</surname>
          <given-names>Arnout</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff007" ref-type="aff">
          <sup>7</sup>
        </xref>
        <xref rid="currentaff002" ref-type="author-notes">
          <sup>¤b</sup>
        </xref>
        <xref rid="econtrib001" ref-type="author-notes">
          <sup>‡</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8190-3839</contrib-id>
        <name>
          <surname>De Neve</surname>
          <given-names>Wesley</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff006" ref-type="aff">
          <sup>6</sup>
        </xref>
        <xref rid="econtrib001" ref-type="author-notes">
          <sup>‡</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Center for Biosystems and Biotech Data Science, Ghent University Global Campus, Incheon, South Korea</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Center for Food Chemistry and Technology, Ghent University Global Campus, Incheon, South Korea</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Department of Food Technology, Safety and Health, Ghent University, Ghent, Belgium</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>Department of Chemistry, University of Belgrade, Serbia</addr-line>
    </aff>
    <aff id="aff005">
      <label>5</label>
      <addr-line>Serbian Academy of Sciences and Arts, Belgrade, Serbia</addr-line>
    </aff>
    <aff id="aff006">
      <label>6</label>
      <addr-line>IDLab, Department of Electronics and Information Systems, Ghent University, Ghent, Belgium</addr-line>
    </aff>
    <aff id="aff007">
      <label>7</label>
      <addr-line>Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Hum</surname>
          <given-names>Yan Chai</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University Tunku Abdul Rahman, MALAYSIA</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <fn fn-type="current-aff" id="currentaff001">
        <label>¤a</label>
        <p>Current address: Department of Digital Health, SAIHST, Sungkyunkwan University, Seoul, Korea</p>
      </fn>
      <fn fn-type="current-aff" id="currentaff002">
        <label>¤b</label>
        <p>Current address: Department of Mathematics, University of Liège, Liège, Belgium</p>
      </fn>
      <fn fn-type="other" id="econtrib001">
        <p>‡ AVM and WDN also contributed equally to this work.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>homin.park@ghent.ac.kr</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>17</volume>
    <issue>6</issue>
    <elocation-id>e0269449</elocation-id>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>11</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Park et al</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Park et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0269449.pdf"/>
    <abstract>
      <p>Environmental monitoring of microplastics (MP) contamination has become an area of great research interest, given potential hazards associated with human ingestion of MP. In this context, determination of MP concentration is essential. However, cheap, rapid, and accurate quantification of MP remains a challenge to this date. This study proposes a deep learning-based image segmentation method that properly distinguishes fluorescent MP from other elements in a given microscopy image. A total of nine different deep learning models, six of which are based on U-Net, were investigated. These models were trained using at least 20,000 patches sampled from 99 fluorescence microscopy images of MP and their corresponding binary masks. MP-Net, which is derived from U-Net, was found to be the best performing model, exhibiting the highest mean <italic toggle="yes">F</italic><sub>1</sub>-score (0.736) and mean IoU value (0.617). Test-time augmentation (using brightness, contrast, and HSV) was applied to MP-Net for robust learning. However, compared to the results obtained without augmentation, no clear improvement in predictive performance could be observed. Recovery assessment for both spiked and real images showed that, compared to already existing tools for MP quantification, the MP quantities predicted by MP-Net are those closest to the ground truth. This observation suggests that MP-Net allows creating masks that more accurately reflect the quantitative presence of fluorescent MP in microscopy images. Finally, MAP (Microplastics Annotation Package) is introduced, an integrated software environment for automated MP quantification, offering support for MP-Net, already existing MP analysis tools like MP-VAT, manual annotation, and model fine-tuning.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004385</institution-id>
            <institution>Universiteit Gent</institution>
          </institution-wrap>
        </funding-source>
        <award-id>01N01718</award-id>
        <principal-award-recipient>
          <name>
            <surname>Cirkovic Velickovic</surname>
            <given-names>Tanja</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>Horizon2020 project FoodEnTwin</institution>
        </funding-source>
        <award-id>810752</award-id>
        <principal-award-recipient>
          <name>
            <surname>Cirkovic Velickovic</surname>
            <given-names>Tanja</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>Serbian Academy of Sciences and Arts</institution>
        </funding-source>
        <principal-award-recipient>
          <name>
            <surname>Cirkovic Velickovic</surname>
            <given-names>Tanja</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award004">
        <funding-source>
          <institution>Ghent University Global Campus</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8190-3839</contrib-id>
          <name>
            <surname>De Neve</surname>
            <given-names>Wesley</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>The research and development activities described in this paper were funded by Ghent University Global Campus (GUGC, TCV and WDN), the Special Research Fund (BOF) of Ghent University (grant no. 01N01718, TCV), the Serbian Academy of Sciences and Arts Project F-26 (TCV), and the Horizon2020 project FoodEnTwin (grant no. 810752, TCV). The funders had no role in the study design, the collection and analysis of data, the decision to publish, and the preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="12"/>
      <table-count count="6"/>
      <page-count count="27"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>Data are available on Kaggle (<ext-link xlink:href="http://www.kaggle.com/sanghyeonaustinpark/mpset" ext-link-type="uri">http://www.kaggle.com/sanghyeonaustinpark/mpset</ext-link>).</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>Data are available on Kaggle (<ext-link xlink:href="http://www.kaggle.com/sanghyeonaustinpark/mpset" ext-link-type="uri">http://www.kaggle.com/sanghyeonaustinpark/mpset</ext-link>).</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>1 Introduction</title>
    <p>Since the 1940s, the production of plastics has increased rapidly, given the attractive properties of plastic goods (durable, lightweight, corrosion resistant) and the inexpensive methods for manufacturing them. However, this once desirable material has currently become a significant environmental concern [<xref rid="pone.0269449.ref001" ref-type="bibr">1</xref>]. Indeed, plastic pollution has become so severe that even an island of plastic, called the Great Pacific Garbage Patch, can now be found in the Central Pacific Gyre [<xref rid="pone.0269449.ref002" ref-type="bibr">2</xref>]. Furthermore, in the marine environment, microplastics (MP; &lt; 5 mm) are the most dominant form of aquatic plastic litter [<xref rid="pone.0269449.ref003" ref-type="bibr">3</xref>]. They are derived from synthetic polymers that are primarily manufactured in small sizes (primary source) or from the degradation of large plastic fragments (secondary source) [<xref rid="pone.0269449.ref004" ref-type="bibr">4</xref>].</p>
    <p>Given their small size, marine organisms ingest MP with their natural food during feeding. This raises several concerns regarding the safety of seafood consumption [<xref rid="pone.0269449.ref005" ref-type="bibr">5</xref>]. As such, MP ingestion by marine biota, in particular those intended for human consumption, is attracting more and more research attention.</p>
    <p>Investigating MP consumption by marine biota is usually done by first extracting MP, followed by MP collection through filtration. Using a microscope, MP pieces are then manually sorted and counted. The latter steps are typically performed by multiple researchers in order to avoid measurement bias. Since this approach is a labor-intensive and time-consuming task, there is a great interest in the development of automated techniques. In this respect, a number of tools that enable semi- or fully automated quantification of MP in images are already available, such as MP-VAT [<xref rid="pone.0269449.ref006" ref-type="bibr">6</xref>] and MP-VAT 2.0 [<xref rid="pone.0269449.ref007" ref-type="bibr">7</xref>]. However, these tools suffer from drawbacks like overestimation and mislabeling, as demonstrated by the experiments presented in this paper. Therefore, a strong need exists for the development of computational tools that facilitate accurate recognition of MP in images, simultaneously measuring quantity and characterizing attributes such as size and shape.</p>
    <p>In this paper, we propose a novel approach that leverages deep learning for mimicking the way MP is investigated by researchers, making it possible to automatically determine the quantity, size, and shape of stained MP in fluorescence microscopy images. To implement this approach, we explored several image segmentation models that make use of multi-layered artificial neural networks. To that end, we created training examples out of microscopy images, containing MP that were isolated from clams and that were subsequently stained with a fluorescent dye called Nile Red.</p>
    <p>Specifically, in this paper, we introduce MP-Net, an effective U-Net-based deep learning model for MP segmentation. This model is part of a new software package that we have named Microplastics Annotation Package (MAP), coming with a graphical user interface (GUI) that enables user-friendly MP analyses. Furthermore, this software package embeds already existing non-deep learning tools for MP analysis (MP-VAT, MP-VAT 2.0, C-VAT, and Galaxy Count), thus also making it possible for users to select—after a comparison—the tool that best fits their research purposes.</p>
    <p>The initial research efforts dedicated to the creation of MP-Net and MAP were first presented in a workshop paper by Baek et al. [<xref rid="pone.0269449.ref008" ref-type="bibr">8</xref>]. This journal paper builds on top of that preliminary work by providing a more in-depth analysis at different levels:</p>
    <list list-type="bullet">
      <list-item>
        <p>More candidate models: In the previous study, we investigated three U-Net-based models. In this study, we incorporate other types of segmentation models and test-time augmentation (TTA), resulting in the fine-tuning and evaluation of a total of nine different deep learning models.</p>
      </list-item>
      <list-item>
        <p>Improved annotation: In the previous study, we made use of a dataset, further referred to as Dataset A, that was composed of 99 images. These images were labeled by a single researcher using an RGB-based thresholding method in ImageJ [<xref rid="pone.0269449.ref009" ref-type="bibr">9</xref>]. In this study, we make use of Dataset B, created by performing a pixel-based comparison of the labels provided by three different researchers.</p>
      </list-item>
      <list-item>
        <p>Recovery assessment: Next to a number of performance metrics commonly used in the field of computer vision, we also make use of percentage recovery, using both artificial and real-world MP, an approach that is more meaningful from the point-of-view of environmental monitoring.</p>
      </list-item>
    </list>
  </sec>
  <sec id="sec002">
    <title>2 Related work</title>
    <p>The use of deep learning for the detection of MP in environmental samples has been studied before, taking as input different types of spectral data. For instance, the presence of MP in mussels was detected and characterized using a random forest classification (RFC) algorithm, using micro-FTIR and micro-Raman spectral data [<xref rid="pone.0269449.ref010" ref-type="bibr">10</xref>–<xref rid="pone.0269449.ref012" ref-type="bibr">12</xref>]. In soil, MP contamination was detected using a convolutional neural network (CNN) model that works with ultra violet—near infrared spectra (UV-NIR) [<xref rid="pone.0269449.ref013" ref-type="bibr">13</xref>]. However, the collection of spectral data is time-consuming and expensive, mostly due to the high cost of specialized equipment and the need for a trained operator [<xref rid="pone.0269449.ref014" ref-type="bibr">14</xref>]. Because of these constraints, Nile red staining became an attractive alternative for MP visualization and detection.</p>
    <p>The aforementioned staining technique relies on the affinity of Nile red to different plastic polymers [<xref rid="pone.0269449.ref015" ref-type="bibr">15</xref>]. Indeed, thanks to the fluorescent nature of the dye, only a fluorescence microscope is needed to observe the MP, after which images can be captured with a camera, and where these images can then be analyzed to quantify and characterize the MP present in the sample at hand. Taken together, Nile red staining is relatively easier and cheaper to do than the collection of spectral data.</p>
    <p>To date, several image-based techniques are available for working with unstained and Nile red-stained MP, as shown in <xref rid="pone.0269449.t001" ref-type="table">Table 1</xref>. However, a deep learning-based method that takes advantage of Nile red-stained MP has not been investigated yet. The following section describes already existing image-based methods for MP analysis, outlining their most important features.</p>
    <table-wrap position="float" id="pone.0269449.t001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0269449.t001</object-id>
      <label>Table 1</label>
      <caption>
        <title>Literature reports that describe image-based methods for MP analysis.</title>
        <p>Highlighted (bold) papers made applications publicly available to other researchers. NOAA protocol stands for National Oceanic and Atmospheric Administration protocol [<xref rid="pone.0269449.ref016" ref-type="bibr">16</xref>].</p>
      </caption>
      <alternatives>
        <graphic xlink:href="pone.0269449.t001" id="pone.0269449.t001g" position="float"/>
        <table frame="box" rules="all" border="0">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="center" rowspan="1" colspan="1">Authors</th>
              <th align="center" rowspan="1" colspan="1">Tested MP size</th>
              <th align="center" rowspan="1" colspan="1">Extraction protocol</th>
              <th align="center" rowspan="1" colspan="1">Source</th>
              <th align="center" rowspan="1" colspan="1">Analysis approach</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" rowspan="1" colspan="1">Mukhanov et al. [<xref rid="pone.0269449.ref017" ref-type="bibr">17</xref>]</td>
              <td align="center" rowspan="1" colspan="1">0.3 ∼ 5 mm</td>
              <td align="center" rowspan="1" colspan="1">NOAA collect, clean, scan</td>
              <td align="center" rowspan="1" colspan="1">Sea surface</td>
              <td align="center" rowspan="1" colspan="1">Photoshop (TR)<break/>ImageJ (MC, SC, SM)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Lorenzo-Navarro et al. [<xref rid="pone.0269449.ref018" ref-type="bibr">18</xref>]</td>
              <td align="center" rowspan="1" colspan="1">1 ∼ 5 mm</td>
              <td align="center" rowspan="1" colspan="1">NOAA collect, clean, scan</td>
              <td align="center" rowspan="1" colspan="1">Beach sediments</td>
              <td align="center" rowspan="1" colspan="1">Adaptive thresholding (TR)<break/>Machine learning (SC)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Lorenzo-Navarro et al. [<xref rid="pone.0269449.ref019" ref-type="bibr">19</xref>]</td>
              <td align="center" rowspan="1" colspan="1">1 ∼ 5 mm</td>
              <td align="center" rowspan="1" colspan="1">NOAA collect, clean, photograph</td>
              <td align="center" rowspan="1" colspan="1">Beach sediments</td>
              <td align="center" rowspan="1" colspan="1">U-Net segmentation (TR)<break/>VGG-16 (SC)</td>
            </tr>
            <tr>
              <td align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Massarelli et al. [<xref rid="pone.0269449.ref020" ref-type="bibr">20</xref>]</td>
              <td align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">0.5 ∼ 5 mm</td>
              <td align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">NOAA collect, clean, photograph</td>
              <td align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">River water, sediments</td>
              <td align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">OpenCV (MC, SM)<break/>Machine learning (SC)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Maes et al. [<xref rid="pone.0269449.ref021" ref-type="bibr">21</xref>]</td>
              <td align="center" rowspan="1" colspan="1">2 ∼ 20 μm</td>
              <td align="center" rowspan="1" colspan="1">Nile Red</td>
              <td align="center" rowspan="1" colspan="1">Beach sediments</td>
              <td align="center" rowspan="1" colspan="1">ImageJ<break/>(Polymer classification)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1"><bold>Mason et al</bold>. [<xref rid="pone.0269449.ref022" ref-type="bibr">22</xref>]</td>
              <td align="center" rowspan="1" colspan="1">6.5+ μm</td>
              <td align="center" rowspan="1" colspan="1">Nile red</td>
              <td align="center" rowspan="1" colspan="1">Bottled water</td>
              <td align="center" rowspan="1" colspan="1">Galaxy Count (TR, MC, SM)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1"><bold>Prata et al</bold>. [<xref rid="pone.0269449.ref006" ref-type="bibr">6</xref>]</td>
              <td align="center" rowspan="1" colspan="1">0.65+ μm</td>
              <td align="center" rowspan="1" colspan="1">Nile red</td>
              <td align="center" rowspan="1" colspan="1">River water, sediments</td>
              <td align="center" rowspan="1" colspan="1">ImageJ (MC, SC, SM)<break/>Max entropy (TR)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1"><bold>Prata et al</bold>. [<xref rid="pone.0269449.ref007" ref-type="bibr">7</xref>]</td>
              <td align="center" rowspan="1" colspan="1">2+ μm</td>
              <td align="center" rowspan="1" colspan="1">Nile red</td>
              <td align="center" rowspan="1" colspan="1">River and lagoon water, sediments</td>
              <td align="center" rowspan="1" colspan="1">ImageJ (MC, SC, SM)<break/>Renyi entropy (TR)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Patchaiyappan et al. [<xref rid="pone.0269449.ref023" ref-type="bibr">23</xref>]</td>
              <td align="center" rowspan="1" colspan="1">100 ∼ 1, 000 μm</td>
              <td align="center" rowspan="1" colspan="1">Nile red</td>
              <td align="center" rowspan="1" colspan="1">Beach sediments</td>
              <td align="center" rowspan="1" colspan="1">ImageJ (MC, SC, SM)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Dowarah et al. [<xref rid="pone.0269449.ref024" ref-type="bibr">24</xref>]</td>
              <td align="center" rowspan="1" colspan="1">21 ∼ 1, 500 μm</td>
              <td align="center" rowspan="1" colspan="1">Nile red</td>
              <td align="center" rowspan="1" colspan="1">Bivalves</td>
              <td align="center" rowspan="1" colspan="1">ImageJ (MC, SC, SM)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1"><bold>Chen et al</bold>. [<xref rid="pone.0269449.ref025" ref-type="bibr">25</xref>]</td>
              <td align="center" rowspan="1" colspan="1">7.0+ μm</td>
              <td align="center" rowspan="1" colspan="1">Nile red</td>
              <td align="center" rowspan="1" colspan="1">Beverage containers</td>
              <td align="center" rowspan="1" colspan="1">Absolute thresholding (TR)<break/>ImageJ (MC, SC, SM)</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">
                <bold>Our approach</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">10 ∼ 1, 000 μm</td>
              <td align="center" rowspan="1" colspan="1">Nile red</td>
              <td align="center" rowspan="1" colspan="1">Bivalves</td>
              <td align="center" rowspan="1" colspan="1">U-Net segmentation (TR) MAP (MC, SC, SM)</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="t001fn001">
          <p>* TR: Thresholding, MC: MP counting, SC: Shape classification, SM: size measurement.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <sec id="sec003">
      <title>2.1 Image-based methods</title>
      <p>The image-based methods shown in <xref rid="pone.0269449.t001" ref-type="table">Table 1</xref> can be mainly classified based on the type of input image used: unstained or Nile red-stained. The first four studies made use of unstained images, while the remaining seven studies utilized Nile red staining to produce images of fluorescent MP. Each method has its own features, which are summarized in the last column of <xref rid="pone.0269449.t001" ref-type="table">Table 1</xref>. These features can be one or a combination of the following:</p>
      <list list-type="bullet">
        <list-item>
          <p>thresholding (TR);</p>
        </list-item>
        <list-item>
          <p>MP counting (MC);</p>
        </list-item>
        <list-item>
          <p>shape classification (SC); and</p>
        </list-item>
        <list-item>
          <p>size measurement (SM).</p>
        </list-item>
      </list>
      <p>TR corresponds to the binarization step shown in <xref rid="pone.0269449.g001" ref-type="fig">Fig 1</xref>. This technique determines whether pixels are MP (foreground) or non-MP (background). MC determines the quantity of MP present in the mask that is the result of TR, while SC classifies the shape of each piece of MP. In general, we can make a distinction between three types of shapes, as described in Section 3.2: particle, fragment, and fiber. Finally, SM measures the size of each piece of MP. To that end, the Feret diameter (μm) or the occupying area (μm<sup>2</sup>) are used. The Feret diameter is defined as the length of the longest straight line that can be drawn in a given piece of MP.</p>
      <fig position="float" id="pone.0269449.g001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Overview of MP measurement.</title>
          <p>Overall approach towards MP measurement, from sampling to image processing: (a), (b) corresponds to the complete MP measurement process we made use of. We propose a new binarization method (green-colored box) that uses deep learning. (a) Wet-lab phase: MP acquisition in a laboratory. (b) Dry-lab phase: Image acquisition and analysis.</p>
        </caption>
        <graphic xlink:href="pone.0269449.g001" position="float"/>
      </fig>
      <p>In the studies presented in [<xref rid="pone.0269449.ref017" ref-type="bibr">17</xref>–<xref rid="pone.0269449.ref020" ref-type="bibr">20</xref>], which made use of unstained images, the MP can be distinguished by the naked eye (0.3 ∼ 5 mm). The National Oceanic and Atmospheric Administration (NOAA) protocol [<xref rid="pone.0269449.ref016" ref-type="bibr">16</xref>] was used in these studies. This protocol is generally adopted to isolate MP debris from the sea surface and sediment, and to subsequently determine size and shape. In contrast, the remaining studies that employed Nile red observed smaller MP (2 ∼ 1,500 μm). Due to the high fluorescence intensity of the dye, smaller MP, which are typically difficult to distinguish, can be noticed easily.</p>
      <p>Among the studies that involved unstained MP, the authors of [<xref rid="pone.0269449.ref018" ref-type="bibr">18</xref>–<xref rid="pone.0269449.ref020" ref-type="bibr">20</xref>] proposed a machine learning model, trained with unstained and large pieces of MP. For stained MP images, the initial work we presented in [<xref rid="pone.0269449.ref008" ref-type="bibr">8</xref>] showcased the first deep learning approach. The majority of the other existing methods are implemented in ImageJ, an open-source software package for scientific image analysis. Only the work by [<xref rid="pone.0269449.ref022" ref-type="bibr">22</xref>] introduced a new platform, called Galaxy Count, which is a semi-automated tool requiring user input. In 2019, [<xref rid="pone.0269449.ref006" ref-type="bibr">6</xref>] introduced MP-VAT, the first automated MP analysis tool that works in ImageJ. In 2020, an upgraded version of MP-VAT, called MP-VAT 2.0, was released [<xref rid="pone.0269449.ref007" ref-type="bibr">7</xref>]. A modified version of MP-VAT was introduced in 2021 by [<xref rid="pone.0269449.ref025" ref-type="bibr">25</xref>], which will be further referred to as C-VAT for the sake of convenience. Unlike MP-VAT, C-VAT copes with noise by using a despeckle filter and outlier removal, also using an absolute threshold of 222 to distinguish background from MP. Being the most recent and high-throughput MP analysis tools for stained MP images, the effectiveness of MP-VAT, MP-VAT 2.0, and C-VAT is compared to the effectiveness of our deep learning models in Section 5.</p>
      <p>A common feature of Galaxy Count, MP-VAT, MP-VAT 2.0, and C-VAT is the use of global TR, which is the main decision-making function that determines if a pixel in an image is classified as MP or non-MP. The following two sections describe in detail global TR and the differences with deep learning-based TR models.</p>
    </sec>
    <sec id="sec004">
      <title>2.2 Differences between TR-based models</title>
      <p>The approaches followed by the four studies highlighted in bold in <xref rid="pone.0269449.t001" ref-type="table">Table 1</xref> [<xref rid="pone.0269449.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0269449.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0269449.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0269449.ref025" ref-type="bibr">25</xref>] are publicly available. Although these approaches use almost identical methods for MC, SM, and SC, considerable differences can be observed for TR, as discussed below.</p>
      <p>In what follows, we make use of the notation <inline-formula id="pone.0269449.e001"><alternatives><graphic xlink:href="pone.0269449.e001.jpg" id="pone.0269449.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi></mml:math></alternatives></inline-formula> to refer to a TR-based model that is responsible for MP and background separation. <inline-formula id="pone.0269449.e002"><alternatives><graphic xlink:href="pone.0269449.e002.jpg" id="pone.0269449.e002g" position="anchor"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi></mml:math></alternatives></inline-formula> takes as input a microscopy image <inline-formula id="pone.0269449.e003"><alternatives><graphic xlink:href="pone.0269449.e003.jpg" id="pone.0269449.e003g" position="anchor"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and returns a mask <inline-formula id="pone.0269449.e004"><alternatives><graphic xlink:href="pone.0269449.e004.jpg" id="pone.0269449.e004g" position="anchor"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, with <italic toggle="yes">w</italic> denoting the image width, <italic toggle="yes">h</italic> the image height, and <italic toggle="yes">c</italic> the number of color channels:
<disp-formula id="pone.0269449.e005"><alternatives><graphic xlink:href="pone.0269449.e005.jpg" id="pone.0269449.e005g" position="anchor"/><mml:math id="M5" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="script">M</mml:mi><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo><mml:mspace width="0.166667em"/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(1)</label></disp-formula></p>
      <p>Note that a color image usually comes with three channels (Red, Green, Blue; RGB; <italic toggle="yes">c</italic> = 3). If <italic toggle="yes">c</italic> is not mentioned explicitly, then a single-channel image (i.e., a gray-scale image; <italic toggle="yes">c</italic> = 1) is used.</p>
      <p>The TR-based model <inline-formula id="pone.0269449.e006"><alternatives><graphic xlink:href="pone.0269449.e006.jpg" id="pone.0269449.e006g" position="anchor"/><mml:math id="M6" display="inline" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi></mml:math></alternatives></inline-formula> can be split into two sub-models. The first sub-model <italic toggle="yes">f</italic><sub><italic toggle="yes">I</italic> → <italic toggle="yes">G</italic></sub> takes <italic toggle="yes">I</italic> as an input and outputs a one-dimensional monotonic intensity image <inline-formula id="pone.0269449.e007"><alternatives><graphic xlink:href="pone.0269449.e007.jpg" id="pone.0269449.e007g" position="anchor"/><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>. The second sub-model <italic toggle="yes">f</italic><sub><italic toggle="yes">G</italic> → <italic toggle="yes">M</italic></sub> takes G as an input and outputs the corresponding mask <italic toggle="yes">M</italic>. For <italic toggle="yes">f</italic><sub><italic toggle="yes">G</italic> → <italic toggle="yes">M</italic></sub>, a thresholding function can be used, like the one shown in the equation below:
<disp-formula id="pone.0269449.e008"><alternatives><graphic xlink:href="pone.0269449.e008.jpg" id="pone.0269449.e008g" position="anchor"/><mml:math id="M8" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mn>255</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4pt"/><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>T</mml:mi><mml:mspace width="0.166667em"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4pt"/><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>T</mml:mi><mml:mspace width="0.166667em"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/><mml:mo>,</mml:mo><mml:mspace width="1.em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>;</mml:mo><mml:mspace width="0.166667em"/><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(2)</label></disp-formula>
with <italic toggle="yes">T</italic> denoting the threshold value. If a pixel has a value of 255, it is assumed to denote MP, and if a pixel has a value of 0, it is assumed to denote background. This approach, which can be referred to as global thresholding or simple thresholding, allows extracting objects of interest (i.e., microplastics) from an image by making use of a single fixed threshold value <italic toggle="yes">T</italic> for all pixels that can be found in the given image of size <italic toggle="yes">w</italic> × <italic toggle="yes">h</italic>. <xref rid="pone.0269449.t002" ref-type="table">Table 2</xref> shows the different <italic toggle="yes">f</italic><sub><italic toggle="yes">I</italic> → <italic toggle="yes">G</italic></sub> and <italic toggle="yes">f</italic><sub><italic toggle="yes">G</italic> → <italic toggle="yes">M</italic></sub> models used by the research efforts previously highlighted in <xref rid="pone.0269449.t001" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="pone.0269449.t002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Differences between TR-based models.</title>
          <p>All existing methods consist of a channel extracting step (<italic toggle="yes">f</italic><sub><italic toggle="yes">I</italic> → <italic toggle="yes">G</italic></sub>) and a thresholding step (<italic toggle="yes">f</italic><sub><italic toggle="yes">G</italic> → <italic toggle="yes">M</italic></sub>).</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0269449.t002" id="pone.0269449.t002g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="1" colspan="1">Authors</th>
                <th align="center" rowspan="1" colspan="1">
                  <inline-formula id="pone.0269449.e009">
                    <alternatives>
                      <graphic xlink:href="pone.0269449.e009" id="pone.0269449.e009g" position="anchor"/>
                      <mml:math id="M9" display="inline" overflow="scroll">
                        <mml:mi mathvariant="script">M</mml:mi>
                      </mml:math>
                    </alternatives>
                  </inline-formula>
                </th>
                <th align="center" rowspan="1" colspan="1">
                  <italic toggle="yes">f</italic>
                  <sub><italic toggle="yes">I</italic> → <italic toggle="yes">G</italic></sub>
                </th>
                <th align="center" rowspan="1" colspan="1">
                  <italic toggle="yes">f</italic>
                  <sub><italic toggle="yes">G</italic> → <italic toggle="yes">M</italic></sub>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">Mason <italic toggle="yes">et al</italic>. [<xref rid="pone.0269449.ref022" ref-type="bibr">22</xref>]</td>
                <td align="center" rowspan="1" colspan="1">Galaxy Count</td>
                <td align="center" rowspan="1" colspan="1">Gray scale</td>
                <td align="center" rowspan="1" colspan="1">Custom threshold</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Prata <italic toggle="yes">et al</italic>. [<xref rid="pone.0269449.ref006" ref-type="bibr">6</xref>]</td>
                <td align="center" rowspan="1" colspan="1">MP-VAT</td>
                <td align="center" rowspan="1" colspan="1">Gray scale</td>
                <td align="center" rowspan="1" colspan="1">Max entropy</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Prata <italic toggle="yes">et al</italic>. [<xref rid="pone.0269449.ref007" ref-type="bibr">7</xref>]</td>
                <td align="center" rowspan="1" colspan="1">MP-VAT 2.0</td>
                <td align="center" rowspan="1" colspan="1">Red channel from RGB</td>
                <td align="center" rowspan="1" colspan="1">Renyi entropy</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">Chen <italic toggle="yes">et al</italic>. [<xref rid="pone.0269449.ref025" ref-type="bibr">25</xref>]</td>
                <td align="center" rowspan="1" colspan="1">C-VAT</td>
                <td align="center" rowspan="1" colspan="1">Gray scale</td>
                <td align="center" rowspan="1" colspan="1">Absolute threshold</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>In the column labeled ‘<italic toggle="yes">f</italic><sub><italic toggle="yes">I</italic> → <italic toggle="yes">G</italic></sub>’ of <xref rid="pone.0269449.t002" ref-type="table">Table 2</xref>, the value <italic toggle="yes">Gray scale</italic> means that color images have been converted to gray scale. Unlike the other three models shown in <xref rid="pone.0269449.t002" ref-type="table">Table 2</xref>, MP-VAT 2.0 extracts and uses only the red channel of an RGB image, given that Nile red dye contains more red information. Next, in the column labeled ‘<italic toggle="yes">f</italic><sub><italic toggle="yes">G</italic> → <italic toggle="yes">M</italic></sub>’, the value <italic toggle="yes">Custom threshold</italic> means that a user needs to iteratively adjust the threshold per image (based on direct inspection of the generated output). Similarly, <italic toggle="yes">Absolute threshold</italic> refers to thresholding with a value that is manually determined through trial and error, and where this value is used for every image. When using <italic toggle="yes">Max entropy</italic>, a threshold is selected that leads to the largest entropy after summing. <italic toggle="yes">Renyi entropy</italic> was introduced by [<xref rid="pone.0269449.ref026" ref-type="bibr">26</xref>], determining a final threshold by weighing three different max entropy values, calculated by setting <italic toggle="yes">α</italic> to 0.5, 1, and 2 in the following Renyi entropy equation:
<disp-formula id="pone.0269449.e010"><alternatives><graphic xlink:href="pone.0269449.e010.jpg" id="pone.0269449.e010g" position="anchor"/><mml:math id="M10" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>q</mml:mi><mml:mi>k</mml:mi><mml:mi>α</mml:mi></mml:msubsup><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula></p>
      <p>Here, <italic toggle="yes">q</italic><sub><italic toggle="yes">k</italic></sub> denotes the normalized occurrence of pixel value <italic toggle="yes">k</italic>. In other words, <italic toggle="yes">q</italic><sub><italic toggle="yes">k</italic></sub> refers to the number of pixels having a brightness of <italic toggle="yes">k</italic> divided by the total number of pixels. For the case of <italic toggle="yes">α</italic> = 1, a converging Taylor series is obtained (<italic toggle="yes">α</italic> = 1 + <italic toggle="yes">ϵ</italic>, <italic toggle="yes">ϵ</italic> → 0), and the resulting expression corresponds to the Max entropy: <inline-formula id="pone.0269449.e011"><alternatives><graphic xlink:href="pone.0269449.e011.jpg" id="pone.0269449.e011g" position="anchor"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
      <p>In both cases, a large entropy is indicative of a large amount of available information. In other words, a large entropy means that MP pixels can be distinguished well from non-MP pixels. Both max entropy and Renyi entropy can be used in ImageJ (Available at <ext-link xlink:href="https://imagej.net/plugins/auto-threshold" ext-link-type="uri">https://imagej.net/plugins/auto-threshold</ext-link>).</p>
      <p>The use of TR has the advantage of being intuitive and fast. However, because only one value is used to distinguish MP pixels from background pixels, it is difficult to properly recognize ambiguous pixels. In particular, fluorescence images have the property of shining brighter when dyed objects are densely gathered.</p>
      <p>For example, a background pixel can be wrongly recognized as an MP pixel (Type 1 error) when using a low threshold and when the pixel in question is brighter than the surrounding pixels. Alternatively, as illustrated in <xref rid="pone.0269449.g002" ref-type="fig">Fig 2(b)</xref>, when the center of an MP particle is photographed relatively dark due to its thickness, a corresponding MP pixel may be recognized as a background pixel (Type 2 error). Therefore, users that apply single-value thresholding should have a good awareness of these weaknesses. Furthermore, depending on the input image used, interactive (i.e., manual) post-processing may be required for accurate MC, SC, and SM.</p>
      <fig position="float" id="pone.0269449.g002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>Type 1 and Type 2 errors when using a single value for thresholding.</title>
        </caption>
        <graphic xlink:href="pone.0269449.g002" position="float"/>
      </fig>
    </sec>
    <sec id="sec005">
      <title>2.3 Deep learning-based TR models</title>
      <p>Unlike already existing TR-based models that make use of global thresholding, our study focuses on building a TR-based deep learning model <italic toggle="yes">f</italic><sub><italic toggle="yes">I</italic> → <italic toggle="yes">M</italic></sub>(<italic toggle="yes">I</italic>;<italic toggle="yes">θ</italic>) that distinguishes MP pixels from background pixels. This model creates a mask <italic toggle="yes">M</italic> directly from a given image <italic toggle="yes">I</italic>, thus not making use of distinct sub-functions (end-to-end learning).</p>
      <p>Given an image dataset <inline-formula id="pone.0269449.e012"><alternatives><graphic xlink:href="pone.0269449.e012.jpg" id="pone.0269449.e012g" position="anchor"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0269449.e013"><alternatives><graphic xlink:href="pone.0269449.e013.jpg" id="pone.0269449.e013g" position="anchor"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is an original image and where <inline-formula id="pone.0269449.e014"><alternatives><graphic xlink:href="pone.0269449.e014.jpg" id="pone.0269449.e014g" position="anchor"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> is its corresponding mask, let <inline-formula id="pone.0269449.e015"><alternatives><graphic xlink:href="pone.0269449.e015.jpg" id="pone.0269449.e015g" position="anchor"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi></mml:math></alternatives></inline-formula> be a segmentation model that takes <italic toggle="yes">I</italic><sub><italic toggle="yes">k</italic></sub> as input and predicts <inline-formula id="pone.0269449.e016"><alternatives><graphic xlink:href="pone.0269449.e016.jpg" id="pone.0269449.e016g" position="anchor"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>^</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> as an approximation of the true mask <italic toggle="yes">M</italic><sub><italic toggle="yes">k</italic></sub>:
<disp-formula id="pone.0269449.e017"><alternatives><graphic xlink:href="pone.0269449.e017.jpg" id="pone.0269449.e017g" position="anchor"/><mml:math id="M17" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula>
where <italic toggle="yes">θ</italic> are the model parameters. The difference between the predicted mask <inline-formula id="pone.0269449.e018"><alternatives><graphic xlink:href="pone.0269449.e018.jpg" id="pone.0269449.e018g" position="anchor"/><mml:math id="M18" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula> and the ground truth mask <italic toggle="yes">M</italic><sub><italic toggle="yes">k</italic></sub> is quantified by a loss function <italic toggle="yes">L</italic>. Given a segmentation model <inline-formula id="pone.0269449.e019"><alternatives><graphic xlink:href="pone.0269449.e019.jpg" id="pone.0269449.e019g" position="anchor"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi></mml:math></alternatives></inline-formula> and a loss function <italic toggle="yes">L</italic>, we want to find the values for <italic toggle="yes">θ</italic> that minimize the total loss based on the dataset <inline-formula id="pone.0269449.e020"><alternatives><graphic xlink:href="pone.0269449.e020.jpg" id="pone.0269449.e020g" position="anchor"/><mml:math id="M20" display="inline" overflow="scroll"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula>.</p>
      <p>Ideally, when a new image <italic toggle="yes">I</italic><sub><italic toggle="yes">l</italic></sub> comes in, the trained model <inline-formula id="pone.0269449.e021"><alternatives><graphic xlink:href="pone.0269449.e021.jpg" id="pone.0269449.e021g" position="anchor"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi></mml:math></alternatives></inline-formula> makes a prediction <inline-formula id="pone.0269449.e022"><alternatives><graphic xlink:href="pone.0269449.e022.jpg" id="pone.0269449.e022g" position="anchor"/><mml:math id="M22" display="inline" overflow="scroll"><mml:msub><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub></mml:math></alternatives></inline-formula> that is close to the true underlying segmentation mask. Intuitively speaking, the TR-based model then mimics the MP and background selection criteria used by a researcher, given that this TR-based model has been trained on the image and mask pairs previously created by researchers.</p>
      <p>Deep learning-based image segmentation has the disadvantage of being slow, both during training and prediction, requiring more computational power than already existing TR-based models. However, deep learning-based image segmentation has the advantage that it reduces the amount of post-processing needed (e.g., manual correction of annotations). Supported by experimental results, a more detailed comparison between already existing TR-based models and deep learning-based image segmentation models will be provided in Section 5.</p>
    </sec>
  </sec>
  <sec sec-type="materials|methods" id="sec006">
    <title>3 Methods</title>
    <p>This section describes the process of MP acquisition and analysis, from sampling to image processing. The entire process is composed of eight steps that are presented in <xref rid="pone.0269449.g001" ref-type="fig">Fig 1</xref>. The different steps are explained by dividing the overall process into two major phases: the wet-lab phase, as shown in <xref rid="pone.0269449.g001" ref-type="fig">Fig 1(a)</xref>, and the dry-lab phase, as shown in <xref rid="pone.0269449.g001" ref-type="fig">Fig 1(b)</xref>. The image pairs (fluorescent and mask) obtained at the end of the dry-lab phase are, in a next step, fed into the dataset preparation phase, as discussed in Section 4.1.</p>
    <sec id="sec007">
      <title>3.1 Wet-lab phase</title>
      <sec id="sec008">
        <title>Sampling</title>
        <p>Manila clam samples (<italic toggle="yes">Ruditapes philippinarum</italic>), as can be seen in <xref rid="pone.0269449.g003" ref-type="fig">Fig 3(a)</xref>, were bought at the Incheon Complex Fish Market (Incheon, Korea) in May 2019. Immediately after the purchase, the samples were kept on ice. After transport to the laboratory, the clams were wrapped in aluminum foil and stored in a -20°C freezer.</p>
        <fig position="float" id="pone.0269449.g003">
          <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g003</object-id>
          <label>Fig 3</label>
          <caption>
            <p>(a) Manila clams used in our experiments, (b) microscopy image of MP stained with Nile red dye, and (c) binary mask for (b). The white rectangle that can be seen in the left-upper part of (b) is used for calibration purposes. In (c), black pixels belong to MP (foreground), whereas white pixels denote non-MP (background).</p>
          </caption>
          <graphic xlink:href="pone.0269449.g003" position="float"/>
        </fig>
      </sec>
      <sec id="sec009">
        <title>Extraction (digestion)</title>
        <p>All steps were performed inside a clean bench and all solutions were filtered using a 0.22μm membrane filter to avoid contamination. For sample preparation, frozen whole clam tissue was separated from the shell using a scalpel. To prevent MP from being released due to thawing, this step was executed as quick as possible. The wet weight of each clam was recorded. To dissolve organic matter, samples were incubated in 250 ml 10% KOH at 60°C with stirring for 24 hours. Through this digestion step, the MP that were inside the organism were obtained [<xref rid="pone.0269449.ref027" ref-type="bibr">27</xref>]. For clams weighing more than 10 g, the tissue was randomly split into two similar pieces and digested separately. Once digestion was completed, samples were vacuum filtered using GF/A (glass microfiber) filters.</p>
      </sec>
      <sec id="sec010">
        <title>Purification (separation)</title>
        <p>After digestion and filtration, the MPs were separated from marine contaminants (sand, silt, and shell) through density separation. Filter papers were resuspended in 10 ml 1.37 g mL<sup>-1</sup> zinc chloride with sonication for 3 mins. After three repetitions, the solutions were centrifuged and the supernatant was filtered to recover MP [<xref rid="pone.0269449.ref021" ref-type="bibr">21</xref>].</p>
      </sec>
      <sec id="sec011">
        <title>Staining</title>
        <p>Nile Red (1 <italic toggle="yes">μ</italic>g mL<sup>-1</sup> in acetone) was added to the purified sample solution at a final concentration of 10 <italic toggle="yes">μ</italic>g mL<sup>-1</sup>. The dye was incubated with the sample at 60°C for 30 mins with constant mixing. Solutions were vacuum filtered using a GF/A filter. To remove excess dye, the GF/A filters were washed with absolute ethanol three times. Additional washing was performed for samples that showed high background fluorescence [<xref rid="pone.0269449.ref021" ref-type="bibr">21</xref>].</p>
      </sec>
    </sec>
    <sec id="sec012">
      <title>3.2 Dry-lab phase</title>
      <sec id="sec013">
        <title>Capturing</title>
        <p>Stained MP debris on filter paper was viewed under a stereomicroscope (Olympus SZX10). This stereomicroscope was equipped with a SZX2-FGFPHQ filter set having 460–480 nm excitation and 495–540 nm emission. Because each filter was too large to be captured as a single photo, images of different filter sections were taken using a DigiRetina 16 microscope camera, with the images having a resolution of 1600 × 1200 and an exposure time of 80 ms.</p>
      </sec>
      <sec id="sec014">
        <title>Stitching</title>
        <p>To create a complete image of a sample, images containing different sections of the same filter paper were combined using Microsoft Image Composite Editor. That way, a total of 99 composite filter paper images were obtained from the Manila clam samples used. A representative image can be found in <xref rid="pone.0269449.g003" ref-type="fig">Fig 3(b)</xref>.</p>
      </sec>
      <sec id="sec015">
        <title>RGB thresholding and binarization</title>
        <p>Each composite image was loaded in ImageJ. First, the scale was set (Analyze &gt; Set Scale) and a representative fluorescent MP piece was selected to determine the optimal RGB threshold (Image &gt; Adjust &gt; Color threshold) for automated MP selection. When necessary, RGB values were adjusted manually to ensure selection of all MP. Afterwards, a “mask” image was generated (Edit &gt; Selection &gt; Create mask), as illustrated in <xref rid="pone.0269449.g003" ref-type="fig">Fig 3(c)</xref>.</p>
      </sec>
      <sec id="sec016">
        <title>Counting and measuring</title>
        <p>The Microplastics Automated Counting Tool (MP-ACT) macro [<xref rid="pone.0269449.ref006" ref-type="bibr">6</xref>] in ImageJ was used to automatically quantify the MP in each mask (Plugins &gt; Macros &gt; MP-ACT). This macro utilizes ImageJ functionalities such as circularity (range goes from 0.0 to 1.0) to perform shape identification and Feret diameter (provides an approximation of the longest dimension) to perform size measurement. The shapes that can be identified include fiber (0.0–0.3), fragment (0.3–0.6), and particle (0.6–1.0). MP-ACT creates an output file that contains a tabulated summary of each MP piece that could be found in the generated mask, describing its shape and size.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec017">
    <title>4 Experimental design</title>
    <p>As shown in <xref rid="pone.0269449.g004" ref-type="fig">Fig 4</xref>, the process of generating a TR-based deep learning model can be roughly summarized by making use of three major steps: (1) dataset preparation; (2) model pre-training, fine-tuning, and validation; and (3) model testing. Aside from the common performance metrics described in Section 4.3, recovery assessment, an additional measure of model effectiveness, is introduced in Section 4.4.</p>
    <fig position="float" id="pone.0269449.g004">
      <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g004</object-id>
      <label>Fig 4</label>
      <caption>
        <title>Overview of the overall approach.</title>
        <p>This approach can be divided into three phases: (1) dataset preparation, (2) model pre-training, fine-tuning, and validation, and (3) model testing.</p>
      </caption>
      <graphic xlink:href="pone.0269449.g004" position="float"/>
    </fig>
    <sec id="sec018">
      <title>4.1 Dataset preparation</title>
      <p>The first step, namely <italic toggle="yes">Dataset preparation</italic>, involves preparing the image and mask pair dataset <inline-formula id="pone.0269449.e023"><alternatives><graphic xlink:href="pone.0269449.e023.jpg" id="pone.0269449.e023g" position="anchor"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mi mathvariant="script">D</mml:mi></mml:math></alternatives></inline-formula> and dividing it into five subsets for the purpose of 4-fold cross-validation and testing, making sure no data leakage is occurring.</p>
      <p>In our previous study [<xref rid="pone.0269449.ref008" ref-type="bibr">8</xref>], only one researcher performed labeling using RGB thresholding. For the study presented in this paper, the previously obtained thresholded RGB masks were used as a starting point. Three researchers manually and independently performed individual pixel annotation (IPA) for each image, as discussed in more detail in S1.1 Section in <xref rid="pone.0269449.s001" ref-type="supplementary-material">S1 File</xref>. The final mask for each image was subsequently obtained by making use of pixel-wise majority voting, an approach also described in more detail in S1.1 Section in <xref rid="pone.0269449.s001" ref-type="supplementary-material">S1 File</xref>. These final masks were then used as the ground truth. A total of 99 ground truth masks, corresponding to the 99 fluorescence microscopy images, were created. It should be clear that, by taking into account the input of three different researchers, labeling can be expected to be more reliable and objective. For the sake of convenience, the dataset used in our previous study [<xref rid="pone.0269449.ref008" ref-type="bibr">8</xref>] is denoted as Dataset A, and the dataset newly created for this study is denoted as Dataset B.</p>
      <p>For both Dataset A and Dataset B, the 99 fluorescence images were distributed over the four cross-validation folds (further referred to as CVF (1), CVF (2), CVF (3), and CVF (4)) and the test set in such a way that the amount of MP pixels is evenly distributed. This was achieved by first arranging the ground truth masks according to an increasing number of MP pixels and by then distributing them one by one over the four cross-validation folds and the test set. In particular, we started with the test set, followed by CVF (1), CVF (2), CVF (3), and then CVF (4), after which the order was inverted. This process was repeated until all images were assigned to a particular set. That way, we prevented a particular set from containing images with a larger MP composition than the others. In the end, 19 images were assigned to the test set while each CVF contained 20 images. Note that, since the ground truth masks from Dataset A were further annotated (i.e., refined) to obtain Dataset B, the order of the images, according to an increasing number of MP pixels, was different between both datasets. As a result, the masks making up each set were also different between the two datasets, as illustrated by <xref rid="pone.0269449.t003" ref-type="table">Table 3</xref>.</p>
      <table-wrap position="float" id="pone.0269449.t003">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title>Overview of the datasets used.</title>
          <p>The last two columns describe the patches sampled using a sliding window approach. Other values are expressed as mean ± standard deviation.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0269449.t003" id="pone.0269449.t003g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="2" colspan="1">Dataset</th>
                <th align="center" rowspan="2" colspan="1">Split</th>
                <th align="center" rowspan="2" colspan="1">Number of images</th>
                <th align="center" rowspan="2" colspan="1">Width</th>
                <th align="center" rowspan="2" colspan="1">Height</th>
                <th align="center" rowspan="2" colspan="1">MP (10<sup>−2</sup>)%</th>
                <th align="center" colspan="2" rowspan="1">Patches created before random deletion</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">With MP</th>
                <th align="center" rowspan="1" colspan="1">Without MP</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="5" colspan="1">
                  <bold>A</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>Test set</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">19</td>
                <td align="center" rowspan="1" colspan="1">3380.1±1523.1</td>
                <td align="center" rowspan="1" colspan="1">2456.8±1113.1</td>
                <td align="center" rowspan="1" colspan="1">0.010±0.013</td>
                <td align="center" rowspan="1" colspan="1">20640</td>
                <td align="center" rowspan="1" colspan="1">156150</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>CVF (1)</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">3299.4±1473.5</td>
                <td align="center" rowspan="1" colspan="1">2558.8±1282.6</td>
                <td align="center" rowspan="1" colspan="1">0.012±0.015</td>
                <td align="center" rowspan="1" colspan="1">29798</td>
                <td align="center" rowspan="1" colspan="1">161496</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>CVF (2)</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">3567.7±1704.0</td>
                <td align="center" rowspan="1" colspan="1">2510.0±1610.0</td>
                <td align="center" rowspan="1" colspan="1">0.015±0.017</td>
                <td align="center" rowspan="1" colspan="1">34799</td>
                <td align="center" rowspan="1" colspan="1">179759</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>CVF (3)</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">2894.5±1760.2</td>
                <td align="center" rowspan="1" colspan="1">2387.8±1670.3</td>
                <td align="center" rowspan="1" colspan="1">0.014±0.016</td>
                <td align="center" rowspan="1" colspan="1">34027</td>
                <td align="center" rowspan="1" colspan="1">149959</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>CVF (4)</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">3474.0±1638.5</td>
                <td align="center" rowspan="1" colspan="1">2474.9±1350.9</td>
                <td align="center" rowspan="1" colspan="1">0.012±0.018</td>
                <td align="center" rowspan="1" colspan="1">30854</td>
                <td align="center" rowspan="1" colspan="1">169608</td>
              </tr>
              <tr>
                <td align="center" rowspan="5" colspan="1">
                  <bold>B</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">
                  <bold>Test set</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">19</td>
                <td align="center" rowspan="1" colspan="1">2949.6±1709.7</td>
                <td align="center" rowspan="1" colspan="1">2425.6±1594.5</td>
                <td align="center" rowspan="1" colspan="1">1.00±1.11</td>
                <td align="center" rowspan="1" colspan="1">30486</td>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>CVF (1)</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">3558.3±1718.0</td>
                <td align="center" rowspan="1" colspan="1">2747.7±1630.5</td>
                <td align="center" rowspan="1" colspan="1">1.40±1.97</td>
                <td align="center" rowspan="1" colspan="1">29793</td>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>CVF (2)</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">3041.5±1486.7</td>
                <td align="center" rowspan="1" colspan="1">2433.4±1209.3</td>
                <td align="center" rowspan="1" colspan="1">1.30±1.62</td>
                <td align="center" rowspan="1" colspan="1">30700</td>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>CVF (3)</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">3620.0±1425.1</td>
                <td align="center" rowspan="1" colspan="1">2420.4±1247.6</td>
                <td align="center" rowspan="1" colspan="1">1.28±1.55</td>
                <td align="center" rowspan="1" colspan="1">33490</td>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">
                  <bold>CVF (4)</bold>
                </td>
                <td align="center" rowspan="1" colspan="1">20</td>
                <td align="center" rowspan="1" colspan="1">3424.8±1730.0</td>
                <td align="center" rowspan="1" colspan="1">2359.5±1362.6</td>
                <td align="center" rowspan="1" colspan="1">1.30±1.50</td>
                <td align="center" rowspan="1" colspan="1">26708</td>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>As shown in <xref rid="pone.0269449.g005" ref-type="fig">Fig 5</xref>, CVF (1)—(4) were used for either fine-tuning or validation. For instance, for Fine-tuning (1), CVF (4) was used as the validation set (indicated as a green box), while the remaining three folds were used as the fine-tuning set (indicated as white boxes). As can be seen in the columns ‘Width’ and ‘Height’ of <xref rid="pone.0269449.t003" ref-type="table">Table 3</xref>, our images do not have the same resolution (given the use of stitching). Since a deep learning model can only receive input of a fixed size, small patches <inline-formula id="pone.0269449.e024"><alternatives><graphic xlink:href="pone.0269449.e024.jpg" id="pone.0269449.e024g" position="anchor"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi><mml:mrow><mml:mn>256</mml:mn><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> were sampled from each image <italic toggle="yes">I</italic> using a sliding window approach.</p>
      <fig position="float" id="pone.0269449.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>Dataset segregation.</title>
          <p>Dataset segregation for fine-tuning, validation, and testing of the already existing TR-based models and the TR-based deep learning models. Green-colored boxes refer to the fold used for validation, while white-colored boxes denote the folds used for fine-tuning.</p>
        </caption>
        <graphic xlink:href="pone.0269449.g005" position="float"/>
      </fig>
      <p>Finally, detailed information about the sampled patches created after the dataset preparation phase can also be found in <xref rid="pone.0269449.t003" ref-type="table">Table 3</xref>, in the column labeled ‘Patches created before random deletion’. We can observe that, for Dataset A, the number of patches containing MP is significantly smaller than the number of patches without MP. This can be interpreted as our dataset having more background pixels than MP pixels. To solve this imbalance problem, we adopted over-sampling, with Dataset B not including patches without MP. A detailed description of our sliding window approach and oversampling is provided in S1.2 Section in <xref rid="pone.0269449.s001" ref-type="supplementary-material">S1 File</xref>.</p>
    </sec>
    <sec id="sec019">
      <title>4.2 Model pre-training, fine-tuning, and validation</title>
      <p>This section discusses the way our TR-based deep learning models were created. As can be seen in <xref rid="pone.0269449.g006" ref-type="fig">Fig 6</xref>, and as explained in Section 2.3, images are first cropped to patches of the same size of 256 × 256. The obtained patches are then fed to a TR-based deep learning model, resulting in a predicted mask for each patch (<xref rid="pone.0269449.g006" ref-type="fig">Fig 6</xref>, red arrows). In <xref rid="pone.0269449.g006" ref-type="fig">Fig 6</xref>, <italic toggle="yes">N</italic> represents the size of the mini-batch, i.e., the number of images that is fed to the model at the same time. Next, the loss, which quantifies the difference between the ground truth mask and the predicted mask, is calculated (<xref rid="pone.0269449.g006" ref-type="fig">Fig 6</xref>, blue arrows and blue box). Based on the calculated loss, the model parameters <italic toggle="yes">θ</italic> are updated by relying on stochastic gradient descent and backpropagation [<xref rid="pone.0269449.ref028" ref-type="bibr">28</xref>] (<xref rid="pone.0269449.g006" ref-type="fig">Fig 6</xref>, green arrow). That way, it is possible for a TR-based deep learning model to iteratively learn how to predict masks that, after each step, are closer to the ground truth masks.</p>
      <fig position="float" id="pone.0269449.g006">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g006</object-id>
        <label>Fig 6</label>
        <caption>
          <title>Fine-tuning of a TR-based deep learning model.</title>
          <p><italic toggle="yes">N</italic> corresponds to the size of the mini-batch of images fed to the model. The dotted box represents an additional step that is performed when TTA is included.</p>
        </caption>
        <graphic xlink:href="pone.0269449.g006" position="float"/>
      </fig>
      <p>In our experiments, nine different TR-based deep learning models were evaluated by making use of Dataset B. Information about the loss and optimizer used by each model is presented in Section 5.1.2. U-Net (1) to U-Net (4) all have the same U-Net structure [<xref rid="pone.0269449.ref029" ref-type="bibr">29</xref>], but the loss and optimizer combinations are different. TTA U-Net (3) and TTA U-Net (4) are models that apply TTA to U-Net (3) and U-Net (4), respectively. Finally, in order to obtain a more complete performance comparison, segmentation models different from U-Net were also tested, namely FCN [<xref rid="pone.0269449.ref030" ref-type="bibr">30</xref>], DeepLabv3 [<xref rid="pone.0269449.ref031" ref-type="bibr">31</xref>], and Nested U-Net [<xref rid="pone.0269449.ref032" ref-type="bibr">32</xref>].</p>
      <p>Each of the nine TR-based deep learning models consists of an encoder that extracts features from a patch and a decoder that reconstructs the patch as a mask. We initialized the encoder through transfer learning [<xref rid="pone.0269449.ref033" ref-type="bibr">33</xref>], making use of pre-trained parameters from two benchmark datasets, namely ImageNet [<xref rid="pone.0269449.ref034" ref-type="bibr">34</xref>] and MS COCO 2017 [<xref rid="pone.0269449.ref035" ref-type="bibr">35</xref>]. Afterwards, Dataset B was used to fine-tune the parameters of each model. Specifically, as shown in <xref rid="pone.0269449.g005" ref-type="fig">Fig 5</xref>, 4-fold cross-validation was performed. Finally, each fine-tuned model was evaluated through the respective test set and the average of the four results was taken to obtain the final effectiveness.</p>
      <p>More details on the fine-tuning process are provided in S2.1 Section in <xref rid="pone.0269449.s002" ref-type="supplementary-material">S2 File</xref>. Information on the used models is included in S2.2 Section in <xref rid="pone.0269449.s002" ref-type="supplementary-material">S2 File</xref>, and a detailed description of the loss and the optimizer used can be found in S2.3 Section in <xref rid="pone.0269449.s002" ref-type="supplementary-material">S2 File</xref> and S2.4 Section in <xref rid="pone.0269449.s002" ref-type="supplementary-material">S2 File</xref>, respectively. Transfer learning is discussed in S2.5 Section in <xref rid="pone.0269449.s002" ref-type="supplementary-material">S2 File</xref>, while the learning curves for each model are included in <xref rid="pone.0269449.s003" ref-type="supplementary-material">S3 File</xref>.</p>
    </sec>
    <sec id="sec020">
      <title>4.3 Model testing</title>
      <p>The testing phase aims at determining the effectiveness of the models that have been created after fine-tuning. As shown in <xref rid="pone.0269449.g007" ref-type="fig">Fig 7</xref>, an input image <italic toggle="yes">I</italic> is split into small patches of the same size, and each patch is then fed into the fine-tuned model. This model subsequently predicts the corresponding mask patch. In the next step, the predicted mask patches are merged back into a predicted mask. The effectiveness of a model is evaluated by determining the similarity between the predicted mask and the ground truth mask. In this study, we also made use of TTA, which is a method that aims at increasing the prediction effectiveness by applying one or more augmentations to each patch. The augmented patches are then fed into the model to make multiple predictions. These predictions, including that of the original patch, are combined into a single prediction through simple pixel-wise averaging. A detailed explanation of TTA can be found in S4.1 Section in <xref rid="pone.0269449.s004" ref-type="supplementary-material">S4 File</xref>.</p>
      <fig position="float" id="pone.0269449.g007">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g007</object-id>
        <label>Fig 7</label>
        <caption>
          <title>Mask generation for the fluorescence images in the set used for testing.</title>
          <p>The generated masks are compared with the ground truth masks to evaluate the effectiveness of a model, hereby using different metrics. When TTA is not implemented, the predicted mask patches are generated directly from the fluorescence patches. In contrast, when TTA is implemented (depicted with a dotted box), the fluorescence patches are augmented and the masks obtained for these patches are merged into a single mask by taking the pixel-wise average of each prediction.</p>
        </caption>
        <graphic xlink:href="pone.0269449.g007" position="float"/>
      </fig>
      <p>To determine the final effectiveness of the different models, we adopted five metrics commonly used in the field of computer vision, namely balanced accuracy, recall, precision, <italic toggle="yes">F</italic><sub>1</sub>-score, and the Intersection-over-Union (IoU). A detailed description of each metric can be found in S4.2 Section in <xref rid="pone.0269449.s004" ref-type="supplementary-material">S4 File</xref>. Since the already existing TR-based models do not need fine-tuning, the final effectiveness, as measured by the different metrics, corresponds to the mean score of that metric over all images in the test set. On the other hand, since the TR-based deep learning models were fine-tuned using four different fold combinations, the final effectiveness was calculated by averaging the four means obtained for the different metrics (one mean per fine-tuned model). A lowercase letter m is added in front of each metric to signify that the numbers represent mean values.</p>
    </sec>
    <sec id="sec021">
      <title>4.4 Recovery assessment</title>
      <p>To take into account the point-of-view of environmental scientists, we also evaluated the effectiveness of the best model in predicting MP count by assessing its recovery ability. This is done by calculating the ratio of the predicted MP count to the baseline MP count (as obtained for the ground truth mask), subsequently converting this ratio to a percentage. Percentage recovery is more relevant in the field of MP monitoring since the level of MP contamination is usually expressed as the number of MP per weight or volume of the sample under investigation [<xref rid="pone.0269449.ref036" ref-type="bibr">36</xref>]. In this respect, it should be clear that an accurate prediction of the MP count is of utmost importance.</p>
      <p>For this purpose, five images containing fluorescent MP standards, which are microplastics of known composition (polymer type and average size), were created. These images are referred to as spiked images since microplastics were intentionally added (available at <ext-link xlink:href="http://www.kaggle.com/sanghyeonaustinpark/mpset" ext-link-type="uri">http://www.kaggle.com/sanghyeonaustinpark/mpset</ext-link>). The polymer types used were HDPE (high-density polyethylene; Dow<sup>™</sup> HDPE KT 10000 UE), and PET (polyethylene terephthalate; Lighter<sup>™</sup> C93 PET). The MP standards were prepared using a controlled milling process by the Center of Polymer and Material Technologies of the Department of Materials, Textiles and Chemical Engineering of Ghent University. HDPE (1 mg) and PET (0.5 mg) were suspended in 15 ml zinc chloride and stained following the procedure described in Section 3.1. Afterwards, images were captured and the masks were generated following the steps outlined in Section 3.2. These masks were then manually corrected through individual pixel annotation by three different researchers (see S1.1 Section in <xref rid="pone.0269449.s001" ref-type="supplementary-material">S1 File</xref>). Through a majority voting strategy, the final MP count was determined, which was used as the baseline (ground truth).</p>
    </sec>
    <sec id="sec022">
      <title>4.5 Statistical analysis</title>
      <p>Student’s t-test was used to compare the mean positive pixels (MP pixels in a mask) between the ground truth and the different methods under investigation, which include the deep learning models and the already existing tools. The Bartlett test was performed prior to the Student’s t-test to validate equal or unequal variance between the ground truth and the predicted mask that is under consideration. In addition, ANOVA was applied to detect differences in the means of the number of TPs (true positives), FPs (false positives), TNs (true negatives), and FNs (false negatives) obtained from the masks generated by the different methods. A TP denotes an MP pixel correctly predicted as MP, an FP denotes a background pixel wrongly predicted as an MP pixel (Type 2 error), a TN refers to a background pixel correctly predicted as background, and an FN denotes an MP pixel wrongly predicted as a background pixel (Type 1 error). Failure to reject the null hypothesis would indicate that the mean number of either TPs, FPs, TNs, or FNs between different models is not significantly different.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec023">
    <title>5 Results and discussion</title>
    <p>The ability of MP-VAT, MP-VAT 2.0, and C-VAT to predict the quantity of MP in a fluorescence image was first evaluated using the 99 images in Dataset B. These predicted MP quantities were compared to what we consider to be the true MP count derived from the masks, following the procedure described in S1.1 Section in <xref rid="pone.0269449.s001" ref-type="supplementary-material">S1 File</xref>.</p>
    <p>Based on <xref rid="pone.0269449.g008" ref-type="fig">Fig 8</xref>, the predicted quantity of MP is frequently higher than the ground truth quantity when MP-VAT and MP-VAT 2.0 were used. In the case of C-VAT, all predictions were over-estimated. This mismatch between the binary masks created by the three existing methods and the fluorescence images is most likely caused by the use of global thresholding, as previously described in Section 2.2. Furthermore, the observed errors, particularly for MP-VAT and MP-VAT 2.0, may also be caused by background fluorescence and fluorescence halos belonging to very bright particles, as previously noted by the authors of [<xref rid="pone.0269449.ref006" ref-type="bibr">6</xref>].</p>
    <fig position="float" id="pone.0269449.g008">
      <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g008</object-id>
      <label>Fig 8</label>
      <caption>
        <title>Accuracy of MP counts by MP-VAT, MP-VAT 2.0, and C-VAT.</title>
        <p>Predictions of MP quantity that are less than, equal to, and more than that of the ground truth are denoted as under-estimation, exact, and over-estimation, respectively. At least 94% of the predicted values from C-VAT, MP-VAT, and MP-VAT 2.0 were over-estimations.</p>
      </caption>
      <graphic xlink:href="pone.0269449.g008" position="float"/>
    </fig>
    <p>In the following sections, we compare the effectiveness of the three already existing methods to the effectiveness of our deep learning models. We would like to emphasize that we first carried out experiments using U-Net (1)—(3), as previously discussed in [<xref rid="pone.0269449.ref008" ref-type="bibr">8</xref>]. After observing the good effectiveness of U-Net (3), as shown in <xref rid="pone.0269449.t004" ref-type="table">Table 4</xref>, we decided to additionally investigate the effect of changing the optimizer from Adaptive Moment Estimation (Adam) to Stochastic Gradient Descent (SGD) [<xref rid="pone.0269449.ref037" ref-type="bibr">37</xref>, <xref rid="pone.0269449.ref038" ref-type="bibr">38</xref>]. This new combination of Dice loss and SGD resulted in the creation of U-Net (4). Moreover, given that U-Net (3) and U-Net (4) are the most effective U-Net variations, we also decided to equip these models with TTA, as described in more detail in S4.1 Section in <xref rid="pone.0269449.s004" ref-type="supplementary-material">S4 File</xref>. Finally, we also experimented with a number of other contemporary models, namely FCN [<xref rid="pone.0269449.ref030" ref-type="bibr">30</xref>], DeepLabv3 [<xref rid="pone.0269449.ref031" ref-type="bibr">31</xref>], and Nested U-Net [<xref rid="pone.0269449.ref032" ref-type="bibr">32</xref>].</p>
    <table-wrap position="float" id="pone.0269449.t004">
      <object-id pub-id-type="doi">10.1371/journal.pone.0269449.t004</object-id>
      <label>Table 4</label>
      <caption>
        <title>Performance results obtained for Dataset B.</title>
        <p>The numbers between parentheses are standard deviations. The last column contains the p-values of the t-test comparing the mean MP pixel count between ground truth and predicted masks.</p>
      </caption>
      <alternatives>
        <graphic xlink:href="pone.0269449.t004" id="pone.0269449.t004g" position="float"/>
        <table frame="box" rules="all" border="0">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="center" rowspan="1" colspan="1">Model</th>
              <th align="center" rowspan="1" colspan="1">Loss</th>
              <th align="center" rowspan="1" colspan="1">Optimizer</th>
              <th align="center" rowspan="1" colspan="1">mAccuracy</th>
              <th align="center" rowspan="1" colspan="1">mRecall</th>
              <th align="center" rowspan="1" colspan="1">mPrecision</th>
              <th align="center" rowspan="1" colspan="1">m<italic toggle="yes">F</italic><sub>1</sub>-score</th>
              <th align="center" rowspan="1" colspan="1">mIoU</th>
              <th align="center" rowspan="1" colspan="1">p-value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" rowspan="1" colspan="1">MP VAT</td>
              <td align="center" rowspan="1" colspan="1">X</td>
              <td align="center" rowspan="1" colspan="1">X</td>
              <td align="center" rowspan="1" colspan="1">0.731 (0.056)</td>
              <td align="center" rowspan="1" colspan="1">0.965 (0.111)</td>
              <td align="center" rowspan="1" colspan="1">0.451 (0.212)</td>
              <td align="center" rowspan="1" colspan="1">0.574 (0.218)</td>
              <td align="center" rowspan="1" colspan="1">0.429 (0.190)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.001</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">MP VAT 2.0</td>
              <td align="center" rowspan="1" colspan="1">X</td>
              <td align="center" rowspan="1" colspan="1">X</td>
              <td align="center" rowspan="1" colspan="1">0.527 (0.161)</td>
              <td align="center" rowspan="1" colspan="1">0.561 (0.317)</td>
              <td align="center" rowspan="1" colspan="1">0.172 (0.248)</td>
              <td align="center" rowspan="1" colspan="1">0.191 (0.199)</td>
              <td align="center" rowspan="1" colspan="1">0.119 (0.131)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.001</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">C-VAT</td>
              <td align="center" rowspan="1" colspan="1">X</td>
              <td align="center" rowspan="1" colspan="1">X</td>
              <td align="center" rowspan="1" colspan="1">0.542 (0.174)</td>
              <td align="center" rowspan="1" colspan="1">0.629 (0.383)</td>
              <td align="center" rowspan="1" colspan="1">0.007 (0.021)</td>
              <td align="center" rowspan="1" colspan="1">0.012 (0.035)</td>
              <td align="center" rowspan="1" colspan="1">0.006 (0.019)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.002</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">U-Net (1)</td>
              <td align="center" rowspan="1" colspan="1">BCEWithLogits</td>
              <td align="center" rowspan="1" colspan="1">SGD</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>0.692 (0.013)</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">
                <bold>0.883 (0.026)</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">0.517 (0.091)</td>
              <td align="center" rowspan="1" colspan="1">0.601 (0.079)</td>
              <td align="center" rowspan="1" colspan="1">0.466 (0.070)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.278</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">U-Net (2)</td>
              <td align="center" rowspan="1" colspan="1">DiceBCE</td>
              <td align="center" rowspan="1" colspan="1">Adam</td>
              <td align="center" rowspan="1" colspan="1">0.637 (0.011)</td>
              <td align="center" rowspan="1" colspan="1">0.775 (0.022)</td>
              <td align="center" rowspan="1" colspan="1">0.783 (0.022)</td>
              <td align="center" rowspan="1" colspan="1">0.727 (0.004)</td>
              <td align="center" rowspan="1" colspan="1">0.601 (0.004)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.723</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">U-Net (3)</td>
              <td align="center" rowspan="1" colspan="1">Dice</td>
              <td align="center" rowspan="1" colspan="1">Adam</td>
              <td align="center" rowspan="1" colspan="1">0.633 (0.011)</td>
              <td align="center" rowspan="1" colspan="1">0.767 (0.021)</td>
              <td align="center" rowspan="1" colspan="1">0.794 (0.013)</td>
              <td align="center" rowspan="1" colspan="1">0.728 (0.002)</td>
              <td align="center" rowspan="1" colspan="1">0.606 (0.004)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.896</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">U-Net (4)</td>
              <td align="center" rowspan="1" colspan="1">Dice</td>
              <td align="center" rowspan="1" colspan="1">SGD</td>
              <td align="center" rowspan="1" colspan="1">0.640 (0.020)</td>
              <td align="center" rowspan="1" colspan="1">0.779 (0.040)</td>
              <td align="center" rowspan="1" colspan="1">0.803 (0.041)</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>0.736 (0.014)</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">
                <bold>0.617 (0.018)</bold>
              </td>
              <td align="char" char="." rowspan="1" colspan="1">0.639</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">TTA U-Net (3)</td>
              <td align="center" rowspan="1" colspan="1">Dice</td>
              <td align="center" rowspan="1" colspan="1">Adam</td>
              <td align="center" rowspan="1" colspan="1">0.597 (0.029)</td>
              <td align="center" rowspan="1" colspan="1">0.694 (0.058)</td>
              <td align="center" rowspan="1" colspan="1">
                <bold>0.820 (0.055)</bold>
              </td>
              <td align="center" rowspan="1" colspan="1">0.696 (0.015)</td>
              <td align="center" rowspan="1" colspan="1">0.562 (0.016)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.607</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">TTA U-Net (4)</td>
              <td align="center" rowspan="1" colspan="1">Dice</td>
              <td align="center" rowspan="1" colspan="1">SGD</td>
              <td align="center" rowspan="1" colspan="1">0.648 (0.026)</td>
              <td align="center" rowspan="1" colspan="1">0.795 (0.052)</td>
              <td align="center" rowspan="1" colspan="1">0.766 (0.072)</td>
              <td align="center" rowspan="1" colspan="1">0.726 (0.017)</td>
              <td align="center" rowspan="1" colspan="1">0.600 (0.024)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.711</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">FCN</td>
              <td align="center" rowspan="1" colspan="1">Dice</td>
              <td align="center" rowspan="1" colspan="1">SGD</td>
              <td align="center" rowspan="1" colspan="1">0.610 (0.020)</td>
              <td align="center" rowspan="1" colspan="1">0.719 (0.039)</td>
              <td align="center" rowspan="1" colspan="1">0.659 (0.024)</td>
              <td align="center" rowspan="1" colspan="1">0.637 (0.012)</td>
              <td align="center" rowspan="1" colspan="1">0.488 (0.009)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.751</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">DeepLabv3</td>
              <td align="center" rowspan="1" colspan="1">Dice</td>
              <td align="center" rowspan="1" colspan="1">Adam</td>
              <td align="center" rowspan="1" colspan="1">0.623 (0.021)</td>
              <td align="center" rowspan="1" colspan="1">0.745 (0.042)</td>
              <td align="center" rowspan="1" colspan="1">0.634 (0.030)</td>
              <td align="center" rowspan="1" colspan="1">0.643 (0.008)</td>
              <td align="center" rowspan="1" colspan="1">0.495 (0.009)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.917</td>
            </tr>
            <tr>
              <td align="center" rowspan="1" colspan="1">Nested U-Net</td>
              <td align="center" rowspan="1" colspan="1">BCEWithLogits</td>
              <td align="center" rowspan="1" colspan="1">Adam</td>
              <td align="center" rowspan="1" colspan="1">0.675 (0.019)</td>
              <td align="center" rowspan="1" colspan="1">0.851 (0.039)</td>
              <td align="center" rowspan="1" colspan="1">0.710 (0.045)</td>
              <td align="center" rowspan="1" colspan="1">0.725 (0.007)</td>
              <td align="center" rowspan="1" colspan="1">0.597 (0.010)</td>
              <td align="char" char="." rowspan="1" colspan="1">0.556</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
    </table-wrap>
    <sec id="sec024">
      <title>5.1 MP and background classification (segmentation)</title>
      <sec id="sec025">
        <title>5.1.1 Qualitative results</title>
        <p>Figs <xref rid="pone.0269449.g009" ref-type="fig">9</xref> and <xref rid="pone.0269449.g010" ref-type="fig">10</xref> visualize the differences in the binary masks generated by the different approaches used, relying on a representative patch taken from one of the images in the test set. In the different masks shown, the background pixels are white (TNs). The TPs, the FPs, and the FNs are colored black, red, and green, respectively.</p>
        <fig position="float" id="pone.0269449.g009">
          <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g009</object-id>
          <label>Fig 9</label>
          <caption>
            <title>Visual comparison of the masks generated when making use of MP-VAT, MP-VAT 2.0, C-VAT, and the U-Net variations.</title>
            <p>For better visualization and understanding, regions with MP are also represented by making use of zoomed-in insets. Black denotes MP that has been correctly predicted (TP), red denotes predicted MP that is not MP (FP), and green denotes MP that has not been predicted as MP (FN). (a) Fluorescent patch. (b) Inset of each MP. (c) Ground truth. (d) MP-VAT. (e) MP-VAT 2.0. (f) C-VAT. (g) U-Net (1). (h) U-Net (2). (i) U-Net (3). (j) U-Net (4).</p>
          </caption>
          <graphic xlink:href="pone.0269449.g009" position="float"/>
        </fig>
        <fig position="float" id="pone.0269449.g010">
          <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g010</object-id>
          <label>Fig 10</label>
          <caption>
            <title>Visual comparison of the masks generated when making use of TTA, FCN, DeepLabv3, and Nested U-Net.</title>
            <p>For better visualization and understanding, regions with MP are also represented by making use of zoomed-in insets. Black denotes MP that has been correctly predicted (TP), red denotes predicted MP that is not MP (FP), and green denotes MP that has not been predicted as MP (FN). (a) Ground truth. (b) TTA U-Net (3). (c) TTA U-Net (4). (d) FCN. (e) DeepLabv3. (f) Nested U-Net.</p>
          </caption>
          <graphic xlink:href="pone.0269449.g010" position="float"/>
        </fig>
        <p>As shown in <xref rid="pone.0269449.g009" ref-type="fig">Fig 9</xref>, it is evident that all deep learning models perform better than MP-VAT 2.0 and C-VAT. In particular, for the different U-Net variations, we observe a significant reduction in the number of FPs and the number of FNs. The same observation can be made for the masks generated by TTA U-Net (3), TTA U-Net (4), FCN, DeepLabv3, and Nested U-Net, as illustrated in <xref rid="pone.0269449.g010" ref-type="fig">Fig 10</xref>. Among the different models evaluated, U-Net (1) closely resembles MP-VAT and comes with the highest number of FPs. On the other hand, FCN generates the highest number of FNs. FCN and DeepLabv3 seem to be prone to errors caused by MP pieces that are in close proximity to each other. This can be seen in the lower right insets of <xref rid="pone.0269449.g010" ref-type="fig">Fig 10(d) and 10(e)</xref>, showing how two MP pieces are merged into one. Based on the aforementioned observations, we can conclude that U-Net (1), FCN, and DeepLabv3 perform poorly compared to the other models.</p>
        <p>Furthermore, we observe that only minute differences can be found between the masks generated by U-Net (2), U-Net (3), and Nested U-Net. On the other hand, U-Net (4) shows a substantial reduction in the number and the size of the FP regions. Although there are faint traces of visible FN regions, the mask created by U-Net (4) still closely resembles the ground truth mask, compared to the masks produced by U-Net (2), U-Net (3), and Nested U-Net. Moving to the TTA counterparts of U-Net (3) and U-Net (4), there seems to exist a subtle difference between U-Net (3) and TTA U-Net (3), as illustrated by the masks that can be found in Figs <xref rid="pone.0269449.g009" ref-type="fig">9(h)</xref> and <xref rid="pone.0269449.g010" ref-type="fig">10(b)</xref>, respectively. In contrast, the mask produced by TTA U-Net (4), as shown in <xref rid="pone.0269449.g010" ref-type="fig">Fig 10(c)</xref>, contains more FP regions compared to the mask produced by U-Net (4), as shown in <xref rid="pone.0269449.g009" ref-type="fig">Fig 9(i)</xref>. In summary, U-Net (4) comes with the highest number of TPs and the lowest number of FPs and FNs, making it the best performing model in qualitative terms.</p>
      </sec>
      <sec id="sec026">
        <title>5.1.2 Quantitative results</title>
        <p><xref rid="pone.0269449.t004" ref-type="table">Table 4</xref> shows the quantitative results obtained for Dataset B, as described in detail in S1.1 and S1.2 Sections in <xref rid="pone.0269449.s001" ref-type="supplementary-material">S1 File</xref>.</p>
        <p>A comparison of the mean MP pixel count between the ground truth and the predicted masks was performed for the three existing methods and the TR-based deep learning models. A significantly higher mean pixel count was observed for MP-VAT, MP-VAT 2.0, and for C-VAT, as shown in the last column of <xref rid="pone.0269449.t004" ref-type="table">Table 4</xref>. This result agrees with the previous observations that these three existing tools overestimate the quantity of MP. On the other hand, none of the predictions made by the deep learning models showed significant differences from the ground truth, suggesting that, in this respect, these models perform better than already existing tools. When the mean values of pixels corresponding to TPs, FPs, TNs, and FNs were compared through ANOVA, both mean FP and mean TN were found to be considerably different (<italic toggle="yes">p</italic> &lt; 0.05) between MP-VAT, MP-VAT 2.0, C-VAT, and each of the deep learning models (the detailed numbers can be found in the <xref rid="pone.0269449.s006" ref-type="supplementary-material">S1 Table</xref>). Based on this observation, a reduction in the number of FPs and subsequent improved recognition of TNs seems to have been achieved by the deep learning models, thereby leading to MP quantity predictions that are similar to the ground truth. Among the nine TR-based deep learning models, U-Net (1) showed the highest effectiveness in terms of mAccuracy (0.692) and mRecall (0.883). On the other hand, TTA U-Net (3) obtained the highest mPrecision (0.820), whereas U-Net (4) achieved the highest m<italic toggle="yes">F</italic><sub>1</sub>-score (0.736) and the highest mIoU (0.617). Given these observations, we conclude that the most effective models are U-Net (1), TTA U-Net (3), and U-Net (4), in no particular order. In order to select the most effective model, we decided to attach more importance to metrics that appropriately capture the concept of distinguishing MP, which is a sparse class, from other image elements. In this respect, the metrics highly influenced by TP were considered to be relatively more significant.</p>
        <p>As shown in S4.2 Section in <xref rid="pone.0269449.s004" ref-type="supplementary-material">S4 File</xref>, mAccuracy is highly affected by TN. Indeed, since TN corresponds to correctly identified background, its value is substantially higher than the value of TP. For this reason, mAccuracy was considered to be a metric of less importance. On a similar note, both mRecall and mPrecision assign significant weight to TP. However, these two metrics only account for either FP or FN. Because of this, mRecall and mPrecision were also considered to be of less importance. Furthermore, m<italic toggle="yes">F</italic><sub>1</sub>-score and mIoU comprehensively take into account TP, FP, and FN. In addition, they are not affected by TN, thereby making them the most suitable metrics to determine the most effective model. As a result, U-Net (4), which showed the highest m<italic toggle="yes">F</italic><sub>1</sub>-score and mIoU, was chosen as the best TR-based deep learning model. This choice is also supported by the mask patches presented in Figs <xref rid="pone.0269449.g009" ref-type="fig">9</xref> and <xref rid="pone.0269449.g010" ref-type="fig">10</xref>. As discussed in Section 5.1.1, the masks generated by U-Net (4) resemble the ground truth masks more closely, compared to the masks produced by other models. Hence, U-Net (4), which is referred to as MP-Net throughout the remainder of the paper, exhibited the best performance, both quantitatively and qualitatively.</p>
        <p>Additional experimental results such as assessment of the use of different encoders, analysis of the impact of TTA, and an ablation study on different types of augmentation, can be found in S6.1, S6.2 and S6.4 Sections in <xref rid="pone.0269449.s005" ref-type="supplementary-material">S5 File</xref>, respectively.</p>
      </sec>
    </sec>
    <sec id="sec027">
      <title>5.2 Recovery assessment</title>
      <p>Recovery assessment was performed using MP-Net, which was found to be the best TR-based deep learning model. This type of assessment was conducted using two sets of images with a different level of complexity: (1) a first set of five spiked images containing only MP standards and (2) a second set of 15 real images of MP extracted from clams. In the spiked images (available at <ext-link xlink:href="http://www.kaggle.com/sanghyeonaustinpark/mpset" ext-link-type="uri">http://www.kaggle.com/sanghyeonaustinpark/mpset</ext-link>), the MP standards have less variability in particle size and shape since they were produced through a controlled milling process. On the other hand, the real images contain MP with a varying size and shape since they were extracted from environmental samples, specifically from clams, thus adding complexity to the images. Due to the limited availability of MP standards, only five spiked images were created for recovery assessment.</p>
      <p>As shown in <xref rid="pone.0269449.t005" ref-type="table">Table 5</xref>, the different versions of MP-VAT obtain extremely high percentage recoveries for the spiked images, with a minimum of 169% and a maximum of 299%. Similarly, C-VAT shows percentage recoveries of more than 158%. This translates to detecting two to three times the actual MP quantity. On the other hand, MP-Net shows percentage recoveries close to 100% for the majority of the spiked images, obtaining an average of 107.8±9.2%.</p>
      <table-wrap position="float" id="pone.0269449.t005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.t005</object-id>
        <label>Table 5</label>
        <caption>
          <title>MP count and percentage recovery obtained for five spiked MP images.</title>
          <p>The MP quantity predicted by MP-Net was much closer to the ground truth compared to MP-VAT, MP-VAT 2.0, and C-VAT.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0269449.t005" id="pone.0269449.t005g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="2" colspan="1">Sample number</th>
                <th align="center" rowspan="2" colspan="1">Ground truth (Majority vote)</th>
                <th align="center" colspan="2" rowspan="1">MP-VAT</th>
                <th align="center" colspan="2" rowspan="1">MP-VAT 2.0</th>
                <th align="center" colspan="2" rowspan="1">C-VAT</th>
                <th align="center" colspan="2" rowspan="1">MP-Net</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">MP count</th>
                <th align="center" rowspan="1" colspan="1">Percentage recovery</th>
                <th align="center" rowspan="1" colspan="1">MP count</th>
                <th align="center" rowspan="1" colspan="1">Percentage recovery</th>
                <th align="center" rowspan="1" colspan="1">MP count</th>
                <th align="center" rowspan="1" colspan="1">Percentage recovery</th>
                <th align="center" rowspan="1" colspan="1">MP count</th>
                <th align="center" rowspan="1" colspan="1">Percentage recovery</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">170</td>
                <td align="center" rowspan="1" colspan="1">287</td>
                <td align="center" rowspan="1" colspan="1">169</td>
                <td align="center" rowspan="1" colspan="1">396</td>
                <td align="center" rowspan="1" colspan="1">233</td>
                <td align="center" rowspan="1" colspan="1">268</td>
                <td align="center" rowspan="1" colspan="1">158</td>
                <td align="center" rowspan="1" colspan="1">175</td>
                <td align="center" rowspan="1" colspan="1">103</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">91</td>
                <td align="center" rowspan="1" colspan="1">163</td>
                <td align="center" rowspan="1" colspan="1">179</td>
                <td align="center" rowspan="1" colspan="1">204</td>
                <td align="center" rowspan="1" colspan="1">224</td>
                <td align="center" rowspan="1" colspan="1">166</td>
                <td align="center" rowspan="1" colspan="1">182</td>
                <td align="center" rowspan="1" colspan="1">100</td>
                <td align="center" rowspan="1" colspan="1">110</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">3</td>
                <td align="center" rowspan="1" colspan="1">132</td>
                <td align="center" rowspan="1" colspan="1">337</td>
                <td align="center" rowspan="1" colspan="1">255</td>
                <td align="center" rowspan="1" colspan="1">389</td>
                <td align="center" rowspan="1" colspan="1">295</td>
                <td align="center" rowspan="1" colspan="1">353</td>
                <td align="center" rowspan="1" colspan="1">267</td>
                <td align="center" rowspan="1" colspan="1">135</td>
                <td align="center" rowspan="1" colspan="1">102</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">4</td>
                <td align="center" rowspan="1" colspan="1">53</td>
                <td align="center" rowspan="1" colspan="1">122</td>
                <td align="center" rowspan="1" colspan="1">230</td>
                <td align="center" rowspan="1" colspan="1">153</td>
                <td align="center" rowspan="1" colspan="1">289</td>
                <td align="center" rowspan="1" colspan="1">128</td>
                <td align="center" rowspan="1" colspan="1">242</td>
                <td align="center" rowspan="1" colspan="1">65</td>
                <td align="center" rowspan="1" colspan="1">123</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">5</td>
                <td align="center" rowspan="1" colspan="1">131</td>
                <td align="center" rowspan="1" colspan="1">258</td>
                <td align="center" rowspan="1" colspan="1">197</td>
                <td align="center" rowspan="1" colspan="1">392</td>
                <td align="center" rowspan="1" colspan="1">299</td>
                <td align="center" rowspan="1" colspan="1">284</td>
                <td align="center" rowspan="1" colspan="1">217</td>
                <td align="center" rowspan="1" colspan="1">132</td>
                <td align="center" rowspan="1" colspan="1">101</td>
              </tr>
              <tr>
                <td align="center" colspan="2" rowspan="1">Average (Standard deviation)</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">206.0<break/>(35.9)</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">268.0<break/>(36.4)</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">213.2<break/>(36.03)</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">107.8<break/>(9.2)</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>When it comes to real images, the different versions of MP-VAT have even more extreme percentage recoveries, as demonstrated in <xref rid="pone.0269449.t006" ref-type="table">Table 6</xref>. Significantly high values of 3,200% and 5,940% were observed for MP-VAT and MP-VAT 2.0, respectively. C-VAT exhibits even higher percentage recoveries, reaching values of up to 57,020%. For MP-Net, overestimation was also observed but to a lesser extent, with a maximum percentage recovery of 150%. Some cases of underestimation were also detected. For instance, samples 5 and 8 have percentage recoveries of only 83% and 77%, respectively. Despite these high and low percentage recoveries obtained by MP-Net for real images, its output is still the closest to the ground truth, with an average percentage recovery of 107.5±21.0%.</p>
      <table-wrap position="float" id="pone.0269449.t006">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.t006</object-id>
        <label>Table 6</label>
        <caption>
          <title>MP count and percentage recovery obtained for 15 clam MP images.</title>
          <p>The MP quantity predicted by MP-Net was much closer to the ground truth compared to MP-VAT, MP-VAT 2.0, and C-VAT.</p>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0269449.t006" id="pone.0269449.t006g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="center" rowspan="2" colspan="1">Sample number</th>
                <th align="center" rowspan="2" colspan="1">Ground truth<break/>(Majority vote)</th>
                <th align="center" colspan="2" rowspan="1">MP-VAT</th>
                <th align="center" colspan="2" rowspan="1">MP-VAT 2.0</th>
                <th align="center" colspan="2" rowspan="1">C-VAT</th>
                <th align="center" colspan="2" rowspan="1">MP-Net</th>
              </tr>
              <tr>
                <th align="center" rowspan="1" colspan="1">MP count</th>
                <th align="center" rowspan="1" colspan="1">Percentage recovery</th>
                <th align="center" rowspan="1" colspan="1">MP count</th>
                <th align="center" rowspan="1" colspan="1">Percentage recovery</th>
                <th align="center" rowspan="1" colspan="1">MP count</th>
                <th align="center" rowspan="1" colspan="1">Percentage recovery</th>
                <th align="center" rowspan="1" colspan="1">MP count</th>
                <th align="center" rowspan="1" colspan="1">Percentage recovery</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="center" rowspan="1" colspan="1">1</td>
                <td align="center" rowspan="1" colspan="1">19</td>
                <td align="center" rowspan="1" colspan="1">29</td>
                <td align="center" rowspan="1" colspan="1">153</td>
                <td align="center" rowspan="1" colspan="1">602</td>
                <td align="center" rowspan="1" colspan="1">3,168</td>
                <td align="center" rowspan="1" colspan="1">2,951</td>
                <td align="center" rowspan="1" colspan="1">15,531</td>
                <td align="center" rowspan="1" colspan="1">17</td>
                <td align="center" rowspan="1" colspan="1">89</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">5</td>
                <td align="center" rowspan="1" colspan="1">19</td>
                <td align="center" rowspan="1" colspan="1">380</td>
                <td align="center" rowspan="1" colspan="1">21</td>
                <td align="center" rowspan="1" colspan="1">420</td>
                <td align="center" rowspan="1" colspan="1">2,851</td>
                <td align="center" rowspan="1" colspan="1">57,020</td>
                <td align="center" rowspan="1" colspan="1">6</td>
                <td align="center" rowspan="1" colspan="1">120</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">3</td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">11</td>
                <td align="center" rowspan="1" colspan="1">138</td>
                <td align="center" rowspan="1" colspan="1">14</td>
                <td align="center" rowspan="1" colspan="1">175</td>
                <td align="center" rowspan="1" colspan="1">185</td>
                <td align="center" rowspan="1" colspan="1">2,312</td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">100</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">4</td>
                <td align="center" rowspan="1" colspan="1">6</td>
                <td align="center" rowspan="1" colspan="1">7</td>
                <td align="center" rowspan="1" colspan="1">117</td>
                <td align="center" rowspan="1" colspan="1">326</td>
                <td align="center" rowspan="1" colspan="1">5,433</td>
                <td align="center" rowspan="1" colspan="1">256</td>
                <td align="center" rowspan="1" colspan="1">4,266</td>
                <td align="center" rowspan="1" colspan="1">5</td>
                <td align="center" rowspan="1" colspan="1">83</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">5</td>
                <td align="center" rowspan="1" colspan="1">7</td>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">114</td>
                <td align="center" rowspan="1" colspan="1">18</td>
                <td align="center" rowspan="1" colspan="1">257</td>
                <td align="center" rowspan="1" colspan="1">355</td>
                <td align="center" rowspan="1" colspan="1">5,071</td>
                <td align="center" rowspan="1" colspan="1">6</td>
                <td align="center" rowspan="1" colspan="1">86</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">6</td>
                <td align="center" rowspan="1" colspan="1">5</td>
                <td align="center" rowspan="1" colspan="1">16</td>
                <td align="center" rowspan="1" colspan="1">320</td>
                <td align="center" rowspan="1" colspan="1">297</td>
                <td align="center" rowspan="1" colspan="1">5,940</td>
                <td align="center" rowspan="1" colspan="1">15</td>
                <td align="center" rowspan="1" colspan="1">300</td>
                <td align="center" rowspan="1" colspan="1">7</td>
                <td align="center" rowspan="1" colspan="1">140</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">7</td>
                <td align="center" rowspan="1" colspan="1">383</td>
                <td align="center" rowspan="1" colspan="1">655</td>
                <td align="center" rowspan="1" colspan="1">171</td>
                <td align="center" rowspan="1" colspan="1">609</td>
                <td align="center" rowspan="1" colspan="1">159</td>
                <td align="center" rowspan="1" colspan="1">46,066</td>
                <td align="center" rowspan="1" colspan="1">12,027</td>
                <td align="center" rowspan="1" colspan="1">294</td>
                <td align="center" rowspan="1" colspan="1">77</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">8</td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">64</td>
                <td align="center" rowspan="1" colspan="1">3,200</td>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">450</td>
                <td align="center" rowspan="1" colspan="1">283</td>
                <td align="center" rowspan="1" colspan="1">14,150</td>
                <td align="center" rowspan="1" colspan="1">2</td>
                <td align="center" rowspan="1" colspan="1">100</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">9</td>
                <td align="center" rowspan="1" colspan="1">4</td>
                <td align="center" rowspan="1" colspan="1">39</td>
                <td align="center" rowspan="1" colspan="1">975</td>
                <td align="center" rowspan="1" colspan="1">34</td>
                <td align="center" rowspan="1" colspan="1">850</td>
                <td align="center" rowspan="1" colspan="1">26</td>
                <td align="center" rowspan="1" colspan="1">650</td>
                <td align="center" rowspan="1" colspan="1">6</td>
                <td align="center" rowspan="1" colspan="1">150</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">10</td>
                <td align="center" rowspan="1" colspan="1">223</td>
                <td align="center" rowspan="1" colspan="1">564</td>
                <td align="center" rowspan="1" colspan="1">253</td>
                <td align="center" rowspan="1" colspan="1">449</td>
                <td align="center" rowspan="1" colspan="1">201</td>
                <td align="center" rowspan="1" colspan="1">22,819</td>
                <td align="center" rowspan="1" colspan="1">10,232</td>
                <td align="center" rowspan="1" colspan="1">230</td>
                <td align="center" rowspan="1" colspan="1">103</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">11</td>
                <td align="center" rowspan="1" colspan="1">11</td>
                <td align="center" rowspan="1" colspan="1">38</td>
                <td align="center" rowspan="1" colspan="1">345</td>
                <td align="center" rowspan="1" colspan="1">28</td>
                <td align="center" rowspan="1" colspan="1">255</td>
                <td align="center" rowspan="1" colspan="1">653</td>
                <td align="center" rowspan="1" colspan="1">5,936</td>
                <td align="center" rowspan="1" colspan="1">13</td>
                <td align="center" rowspan="1" colspan="1">118</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">12</td>
                <td align="center" rowspan="1" colspan="1">26</td>
                <td align="center" rowspan="1" colspan="1">42</td>
                <td align="center" rowspan="1" colspan="1">162</td>
                <td align="center" rowspan="1" colspan="1">49</td>
                <td align="center" rowspan="1" colspan="1">188</td>
                <td align="center" rowspan="1" colspan="1">261</td>
                <td align="center" rowspan="1" colspan="1">1,003</td>
                <td align="center" rowspan="1" colspan="1">27</td>
                <td align="center" rowspan="1" colspan="1">104</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">13</td>
                <td align="center" rowspan="1" colspan="1">74</td>
                <td align="center" rowspan="1" colspan="1">268</td>
                <td align="center" rowspan="1" colspan="1">362</td>
                <td align="center" rowspan="1" colspan="1">138</td>
                <td align="center" rowspan="1" colspan="1">186</td>
                <td align="center" rowspan="1" colspan="1">5,378</td>
                <td align="center" rowspan="1" colspan="1">7,267</td>
                <td align="center" rowspan="1" colspan="1">92</td>
                <td align="center" rowspan="1" colspan="1">124</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">14</td>
                <td align="center" rowspan="1" colspan="1">94</td>
                <td align="center" rowspan="1" colspan="1">294</td>
                <td align="center" rowspan="1" colspan="1">313</td>
                <td align="center" rowspan="1" colspan="1">3,145</td>
                <td align="center" rowspan="1" colspan="1">3,346</td>
                <td align="center" rowspan="1" colspan="1">20,943</td>
                <td align="center" rowspan="1" colspan="1">22,279</td>
                <td align="center" rowspan="1" colspan="1">114</td>
                <td align="center" rowspan="1" colspan="1">121</td>
              </tr>
              <tr>
                <td align="center" rowspan="1" colspan="1">15</td>
                <td align="center" rowspan="1" colspan="1">138</td>
                <td align="center" rowspan="1" colspan="1">288</td>
                <td align="center" rowspan="1" colspan="1">209</td>
                <td align="center" rowspan="1" colspan="1">1,301</td>
                <td align="center" rowspan="1" colspan="1">943</td>
                <td align="center" rowspan="1" colspan="1">2,304</td>
                <td align="center" rowspan="1" colspan="1">1,669</td>
                <td align="center" rowspan="1" colspan="1">135</td>
                <td align="center" rowspan="1" colspan="1">98</td>
              </tr>
              <tr>
                <td align="center" colspan="2" rowspan="1">Average<break/>(Standard deviation)</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">480.8<break/>(781.5)</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">1,467.7<break/>(2,002.5)</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">10,647<break/>(13,415.1)</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">107.5<break/>(21.0)</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>Overall, the different versions of MP-VAT and C-VAT consistently produce over-inflated estimates of the MP quantity in both spiked and real images. As shown in <xref rid="pone.0269449.g011" ref-type="fig">Fig 11(a)–11(c) and 11e–11(g)</xref>, the predicted values fall far from the ground truth, which is depicted by the blue line. On the other hand, predictions from MP-Net lie closer to the blue line, as illustrated in <xref rid="pone.0269449.g011" ref-type="fig">Fig 11(d) and 11(h)</xref>.</p>
      <fig position="float" id="pone.0269449.g011">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g011</object-id>
        <label>Fig 11</label>
        <caption>
          <title>Recovery assessment.</title>
          <p>Comparison of MP counts obtained for five spiked images (a)—(d) and 15 real images from clam samples (e)—(h). Red dots represent tested images. The x-coordinate refers to the ground truth MP count and the y-coordinate denotes the predicted MP count. For a red dot above the first bisector (blue line), the number of MPs is overestimated by the model (MP-VAT, MP-VAT 2.0, C-VAT and MP-Net). (a) MP-VAT. (b) MP-VAT 2.0. (c) C-VAT. (d) MP-Net. (e) MP-VAT. (f) MP-VAT. (g) C-VAT. (h) MP-Net. MP count on spiked images (a)–(d). MP count on images from clam samples (e)–(h).</p>
        </caption>
        <graphic xlink:href="pone.0269449.g011" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec028">
    <title>6 Conclusions</title>
    <p>In this study, we proposed and evaluated several computational approaches towards MP quantification. To that end, we created a set of fluorescence microscopy images of MP, together with corresponding binary masks. This was done by first extracting MP from clams and by subsequently applying Nile red staining, making it possible to highlight MP debris under a microscope. Through a thorough survey of the literature, we identified already existing semi- or fully automated tools used to analyze MP. Applying three of these tools, namely MP-VAT, MP-VAT 2.0, and C-VAT, to our set of MP images showed that the use of a global TR-based method results in MP overestimation and mislabeling. We hypothesized that these weaknesses could be overcome by leveraging a TR-based deep learning model, with this end-to-end learning approach being able to mimic the way researchers identify MP in given fluorescence microscopy images.</p>
    <p>Next to MP-VAT, MP-VAT 2.0, and C-VAT, we tested and compared a total of nine deep learning models through common performance metrics and visual inspection of the resulting mask patches. U-Net (4), which we named MP-Net, was found to be the most effective in terms of MP segmentation. Specifically, MP-Net exhibited the highest m<italic toggle="yes">F</italic><sub>1</sub>-score and mIoU. Furthermore, we found the masks generated by MP-Net to be the most consistent with the ground truth masks. Recovery assessment using both spiked and real images also demonstrated that MP-Net produces better predictions of MP count than the three already existing tools. Indeed, even though slightly under- and overestimated MP counts were observed for MP-Net, the percentage recoveries were much closer to 100%, compared to the percentage recoveries obtained by MP-VAT, MP-VAT 2.0, and C-VAT.</p>
    <p>As part of the study presented in this manuscript, we make available MAP, which stands for Microplastics Annotation Package, an integrated MP annotation tool equipped with a user-friendly graphical user interface. Specifically, MAP incorporates MP-Net, the first TR-based deep learning model for MP detection, and the different TR-based models shown in <xref rid="pone.0269449.t002" ref-type="table">Table 2</xref>. For each piece of MP found in a mask (MC), MAP also creates a summary table containing SM and SC. Furthermore, MAP provides options for manual annotation and model fine-tuning, thereby allowing a model to be tailored to the specific needs of researchers.</p>
    <p>Through this study, we would like to encourage researchers in the field of computer vision to consider expanding their field of research with the topic of MP analysis. To that end, all the code used in our experiments is made publicly available, together with a representative set of patches obtained from real images (see Section 8.2). The five spiked images used in our recovery assessment experiment are also released, together with the bright-field microscopy images and the corresponding masks (see Section 8.3).</p>
  </sec>
  <sec id="sec029">
    <title>7 Directions for future work</title>
    <sec id="sec030">
      <title>7.1 Improving segmentation effectiveness</title>
      <sec id="sec031">
        <title>7.1.1 Investigating encoder and augmentation combinations</title>
        <p>Looking at the additional experimental results provided in S6.1 and S6.2 Sections in <xref rid="pone.0269449.s005" ref-type="supplementary-material">S5 File</xref>, we believe it is possible to still find a better model than the already existing ones. In particular, although the performance evaluation presented in S6.1 and S6.2 Sections in <xref rid="pone.0269449.s005" ref-type="supplementary-material">S5 File</xref> was conducted only for Fine-tuning (1) (see <xref rid="pone.0269449.g005" ref-type="fig">Fig 5</xref>), the use of different encoder and augmentation combinations may result in a higher effectiveness than that of MP-Net. Indeed, as illustrated in S6.1 Section in <xref rid="pone.0269449.s005" ref-type="supplementary-material">S5 File</xref>, the use of a ResNet-18 encoder or a ResNet-34 encoder allows for a higher effectiveness than the use of a ResNet-101 encoder. In addition, augmentation by combining random brightness and random contrast allows achieving the highest values in terms of mAccuracy, mRecall, m<italic toggle="yes">F</italic><sub>1</sub>-score, and mIoU, as shown in S6.3 Section in <xref rid="pone.0269449.s005" ref-type="supplementary-material">S5 File</xref>. Considering these encoder and augmentation combinations at the same time, it may be possible to obtain a TR-based deep learning model that is even better performing. However, more extensive testing is needed to confirm this hypothesis.</p>
      </sec>
      <sec id="sec032">
        <title>7.1.2 Use of generative adversarial networks</title>
        <p>As a candidate technique for building TR-based deep learning models, it may be of interest to consider the adoption of Generative Adversarial Networks (GANs). A GAN is a type of deep learning model that was introduced in [<xref rid="pone.0269449.ref039" ref-type="bibr">39</xref>]. If we see a conventional deep learning model as a single model that learns the relationship between input and ground truth pairs, making predictions close to the ground truth, then GANs can be seen as structures that are composed of two models, namely a generator and a discriminator. Assuming a computer vision use case, the generator receives a random number and generates a fake input image. The discriminator then determines whether the input image is real or fake. When these two models go through a series of training steps, the generator learns to produce more plausible images. In other words, the generator learns to understand the desired image characteristics.</p>
        <p>Starting from the MNIST dataset [<xref rid="pone.0269449.ref040" ref-type="bibr">40</xref>], GANs have been gradually applied to computer vision problems. For instance, [<xref rid="pone.0269449.ref041" ref-type="bibr">41</xref>, <xref rid="pone.0269449.ref042" ref-type="bibr">42</xref>] present semi-supervised classification approaches using GANs. Furthermore, Xue et al. [<xref rid="pone.0269449.ref043" ref-type="bibr">43</xref>], Zhang et al. [<xref rid="pone.0269449.ref044" ref-type="bibr">44</xref>], and Majurski et al. [<xref rid="pone.0269449.ref045" ref-type="bibr">45</xref>] are addressing image segmentation problems using GANs.</p>
        <p>Pix2pix [<xref rid="pone.0269449.ref046" ref-type="bibr">46</xref>], which uses conditional GANs, may be of particular interest. Here, a generator takes as input an edge outline, a map, or a mask and then produces an output image corresponding to the given input. The following use cases are suggested by the authors: (1) input: satellite image, output: corresponding map image and (2) input: a silhouette image of an object (e.g., a pair of shoes), output: an image containing the corresponding object. If such a model receives a fluorescence microscopy image as input and subsequently outputs a corresponding MP mask, then the effectiveness of this model can be evaluated and compared with other deep learning-based approaches.</p>
      </sec>
      <sec id="sec033">
        <title>7.1.3 Use of active learning (human-in-the-loop)</title>
        <p>It may be of interest to incorporate a tool from the field of artificial intelligence that is commonly referred to as Active Learning [<xref rid="pone.0269449.ref047" ref-type="bibr">47</xref>] or Suggestive Annotation. Specifically, using this tool, it is possible to calculate the uncertainty of each pixel in a given image and to actively suggest regions of high uncertainty for researchers to examine and decide upon. That way, the model is able to learn the annotation strategy used by researchers, leading to more accurate annotations.</p>
      </sec>
    </sec>
    <sec id="sec034">
      <title>7.2 Upgrading MAP</title>
      <p>Our annotation software can be further improved in three ways.</p>
      <p>First, it would be of interest to implement the RGB thresholding method of ImageJ. Unlike already existing TR-based models that use a single threshold value, ImageJ has an option for distinguishing MP pixels from background pixels by setting six thresholds, namely a minimum and maximum threshold for each RGB channel. By incorporating this approach, users will be able to proceed with MP monitoring in a more flexible way.</p>
      <p>Second, it would be of interest to have support for individual MP visualization. Currently, it is not possible to highlight a particular piece of MP in an image. Instead, all detected MP are highlighted and enclosed in a box as shown in <xref rid="pone.0269449.g012" ref-type="fig">Fig 12(a)</xref>. This can be improved by adding a function that highlights a selected piece of MP, as shown in <xref rid="pone.0269449.g012" ref-type="fig">Fig 12(b)</xref>, providing detailed visual information regarding its size and shape. That way, it is possible to generate better visualizations for images having a high MP density. In relation to this, filtered visualizations could also be incorporated where only MP with a specific shape or a particular size range are highlighted. In that manner, the annotation software becomes more interactive.</p>
      <fig position="float" id="pone.0269449.g012">
        <object-id pub-id-type="doi">10.1371/journal.pone.0269449.g012</object-id>
        <label>Fig 12</label>
        <caption>
          <title>Representative patches illustrating MP visualization in MAP.</title>
        </caption>
        <graphic xlink:href="pone.0269449.g012" position="float"/>
      </fig>
      <p>Third, it would be of interest to have support for batch processing and batch reports. Usually, MP monitoring studies generate one fluorescence image per sample. Each image is then individually analyzed using the desired TR-based method, resulting in the generation of a file containing MC, SM, and SC for each piece of MP found in the given image. This means that, when there are multiple images, an identical number of files will be produced. Researchers have to collect and organize these files in order to analyze and synthesize the data generated. To assist researchers in this task, we plan to add a function to MAP that provides batch processing of multiple images, followed by the generation of an integrated statistical report containing summarizing charts and tables, thus making it possible to reduce the amount of time spent by researchers on data curation.</p>
    </sec>
  </sec>
  <sec id="sec035">
    <title>8 Software and dataset availability</title>
    <sec id="sec036">
      <title>8.1 MAP: A GUI-based annotation tool</title>
      <list list-type="bullet">
        <list-item>
          <p>Software name: Microplastics Annotation Package (MAP)</p>
        </list-item>
        <list-item>
          <p>Hardware &amp; OS requirements: PC (Windows, Linux, Mac)</p>
        </list-item>
        <list-item>
          <p>Year of first official release: 2021</p>
        </list-item>
        <list-item>
          <p>Code size: 8 MB (+35 ∼ 233 MB for model parameters)</p>
        </list-item>
        <list-item>
          <p>Availability: <ext-link xlink:href="https://github.com/powersimmani/Microplastics-Annotation-Package" ext-link-type="uri">https://github.com/powersimmani/Microplastics-Annotation-Package</ext-link></p>
        </list-item>
        <list-item>
          <p>License: GPL 3.0</p>
        </list-item>
        <list-item>
          <p>Documentation: README in GitHub repository</p>
        </list-item>
        <list-item>
          <p>Video tutorial: <ext-link xlink:href="https://www.youtube.com/watch?v=ehmRbTbqOKU" ext-link-type="uri">https://www.youtube.com/watch?v=ehmRbTbqOKU</ext-link></p>
        </list-item>
      </list>
    </sec>
    <sec id="sec037">
      <title>8.2 MP-Net: Training and evaluation code for MP segmentation</title>
      <list list-type="bullet">
        <list-item>
          <p>Software name: MP-Net</p>
        </list-item>
        <list-item>
          <p>Hardware &amp; OS requirements: PC (Windows, Linux, Mac)</p>
        </list-item>
        <list-item>
          <p>Year of first official release: 2021</p>
        </list-item>
        <list-item>
          <p>Code size: 8 MB (+35 ∼ 233 MB for model parameters)</p>
        </list-item>
        <list-item>
          <p>Availability: <ext-link xlink:href="https://github.com/sanghyeonp/MP-Net" ext-link-type="uri">https://github.com/sanghyeonp/MP-Net</ext-link></p>
        </list-item>
        <list-item>
          <p>License: MIT 3.0</p>
        </list-item>
        <list-item>
          <p>Documentation: README in GitHub repository</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec038">
      <title>8.3 MP-Set: Fluorescence image and mask pairs used to obtain the experimental results</title>
      <list list-type="bullet">
        <list-item>
          <p>Dataset name: MP-Set</p>
        </list-item>
        <list-item>
          <p>Year of first official release: 2022</p>
        </list-item>
        <list-item>
          <p>Dataset size: 2.25 GB</p>
        </list-item>
        <list-item>
          <p>Availability: <ext-link xlink:href="http://www.kaggle.com/sanghyeonaustinpark/mpset" ext-link-type="uri">www.kaggle.com/sanghyeonaustinpark/mpset</ext-link></p>
        </list-item>
        <list-item>
          <p>License: CC-BY 4.0</p>
        </list-item>
        <list-item>
          <p>Documentation: README in repository</p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec id="sec039" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pone.0269449.s001" position="float" content-type="local-data">
      <label>S1 File</label>
      <caption>
        <title>Dataset preparation.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0269449.s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0269449.s002" position="float" content-type="local-data">
      <label>S2 File</label>
      <caption>
        <title>Fine-tuning phase.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0269449.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0269449.s003" position="float" content-type="local-data">
      <label>S3 File</label>
      <caption>
        <title>Learning curves.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0269449.s003.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0269449.s004" position="float" content-type="local-data">
      <label>S4 File</label>
      <caption>
        <title>Testing phase.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0269449.s004.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0269449.s005" position="float" content-type="local-data">
      <label>S5 File</label>
      <caption>
        <title>Additional results.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0269449.s005.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0269449.s006" position="float" content-type="local-data">
      <label>S1 Table</label>
      <caption>
        <title>Statistical analysis.</title>
        <p>(XLSX)</p>
      </caption>
      <media xlink:href="pone.0269449.s006.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0269449.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Cole</surname><given-names>M</given-names></name>, <name><surname>Lindeque</surname><given-names>P</given-names></name>, <name><surname>Halsband</surname><given-names>C</given-names></name>, <name><surname>Galloway</surname><given-names>TS</given-names></name>. <article-title>Microplastics as contaminants in the marine environment: A review</article-title>. <source>Marine Pollution Bulletin</source>. <year>2011</year>;<volume>62</volume>(<issue>12</issue>):<fpage>2588</fpage>–<lpage>2597</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.marpolbul.2011.09.025</pub-id><?supplied-pmid 22001295?><pub-id pub-id-type="pmid">22001295</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Lebreton</surname><given-names>L</given-names></name>, <name><surname>Slat</surname><given-names>B</given-names></name>, <name><surname>Ferrari</surname><given-names>F</given-names></name>, <name><surname>Sainte-Rose</surname><given-names>B</given-names></name>, <name><surname>Aitken</surname><given-names>J</given-names></name>, <name><surname>Marthouse</surname><given-names>R</given-names></name>. <article-title>Evidence that the Great Pacific Garbage Patch is rapidly accumulating plastic</article-title>. <source>Scientific Reports</source>. <year>2018</year>;<volume>8</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41598-018-22939-w</pub-id><?supplied-pmid 29568057?><pub-id pub-id-type="pmid">29311619</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Cressey</surname><given-names>D</given-names></name>. <article-title>The plastic ocean</article-title>. <source>Nature</source>. <year>2016</year>;<volume>536</volume>(<issue>7616</issue>):<fpage>263</fpage>–<lpage>265</lpage>.<pub-id pub-id-type="pmid">27535517</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Silva</surname><given-names>AB</given-names></name>, <name><surname>Bastos</surname><given-names>AS</given-names></name>, <name><surname>Justino</surname><given-names>CIL</given-names></name>, <name><surname>da Costa</surname><given-names>JP</given-names></name>, <name><surname>Duarte</surname><given-names>AC</given-names></name>, <name><surname>Rocha-Santos</surname><given-names>TAP</given-names></name>. <article-title>Microplastics in the environment: Challenges in analytical chemistry—A review</article-title>. <source>Analytica Chimica Acta</source>. <year>2018</year>;<volume>1017</volume>:<fpage>1</fpage>–<lpage>19</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.aca.2018.02.043</pub-id><?supplied-pmid 29534790?><pub-id pub-id-type="pmid">29534790</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Wesch</surname><given-names>C</given-names></name>, <name><surname>Bredimus</surname><given-names>K</given-names></name>, <name><surname>Paulus</surname><given-names>M</given-names></name>, <name><surname>Klein</surname><given-names>R</given-names></name>. <article-title>Towards the suitable monitoring of ingestion of microplastics by marine biota: A review</article-title>. <source>Environmental Pollution</source>. <year>2016</year>;<volume>218</volume>:<fpage>1200</fpage>–<lpage>1208</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.envpol.2016.08.076</pub-id><?supplied-pmid 27593351?><pub-id pub-id-type="pmid">27593351</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Prata</surname><given-names>JC</given-names></name>, <name><surname>Reis</surname><given-names>V</given-names></name>, <name><surname>Matos</surname><given-names>JT</given-names></name>, <name><surname>da Costa</surname><given-names>JP</given-names></name>, <name><surname>Duarte</surname><given-names>AC</given-names></name>, <name><surname>Rocha-Santos</surname><given-names>T</given-names></name>. <article-title>A new approach for routine quantification of microplastics using Nile Red and automated software (MP-VAT)</article-title>. <source>Science of The Total Environment</source>. <year>2019</year>;<volume>690</volume>:<fpage>1277</fpage>–<lpage>1283</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.scitotenv.2019.07.060</pub-id><?supplied-pmid 31470490?><pub-id pub-id-type="pmid">31470490</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Prata</surname><given-names>JC</given-names></name>, <name><surname>Alves</surname><given-names>JR</given-names></name>, <name><surname>da Costa</surname><given-names>JP</given-names></name>, <name><surname>Duarte</surname><given-names>AC</given-names></name>, <name><surname>Rocha-Santos</surname><given-names>T</given-names></name>. <article-title>Major factors influencing the quantification of Nile Red stained microplastics and improved automatic quantification (MP-VAT 2.0)</article-title>. <source>Science of The Total Environment</source>. <year>2020</year>;<volume>719</volume>:<fpage>137498</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.scitotenv.2020.137498</pub-id><?supplied-pmid 32120106?><pub-id pub-id-type="pmid">32120106</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref008">
      <label>8</label>
      <mixed-citation publication-type="other">Baek JY, de Guzman MK, Park HM, Park S, Shin B, Velickovic TC, et al. Developing a Segmentation Model for Microscopic Images of Microplastics Isolated from Clams. In: Pattern Recognition. ICPR International Workshops and Challenges. Springer International Publishing; 2021. p. 86–97.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Schneider</surname><given-names>CA</given-names></name>, <name><surname>Rasband</surname><given-names>WS</given-names></name>, <name><surname>Eliceiri</surname><given-names>KW</given-names></name>. <article-title>NIH Image to ImageJ: 25 years of image analysis</article-title>. <source>Nature Methods</source>. <year>2012</year>;<volume>9</volume>(<issue>7</issue>):<fpage>671</fpage>–<lpage>675</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/nmeth.2089</pub-id><?supplied-pmid 22930834?><pub-id pub-id-type="pmid">22930834</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Anger</surname><given-names>PM</given-names></name>, <name><surname>von der Esch</surname><given-names>E</given-names></name>, <name><surname>Baumann</surname><given-names>T</given-names></name>, <name><surname>Elsner</surname><given-names>M</given-names></name>, <name><surname>Niessner</surname><given-names>R</given-names></name>, <name><surname>Ivleva</surname><given-names>NP</given-names></name>. <article-title>Raman microspectroscopy as a tool for microplastic particle analysis</article-title>. <source>TrAC Trends in Analytical Chemistry</source>. <year>2018</year>;<volume>109</volume>:<fpage>214</fpage>–<lpage>226</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.trac.2018.10.010</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>von der Esch</surname><given-names>E</given-names></name>, <name><surname>Kohles</surname><given-names>AJ</given-names></name>, <name><surname>Anger</surname><given-names>PM</given-names></name>, <name><surname>Hoppe</surname><given-names>R</given-names></name>, <name><surname>Niessner</surname><given-names>R</given-names></name>, <name><surname>Elsner</surname><given-names>M</given-names></name>. <article-title>TUM-ParticleTyper: A detection and quantification tool for automated analysis of (Microplastic) particles and fibers</article-title>. <source>PLOS ONE</source>. <year>2020</year>;<volume>15</volume>(<issue>6</issue>):<fpage>1</fpage>–<lpage>20</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0234766</pub-id><?supplied-pmid 32574195?><pub-id pub-id-type="pmid">32574195</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Kumar</surname><given-names>BV</given-names></name>, <name><surname>Löschel</surname><given-names>LA</given-names></name>, <name><surname>Imhof</surname><given-names>HK</given-names></name>, <name><surname>Löder</surname><given-names>MG</given-names></name>, <name><surname>Laforsch</surname><given-names>C</given-names></name>. <article-title>Analysis of microplastics of a broad size range in commercially important mussels by combining FTIR and Raman spectroscopy approaches</article-title>. <source>Environmental Pollution</source>. <year>2021</year>;<volume>269</volume>:<fpage>116147</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.envpol.2020.116147</pub-id><pub-id pub-id-type="pmid">33280916</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Ng</surname><given-names>W</given-names></name>, <name><surname>Minasny</surname><given-names>B</given-names></name>, <name><surname>McBratney</surname><given-names>A</given-names></name>. <article-title>Convolutional neural network for soil microplastic contamination screening using infrared spectroscopy</article-title>. <source>Science of the Total Environment</source>. <year>2020</year>;<volume>702</volume>:<fpage>134723</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.scitotenv.2019.134723</pub-id><?supplied-pmid 31731131?><pub-id pub-id-type="pmid">31731131</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Sridhar</surname><given-names>A</given-names></name>, <name><surname>Kannan</surname><given-names>D</given-names></name>, <name><surname>Kapoor</surname><given-names>A</given-names></name>, <name><surname>Prabhakar</surname><given-names>S</given-names></name>. <article-title>Extraction and detection methods of microplastics in food and marine systems: a critical review</article-title>. <source>Chemosphere</source>. <year>2022</year>;<volume>286</volume>:<fpage>131653</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.chemosphere.2021.131653</pub-id><?supplied-pmid 34346338?><pub-id pub-id-type="pmid">34346338</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Shruti</surname><given-names>V</given-names></name>, <name><surname>Pérez-Guevara</surname><given-names>F</given-names></name>, <name><surname>Roy</surname><given-names>PD</given-names></name>, <name><surname>Kutralam-Muniasamy</surname><given-names>G</given-names></name>. <article-title>Analyzing microplastics with Nile Red: Emerging trends, challenges, and prospects</article-title>. <source>Journal of Hazardous Materials</source>. <year>2022</year>;<volume>423</volume>:<fpage>127171</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jhazmat.2021.127171</pub-id><?supplied-pmid 34537648?><pub-id pub-id-type="pmid">34537648</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Masura</surname><given-names>J</given-names></name>, <name><surname>Baker</surname><given-names>J</given-names></name>, <name><surname>Foster</surname><given-names>G</given-names></name>, <name><surname>Arthur</surname><given-names>C</given-names></name>. <article-title>Laboratory Methods for the Analysis of Microplastics in the Marine Environment: Recommendations for quantifying synthetic particles in waters and sediments</article-title>. <source>NOAA Technical Memorandum NOS-OR&amp;R-48</source>. <year>2015</year>;.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Mukhanov</surname><given-names>VS</given-names></name>, <name><surname>Litvinyuk</surname><given-names>DA</given-names></name>, <name><surname>Sakhon</surname><given-names>EG</given-names></name>, <name><surname>Bagaev</surname><given-names>AV</given-names></name>, <name><surname>Veerasingam</surname><given-names>S</given-names></name>, <name><surname>Venkatachalapathy</surname><given-names>R</given-names></name>. <article-title>A new method for analyzing microplastic particle size distribution in marine environmental samples</article-title>. <source>Ecologica Montenegrina</source>. <year>2019</year>;<volume>23</volume>:<fpage>77</fpage>–<lpage>86</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.37828/em.2019.23.10</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Lorenzo-Navarro</surname><given-names>J</given-names></name>, <name><surname>Castrillón-Santana</surname><given-names>M</given-names></name>, <name><surname>Santesarti</surname><given-names>E</given-names></name>, <name><surname>De Marsico</surname><given-names>M</given-names></name>, <name><surname>Martínez</surname><given-names>I</given-names></name>, <name><surname>Raymond</surname><given-names>E</given-names></name>, <etal>et al</etal>. <article-title>SMACC: A System for Microplastics Automatic Counting and Classification</article-title>. <source>IEEE Access</source>. <year>2020</year>;<volume>8</volume>:<fpage>25249</fpage>–<lpage>25261</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ACCESS.2020.2970498</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Lorenzo-Navarro</surname><given-names>J</given-names></name>, <name><surname>Castrillón-Santana</surname><given-names>M</given-names></name>, <name><surname>Sánchez-Nielsen</surname><given-names>E</given-names></name>, <name><surname>Zarco</surname><given-names>B</given-names></name>, <name><surname>Herrera</surname><given-names>A</given-names></name>, <name><surname>Martínez</surname><given-names>I</given-names></name>. <article-title>Deep learning approach for automatic microplastics counting and classification</article-title>. <source>Science of The Total Environment</source>. <year>2021</year>;<volume>765</volume>:<fpage>142728</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.scitotenv.2020.142728</pub-id><?supplied-pmid 33127127?><pub-id pub-id-type="pmid">33127127</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Massarelli</surname><given-names>C</given-names></name>, <name><surname>Campanale</surname><given-names>C</given-names></name>, <name><surname>Uricchio</surname><given-names>VF</given-names></name>. <article-title>A Handy Open-Source Application Based on Computer Vision and Machine Learning Algorithms to Count and Classify Microplastics</article-title>. <source>Water</source>. <year>2021</year>;<volume>13</volume>(<issue>15</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/w13152104</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Maes</surname><given-names>T</given-names></name>, <name><surname>Jessop</surname><given-names>R</given-names></name>, <name><surname>Wellner</surname><given-names>N</given-names></name>, <name><surname>Haupt</surname><given-names>K</given-names></name>, <name><surname>Mayes</surname><given-names>AG</given-names></name>. <article-title>A rapid-screening approach to detect and quantify microplastics based on fluorescent tagging with Nile Red</article-title>. <source>Scientific Reports</source>. <year>2017</year>;<volume>7</volume>(<issue>1</issue>). <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/srep44501</pub-id><?supplied-pmid 28300146?><pub-id pub-id-type="pmid">28300146</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Mason</surname><given-names>SA</given-names></name>, <name><surname>Welch</surname><given-names>VG</given-names></name>, <name><surname>Neratko</surname><given-names>J</given-names></name>. <article-title>Synthetic Polymer Contamination in Bottled Water</article-title>. <source>Frontiers in Chemistry</source>. <year>2018</year>;<volume>6</volume>:<fpage>407</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fchem.2018.00407</pub-id><?supplied-pmid 30255015?><pub-id pub-id-type="pmid">30255015</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>Patchaiyappan</surname><given-names>A</given-names></name>, <name><surname>Ahmed</surname><given-names>SZ</given-names></name>, <name><surname>Dowarah</surname><given-names>K</given-names></name>, <name><surname>Jayakumar</surname><given-names>S</given-names></name>, <name><surname>Devipriya</surname><given-names>SP</given-names></name>. <article-title>Occurrence, distribution and composition of microplastics in the sediments of South Andaman beaches</article-title>. <source>Marine Pollution Bulletin</source>. <year>2020</year>;<volume>156</volume>:<fpage>111227</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.marpolbul.2020.111227</pub-id><?supplied-pmid 32510373?><pub-id pub-id-type="pmid">32510373</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Dowarah</surname><given-names>K</given-names></name>, <name><surname>Patchaiyappan</surname><given-names>A</given-names></name>, <name><surname>Thirunavukkarasu</surname><given-names>C</given-names></name>, <name><surname>Jayakumar</surname><given-names>S</given-names></name>, <name><surname>Devipriya</surname><given-names>SP</given-names></name>. <article-title>Quantification of microplastics using Nile Red in two bivalve species Perna viridis and Meretrix meretrix from three estuaries in Pondicherry, India and microplastic uptake by local communities through bivalve diet</article-title>. <source>Marine Pollution Bulletin</source>. <year>2020</year>;<volume>153</volume>:<fpage>110982</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.marpolbul.2020.110982</pub-id><?supplied-pmid 32275539?><pub-id pub-id-type="pmid">32275539</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>S</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Mawhorter</surname><given-names>C</given-names></name>, <name><surname>Legoski</surname><given-names>S</given-names></name>. <article-title>Quantification of microplastics by count, size and morphology in beverage containers using Nile Red and ImageJ</article-title>. <source>Journal of Water and Health</source>. <year>2021</year>;<volume>19</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>88</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.2166/wh.2020.171</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Kapur</surname><given-names>JN</given-names></name>, <name><surname>Sahoo</surname><given-names>PK</given-names></name>, <name><surname>Wong</surname><given-names>AKC</given-names></name>. <article-title>A new method for gray-level picture thresholding using the entropy of the histogram</article-title>. <source>Computer Vision, Graphics, and Image Processing</source>. <year>1985</year>;<volume>29</volume>(<issue>3</issue>):<fpage>273</fpage>–<lpage>285</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0734-189X(85)90125-2</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Dehaut</surname><given-names>A</given-names></name>, <name><surname>Cassone</surname><given-names>AL</given-names></name>, <name><surname>Frère</surname><given-names>L</given-names></name>, <name><surname>Hermabessiere</surname><given-names>L</given-names></name>, <name><surname>Himber</surname><given-names>C</given-names></name>, <name><surname>Rinnert</surname><given-names>E</given-names></name>. <article-title>Microplastics in seafood: Benchmark protocol for their extraction and characterization</article-title>. <source>Environmental Pollution</source>. <year>2016</year>;<volume>215</volume>:<fpage>223</fpage>–<lpage>233</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.envpol.2016.05.018</pub-id><?supplied-pmid 27209243?><pub-id pub-id-type="pmid">27209243</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>LeCun</surname><given-names>Y</given-names></name>, <name><surname>Boser</surname><given-names>B</given-names></name>, <name><surname>Denker</surname><given-names>JS</given-names></name>, <name><surname>Henderson</surname><given-names>D</given-names></name>, <name><surname>Howard</surname><given-names>RE</given-names></name>, <name><surname>Hubbard</surname><given-names>W</given-names></name>. <article-title>Backpropagation Applied to Handwritten Zip Code Recognition</article-title>. <source>Neural computation</source>. <year>1989</year>;<volume>1</volume>(<issue>4</issue>):<fpage>541</fpage>–<lpage>551</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/neco.1989.1.4.541</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref029">
      <label>29</label>
      <mixed-citation publication-type="other">Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. Springer International Publishing; 2015. p. 234–241.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Shelhamer</surname><given-names>E</given-names></name>, <name><surname>Long</surname><given-names>J</given-names></name>, <name><surname>Darrell</surname><given-names>T</given-names></name>. <article-title>Fully Convolutional Networks for Semantic Segmentation</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2017</year>;<volume>39</volume>(<issue>4</issue>):<fpage>640</fpage>–<lpage>651</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2572683</pub-id><?supplied-pmid 27244717?><pub-id pub-id-type="pmid">27244717</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref031">
      <label>31</label>
      <mixed-citation publication-type="other">Chen LC, Zhu Y, Papandreou G, Schroff F, Adam H. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. Computer Vision—ECCV 2018 Lecture Notes in Computer Science. 2018; p. 833–851.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref032">
      <label>32</label>
      <mixed-citation publication-type="book"><name><surname>Zhou</surname><given-names>Z</given-names></name>, <name><surname>Rahman Siddiquee</surname><given-names>MM</given-names></name>, <name><surname>Tajbakhsh</surname><given-names>N</given-names></name>, <name><surname>Liang</surname><given-names>J</given-names></name>. <part-title>UNet++: A Nested U-Net Architecture for Medical Image Segmentation</part-title>. In: <name><surname>Stoyanov</surname><given-names>D</given-names></name>, <name><surname>Taylor</surname><given-names>Z</given-names></name>, <name><surname>Carneiro</surname><given-names>G</given-names></name>, <name><surname>Syeda-Mahmood</surname><given-names>T</given-names></name>, <name><surname>Martel</surname><given-names>A</given-names></name>, <name><surname>Maier-Hein</surname><given-names>L</given-names></name>, <name><surname>Tavares</surname><given-names>JMRS</given-names></name>, <name><surname>Bradley</surname><given-names>A</given-names></name>, <name><surname>Papa</surname><given-names>JP</given-names></name>, <name><surname>Belagiannis</surname><given-names>V</given-names></name>, <name><surname>Nascimento</surname><given-names>JC</given-names></name>, <name><surname>Lu</surname><given-names>Z</given-names></name>, <name><surname>Conjeti</surname><given-names>S</given-names></name>, <name><surname>Moradi</surname><given-names>M</given-names></name>, <name><surname>Greenspan</surname><given-names>H</given-names></name>, <name><surname>Madabhushi</surname><given-names>A</given-names></name>, editors. <source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2018</year>. p. <fpage>3</fpage>–<lpage>11</lpage>.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref033">
      <label>33</label>
      <mixed-citation publication-type="other">Tan C, Sun F, Kong T, Zhang W, Yang C, Liu C. A Survey on Deep Transfer Learning. In: International conference on artificial neural networks (ICANN). Springer International Publishing; 2018. p. 270–279.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name><surname>Sutskever</surname><given-names>I</given-names></name>, <name><surname>Hinton</surname><given-names>GE</given-names></name>. <article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title>. <source>Commun ACM</source>. <year>2017</year>;<volume>60</volume>(<issue>6</issue>):<fpage>84</fpage>–<lpage>90</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1145/3065386</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref035">
      <label>35</label>
      <mixed-citation publication-type="book"><name><surname>Lin</surname><given-names>TY</given-names></name>, <name><surname>Maire</surname><given-names>M</given-names></name>, <name><surname>Belongie</surname><given-names>S</given-names></name>, <name><surname>Hays</surname><given-names>J</given-names></name>, <name><surname>Perona</surname><given-names>P</given-names></name>, <name><surname>Ramanan</surname><given-names>D</given-names></name>. <part-title>Microsoft COCO: Common Objects in Context</part-title>. In: <name><surname>Fleet</surname><given-names>D</given-names></name>, <name><surname>Pajdla</surname><given-names>T</given-names></name>, <name><surname>Schiele</surname><given-names>B</given-names></name>, <name><surname>Tuytelaars</surname><given-names>T</given-names></name>, editors. <source>Computer Vision—ECCV 2014</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2014</year>. p. <fpage>740</fpage>–<lpage>755</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/978-3-319-10602-1_48</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Cho</surname><given-names>Y</given-names></name>, <name><surname>Shim</surname><given-names>WJ</given-names></name>, <name><surname>Jang</surname><given-names>M</given-names></name>, <name><surname>Han</surname><given-names>GM</given-names></name>, <name><surname>Hong</surname><given-names>SH</given-names></name>. <article-title>Nationwide monitoring of microplastics in bivalves from the coastal environment of Korea</article-title>. <source>Environmental Pollution</source>. <year>2021</year>;<volume>270</volume>:<fpage>116175</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.envpol.2020.116175</pub-id><?supplied-pmid 33352481?><pub-id pub-id-type="pmid">33352481</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref037">
      <label>37</label>
      <mixed-citation publication-type="other">Ruder S. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:160904747. 2016;.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref038">
      <label>38</label>
      <mixed-citation publication-type="other">Kingma DP, Ba J. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:14126980. 2014;.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref039">
      <label>39</label>
      <mixed-citation publication-type="book"><name><surname>Goodfellow</surname><given-names>I</given-names></name>, <name><surname>Pouget-Abadie</surname><given-names>J</given-names></name>, <name><surname>Mirza</surname><given-names>M</given-names></name>, <name><surname>Xu</surname><given-names>B</given-names></name>, <name><surname>Warde-Farley</surname><given-names>D</given-names></name>, <name><surname>Ozair</surname><given-names>S</given-names></name>. <part-title>Generative Adversarial Nets</part-title>. In: <name><surname>Ghahramani</surname><given-names>Z</given-names></name>, <name><surname>Welling</surname><given-names>M</given-names></name>, <name><surname>Cortes</surname><given-names>C</given-names></name>, <name><surname>Lawrence</surname><given-names>N</given-names></name>, <name><surname>Weinberger</surname><given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source>. <volume>vol. 27</volume>. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2014</year>. p. <fpage>2672</fpage>–<lpage>2680</lpage>. Available from: <ext-link xlink:href="https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html" ext-link-type="uri">https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Lecun</surname><given-names>Y</given-names></name>, <name><surname>Bottou</surname><given-names>L</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Haffner</surname><given-names>P</given-names></name>. <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proceedings of the IEEE</source>. <year>1998</year>;<volume>86</volume>(<issue>11</issue>):<fpage>2278</fpage>–<lpage>2324</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref041">
      <label>41</label>
      <mixed-citation publication-type="other">Liu Z, Wang J, Liang Z. CatGAN: Category-Aware Generative Adversarial Networks with Hierarchical Evolutionary Learning for Category Text Generation. Proceedings of the AAAI Conference on Artificial Intelligence. 2020;34(05):8425–8432.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref042">
      <label>42</label>
      <mixed-citation publication-type="other">Chavdarova T, Fleuret F. SGAN: An Alternative Training of Generative Adversarial Networks. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition; 2018. p. 9407–9415.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Xue</surname><given-names>Y</given-names></name>, <name><surname>Xu</surname><given-names>T</given-names></name>, <name><surname>Zhang</surname><given-names>H</given-names></name>, <name><surname>Long</surname><given-names>LR</given-names></name>, <name><surname>Huang</surname><given-names>X</given-names></name>. <article-title>SegAN: Adversarial Network with Multi-scale L1 Loss for Medical Image Segmentation</article-title>. <source>Neuroinformatics</source>. <year>2018</year>;<volume>16</volume>(<issue>3</issue>):<fpage>383</fpage>–<lpage>392</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s12021-018-9377-x</pub-id><?supplied-pmid 29725916?><pub-id pub-id-type="pmid">29725916</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0269449.ref044">
      <label>44</label>
      <mixed-citation publication-type="other">Zhang X, Zhu X, Zhang X, Zhang N, Li P, Wang L. SegGAN: Semantic Segmentation with Generative Adversarial Network. In: 2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM). Los Alamitos, CA, USA: IEEE Computer Society; 2018. p. 1–5.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref045">
      <label>45</label>
      <mixed-citation publication-type="other">Majurski M, Manescu P, Padi S, Schaub N, Hotaling N, Simon C. Cell Image Segmentation Using Generative Adversarial Networks, Transfer Learning, and Augmentations. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW); 2019. p. 1114–1122.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref046">
      <label>46</label>
      <mixed-citation publication-type="other">Isola P, Zhu JY, Zhou T, Efros AA. Image-to-Image Translation with Conditional Adversarial Networks. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2017. p. 5967–5976.</mixed-citation>
    </ref>
    <ref id="pone.0269449.ref047">
      <label>47</label>
      <mixed-citation publication-type="other">Yang L, Zhang Y, Chen J, Zhang S, Chen DZ. Suggestive Annotation: A Deep Active Learning Framework for Biomedical Image Segmentation. In: Descoteaux M, Maier-Hein L, Franz A, Jannin P, Collins DL, Duchesne S, editors. Medical Image Computing and Computer Assisted Intervention − MICCAI 2017. Cham: Springer International Publishing; 2017. p. 399–407.</mixed-citation>
    </ref>
  </ref-list>
</back>
