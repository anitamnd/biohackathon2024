<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_HLY23187 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEgr8 jpg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Heliyon</journal-id>
    <journal-id journal-id-type="iso-abbrev">Heliyon</journal-id>
    <journal-title-group>
      <journal-title>Heliyon</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2405-8440</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10750070</article-id>
    <article-id pub-id-type="pii">S2405-8440(23)10395-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.heliyon.2023.e23187</article-id>
    <article-id pub-id-type="publisher-id">e23187</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SNO-DCA: A model for predicting <italic>S</italic>-nitrosylation sites based on densely connected convolutional networks and attention mechanism</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Jia</surname>
          <given-names>Jianhua</given-names>
        </name>
        <email>jjh163yx@163.com</email>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Lv</surname>
          <given-names>Peinuo</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Wei</surname>
          <given-names>Xin</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Qiu</surname>
          <given-names>Wangren</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <aff id="aff1"><label>a</label>Computer Department, Jingdezhen Ceramic University, Jingdezhen, 330403, China</aff>
      <aff id="aff2"><label>b</label>Business School, Jiangxi Institute of Fashion Technology, Nanchang, 330201, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author. <email>jjh163yx@163.com</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>15</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>03</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>10</volume>
    <issue>1</issue>
    <elocation-id>e23187</elocation-id>
    <history>
      <date date-type="received">
        <day>4</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>11</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by Elsevier Ltd.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <p>Protein <italic>S</italic>-nitrosylation is a reversible oxidative reduction post-translational modification that is widely present in the biological community. <italic>S</italic>-nitrosylation can regulate protein function and is closely associated with a variety of diseases, thus identifying <italic>S</italic>-nitrosylation sites are crucial for revealing the function of proteins and related drug discovery. Traditional experimental methods are time-consuming and expensive; therefore, it is necessary to explore more efficient computational methods. Deep learning algorithms perform well in the field of bioinformatics sites prediction, and many studies show that they outperform existing machine learning algorithms. In this work, we proposed a deep learning algorithm-based predictor SNO-DCA for distinguishing between <italic>S</italic>-nitrosylated and non-<italic>S</italic>-nitrosylated sequences. First, one-hot encoding of protein sequences was performed. Second, the dense convolutional blocks were used to capture feature information, and an attention module was added to weigh different features to improve the prediction ability of the model. The 10-fold cross-validation and independent testing experimental results show that our SNO-DCA model outperforms existing <italic>S</italic>-nitrosylation sites prediction models under imbalanced data. In this paper, a web server prediction website: <ext-link ext-link-type="uri" xlink:href="https://sno.cangmang.xyz/SNO-DCA/" id="intref0010">https://sno.cangmang.xyz/SNO-DCA/</ext-link>was established to provide an online prediction service for users. SNO-DCA can be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/peanono/SNO-DCA" id="intref0015">https://github.com/peanono/SNO-DCA</ext-link>.</p>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0015">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Proposed a new deep learning model to predict the <italic>S</italic>-nitrosylation sites.</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Simple features can reduce the intervention of manual features extraction.</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Dense convolutional blocks focus on lower-level features and higher-level features.</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">The efficient channel attention mechanism automatically assigns different weights.</p>
          </list-item>
          <list-item id="u0030">
            <label>•</label>
            <p id="p0030">A webserver has been provided by which users can easily obtain results.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>S-nitrosoylation</kwd>
      <kwd>Dense convolutional networks</kwd>
      <kwd>Attention mechanisms</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0035">As one of the most important PTMs, <italic>S</italic>-nitrosylation (SNO) is the process of covalent binding of nitro ions to protein cysteine residues [<xref rid="bib1" ref-type="bibr">1</xref>]. NO is a new type of messenger molecule that participates in and regulates various life activities and plays an important role in human functions such as the nervous system, immune system, cardiovascular and cerebrovascular. NO generate nitrogen oxides with oxygen and thus <italic>S</italic>-nitrosylate proteins containing free hydrophobic groups, as well as <italic>trans</italic>-nitrosylation reactions with protein cysteine hydrophobic groups. In brief, SNOcan directly or indirectly regulate protein function [<xref rid="bib2" ref-type="bibr">2</xref>]. In addition, SNOis also involved in vital activities such as ion channels, influencing kinase activity, and DNA activity to participate in cell signaling, apoptosis, etc. Levels of <italic>S</italic>-nitrosylated products are associated with various human diseases such as cardiovascular disease [<xref rid="bib3" ref-type="bibr">3</xref>], cancer [<xref rid="bib4" ref-type="bibr">4</xref>], diabetes, asthma, chronic renal failure [<xref rid="bib5" ref-type="bibr">5</xref>], and Alzheimer's disease [<xref rid="bib6" ref-type="bibr">6</xref>]. Accurate identification of protein SNOsites are essential for 1relevant drug development and understanding of fundamental biological processes, therefore, it is necessary for understanding the underlying cellular and protein mechanisms as well as disease prevention and treatment.</p>
    <p id="p0040">The traditional biochemical methods for predicting SNOsites include biotin switch assay (BSA) [<xref rid="bib7" ref-type="bibr">7</xref>], BSA combined with protein sequencing technology [<xref rid="bib8" ref-type="bibr">8</xref>], and BSA combined with isotope-coded affinity tag (ICAT) [<xref rid="bib9" ref-type="bibr">9</xref>]. In the era of big data, as the volume of data increases, these experimental methods can no longer meet the prediction demand, and there is an urgent need for effective computational means to improve the efficiency of SNO sites prediction.</p>
    <p id="p0045">In recent years, machine learning has become a common method for protein SNO sites prediction. Hao et al. [<xref rid="bib10" ref-type="bibr">10</xref>] proposed the SNOSID method and introduced Support Vector Machine (SVM). Xue et al. [<xref rid="bib11" ref-type="bibr">11</xref>] used the amino acid substitution matrix to calculate the <italic>S</italic>-nitrosylated peptide sequences and obtained the corresponding score to develop a GPS-SNO predictor. Lee et al. [<xref rid="bib12" ref-type="bibr">12</xref>]developed a SNO sites predictor using maximal dependence decomposition (MDD) to divide the SNO sites into different groups and used SVM to generate a prediction model for each MDD cluster motif. iSNO-PseAAC predictor proposed by XU et al. [<xref rid="bib13" ref-type="bibr">13</xref>] used PseAAC to represent protein sequence information, constructed a position-specific amino acid Propensity (PSAAP) matrix, and predicted sites using a conditional random field (CRF) algorithm. Although the above methods have achieved several results, the training sets are small, the features are not comprehensive, and the potential information is easily ignored. And machine learning needs to rely on a variety of manually obtained features, which cannot extract advanced features of protein sequences. Deep learning models are less affected by the manual intervention and can extract protein sequence features in depth.</p>
    <p id="p0050">Xie et al. [<xref rid="bib14" ref-type="bibr">14</xref>] []<xref rid="bib14" ref-type="bibr">[]</xref>[]first developed DeepNitro, a prediction tool for nitro sites based on deep learning algorithms, which used four coding features to construct a neural network model with an eight-layer architecture. Hasan et al. [<xref rid="bib15" ref-type="bibr">15</xref>] undersampled Xie's dataset to form a training set with an equal number of positive and negative samples and used four different coding schemes to develop a prediction tool PreSNO by integrating SVM and random forest (RF) methods. Siraj et al. [<xref rid="bib16" ref-type="bibr">16</xref>] applied the method of natural language processing (NLP) to protein feature coding and developed the RecSNO predictor using a bi-directional long and short-term memory network BiLSTM architecture to process sequence data. Although DeepNitro predictors are trained on unbalanced data sets, their features are subject to more manual intervention and there is much room for further improvement in terms of accuracy. Both PreSNO and RecSNO used balanced training sets, while predictions on balanced sets are rarely encountered in practice, and predictions under imbalanced data will be more practical and more rigorous.</p>
    <p id="p0055">To solve the above problems, densely connected convolutional networks (DenseNets) [<xref rid="bib17" ref-type="bibr">17</xref>]and attention mechanisms are introduced in this study to propose a new prediction model for SNOsites called SNO-DCA. We reduced the intervention caused by manual feature extraction by using only one-hot encoding, focus onhigh-dimensional features by densely connected convolutional networks to extract sequence high-level features. The Efficient Channel Attention Module (ECA-Net) [<xref rid="bib18" ref-type="bibr">18</xref>] to evaluate the importance of features on weighting. Fully connected layers to capture the relationship between high-dimensional features, the introduction of class weights to solve the problem of data imbalance, and finally softmax classification. The experimental results show that our model outperforms existing models and can effectively identify SNO sites under imbalanced data.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Materials and methods</title>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Benchmark dataset</title>
      <p id="p0060">A high-quality dataset is the cornerstone of scientific research. In this study, our benchmark dataset was derived from Xie et al. [<xref rid="bib14" ref-type="bibr">14</xref>], which is based on an extensive literature search and a summary of previously reported datasets. 4762 SNO sites were obtained from 3113 proteins. Of these, the experimentally validated SNO sites were positive samples and all other samples were negative sites. To reduce the risk of overfitting the model due to sequence redundancy [<xref rid="bib19" ref-type="bibr">19</xref>], the collected protein sequences were clustered using a CD-HIT with a threshold of 40 %. Finally, we retained 3409 positive sites and 17,103 negative sites as the training set and 365 positive sites and 3354 negative sites as the independent test set.</p>
      <p id="p0065">Potential peptide samples containing SNO sites can be expressed as shown in Equation <xref rid="fd1" ref-type="disp-formula">(1)</xref>:<disp-formula id="fd1"><label>(1)</label><mml:math id="M1" altimg="si1.svg" alttext="Equation 1."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>ζ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ζ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ζ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>…</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>…</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ζ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mi>ζ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>P</italic> is the peptide, the center C is the cysteine, <italic>ζ</italic> is an integer, and R_(-ζ) and R_(+ζ) represent the ξth upstream amino acid residue and downstream amino acid residue of cysteine C, respectively. We used a sliding window to extract the fragments with a window size <italic>L</italic> = 2<italic>ξ</italic>+1. In this paper, we set <italic>ξ</italic> = 20 and fill the missing amino acids with X if the peptide sequence length is less than 20. The statistical information of the dataset is shown in <xref rid="tbl1" ref-type="table">Table 1</xref>.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Statistics for the original sample data.</p></caption><alt-text id="alttext0050">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Original data set</th><th>Positive site</th><th>Negative site</th></tr></thead><tbody><tr><td align="left">Training set</td><td align="left">3409</td><td align="left">17,103</td></tr><tr><td align="left">Independent test set</td><td align="left">365</td><td align="left">3354</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>One-hot encoding</title>
      <p id="p0070">In this work, we encode the protein sequence using one-hot encoding. This encoding is represented by the binary numbers 0 and 1. The amino acids in the protein sequence corresponding to an index of 1 and other positions are 0 [<xref rid="bib20" ref-type="bibr">20</xref>], for example, alanine is encoded as (10⋯0). There are 20 amino acids in total, in addition, the unknown amino acid is set to X. So, for the sequence fragment of length L, the final two-dimensional sparse matrix of <italic>L</italic> × 21 dimensions is obtained. In this paper, <italic>L</italic> is 41, then we have a 41 × 21 (861) dimensional vector matrix.</p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>Model structure</title>
      <p id="p0075">In this paper, we used deep learning algorithms to build a prediction model which can explore the deep features of SNO sites. In this model, we extracted high-level features from a matrix of sequences that have been one-hot encoded by a densely connected convolutional network. The features are fed into an efficient channel attention mechanism to weigh the important features. Finally, after four fully connected layers, the output high-level features are fed to the softmax layer for classification, which effectively predicts the SNO sites. The structure of the SNO-DCA model is shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. For the information encoded by one-hot, advanced features are extracted through the densely connected convolutional networks. Input the features into the attention mechanism of the efficient channel to weigh the important features. After four fully-connected layers, the output advanced features are input to the softmax layer for classification.<fig id="fig1"><label>Fig. 1</label><caption><p>The model framework of SNO-DCA.</p></caption><alt-text id="alttext0010">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p>
    </sec>
    <sec id="sec2.4">
      <label>2.4</label>
      <title>Densely connected convolutional networks to extract features</title>
      <p id="p0080">It has been shown that DenseNets have better performance compared to traditional Convolutional Neural Network (CNN) and Residual Networks (ResNet) [<xref rid="bib21" ref-type="bibr">21</xref>]. DenseNets use a dense connectivity mechanism, where the input of each layer is the output of all previous layers in the channel dimension. This not only mitigates the gradient disappearance to some extent but also enhances the transmission of features and achieves better performance with fewer parameters and less computation. The propagation process of different networks is shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>(A-C).<fig id="fig2"><label>Fig. 2</label><caption><p>Communication process of different networks (A) Densely connected convolution network structure. (B) Short-circuit connection mechanism of Residual Network. (C) Convolutional Neural Network structure.</p></caption><alt-text id="alttext0015">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p>
      <p id="p0085">Before applying the dense convolution block, the one-hot encoded vector of length <italic>L</italic> is firstly convolved by one-dimensional convolution to generate a low-level feature map of protein sequence information using the ELU activation function, which implements the nonlinear transformation. As shown in Equation <xref rid="fd2" ref-type="disp-formula">(2)</xref>.<disp-formula id="fd2"><label>(2)</label><mml:math id="M2" altimg="si2.svg" alttext="Equation 2."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>W</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>E</italic> is one-hot encoding, W is the weight matrix with size 21 <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>S</italic>
<inline-formula><mml:math id="M4" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>D</italic>, 21 is the length of one-hot encoding, <italic>S</italic> is the convolutional kernel size, <italic>D</italic> is the number of convolutional kernels, and b is the bias term. Here, <italic>S</italic> is set to 4 and D is set to 96. <inline-formula><mml:math id="M5" altimg="si4.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the output of the one-dimensional convolutional layer of size <italic>L</italic>
<inline-formula><mml:math id="M6" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>D</italic>.</p>
      <p id="p0090">The output vector of the feature encoding is the input vector of the dense convolutional network. The dense convolutional block uses a series of convolutional operations to obtain a high-dimensional feature representation map, as shown in Equation <xref rid="fd3" ref-type="disp-formula">(3)</xref>.<disp-formula id="fd3"><label>(3)</label><mml:math id="M7" altimg="si5.svg" alttext="Equation 3."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M8" altimg="si6.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the feature vector generated by the <inline-formula><mml:math id="M9" altimg="si7.svg"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> convolution in the dense convolution block. <inline-formula><mml:math id="M10" altimg="si8.svg"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix, <inline-formula><mml:math id="M11" altimg="si9.svg"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is determined by <italic>K</italic> and denotes the number of convolution kernels in the kth convolution layer, <inline-formula><mml:math id="M12" altimg="si10.svg"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> = 32. <inline-formula><mml:math id="M13" altimg="si11.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the bias term, <inline-formula><mml:math id="M14" altimg="si12.svg"><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></inline-formula> indicates that the outputs <inline-formula><mml:math id="M15" altimg="si13.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of the dense convolution blocks are concatenated along the feature dimension.</p>
      <p id="p0095">Then, a transition layer is used between the two dense convolution blocks to perform convolution and pooling operations on the feature maps of the obtained protein sequence information. The transition layer is shown as Equations <xref rid="fd4" ref-type="disp-formula">(4)</xref>, <xref rid="fd5" ref-type="disp-formula">(5)</xref>.<disp-formula id="fd4"><label>(4)</label><mml:math id="M16" altimg="si14.svg" alttext="Equation 4."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd5"><label>(5)</label><mml:math id="M17" altimg="si15.svg" alttext="Equation 5."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M18" altimg="si16.svg"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix, <inline-formula><mml:math id="M19" altimg="si17.svg"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the size of the convolution kernel set to 1, and <inline-formula><mml:math id="M20" altimg="si18.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the bias. Finally, the average pooling operation is applied to <inline-formula><mml:math id="M21" altimg="si19.svg"><mml:mrow><mml:msup><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> to reduce the dimensionality and obtain the output <inline-formula><mml:math id="M22" altimg="si20.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of the transition layer.</p>
      <p id="p0100">Multiple dense convolutional blocks and transition layers are connected in series to construct a dense convolutional network. In this study, the number of dense convolutional blocks is set to 5. Finally, we can extract the high-level features of protein sequences <italic>h.</italic></p>
    </sec>
    <sec id="sec2.5">
      <label>2.5</label>
      <title>-net</title>
      <p id="p0105">Since different features have different levels of importance, we add ECA-Net to each module to weigh the features to enhance useful information and improve model prediction. ECA-Net is implemented based on global average pooling and adaptive computation of one-dimensional convolution of the convolution kernel size k, which effectively avoids dimensionality reduction (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). After using global average pooling <bold>(</bold>GAP) aggregation convolution features without dimension reduction, the ECA module first adaptively determines the kernel size k, then conducts one-dimensional convolution, and the sigmoid function learns the channel attention.<fig id="fig3"><label>Fig. 3</label><caption><p>The efficient channel attention model (ECA-Net).</p></caption><alt-text id="alttext0020">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p>
      <p id="p0110">First, the global averaging pooling operation is performed on the high-level features extracted from the dense convolutional blocks, and the features <italic>h</italic> with width <italic>w</italic> and height <italic>H</italic> and a number of channels <italic>C</italic> are compressed into features <inline-formula><mml:math id="M23" altimg="si21.svg"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of size 1 × 1 × <italic>C</italic>. As shown in Equation <xref rid="fd6" ref-type="disp-formula">(6)</xref>.<disp-formula id="fd6"><label>(6)</label><mml:math id="M24" altimg="si22.svg" alttext="Equation 6."><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p id="p0115">Then a one-dimensional convolution operation is performed to capture the information between different channels, and the feature size obtained after one-dimensional convolution remains 1 × 1 × <italic>C</italic>. As shown in Equation <xref rid="fd7" ref-type="disp-formula">(7)</xref>.<disp-formula id="fd7"><label>(7)</label><mml:math id="M25" altimg="si23.svg" alttext="Equation 7."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>q</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>q</italic> is the weight of each channel and a is the Sigmoid activation function. Where <italic>C1D</italic> denotes a one-dimensional convolution and the convolution kernel size is <italic>k</italic>, i.e., the coverage of cross-channel information interaction, and <italic>k</italic> is proportional to the number of channels <italic>C</italic>. Since the channel dimension, <italic>C</italic> is usually set to the power of 2, a nonlinear mapping relation <italic>φ</italic> is introduced as shown in Equation <xref rid="fd8" ref-type="disp-formula">(8)</xref>.<disp-formula id="fd8"><label>(8)</label><mml:math id="M26" altimg="si24.svg" alttext="Equation 8."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0120">Given the channel dimension <italic>C</italic>, the adaptive function of the one-dimensional convolutional kernel size is shown in Equation <xref rid="fd9" ref-type="disp-formula">(9)</xref>.<disp-formula id="fd9"><label>(9)</label><mml:math id="M27" altimg="si25.svg" alttext="Equation 9."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi mathvariant="italic">log</mml:mi><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:msub><mml:mi>γ</mml:mi></mml:mfrac><mml:mo linebreak="badbreak">+</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>γ</italic> = 2 and <italic>b</italic> = 1. Finally, the weights <italic>q</italic> is multiplied channel by channel with the original input high-level feature maps to obtain the weighted high-level feature maps. As shown in Equation <xref rid="fd10" ref-type="disp-formula">(10)</xref>.<disp-formula id="fd10"><label>(10)</label><mml:math id="M28" altimg="si26.svg" alttext="Equation 10."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>h</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0125">Next, all information is integrated by four fully connected layers with 80, 40, 40 and 20 units, respectively, and a softmax classifier is used to classify the SNO sites, and the predicted classes of the samples are obtained after weighting and activation operations, as shown in Equation <xref rid="fd11" ref-type="disp-formula">(11)</xref>.<disp-formula id="fd11"><label>(11)</label><mml:math id="M29" altimg="si27.svg" alttext="Equation 11."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M30" altimg="si28.svg"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M31" altimg="si29.svg"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are the weight matrices and <inline-formula><mml:math id="M32" altimg="si30.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the bias. <inline-formula><mml:math id="M33" altimg="si31.svg"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability that sample <italic>x</italic> is predicted to be class <inline-formula><mml:math id="M34" altimg="si32.svg"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula><italic>,</italic>
<inline-formula><mml:math id="M35" altimg="si33.svg"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">∈</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p id="p0130">Imbalanced data will affect the prediction ability of the model, which is the problem to be solved [<xref rid="bib22" ref-type="bibr">[22]</xref>, <xref rid="bib23" ref-type="bibr">[23]</xref>, <xref rid="bib24" ref-type="bibr">[24]</xref>]. For this reason, we adopt the method of class weights and set the weight ratio of positive and negative samples to 5:1. Increasing the influence of positive samples allows the model to learn the sequence mechanism of SNO samples better, thus improving the prediction ability of the model.</p>
    </sec>
    <sec id="sec2.6">
      <label>2.6</label>
      <title>Performance evaluation</title>
      <p id="p0135">In this study, we evaluate the performance of the model by utilizing both ten-fold cross-validation and independent test sets. We calculated four statistical metrics: sensitivity (Sn), specificity (Sp), accuracy (Acc), and Mathews correlation coefficient (MCC) [<xref rid="bib25" ref-type="bibr">[25]</xref>, <xref rid="bib26" ref-type="bibr">[26]</xref>, <xref rid="bib27" ref-type="bibr">[27]</xref>] with the following equations. As shown in Equation <xref rid="fd12" ref-type="disp-formula">(12)</xref>, <xref rid="fd13" ref-type="disp-formula">(13)</xref>, <xref rid="fd14" ref-type="disp-formula">(14)</xref>, <xref rid="fd15" ref-type="disp-formula">(15)</xref><disp-formula id="fd12"><label>(12)</label><mml:math id="M36" altimg="si34.svg" alttext="Equation 12."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Sn</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd13"><label>(13)</label><mml:math id="M37" altimg="si35.svg" alttext="Equation 13."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Sp</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd14"><label>(14)</label><mml:math id="M38" altimg="si36.svg" alttext="Equation 14."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Acc</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd15"><label>(15)</label><mml:math id="M39" altimg="si37.svg" alttext="Equation 15."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>MCC</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>−</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0140">Sn was used as a measure of positive accuracy, i.e., an index of accuracy in identifying SNO sites. Sp was used as a measure of negative accuracy, i.e., an index of accuracy in identifying non-SNO sites. Acc represented the proportion of correctly classified samples to the total number of samples, and MCC was considered a balanced measure [<xref rid="bib28" ref-type="bibr">28</xref>]. Here TP, TN, FP, and FN denote true positive, true negative, false positive, and false negative, respectively. In addition, we used the receiver operating characteristic (ROC) area under the ROC curve (AUC) as a measure of model goodness [<xref rid="bib29" ref-type="bibr">29</xref>]. It is obvious that the larger AUC value indicates, the higher model performance.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Results and discussion</title>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Optimization of important hyperparameters</title>
      <p id="p0145">In order to find the optimal network configuration for the model SNO-DCA, which leads to the training of a better site predictor. A few important hyperparameters in the model SNO-DCA, including the number of convolutional kernels in the first CNN layer, the number of dense blocks and the number of convolutional layers in the dense blocks, were determined by ten-fold cross-validation.</p>
      <p id="p0150">Firstly, the number of convolution kernels of the first layer CNN and the layers of the dense blocks were preset to 96 and 4. Adjusted the number of dense blocks, the best performance of the model is achieved when the number of dense blocks were 5 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>A). Secondly, after fixing the number of dense blocks, we adjusted the number of convolution layers in the dense blocks, and determined that the best result is achieved when the number of layers were 4 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>B). Lastly, we fixed the two parameters and then adjusted the number of convolutional kernels in the first layer of CNN to reach the best performance when the number of filters were 96 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>C).<fig id="fig4"><label>Fig. 4</label><caption><p>The results of parameter selection of the model SNO-DCA (A) Selection of the number of dense blocks. (B) Selection of the number of layers. (C) Selection of the number of filters.</p></caption><alt-text id="alttext0025">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>SNO-DCA ablation experiments</title>
      <p id="p0155">To verify the necessity of each component in SNO-DCA, we designed an ablation experiment by ten-fold validation using the training dataset. The experiments included the use of densely connected convolutional networks and the introduction of the ECA-Net layer in the model. The benchmark model adopted the concatenation of the original protein sequence information as input and extracted features by traditional CNNs.</p>
      <p id="p0160">The experimental results are shown in <xref rid="tbl2" ref-type="table">Table 2</xref>, different columns represent different model architectures. The proposed model performed best overall when densely connected convolutional networks and ECA-Net were used. First, to further verify the advantages of dense convolutional networks, we replaced conventional CNNs with DenseNets. Then, ECA-Net was added after each dense convolutional network. The results of ten-fold cross-validation showed that all metrics are significantly improved and dense convolutional networks have significant advantages over conventional CNNs. In addition, ECA-Net also could raise the model performance.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>The 10-flod cross-validation performance with different model structure.</p></caption><alt-text id="alttext0055">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th>CNN</th><th>✓</th><th>✓</th><th>✓</th></tr></thead><tbody><tr><td align="left">+ DenseNets</td><td/><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">+ECA-Net</td><td/><td/><td align="left">✓</td></tr><tr><td align="left">SN</td><td align="left">0.32</td><td align="left">0.80</td><td align="left">0.80</td></tr><tr><td align="left">SP</td><td align="left">0.80</td><td align="left">0.59</td><td align="left">0.62</td></tr><tr><td align="left">ACC</td><td align="left">0.40</td><td align="left">0.63</td><td align="left">0.65</td></tr><tr><td align="left">MCC</td><td align="left">0.09</td><td align="left">0.30</td><td align="left">0.31</td></tr><tr><td align="left">AUC</td><td align="left">0.60</td><td align="left">0.76</td><td align="left">0.77</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Cross-validation performance</title>
      <p id="p0165">We used ten-fold cross-validation to test the results. For an accurate comparison, we used the same training set and test set as the DeepNitro model. The comparison results of all metrics are shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. The AUC of the SNO-DCA model is 0.77, and the ROC curve is shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Comparison with other methods on the training data set.</p></caption><alt-text id="alttext0060">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Predictors</th><th>Sn</th><th>Sp</th><th>Acc</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td align="left">DeepNitro</td><td align="left">NA</td><td align="left">NA</td><td align="left">NA</td><td align="left">NA</td><td align="left">0.71</td></tr><tr><td align="left">SNO-DCA</td><td align="left"><bold>0.80</bold></td><td align="left">0.62</td><td align="left"><bold>0.65</bold></td><td align="left"><bold>0.31</bold></td><td align="left"><bold>0.77</bold></td></tr></tbody></table></table-wrap><fig id="fig5"><label>Fig. 5</label><caption><p>Ten-fold cross-validated ROC curve.</p></caption><alt-text id="alttext0030">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Comparison of independent datasets with existing models</title>
      <p id="p0170">To evaluate the performance of SNO-DCA and ensure the good generalization ability of the model, we have selected the following methods for predicting SNO sites for comparison with our model, taking into account the fact that some models use different training sets and do not provide independent prediction tools. DeepNitro is based on an eight-layer neural network, PreSNO is a linear regression model based on SVM and RF, and RecSNO uses protein sequences as input and is a predictor based on embedded layers and BILSTM. The results are shown in <xref rid="tbl4" ref-type="table">Table 4</xref>. The prediction results of the deep learning predictor SNO-DCA constructed in this paper are Sn = 0.82; Sp = 0.69; Acc = 0.70; MCC = 0.32; AUC = 0.82. The ROC curves are shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Comparison with other methods on the same test data set.</p></caption><alt-text id="alttext0065">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Predictors</th><th>Sn</th><th>Sp</th><th>Acc</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td align="left">GPS-SNO</td><td align="left">0.28</td><td align="left">0.74</td><td align="left">0.69</td><td align="left">0.01</td><td align="left">0.52</td></tr><tr><td align="left">ISNOPseAAC</td><td align="left">0.29</td><td align="left">0.76</td><td align="left">0.71</td><td align="left">0.03</td><td align="left">NA</td></tr><tr><td align="left">SNOSite</td><td align="left">0.67</td><td align="left">0.45</td><td align="left">0.47</td><td align="left">0.07</td><td align="left">NA</td></tr><tr><td align="left">DeepNitro</td><td align="left">0.58</td><td align="left">0.76</td><td align="left">0.73</td><td align="left">0.22</td><td align="left">0.73</td></tr><tr><td align="left">PreSNO</td><td align="left">0.60</td><td align="left">0.77</td><td align="left">0.75</td><td align="left">0.25</td><td align="left">0.76</td></tr><tr><td align="left">RecSNO</td><td align="left">0.77</td><td align="left">0.71</td><td align="left">0.71</td><td align="left">0.30</td><td align="left">0.80</td></tr><tr><td align="left">SNO-DCA</td><td align="left"><bold>0.82</bold></td><td align="left">0.69</td><td align="left">0.70</td><td align="left"><bold>0.32</bold></td><td align="left"><bold>0.81</bold></td></tr></tbody></table></table-wrap><fig id="fig6"><label>Fig. 6</label><caption><p>ROC curve of the independent test set.</p></caption><alt-text id="alttext0035">Fig. 6</alt-text><graphic xlink:href="gr6"/></fig></p>
      <p id="p0175">In <xref rid="tbl4" ref-type="table">Table 4</xref>, we can find that SNO-DCA has the highest Sn, MCC, and AUC; however, it has relatively low Sp due to the antagonistic formula of Sn and Sp. Similar to the finding of Ref [<xref rid="bib30" ref-type="bibr">30</xref>], a significant increase in Sn usually lead to a decrease in Sp and thus Acc. Both PreSNO and RecSNO were constructed on a balanced dataset after downsampling the original dataset trained, in which the negative samples of the dataset are not sufficient. However, models trained on unbalanced training sets are more relevant. In the biological field, the real datasets are often unbalanced and it is especially important to identify as many SNO sites as possible, i.e., the values of Sn and MCC are more important than the other parameters. In contrast, the SNO-DCA predictor constructed in this paper solves the problem of unbalanced data by class weights and does not require the construction of balanced data sets with high Sn and MCC by undersampling, which effectively overcomes the disadvantage of the inadequacy for negative samples brought by the training under balanced data, avoids the lack of information to some extent. Therefore, our SNO-DCA model is more relevant and practical for predicting SNO sites.</p>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Server online prediction website</title>
      <p id="p0180">To facilitate the utilization of our prediction model and provide experimental help to scientific researchers, we have constructed a user-friendly web server website <ext-link ext-link-type="uri" xlink:href="https://sno.cangmang.xyz/SNO-DCA/" id="intref0020">https://sno.cangmang.xyz/SNO-DCA/</ext-link>, whose homepage is shown in <xref rid="fig7" ref-type="fig">Fig. 7</xref>.<list list-type="simple" id="olist0010"><list-item id="o0010"><label>Step1</label><p id="p0185">Enter the URL in your browser and click Search to open the SNO-DCA sever homepage as shown in Figure. Click the Help button in the upper right corner or the More info … Button in the middle of Home to jump to the User's Guide, where one can view the background and a brief introduction about the SNO-DCA prediction model, and learn about the model structure and advantages; the user can see the sequence input format, and click example to jump to the Example interface, which shows the correct format and the incorrect format of the input sequences.</p></list-item><list-item id="o0015"><label>Step2</label><p id="p0190">Visitors can enter a single protein sequence for query in the center box of the homepage and click the Submit button to make a prediction, and the predicted result will be highlighted in red, indicating SNO sites. As shown in <xref rid="fig8" ref-type="fig">Fig. 8</xref>.</p></list-item><list-item id="o0020"><label>Step3</label><p id="p0195">You can also pass the file in FASTA format to the bottom left box of the homepage, enter your username and email address in the bottom right box, and click Submit for batch prediction, and the final prediction will be emailed to the recipient.</p></list-item></list><fig id="fig7"><label>Fig. 7</label><caption><p>Screenshot of the homepage of the web server SNO-DCA.</p></caption><alt-text id="alttext0040">Fig. 7</alt-text><graphic xlink:href="gr7"/></fig><fig id="fig8"><label>Fig. 8</label><caption><p>Screenshot of the predicted results of the web server SNO-DCA.</p></caption><alt-text id="alttext0045">Fig. 8</alt-text><graphic xlink:href="gr8"/></fig></p>
      <p id="p0200">This platform simply encodes the original sequence of proteins to reduce manual feature selection intervention. A dense convolutional network is introduced to extract the high-level features, and an efficient channel attention mechanism is added to weight different features and learn important features. The efficient channel attention mechanism is added to weight different features to learn important features. Finally, the information is fused by the fully connected layer and inputted into the softmax layer for site prediction, and the SNO sites are identified by the prediction probability. The platform gives full effect to the deep learning models to explore the potential information of SNO sites, extract important features and improve the accuracy of SNO sites prediction.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Conclusions</title>
    <p id="p0205">In this study, we proposed SNO-DCA, a deep learning model for SNO sites prediction based on DenseNets and an efficient channel attention module under unbalanced data. The model performs a simple feature representation of protein sequence information, extracts high-level features by dense convolutional blocks, and adds an efficient attention module to further enhance the feature information. The results of both ten-fold cross-validation and independent tests show that SNO-DCA is a powerful prediction tool for identifying SNO sites. We developed a web online server to provide convenience to researchers. Both the training and test sets in this study are non-equilibrium datasets, which meet the practical prediction conditions. Another advantage in this study is that only protein sequences are simply encoded, which greatly reduces the intervention problem of manual feature selection. Meanwhile, the developed deep learning algorithm architecture is used to dig the potential information of the sequences in depth, which can give full play to the deep learning model.</p>
    <p id="p0210">In the follow-up work for this study, we will try not to encode the protein sequences in advance, but to embed the original sequences into the deep learning model to achieve end-to-end prediction, directly mining the hidden information of the samples and improving the prediction accuracy. In addition, we will also explore more possibilities of deep learning algorithms for protein locus prediction, such as capsule network for prediction of small sample data, improved long and short-term memory network combined with improved CNN, etc. At present, the problem of data imbalance is still a major difficulty in the field of bioinformatics. Though the traditional methods of balancing data may have some certain value in the data information processing, we should transform our focus into the imbalanced data with employing the improved/enhanced deep learning algorithm architecture to adapt to the actual protein prediction circumstance.</p>
  </sec>
  <sec sec-type="data-availability" id="sec5">
    <title>Data availability statement</title>
    <p id="p0215">Data will be made available on request. SNO-DCA model and datasets can be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/peanono/SNO-DCA" id="intref0025">https://github.com/peanono/SNO-DCA</ext-link>.</p>
  </sec>
  <sec id="sec6">
    <title>Additional information</title>
    <p id="p0220">No additional information is available for this paper.</p>
  </sec>
  <sec id="sec7">
    <title>CRediT authorship contribution statement</title>
    <p id="p0225"><bold>Jianhua Jia:</bold> Writing – review &amp; editing. <bold>Peinuo Lv:</bold> Writing – original draft. <bold>Xin Wei:</bold> Data curation. <bold>Wangren Qiu:</bold> Validation.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of competing interest</title>
    <p id="p0230">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <mixed-citation publication-type="other" id="sref1">Matthew, W., Foster, et al. Protein S-nitrosylation in health and disease: a current perspective - ScienceDirect [J]. Trends Mol. Med., 15(9): 391-404.</mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>B</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Regulation of cellular function by protein sulfhydryl nitrosylation [J]</article-title>
        <source>Chin. J. Pathophysiol.</source>
        <volume>27</volume>
        <issue>11</issue>
        <year>2011</year>
        <fpage>2237</fpage>
        <lpage>2240</lpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Turko</surname>
            <given-names>I.V.</given-names>
          </name>
          <name>
            <surname>Murad</surname>
            <given-names>F.J.P.R.</given-names>
          </name>
        </person-group>
        <article-title>Protein nitration in cardiovascular diseases [J]</article-title>
        <source>Pharmacol. Rev.</source>
        <volume>54</volume>
        <issue>4</issue>
        <year>2002</year>
        <fpage>619</fpage>
        <pub-id pub-id-type="pmid">12429871</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Protein S-nitrosylation and cancer [J]</article-title>
        <source>Cancer Lett.</source>
        <volume>320</volume>
        <issue>2</issue>
        <year>2012</year>
        <fpage>123</fpage>
        <lpage>129</lpage>
        <pub-id pub-id-type="pmid">22425962</pub-id>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Piroddi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Palmese</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pilolli</surname>
            <given-names>F.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Plasma nitroproteome of kidney disease patients [J]</article-title>
        <source>Amino Acids</source>
        <volume>40</volume>
        <issue>2</issue>
        <year>2011</year>
        <fpage>653</fpage>
        <lpage>657</lpage>
        <pub-id pub-id-type="pmid">20676907</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <article-title>Heneka T S W S B-a S S G K B T. Quantitative proteomics of synaptosome S-nitrosylation in Alzheimer's disease [J]</article-title>
        <source>J. Neurochem.</source>
        <volume>152</volume>
        <issue>6</issue>
        <year>2020</year>
        <fpage>710</fpage>
        <lpage>726</lpage>
        <pub-id pub-id-type="pmid">31520481</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Jaffrey</surname>
            <given-names>S.R.</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>S.H.</given-names>
          </name>
        </person-group>
        <article-title>The biotin switch method for the detection of S-nitrosylated proteins [J]</article-title>
        <source>Sci. STKE : Signal Transduct. Knowl. Environ.</source>
        <volume>2001</volume>
        <issue>86</issue>
        <year>2001</year>
        <fpage>pl1</fpage>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Lindermayr</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Saalbach</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Durner</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Proteomic identification of S-nitrosylated proteins in Arabidopsis [J]</article-title>
        <source>Plant Physiol.</source>
        <volume>137</volume>
        <issue>3</issue>
        <year>2005</year>
        <fpage>921</fpage>
        <lpage>930</lpage>
        <pub-id pub-id-type="pmid">15734904</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Redox regulatory mechanism of transnitrosylation by thioredoxin [J]</article-title>
        <source>Mol. Cell. Proteomics : MCP</source>
        <volume>9</volume>
        <issue>10</issue>
        <year>2010</year>
        <fpage>2262</fpage>
        <lpage>2275</lpage>
        <pub-id pub-id-type="pmid">20660346</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Hao</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Derakhshan</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SNOSID, a proteomic method for identification of cysteine S-nitrosylation sites in complex protein mixtures</article-title>
        <source>Proc. Natl. Acad. Sci. U.S.A.</source>
        <volume>103</volume>
        <issue>4</issue>
        <year>2006</year>
        <fpage>1012</fpage>
        <lpage>1017</lpage>
        <pub-id pub-id-type="pmid">16418269</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>GPS-SNO: computational prediction of protein S-nitrosylation sites with a modified GPS algorithm [J]</article-title>
        <source>PLoS One</source>
        <volume>5</volume>
        <issue>6</issue>
        <year>2010</year>
        <object-id pub-id-type="publisher-id">e11290</object-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>T.Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>T.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SNOSite: exploiting maximal dependence decomposition to identify cysteine S-nitrosylation with substrate site specificity [J]</article-title>
        <source>PLoS One</source>
        <volume>6</volume>
        <issue>7</issue>
        <year>2011</year>
        <object-id pub-id-type="publisher-id">e21849</object-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>L.Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>iSNO-PseAAC: predict cysteine S-nitrosylation sites in proteins by incorporating position specific amino acid propensity into pseudo amino acid composition [J]</article-title>
        <source>PLoS One</source>
        <volume>8</volume>
        <issue>2</issue>
        <year>2013</year>
        <object-id pub-id-type="publisher-id">e55844</object-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepNitro: prediction of protein nitration and nitrosylation sites by deep learning [J]</article-title>
        <source>Dev. Reprod. Biol.</source>
        <volume>16</volume>
        <issue>4</issue>
        <year>2018</year>
        <fpage>294</fpage>
        <lpage>306</lpage>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Hasan</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Manavalan</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Khatun</surname>
            <given-names>M.S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prediction of S-nitrosylation sites by integrating support vector machines and random forest [J]</article-title>
        <source>Molecular omics</source>
        <volume>15</volume>
        <issue>6</issue>
        <year>2019</year>
        <fpage>451</fpage>
        <lpage>458</lpage>
        <pub-id pub-id-type="pmid">31710075</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Siraj</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chantsalnyam</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tayara</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>RecSNO: prediction of protein S-nitrosylation sites using a recurrent neural network [J]</article-title>
        <source>IEEE Access</source>
        <volume>9</volume>
        <year>2021</year>
        <fpage>6674</fpage>
        <lpage>6682</lpage>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="book" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Laurens</surname>
            <given-names>V.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Densely Connected Convolutional Networks</part-title>
        <year>2016</year>
        <publisher-name>proceedings of the IEEE Computer Society, F</publisher-name>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="book" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>P.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>ECA-net: efficient Channel Attention for deep convolutional neural networks</part-title>
        <source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), F</source>
        <year>2020</year>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>LiminNiu</surname>
            <given-names>Beifang</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data [J]</article-title>
        <source>Bioinformatics</source>
        <volume>28</volume>
        <issue>23</issue>
        <year>2012</year>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Capsule network for protein post-translational modification site prediction [J]</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <issue>14</issue>
        <year>2018</year>
        <fpage>2386</fpage>
        <lpage>2394</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="book" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Deep Residual Learning for Image Recognition; Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), F 27-30 June 2016</part-title>
        <year>2016</year>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <mixed-citation publication-type="other" id="sref22">Wei L, Hu J, Li F, et al. Comparative analysis and prediction of quorum-sensing peptides using feature representation learning and machine learning algorithms. LID - 10.1093/bib/bby107 [doi] [J]. (1477-4054 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improved prediction of protein–protein interactions using novel negative samples, features, and an ensemble classifier [J]</article-title>
        <source>Artif. Intell. Med.</source>
        <volume>83</volume>
        <year>2017</year>
        <fpage>67</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="pmid">28320624</pub-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <mixed-citation publication-type="other" id="sref24">Wei L, Zhou C, Chen H, et al. ACPred-FL: a sequence-based predictor using effective feature representation to improve the prediction of anti-cancer peptides [J]. (1367-4811 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <mixed-citation publication-type="other" id="sref25">Su R, Liu X, Xiao G, et al. Meta-GDBP: a high-level stacked regression model to improve anticancer drug response prediction [J]. (1477-4054 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Computational prediction and interpretation of cell-specific replication origin sites from multiple eukaryotes by exploiting stacking framework [J]</article-title>
        <source>Briefings Bioinf.</source>
        <volume>22</volume>
        <issue>4</issue>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Local-DPP: an improved DNA-binding protein prediction method by exploring local evolutionary information [J]</article-title>
        <source>Inf. Sci.</source>
        <volume>384</volume>
        <year>2017</year>
        <fpage>135</fpage>
        <lpage>144</lpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Eisenberg</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Marcotte</surname>
            <given-names>E.M.</given-names>
          </name>
          <name>
            <surname>Xenarios</surname>
            <given-names>I.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein function in the post-genomic era [J]</article-title>
        <source>Nature</source>
        <volume>405</volume>
        <issue>6788</issue>
        <year>2000</year>
        <fpage>823</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="pmid">10866208</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Jurman</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation [J]</article-title>
        <source>BMC Genom.</source>
        <volume>21</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>6</fpage>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="journal" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>E.A.</given-names>
          </name>
        </person-group>
        <article-title>Learning from imbalanced data [J]</article-title>
        <source>IEEE Trans. Knowl. Data Eng.</source>
        <volume>21</volume>
        <issue>9</issue>
        <year>2009</year>
        <fpage>1263</fpage>
        <lpage>1284</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec1">
    <title>Abbreviations</title>
    <p id="p0240">
      <def-list>
        <def-item>
          <term id="d0010">SNO</term>
          <def>
            <p id="p0245"><italic>S</italic>-nitrosylation</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0015">BSA</term>
          <def>
            <p id="p0250">biotin switch assay</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0020">ICAT</term>
          <def>
            <p id="p0255">isotope-coded affinity tag</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0025">SVM</term>
          <def>
            <p id="p0260">Support Vector Machine</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0030">MDD</term>
          <def>
            <p id="p0265">maximal dependence decomposition</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0035">PSAAP</term>
          <def>
            <p id="p0270">position-specific amino acid Propensity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0040">CRF</term>
          <def>
            <p id="p0275">conditional random field</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0045">RF</term>
          <def>
            <p id="p0280">random forest</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0050">NLP</term>
          <def>
            <p id="p0285">natural language processing</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0055">DenseNets</term>
          <def>
            <p id="p0290">densely connected convolutional networks</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0060">ECA-Net</term>
          <def>
            <p id="p0295">Efficient Channel Attention Module</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0065">CNN</term>
          <def>
            <p id="p0300">Convolutional Neural Network</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0070">ResNet</term>
          <def>
            <p id="p0305">Residual Networks</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0075">GAP</term>
          <def>
            <p id="p0310">global average pooling</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0080">Sn</term>
          <def>
            <p id="p0315">sensitivity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0085">Sp</term>
          <def>
            <p id="p0320">specificity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0090">Acc</term>
          <def>
            <p id="p0325">accuracy</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0095">MCC</term>
          <def>
            <p id="p0330">Mathews correlation coefficient</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0100">ROC</term>
          <def>
            <p id="p0335">receiver operating characteristic</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0105">AUC</term>
          <def>
            <p id="p0340">area under the ROC curve</p>
          </def>
        </def-item>
      </def-list>
    </p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0235">This work was partially supported by the <funding-source id="gs4">National Nature Science Foundation of China</funding-source> (Nos. 61761023 and 62162032), the <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100004479</institution-id><institution>Natural Science Foundation of Jiangxi Province</institution></institution-wrap></funding-source>, China (Nos. 20202BABL202004), the Scientific Research Plan of the <funding-source id="gs3"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100009102</institution-id><institution>Department of Education of Jiangxi Province</institution></institution-wrap></funding-source> (GJJ212419 and GJJ2201004). These funders had no role in the study design, data collection and analysis, decision to publish or preparation of manuscript.</p>
  </ack>
</back>
<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_HLY23187 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEgr8 jpg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Heliyon</journal-id>
    <journal-id journal-id-type="iso-abbrev">Heliyon</journal-id>
    <journal-title-group>
      <journal-title>Heliyon</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2405-8440</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10750070</article-id>
    <article-id pub-id-type="pii">S2405-8440(23)10395-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.heliyon.2023.e23187</article-id>
    <article-id pub-id-type="publisher-id">e23187</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SNO-DCA: A model for predicting <italic>S</italic>-nitrosylation sites based on densely connected convolutional networks and attention mechanism</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Jia</surname>
          <given-names>Jianhua</given-names>
        </name>
        <email>jjh163yx@163.com</email>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Lv</surname>
          <given-names>Peinuo</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Wei</surname>
          <given-names>Xin</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Qiu</surname>
          <given-names>Wangren</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <aff id="aff1"><label>a</label>Computer Department, Jingdezhen Ceramic University, Jingdezhen, 330403, China</aff>
      <aff id="aff2"><label>b</label>Business School, Jiangxi Institute of Fashion Technology, Nanchang, 330201, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author. <email>jjh163yx@163.com</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>15</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>03</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>10</volume>
    <issue>1</issue>
    <elocation-id>e23187</elocation-id>
    <history>
      <date date-type="received">
        <day>4</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>11</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by Elsevier Ltd.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <p>Protein <italic>S</italic>-nitrosylation is a reversible oxidative reduction post-translational modification that is widely present in the biological community. <italic>S</italic>-nitrosylation can regulate protein function and is closely associated with a variety of diseases, thus identifying <italic>S</italic>-nitrosylation sites are crucial for revealing the function of proteins and related drug discovery. Traditional experimental methods are time-consuming and expensive; therefore, it is necessary to explore more efficient computational methods. Deep learning algorithms perform well in the field of bioinformatics sites prediction, and many studies show that they outperform existing machine learning algorithms. In this work, we proposed a deep learning algorithm-based predictor SNO-DCA for distinguishing between <italic>S</italic>-nitrosylated and non-<italic>S</italic>-nitrosylated sequences. First, one-hot encoding of protein sequences was performed. Second, the dense convolutional blocks were used to capture feature information, and an attention module was added to weigh different features to improve the prediction ability of the model. The 10-fold cross-validation and independent testing experimental results show that our SNO-DCA model outperforms existing <italic>S</italic>-nitrosylation sites prediction models under imbalanced data. In this paper, a web server prediction website: <ext-link ext-link-type="uri" xlink:href="https://sno.cangmang.xyz/SNO-DCA/" id="intref0010">https://sno.cangmang.xyz/SNO-DCA/</ext-link>was established to provide an online prediction service for users. SNO-DCA can be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/peanono/SNO-DCA" id="intref0015">https://github.com/peanono/SNO-DCA</ext-link>.</p>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0015">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Proposed a new deep learning model to predict the <italic>S</italic>-nitrosylation sites.</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Simple features can reduce the intervention of manual features extraction.</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Dense convolutional blocks focus on lower-level features and higher-level features.</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">The efficient channel attention mechanism automatically assigns different weights.</p>
          </list-item>
          <list-item id="u0030">
            <label>•</label>
            <p id="p0030">A webserver has been provided by which users can easily obtain results.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>S-nitrosoylation</kwd>
      <kwd>Dense convolutional networks</kwd>
      <kwd>Attention mechanisms</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0035">As one of the most important PTMs, <italic>S</italic>-nitrosylation (SNO) is the process of covalent binding of nitro ions to protein cysteine residues [<xref rid="bib1" ref-type="bibr">1</xref>]. NO is a new type of messenger molecule that participates in and regulates various life activities and plays an important role in human functions such as the nervous system, immune system, cardiovascular and cerebrovascular. NO generate nitrogen oxides with oxygen and thus <italic>S</italic>-nitrosylate proteins containing free hydrophobic groups, as well as <italic>trans</italic>-nitrosylation reactions with protein cysteine hydrophobic groups. In brief, SNOcan directly or indirectly regulate protein function [<xref rid="bib2" ref-type="bibr">2</xref>]. In addition, SNOis also involved in vital activities such as ion channels, influencing kinase activity, and DNA activity to participate in cell signaling, apoptosis, etc. Levels of <italic>S</italic>-nitrosylated products are associated with various human diseases such as cardiovascular disease [<xref rid="bib3" ref-type="bibr">3</xref>], cancer [<xref rid="bib4" ref-type="bibr">4</xref>], diabetes, asthma, chronic renal failure [<xref rid="bib5" ref-type="bibr">5</xref>], and Alzheimer's disease [<xref rid="bib6" ref-type="bibr">6</xref>]. Accurate identification of protein SNOsites are essential for 1relevant drug development and understanding of fundamental biological processes, therefore, it is necessary for understanding the underlying cellular and protein mechanisms as well as disease prevention and treatment.</p>
    <p id="p0040">The traditional biochemical methods for predicting SNOsites include biotin switch assay (BSA) [<xref rid="bib7" ref-type="bibr">7</xref>], BSA combined with protein sequencing technology [<xref rid="bib8" ref-type="bibr">8</xref>], and BSA combined with isotope-coded affinity tag (ICAT) [<xref rid="bib9" ref-type="bibr">9</xref>]. In the era of big data, as the volume of data increases, these experimental methods can no longer meet the prediction demand, and there is an urgent need for effective computational means to improve the efficiency of SNO sites prediction.</p>
    <p id="p0045">In recent years, machine learning has become a common method for protein SNO sites prediction. Hao et al. [<xref rid="bib10" ref-type="bibr">10</xref>] proposed the SNOSID method and introduced Support Vector Machine (SVM). Xue et al. [<xref rid="bib11" ref-type="bibr">11</xref>] used the amino acid substitution matrix to calculate the <italic>S</italic>-nitrosylated peptide sequences and obtained the corresponding score to develop a GPS-SNO predictor. Lee et al. [<xref rid="bib12" ref-type="bibr">12</xref>]developed a SNO sites predictor using maximal dependence decomposition (MDD) to divide the SNO sites into different groups and used SVM to generate a prediction model for each MDD cluster motif. iSNO-PseAAC predictor proposed by XU et al. [<xref rid="bib13" ref-type="bibr">13</xref>] used PseAAC to represent protein sequence information, constructed a position-specific amino acid Propensity (PSAAP) matrix, and predicted sites using a conditional random field (CRF) algorithm. Although the above methods have achieved several results, the training sets are small, the features are not comprehensive, and the potential information is easily ignored. And machine learning needs to rely on a variety of manually obtained features, which cannot extract advanced features of protein sequences. Deep learning models are less affected by the manual intervention and can extract protein sequence features in depth.</p>
    <p id="p0050">Xie et al. [<xref rid="bib14" ref-type="bibr">14</xref>] []<xref rid="bib14" ref-type="bibr">[]</xref>[]first developed DeepNitro, a prediction tool for nitro sites based on deep learning algorithms, which used four coding features to construct a neural network model with an eight-layer architecture. Hasan et al. [<xref rid="bib15" ref-type="bibr">15</xref>] undersampled Xie's dataset to form a training set with an equal number of positive and negative samples and used four different coding schemes to develop a prediction tool PreSNO by integrating SVM and random forest (RF) methods. Siraj et al. [<xref rid="bib16" ref-type="bibr">16</xref>] applied the method of natural language processing (NLP) to protein feature coding and developed the RecSNO predictor using a bi-directional long and short-term memory network BiLSTM architecture to process sequence data. Although DeepNitro predictors are trained on unbalanced data sets, their features are subject to more manual intervention and there is much room for further improvement in terms of accuracy. Both PreSNO and RecSNO used balanced training sets, while predictions on balanced sets are rarely encountered in practice, and predictions under imbalanced data will be more practical and more rigorous.</p>
    <p id="p0055">To solve the above problems, densely connected convolutional networks (DenseNets) [<xref rid="bib17" ref-type="bibr">17</xref>]and attention mechanisms are introduced in this study to propose a new prediction model for SNOsites called SNO-DCA. We reduced the intervention caused by manual feature extraction by using only one-hot encoding, focus onhigh-dimensional features by densely connected convolutional networks to extract sequence high-level features. The Efficient Channel Attention Module (ECA-Net) [<xref rid="bib18" ref-type="bibr">18</xref>] to evaluate the importance of features on weighting. Fully connected layers to capture the relationship between high-dimensional features, the introduction of class weights to solve the problem of data imbalance, and finally softmax classification. The experimental results show that our model outperforms existing models and can effectively identify SNO sites under imbalanced data.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Materials and methods</title>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Benchmark dataset</title>
      <p id="p0060">A high-quality dataset is the cornerstone of scientific research. In this study, our benchmark dataset was derived from Xie et al. [<xref rid="bib14" ref-type="bibr">14</xref>], which is based on an extensive literature search and a summary of previously reported datasets. 4762 SNO sites were obtained from 3113 proteins. Of these, the experimentally validated SNO sites were positive samples and all other samples were negative sites. To reduce the risk of overfitting the model due to sequence redundancy [<xref rid="bib19" ref-type="bibr">19</xref>], the collected protein sequences were clustered using a CD-HIT with a threshold of 40 %. Finally, we retained 3409 positive sites and 17,103 negative sites as the training set and 365 positive sites and 3354 negative sites as the independent test set.</p>
      <p id="p0065">Potential peptide samples containing SNO sites can be expressed as shown in Equation <xref rid="fd1" ref-type="disp-formula">(1)</xref>:<disp-formula id="fd1"><label>(1)</label><mml:math id="M1" altimg="si1.svg" alttext="Equation 1."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>ζ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ζ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ζ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>…</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>…</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ζ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mi>ζ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>P</italic> is the peptide, the center C is the cysteine, <italic>ζ</italic> is an integer, and R_(-ζ) and R_(+ζ) represent the ξth upstream amino acid residue and downstream amino acid residue of cysteine C, respectively. We used a sliding window to extract the fragments with a window size <italic>L</italic> = 2<italic>ξ</italic>+1. In this paper, we set <italic>ξ</italic> = 20 and fill the missing amino acids with X if the peptide sequence length is less than 20. The statistical information of the dataset is shown in <xref rid="tbl1" ref-type="table">Table 1</xref>.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Statistics for the original sample data.</p></caption><alt-text id="alttext0050">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Original data set</th><th>Positive site</th><th>Negative site</th></tr></thead><tbody><tr><td align="left">Training set</td><td align="left">3409</td><td align="left">17,103</td></tr><tr><td align="left">Independent test set</td><td align="left">365</td><td align="left">3354</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>One-hot encoding</title>
      <p id="p0070">In this work, we encode the protein sequence using one-hot encoding. This encoding is represented by the binary numbers 0 and 1. The amino acids in the protein sequence corresponding to an index of 1 and other positions are 0 [<xref rid="bib20" ref-type="bibr">20</xref>], for example, alanine is encoded as (10⋯0). There are 20 amino acids in total, in addition, the unknown amino acid is set to X. So, for the sequence fragment of length L, the final two-dimensional sparse matrix of <italic>L</italic> × 21 dimensions is obtained. In this paper, <italic>L</italic> is 41, then we have a 41 × 21 (861) dimensional vector matrix.</p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>Model structure</title>
      <p id="p0075">In this paper, we used deep learning algorithms to build a prediction model which can explore the deep features of SNO sites. In this model, we extracted high-level features from a matrix of sequences that have been one-hot encoded by a densely connected convolutional network. The features are fed into an efficient channel attention mechanism to weigh the important features. Finally, after four fully connected layers, the output high-level features are fed to the softmax layer for classification, which effectively predicts the SNO sites. The structure of the SNO-DCA model is shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. For the information encoded by one-hot, advanced features are extracted through the densely connected convolutional networks. Input the features into the attention mechanism of the efficient channel to weigh the important features. After four fully-connected layers, the output advanced features are input to the softmax layer for classification.<fig id="fig1"><label>Fig. 1</label><caption><p>The model framework of SNO-DCA.</p></caption><alt-text id="alttext0010">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p>
    </sec>
    <sec id="sec2.4">
      <label>2.4</label>
      <title>Densely connected convolutional networks to extract features</title>
      <p id="p0080">It has been shown that DenseNets have better performance compared to traditional Convolutional Neural Network (CNN) and Residual Networks (ResNet) [<xref rid="bib21" ref-type="bibr">21</xref>]. DenseNets use a dense connectivity mechanism, where the input of each layer is the output of all previous layers in the channel dimension. This not only mitigates the gradient disappearance to some extent but also enhances the transmission of features and achieves better performance with fewer parameters and less computation. The propagation process of different networks is shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>(A-C).<fig id="fig2"><label>Fig. 2</label><caption><p>Communication process of different networks (A) Densely connected convolution network structure. (B) Short-circuit connection mechanism of Residual Network. (C) Convolutional Neural Network structure.</p></caption><alt-text id="alttext0015">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p>
      <p id="p0085">Before applying the dense convolution block, the one-hot encoded vector of length <italic>L</italic> is firstly convolved by one-dimensional convolution to generate a low-level feature map of protein sequence information using the ELU activation function, which implements the nonlinear transformation. As shown in Equation <xref rid="fd2" ref-type="disp-formula">(2)</xref>.<disp-formula id="fd2"><label>(2)</label><mml:math id="M2" altimg="si2.svg" alttext="Equation 2."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>W</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>E</italic> is one-hot encoding, W is the weight matrix with size 21 <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>S</italic>
<inline-formula><mml:math id="M4" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>D</italic>, 21 is the length of one-hot encoding, <italic>S</italic> is the convolutional kernel size, <italic>D</italic> is the number of convolutional kernels, and b is the bias term. Here, <italic>S</italic> is set to 4 and D is set to 96. <inline-formula><mml:math id="M5" altimg="si4.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the output of the one-dimensional convolutional layer of size <italic>L</italic>
<inline-formula><mml:math id="M6" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>D</italic>.</p>
      <p id="p0090">The output vector of the feature encoding is the input vector of the dense convolutional network. The dense convolutional block uses a series of convolutional operations to obtain a high-dimensional feature representation map, as shown in Equation <xref rid="fd3" ref-type="disp-formula">(3)</xref>.<disp-formula id="fd3"><label>(3)</label><mml:math id="M7" altimg="si5.svg" alttext="Equation 3."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M8" altimg="si6.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the feature vector generated by the <inline-formula><mml:math id="M9" altimg="si7.svg"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> convolution in the dense convolution block. <inline-formula><mml:math id="M10" altimg="si8.svg"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix, <inline-formula><mml:math id="M11" altimg="si9.svg"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is determined by <italic>K</italic> and denotes the number of convolution kernels in the kth convolution layer, <inline-formula><mml:math id="M12" altimg="si10.svg"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> = 32. <inline-formula><mml:math id="M13" altimg="si11.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the bias term, <inline-formula><mml:math id="M14" altimg="si12.svg"><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></inline-formula> indicates that the outputs <inline-formula><mml:math id="M15" altimg="si13.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of the dense convolution blocks are concatenated along the feature dimension.</p>
      <p id="p0095">Then, a transition layer is used between the two dense convolution blocks to perform convolution and pooling operations on the feature maps of the obtained protein sequence information. The transition layer is shown as Equations <xref rid="fd4" ref-type="disp-formula">(4)</xref>, <xref rid="fd5" ref-type="disp-formula">(5)</xref>.<disp-formula id="fd4"><label>(4)</label><mml:math id="M16" altimg="si14.svg" alttext="Equation 4."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd5"><label>(5)</label><mml:math id="M17" altimg="si15.svg" alttext="Equation 5."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M18" altimg="si16.svg"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix, <inline-formula><mml:math id="M19" altimg="si17.svg"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the size of the convolution kernel set to 1, and <inline-formula><mml:math id="M20" altimg="si18.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the bias. Finally, the average pooling operation is applied to <inline-formula><mml:math id="M21" altimg="si19.svg"><mml:mrow><mml:msup><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> to reduce the dimensionality and obtain the output <inline-formula><mml:math id="M22" altimg="si20.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of the transition layer.</p>
      <p id="p0100">Multiple dense convolutional blocks and transition layers are connected in series to construct a dense convolutional network. In this study, the number of dense convolutional blocks is set to 5. Finally, we can extract the high-level features of protein sequences <italic>h.</italic></p>
    </sec>
    <sec id="sec2.5">
      <label>2.5</label>
      <title>-net</title>
      <p id="p0105">Since different features have different levels of importance, we add ECA-Net to each module to weigh the features to enhance useful information and improve model prediction. ECA-Net is implemented based on global average pooling and adaptive computation of one-dimensional convolution of the convolution kernel size k, which effectively avoids dimensionality reduction (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). After using global average pooling <bold>(</bold>GAP) aggregation convolution features without dimension reduction, the ECA module first adaptively determines the kernel size k, then conducts one-dimensional convolution, and the sigmoid function learns the channel attention.<fig id="fig3"><label>Fig. 3</label><caption><p>The efficient channel attention model (ECA-Net).</p></caption><alt-text id="alttext0020">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p>
      <p id="p0110">First, the global averaging pooling operation is performed on the high-level features extracted from the dense convolutional blocks, and the features <italic>h</italic> with width <italic>w</italic> and height <italic>H</italic> and a number of channels <italic>C</italic> are compressed into features <inline-formula><mml:math id="M23" altimg="si21.svg"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of size 1 × 1 × <italic>C</italic>. As shown in Equation <xref rid="fd6" ref-type="disp-formula">(6)</xref>.<disp-formula id="fd6"><label>(6)</label><mml:math id="M24" altimg="si22.svg" alttext="Equation 6."><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p id="p0115">Then a one-dimensional convolution operation is performed to capture the information between different channels, and the feature size obtained after one-dimensional convolution remains 1 × 1 × <italic>C</italic>. As shown in Equation <xref rid="fd7" ref-type="disp-formula">(7)</xref>.<disp-formula id="fd7"><label>(7)</label><mml:math id="M25" altimg="si23.svg" alttext="Equation 7."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>q</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>q</italic> is the weight of each channel and a is the Sigmoid activation function. Where <italic>C1D</italic> denotes a one-dimensional convolution and the convolution kernel size is <italic>k</italic>, i.e., the coverage of cross-channel information interaction, and <italic>k</italic> is proportional to the number of channels <italic>C</italic>. Since the channel dimension, <italic>C</italic> is usually set to the power of 2, a nonlinear mapping relation <italic>φ</italic> is introduced as shown in Equation <xref rid="fd8" ref-type="disp-formula">(8)</xref>.<disp-formula id="fd8"><label>(8)</label><mml:math id="M26" altimg="si24.svg" alttext="Equation 8."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0120">Given the channel dimension <italic>C</italic>, the adaptive function of the one-dimensional convolutional kernel size is shown in Equation <xref rid="fd9" ref-type="disp-formula">(9)</xref>.<disp-formula id="fd9"><label>(9)</label><mml:math id="M27" altimg="si25.svg" alttext="Equation 9."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi mathvariant="italic">log</mml:mi><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:msub><mml:mi>γ</mml:mi></mml:mfrac><mml:mo linebreak="badbreak">+</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>γ</italic> = 2 and <italic>b</italic> = 1. Finally, the weights <italic>q</italic> is multiplied channel by channel with the original input high-level feature maps to obtain the weighted high-level feature maps. As shown in Equation <xref rid="fd10" ref-type="disp-formula">(10)</xref>.<disp-formula id="fd10"><label>(10)</label><mml:math id="M28" altimg="si26.svg" alttext="Equation 10."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>h</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0125">Next, all information is integrated by four fully connected layers with 80, 40, 40 and 20 units, respectively, and a softmax classifier is used to classify the SNO sites, and the predicted classes of the samples are obtained after weighting and activation operations, as shown in Equation <xref rid="fd11" ref-type="disp-formula">(11)</xref>.<disp-formula id="fd11"><label>(11)</label><mml:math id="M29" altimg="si27.svg" alttext="Equation 11."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M30" altimg="si28.svg"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M31" altimg="si29.svg"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are the weight matrices and <inline-formula><mml:math id="M32" altimg="si30.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the bias. <inline-formula><mml:math id="M33" altimg="si31.svg"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability that sample <italic>x</italic> is predicted to be class <inline-formula><mml:math id="M34" altimg="si32.svg"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula><italic>,</italic>
<inline-formula><mml:math id="M35" altimg="si33.svg"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">∈</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p id="p0130">Imbalanced data will affect the prediction ability of the model, which is the problem to be solved [<xref rid="bib22" ref-type="bibr">[22]</xref>, <xref rid="bib23" ref-type="bibr">[23]</xref>, <xref rid="bib24" ref-type="bibr">[24]</xref>]. For this reason, we adopt the method of class weights and set the weight ratio of positive and negative samples to 5:1. Increasing the influence of positive samples allows the model to learn the sequence mechanism of SNO samples better, thus improving the prediction ability of the model.</p>
    </sec>
    <sec id="sec2.6">
      <label>2.6</label>
      <title>Performance evaluation</title>
      <p id="p0135">In this study, we evaluate the performance of the model by utilizing both ten-fold cross-validation and independent test sets. We calculated four statistical metrics: sensitivity (Sn), specificity (Sp), accuracy (Acc), and Mathews correlation coefficient (MCC) [<xref rid="bib25" ref-type="bibr">[25]</xref>, <xref rid="bib26" ref-type="bibr">[26]</xref>, <xref rid="bib27" ref-type="bibr">[27]</xref>] with the following equations. As shown in Equation <xref rid="fd12" ref-type="disp-formula">(12)</xref>, <xref rid="fd13" ref-type="disp-formula">(13)</xref>, <xref rid="fd14" ref-type="disp-formula">(14)</xref>, <xref rid="fd15" ref-type="disp-formula">(15)</xref><disp-formula id="fd12"><label>(12)</label><mml:math id="M36" altimg="si34.svg" alttext="Equation 12."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Sn</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd13"><label>(13)</label><mml:math id="M37" altimg="si35.svg" alttext="Equation 13."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Sp</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd14"><label>(14)</label><mml:math id="M38" altimg="si36.svg" alttext="Equation 14."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Acc</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd15"><label>(15)</label><mml:math id="M39" altimg="si37.svg" alttext="Equation 15."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>MCC</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>−</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0140">Sn was used as a measure of positive accuracy, i.e., an index of accuracy in identifying SNO sites. Sp was used as a measure of negative accuracy, i.e., an index of accuracy in identifying non-SNO sites. Acc represented the proportion of correctly classified samples to the total number of samples, and MCC was considered a balanced measure [<xref rid="bib28" ref-type="bibr">28</xref>]. Here TP, TN, FP, and FN denote true positive, true negative, false positive, and false negative, respectively. In addition, we used the receiver operating characteristic (ROC) area under the ROC curve (AUC) as a measure of model goodness [<xref rid="bib29" ref-type="bibr">29</xref>]. It is obvious that the larger AUC value indicates, the higher model performance.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Results and discussion</title>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Optimization of important hyperparameters</title>
      <p id="p0145">In order to find the optimal network configuration for the model SNO-DCA, which leads to the training of a better site predictor. A few important hyperparameters in the model SNO-DCA, including the number of convolutional kernels in the first CNN layer, the number of dense blocks and the number of convolutional layers in the dense blocks, were determined by ten-fold cross-validation.</p>
      <p id="p0150">Firstly, the number of convolution kernels of the first layer CNN and the layers of the dense blocks were preset to 96 and 4. Adjusted the number of dense blocks, the best performance of the model is achieved when the number of dense blocks were 5 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>A). Secondly, after fixing the number of dense blocks, we adjusted the number of convolution layers in the dense blocks, and determined that the best result is achieved when the number of layers were 4 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>B). Lastly, we fixed the two parameters and then adjusted the number of convolutional kernels in the first layer of CNN to reach the best performance when the number of filters were 96 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>C).<fig id="fig4"><label>Fig. 4</label><caption><p>The results of parameter selection of the model SNO-DCA (A) Selection of the number of dense blocks. (B) Selection of the number of layers. (C) Selection of the number of filters.</p></caption><alt-text id="alttext0025">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>SNO-DCA ablation experiments</title>
      <p id="p0155">To verify the necessity of each component in SNO-DCA, we designed an ablation experiment by ten-fold validation using the training dataset. The experiments included the use of densely connected convolutional networks and the introduction of the ECA-Net layer in the model. The benchmark model adopted the concatenation of the original protein sequence information as input and extracted features by traditional CNNs.</p>
      <p id="p0160">The experimental results are shown in <xref rid="tbl2" ref-type="table">Table 2</xref>, different columns represent different model architectures. The proposed model performed best overall when densely connected convolutional networks and ECA-Net were used. First, to further verify the advantages of dense convolutional networks, we replaced conventional CNNs with DenseNets. Then, ECA-Net was added after each dense convolutional network. The results of ten-fold cross-validation showed that all metrics are significantly improved and dense convolutional networks have significant advantages over conventional CNNs. In addition, ECA-Net also could raise the model performance.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>The 10-flod cross-validation performance with different model structure.</p></caption><alt-text id="alttext0055">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th>CNN</th><th>✓</th><th>✓</th><th>✓</th></tr></thead><tbody><tr><td align="left">+ DenseNets</td><td/><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">+ECA-Net</td><td/><td/><td align="left">✓</td></tr><tr><td align="left">SN</td><td align="left">0.32</td><td align="left">0.80</td><td align="left">0.80</td></tr><tr><td align="left">SP</td><td align="left">0.80</td><td align="left">0.59</td><td align="left">0.62</td></tr><tr><td align="left">ACC</td><td align="left">0.40</td><td align="left">0.63</td><td align="left">0.65</td></tr><tr><td align="left">MCC</td><td align="left">0.09</td><td align="left">0.30</td><td align="left">0.31</td></tr><tr><td align="left">AUC</td><td align="left">0.60</td><td align="left">0.76</td><td align="left">0.77</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Cross-validation performance</title>
      <p id="p0165">We used ten-fold cross-validation to test the results. For an accurate comparison, we used the same training set and test set as the DeepNitro model. The comparison results of all metrics are shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. The AUC of the SNO-DCA model is 0.77, and the ROC curve is shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Comparison with other methods on the training data set.</p></caption><alt-text id="alttext0060">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Predictors</th><th>Sn</th><th>Sp</th><th>Acc</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td align="left">DeepNitro</td><td align="left">NA</td><td align="left">NA</td><td align="left">NA</td><td align="left">NA</td><td align="left">0.71</td></tr><tr><td align="left">SNO-DCA</td><td align="left"><bold>0.80</bold></td><td align="left">0.62</td><td align="left"><bold>0.65</bold></td><td align="left"><bold>0.31</bold></td><td align="left"><bold>0.77</bold></td></tr></tbody></table></table-wrap><fig id="fig5"><label>Fig. 5</label><caption><p>Ten-fold cross-validated ROC curve.</p></caption><alt-text id="alttext0030">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Comparison of independent datasets with existing models</title>
      <p id="p0170">To evaluate the performance of SNO-DCA and ensure the good generalization ability of the model, we have selected the following methods for predicting SNO sites for comparison with our model, taking into account the fact that some models use different training sets and do not provide independent prediction tools. DeepNitro is based on an eight-layer neural network, PreSNO is a linear regression model based on SVM and RF, and RecSNO uses protein sequences as input and is a predictor based on embedded layers and BILSTM. The results are shown in <xref rid="tbl4" ref-type="table">Table 4</xref>. The prediction results of the deep learning predictor SNO-DCA constructed in this paper are Sn = 0.82; Sp = 0.69; Acc = 0.70; MCC = 0.32; AUC = 0.82. The ROC curves are shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Comparison with other methods on the same test data set.</p></caption><alt-text id="alttext0065">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Predictors</th><th>Sn</th><th>Sp</th><th>Acc</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td align="left">GPS-SNO</td><td align="left">0.28</td><td align="left">0.74</td><td align="left">0.69</td><td align="left">0.01</td><td align="left">0.52</td></tr><tr><td align="left">ISNOPseAAC</td><td align="left">0.29</td><td align="left">0.76</td><td align="left">0.71</td><td align="left">0.03</td><td align="left">NA</td></tr><tr><td align="left">SNOSite</td><td align="left">0.67</td><td align="left">0.45</td><td align="left">0.47</td><td align="left">0.07</td><td align="left">NA</td></tr><tr><td align="left">DeepNitro</td><td align="left">0.58</td><td align="left">0.76</td><td align="left">0.73</td><td align="left">0.22</td><td align="left">0.73</td></tr><tr><td align="left">PreSNO</td><td align="left">0.60</td><td align="left">0.77</td><td align="left">0.75</td><td align="left">0.25</td><td align="left">0.76</td></tr><tr><td align="left">RecSNO</td><td align="left">0.77</td><td align="left">0.71</td><td align="left">0.71</td><td align="left">0.30</td><td align="left">0.80</td></tr><tr><td align="left">SNO-DCA</td><td align="left"><bold>0.82</bold></td><td align="left">0.69</td><td align="left">0.70</td><td align="left"><bold>0.32</bold></td><td align="left"><bold>0.81</bold></td></tr></tbody></table></table-wrap><fig id="fig6"><label>Fig. 6</label><caption><p>ROC curve of the independent test set.</p></caption><alt-text id="alttext0035">Fig. 6</alt-text><graphic xlink:href="gr6"/></fig></p>
      <p id="p0175">In <xref rid="tbl4" ref-type="table">Table 4</xref>, we can find that SNO-DCA has the highest Sn, MCC, and AUC; however, it has relatively low Sp due to the antagonistic formula of Sn and Sp. Similar to the finding of Ref [<xref rid="bib30" ref-type="bibr">30</xref>], a significant increase in Sn usually lead to a decrease in Sp and thus Acc. Both PreSNO and RecSNO were constructed on a balanced dataset after downsampling the original dataset trained, in which the negative samples of the dataset are not sufficient. However, models trained on unbalanced training sets are more relevant. In the biological field, the real datasets are often unbalanced and it is especially important to identify as many SNO sites as possible, i.e., the values of Sn and MCC are more important than the other parameters. In contrast, the SNO-DCA predictor constructed in this paper solves the problem of unbalanced data by class weights and does not require the construction of balanced data sets with high Sn and MCC by undersampling, which effectively overcomes the disadvantage of the inadequacy for negative samples brought by the training under balanced data, avoids the lack of information to some extent. Therefore, our SNO-DCA model is more relevant and practical for predicting SNO sites.</p>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Server online prediction website</title>
      <p id="p0180">To facilitate the utilization of our prediction model and provide experimental help to scientific researchers, we have constructed a user-friendly web server website <ext-link ext-link-type="uri" xlink:href="https://sno.cangmang.xyz/SNO-DCA/" id="intref0020">https://sno.cangmang.xyz/SNO-DCA/</ext-link>, whose homepage is shown in <xref rid="fig7" ref-type="fig">Fig. 7</xref>.<list list-type="simple" id="olist0010"><list-item id="o0010"><label>Step1</label><p id="p0185">Enter the URL in your browser and click Search to open the SNO-DCA sever homepage as shown in Figure. Click the Help button in the upper right corner or the More info … Button in the middle of Home to jump to the User's Guide, where one can view the background and a brief introduction about the SNO-DCA prediction model, and learn about the model structure and advantages; the user can see the sequence input format, and click example to jump to the Example interface, which shows the correct format and the incorrect format of the input sequences.</p></list-item><list-item id="o0015"><label>Step2</label><p id="p0190">Visitors can enter a single protein sequence for query in the center box of the homepage and click the Submit button to make a prediction, and the predicted result will be highlighted in red, indicating SNO sites. As shown in <xref rid="fig8" ref-type="fig">Fig. 8</xref>.</p></list-item><list-item id="o0020"><label>Step3</label><p id="p0195">You can also pass the file in FASTA format to the bottom left box of the homepage, enter your username and email address in the bottom right box, and click Submit for batch prediction, and the final prediction will be emailed to the recipient.</p></list-item></list><fig id="fig7"><label>Fig. 7</label><caption><p>Screenshot of the homepage of the web server SNO-DCA.</p></caption><alt-text id="alttext0040">Fig. 7</alt-text><graphic xlink:href="gr7"/></fig><fig id="fig8"><label>Fig. 8</label><caption><p>Screenshot of the predicted results of the web server SNO-DCA.</p></caption><alt-text id="alttext0045">Fig. 8</alt-text><graphic xlink:href="gr8"/></fig></p>
      <p id="p0200">This platform simply encodes the original sequence of proteins to reduce manual feature selection intervention. A dense convolutional network is introduced to extract the high-level features, and an efficient channel attention mechanism is added to weight different features and learn important features. The efficient channel attention mechanism is added to weight different features to learn important features. Finally, the information is fused by the fully connected layer and inputted into the softmax layer for site prediction, and the SNO sites are identified by the prediction probability. The platform gives full effect to the deep learning models to explore the potential information of SNO sites, extract important features and improve the accuracy of SNO sites prediction.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Conclusions</title>
    <p id="p0205">In this study, we proposed SNO-DCA, a deep learning model for SNO sites prediction based on DenseNets and an efficient channel attention module under unbalanced data. The model performs a simple feature representation of protein sequence information, extracts high-level features by dense convolutional blocks, and adds an efficient attention module to further enhance the feature information. The results of both ten-fold cross-validation and independent tests show that SNO-DCA is a powerful prediction tool for identifying SNO sites. We developed a web online server to provide convenience to researchers. Both the training and test sets in this study are non-equilibrium datasets, which meet the practical prediction conditions. Another advantage in this study is that only protein sequences are simply encoded, which greatly reduces the intervention problem of manual feature selection. Meanwhile, the developed deep learning algorithm architecture is used to dig the potential information of the sequences in depth, which can give full play to the deep learning model.</p>
    <p id="p0210">In the follow-up work for this study, we will try not to encode the protein sequences in advance, but to embed the original sequences into the deep learning model to achieve end-to-end prediction, directly mining the hidden information of the samples and improving the prediction accuracy. In addition, we will also explore more possibilities of deep learning algorithms for protein locus prediction, such as capsule network for prediction of small sample data, improved long and short-term memory network combined with improved CNN, etc. At present, the problem of data imbalance is still a major difficulty in the field of bioinformatics. Though the traditional methods of balancing data may have some certain value in the data information processing, we should transform our focus into the imbalanced data with employing the improved/enhanced deep learning algorithm architecture to adapt to the actual protein prediction circumstance.</p>
  </sec>
  <sec sec-type="data-availability" id="sec5">
    <title>Data availability statement</title>
    <p id="p0215">Data will be made available on request. SNO-DCA model and datasets can be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/peanono/SNO-DCA" id="intref0025">https://github.com/peanono/SNO-DCA</ext-link>.</p>
  </sec>
  <sec id="sec6">
    <title>Additional information</title>
    <p id="p0220">No additional information is available for this paper.</p>
  </sec>
  <sec id="sec7">
    <title>CRediT authorship contribution statement</title>
    <p id="p0225"><bold>Jianhua Jia:</bold> Writing – review &amp; editing. <bold>Peinuo Lv:</bold> Writing – original draft. <bold>Xin Wei:</bold> Data curation. <bold>Wangren Qiu:</bold> Validation.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of competing interest</title>
    <p id="p0230">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <mixed-citation publication-type="other" id="sref1">Matthew, W., Foster, et al. Protein S-nitrosylation in health and disease: a current perspective - ScienceDirect [J]. Trends Mol. Med., 15(9): 391-404.</mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>B</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Regulation of cellular function by protein sulfhydryl nitrosylation [J]</article-title>
        <source>Chin. J. Pathophysiol.</source>
        <volume>27</volume>
        <issue>11</issue>
        <year>2011</year>
        <fpage>2237</fpage>
        <lpage>2240</lpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Turko</surname>
            <given-names>I.V.</given-names>
          </name>
          <name>
            <surname>Murad</surname>
            <given-names>F.J.P.R.</given-names>
          </name>
        </person-group>
        <article-title>Protein nitration in cardiovascular diseases [J]</article-title>
        <source>Pharmacol. Rev.</source>
        <volume>54</volume>
        <issue>4</issue>
        <year>2002</year>
        <fpage>619</fpage>
        <pub-id pub-id-type="pmid">12429871</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Protein S-nitrosylation and cancer [J]</article-title>
        <source>Cancer Lett.</source>
        <volume>320</volume>
        <issue>2</issue>
        <year>2012</year>
        <fpage>123</fpage>
        <lpage>129</lpage>
        <pub-id pub-id-type="pmid">22425962</pub-id>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Piroddi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Palmese</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pilolli</surname>
            <given-names>F.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Plasma nitroproteome of kidney disease patients [J]</article-title>
        <source>Amino Acids</source>
        <volume>40</volume>
        <issue>2</issue>
        <year>2011</year>
        <fpage>653</fpage>
        <lpage>657</lpage>
        <pub-id pub-id-type="pmid">20676907</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <article-title>Heneka T S W S B-a S S G K B T. Quantitative proteomics of synaptosome S-nitrosylation in Alzheimer's disease [J]</article-title>
        <source>J. Neurochem.</source>
        <volume>152</volume>
        <issue>6</issue>
        <year>2020</year>
        <fpage>710</fpage>
        <lpage>726</lpage>
        <pub-id pub-id-type="pmid">31520481</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Jaffrey</surname>
            <given-names>S.R.</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>S.H.</given-names>
          </name>
        </person-group>
        <article-title>The biotin switch method for the detection of S-nitrosylated proteins [J]</article-title>
        <source>Sci. STKE : Signal Transduct. Knowl. Environ.</source>
        <volume>2001</volume>
        <issue>86</issue>
        <year>2001</year>
        <fpage>pl1</fpage>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Lindermayr</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Saalbach</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Durner</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Proteomic identification of S-nitrosylated proteins in Arabidopsis [J]</article-title>
        <source>Plant Physiol.</source>
        <volume>137</volume>
        <issue>3</issue>
        <year>2005</year>
        <fpage>921</fpage>
        <lpage>930</lpage>
        <pub-id pub-id-type="pmid">15734904</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Redox regulatory mechanism of transnitrosylation by thioredoxin [J]</article-title>
        <source>Mol. Cell. Proteomics : MCP</source>
        <volume>9</volume>
        <issue>10</issue>
        <year>2010</year>
        <fpage>2262</fpage>
        <lpage>2275</lpage>
        <pub-id pub-id-type="pmid">20660346</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Hao</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Derakhshan</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SNOSID, a proteomic method for identification of cysteine S-nitrosylation sites in complex protein mixtures</article-title>
        <source>Proc. Natl. Acad. Sci. U.S.A.</source>
        <volume>103</volume>
        <issue>4</issue>
        <year>2006</year>
        <fpage>1012</fpage>
        <lpage>1017</lpage>
        <pub-id pub-id-type="pmid">16418269</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>GPS-SNO: computational prediction of protein S-nitrosylation sites with a modified GPS algorithm [J]</article-title>
        <source>PLoS One</source>
        <volume>5</volume>
        <issue>6</issue>
        <year>2010</year>
        <object-id pub-id-type="publisher-id">e11290</object-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>T.Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>T.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SNOSite: exploiting maximal dependence decomposition to identify cysteine S-nitrosylation with substrate site specificity [J]</article-title>
        <source>PLoS One</source>
        <volume>6</volume>
        <issue>7</issue>
        <year>2011</year>
        <object-id pub-id-type="publisher-id">e21849</object-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>L.Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>iSNO-PseAAC: predict cysteine S-nitrosylation sites in proteins by incorporating position specific amino acid propensity into pseudo amino acid composition [J]</article-title>
        <source>PLoS One</source>
        <volume>8</volume>
        <issue>2</issue>
        <year>2013</year>
        <object-id pub-id-type="publisher-id">e55844</object-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepNitro: prediction of protein nitration and nitrosylation sites by deep learning [J]</article-title>
        <source>Dev. Reprod. Biol.</source>
        <volume>16</volume>
        <issue>4</issue>
        <year>2018</year>
        <fpage>294</fpage>
        <lpage>306</lpage>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Hasan</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Manavalan</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Khatun</surname>
            <given-names>M.S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prediction of S-nitrosylation sites by integrating support vector machines and random forest [J]</article-title>
        <source>Molecular omics</source>
        <volume>15</volume>
        <issue>6</issue>
        <year>2019</year>
        <fpage>451</fpage>
        <lpage>458</lpage>
        <pub-id pub-id-type="pmid">31710075</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Siraj</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chantsalnyam</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tayara</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>RecSNO: prediction of protein S-nitrosylation sites using a recurrent neural network [J]</article-title>
        <source>IEEE Access</source>
        <volume>9</volume>
        <year>2021</year>
        <fpage>6674</fpage>
        <lpage>6682</lpage>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="book" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Laurens</surname>
            <given-names>V.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Densely Connected Convolutional Networks</part-title>
        <year>2016</year>
        <publisher-name>proceedings of the IEEE Computer Society, F</publisher-name>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="book" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>P.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>ECA-net: efficient Channel Attention for deep convolutional neural networks</part-title>
        <source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), F</source>
        <year>2020</year>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>LiminNiu</surname>
            <given-names>Beifang</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data [J]</article-title>
        <source>Bioinformatics</source>
        <volume>28</volume>
        <issue>23</issue>
        <year>2012</year>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Capsule network for protein post-translational modification site prediction [J]</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <issue>14</issue>
        <year>2018</year>
        <fpage>2386</fpage>
        <lpage>2394</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="book" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Deep Residual Learning for Image Recognition; Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), F 27-30 June 2016</part-title>
        <year>2016</year>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <mixed-citation publication-type="other" id="sref22">Wei L, Hu J, Li F, et al. Comparative analysis and prediction of quorum-sensing peptides using feature representation learning and machine learning algorithms. LID - 10.1093/bib/bby107 [doi] [J]. (1477-4054 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improved prediction of protein–protein interactions using novel negative samples, features, and an ensemble classifier [J]</article-title>
        <source>Artif. Intell. Med.</source>
        <volume>83</volume>
        <year>2017</year>
        <fpage>67</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="pmid">28320624</pub-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <mixed-citation publication-type="other" id="sref24">Wei L, Zhou C, Chen H, et al. ACPred-FL: a sequence-based predictor using effective feature representation to improve the prediction of anti-cancer peptides [J]. (1367-4811 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <mixed-citation publication-type="other" id="sref25">Su R, Liu X, Xiao G, et al. Meta-GDBP: a high-level stacked regression model to improve anticancer drug response prediction [J]. (1477-4054 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Computational prediction and interpretation of cell-specific replication origin sites from multiple eukaryotes by exploiting stacking framework [J]</article-title>
        <source>Briefings Bioinf.</source>
        <volume>22</volume>
        <issue>4</issue>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Local-DPP: an improved DNA-binding protein prediction method by exploring local evolutionary information [J]</article-title>
        <source>Inf. Sci.</source>
        <volume>384</volume>
        <year>2017</year>
        <fpage>135</fpage>
        <lpage>144</lpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Eisenberg</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Marcotte</surname>
            <given-names>E.M.</given-names>
          </name>
          <name>
            <surname>Xenarios</surname>
            <given-names>I.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein function in the post-genomic era [J]</article-title>
        <source>Nature</source>
        <volume>405</volume>
        <issue>6788</issue>
        <year>2000</year>
        <fpage>823</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="pmid">10866208</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Jurman</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation [J]</article-title>
        <source>BMC Genom.</source>
        <volume>21</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>6</fpage>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="journal" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>E.A.</given-names>
          </name>
        </person-group>
        <article-title>Learning from imbalanced data [J]</article-title>
        <source>IEEE Trans. Knowl. Data Eng.</source>
        <volume>21</volume>
        <issue>9</issue>
        <year>2009</year>
        <fpage>1263</fpage>
        <lpage>1284</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec1">
    <title>Abbreviations</title>
    <p id="p0240">
      <def-list>
        <def-item>
          <term id="d0010">SNO</term>
          <def>
            <p id="p0245"><italic>S</italic>-nitrosylation</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0015">BSA</term>
          <def>
            <p id="p0250">biotin switch assay</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0020">ICAT</term>
          <def>
            <p id="p0255">isotope-coded affinity tag</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0025">SVM</term>
          <def>
            <p id="p0260">Support Vector Machine</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0030">MDD</term>
          <def>
            <p id="p0265">maximal dependence decomposition</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0035">PSAAP</term>
          <def>
            <p id="p0270">position-specific amino acid Propensity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0040">CRF</term>
          <def>
            <p id="p0275">conditional random field</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0045">RF</term>
          <def>
            <p id="p0280">random forest</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0050">NLP</term>
          <def>
            <p id="p0285">natural language processing</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0055">DenseNets</term>
          <def>
            <p id="p0290">densely connected convolutional networks</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0060">ECA-Net</term>
          <def>
            <p id="p0295">Efficient Channel Attention Module</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0065">CNN</term>
          <def>
            <p id="p0300">Convolutional Neural Network</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0070">ResNet</term>
          <def>
            <p id="p0305">Residual Networks</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0075">GAP</term>
          <def>
            <p id="p0310">global average pooling</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0080">Sn</term>
          <def>
            <p id="p0315">sensitivity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0085">Sp</term>
          <def>
            <p id="p0320">specificity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0090">Acc</term>
          <def>
            <p id="p0325">accuracy</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0095">MCC</term>
          <def>
            <p id="p0330">Mathews correlation coefficient</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0100">ROC</term>
          <def>
            <p id="p0335">receiver operating characteristic</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0105">AUC</term>
          <def>
            <p id="p0340">area under the ROC curve</p>
          </def>
        </def-item>
      </def-list>
    </p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0235">This work was partially supported by the <funding-source id="gs4">National Nature Science Foundation of China</funding-source> (Nos. 61761023 and 62162032), the <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100004479</institution-id><institution>Natural Science Foundation of Jiangxi Province</institution></institution-wrap></funding-source>, China (Nos. 20202BABL202004), the Scientific Research Plan of the <funding-source id="gs3"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100009102</institution-id><institution>Department of Education of Jiangxi Province</institution></institution-wrap></funding-source> (GJJ212419 and GJJ2201004). These funders had no role in the study design, data collection and analysis, decision to publish or preparation of manuscript.</p>
  </ack>
</back>
<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_HLY23187 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEgr8 jpg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Heliyon</journal-id>
    <journal-id journal-id-type="iso-abbrev">Heliyon</journal-id>
    <journal-title-group>
      <journal-title>Heliyon</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2405-8440</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10750070</article-id>
    <article-id pub-id-type="pii">S2405-8440(23)10395-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.heliyon.2023.e23187</article-id>
    <article-id pub-id-type="publisher-id">e23187</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SNO-DCA: A model for predicting <italic>S</italic>-nitrosylation sites based on densely connected convolutional networks and attention mechanism</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Jia</surname>
          <given-names>Jianhua</given-names>
        </name>
        <email>jjh163yx@163.com</email>
        <xref rid="aff1" ref-type="aff">a</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Lv</surname>
          <given-names>Peinuo</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Wei</surname>
          <given-names>Xin</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Qiu</surname>
          <given-names>Wangren</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <aff id="aff1"><label>a</label>Computer Department, Jingdezhen Ceramic University, Jingdezhen, 330403, China</aff>
      <aff id="aff2"><label>b</label>Business School, Jiangxi Institute of Fashion Technology, Nanchang, 330201, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Corresponding author. <email>jjh163yx@163.com</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <day>15</day>
      <month>1</month>
      <year>2024</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>03</day>
      <month>12</month>
      <year>2023</year>
    </pub-date>
    <volume>10</volume>
    <issue>1</issue>
    <elocation-id>e23187</elocation-id>
    <history>
      <date date-type="received">
        <day>4</day>
        <month>5</month>
        <year>2023</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>11</month>
        <year>2023</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>11</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors. Published by Elsevier Ltd.</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder/>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <p>Protein <italic>S</italic>-nitrosylation is a reversible oxidative reduction post-translational modification that is widely present in the biological community. <italic>S</italic>-nitrosylation can regulate protein function and is closely associated with a variety of diseases, thus identifying <italic>S</italic>-nitrosylation sites are crucial for revealing the function of proteins and related drug discovery. Traditional experimental methods are time-consuming and expensive; therefore, it is necessary to explore more efficient computational methods. Deep learning algorithms perform well in the field of bioinformatics sites prediction, and many studies show that they outperform existing machine learning algorithms. In this work, we proposed a deep learning algorithm-based predictor SNO-DCA for distinguishing between <italic>S</italic>-nitrosylated and non-<italic>S</italic>-nitrosylated sequences. First, one-hot encoding of protein sequences was performed. Second, the dense convolutional blocks were used to capture feature information, and an attention module was added to weigh different features to improve the prediction ability of the model. The 10-fold cross-validation and independent testing experimental results show that our SNO-DCA model outperforms existing <italic>S</italic>-nitrosylation sites prediction models under imbalanced data. In this paper, a web server prediction website: <ext-link ext-link-type="uri" xlink:href="https://sno.cangmang.xyz/SNO-DCA/" id="intref0010">https://sno.cangmang.xyz/SNO-DCA/</ext-link>was established to provide an online prediction service for users. SNO-DCA can be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/peanono/SNO-DCA" id="intref0015">https://github.com/peanono/SNO-DCA</ext-link>.</p>
    </abstract>
    <abstract abstract-type="author-highlights" id="abs0015">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="ulist0010">
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Proposed a new deep learning model to predict the <italic>S</italic>-nitrosylation sites.</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Simple features can reduce the intervention of manual features extraction.</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Dense convolutional blocks focus on lower-level features and higher-level features.</p>
          </list-item>
          <list-item id="u0025">
            <label>•</label>
            <p id="p0025">The efficient channel attention mechanism automatically assigns different weights.</p>
          </list-item>
          <list-item id="u0030">
            <label>•</label>
            <p id="p0030">A webserver has been provided by which users can easily obtain results.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>S-nitrosoylation</kwd>
      <kwd>Dense convolutional networks</kwd>
      <kwd>Attention mechanisms</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0035">As one of the most important PTMs, <italic>S</italic>-nitrosylation (SNO) is the process of covalent binding of nitro ions to protein cysteine residues [<xref rid="bib1" ref-type="bibr">1</xref>]. NO is a new type of messenger molecule that participates in and regulates various life activities and plays an important role in human functions such as the nervous system, immune system, cardiovascular and cerebrovascular. NO generate nitrogen oxides with oxygen and thus <italic>S</italic>-nitrosylate proteins containing free hydrophobic groups, as well as <italic>trans</italic>-nitrosylation reactions with protein cysteine hydrophobic groups. In brief, SNOcan directly or indirectly regulate protein function [<xref rid="bib2" ref-type="bibr">2</xref>]. In addition, SNOis also involved in vital activities such as ion channels, influencing kinase activity, and DNA activity to participate in cell signaling, apoptosis, etc. Levels of <italic>S</italic>-nitrosylated products are associated with various human diseases such as cardiovascular disease [<xref rid="bib3" ref-type="bibr">3</xref>], cancer [<xref rid="bib4" ref-type="bibr">4</xref>], diabetes, asthma, chronic renal failure [<xref rid="bib5" ref-type="bibr">5</xref>], and Alzheimer's disease [<xref rid="bib6" ref-type="bibr">6</xref>]. Accurate identification of protein SNOsites are essential for 1relevant drug development and understanding of fundamental biological processes, therefore, it is necessary for understanding the underlying cellular and protein mechanisms as well as disease prevention and treatment.</p>
    <p id="p0040">The traditional biochemical methods for predicting SNOsites include biotin switch assay (BSA) [<xref rid="bib7" ref-type="bibr">7</xref>], BSA combined with protein sequencing technology [<xref rid="bib8" ref-type="bibr">8</xref>], and BSA combined with isotope-coded affinity tag (ICAT) [<xref rid="bib9" ref-type="bibr">9</xref>]. In the era of big data, as the volume of data increases, these experimental methods can no longer meet the prediction demand, and there is an urgent need for effective computational means to improve the efficiency of SNO sites prediction.</p>
    <p id="p0045">In recent years, machine learning has become a common method for protein SNO sites prediction. Hao et al. [<xref rid="bib10" ref-type="bibr">10</xref>] proposed the SNOSID method and introduced Support Vector Machine (SVM). Xue et al. [<xref rid="bib11" ref-type="bibr">11</xref>] used the amino acid substitution matrix to calculate the <italic>S</italic>-nitrosylated peptide sequences and obtained the corresponding score to develop a GPS-SNO predictor. Lee et al. [<xref rid="bib12" ref-type="bibr">12</xref>]developed a SNO sites predictor using maximal dependence decomposition (MDD) to divide the SNO sites into different groups and used SVM to generate a prediction model for each MDD cluster motif. iSNO-PseAAC predictor proposed by XU et al. [<xref rid="bib13" ref-type="bibr">13</xref>] used PseAAC to represent protein sequence information, constructed a position-specific amino acid Propensity (PSAAP) matrix, and predicted sites using a conditional random field (CRF) algorithm. Although the above methods have achieved several results, the training sets are small, the features are not comprehensive, and the potential information is easily ignored. And machine learning needs to rely on a variety of manually obtained features, which cannot extract advanced features of protein sequences. Deep learning models are less affected by the manual intervention and can extract protein sequence features in depth.</p>
    <p id="p0050">Xie et al. [<xref rid="bib14" ref-type="bibr">14</xref>] []<xref rid="bib14" ref-type="bibr">[]</xref>[]first developed DeepNitro, a prediction tool for nitro sites based on deep learning algorithms, which used four coding features to construct a neural network model with an eight-layer architecture. Hasan et al. [<xref rid="bib15" ref-type="bibr">15</xref>] undersampled Xie's dataset to form a training set with an equal number of positive and negative samples and used four different coding schemes to develop a prediction tool PreSNO by integrating SVM and random forest (RF) methods. Siraj et al. [<xref rid="bib16" ref-type="bibr">16</xref>] applied the method of natural language processing (NLP) to protein feature coding and developed the RecSNO predictor using a bi-directional long and short-term memory network BiLSTM architecture to process sequence data. Although DeepNitro predictors are trained on unbalanced data sets, their features are subject to more manual intervention and there is much room for further improvement in terms of accuracy. Both PreSNO and RecSNO used balanced training sets, while predictions on balanced sets are rarely encountered in practice, and predictions under imbalanced data will be more practical and more rigorous.</p>
    <p id="p0055">To solve the above problems, densely connected convolutional networks (DenseNets) [<xref rid="bib17" ref-type="bibr">17</xref>]and attention mechanisms are introduced in this study to propose a new prediction model for SNOsites called SNO-DCA. We reduced the intervention caused by manual feature extraction by using only one-hot encoding, focus onhigh-dimensional features by densely connected convolutional networks to extract sequence high-level features. The Efficient Channel Attention Module (ECA-Net) [<xref rid="bib18" ref-type="bibr">18</xref>] to evaluate the importance of features on weighting. Fully connected layers to capture the relationship between high-dimensional features, the introduction of class weights to solve the problem of data imbalance, and finally softmax classification. The experimental results show that our model outperforms existing models and can effectively identify SNO sites under imbalanced data.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Materials and methods</title>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Benchmark dataset</title>
      <p id="p0060">A high-quality dataset is the cornerstone of scientific research. In this study, our benchmark dataset was derived from Xie et al. [<xref rid="bib14" ref-type="bibr">14</xref>], which is based on an extensive literature search and a summary of previously reported datasets. 4762 SNO sites were obtained from 3113 proteins. Of these, the experimentally validated SNO sites were positive samples and all other samples were negative sites. To reduce the risk of overfitting the model due to sequence redundancy [<xref rid="bib19" ref-type="bibr">19</xref>], the collected protein sequences were clustered using a CD-HIT with a threshold of 40 %. Finally, we retained 3409 positive sites and 17,103 negative sites as the training set and 365 positive sites and 3354 negative sites as the independent test set.</p>
      <p id="p0065">Potential peptide samples containing SNO sites can be expressed as shown in Equation <xref rid="fd1" ref-type="disp-formula">(1)</xref>:<disp-formula id="fd1"><label>(1)</label><mml:math id="M1" altimg="si1.svg" alttext="Equation 1."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>ζ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ζ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ζ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>…</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>C</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>…</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>ζ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mi>ζ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>P</italic> is the peptide, the center C is the cysteine, <italic>ζ</italic> is an integer, and R_(-ζ) and R_(+ζ) represent the ξth upstream amino acid residue and downstream amino acid residue of cysteine C, respectively. We used a sliding window to extract the fragments with a window size <italic>L</italic> = 2<italic>ξ</italic>+1. In this paper, we set <italic>ξ</italic> = 20 and fill the missing amino acids with X if the peptide sequence length is less than 20. The statistical information of the dataset is shown in <xref rid="tbl1" ref-type="table">Table 1</xref>.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Statistics for the original sample data.</p></caption><alt-text id="alttext0050">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Original data set</th><th>Positive site</th><th>Negative site</th></tr></thead><tbody><tr><td align="left">Training set</td><td align="left">3409</td><td align="left">17,103</td></tr><tr><td align="left">Independent test set</td><td align="left">365</td><td align="left">3354</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>One-hot encoding</title>
      <p id="p0070">In this work, we encode the protein sequence using one-hot encoding. This encoding is represented by the binary numbers 0 and 1. The amino acids in the protein sequence corresponding to an index of 1 and other positions are 0 [<xref rid="bib20" ref-type="bibr">20</xref>], for example, alanine is encoded as (10⋯0). There are 20 amino acids in total, in addition, the unknown amino acid is set to X. So, for the sequence fragment of length L, the final two-dimensional sparse matrix of <italic>L</italic> × 21 dimensions is obtained. In this paper, <italic>L</italic> is 41, then we have a 41 × 21 (861) dimensional vector matrix.</p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>Model structure</title>
      <p id="p0075">In this paper, we used deep learning algorithms to build a prediction model which can explore the deep features of SNO sites. In this model, we extracted high-level features from a matrix of sequences that have been one-hot encoded by a densely connected convolutional network. The features are fed into an efficient channel attention mechanism to weigh the important features. Finally, after four fully connected layers, the output high-level features are fed to the softmax layer for classification, which effectively predicts the SNO sites. The structure of the SNO-DCA model is shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. For the information encoded by one-hot, advanced features are extracted through the densely connected convolutional networks. Input the features into the attention mechanism of the efficient channel to weigh the important features. After four fully-connected layers, the output advanced features are input to the softmax layer for classification.<fig id="fig1"><label>Fig. 1</label><caption><p>The model framework of SNO-DCA.</p></caption><alt-text id="alttext0010">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p>
    </sec>
    <sec id="sec2.4">
      <label>2.4</label>
      <title>Densely connected convolutional networks to extract features</title>
      <p id="p0080">It has been shown that DenseNets have better performance compared to traditional Convolutional Neural Network (CNN) and Residual Networks (ResNet) [<xref rid="bib21" ref-type="bibr">21</xref>]. DenseNets use a dense connectivity mechanism, where the input of each layer is the output of all previous layers in the channel dimension. This not only mitigates the gradient disappearance to some extent but also enhances the transmission of features and achieves better performance with fewer parameters and less computation. The propagation process of different networks is shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>(A-C).<fig id="fig2"><label>Fig. 2</label><caption><p>Communication process of different networks (A) Densely connected convolution network structure. (B) Short-circuit connection mechanism of Residual Network. (C) Convolutional Neural Network structure.</p></caption><alt-text id="alttext0015">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p>
      <p id="p0085">Before applying the dense convolution block, the one-hot encoded vector of length <italic>L</italic> is firstly convolved by one-dimensional convolution to generate a low-level feature map of protein sequence information using the ELU activation function, which implements the nonlinear transformation. As shown in Equation <xref rid="fd2" ref-type="disp-formula">(2)</xref>.<disp-formula id="fd2"><label>(2)</label><mml:math id="M2" altimg="si2.svg" alttext="Equation 2."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>W</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>E</italic> is one-hot encoding, W is the weight matrix with size 21 <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>S</italic>
<inline-formula><mml:math id="M4" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>D</italic>, 21 is the length of one-hot encoding, <italic>S</italic> is the convolutional kernel size, <italic>D</italic> is the number of convolutional kernels, and b is the bias term. Here, <italic>S</italic> is set to 4 and D is set to 96. <inline-formula><mml:math id="M5" altimg="si4.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is the output of the one-dimensional convolutional layer of size <italic>L</italic>
<inline-formula><mml:math id="M6" altimg="si3.svg"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo></mml:mrow></mml:math></inline-formula>
<italic>D</italic>.</p>
      <p id="p0090">The output vector of the feature encoding is the input vector of the dense convolutional network. The dense convolutional block uses a series of convolutional operations to obtain a high-dimensional feature representation map, as shown in Equation <xref rid="fd3" ref-type="disp-formula">(3)</xref>.<disp-formula id="fd3"><label>(3)</label><mml:math id="M7" altimg="si5.svg" alttext="Equation 3."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M8" altimg="si6.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the feature vector generated by the <inline-formula><mml:math id="M9" altimg="si7.svg"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> convolution in the dense convolution block. <inline-formula><mml:math id="M10" altimg="si8.svg"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:mi>S</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix, <inline-formula><mml:math id="M11" altimg="si9.svg"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is determined by <italic>K</italic> and denotes the number of convolution kernels in the kth convolution layer, <inline-formula><mml:math id="M12" altimg="si10.svg"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> = 32. <inline-formula><mml:math id="M13" altimg="si11.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the bias term, <inline-formula><mml:math id="M14" altimg="si12.svg"><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></inline-formula> indicates that the outputs <inline-formula><mml:math id="M15" altimg="si13.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of the dense convolution blocks are concatenated along the feature dimension.</p>
      <p id="p0095">Then, a transition layer is used between the two dense convolution blocks to perform convolution and pooling operations on the feature maps of the obtained protein sequence information. The transition layer is shown as Equations <xref rid="fd4" ref-type="disp-formula">(4)</xref>, <xref rid="fd5" ref-type="disp-formula">(5)</xref>.<disp-formula id="fd4"><label>(4)</label><mml:math id="M16" altimg="si14.svg" alttext="Equation 4."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>;</mml:mo><mml:mo>…</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mo>″</mml:mo></mml:msup><mml:mo linebreak="badbreak">+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd5"><label>(5)</label><mml:math id="M17" altimg="si15.svg" alttext="Equation 5."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M18" altimg="si16.svg"><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>D</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the weight matrix, <inline-formula><mml:math id="M19" altimg="si17.svg"><mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the size of the convolution kernel set to 1, and <inline-formula><mml:math id="M20" altimg="si18.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mo>″</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is the bias. Finally, the average pooling operation is applied to <inline-formula><mml:math id="M21" altimg="si19.svg"><mml:mrow><mml:msup><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> to reduce the dimensionality and obtain the output <inline-formula><mml:math id="M22" altimg="si20.svg"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> of the transition layer.</p>
      <p id="p0100">Multiple dense convolutional blocks and transition layers are connected in series to construct a dense convolutional network. In this study, the number of dense convolutional blocks is set to 5. Finally, we can extract the high-level features of protein sequences <italic>h.</italic></p>
    </sec>
    <sec id="sec2.5">
      <label>2.5</label>
      <title>-net</title>
      <p id="p0105">Since different features have different levels of importance, we add ECA-Net to each module to weigh the features to enhance useful information and improve model prediction. ECA-Net is implemented based on global average pooling and adaptive computation of one-dimensional convolution of the convolution kernel size k, which effectively avoids dimensionality reduction (<xref rid="fig3" ref-type="fig">Fig. 3</xref>). After using global average pooling <bold>(</bold>GAP) aggregation convolution features without dimension reduction, the ECA module first adaptively determines the kernel size k, then conducts one-dimensional convolution, and the sigmoid function learns the channel attention.<fig id="fig3"><label>Fig. 3</label><caption><p>The efficient channel attention model (ECA-Net).</p></caption><alt-text id="alttext0020">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p>
      <p id="p0110">First, the global averaging pooling operation is performed on the high-level features extracted from the dense convolutional blocks, and the features <italic>h</italic> with width <italic>w</italic> and height <italic>H</italic> and a number of channels <italic>C</italic> are compressed into features <inline-formula><mml:math id="M23" altimg="si21.svg"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of size 1 × 1 × <italic>C</italic>. As shown in Equation <xref rid="fd6" ref-type="disp-formula">(6)</xref>.<disp-formula id="fd6"><label>(6)</label><mml:math id="M24" altimg="si22.svg" alttext="Equation 6."><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mo linebreak="badbreak">=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p id="p0115">Then a one-dimensional convolution operation is performed to capture the information between different channels, and the feature size obtained after one-dimensional convolution remains 1 × 1 × <italic>C</italic>. As shown in Equation <xref rid="fd7" ref-type="disp-formula">(7)</xref>.<disp-formula id="fd7"><label>(7)</label><mml:math id="M25" altimg="si23.svg" alttext="Equation 7."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>q</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mn>1</mml:mn><mml:msub><mml:mi>D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>q</italic> is the weight of each channel and a is the Sigmoid activation function. Where <italic>C1D</italic> denotes a one-dimensional convolution and the convolution kernel size is <italic>k</italic>, i.e., the coverage of cross-channel information interaction, and <italic>k</italic> is proportional to the number of channels <italic>C</italic>. Since the channel dimension, <italic>C</italic> is usually set to the power of 2, a nonlinear mapping relation <italic>φ</italic> is introduced as shown in Equation <xref rid="fd8" ref-type="disp-formula">(8)</xref>.<disp-formula id="fd8"><label>(8)</label><mml:math id="M26" altimg="si24.svg" alttext="Equation 8."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>C</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>φ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0120">Given the channel dimension <italic>C</italic>, the adaptive function of the one-dimensional convolutional kernel size is shown in Equation <xref rid="fd9" ref-type="disp-formula">(9)</xref>.<disp-formula id="fd9"><label>(9)</label><mml:math id="M27" altimg="si25.svg" alttext="Equation 9."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">|</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi mathvariant="italic">log</mml:mi><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:msub><mml:mi>γ</mml:mi></mml:mfrac><mml:mo linebreak="badbreak">+</mml:mo><mml:mfrac><mml:mi>b</mml:mi><mml:mi>γ</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="true">|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <italic>γ</italic> = 2 and <italic>b</italic> = 1. Finally, the weights <italic>q</italic> is multiplied channel by channel with the original input high-level feature maps to obtain the weighted high-level feature maps. As shown in Equation <xref rid="fd10" ref-type="disp-formula">(10)</xref>.<disp-formula id="fd10"><label>(10)</label><mml:math id="M28" altimg="si26.svg" alttext="Equation 10."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>h</mml:mi><mml:mo linebreak="badbreak">×</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0125">Next, all information is integrated by four fully connected layers with 80, 40, 40 and 20 units, respectively, and a softmax classifier is used to classify the SNO sites, and the predicted classes of the samples are obtained after weighting and activation operations, as shown in Equation <xref rid="fd11" ref-type="disp-formula">(11)</xref>.<disp-formula id="fd11"><label>(11)</label><mml:math id="M29" altimg="si27.svg" alttext="Equation 11."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:munderover><mml:mrow><mml:mi mathvariant="italic">exp</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>.</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M30" altimg="si28.svg"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M31" altimg="si29.svg"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>j</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> are the weight matrices and <inline-formula><mml:math id="M32" altimg="si30.svg"><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the bias. <inline-formula><mml:math id="M33" altimg="si31.svg"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true">|</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the probability that sample <italic>x</italic> is predicted to be class <inline-formula><mml:math id="M34" altimg="si32.svg"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula><italic>,</italic>
<inline-formula><mml:math id="M35" altimg="si33.svg"><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">∈</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy="true">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p id="p0130">Imbalanced data will affect the prediction ability of the model, which is the problem to be solved [<xref rid="bib22" ref-type="bibr">[22]</xref>, <xref rid="bib23" ref-type="bibr">[23]</xref>, <xref rid="bib24" ref-type="bibr">[24]</xref>]. For this reason, we adopt the method of class weights and set the weight ratio of positive and negative samples to 5:1. Increasing the influence of positive samples allows the model to learn the sequence mechanism of SNO samples better, thus improving the prediction ability of the model.</p>
    </sec>
    <sec id="sec2.6">
      <label>2.6</label>
      <title>Performance evaluation</title>
      <p id="p0135">In this study, we evaluate the performance of the model by utilizing both ten-fold cross-validation and independent test sets. We calculated four statistical metrics: sensitivity (Sn), specificity (Sp), accuracy (Acc), and Mathews correlation coefficient (MCC) [<xref rid="bib25" ref-type="bibr">[25]</xref>, <xref rid="bib26" ref-type="bibr">[26]</xref>, <xref rid="bib27" ref-type="bibr">[27]</xref>] with the following equations. As shown in Equation <xref rid="fd12" ref-type="disp-formula">(12)</xref>, <xref rid="fd13" ref-type="disp-formula">(13)</xref>, <xref rid="fd14" ref-type="disp-formula">(14)</xref>, <xref rid="fd15" ref-type="disp-formula">(15)</xref><disp-formula id="fd12"><label>(12)</label><mml:math id="M36" altimg="si34.svg" alttext="Equation 12."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Sn</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mtext>TP</mml:mtext><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd13"><label>(13)</label><mml:math id="M37" altimg="si35.svg" alttext="Equation 13."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Sp</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mtext>TN</mml:mtext><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd14"><label>(14)</label><mml:math id="M38" altimg="si36.svg" alttext="Equation 14."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Acc</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="fd15"><label>(15)</label><mml:math id="M39" altimg="si37.svg" alttext="Equation 15."><mml:mrow><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>MCC</mml:mtext><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>−</mml:mo><mml:mtext>FP</mml:mtext><mml:mo linebreak="badbreak">×</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">×</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext>TN</mml:mtext><mml:mo linebreak="badbreak">+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p id="p0140">Sn was used as a measure of positive accuracy, i.e., an index of accuracy in identifying SNO sites. Sp was used as a measure of negative accuracy, i.e., an index of accuracy in identifying non-SNO sites. Acc represented the proportion of correctly classified samples to the total number of samples, and MCC was considered a balanced measure [<xref rid="bib28" ref-type="bibr">28</xref>]. Here TP, TN, FP, and FN denote true positive, true negative, false positive, and false negative, respectively. In addition, we used the receiver operating characteristic (ROC) area under the ROC curve (AUC) as a measure of model goodness [<xref rid="bib29" ref-type="bibr">29</xref>]. It is obvious that the larger AUC value indicates, the higher model performance.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Results and discussion</title>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Optimization of important hyperparameters</title>
      <p id="p0145">In order to find the optimal network configuration for the model SNO-DCA, which leads to the training of a better site predictor. A few important hyperparameters in the model SNO-DCA, including the number of convolutional kernels in the first CNN layer, the number of dense blocks and the number of convolutional layers in the dense blocks, were determined by ten-fold cross-validation.</p>
      <p id="p0150">Firstly, the number of convolution kernels of the first layer CNN and the layers of the dense blocks were preset to 96 and 4. Adjusted the number of dense blocks, the best performance of the model is achieved when the number of dense blocks were 5 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>A). Secondly, after fixing the number of dense blocks, we adjusted the number of convolution layers in the dense blocks, and determined that the best result is achieved when the number of layers were 4 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>B). Lastly, we fixed the two parameters and then adjusted the number of convolutional kernels in the first layer of CNN to reach the best performance when the number of filters were 96 (<xref rid="fig4" ref-type="fig">Fig. 4</xref>C).<fig id="fig4"><label>Fig. 4</label><caption><p>The results of parameter selection of the model SNO-DCA (A) Selection of the number of dense blocks. (B) Selection of the number of layers. (C) Selection of the number of filters.</p></caption><alt-text id="alttext0025">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>SNO-DCA ablation experiments</title>
      <p id="p0155">To verify the necessity of each component in SNO-DCA, we designed an ablation experiment by ten-fold validation using the training dataset. The experiments included the use of densely connected convolutional networks and the introduction of the ECA-Net layer in the model. The benchmark model adopted the concatenation of the original protein sequence information as input and extracted features by traditional CNNs.</p>
      <p id="p0160">The experimental results are shown in <xref rid="tbl2" ref-type="table">Table 2</xref>, different columns represent different model architectures. The proposed model performed best overall when densely connected convolutional networks and ECA-Net were used. First, to further verify the advantages of dense convolutional networks, we replaced conventional CNNs with DenseNets. Then, ECA-Net was added after each dense convolutional network. The results of ten-fold cross-validation showed that all metrics are significantly improved and dense convolutional networks have significant advantages over conventional CNNs. In addition, ECA-Net also could raise the model performance.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>The 10-flod cross-validation performance with different model structure.</p></caption><alt-text id="alttext0055">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th>CNN</th><th>✓</th><th>✓</th><th>✓</th></tr></thead><tbody><tr><td align="left">+ DenseNets</td><td/><td align="left">✓</td><td align="left">✓</td></tr><tr><td align="left">+ECA-Net</td><td/><td/><td align="left">✓</td></tr><tr><td align="left">SN</td><td align="left">0.32</td><td align="left">0.80</td><td align="left">0.80</td></tr><tr><td align="left">SP</td><td align="left">0.80</td><td align="left">0.59</td><td align="left">0.62</td></tr><tr><td align="left">ACC</td><td align="left">0.40</td><td align="left">0.63</td><td align="left">0.65</td></tr><tr><td align="left">MCC</td><td align="left">0.09</td><td align="left">0.30</td><td align="left">0.31</td></tr><tr><td align="left">AUC</td><td align="left">0.60</td><td align="left">0.76</td><td align="left">0.77</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Cross-validation performance</title>
      <p id="p0165">We used ten-fold cross-validation to test the results. For an accurate comparison, we used the same training set and test set as the DeepNitro model. The comparison results of all metrics are shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. The AUC of the SNO-DCA model is 0.77, and the ROC curve is shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Comparison with other methods on the training data set.</p></caption><alt-text id="alttext0060">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Predictors</th><th>Sn</th><th>Sp</th><th>Acc</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td align="left">DeepNitro</td><td align="left">NA</td><td align="left">NA</td><td align="left">NA</td><td align="left">NA</td><td align="left">0.71</td></tr><tr><td align="left">SNO-DCA</td><td align="left"><bold>0.80</bold></td><td align="left">0.62</td><td align="left"><bold>0.65</bold></td><td align="left"><bold>0.31</bold></td><td align="left"><bold>0.77</bold></td></tr></tbody></table></table-wrap><fig id="fig5"><label>Fig. 5</label><caption><p>Ten-fold cross-validated ROC curve.</p></caption><alt-text id="alttext0030">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Comparison of independent datasets with existing models</title>
      <p id="p0170">To evaluate the performance of SNO-DCA and ensure the good generalization ability of the model, we have selected the following methods for predicting SNO sites for comparison with our model, taking into account the fact that some models use different training sets and do not provide independent prediction tools. DeepNitro is based on an eight-layer neural network, PreSNO is a linear regression model based on SVM and RF, and RecSNO uses protein sequences as input and is a predictor based on embedded layers and BILSTM. The results are shown in <xref rid="tbl4" ref-type="table">Table 4</xref>. The prediction results of the deep learning predictor SNO-DCA constructed in this paper are Sn = 0.82; Sp = 0.69; Acc = 0.70; MCC = 0.32; AUC = 0.82. The ROC curves are shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Comparison with other methods on the same test data set.</p></caption><alt-text id="alttext0065">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th>Predictors</th><th>Sn</th><th>Sp</th><th>Acc</th><th>MCC</th><th>AUC</th></tr></thead><tbody><tr><td align="left">GPS-SNO</td><td align="left">0.28</td><td align="left">0.74</td><td align="left">0.69</td><td align="left">0.01</td><td align="left">0.52</td></tr><tr><td align="left">ISNOPseAAC</td><td align="left">0.29</td><td align="left">0.76</td><td align="left">0.71</td><td align="left">0.03</td><td align="left">NA</td></tr><tr><td align="left">SNOSite</td><td align="left">0.67</td><td align="left">0.45</td><td align="left">0.47</td><td align="left">0.07</td><td align="left">NA</td></tr><tr><td align="left">DeepNitro</td><td align="left">0.58</td><td align="left">0.76</td><td align="left">0.73</td><td align="left">0.22</td><td align="left">0.73</td></tr><tr><td align="left">PreSNO</td><td align="left">0.60</td><td align="left">0.77</td><td align="left">0.75</td><td align="left">0.25</td><td align="left">0.76</td></tr><tr><td align="left">RecSNO</td><td align="left">0.77</td><td align="left">0.71</td><td align="left">0.71</td><td align="left">0.30</td><td align="left">0.80</td></tr><tr><td align="left">SNO-DCA</td><td align="left"><bold>0.82</bold></td><td align="left">0.69</td><td align="left">0.70</td><td align="left"><bold>0.32</bold></td><td align="left"><bold>0.81</bold></td></tr></tbody></table></table-wrap><fig id="fig6"><label>Fig. 6</label><caption><p>ROC curve of the independent test set.</p></caption><alt-text id="alttext0035">Fig. 6</alt-text><graphic xlink:href="gr6"/></fig></p>
      <p id="p0175">In <xref rid="tbl4" ref-type="table">Table 4</xref>, we can find that SNO-DCA has the highest Sn, MCC, and AUC; however, it has relatively low Sp due to the antagonistic formula of Sn and Sp. Similar to the finding of Ref [<xref rid="bib30" ref-type="bibr">30</xref>], a significant increase in Sn usually lead to a decrease in Sp and thus Acc. Both PreSNO and RecSNO were constructed on a balanced dataset after downsampling the original dataset trained, in which the negative samples of the dataset are not sufficient. However, models trained on unbalanced training sets are more relevant. In the biological field, the real datasets are often unbalanced and it is especially important to identify as many SNO sites as possible, i.e., the values of Sn and MCC are more important than the other parameters. In contrast, the SNO-DCA predictor constructed in this paper solves the problem of unbalanced data by class weights and does not require the construction of balanced data sets with high Sn and MCC by undersampling, which effectively overcomes the disadvantage of the inadequacy for negative samples brought by the training under balanced data, avoids the lack of information to some extent. Therefore, our SNO-DCA model is more relevant and practical for predicting SNO sites.</p>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Server online prediction website</title>
      <p id="p0180">To facilitate the utilization of our prediction model and provide experimental help to scientific researchers, we have constructed a user-friendly web server website <ext-link ext-link-type="uri" xlink:href="https://sno.cangmang.xyz/SNO-DCA/" id="intref0020">https://sno.cangmang.xyz/SNO-DCA/</ext-link>, whose homepage is shown in <xref rid="fig7" ref-type="fig">Fig. 7</xref>.<list list-type="simple" id="olist0010"><list-item id="o0010"><label>Step1</label><p id="p0185">Enter the URL in your browser and click Search to open the SNO-DCA sever homepage as shown in Figure. Click the Help button in the upper right corner or the More info … Button in the middle of Home to jump to the User's Guide, where one can view the background and a brief introduction about the SNO-DCA prediction model, and learn about the model structure and advantages; the user can see the sequence input format, and click example to jump to the Example interface, which shows the correct format and the incorrect format of the input sequences.</p></list-item><list-item id="o0015"><label>Step2</label><p id="p0190">Visitors can enter a single protein sequence for query in the center box of the homepage and click the Submit button to make a prediction, and the predicted result will be highlighted in red, indicating SNO sites. As shown in <xref rid="fig8" ref-type="fig">Fig. 8</xref>.</p></list-item><list-item id="o0020"><label>Step3</label><p id="p0195">You can also pass the file in FASTA format to the bottom left box of the homepage, enter your username and email address in the bottom right box, and click Submit for batch prediction, and the final prediction will be emailed to the recipient.</p></list-item></list><fig id="fig7"><label>Fig. 7</label><caption><p>Screenshot of the homepage of the web server SNO-DCA.</p></caption><alt-text id="alttext0040">Fig. 7</alt-text><graphic xlink:href="gr7"/></fig><fig id="fig8"><label>Fig. 8</label><caption><p>Screenshot of the predicted results of the web server SNO-DCA.</p></caption><alt-text id="alttext0045">Fig. 8</alt-text><graphic xlink:href="gr8"/></fig></p>
      <p id="p0200">This platform simply encodes the original sequence of proteins to reduce manual feature selection intervention. A dense convolutional network is introduced to extract the high-level features, and an efficient channel attention mechanism is added to weight different features and learn important features. The efficient channel attention mechanism is added to weight different features to learn important features. Finally, the information is fused by the fully connected layer and inputted into the softmax layer for site prediction, and the SNO sites are identified by the prediction probability. The platform gives full effect to the deep learning models to explore the potential information of SNO sites, extract important features and improve the accuracy of SNO sites prediction.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Conclusions</title>
    <p id="p0205">In this study, we proposed SNO-DCA, a deep learning model for SNO sites prediction based on DenseNets and an efficient channel attention module under unbalanced data. The model performs a simple feature representation of protein sequence information, extracts high-level features by dense convolutional blocks, and adds an efficient attention module to further enhance the feature information. The results of both ten-fold cross-validation and independent tests show that SNO-DCA is a powerful prediction tool for identifying SNO sites. We developed a web online server to provide convenience to researchers. Both the training and test sets in this study are non-equilibrium datasets, which meet the practical prediction conditions. Another advantage in this study is that only protein sequences are simply encoded, which greatly reduces the intervention problem of manual feature selection. Meanwhile, the developed deep learning algorithm architecture is used to dig the potential information of the sequences in depth, which can give full play to the deep learning model.</p>
    <p id="p0210">In the follow-up work for this study, we will try not to encode the protein sequences in advance, but to embed the original sequences into the deep learning model to achieve end-to-end prediction, directly mining the hidden information of the samples and improving the prediction accuracy. In addition, we will also explore more possibilities of deep learning algorithms for protein locus prediction, such as capsule network for prediction of small sample data, improved long and short-term memory network combined with improved CNN, etc. At present, the problem of data imbalance is still a major difficulty in the field of bioinformatics. Though the traditional methods of balancing data may have some certain value in the data information processing, we should transform our focus into the imbalanced data with employing the improved/enhanced deep learning algorithm architecture to adapt to the actual protein prediction circumstance.</p>
  </sec>
  <sec sec-type="data-availability" id="sec5">
    <title>Data availability statement</title>
    <p id="p0215">Data will be made available on request. SNO-DCA model and datasets can be available at <ext-link ext-link-type="uri" xlink:href="https://github.com/peanono/SNO-DCA" id="intref0025">https://github.com/peanono/SNO-DCA</ext-link>.</p>
  </sec>
  <sec id="sec6">
    <title>Additional information</title>
    <p id="p0220">No additional information is available for this paper.</p>
  </sec>
  <sec id="sec7">
    <title>CRediT authorship contribution statement</title>
    <p id="p0225"><bold>Jianhua Jia:</bold> Writing – review &amp; editing. <bold>Peinuo Lv:</bold> Writing – original draft. <bold>Xin Wei:</bold> Data curation. <bold>Wangren Qiu:</bold> Validation.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of competing interest</title>
    <p id="p0230">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>References</title>
    <ref id="bib1">
      <label>1</label>
      <mixed-citation publication-type="other" id="sref1">Matthew, W., Foster, et al. Protein S-nitrosylation in health and disease: a current perspective - ScienceDirect [J]. Trends Mol. Med., 15(9): 391-404.</mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>B</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Regulation of cellular function by protein sulfhydryl nitrosylation [J]</article-title>
        <source>Chin. J. Pathophysiol.</source>
        <volume>27</volume>
        <issue>11</issue>
        <year>2011</year>
        <fpage>2237</fpage>
        <lpage>2240</lpage>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Turko</surname>
            <given-names>I.V.</given-names>
          </name>
          <name>
            <surname>Murad</surname>
            <given-names>F.J.P.R.</given-names>
          </name>
        </person-group>
        <article-title>Protein nitration in cardiovascular diseases [J]</article-title>
        <source>Pharmacol. Rev.</source>
        <volume>54</volume>
        <issue>4</issue>
        <year>2002</year>
        <fpage>619</fpage>
        <pub-id pub-id-type="pmid">12429871</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Protein S-nitrosylation and cancer [J]</article-title>
        <source>Cancer Lett.</source>
        <volume>320</volume>
        <issue>2</issue>
        <year>2012</year>
        <fpage>123</fpage>
        <lpage>129</lpage>
        <pub-id pub-id-type="pmid">22425962</pub-id>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Piroddi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Palmese</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pilolli</surname>
            <given-names>F.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Plasma nitroproteome of kidney disease patients [J]</article-title>
        <source>Amino Acids</source>
        <volume>40</volume>
        <issue>2</issue>
        <year>2011</year>
        <fpage>653</fpage>
        <lpage>657</lpage>
        <pub-id pub-id-type="pmid">20676907</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <article-title>Heneka T S W S B-a S S G K B T. Quantitative proteomics of synaptosome S-nitrosylation in Alzheimer's disease [J]</article-title>
        <source>J. Neurochem.</source>
        <volume>152</volume>
        <issue>6</issue>
        <year>2020</year>
        <fpage>710</fpage>
        <lpage>726</lpage>
        <pub-id pub-id-type="pmid">31520481</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Jaffrey</surname>
            <given-names>S.R.</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>S.H.</given-names>
          </name>
        </person-group>
        <article-title>The biotin switch method for the detection of S-nitrosylated proteins [J]</article-title>
        <source>Sci. STKE : Signal Transduct. Knowl. Environ.</source>
        <volume>2001</volume>
        <issue>86</issue>
        <year>2001</year>
        <fpage>pl1</fpage>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Lindermayr</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Saalbach</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Durner</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Proteomic identification of S-nitrosylated proteins in Arabidopsis [J]</article-title>
        <source>Plant Physiol.</source>
        <volume>137</volume>
        <issue>3</issue>
        <year>2005</year>
        <fpage>921</fpage>
        <lpage>930</lpage>
        <pub-id pub-id-type="pmid">15734904</pub-id>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Redox regulatory mechanism of transnitrosylation by thioredoxin [J]</article-title>
        <source>Mol. Cell. Proteomics : MCP</source>
        <volume>9</volume>
        <issue>10</issue>
        <year>2010</year>
        <fpage>2262</fpage>
        <lpage>2275</lpage>
        <pub-id pub-id-type="pmid">20660346</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Hao</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Derakhshan</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SNOSID, a proteomic method for identification of cysteine S-nitrosylation sites in complex protein mixtures</article-title>
        <source>Proc. Natl. Acad. Sci. U.S.A.</source>
        <volume>103</volume>
        <issue>4</issue>
        <year>2006</year>
        <fpage>1012</fpage>
        <lpage>1017</lpage>
        <pub-id pub-id-type="pmid">16418269</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>GPS-SNO: computational prediction of protein S-nitrosylation sites with a modified GPS algorithm [J]</article-title>
        <source>PLoS One</source>
        <volume>5</volume>
        <issue>6</issue>
        <year>2010</year>
        <object-id pub-id-type="publisher-id">e11290</object-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>T.Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.J.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>T.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SNOSite: exploiting maximal dependence decomposition to identify cysteine S-nitrosylation with substrate site specificity [J]</article-title>
        <source>PLoS One</source>
        <volume>6</volume>
        <issue>7</issue>
        <year>2011</year>
        <object-id pub-id-type="publisher-id">e21849</object-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>L.Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>iSNO-PseAAC: predict cysteine S-nitrosylation sites in proteins by incorporating position specific amino acid propensity into pseudo amino acid composition [J]</article-title>
        <source>PLoS One</source>
        <volume>8</volume>
        <issue>2</issue>
        <year>2013</year>
        <object-id pub-id-type="publisher-id">e55844</object-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepNitro: prediction of protein nitration and nitrosylation sites by deep learning [J]</article-title>
        <source>Dev. Reprod. Biol.</source>
        <volume>16</volume>
        <issue>4</issue>
        <year>2018</year>
        <fpage>294</fpage>
        <lpage>306</lpage>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Hasan</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Manavalan</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Khatun</surname>
            <given-names>M.S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prediction of S-nitrosylation sites by integrating support vector machines and random forest [J]</article-title>
        <source>Molecular omics</source>
        <volume>15</volume>
        <issue>6</issue>
        <year>2019</year>
        <fpage>451</fpage>
        <lpage>458</lpage>
        <pub-id pub-id-type="pmid">31710075</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Siraj</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chantsalnyam</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Tayara</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>RecSNO: prediction of protein S-nitrosylation sites using a recurrent neural network [J]</article-title>
        <source>IEEE Access</source>
        <volume>9</volume>
        <year>2021</year>
        <fpage>6674</fpage>
        <lpage>6682</lpage>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="book" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Laurens</surname>
            <given-names>V.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Densely Connected Convolutional Networks</part-title>
        <year>2016</year>
        <publisher-name>proceedings of the IEEE Computer Society, F</publisher-name>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="book" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>P.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>ECA-net: efficient Channel Attention for deep convolutional neural networks</part-title>
        <source>Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), F</source>
        <year>2020</year>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>LiminNiu</surname>
            <given-names>Beifang</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data [J]</article-title>
        <source>Bioinformatics</source>
        <volume>28</volume>
        <issue>23</issue>
        <year>2012</year>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="journal" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Capsule network for protein post-translational modification site prediction [J]</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <issue>14</issue>
        <year>2018</year>
        <fpage>2386</fpage>
        <lpage>2394</lpage>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="book" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Deep Residual Learning for Image Recognition; Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), F 27-30 June 2016</part-title>
        <year>2016</year>
        <comment>[C]</comment>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <mixed-citation publication-type="other" id="sref22">Wei L, Hu J, Li F, et al. Comparative analysis and prediction of quorum-sensing peptides using feature representation learning and machine learning algorithms. LID - 10.1093/bib/bby107 [doi] [J]. (1477-4054 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="journal" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improved prediction of protein–protein interactions using novel negative samples, features, and an ensemble classifier [J]</article-title>
        <source>Artif. Intell. Med.</source>
        <volume>83</volume>
        <year>2017</year>
        <fpage>67</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="pmid">28320624</pub-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <mixed-citation publication-type="other" id="sref24">Wei L, Zhou C, Chen H, et al. ACPred-FL: a sequence-based predictor using effective feature representation to improve the prediction of anti-cancer peptides [J]. (1367-4811 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <mixed-citation publication-type="other" id="sref25">Su R, Liu X, Xiao G, et al. Meta-GDBP: a high-level stacked regression model to improve anticancer drug response prediction [J]. (1477-4054 (Electronic)).</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Malik</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Computational prediction and interpretation of cell-specific replication origin sites from multiple eukaryotes by exploiting stacking framework [J]</article-title>
        <source>Briefings Bioinf.</source>
        <volume>22</volume>
        <issue>4</issue>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Local-DPP: an improved DNA-binding protein prediction method by exploring local evolutionary information [J]</article-title>
        <source>Inf. Sci.</source>
        <volume>384</volume>
        <year>2017</year>
        <fpage>135</fpage>
        <lpage>144</lpage>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Eisenberg</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Marcotte</surname>
            <given-names>E.M.</given-names>
          </name>
          <name>
            <surname>Xenarios</surname>
            <given-names>I.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Protein function in the post-genomic era [J]</article-title>
        <source>Nature</source>
        <volume>405</volume>
        <issue>6788</issue>
        <year>2000</year>
        <fpage>823</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="pmid">10866208</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="journal" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Jurman</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation [J]</article-title>
        <source>BMC Genom.</source>
        <volume>21</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>6</fpage>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="journal" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>E.A.</given-names>
          </name>
        </person-group>
        <article-title>Learning from imbalanced data [J]</article-title>
        <source>IEEE Trans. Knowl. Data Eng.</source>
        <volume>21</volume>
        <issue>9</issue>
        <year>2009</year>
        <fpage>1263</fpage>
        <lpage>1284</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec1">
    <title>Abbreviations</title>
    <p id="p0240">
      <def-list>
        <def-item>
          <term id="d0010">SNO</term>
          <def>
            <p id="p0245"><italic>S</italic>-nitrosylation</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0015">BSA</term>
          <def>
            <p id="p0250">biotin switch assay</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0020">ICAT</term>
          <def>
            <p id="p0255">isotope-coded affinity tag</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0025">SVM</term>
          <def>
            <p id="p0260">Support Vector Machine</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0030">MDD</term>
          <def>
            <p id="p0265">maximal dependence decomposition</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0035">PSAAP</term>
          <def>
            <p id="p0270">position-specific amino acid Propensity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0040">CRF</term>
          <def>
            <p id="p0275">conditional random field</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0045">RF</term>
          <def>
            <p id="p0280">random forest</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0050">NLP</term>
          <def>
            <p id="p0285">natural language processing</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0055">DenseNets</term>
          <def>
            <p id="p0290">densely connected convolutional networks</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0060">ECA-Net</term>
          <def>
            <p id="p0295">Efficient Channel Attention Module</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0065">CNN</term>
          <def>
            <p id="p0300">Convolutional Neural Network</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0070">ResNet</term>
          <def>
            <p id="p0305">Residual Networks</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0075">GAP</term>
          <def>
            <p id="p0310">global average pooling</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0080">Sn</term>
          <def>
            <p id="p0315">sensitivity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0085">Sp</term>
          <def>
            <p id="p0320">specificity</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0090">Acc</term>
          <def>
            <p id="p0325">accuracy</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0095">MCC</term>
          <def>
            <p id="p0330">Mathews correlation coefficient</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0100">ROC</term>
          <def>
            <p id="p0335">receiver operating characteristic</p>
          </def>
        </def-item>
        <def-item>
          <term id="d0105">AUC</term>
          <def>
            <p id="p0340">area under the ROC curve</p>
          </def>
        </def-item>
      </def-list>
    </p>
  </sec>
  <ack id="ack0010">
    <title>Acknowledgments</title>
    <p id="p0235">This work was partially supported by the <funding-source id="gs4">National Nature Science Foundation of China</funding-source> (Nos. 61761023 and 62162032), the <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100004479</institution-id><institution>Natural Science Foundation of Jiangxi Province</institution></institution-wrap></funding-source>, China (Nos. 20202BABL202004), the Scientific Research Plan of the <funding-source id="gs3"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100009102</institution-id><institution>Department of Education of Jiangxi Province</institution></institution-wrap></funding-source> (GJJ212419 and GJJ2201004). These funders had no role in the study design, data collection and analysis, decision to publish or preparation of manuscript.</p>
  </ack>
</back>
