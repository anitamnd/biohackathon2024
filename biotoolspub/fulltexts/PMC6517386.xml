<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6517386</article-id>
    <article-id pub-id-type="publisher-id">43708</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-019-43708-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DEEPred: Automated Protein Function Prediction with Multi-task Feed-forward Deep Neural Networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6717-4767</contrib-id>
        <name>
          <surname>Sureyya Rifaioglu</surname>
          <given-names>Ahmet</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <name>
          <surname>Doğan</surname>
          <given-names>Tunca</given-names>
        </name>
        <address>
          <email>tdogan@ebi.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jesus Martin</surname>
          <given-names>Maria</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2408-6606</contrib-id>
        <name>
          <surname>Cetin-Atalay</surname>
          <given-names>Rengul</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Atalay</surname>
          <given-names>Volkan</given-names>
        </name>
        <address>
          <email>vatalay@metu.edu.tr</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 1881 7391</institution-id><institution-id institution-id-type="GRID">grid.6935.9</institution-id><institution>Department of Computer Engineering, METU, </institution></institution-wrap>Ankara, 06800 Turkey </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 5896 2288</institution-id><institution-id institution-id-type="GRID">grid.503005.3</institution-id><institution>Department of Computer Engineering, </institution><institution>İskenderun Technical University, </institution></institution-wrap>Hatay, 31200 Turkey </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9709 7726</institution-id><institution-id institution-id-type="GRID">grid.225360.0</institution-id><institution>European Molecular Biology Laboratory, </institution><institution>European Bioinformatics Institute (EMBL-EBI), </institution></institution-wrap>Hinxton, Cambridge CB10 1SD UK </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 1881 7391</institution-id><institution-id institution-id-type="GRID">grid.6935.9</institution-id><institution>KanSiL, Department of Health Informatics, </institution><institution>Graduate School of Informatics, METU, </institution></institution-wrap>Ankara, 06800 Turkey </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>7344</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>8</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>27</day>
        <month>4</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Automated protein function prediction is critical for the annotation of uncharacterized protein sequences, where accurate prediction methods are still required. Recently, deep learning based methods have outperformed conventional algorithms in computer vision and natural language processing due to the prevention of overfitting and efficient training. Here, we propose DEEPred, a hierarchical stack of multi-task feed-forward deep neural networks, as a solution to Gene Ontology (GO) based protein function prediction. DEEPred was optimized through rigorous hyper-parameter tests, and benchmarked using three types of protein descriptors, training datasets with varying sizes and GO terms form different levels. Furthermore, in order to explore how training with larger but potentially noisy data would change the performance, electronically made GO annotations were also included in the training process. The overall predictive performance of DEEPred was assessed using CAFA2 and CAFA3 challenge datasets, in comparison with the state-of-the-art protein function prediction methods. Finally, we evaluated selected novel annotations produced by DEEPred with a literature-based case study considering the ‘biofilm formation process’ in <italic>Pseudomonas aeruginosa</italic>. This study reports that deep learning algorithms have significant potential in protein function prediction; particularly when the source data is large. The neural network architecture of DEEPred can also be applied to the prediction of the other types of ontological associations. The source code and all datasets used in this study are available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/cansyl/DEEPred">https://github.com/cansyl/DEEPred</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Data mining</kwd>
      <kwd>Gene ontology</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Protein function predictions</kwd>
      <kwd>Sequence annotation</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par2">Functional annotation of proteins is crucial for understanding the cellular mechanisms, identifying disease-causing functional changes in genes/proteins, and for discovering novel tools for disease prevention, diagnosis, and treatment. Traditionally, gene/protein functions are first identified by <italic>in vitro</italic> and <italic>in vivo</italic> experiments and recorded in biological databases via literature-based curation. However, wet-lab experiments and manual curation efforts are cumbersome and time consuming. Thus, they are unable to resolve the knowledge gap that is being produced due to the continuous growth of biological sequence data<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Therefore, accurate computational methods have been sought to automatically annotate functions of proteins.</p>
    <p id="Par3">The Gene Ontology (GO) provides a controlled vocabulary to classify the attributes of proteins based upon representative terms, referred to as “GO terms”<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. The GO system divides protein attributes into three main categories: molecular function (MF), biological process (BP) and cellular component (CC). Each GO term represents a unique functional attribute and all terms are associated to each other in a directed acyclic graph (DAG) structure based on inheritance relationships. Several GO term-based protein function prediction methods have been proposed in the last decade to automatically annotate protein sequences using machine learning and statistical analysis techniques<sup><xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR8">8</xref></sup>. Considering the prediction performances of the current methods, it can be stated that there is still room for significant improvement in this area. Critical Assessment of Protein Function Annotation (CAFA) is an initiative, whose aim is the large-scale evaluation of protein function prediction methods, and the results of the first two CAFA challenges showed that protein function prediction is still a challenging area<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>.</p>
    <p id="Par4">Several machine learning techniques have been employed for protein function prediction, such as the artificial neural networks (ANNs)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. Deep Neural Network (DNN) algorithms, a sub-group of ANNs, have multiple hidden layers. DNNs take low level features as input and build more advanced features at each subsequent layer. DNN-based methods have already become industry standards in the fields of computer vision and natural language processing<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR16">16</xref></sup>. Recent improvements in affordable computational power have allowed the scientific community to apply DNN-based methods on numerous research fields including biomedical data analysis; where, DNN algorithms have been shown to outperform the traditional predictive methods in bioinformatics and cheminformatics<sup><xref ref-type="bibr" rid="CR17">17</xref>–<xref ref-type="bibr" rid="CR21">21</xref></sup>. DNNs are divided into two groups in terms of the task modelling approach. Multi-task DNNs are designed for classifying the input instances into multiple pre-defined classes/tasks<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, as opposed to single-task DNNs, where the aim is to make a binary prediction. In terms of the model architecture and properties, DNNs are classified into multiple groups, the most popular architectures are feed-forward DNN (i.e., multi-layered perceptron), recurrent neural network (RNN), restricted Boltzmann machine (RBM) and deep belief network (DBN), auto encoder deep neural networks, convolutional neural network (CNN), and graph convolutional network (GCN)<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>.</p>
    <p id="Par5">Investigative studies showed that, applications of multi-task DNNs provided a significant performance increase in ligand-based drug discovery. Ligand-based drug discovery can be considered similar to the problem of protein function prediction<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. In protein function prediction, the associations between the ontology-based function defining terms (e.g., GO terms) and proteins are identified, where a protein may have more than one functional association. Therefore, protein function prediction is a multi-label learning problem and thus can be solved using multi-task deep neural networks, similar to the applications in drug discovery<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Multi-task DNN algorithms inherently extract the relationships between multiple classes by building complex features from the raw input data at each layer in a hierarchical manner. Additionally, shared hidden units among different classes enhance the prediction results of the classes that have a low number of training samples, which often has a positive impact on the predictive performance.</p>
    <p id="Par6">To the best of our knowledge, deep learning algorithms have not been thoroughly investigated in terms of generating practical large-scale protein function prediction pipelines. However, there have been a some studies mostly confined to small sets of proteins and functional classes. In these studies, DNNs were applied to predict protein functions using different types of protein features such as amino acid sequences<sup><xref ref-type="bibr" rid="CR26">26</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>, 3-D structural properties<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, protein-protein interaction networks<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup> or other molecular and functional aspects<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR32">32</xref>–<xref ref-type="bibr" rid="CR34">34</xref></sup>, and various types of DNN architectures such as single or multi-task feed-forward DNNs<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, recurrent neural networks<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>, deep autoencoder neural networks<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>, deep restricted Boltzmann machines<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> or convolutional neural networks<sup><xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR30">30</xref></sup>. We have discussed and compared each study mentioned above in the Supplementary Material <xref rid="MOESM1" ref-type="media">1</xref> Document, Section <xref rid="MOESM1" ref-type="media">1</xref>.</p>
    <p id="Par7">One of the most critical obstacles against developing a practical DNN-based predictive tool is the computationally intensive training processes that limits the size of input data and the number of functional categories that can be included in the system. Due to this reason, previous studies mostly focused on a small number of protein families or GO terms. Whilst, methods covering large sets of GO terms suffered from long training duration and reduced predictive performance issues. Therefore, there is a need for new predictive approaches not only with high performance, but also with real-world usability, to be able to support <italic>in vitro</italic> studies in protein function identification.</p>
    <p id="Par8">In this study, we propose a novel multi-task hierarchical deep learning method, DEEPred, for the prediction of GO term associations to protein sequence records in biological data resources such as the UniProtKB, as well as for poorly and uncharacterized open reading frames. We also provide a comprehensive investigation on DNN-based predictive model characteristics when applied on protein sequence and ontology data. Our initial preprint work on this topic was one of the first applications of deep neural networks for sequence based protein function prediction<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. This study contributes to the existing literature in terms of designing a large-scale deep learning based predictive system using a stack of 1,101 multi-task feed-forward DNNs, capable of predicting thousands of Gene Ontology based functional definitions. Additionally, the prediction of GO terms with very low number of training instances, which is a major problem in the field of automated protein function prediction, has been addressed by proposing a practical data augmentation solution by incorporating previously produced automated functional predictions into the system training.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <p id="Par9">The technical details of DEEPred are given in the Methods section. The results of several performance and validation analyses are provided below.</p>
    <sec id="Sec3">
      <title>Input feature type performance analysis</title>
      <p id="Par10">In predictive modeling, input instances/samples are quantized as feature vectors, and these feature vectors are required to reflect the intrinsic properties of the samples they represent, which should also be correlated with their known labels (i.e., GO term associations in our case). For this reason, finding the best representative feature type is important for any machine learning application. In this analysis, our aim was to investigate the best representative feature type for proteins, to be incorporated in DEEPred. For this purpose, we randomly selected three DEEPred DNN models that contain MF GO terms from different levels on the GO hierarchy, and trained each model using three different feature types (i.e., SPMap, pseudo amino acid composition - PAAC and the conjoint triad) as explained in the Methods section. The reason behind using MF GO term models was MF being the most clearly defined aspect of GO and also the easiest one to predict.</p>
      <p id="Par11">We measured the performance of the models using cross-validation, with 80% to 20% separation of the source training data, to observe the best representative feature. Table <xref rid="Tab1" ref-type="table">1</xref> shows the selected models together with the incorporated GO terms, their levels on the GO DAG, the number of annotated proteins and the performances for each feature type. The average performances (F1-score) were calculated as 0.63, 0.36 and 0.43 for SPMap, PAAC and the conjoint triad features, respectively. Since the predictive performance with SPMap feature was the best, we incorporated SPMap into the DEEPred system for the rest of the study.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Input feature type performance comparison results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Model &amp; GO level</th><th rowspan="2">GO term id</th><th rowspan="2">GO description</th><th rowspan="2"># of annotated proteins</th><th colspan="3">Predictive performance (F1-score)</th></tr><tr><th>SPMap</th><th>Pseudo-amino acid composition</th><th>Conjoint triad</th></tr></thead><tbody><tr><td rowspan="5">Model 1<break/>(GO level: 2)</td><td>GO:0036094</td><td>small molecule binding</td><td>1 847</td><td rowspan="5">0.49</td><td rowspan="5">0.29</td><td rowspan="5">0.23</td></tr><tr><td>GO:0003700</td><td>DNA binding transcription factor activity</td><td>1 652</td></tr><tr><td>GO:0004872</td><td>receptor activity</td><td>1 332</td></tr><tr><td>GO:0044877</td><td>protein-containing complex binding</td><td>1 296</td></tr><tr><td>GO:0097367</td><td>carbohydrate derivative binding</td><td>1 252</td></tr><tr><td rowspan="5">Model 2<break/>(GO level: 4)</td><td>GO:0004529</td><td>exodeoxyribonuclease activity</td><td>50</td><td rowspan="5">0.68</td><td rowspan="5">0.53</td><td rowspan="5">0.38</td></tr><tr><td>GO:0045309</td><td>protein phosphorylated amino acid binding</td><td>50</td></tr><tr><td>GO:0008395</td><td>steroid hydroxylase activity</td><td>49</td></tr><tr><td>GO:0008649</td><td>rRNA methyltransferase activity</td><td>49</td></tr><tr><td>GO:0015645</td><td>fatty acid ligase activity</td><td>49</td></tr><tr><td rowspan="5">Model 3<break/>(GO level: 7)</td><td>GO:0001012</td><td>RNA polymerase II regulatory region DNA binding</td><td>818</td><td rowspan="5">0.74</td><td rowspan="5">0.53</td><td rowspan="5">0.47</td></tr><tr><td>GO:0016887</td><td>ATPase activity</td><td>764</td></tr><tr><td>GO:0046873</td><td>metal ion transmembrane transporter activity</td><td>685</td></tr><tr><td>GO:0001159</td><td>core promoter proximal region DNA binding</td><td>504</td></tr><tr><td>GO:0015077</td><td>monovalent inorganic cation transmembrane transporter activity</td><td>480</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Effect of training dataset sizes on system performance</title>
      <p id="Par12">DNN models usually require a high number of training instances in order to produce accurate predictions. A significant disadvantage in this regard is that, large-scale biological training datasets are not generally available. One solution to this problem would be to discard GO terms with a low number of training instances from the system. In this case, the problem is that there are only a small number of GO terms available for prediction, most of which are shallow (i.e., generic and less informative terms). In order to investigate the effect of training dataset sizes on the predictive performance, we carried out a detailed analysis with multiple training and testing processes.</p>
      <p id="Par13">We constructed 6 different training datasets based on the annotated protein counts of different GO terms, as described in the Methods section. Table <xref rid="Tab2" ref-type="table">2</xref> summarizes the training dataset sizes and contents based upon the MF annotations. There are two vertical blocks in Table <xref rid="Tab2" ref-type="table">2</xref>, the first one belongs to “Annotations with only manual experimental evidence codes”; and the second block belongs to “Annotations with all evidence codes”. As observed from the first block, the number of available GO levels and GO terms decreases as the minimum compulsory number of annotations increases, since specific GO terms usually have less number of annotations. We trained the DEEPred system with each of these training datasets (i.e., annotations with only manual experimental evidence codes) and measured the predictive performance individually. We then compared them with each other to observe if there is a correlation. The average performance of the models for each training dataset is given in Table <xref rid="Tab3" ref-type="table">3</xref> and in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Each column in Table <xref rid="Tab3" ref-type="table">3</xref> corresponds to an average F1-score value of the GO terms belonging to a particular training dataset. Box plots in Fig. <xref rid="Fig1" ref-type="fig">1</xref> additionally display median and variance values. Here, it is evident that there is a strong correlation between the training sample size and performance. As expected, increasing the training dataset sizes elevated the classification performance for all GO categories. High variance values at low training dataset sizes indicates that these models are less stable. In this part of the study, we also carried out a GO level specific performance analysis. The details of this analysis can be found in the Supplementary Material <xref rid="MOESM1" ref-type="media">1</xref> Document, Section <xref rid="MOESM1" ref-type="media">7</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Statistics for the training datasets created by only using annotations with manual and experimental evidence codes and the training datasets created by using annotations with all evidence codes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="8">Training Dataset Statistics</th></tr><tr><th rowspan="2"/><th rowspan="2">Annotation Count</th><th colspan="3">Annotations with only manual experimental evidence codes</th><th colspan="3">Annotations with all evidence codes</th></tr><tr><th># of available levels</th><th># of GO terms</th><th># of annotations</th><th># of available levels</th><th># of GO terms</th><th># of annotations</th></tr></thead><tbody><tr><td rowspan="6">Molecular Function</td><td><bold>≥30</bold></td><td>9</td><td>838</td><td>281 125</td><td>11</td><td>2 776</td><td>6 451 530</td></tr><tr><td><bold>≥100</bold></td><td>9</td><td>605</td><td>272 235</td><td>10</td><td>1 598</td><td>6 386 105</td></tr><tr><td><bold>≥200</bold></td><td>9</td><td>395</td><td>257 404</td><td>10</td><td>1 174</td><td>6 326 109</td></tr><tr><td><bold>≥300</bold></td><td>8</td><td>226</td><td>233 476</td><td>9</td><td>942</td><td>6 269 643</td></tr><tr><td><bold>≥400</bold></td><td>8</td><td>165</td><td>218 591</td><td>9</td><td>809</td><td>6 223 762</td></tr><tr><td><bold>≥500</bold></td><td>8</td><td>142</td><td>210 790</td><td>9</td><td>698</td><td>6 173 867</td></tr><tr><td rowspan="6">Biological Process</td><td><bold>≥30</bold></td><td>10</td><td>4 215</td><td>1 433 220</td><td>12</td><td>8 404</td><td>16 537 812</td></tr><tr><td><bold>≥100</bold></td><td>10</td><td>2 993</td><td>1 386 588</td><td>12</td><td>4 768</td><td>16 335 538</td></tr><tr><td><bold>≥200</bold></td><td>9</td><td>1 782</td><td>1 302 577</td><td>11</td><td>3 299</td><td>16 129 271</td></tr><tr><td><bold>≥300</bold></td><td>9</td><td>1 059</td><td>1 199 604</td><td>10</td><td>2 631</td><td>15 965 583</td></tr><tr><td><bold>≥400</bold></td><td>8</td><td>743</td><td>1 123 037</td><td>9</td><td>2 233</td><td>15 828 012</td></tr><tr><td><bold>≥500</bold></td><td>8</td><td>603</td><td>1 075 353</td><td>9</td><td>1 978</td><td>15 713 431</td></tr><tr><td rowspan="6">Cellular Comp.</td><td><bold>≥30</bold></td><td>7</td><td>606</td><td>340 995</td><td>8</td><td>1 268</td><td>4 167 000</td></tr><tr><td><bold>≥100</bold></td><td>6</td><td>460</td><td>335 445</td><td>8</td><td>750</td><td>4 138 327</td></tr><tr><td><bold>≥200</bold></td><td>6</td><td>324</td><td>325 687</td><td>7</td><td>549</td><td>4 110 383</td></tr><tr><td><bold>≥300</bold></td><td>6</td><td>206</td><td>309 390</td><td>6</td><td>442</td><td>4 083 834</td></tr><tr><td><bold>≥400</bold></td><td>6</td><td>155</td><td>296 929</td><td>6</td><td>377</td><td>4 061 654</td></tr><tr><td><bold>≥ 500</bold></td><td>5</td><td>118</td><td>283 616</td><td>6</td><td>335</td><td>4 043 150</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>The average prediction performance (F1-score) for GO term models belonging to different training dataset size bins.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">GO categories</th><th colspan="6">Performance measures (F1-score) for different training dataset sizes</th></tr><tr><th>≥ 30</th><th>≥ 100</th><th>≥ 200</th><th>≥ 300</th><th>≥ 400</th><th>≥ 500</th></tr></thead><tbody><tr><td>Molecular Function</td><td>0.66</td><td>0.68</td><td>0.77</td><td>0.82</td><td>0.82</td><td>0.83</td></tr><tr><td>Biological Process</td><td>0.42</td><td>0.50</td><td>0.52</td><td>0.52</td><td>0.56</td><td>0.55</td></tr><tr><td>Cellular Component</td><td>0.50</td><td>0.59</td><td>0.64</td><td>0.63</td><td>0.64</td><td>0.65</td></tr></tbody></table><table-wrap-foot><p>In this analysis, the training was done using only the annotations with manual and experimental evidence codes.</p></table-wrap-foot></table-wrap><fig id="Fig1"><label>Figure 1</label><caption><p>Box plots for training dataset size specific performance evaluation. Each box plot represents variance, mean and standard deviations of F1-score values (vertical axis) for models with differently sized training datasets (horizontal axis), for each GO category. In this analysis, the training was done using only the annotations with manual and experimental evidence codes.</p></caption><graphic xlink:href="41598_2019_43708_Fig1_HTML" id="d29e1362"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Performance evaluation of training with electronic annotations</title>
      <p id="Par14">In DEEPred, the minimum required number of annotated proteins for each GO term (to be used in the training) is 30, which was considered as the minimum number required for statistical power. Due to this threshold, all GO terms with less than 30 annotated proteins were eliminated from the system. The eliminated terms corresponded to 25,257 out of 31,352 GO terms (31,352 is the total number of terms that have been annotated to at least one UniProtKB/Swiss-Prot protein entry with manual and experimental evidence codes), which can be considered as a significant loss. The same problem exists for most of the machine learning based methods in the automated protein function prediction domain. Moreover, the DNN models with a low or moderate number of training instances (i.e., between 30 to 100 for each incorporated GO term) displayed lower performance compared to the models with high number of training samples, as discussed above. In this section, we investigated a potential way to increase the statistical power of our models by enriching the training datasets.</p>
      <p id="Par15">In the UniProtKB/SwissProt database, only 1% of the total number of GO term annotations are tagged with manual and experimental evidence codes. The remaining of the GO term annotations are electronically made (evidence code: IEA), and these annotations are usually considered as less reliable due to potential errors (i.e., false positives). Normally, electronic annotations are not used for system training to avoid error propagation. In this test, we investigated the performance change when all annotations (including electronic ones) were included in the training procedure of DEEPred, and we discussed whether deep learning algorithms could handle noisy training data, as stated in the literature.</p>
      <p id="Par16">To perform this experiment, we first identified the MF GO terms whose annotation count was increased at least four times, when electronically made annotations were incorporated. We randomly selected 25 MF GO terms that satisfied this condition, and trained/evaluated the models with 80% to 20% training-validation separation, similar to our previous tests. The training dataset sizes and performance values for the “all-annotation-training” analysis are given in Table <xref rid="Tab4" ref-type="table">4</xref>. In this table, we divided GO terms into two main categories as “previously high performance models” and “previously low performance models” based on the performances when the system was trained only with annotations of manual experimental evidence codes. The results showed that adding electronic annotations to the training procedure increased the performances of selected “previously low performance models”. On the other hand, including electronic annotations in the training of “previously high performance models” decreased their performances in some of the cases. Overall, the performance change was positive.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance (F1-score) changes for the selected GO terms after the enrichment of training datasets with electronic annotations. In this analysis, the training was done using all of the available annotations, without any selection based on the evidence code.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>GO Term</th><th>GO Description</th><th>NoA* (ME*)</th><th>NoA (AE*)</th><th>F1-score perf. (ME)</th><th>F1-score perf. (AE)</th><th>Perf. Change</th></tr></thead><tbody><tr><td rowspan="12">Previously low performance models</td><td>GO:0070569</td><td>uridylyltransferase activity</td><td>35</td><td>970</td><td>0.58</td><td>0.88</td><td>0.30</td></tr><tr><td>GO:0019203</td><td>carbohydrate phosphatase activity</td><td>63</td><td>681</td><td>0.51</td><td>0.84</td><td>0.33</td></tr><tr><td>GO:0004197</td><td>cysteine-type endopeptidase activity</td><td>100</td><td>853</td><td>0.45</td><td>0.40</td><td>-0.05</td></tr><tr><td>GO:0005524</td><td>ATP binding</td><td>596</td><td>85 442</td><td>0.53</td><td>0.93</td><td>0.40</td></tr><tr><td>GO:0030554</td><td>adenyl nucleotide binding</td><td>689</td><td>86 319</td><td>0.51</td><td>0.90</td><td>0.39</td></tr><tr><td>GO:0035639</td><td>purine ribonucleoside triphosphate binding</td><td>834</td><td>98 924</td><td>0.43</td><td>0.80</td><td>0.37</td></tr><tr><td>GO:0032555</td><td>purine ribonucleotide binding</td><td>951</td><td>99 286</td><td>0.51</td><td>0.89</td><td>0.38</td></tr><tr><td>GO:0097367</td><td>carbohydrate derivative binding</td><td>1 395</td><td>10 413</td><td>0.41</td><td>0.73</td><td>0.32</td></tr><tr><td>GO:0000166</td><td>nucleotide binding</td><td>1 487</td><td>116 408</td><td>0,53</td><td>0.82</td><td>0.29</td></tr><tr><td>GO:0036094</td><td>small molecule binding</td><td>2 059</td><td>12 634</td><td>0.40</td><td>0.72</td><td>032</td></tr><tr><td>GO:0043169</td><td>cation binding</td><td>2 145</td><td>119 698</td><td>0.48</td><td>0.71</td><td>0.23</td></tr><tr><td>GO:0043167</td><td>ion binding</td><td>4 132</td><td>20 278</td><td>0.33</td><td>0.79</td><td>0.46</td></tr><tr><td rowspan="13">Previously high performance models</td><td>GO:0004784</td><td>superoxide dismutase activity</td><td>38</td><td>459</td><td>0.81</td><td>0.68</td><td>-0.13</td></tr><tr><td>GO:0004004</td><td>ATP-dependent RNA helicase activity</td><td>48</td><td>954</td><td>0.75</td><td>0.73</td><td>-0.02</td></tr><tr><td>GO:0005525</td><td>GTP binding</td><td>258</td><td>14 479</td><td>0.95</td><td>0.79</td><td>-0.16</td></tr><tr><td>GO:0032550</td><td>purine ribonucleoside binding</td><td>286</td><td>14 496</td><td>0.89</td><td>0.62</td><td>-0.27</td></tr><tr><td>GO:0001883</td><td>purine nucleoside binding</td><td>289</td><td>14 506</td><td>0.89</td><td>0.87</td><td>-0.02</td></tr><tr><td>GO:0032549</td><td>ribonucleoside binding</td><td>296</td><td>15 460</td><td>0.78</td><td>0.80</td><td>0.02</td></tr><tr><td>GO:0001882</td><td>nucleoside binding</td><td>304</td><td>15 508</td><td>0.92</td><td>0.79</td><td>-0.13</td></tr><tr><td>GO:0008270</td><td>zinc ion binding</td><td>520</td><td>11 385</td><td>0.83</td><td>0.71</td><td>-0.12</td></tr><tr><td>GO:0032559</td><td>adenyl ribonucleotide binding</td><td>673</td><td>85 691</td><td>0.80</td><td>0.80</td><td>0.00</td></tr><tr><td>GO:0017076</td><td>purine nucleotide binding</td><td>975</td><td>99 924</td><td>0.91</td><td>0.65</td><td>-0.26</td></tr><tr><td>GO:0032553</td><td>ribonucleotide binding</td><td>1 025</td><td>100 844</td><td>0.87</td><td>0.61</td><td>-0.26</td></tr><tr><td>GO:0046872</td><td>metal ion binding</td><td>1 985</td><td>118 577</td><td>0.81</td><td>0.80</td><td>-0.01</td></tr><tr><td>GO:0004784</td><td>superoxide dismutase activity</td><td>38</td><td>459</td><td>0.81</td><td>0.68</td><td>-0.13</td></tr><tr><td/><td><bold>Average</bold></td><td/><td><bold>861</bold></td><td><bold>47 658</bold></td><td><bold>0.68</bold></td><td><bold>0.77</bold></td><td><bold>0.10</bold></td></tr></tbody></table><table-wrap-foot><p>*NoA: Number of Annotations, ME: Manual-Experimental Evidence, AE: All Evidence.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec6">
      <title>Evaluation of the overall system performance</title>
      <p id="Par17">For the final system training of DEEPred, we used the training dataset of GO terms with at least 30 annotated proteins (with manual and experimental evidence codes); this dataset was also used to measure the overall performance of DEEPred.</p>
      <p id="Par18">The overall system performance was evaluated by considering all 1,101 predictive models. For testing, the hold-out dataset (see Methods) was employed, which was not used at all during system training. The test proteins were fed to all of the models and the system performance was calculated using precision, recall and F1-score (Table <xref rid="Tab5" ref-type="table">5</xref>). The average prediction performance (F1-score) was calculated as 0.62, 0.46 and 0.55 for MF, BP and CC categories, respectively, without using the hierarchical post-processing method (see Methods). When we employed the hierarchical post-processing procedure, which represents the finalized version of DEEPred, the overall average system performance (F1-score) was increased to 0.67, 0.51 and 0.58 for MF, BP and CC categories respectively.<table-wrap id="Tab5"><label>Table 5</label><caption><p>The average overall performance results of DEEPred, with and without the hierarchical post-processing procedure.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"/><th colspan="3">without Hierarchical Post-processing</th><th colspan="3">with Hierarchical Post-processing</th></tr><tr><th>F1-score</th><th>Precision</th><th>Recall</th><th>F1-score</th><th>Precision</th><th>Recall</th></tr></thead><tbody><tr><td>Molecular Function</td><td>0.62</td><td>0.52</td><td>0.77</td><td>0.67</td><td>0.61</td><td>0.74</td></tr><tr><td>Biological Process</td><td>0.46</td><td>0.36</td><td>0.65</td><td>0.51</td><td>0.44</td><td>0.62</td></tr><tr><td>Cellular Component</td><td>0.55</td><td>0.50</td><td>0.61</td><td>0.58</td><td>0.58</td><td>0.58</td></tr></tbody></table><table-wrap-foot><p>In this analysis, the training was done using the annotations with manual and experimental evidence codes.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec7">
      <title>Performance comparison against the state-of-the-art</title>
      <p id="Par19">Two separate analyses were carried out for the comparison against the state-of-the-art. In the first one, the CAFA2 challenge data was used. In CAFA2, GO term based function predictions of 126 methods from 56 research groups were evaluated. The performance results of best performing 10 methods are available in the CAFA2 report<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. In order to yield a fair comparison with the CAFA2 participating methods, DEEPred models were re-trained using the GO annotation data from September 2013. Afterwards, DEEPred was run on the CAFA2 benchmark protein sequences and the performance results (F-max) of 0.49, 0.26, and 0.43 were obtained for MF, BP, and CC categories respectively; considering the no-knowledge benchmark set in the full evaluation mode (the official CAFA2 performance calculation setting). For the CC category, DEEPred was among the 10 best performing methods.</p>
      <p id="Par20">Figure <xref rid="Fig2" ref-type="fig">2A–C</xref> displays the 10 top performing methods in CAFA2 in terms of F-max measure along with the results of DEEPred, for the selected taxonomies where DEEPred performed well. As observed from Fig. <xref rid="Fig2" ref-type="fig">2A,B</xref>, DEEPred is among the best performers in terms of predicting MF GO terms for all prokaryotic sequences (Fig. <xref rid="Fig2" ref-type="fig">2A</xref>), specifically for <italic>E. coli</italic> (Fig. <xref rid="Fig2" ref-type="fig">2B</xref>). Figure <xref rid="Fig2" ref-type="fig">2C</xref> shows that DEEPred came third in predicting BP terms for the mouse (<italic>Mus musculus</italic>) proteins. These results (Fig. <xref rid="Fig2" ref-type="fig">2A–C</xref>) also indicate that DEEPred has an added value over the conventional baseline predictors (i.e., BLAST and naive). In Fig. <xref rid="Fig2" ref-type="fig">2D</xref>, we also compared our results with the BLAST baseline classifier in terms of the GO term-centric mean area under the ROC curve (AUC) for predicting MF terms for CAFA2 benchmark sequences. As it can be seen in Fig. <xref rid="Fig2" ref-type="fig">2D</xref>, the performance of DEEPred is slightly higher than the BLAST classifier in the overall comparison considering all MF GO terms. Whereas, the performance is low for DEEPred when MF GO terms of comparably low number of training instances (&lt;1,000) was used (i.e., low terms). Finally, when the MF GO terms with comparably high number of training instances (&gt;1,000) was employed (i.e., high terms), DEEPred’s performance surpassed BLAST. The results indicate that DEEPred is especially effective and have a significant added value over conventional methods, when the number of training instances are high.<fig id="Fig2"><label>Figure 2</label><caption><p>The prediction performance of DEEPred on CAFA2 challenge benchmark set. Dark gray colored bars represent the performance of DEEPred, whereas the light gray colored bars represent the state-of-the-art methods. The evaluation was carried out in the standard mode (i.e., no-knowledge benchmark sequences, the full evaluation mode), more details about the CAFA analysis can be found in CAFA GitHub repository; (<bold>A</bold>) MF term prediction performance (F-max) of top 10 CAFA participants and DEEPred on all prokaryotic benchmark sequences; (<bold>B</bold>) MF term prediction performance (F-max) of top 10 CAFA participants and DEEPred on E. coli benchmark sequences; (<bold>C</bold>) BP term prediction performance (F-max) of top 10 CAFA participants and DEEPred on mouse benchmark sequences; and (<bold>D</bold>) MF GO term-centric mean area under the ROC curve measurement comparison between BLAST and DEEPred for all MF GO terms, bars represent terms with less than 1000 training instances (i.e., low terms) and terms with more than 1000 training instances (i.e., high terms).</p></caption><graphic xlink:href="41598_2019_43708_Fig2_HTML" id="d29e2201"/></fig></p>
      <p id="Par21">The second performance analysis was done using CAFA3 challenge data, the submission period of which has ended in February 2017. The finalized benchmark dataset (protein sequences and their GO annotations) of CAFA3 was downloaded from the CAFA challenge repository on Synapse system (“benchmark20171115.tar” from <ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn12278085">https://www.synapse.org/#!Synapse:syn12278085</ext-link>). In total, this dataset contains 7,173 annotations (BP: 3,608, CC: 1,800 and MF: 1,765) for 3,312 proteins. The DEEPred models were re-trained using UniProt-GOA manual experimental evidence coded GO annotation data from September 2016 (the date of the official training data provided by CAFA3 organizers), and the predictions were generated for benchmark dataset protein sequences. For this analysis, we could not directly use the officially announced performance data of the top challenge performers since the results are yet to be published as of April 2019. Instead, three other sequence-based function prediction methods, namely FFPred3<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, GoFDR<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> and DeepGO<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, were selected to be compared with DEEPred. These methods were developed and published in the last 2 years, and reported predictive performances that are better than the state-of-the-art in their own publications. For DeepGO, we downloaded the stand alone tool, train the models with the provided training data (considering the CAFA3 submission deadline) and produced the benchmark dataset predictions. The stand-alone tool was not available for FFPred3 and GoFDR; however, CAFA3 target set function predictions were already available, as a result, we directly employed those prediction files for our analysis. We also built baseline predictors (i.e., Naïve Bayes and Blast) with CAFA3 data, as described by the CAFA team. We employed performance evaluation scripts released by the CAFA team in order to calculate the performances of DEEPred, the state-of-the-art methods and the baseline classifiers. DEEPred is composed of multiple independent classifiers, each of which has its own best score threshold. For the calculation of F-max, CAFA evaluation script applies the same prediction score threshold to all predictions, which would result in the underestimation of DEEPred’s performance. To avoid this, we transformed DEEPred’s prediction scores and made them comparable to each other by applying min-max normalization.</p>
      <p id="Par22">Table <xref rid="Tab6" ref-type="table">6</xref> displays the performance results for Molecular function (MF), cellular component (CC) and biological process (BP) categories, in terms of F-max, precision, recall and Smin measures. Only the precision and recall values corresponding to the score threshold that produced the given F-max are shown. A better performance is indicated by higher F-max, precision and recall values and lower Smin values. “No-knowledge” and “All” indicates 2 different evaluation modes, where the former indicates that the methods are evaluated only using the proteins that did not have any manually curated GO annotation in the training dataset (before the challenge submission deadline), and the latter indicates that the methods are evaluated using all benchmark proteins. DEEPred was analzyed in terms of two different versions: <italic>(i)</italic> raw predictions coming from all predictive models, without any post-processing (i.e., DEEPred_raw), and <italic>(ii)</italic> finalized predictions after the hierarchical post-processing procedure (i.e., DEEPred_hrchy). The results are shown in Table <xref rid="Tab6" ref-type="table">6</xref>, where the best results for each GO category and for each performance measure is highlighted with bold font. When the second best method’s performance was close to the best one, both of them are highlighted. As observed from Table <xref rid="Tab6" ref-type="table">6</xref>, the finalized version of DEEPred (i.e., DEEPred_hrchy) consistently beat the performance of the raw DEEPred predictions, indicating the effectiveness of the proposed hierarchical post-processing approach. In MF term prediction, DEEPred_hrchy shared the top place with GoFDR in terms of F-max, precision and recall (GOFDR performed slightly better in terms of Smin). Considering CC term prediction, DeepGO shared the first place with the naïve classifier, in terms of both F-max and Smin. DEEPred_hrchy was the best (in terms of F-max) for predicting BP terms of the no-knowledge proteins, and shared the first place with DeepGO and FFPred3 considering all benchmark proteins. DEEPred_hrchy was also the first in terms of Smin, for the BP category. In CAFA3 analysis, at the points of maximum performance for DEEPred (i.e., Fmax) DEEPred’s precision was maximum (i.e., no FPs) and the recall was relatively low (i.e., high number of FNs), for all categories of GO, according to Table <xref rid="Tab6" ref-type="table">6</xref>. The most probable reason behind this observation can be explained as follows. Since the multi-task modeling approach is used, the negative training instances/samples of a GO term in a model constitutes the positive training instances of other GO terms in the same model. In an example model where there are 5 GO terms, each with 50 positive training instances, there are 50 positive and 200 negative instances for each GO term. The ratio of 1 to 4 may lead to a bias towards negative predictions, especially for the cases where the proper learning was not achieved during training (i.e., a fully negative set of predictions would lead to success with a ratio of 4 to 5, whereas, a fully positive set of predictions would lead to successes with a ratio of 1 to 5). As a result, the system generally prefers fewer positive predictions, which usually means low number of FPs (high precision) and relatively higher number of FNs (low recall). Another probable reason behind this observation (for the finalized version of DEEPred) is that, the hierarchical post-processing of predictions reduces the number of false positives by eliminating predictions with low or moderate reliability (i.e., by checking the prediction consistency using the prediction results of the GO terms, which are the parents of the corresponding GO term). Thus, precision is increased due to discarding of some of the predictions; however, along with discarding FPs, some of the TPs were eliminated as well (i.e., those TPs turned into FNs), so the recall was reduced.<table-wrap id="Tab6"><label>Table 6</label><caption><p>The prediction performance of DEEPred and the state-of-the-art protein function prediction methods on CAFA3 challenge benchmark dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"/><th colspan="2">F-max</th><th colspan="2"> Precision</th><th colspan="2">Recall (at F-max)</th><th colspan="2">Smin</th></tr><tr><th>No-knowledge</th><th>All</th><th>No-knowledge</th><th>All</th><th>No-knowledge</th><th>All</th><th>No-knowledge</th><th>All</th></tr></thead><tbody><tr><td colspan="9"><bold>MF</bold></td></tr><tr><td>Naive</td><td>0.35</td><td>0.29</td><td>0.49</td><td>0.41</td><td>0.27</td><td>0.23</td><td>6.87</td><td>6.43</td></tr><tr><td>Blast</td><td>0.40</td><td>0.39</td><td>0.42</td><td>0.36</td><td>0.38</td><td><bold>0.44</bold></td><td>6.99</td><td>6.48</td></tr><tr><td>FFPred3</td><td>0.32</td><td>0.31</td><td>0.34</td><td>0.30</td><td>0.30</td><td>0.32</td><td>7.35</td><td>6.66</td></tr><tr><td>GoFDR*</td><td><bold>0.55</bold></td><td>0.45</td><td>0.67</td><td>0.55</td><td><bold>0.46</bold></td><td>0.38</td><td><bold>5.06</bold></td><td><bold>4.41</bold></td></tr><tr><td>DeepGO</td><td>0.40</td><td>0.34</td><td>0.58</td><td>0.48</td><td>0.30</td><td>0.27</td><td>6.36</td><td>6.01</td></tr><tr><td>DEEPred_raw</td><td>0.32</td><td>0.33</td><td><bold>1.00</bold></td><td><bold>1.00</bold></td><td>0.19</td><td>0.19</td><td>6.63</td><td>6.16</td></tr><tr><td>DEEPred_hrchy</td><td>0.49</td><td><bold>0.50</bold></td><td><bold>1.00</bold></td><td><bold>1.00</bold></td><td>0.32</td><td>0.33</td><td>5.41</td><td>5.03</td></tr><tr><td colspan="9"><bold>CC</bold></td></tr><tr><td>Naive</td><td><bold>0.55</bold></td><td><bold>0.54</bold></td><td>0.56</td><td>0.58</td><td>0.55</td><td>0.50</td><td><bold>7.61</bold></td><td><bold>7.65</bold></td></tr><tr><td>Blast</td><td>0.46</td><td>0.45</td><td>0.39</td><td>0.39</td><td>0.56</td><td>0.53</td><td>9.74</td><td>9.94</td></tr><tr><td>FFPred3</td><td>0.54</td><td>0.52</td><td>0.54</td><td>0.54</td><td>0.53</td><td>0.50</td><td>8.61</td><td>8.44</td></tr><tr><td>GoFDR*</td><td>0.48</td><td>0.45</td><td>0.46</td><td>0.42</td><td>0.51</td><td>0.48</td><td>10.98</td><td>10.86</td></tr><tr><td>DeepGO</td><td><bold>0.54</bold></td><td><bold>0.53</bold></td><td>0.61</td><td>0.58</td><td>0.48</td><td>0.49</td><td><bold>7.68</bold></td><td><bold>7.55</bold></td></tr><tr><td>DEEPred_raw</td><td>0.30</td><td>0.29</td><td>0.19</td><td>0.18</td><td><bold>0.69</bold></td><td><bold>0.69</bold></td><td>10.68</td><td>10.41</td></tr><tr><td>DEEPred_hrchy</td><td>0.34</td><td>0.35</td><td><bold>1.00</bold></td><td><bold>1.00</bold></td><td>0.20</td><td>0.22</td><td>9.85</td><td>9.53</td></tr><tr><td colspan="9"><bold>BP</bold></td></tr><tr><td>Naive</td><td>0.26</td><td>0.30</td><td>0.25</td><td>0.39</td><td>0.26</td><td>0.24</td><td>24.27</td><td>20.85</td></tr><tr><td>Blast</td><td>0.28</td><td>0.32</td><td>0.22</td><td>0.27</td><td><bold>0.37</bold></td><td>0.38</td><td>25.11</td><td>21.35</td></tr><tr><td>FFPred3</td><td>0.26</td><td><bold>0.34</bold></td><td>0.23</td><td>0.29</td><td>0.30</td><td><bold>0.40</bold></td><td>24.74</td><td>21.48</td></tr><tr><td>GoFDR*</td><td>0.19</td><td>0.18</td><td>0.25</td><td>0.26</td><td>0.15</td><td>0.14</td><td>24.75</td><td>28.83</td></tr><tr><td>DeepGO</td><td>0.28</td><td><bold>0.34</bold></td><td>0.40</td><td>0.52</td><td>0.21</td><td>0.26</td><td>23.41</td><td>20.19</td></tr><tr><td>DEEPred_raw</td><td>0.16</td><td>0.16</td><td><bold>1.00</bold></td><td><bold>1.00</bold></td><td>0.09</td><td>0.09</td><td>24.65</td><td>22.05</td></tr><tr><td>DEEPred_hrchy</td><td><bold>0.32</bold></td><td><bold>0.33</bold></td><td><bold>1.00</bold></td><td>1.00</td><td>0.19</td><td>0.19</td><td><bold>22.04</bold></td><td><bold>19.69</bold></td></tr></tbody></table><table-wrap-foot><p>*The results of GoFDR is given based on the CAFA3 preliminary benchmark set since the results for the full benchmark dataset were not available for this method.</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec8">
      <title><italic>P. aureginosa</italic> Case Study on biofilm formation process</title>
      <p id="Par23">We analyzed the biological relevance of the results of DEEPred over selected example predictions. For this purpose, we employed the recent CAFA Pi biological process GO term assignment challenge. One of the goals in CAFA Pi was the prediction of the proteins responsible for the biofilm formation (GO:0042710) process using electronically translated open reading frames (ORFs) from a specific <italic>Pseudomonas aureginosa</italic> strain (UCBPP-PA14) genome. A short introduction about <italic>P. aureginosa</italic> and biofilm formation process can be found in the Supplementary Material <xref rid="MOESM1" ref-type="media">1</xref> Document, Section <xref rid="MOESM1" ref-type="media">8</xref>.</p>
      <p id="Par24">In order to annotate ORF sequences from <italic>P. aureginosa</italic> UCBPP-PA14 strain with biofilm formation GO term using DEEPred, we generated a single task feed-forward DNN model. The reason behind not using a multi-task model here was to prevent the potential effect of the selection of the accompanying GO terms to the predictive performance. The positive training dataset for this model was generated from all UniProtKB/Swiss-Prot protein records that were annotated either with the corresponding GO term or with its descendants with manual and experimental evidence codes, yielding 254 proteins. The negative training dataset was selected from the protein entries that were neither annotated with the corresponding GO term nor any of its descendants (the same number of samples were selected randomly to match the positive training dataset). The model was trained, and the hyper-parameters were optimized and the performance was measured via 5-fold cross validation. The performance results in terms of precision, recall and F1-score were 0.71, 0.84 and 0.77 respectively. The finalized models were then employed to predict functions for CAFA Pi <italic>P. aureginosa</italic> ORF targets.</p>
      <p id="Par25">From a literature review, we identified 8 genes (wspA, wspR, rocR, yfiN, tpbB, fleQ, fimX and PA2572) in the <italic>P. aureginosa</italic> reference genome that are associated with biofilm formation, but not annotated with the corresponding GO term or its functionally related neighboring terms, in the source databases at the time of this analysis (as a result, they are not presented in our training dataset). Out of these 8 genes/proteins wspR, yfiN, tpbB and fimX contain the GGDEF domain, which is responsible for synthesizing cyclic di-GMP and thus take part in the biofilm formation process<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Two of these genes/proteins, yfiN and tpbB, additionally contain the CHASE8 sensor domain, which controls the levels of extracellular DNA and regulates biofilm formation<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The mechanism by which these 8 genes/proteins contribute to the formation of biofilm are explained in two articles by Cheng<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> and Ryan <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. We obtained the protein sequences of these genes from the UniProt database, then aligned them to CAFA Pi <italic>P. aureginosa</italic> UCBPP-PA14 strain’s target ORF sequences to identify CAFA Pi target sequences corresponding to these genes with a cut-off of greater than 98% identity. The reason behind this application was that the CAFA Pi target dataset ORF sequences were unknown. Finally, we analyzed the equivalent <italic>P. aureginosa</italic> ORFs of these 8 genes in the target dataset using DEEPred’s biofilm formation process model and examined the prediction scores.</p>
      <p id="Par26">Table <xref rid="Tab7" ref-type="table">7</xref> displays the gene symbols, protein (UniProt) accessions and biofilm formation GO term prediction scores produced by DEEPred for the selected genes/proteins. As observed in Table <xref rid="Tab7" ref-type="table">7</xref>, 4 out of 8 genes/proteins (i.e., gene symbols: wspA, wspR, rocR and PA2572) received high prediction scores for the biofilm production term and thus successfully identified by DEEPred. Two genes/proteins (i.e., gene symbols: yfiN and tpbB) received moderate scores, which were still sufficient to produce a prediction. The remaining two genes/proteins (i.e., gene symbols: fleQ and fimX) could not be associated with the corresponding GO term at all. We also carried out a BLAST search in order to observe if these predictions could be produced by a conventional sequence similarity search. For this, the amino acid sequence of each of the 8 genes/proteins was searched against the whole UniProtKB with an e-value threshold of 100. The BLAST search revealed that none of the best 1,000 BLAST hits (50% or greater identity) possessed the biofilm formation GO term or any of its ancestor or descendant terms as annotations, and thus BLAST failed to annotate these genes/proteins. Since none of these 8 genes/proteins (or their BLAST hits) have been annotated with a GO term related to the biofilm formation function on the GO DAG; there were no protein sequences in the training dataset of DEEPred that were similar to these genes. As a result, the accurate predictions cannot be the result of a simple annotation transfer between close homologs.<table-wrap id="Tab7"><label>Table 7</label><caption><p>DEEPred’s biofilm formation term (GO:0042710) prediction results for the selected <italic>P. aureginosa</italic> proteins.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Gene symbol</th><th>Protein accession (UniProt)</th><th>DEEPred prediction score</th></tr></thead><tbody><tr><td>wspA</td><td>A0A0H2ZEY3</td><td>0.99</td></tr><tr><td>rocR</td><td>A0A0C7D525</td><td>0.98</td></tr><tr><td>PA2572</td><td>Q9I0R4</td><td>0.98</td></tr><tr><td>wspR</td><td>A0A0H2ZEX4</td><td>0.95</td></tr><tr><td>yfiN</td><td>A0A0C7ADU5</td><td>0.68</td></tr><tr><td>tpbB</td><td>Q9I4L5</td><td>0.68</td></tr><tr><td>fleQ</td><td>A0A0H2Z7X4</td><td>0.05</td></tr><tr><td>fimX</td><td>A0A0H2ZHA6</td><td>0.02</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Discussion and Conclusion</title>
    <p id="Par27">Deep learning algorithms have shown to enhance the classification performances in various fields; however, it was not thoroughly investigated in terms of their applications to the protein function prediction area at large-scale. In this study, we described the DEEPred method for predicting GO term based protein functions using a stack of feed-forward multi-task deep neural networks. As input, DEEPred only requires the amino acid sequences of proteins. We carried out several tests to investigate the behavior of DNN-based models in protein function prediction. The approach developed and applied in this study is novel in terms of:<list list-type="simple"><list-item><label>i.</label><p id="Par28">timeliness of the work: the application of deep learning based methods on different bioinformatics related problems is currently a hot topic, the pre-print of DEEPred was one of the first in the protein function prediction literature<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>;</p></list-item><list-item><label>ii.</label><p id="Par29">technical contribution in terms of designing a novel DNN-based system: 1,101 multi-task feed-forward deep neural networks are constructed, individually optimized, and stacked according to the inheritance relationships of the Gene Ontology system, which enables a hierarchical prediction and post-processing process;</p></list-item><list-item><label>iii.</label><p id="Par30">a thorough investigation of the behavior of deep neural networks when applied to GO-based protein function prediction, considering different input feature types, dataset sizes and GO term levels;</p></list-item><list-item><label>iv.</label><p id="Par31">data modelling approach: electronically made annotations (i.e., predictions of previous protein function prediction methods) are included in the training set of the predictor, with the aim of enriching training data (especially for the GO terms with insufficient number of training instances), and the results are examined in detail; as far as we are aware, this investigation is the first in protein function prediction literature;</p></list-item><list-item><label>v.</label><p id="Par32">DEEPred contributes to the state of the art in the field of protein function prediction, in terms of designing a large-scale deep learning based system that is able to model thousands of GO terms, which is also practical to use.</p></list-item></list></p>
    <p id="Par33">The input feature type selection analysis revealed that our in-house protein descriptor SPMap had a better performance compared to the conventional conjoint triad and pseudo-amino acid composition features. However, this performance increase comes with a cost in terms of a higher vector dimensionality (i.e., SPMap has between 1000 to 2000 dimensions as opposed to 373 for conjoint triad and 50 for pseudo-amino acid composition), which elevates the computational complexity. It would also be interesting to analyze additional protein feature types, especially the descriptors frequently used in protein-ligand binding prediction studies<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>.</p>
    <p id="Par34">The reasons behind choosing DEEPred’s specific DNN architecture was first, this is a basic form and thus, it is straightforward to train and optimize. In other words, it requires minimal amount manual design work compared to specialized complex networks such as the Inception Network<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. This is especially important considering the fact that more than one thousand independent networks should have been trained. Second, computational resources required to train this architecture is lower compared to, again, complex networks.</p>
    <p id="Par35">In DEEPred, we considered multi-task DNNs (as opposed to single-task DNNs) due to various advantages attributed to multi-task networks such as: <italic>(i)</italic> the ability to share knowledge between tasks; which supports the system in the case where there are a low number of training instances and <italic>(ii)</italic> training a lower number of models in total, which improves training run times. On the other hand, multi-task DNNs also have disadvantages especially when the high number of tasks compels the generation of multiple models. The problem here is the efficient grouping of the tasks (i.e., GO terms in our case) so that the tasks under an individual model would become alternatives (i.e. orthogonal) to each other. We tried to achieve this by first, grouping GO terms from the same level and second, placing the sibling terms together under the same model, where possible. In most cases, it was not possible to find a sufficient number of sibling terms, and thus, semantically unrelated terms from the same level ended up in the same model. Nevertheless, this was not a crucial problem since it is possible for a multi-functional query protein to receive high prediction scores for multiple GO terms under the same model. Another important point during the term grouping was placing GO terms with similar number of annotated proteins under the same group. According to our observations, models containing tasks with highly unbalanced number of training instances perform poorly (this is also one of the reasons why generating only one model to predict all GO terms would be a poor design choice). Due to these reasons, generation of the models required a considerable amount of manual work, none of which would be required if we employed single-task networks. It would also be possible to achieve higher performance values with single-task DNNs, especially where there is sufficient number of training instances. We did not consider single-task networks mainly because it is not feasible to train tens of thousands of networks (when the hyper-parameter optimization step is considered the number would increase to billions of training jobs) to cover the whole functional space. In the future, it would be interesting to see algorithmic solutions to the feasibility problems related to single-task networks. With such solutions we could construct and test a single-task DNN-based system for protein function prediction.</p>
    <p id="Par36">In this study, we trained several DNN models using 6 different groups of training datasets containing GO terms with differing number of training samples, to investigate the performance differences due to changes in the training sample size. Our training dataset size performance evaluation results (Fig. <xref rid="Fig1" ref-type="fig">1</xref> and Table <xref rid="Tab3" ref-type="table">3</xref>) showed that there is a general trend of performance increase with the increasing number of training samples, which means that including GO terms with low number of protein associations into models decreases the overall performance. Therefore, our findings are in accordance with the literature regarding training data sizes being one of the key factors that affect the predictive performance of deep learning algorithms; though, the research community started to focus on developing novel deep learning based approaches to address training dataset size related problems<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>.</p>
    <p id="Par37">In this work, we also investigated if there is a relationship between levels of GO terms on the GO DAG and the classification performances. Figure <xref rid="MOESM1" ref-type="media">S.1</xref> indicates that there is no such correlation. In addition, we observed that the variance in performance between different GO levels decreases as the training dataset size increases for molecular function and cellular component categories. For the biological process category, the overall performance increases with increasing GO training dataset sizes, however the variance is relatively higher. The main reason behind this may be attributed to the biological process GO terms representing complex processes (e.g., GO:0006099 - tricarboxylic acid cycle) that involves several molecular events, which is hard to associate with a sequence signature. Figure <xref rid="MOESM1" ref-type="media">S.1</xref> also showed that performance variance of cellular component GO terms is lower compared to the molecular function and biological process categories. The reason for such observation could be that the hierarchy between cellular compartment GO terms is inherently available within cells, which results in well defined hierarchical relationships between cellular component GO terms.</p>
    <p id="Par38">In most of the protein function prediction methods, training was performed using only the annotations with experimental and/or manual evidence codes. The disadvantage of this approach is that most GO terms are left with a small number of annotated proteins, which is usually not sufficient for a machine learning model training. Therefore, the functions defined by these terms cannot be predicted efficiently. One solution would be to include the annotations with no-curation evidence codes such as the electronic annotations (i.e., the annotations produced by other automated approaches). For example, the number of MF GO terms that have more than 30 protein associations is calculated as 911 when we only considered the annotations with manual and experimental evidence codes. However, when we considered the annotations with all evidence codes, this number increases to 2,776, meaning that, if the annotations with all evidences are included, it is possible to provide predictions for significantly more GO terms. The main downside of adding annotations with non-manual/experimental evidence codes to the training dataset is the potentially false positive samples, which would result in error propagation. Another potential limitation of this application would be that the predictive performance of the models with training datasets dominated by the electronic annotations would still be low (even though the number of training instances are increased), due to the fact that the sequences of most of the electronically annotated genes/proteins under a distinct GO term would be extremely similar to each other (due to annotation by sequence similarity); and thus, would not provide the required sample diversity.</p>
    <p id="Par39">At the end of the training dataset enrichment analysis, the evaluation results showed that the performance of the previously low performing GO term models were increased significantly (Table <xref rid="Tab4" ref-type="table">4</xref>), which indicates that deep learning algorithms are tolerant to noise in the learning data. Therefore, annotations with less reliable evidence codes can be included in the training of low performing models, where there is still room for significant performance improvement. However, including less reliable annotations in the training dataset of previously high performaning models decreased the performance for more than half of them.</p>
    <p id="Par40">In DEEPred, we employed a hierarchical post-processing method (in order to avoid false positive hits) by taking the prediction scores of the parents of the target GO term into account, along with the actual prediction score for the target term. The evaluation results indicated that the recall values were slightly decreased and the precision scores were noticeably increased when we employed the hierarchical post-processing procedure, producing an increased overall performance in terms of F1-score (Table <xref rid="Tab5" ref-type="table">5</xref>). In this setting, the resulting predictions can be considered more reliable. This is also indicated by the improved F-max values at the CAFA3 benchmark test (Table <xref rid="Tab6" ref-type="table">6</xref>).</p>
    <p id="Par41">In our performance tests, DEEPred performed slightly better than the state-of-the-art methods in some cases, and produced roughly similar results in others. However, we did not observe an unprecedented performance increase; probably because we did not focus on specific functional families to optimize the system performance. Instead, we investigated the applicability of DNNs for constructing large-scale automated protein function prediction pipelines. We believe that this investigation will be valuable for computational scientists in terms of developing DNN-based biological data prediction methods. According to our observations, it is feasible to use DNNs in large-scale biological data analysis pipelines, where it may be possible to achieve performances higher than the state-of-the-art, with further optimization. However, feed-forward DNN based modeling is probably not a good choice for the functional terms with low or moderate number of annotated proteins (at least without a pre-processing step such as the training dataset enrichment), for which, conventional machine learning solutions or DNN methods specialized in low-data training may be considered.</p>
    <p id="Par42">Generally, function prediction methods that incorporate multiple types of protein features at once (e.g., sequence, protein-protein interactions - PPIs, 3-D structures and annotations and etc.) perform better compared to methods that incorporate sequences, solely<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR10">10</xref></sup>. However, there are two main disadvantages of this approach. First of all, query proteins are required to have a substantial amount of characterization (especially in terms of PPIs and 3-D structures) in order for these methods to accept them as queries. Structurally well characterized proteins usually have high quality functional annotations, thus, function prediction methods are not required in the first place. Second, running times of these methods are generally multiple orders of magnitude higher compared to the sequence-based predictors, which significantly hinders their large-scale use, such as the analysis of newly sequenced genomes.</p>
    <p id="Par43">Finally in this study, we carried out a case study to discuss the biological relevance of the results produced by DEEPred, by predicting the <italic>Pseudomonas aureginosa</italic> ORF sequences that take part in the biofilm formation biological process. DEEPred managed to identify 6 out of 8 proteins that are reported to play roles in the biofilm formation function, which are not annotated with the corresponding GO term (or any of its descendent terms) in the source biological databases as of April 2018. As a result, it can be said that without any prior knowledge DEEPred produced biologically relevant predictions considering the selected function. It is also evident that DEEPred performed significantly better in this test, compared to the baseline classifier (i.e., BLAST). It is difficult to identify how deep neural networks managed to annotate these proteins where BLAST failed significantly; however, it can be attributed to DNN’s ability to extract signatures (relevant to the task at hand) hidden in the sequences, by consequent levels of data abstraction.</p>
    <p id="Par44">The methodological approach proposed in this study can easily be translated into the prediction of various types of biomolecular ontologies/attributes (e.g., protein families, interactions, pathways, subcellular locations, catalytic activities, EC numbers and structural features) and biomedical entity associations (e.g., gene-phenotype-disease relations and drug-target interactions).</p>
  </sec>
  <sec id="Sec10" sec-type="materials|methods">
    <title>Materials and Methods</title>
    <sec id="Sec11">
      <title>Training dataset construction</title>
      <p id="Par45">The training dataset of DEEPred was created using the UniProtKB/Swiss-Prot database (version 2017_08) protein entries. UniProt supports each functional annotation with one of the 21 different evidence codes, which indicate the source of the particular annotation. In this study, we used annotations with manual curation or experimental evidences, which are considered to be highly reliable. In order to generate the training dataset, the corresponding annotations were extracted from the UniProt-GOA database, propagated to their parent terms according to the “<italic>true path rule</italic>”, which defines the inheritance relationship between GO terms<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Using this dataset, a positive training dataset was constructed for each GO term. In short, proteins that are annotated either with the corresponding GO term or with one of its children terms, were included in the positive training dataset of the corresponding GO term. Since our multi-task DNN models are composed of multiple GO terms, the positive training instances for one GO term, in a model, constitute the negative training instances of the other GO terms in the same model, except the proteins that are annotated with both GO terms.</p>
      <p id="Par46">In order to analyze the effect of the extent of training datasets on the predictive performance, we constructed multiple “training-set-size-based” datasets, taking into account the number of protein associations of GO terms. For example, one of our training-set-size-based datasets includes all GO terms that have more than or equal to 30 protein associations. Hence, we created six different datasets, where GO terms in each dataset have greater than 30, 100, 200, 300, 400 and finally 500 protein associations, respectively. These datasets comprise each other (e.g., GO-terms-with-greater-than-30-proteins dataset covers the GO-terms-with-greater-than-100-proteins dataset). The statistics (i.e., number of annotations, GO terms and proteins) of these datasets are given in the Results section.</p>
    </sec>
    <sec id="Sec12">
      <title>DEEPred architecture</title>
      <p id="Par47">DEEPred was built as a stack of multi-task feed-forward deep neural networks (i.e., a stack of multi-task multi-layered perceptrons), connected to each other. In DEEPred, each DNN was independently modelled to predict 4 or 5 GO terms, thus multiple DNNs were required to cover thousands of terms. Figure <xref rid="Fig3" ref-type="fig">3</xref> displays a representative DNN model in DEEPred.<fig id="Fig3"><label>Figure 3</label><caption><p>The representation of an individual multi-task feed-forward DNN model of DEEPred (i.e., model N). Here, each task at the output layer (i.e., red squares) corresponds to a different GO term. In the example above, a query input vector is fed to the trained model N and a score greater than the pre-defined threshold is produced for GON,3, which is marked as a prediction.</p></caption><graphic xlink:href="41598_2019_43708_Fig3_HTML" id="d29e3332"/></fig></p>
      <p id="Par48">The selection of GO terms for each DNN model was based on the levels of the terms on the GO DAG. The main objective of this approach was to create a multi-task deep neural network model for each level. For this, the levels of all GO terms were extracted and the terms were separated into groups based on the level information (via topological sorting). We started the level numbering from generic terms; thus, they received low numbers (e.g., level 1, 2, 3, …) and the levels of specific terms received high numbers (e.g., level 10, 11, 12, …). In most cases, the number of protein associations of GO terms within a level were highly variable; therefore, we created subgroups to further avoid bias (i.e., tendency of a classifier to give predictions to classes with significantly higher number of training instances). Here, each subgroup included GO terms with similar number of annotations. Another reason behind generating multiple models under a specific GO level was the high number of GO terms. According to our tests, when the number of tasks under a model exceed 5 or 6, the models usually perform poorly. Due to this reason, we limited the number of tasks under a model to 5 in most cases. This procedure generated 1,101 different models concerning all GO categories. Figure <xref rid="Fig4" ref-type="fig">4</xref> represents the GO-level-based arrangement of the individual DNN models in DEEPred. Supplementary Material <xref rid="MOESM2" ref-type="media">2</xref> lists the GO terms at each GO level, and at each sub-level model (when a GO level is further divided to multiple models).<fig id="Fig4"><label>Figure 4</label><caption><p>Illustration of the GO-level-based architecture of DEEPred on a simplified hypothetical GO DAG. We omitted highly generic GO terms (shown with red colored boxes) at the top of the GO hierarchy (e.g., GO:0005488 - Binding) from our models, since they are less informative and their training datasets are highly heterogeneous. In the illustration, DNN model 1.1 incorporates GO terms: GO<sub>1,1</sub> to GO<sub>1,5</sub> from GO-level 1. In the real application, most of the GO levels were too crowded to be modeled in one DNN; in these cases, multiple DNN models were created for the same GO level (red dashed lines represent how GO terms are grouped to be modeled together). In this example, DNN models N.1, N.2 and N.3 incorporates GO terms: GO<sub>N,1</sub> to GO<sub>N,5</sub>, GO<sub>N,6</sub> to GO<sub>N,10</sub>, GO<sub>N,11</sub> to GO<sub>N,15</sub>; respectively, due to the high number of GO terms on level N. At the prediction step, when a list of query sequences is run on DEEPred, all sequences are transformed into feature vectors and fed to the multi-task DNN models. Afterwards, GO term predictions from each model are evaluated together in the hierarchical post-processing procedure to present the finalized prediction list.</p></caption><graphic xlink:href="41598_2019_43708_Fig4_HTML" id="d29e3374"/></fig></p>
      <p id="Par49">In a feed-forward DNN, forward propagation (<italic>z</italic>) for the layer <italic>l</italic> is calculated by the following equation:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z}^{l}={b}^{l}+{W}^{l}\,\ast \,{a}^{l-1}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mspace width=".25em"/><mml:mo>⁎</mml:mo><mml:mspace width=".25em"/><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math><graphic xlink:href="41598_2019_43708_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>b</italic><sup><italic>l</italic></sup> is the bias vector, <italic>W</italic><sup><italic>l</italic></sup> is the weight matrix for the <italic>l</italic><sup><italic>th</italic></sup> layer and <italic>a</italic><sup><italic>l</italic>−1</sup> is the activation value vector of the neurons at the previous layer. Subsequently, an activation function, <italic>g</italic><sup><italic>l</italic></sup>(*), is applied to the calculated <italic>z</italic><sup><italic>l</italic></sup> vector and the result of the activation function is used to compute the outputs of the <italic>l</italic><sup><italic>th</italic></sup> layer:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${a}^{l}={g}^{l}({z}^{l})$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2019_43708_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${g}^{l}=max(0,{z}^{l}\,)$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mspace width=".25em"/><mml:mo stretchy="false">)</mml:mo></mml:math><graphic xlink:href="41598_2019_43708_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>There are alternative activation functions such as sigmoid, <italic>tanh</italic> and rectified linear unit (ReLU). Here, we employed ReLU activation function for the hidden layers. The prediction scores are calculated by applying <italic>softmax</italic> function to the neurons at the output layer. The score for the <italic>j</italic><sup><italic>th</italic></sup> task is calculated by the following equation:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{j}=\frac{{e}^{j}}{{\sum }_{k}{e}^{k}}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2019_43708_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>k</italic> is the number of tasks to be trained within a model. At the end of the forward propagation step, prediction scores are used to calculate a cost function, <italic>C</italic>, based on the labels of the input samples. In this study, we used cross entropy to calculate the cost function. Once the cost function is calculated, it is used to determine how much the weights (<italic>w</italic>) will be changed after the last iteration by taking the partial derivatives of the cost function with respect to the weights:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{i}={w}_{i}-\eta \frac{\partial C}{\partial {w}_{i}}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41598_2019_43708_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <italic>η</italic> is the learning rate in the equation. The forward and back propagation steps were performed until the stopping criteria was met (e.g., after certain number iterations or after the objective performance is reached). For the training of the models of DEEPred, 100 iterations were selected.</p>
      <p id="Par50">In DEEPred, each model was independently trained using the feature vectors of the proteins annotated with the corresponding GO terms of that model. Considering the technical work to accomplish the multi-task training, we created a binary “true label vector” for each protein sequence using one-hot encoding, where each dimension represented a GO term to be trained in the corresponding model. The index of the GO term that was associated with the corresponding protein sequence was set to 1 and the remaining dimensions were set to 0. These true label vectors were employed to calculate the prediction errors at the output layer, which was then used by the optimizer to update weights with the aim of minimizing prediction error at each iteration. The hyper-parameters of the predictive models in DEEPred are explained in the Supplementary Material <xref rid="MOESM1" ref-type="media">1</xref> Document, Section <xref rid="MOESM1" ref-type="media">2</xref>. The results of the hyper-parameter optimization test are explained in Supplementary Material <xref rid="MOESM1" ref-type="media">1</xref>, Section <xref rid="MOESM1" ref-type="media">6</xref>; whereas, the actual comprehensive test results are given in Supplementary Material <xref rid="MOESM3" ref-type="media">3</xref>.</p>
      <p id="Par51">At the prediction stage, a query protein sequence feature vector is first fed to the level 1 predictor model to receive its probabilistic scores for the corresponding GO terms and then fed to the level 2 predictor model to receive probabilistic scores for a different set of GO terms. At the end of the process, GO terms that obtained scores above the pre-determined thresholds were fed to the hierarchical post-processing (explained below under the section entitled: “Hierarchical Post-processing of Predictions”) and the finalized predictions were produced (Fig. <xref rid="Fig4" ref-type="fig">4</xref>).</p>
    </sec>
    <sec id="Sec13">
      <title>Protein feature types and vector generation</title>
      <p id="Par52">In order to select the best protein feature representation for DEEPred, we implemented three alternative protein descriptor generation methods: <italic>(i)</italic> Conjoint triad<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, <italic>(ii)</italic> Pseudo amino acid composition<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> and <italic>(iii)</italic> Subsequence profile map (SPMap)<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. Each of these feature types were used individually to train and to test the system. These protein feature types are explained in the Supplementary Material <xref rid="MOESM1" ref-type="media">1</xref> Document, Section <xref rid="MOESM1" ref-type="media">4</xref>. The details about this analysis is given in the Results section.</p>
    </sec>
    <sec id="Sec14">
      <title>Determining the probabilistic score thresholds</title>
      <p id="Par53">When a query protein is fed to a prediction model of DEEPred, an individual probabilistic score is calculated for each GO term (i.e., task) within that model, representing the probability of the query protein possessing the function defined by the corresponding GO term (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). In some cases, this can be confusing because scores are on a continuous scale (i.e., it is not clear at which point one can conclude that the query protein contains the corresponding function). Usually, the requirement from a model is to make a binary prediction instead of producing a score. Setting a probabilistic score threshold for each GO term at each model solves this problem. At the prediction step, if the received score is equal to or greater than the pre-defined threshold, the model outputs a positive prediction for the corresponding GO term. To determine these thresholds in a validation setting (using the hold-out validation datasets), we calculated F1-score performance values for arbitrary threshold selections using the success of the binary predictions obtained when we fed the system with protein sequences with already known labels (i.e., GO term associations). We considered each GO term separately within a model and determined an individual threshold for each term by choosing the value providing the highest F1-score. These threshold values are stored in ready-to-use predictive models of DEEPred.</p>
    </sec>
    <sec id="Sec15">
      <title>Hierarchical post-processing of predictions</title>
      <p id="Par54">We implemented a methodology to eliminate the unreliable predictions by considering the prediction scores received for the parents of the target GO term. This way, we aimed to reduce the amount of false positive hits. The reason behind the requirement for such a post-processing step was that, multi-task DNNs tended to classify query instances to at least one of the tasks at the output layer. Such a classification scheme would not be a problem if we could generate one model that contain all of the GO terms at its output layer. However, having thousands of nodes in the output layer would be highly impractical and thus we divided GO terms into different models. This time, the problem occurs when a query protein is fed to a model, where the protein does not contain any of the functions defined by the GO terms in the corresponding model. The model often predicts one of the unrelated GO terms for the query protein, producing a false positive. We observed that separating a false positive hit (produced this way) from a reliable prediction would be possible by checking the prediction results for the parents of the predicted GO term. If the query protein consistently received high prediction scores for most of the parent terms as well, we can conclude that this case is probably a reliable prediction; otherwise, it may be a false positive hit.</p>
      <p id="Par55">To construct this methodology, we first topologically sorted the DAG for each GO category and determined all possible paths from each GO term to the root of the corresponding category, and stored this information. When a query protein is run on DEEPred, its feature vector is fed to all trained models to obtain the prediction scores for all GO terms. Starting from the most specific level of GO, the method checks whether the prediction score of the query protein is greater than the previously calculated score thresholds. If the prediction score of a target GO term is greater than its threshold, the method checks the scores it received for the parent terms on all paths to the root, using the previously stored possible-paths-to-root. If the prediction scores given to the majority of parent terms are greater than their individual thresholds (in at least one of the paths), the method presents the case as a positive prediction. This procedure is represented in Fig. <xref rid="Fig5" ref-type="fig">5</xref> with a toy example.<fig id="Fig5"><label>Figure 5</label><caption><p>Post-processing of a prediction (GO:10) for a query protein sequence on a hypothetical GO DAG. Each box corresponds to a different GO term, with identification numbers written inside. The blue colored boxes represent GO terms whose prediction scores are over the pre-calculated threshold values (i.e., predicted terms), whereas the red colored boxes represent GO terms, whose prediction scores are below the pre-calculated threshold values (i.e., non-predicted terms). The arrows indicate the term relationships. There are four different paths from the target term (i.e., GO:10) to the root (i.e., GO:01) in this hypothetical DAG. Since there is at least one path, where the majority of the terms received higher-than-threshold scores (shown by the shaded green line), the target term GO:10 is given as a finalized positive prediction for the query sequence.</p></caption><graphic xlink:href="41598_2019_43708_Fig5_HTML" id="d29e3739"/></fig></p>
    </sec>
    <sec id="Sec16">
      <title>Predictive performance evaluation tests</title>
      <p id="Par56">According to the current deep learning practice, it is not feasible to carry out a fold-based cross validation analysis, especially when the number of model training operations are high, since it usually requires extremely high computational power. This issue was also valid for DEEPred due to the presence of elevated number of models. For this reason, the assessment of DEEPred system was performed using three datasets: <italic>(i)</italic> a hold-out validation dataset, <italic>(ii)</italic> CAFA2, and <italic>(iii)</italic> CAFA3 challenge benchmark datasets. The preparation of these datasets are explained in the Supplementary Material <xref rid="MOESM1" ref-type="media">1</xref> Document, Section <xref rid="MOESM1" ref-type="media">5</xref>. Performance evaluation metrics used in this study are explained in the Supplementary Material <xref rid="MOESM1" ref-type="media">1</xref> Document, Section <xref rid="MOESM1" ref-type="media">3</xref>.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec17">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2019_43708_MOESM1_ESM.docx">
            <caption>
              <p>Supplementary_Material_1 (Document 1)</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41598_2019_43708_MOESM2_ESM.xlsx">
            <caption>
              <p>Supplementary_Material_2 (Dataset 1)</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41598_2019_43708_MOESM3_ESM.xlsx">
            <caption>
              <p>Supplementary_Material_3 (Dataset 2)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Ahmet Sureyya Rifaioglu and Tunca Doğan contributed equally.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1038/s41598-019-43708-3.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to thank Andrew Nightingale for the critical reading of the manuscript. ASR was supported by YOK OYP scholarship.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author Contributions</title>
    <p>A.S.R., M.J.M., R.C.A., V.A. and T.D. conceived the idea. A.S.R., V.A. and T.D. performed DEEPred system construction. A.S.R. and T.D. performed all data analyses. A.S.R., V.A., R.C.A. and T.D. wrote the manuscript. M.J.M., R.C.A., V.A. and T.D. supervised the overall study. All authors have revised and approved the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability</title>
    <p>The source code and all datasets used in this study are available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/cansyl/DEEPred">https://github.com/cansyl/DEEPred</ext-link>.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="FPar1">
      <title>Competing Interests</title>
      <p>The authors declare no competing interests.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Consortium</surname>
            <given-names>TU</given-names>
          </name>
        </person-group>
        <article-title>UniProt: the universal protein knowledgebase</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2016</year>
        <volume>45</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="pmid">27899559</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Blake</surname>
            <given-names>JA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Gene ontology consortium: Going forward</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2015</year>
        <volume>43</volume>
        <fpage>D1049</fpage>
        <lpage>D1056</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gku1179</pub-id>
        <pub-id pub-id-type="pmid">25428369</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rifaioglu</surname>
            <given-names>AS</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Large-scale automated function prediction of protein sequences and an experimental case study validation on PTEN transcript variants</article-title>
        <source>Proteins Struct. Funct. Bioinforma.</source>
        <year>2017</year>
        <volume>86</volume>
        <fpage>135</fpage>
        <lpage>151</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25416</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doğan</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>UniProt-DAAC: domain architecture alignment and classification, a new method for automatic functional annotation in UniProtKB</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <fpage>2264</fpage>
        <lpage>2271</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw114</pub-id>
        <?supplied-pmid 27153729?>
        <pub-id pub-id-type="pmid">27153729</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lan</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Djuric</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Vucetic</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>MS-kNN: protein function prediction by integrating multiple data sources</article-title>
        <source>BMC Bioinformatics</source>
        <year>2013</year>
        <volume>14</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="pmid">23323762</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wass</surname>
            <given-names>MN</given-names>
          </name>
          <name>
            <surname>Barton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sternberg</surname>
            <given-names>MJE</given-names>
          </name>
        </person-group>
        <article-title>CombFunc: Predicting protein function using heterogeneous data sources</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2012</year>
        <volume>40</volume>
        <fpage>466</fpage>
        <lpage>470</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gks489</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tiwari</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Srivastava</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>A survey of computational intelligence techniques in protein function prediction</article-title>
        <source>Int. J. Proteomics</source>
        <year>2014</year>
        <volume>2014</volume>
        <fpage>1</fpage>
        <lpage>22</lpage>
        <pub-id pub-id-type="doi">10.1155/2014/845479</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koskinen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Törönen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Nokso-Koivisto</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Holm</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>PANNZER: High-throughput functional annotation of uncharacterized proteins in an error-prone environment</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <fpage>1544</fpage>
        <lpage>1552</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu851</pub-id>
        <?supplied-pmid 25653249?>
        <pub-id pub-id-type="pmid">25653249</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An expanded evaluation of protein function prediction methods shows an improvement in accuracy</article-title>
        <source>Genome Biol.</source>
        <year>2016</year>
        <volume>17</volume>
        <fpage>1</fpage>
        <lpage>19</lpage>
        <pub-id pub-id-type="doi">10.1186/s13059-015-0866-z</pub-id>
        <pub-id pub-id-type="pmid">26753840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Radivojac</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A large-scale evaluation of computational protein function prediction</article-title>
        <source>Nat. Methods</source>
        <year>2013</year>
        <volume>10</volume>
        <fpage>221</fpage>
        <lpage>229</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2340</pub-id>
        <?supplied-pmid 3584181?>
        <pub-id pub-id-type="pmid">23353650</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Anderson, J. A. <italic>An introduction to neural networks</italic>. (MIT Press, 1995).</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hinton</surname>
            <given-names>Geoffrey</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>Li</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Dong</given-names>
          </name>
          <name>
            <surname>Dahl</surname>
            <given-names>George</given-names>
          </name>
          <name>
            <surname>Mohamed</surname>
            <given-names>Abdel-rahman</given-names>
          </name>
          <name>
            <surname>Jaitly</surname>
            <given-names>Navdeep</given-names>
          </name>
          <name>
            <surname>Senior</surname>
            <given-names>Andrew</given-names>
          </name>
          <name>
            <surname>Vanhoucke</surname>
            <given-names>Vincent</given-names>
          </name>
          <name>
            <surname>Nguyen</surname>
            <given-names>Patrick</given-names>
          </name>
          <name>
            <surname>Sainath</surname>
            <given-names>Tara</given-names>
          </name>
          <name>
            <surname>Kingsbury</surname>
            <given-names>Brian</given-names>
          </name>
        </person-group>
        <article-title>Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</article-title>
        <source>IEEE Signal Processing Magazine</source>
        <year>2012</year>
        <volume>29</volume>
        <issue>6</issue>
        <fpage>82</fpage>
        <lpage>97</lpage>
        <pub-id pub-id-type="doi">10.1109/MSP.2012.2205597</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Deng, L., Hinton, G. &amp; Kingsbury, B. New Types of Deep Neural Network Learning For Speech Recognition And Related Applications: An Overview 1–5 (2013).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Angermueller</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep Learning for Computational Biology</article-title>
        <source>Mol. Syst. Biol.</source>
        <year>2016</year>
        <volume>12</volume>
        <fpage>1</fpage>
        <lpage>16</lpage>
        <pub-id pub-id-type="doi">10.15252/msb.20156651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Min</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in bioinformatics</article-title>
        <source>Brief. Bioinform.</source>
        <year>2016</year>
        <volume>18</volume>
        <fpage>851</fpage>
        <lpage>869</lpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Taigman, Y., Ranzato, M. A., Aviv, T. &amp; Park, M. Deepface 1–8, 10.1109/CVPR.2014.220 (2014)</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lecun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gawehn</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Hiss</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep Learning in Drug Discovery</article-title>
        <source>Mol. Inform.</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>3</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1002/minf.201501008</pub-id>
        <?supplied-pmid 27491648?>
        <pub-id pub-id-type="pmid">27491648</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Baskin</surname>
            <given-names>II</given-names>
          </name>
          <name>
            <surname>Winkler</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tetko</surname>
            <given-names>IV</given-names>
          </name>
        </person-group>
        <article-title>A renaissance of neural networks in drug discovery</article-title>
        <source>Expert Opin. Drug Discov. ISSN</source>
        <year>2016</year>
        <volume>11</volume>
        <fpage>785</fpage>
        <lpage>795</lpage>
        <pub-id pub-id-type="doi">10.1080/17460441.2016.1201262</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mayr</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Klambauer</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Unterthiner</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>DeepTox: Toxicity Prediction using Deep Learning</article-title>
        <source>Front. Environ. Sci.</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.3389/fenvs.2015.00080</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Ramsundar, B. <italic>et al</italic>. Massively Multitask Networks for Drug Discovery arXiv:1502.02072v1. <italic>arXiv</italic> 1–27 (2015).</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Bengio, Y. <italic>Learning Deep Architectures for AI</italic>. <italic>Foundations and Trends® in Machine Learning</italic><bold>2</bold> (2009).</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goh</surname>
            <given-names>GB</given-names>
          </name>
          <name>
            <surname>Hodas</surname>
            <given-names>NO</given-names>
          </name>
          <name>
            <surname>Vishnu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deep Learning for Computational Chemistry</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <volume>1701.04503</volume>
        <fpage>1</fpage>
        <lpage>50</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="editor">
          <name>
            <surname>Saberi Mohamad</surname>
            <given-names>Mohd</given-names>
          </name>
          <name>
            <surname>Rocha</surname>
            <given-names>Miguel P.</given-names>
          </name>
          <name>
            <surname>Fdez-Riverola</surname>
            <given-names>Florentino</given-names>
          </name>
          <name>
            <surname>Domínguez Mayo</surname>
            <given-names>Francisco J.</given-names>
          </name>
          <name>
            <surname>De Paz</surname>
            <given-names>Juan F.</given-names>
          </name>
        </person-group>
        <source>10th International Conference on Practical Applications of Computational Biology &amp; Bioinformatics</source>
        <year>2016</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer International Publishing</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sliwoski</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kothiwale</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Meiler</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lowe</surname>
            <given-names>EW</given-names>
          </name>
        </person-group>
        <article-title>Computational Methods in Drug Discovery</article-title>
        <source>Pharmacol. Rev.</source>
        <year>2014</year>
        <volume>66</volume>
        <fpage>334</fpage>
        <lpage>395</lpage>
        <pub-id pub-id-type="doi">10.1124/pr.112.007336</pub-id>
        <?supplied-pmid 24381236?>
        <pub-id pub-id-type="pmid">24381236</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Liu, X. L. Deep Recurrent Neural Network for Protein Function Prediction from Sequence. <italic>arXiv</italic> 1–38 (2017).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>Renzhi</given-names>
          </name>
          <name>
            <surname>Freitas</surname>
            <given-names>Colton</given-names>
          </name>
          <name>
            <surname>Chan</surname>
            <given-names>Leong</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Miao</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>Haiqing</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Zhangxin</given-names>
          </name>
        </person-group>
        <article-title>ProLanGO: Protein Function Prediction Using Neural Machine Translation Based on a Recurrent Neural Network</article-title>
        <source>Molecules</source>
        <year>2017</year>
        <volume>22</volume>
        <issue>10</issue>
        <fpage>1732</fpage>
        <pub-id pub-id-type="doi">10.3390/molecules22101732</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kulmanov</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Hoehndorf</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>DeepGO: Predicting protein functions from sequence and interactions using a deep ontology-aware classifier</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>34</volume>
        <fpage>660</fpage>
        <lpage>668</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx624</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Szalkai</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grolmusz</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Hancock</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>SECLAF: A Webserver and Deep Neural Network Design Tool for Hierarchical Biological Sequence Classification</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <fpage>2487</fpage>
        <lpage>2489</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty116</pub-id>
        <?supplied-pmid 29490010?>
        <pub-id pub-id-type="pmid">29490010</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Tavanaei, A. <italic>et al</italic>. Towards Recognition of Protein Function based on its Structure using Deep Convolutional Networks. <italic>IEEE Int. Conf. Bioinforma. Biomed</italic>. 145–149, 10.1109/BIBM.2016.7822509 (2016).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Gligorijević, V., Barot, M. &amp; Bonneau, R. DeepNF: Deep network fusion for protein function prediction. <italic>bioRxiv</italic><bold>223339</bold>, 10.1101/223339 (2017).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Fa, R., Cozzetto, D., Wan, C. &amp; Jones, D. T. Predicting Human Protein Function with Multi-task Deep Neural Networks. <italic>bioRxiv</italic> (2018).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sadowski</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Deep autoencoder neural networks for gene ontology annotation predictions</article-title>
        <source>Proc. 5th ACM Conf. Bioinformatics, Comput. Biol. Heal. Informatics - BCB</source>
        <year>2014</year>
        <volume>14</volume>
        <fpage>533</fpage>
        <lpage>540</lpage>
        <pub-id pub-id-type="doi">10.1145/2649387.2649442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Guoxian</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Protein Function Prediction Using Deep Restricted Boltzmann Machines</article-title>
        <source>BioMed Res. Int.</source>
        <year>2017</year>
        <volume>2017</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Rifaioglu, A. S., Doğan, T., Martin, M. J., Cetin-Atalay, R. &amp; Atalay, M. V. Multi-task Deep Neural Networks in Automated Protein Function Prediction. <italic>arXiv</italic> 1–19 (2017).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cozzetto</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Minneci</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Currant</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>FFPred 3: Feature-based function prediction for all Gene Ontology domains</article-title>
        <source>Sci. Rep.</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1038/srep31865</pub-id>
        <pub-id pub-id-type="pmid">28442746</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gong</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Ning</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>GoFDR: A sequence alignment based method for predicting protein functions</article-title>
        <source>Methods</source>
        <year>2016</year>
        <volume>93</volume>
        <fpage>3</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2015.08.009</pub-id>
        <?supplied-pmid 26277418?>
        <pub-id pub-id-type="pmid">26277418</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ryjenkov</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Tarutina</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Moskvin</surname>
            <given-names>OV</given-names>
          </name>
          <name>
            <surname>Gomelsky</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Cyclic diguanylate is a ubiquitous signaling molecule in bacteria: Insights into biochemistry of the GGDEF protein domain</article-title>
        <source>J. Bacteriol.</source>
        <year>2005</year>
        <volume>187</volume>
        <fpage>1792</fpage>
        <lpage>1798</lpage>
        <pub-id pub-id-type="doi">10.1128/JB.187.5.1792-1798.2005</pub-id>
        <?supplied-pmid 15716451?>
        <pub-id pub-id-type="pmid">15716451</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ueda</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wood</surname>
            <given-names>TK</given-names>
          </name>
        </person-group>
        <article-title>Connecting quorum sensing, c-di-GMP, pel polysaccharide, and biofilm formation in Pseudomonas aeruginosa through tyrosine phosphatase TpbA (PA3885)</article-title>
        <source>PLoS Pathog.</source>
        <year>2009</year>
        <volume>5</volume>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.ppat.1000483</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>C-Y</given-names>
          </name>
        </person-group>
        <article-title>Surface Sensing for Biofilm Formation in Pseudomonas aeruginosa</article-title>
        <source>Front. Microbiol.</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.3389/fmicb.2017.02671</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ryan</surname>
            <given-names>RP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>HD-GYP domain proteins regulate biofilm formation and virulence in Pseudomonas aeruginosa</article-title>
        <source>Environ. Microbiol.</source>
        <year>2009</year>
        <volume>11</volume>
        <fpage>1126</fpage>
        <lpage>1136</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1462-2920.2008.01842.x</pub-id>
        <?supplied-pmid 19170727?>
        <pub-id pub-id-type="pmid">19170727</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Westen</surname>
            <given-names>GJP</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Benchmarking of protein descriptor sets in proteochemometric modeling (part 1): comparative study of 13 amino acid descriptor sets</article-title>
        <source>J. Cheminform.</source>
        <year>2013</year>
        <volume>5</volume>
        <fpage>1</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-5-1</pub-id>
        <pub-id pub-id-type="pmid">23289532</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Szegedy</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Going deeper with convolutions</article-title>
        <source>Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.</source>
        <year>2015</year>
        <volume>07–12–June</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altae-Tran</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ramsundar</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pappu</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Low Data Drug Discovery with One-Shot Learning</article-title>
        <source>ACS Cent. Sci.</source>
        <year>2017</year>
        <volume>3</volume>
        <fpage>283</fpage>
        <lpage>293</lpage>
        <pub-id pub-id-type="doi">10.1021/acscentsci.6b00367</pub-id>
        <?supplied-pmid 28470045?>
        <pub-id pub-id-type="pmid">28470045</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting protein-protein interactions based only on sequences information</article-title>
        <source>Proc. Natl. Acad. Sci. USA</source>
        <year>2007</year>
        <volume>104</volume>
        <fpage>4337</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.0607879104</pub-id>
        <?supplied-pmid 17360525?>
        <pub-id pub-id-type="pmid">17360525</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>Prediction of Protein Cellular Attributes Using Pseudo- Amino Acid Composition</article-title>
        <source>Proteins Struct., Funct., Genet.</source>
        <year>2001</year>
        <volume>255</volume>
        <fpage>246</fpage>
        <lpage>255</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.1035</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sarac</surname>
            <given-names>OS</given-names>
          </name>
          <name>
            <surname>Gürsoy-Yüzügüllü</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Cetin-Atalay</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Atalay</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Subsequence-based feature map for protein function classification</article-title>
        <source>Comput. Biol. Chem.</source>
        <year>2008</year>
        <volume>32</volume>
        <fpage>122</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compbiolchem.2007.11.004</pub-id>
        <?supplied-pmid 18243801?>
        <pub-id pub-id-type="pmid">18243801</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
