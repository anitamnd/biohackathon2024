<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Pattern Recognit</journal-id>
    <journal-id journal-id-type="iso-abbrev">Pattern Recognit</journal-id>
    <journal-title-group>
      <journal-title>Pattern Recognition</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0031-3203</issn>
    <issn pub-type="epub">0031-3203</issn>
    <publisher>
      <publisher-name>Elsevier Ltd.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9359771</article-id>
    <article-id pub-id-type="pii">S0031-3203(22)00443-5</article-id>
    <article-id pub-id-type="doi">10.1016/j.patcog.2022.108963</article-id>
    <article-id pub-id-type="publisher-id">108963</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GFNet: Automatic segmentation of COVID-19 lung infection regions using CT images based on boundary features<sup><xref ref-type="fn" rid="d36e3780">☆</xref></sup></article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0001">
        <name>
          <surname>Fan</surname>
          <given-names>Chaodong</given-names>
        </name>
        <xref rid="aff0001" ref-type="aff">a</xref>
        <xref rid="aff0002" ref-type="aff">b</xref>
        <xref rid="aff0003" ref-type="aff">c</xref>
        <xref rid="aff0004" ref-type="aff">d</xref>
      </contrib>
      <contrib contrib-type="author" id="au0002">
        <name>
          <surname>Zeng</surname>
          <given-names>Zhenhuan</given-names>
        </name>
        <xref rid="aff0002" ref-type="aff">b</xref>
        <xref rid="cor0001" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au0003">
        <name>
          <surname>Xiao</surname>
          <given-names>Leyi</given-names>
        </name>
        <xref rid="aff0001" ref-type="aff">a</xref>
        <xref rid="aff0004" ref-type="aff">d</xref>
        <xref rid="aff0005" ref-type="aff">e</xref>
        <xref rid="aff0006" ref-type="aff">f</xref>
        <xref rid="aff0007" ref-type="aff">g</xref>
        <xref rid="cor0001" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au0004">
        <name>
          <surname>Qu</surname>
          <given-names>Xilong</given-names>
        </name>
        <xref rid="aff0004" ref-type="aff">d</xref>
      </contrib>
      <aff id="aff0001"><label>a</label>School of Computer Science and Technology, Hainan University, Haikou 570228, China</aff>
      <aff id="aff0002"><label>b</label>School of Computer Science, Xiangtan University, Xiangtan 411100, China</aff>
      <aff id="aff0003"><label>c</label>Foshan Green Intelligent Manufacturing Research Institute of Xiangtan University, Foshan 528000, China</aff>
      <aff id="aff0004"><label>d</label>School of Information Technology and Management, Hunan University of Finance and Economics, Changsha 410205, China</aff>
      <aff id="aff0005"><label>e</label>AnHui Key Laboratory of Detection Technology and Energy Saving Devices, AnHui Polytechnic University, Wuhu 241000, China</aff>
      <aff id="aff0006"><label>f</label>Fujian Provincial Key Laboratory of Data Intensive Computing, Quanzhou Normal University, Quanzhou 362000 China</aff>
      <aff id="aff0007"><label>g</label>Vehicle Measurement, Control and Safety Key Laboratory of Sichuan Province, Xihua University, Chengdu 610039, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor0001"><label>⁎</label>Corresponding authors.</corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>8</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>8</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <volume>132</volume>
    <fpage>108963</fpage>
    <lpage>108963</lpage>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>31</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>7</day>
        <month>8</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Elsevier Ltd. All rights reserved.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Elsevier Ltd</copyright-holder>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="abs0001">
      <p>In early 2020, the global spread of the COVID-19 has presented the world with a serious health crisis. Due to the large number of infected patients, automatic segmentation of lung infections using computed tomography (CT) images has great potential to enhance traditional medical strategies. However, the segmentation of infected regions in CT slices still faces many challenges. Specially, the most core problem is the high variability of infection characteristics and the low contrast between the infected and the normal regions. This problem leads to fuzzy regions in lung CT segmentation. To address this problem, we have designed a novel global feature network(GFNet) for COVID-19 lung infections: VGG16 as backbone, we design a Edge-guidance module(Eg) that fuses the features of each layer. First, features are extracted by reverse attention module and Eg is combined with it. This series of steps enables each layer to fully extract boundary details that are difficult to be noticed by previous models, thus solving the fuzzy problem of infected regions. The multi-layer output features are fused into the final output to finally achieve automatic and accurate segmentation of infected areas. We compared the traditional medical segmentation networks, UNet, UNet++, the latest model Inf-Net, and methods of few shot learning field. Experiments show that our model is superior to the above models in Dice, Sensitivity, Specificity and other evaluation metrics, and our segmentation results are clear and accurate from the visual effect, which proves the effectiveness of GFNet. In addition, we verify the generalization ability of GFNet on another “never seen” dataset, and the results prove that our model still has better generalization ability than the above model. Our code has been shared at <ext-link ext-link-type="uri" xlink:href="https://github.com/zengzhenhuan/GFNet" id="intrrf0001">https://github.com/zengzhenhuan/GFNet</ext-link>.</p>
    </abstract>
    <kwd-group id="keys0001">
      <title>Keywords</title>
      <kwd>Image segmentation</kwd>
      <kwd>COVID-19</kwd>
      <kwd>Edge-guidance</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>CT image</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0001">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0005">Since December 2019, a large number of novel coronavirus cases have been reported in Wuhan, Hubei Province, China, and the number of infections is increasing. Novel coronavirus can cause acute respiratory diseases in humans and may even cause fatal acute respiratory distress syndrome (ARDS). The International Committee on Taxonomy of Viruses (ICTV) named the novel coronavirus SARS-CoV-2, and the World Health Organization (WHO) named it COVID-19. The Novel Coronavirus has been confirmed as capable of human-to-human transmission. The Novel Coronavirus spreads rapidly in China and the world due to massive traffic and population movement during the Spring Festival. The Novel Coronavirus has extremely high rates of morbidity and mortality. According to the World Health Organization (WHO), as of 10 April 2020, there have been 1521, 252 confirmed cases globally <xref rid="bib0001" ref-type="bibr">[1]</xref>. As of 12 July 2021, there have been 180 million confirmed cases worldwide and 4 million cumulative deaths <xref rid="bib0002" ref-type="bibr">[2]</xref>.</p>
    <p id="p0006">Reverse transcription polymerase chain reaction (RT-PCR) is considered by the industry to be the gold standard for screening for COVID-19. However, this has limited rapid and accurate detection. In addition, RT-PCR testing has been reported to have a high rate of false negatives. As an important complement to RT-PCR testing, radiological imaging techniques such as X-ray and computed tomography (CT) have also played a role in current diagnosis, including follow-up evaluation and assessment of disease progression <xref rid="bib0007" ref-type="bibr">[7]</xref>, <xref rid="bib0008" ref-type="bibr">[8]</xref>. A clinical study of 1014 patients from Wuhan, China, showed that chest CT analysis could achieve a sensitivity of 0.97, a specificity of 0.25, and an accuracy of 0.68 in detecting cases of neocoronary pneumonia, accompanied by RT-PCR test results as a reference <xref rid="bib0023" ref-type="bibr">[23]</xref>. Computed tomography (CT) imaging plays a crucial role in detecting the pulmonary manifestations of COVID-19 <xref rid="bib0003" ref-type="bibr">[3]</xref>, <xref rid="bib0004" ref-type="bibr">[4]</xref>, and the segmentation of infected lesions in CT scan is very important for quantitative measurement of disease progression <xref rid="bib0005" ref-type="bibr">[5]</xref>, <xref rid="bib0006" ref-type="bibr">[6]</xref>. Lung CT image segmentation is a necessary initial step for lung image analysis <xref rid="bib0039" ref-type="bibr">[37]</xref>. The segmentation of lesions can remove unnecessary background areas and assist doctors in diagnosis, which is an important step in CT image analysis. In contrast to common pneumonia, COVID-19 presents with pulmonary ground-glass opacity (GGO) and signs of pulmonary solidity. CT imaging of COVID-19 in its early stages usually appears as one or more GGO nodular, patchy or lamellar shadows and is generally distributed along the field 1/3 of the lung and under the pleura. As the disease progresses, most patients with COVID-19 will develop solid lung lesions. During the COVID-19 outbreak in Wuhan, a large number of patients, including suspected cases, confirmed cases and follow-up cases, required chest CT examinations to observe the changes and severity of pneumonia. The qualitative evaluation of infection and longitudinal changes in CT slices could thus provide useful and important information in fighting against COVID-19. So we need to segment the lesion regions separately. Existing papers <xref rid="bib0038" ref-type="bibr">[36]</xref> have shown that considering a real-world application, segmentation is an important step since it removes background information, reduces the chance of data leak, and forces the model to focus only on important image areas. Under the current circumstances, any missed cases will continue to lead to COVID-19 transmission. Therefore, a large amount of work and high diagnostic accuracy pose great challenges to radiologists. In addition, the radiologist’s eyestrain increases the potential risk of missing some small lesions. In the face of such a serious epidemic, it is very necessary to apply deep learning to disease diagnosis. Early, accurate and rapid diagnosis of suspected cases is crucial to timely isolation and medical treatment, and is of great significance to patient treatment, epidemic control and public health security. So, developing an artificial intelligence (AI) method for COVID-19 computer-aided diagnosis could be very helpful to radiologists.</p>
    <p id="p0007">In <xref rid="sec0002" ref-type="sec">Section 2</xref>, we will introduce some recent examples of deep learning applied to lung CT segmentation, explain their shortcomings, and then introduce the ideas and solutions of this paper. In <xref rid="sec0003" ref-type="sec">Section 3</xref>, we will introduce the network structure and its core modules and loss functions in detail. In <xref rid="sec0009" ref-type="sec">Section 4</xref>, we will introduce the experimental environment, dataset sources, evaluation metrics, and illustrate how to determine the optimal output of the network. Then we will present our experimental results qualitatively and quantitatively and compare them with other methods, and add ablation experiments to verify the effectiveness of the core modules. Finally, the conclusion is given in <xref rid="sec0031" ref-type="sec">Section 5</xref>.</p>
  </sec>
  <sec id="sec0002">
    <label>2</label>
    <title>Related work</title>
    <p id="p0008">Recently, deep learning systems have been designed to examine patients infected with COVID-19 via radiological imaging <xref rid="bib0044" ref-type="bibr">[41]</xref>, <xref rid="bib0045" ref-type="bibr">[42]</xref>. In clinical practice, automatic segmentation of lesions is highly desirable <xref rid="bib0006" ref-type="bibr">[6]</xref>. Although CT scans are important for diagnosis and treatment decisions, automatically segmenting COVID-19 pneumonia lesions from CT scans is challenging for several reasons. First of all, the infected lesions have a variety of complex manifestations, such as ground glass shadow (GGO), consolidation, etc. <xref rid="bib0009" ref-type="bibr">[9]</xref>. Second, the size and location of pneumonia lesions vary greatly between different stages of infection and different patients. As shown in <xref rid="fig0001" ref-type="fig">Fig. 1</xref>
(a–c) <xref rid="bib0010" ref-type="bibr">[10]</xref>, the lesions were irregular in shape and blurred in boundary. Some lesions, such as GGO, had low contrast with the surrounding region. In addition, radiologists’ labeling of infected region is a highly subjective task, often subject to personal biases and influenced by clinical experience. As shown in <xref rid="fig0001" ref-type="fig">Fig. 1</xref>(d). Due to the outbreak of COVID-19, it is difficult to collect enough labeled data for deep model training in a short period of time. In addition, obtaining high quality pixel-level lung infection annotations on CT slices is expensive and time consuming. These challenges make the task of automatically segmenting lesions more difficult.<fig id="fig0001"><label>Fig. 1</label><caption><p>CT scan of COVID-19 patients with complex findings of pneumonia lesions (a-c) from three different patients, with some lesions highlighted by arrows.(d) Display the labels made by different observers on item (c). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><alt-text id="at0001">Fig. 1</alt-text><graphic xlink:href="gr1_lrg"/></fig></p>
    <p id="p0009">Li et al. <xref rid="bib0004" ref-type="bibr">[4]</xref> used U-Net <xref rid="bib0012" ref-type="bibr">[12]</xref> to segment lung from CT scan. UNet++ <xref rid="bib0013" ref-type="bibr">[13]</xref> is also used to detect <xref rid="bib0014" ref-type="bibr">[14]</xref> and segment <xref rid="bib0015" ref-type="bibr">[15]</xref> infected lesions from CT scan. Liu et al. <xref rid="bib0021" ref-type="bibr">[21]</xref> propose a weakly supervised COVID-19 infection segmentation method with scribble supervision. A et al. <xref rid="bib0039" ref-type="bibr">[37]</xref> design and evaluate an automatic tool for automatic COVID-19 Lung Infection segmentation and measurement using chest CT images. An uncertainty-aware mean teacher framework is incorporated into the proposed method to guide the model training, encouraging the segmentation predictions to be consistent under different perturbations for an input image. With the pixel level uncertainty measure on the predictions of the teacher model, the student model is guided with reliable supervision. Zhao et al. <xref rid="bib0022" ref-type="bibr">[22]</xref> proposed a new deep-learning-based method that integrates a 3D V-Net with shape priors for medical image segmentation. The shape prior was used to optimize the model weights in both V-Net input and output, which significantly improved the model performance. Wang et al. <xref rid="bib0016" ref-type="bibr">[16]</xref> trained the lung segmentation network by using ground-truth mask obtained by unsupervised method, designed an effective lightweight 3D residual network and proposed a weakly supervised COVID-19 lesion detection method. Wang et al. <xref rid="bib0010" ref-type="bibr">[10]</xref> proposed a novel noise-robust-DICE loss function and a noise robust learning framework based on CNNs self-integration, which is robust to noise labels and Dice losses. Fan et al. <xref rid="bib0017" ref-type="bibr">[17]</xref> proposed an Inf-Net automatic segmentation of infected region from lung CT images and a semi-supervised segmentation framework based on random selection propagation strategy, which only required a small number of labeled images and mainly used unlabeled data to alleviate the shortage of high-quality labeled data. Maghdid et al. <xref rid="bib0040" ref-type="bibr">[38]</xref> build a comprehensive dataset of X-rays and CT scan images from multiple sources as well as provides a simple but an effective COVID-19 detection technique using deep learning and transfer learning algorithms. Zhou et al. <xref rid="bib0046" ref-type="bibr">[43]</xref> proposed an ensemble deep learning model for novel COVID-19 detection from CT images. Mu et al. <xref rid="bib0048" ref-type="bibr">[45]</xref> proposed a multi-scale multi-level feature recursive aggregation (mmFRA) network which is used to integrate multi-scale features with multi-level features. Katsamenis et al. <xref rid="bib0049" ref-type="bibr">[46]</xref> proposed a deep learning framework that can detect COVID-19 pneumonia in thoracic radiographs, as well as differentiate it from bacterial pneumonia infection. Voulodimos et al. <xref rid="bib0025" ref-type="bibr">[25]</xref>, <xref rid="bib0047" ref-type="bibr">[44]</xref> presented a few-shot learning paradigm for segmenting COVID-19 infectious regions. The main difference of the proposed algorithm compared with the traditional approaches is that it is an online learning paradigm, not the static supervised learning of U-Net.</p>
    <p id="p0010">However, all the above methods have a common deficiency, that is, the fuzzy boundary problem of lesion segmentation is not solved completely, in other words, the fuzzy boundary is not exactly segmented. So the segmentation results are still kind of ambiguous. As shown in <xref rid="fig0002" ref-type="fig">Fig. 2</xref>
. If ambiguous, for segmenting the lesion regions will affect the medical staff of judgment, if the normal tissue segmentation as the lesion regions, can make the patient’s condition mistaken for serious, if the lesion regions segmentation to normal tissue, may let the patient do not get proper treatment cases lead to misdiagnosis and even produce COVID-19 further spread.<fig id="fig0002"><label>Fig. 2</label><caption><p>The position marked by the circle in the figure points out the problem of fuzzy boundary of segmentation results in an existing model <xref rid="bib0017" ref-type="bibr">[17]</xref>. In the two regions marked, it may be difficult for health care workers to determine from the naked eye whether these regions are infected or not. In image segmentation, these regions are difficult identify by previous methods because of their fuzzy boundaries and low contrast with normal tissues. Such areas of ambiguity may interfere with a health care provider’s perception of the infected region. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><alt-text id="at0002">Fig. 2</alt-text><graphic xlink:href="gr2_lrg"/></fig></p>
    <p id="p0011">Therefore, the method proposed in this paper is mainly aimed at solving the problem of fuzzy boundary and the problem that normal organizations are difficult to distinguish correctly. In clinical practice, doctors first determine the location of the lesion and then judge the results according to the characteristics of the diseased region when judging the patient’s condition according to CT images. According to the above steps, we should focus on the location and boundary of the lesion region when we use the deep neural network for learning.</p>
    <p id="p0012">To solve the above problems, we will carry out the following two main operations successively: 1. Locate the lesion.2 Accurately extract contour. As we all know, in the convolutional neural network, the resolution of low-layer features is higher and contains more detailed information, but because of less convolution, it has lower semantics and more noise. High-layer features have stronger semantic information, but the resolution is low and the perception of details is poor <xref rid="bib0011" ref-type="bibr">[11]</xref>. How to integrate them efficiently, take their advantages and discard their faults is the key to improving the segmentation model. Therefore, we designed a kind of framework. First, aggregation high-level feature extracting rough estimate of the lesion region. We also designed a Edge-guidance module, used to provide guidance in the use of reverse attention modules for accurate contour extraction of lesions after localization. Finally, by fusing feature at each layer, the accurate segmentation of the lesion region was obtained.</p>
    <p id="p0013">In short, our contributions in this paper are threefold:</p>
    <p id="p0014">1. A novel deep learning network framework, GFNet, was proposed for the segmentation of infected region of COVID-19 in two-dimensional CT images of lungs. By aggregating the high-layer features using the aggregation module, the aggregated features can capture context information and generate a global location map as an initial boot region for subsequent steps. In order to further dig the boundary information of the target, we use the reverse attention module step by step from the high-layer to the low-layer, then further extract the hidden details of each layer, and finally fuse the features of each layer, so that the network can fully extract the details that are difficult to be noticed by the previous model.</p>
    <p id="p0015">2. We design a Edge-guidance map that contains the boundary features of each layer to further extract the boundary information when the features of each layer are extracted. The experiment proves that this design is very effective.</p>
    <p id="p0016">3. We applied the GFNet framework to VGG16 and used our method on two different datasets. One data set was “seen” to verify learning ability, and the other was “not seen” to verify generalization ability. Experimental results show that GFNet has better learning ability and generalization ability than existing models.</p>
    <p id="p0017">4. We conducted experiments with each model on training datasets of different sizes. Our model can achieve good performance when the training set is relatively small. In real-world case decision making, our GFNet is fully capable of such tasks if it is put into application under time constraints or with few training samples. Our GFNet can also be sufficiently trained to achieve maximum performance if there is sufficient time or a large training sample.</p>
  </sec>
  <sec id="sec0003">
    <label>3</label>
    <title>Proposed method</title>
    <p id="p0018">In this section, we will give a detailed introduction to the GFNet network structure, core network components, and loss functions.</p>
    <sec id="sec0004">
      <label>3.1</label>
      <title>Global-feature network(GFNet)</title>
      <p id="p0019">Network overview: The framework of GFNet is shown in <xref rid="fig0003" ref-type="fig">Fig. 3</xref>
. With VGG16 as the backbone network, the whole network is divided into five layers, and the high-layers features are layer 3, layer 4, and layer 5. We use the aggregation module to aggregate the high-layers features(<inline-formula><mml:math id="M1" altimg="si36.svg"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) to generate global location map <inline-formula><mml:math id="M2" altimg="si37.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula>
<xref rid="bib0017" ref-type="bibr">[17]</xref>. Then, starting from the <inline-formula><mml:math id="M3" altimg="si37.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula>, importing <inline-formula><mml:math id="M4" altimg="si37.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M5" altimg="si38.svg"><mml:msub><mml:mi>f</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> into a reverse attention module (RA), meanwhile, under the guidance of Edge-guidance module(Eg). The output <inline-formula><mml:math id="M6" altimg="si30.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M7" altimg="si39.svg"><mml:msub><mml:mi>f</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula> go to the next RA. Repeating the above operation to get <inline-formula><mml:math id="M8" altimg="si34.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> finally, fuse n <inline-formula><mml:math id="M9" altimg="si40.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and get the prediction graph through sigmoid output. Next, we will introduce the core modules in this network and the loss function in detail.<fig id="fig0003"><label>Fig. 3</label><caption><p>Structure diagram of GFNet model, f1 to f5 are the backbone of VGG16. GFNet contains one Eg, one aggregation module Sg, and five RA modules connected. The input image goes from F1 to F5, then from S5 back to S1 via Sg, Eg, RA modules. The final output result is the sum of each lateral output Si. See III. A for more details.</p></caption><alt-text id="at0003">Fig. 3</alt-text><graphic xlink:href="gr3_lrg"/></fig></p>
    </sec>
    <sec id="sec0005">
      <label>3.2</label>
      <title>Edge guidance module</title>
      <p id="p0020">We know that the deeper the feature, the coarser the information, while the low-layer feature contains a lot of useful details. In order to make good use of the features of each layer, based on the idea of edge detection <xref rid="bib0018" ref-type="bibr">[18]</xref>, we designed a Edge-guidance module(Eg) fusing the features of each layer, it contains the boundary information of each layer and has rich edge features, which helps us to extract the boundary of the lesion region more effectively and accurately in the subsequent feature extraction, so as to solve the problem of fuzzy boundary. The specific approach is as follows: For the VGG16, VGG16 network composes of 13 conv layers and 3 fully connected layers. We cut all the fully connected layers and the pool5 layer. On the one hand, we remove the fully connected layers due to the fact that they do not align with our design of a fully convolutional network. On the other hand, adding pool5 layer will increase the stride by two times, and it’s harmful for edge localization. Its conv layers are divided into five stages, in which a pooling layer is connected after each stage. The useful information captured by each conv layer becomes coarser with its receptive field size increasing. Detailed receptive field sizes of different layers can be seen in <xref rid="tbl0001" ref-type="table">Table 1</xref>
. Hence, we combine hierarchical features from all the conv layers into a holistic framework, in which all of the parameters are learned automatically. Since receptive field sizes of conv layers in VGG16 are different from each other, our network can learn multi-scale, including low-level and objectlevel, information that is helpful to extract more boundary information. Since our boundary guidance module combines all the accessible conv layers to employ richer features, it is expected to achieve a boost in accuracy. At the same time of obtaining the Edge-guidance map, we compute the gradient of Ground-truth of the input image to obtain edge-Ground-truth of the boundary <xref rid="bib0017" ref-type="bibr">[17]</xref>. Then we conduct deep supervision of the Edge-guidance map and let it further learn the boundary features from the rich features. In <xref rid="fig0004" ref-type="fig">Fig. 4</xref>
, we show the Edge-guidance results after combination. The edge obtains strong response at the COVID-19 infection region. Details of this module are shown in <xref rid="fig0004" ref-type="fig">Fig. 4</xref>.<table-wrap position="float" id="tbl0001"><label>Table 1</label><caption><p>Detailed receptive field and stride sizes of standard VGG16 net.</p></caption><alt-text id="at0011">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">layer</th><th align="left" valign="top">conv1<inline-formula><mml:math id="M10" altimg="si2.svg"><mml:msub><mml:mrow/><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="top">conv1<inline-formula><mml:math id="M11" altimg="si3.svg"><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top">pool<inline-formula><mml:math id="M12" altimg="si4.svg"><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top">conv2<inline-formula><mml:math id="M13" altimg="si4.svg"><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top">conv2<inline-formula><mml:math id="M14" altimg="si3.svg"><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top">pool<inline-formula><mml:math id="M15" altimg="si3.svg"><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td valign="top">rf size</td><td valign="top">3</td><td valign="top">5</td><td valign="top">6</td><td valign="top">10</td><td valign="top">14</td><td valign="top">16</td></tr><tr><td valign="top">stride</td><td valign="top">1</td><td valign="top">1</td><td valign="top">2</td><td valign="top">2</td><td valign="top">2</td><td valign="top">4</td></tr><tr><td valign="top">layer</td><td valign="top">conv3<inline-formula><mml:math id="M16" altimg="si4.svg"><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">conv3<inline-formula><mml:math id="M17" altimg="si3.svg"><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">conv3<inline-formula><mml:math id="M18" altimg="si5.svg"><mml:msub><mml:mrow/><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">pool<inline-formula><mml:math id="M19" altimg="si5.svg"><mml:msub><mml:mrow/><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">conv4<inline-formula><mml:math id="M20" altimg="si4.svg"><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">conv4<inline-formula><mml:math id="M21" altimg="si3.svg"><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></td></tr><tr><td valign="top">rf size</td><td valign="top">24</td><td valign="top">32</td><td valign="top">40</td><td valign="top">44</td><td valign="top">60</td><td valign="top">76</td></tr><tr><td valign="top">stride</td><td valign="top">4</td><td valign="top">4</td><td valign="top">4</td><td valign="top">8</td><td valign="top">8</td><td valign="top">8</td></tr><tr><td valign="top">layer</td><td valign="top">conv4<inline-formula><mml:math id="M22" altimg="si5.svg"><mml:msub><mml:mrow/><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">pool<inline-formula><mml:math id="M23" altimg="si6.svg"><mml:msub><mml:mrow/><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">conv5<inline-formula><mml:math id="M24" altimg="si4.svg"><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">conv5<inline-formula><mml:math id="M25" altimg="si3.svg"><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">conv5<inline-formula><mml:math id="M26" altimg="si5.svg"><mml:msub><mml:mrow/><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula></td><td valign="top">pool5</td></tr><tr><td valign="top">rf size</td><td valign="top">92</td><td valign="top">100</td><td valign="top">132</td><td valign="top">164</td><td valign="top">196</td><td valign="top">212</td></tr><tr><td valign="top">stride</td><td valign="top">8</td><td valign="top">16</td><td valign="top">16</td><td valign="top">16</td><td valign="top">16</td><td valign="top">32</td></tr></tbody></table></table-wrap><fig id="fig0004"><label>Fig. 4</label><caption><p>A Edge-guidance map is generated by fusing every layer feature to rich edge features. See III. B for more details.</p></caption><alt-text id="at0004">Fig. 4</alt-text><graphic xlink:href="gr4_lrg"/></fig></p>
      <p id="p0021">We use the standard Binary Cross Entropy(BCE) loss function to measure the dissimilarity between the generated Edge-guidance map (<inline-formula><mml:math id="M27" altimg="si41.svg"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula>) and the edge-Ground-truth map (<inline-formula><mml:math id="M28" altimg="si42.svg"><mml:msub><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>) calculated by using the gradient of the ground-truth (GT):<disp-formula id="eq0001"><label>(1)</label><mml:math id="M29" altimg="si43.svg"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>w</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>h</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Where, <inline-formula><mml:math id="M30" altimg="si44.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> the coordinates of each pixel in the Edge-guidance map <inline-formula><mml:math id="M31" altimg="si41.svg"><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> and edge-ground-truth map <inline-formula><mml:math id="M32" altimg="si42.svg"><mml:msub><mml:mi>G</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:math></inline-formula>. In addition, w and h respectively represent the width and height of the corresponding feature map.</p>
    </sec>
    <sec id="sec0006">
      <label>3.3</label>
      <title>Aggregation module</title>
      <p id="p0022">Many existing medical image segmentation networks <xref rid="bib0019" ref-type="bibr">[19]</xref>, <xref rid="bib0020" ref-type="bibr">[20]</xref>, use high and low level features in encoders to segment target organs and lesion regions. However, Wu et al. <xref rid="bib0037" ref-type="bibr">[35]</xref> pointed out that, compared with high-level features, low-level features require more computing resources due to their greater spatial resolution, but contribute less to performance. Inspired by this, we using a <inline-formula><mml:math id="M33" altimg="si45.svg"><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="M34" altimg="si46.svg"><mml:mrow><mml:mi>P</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="M35" altimg="si47.svg"><mml:mrow><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="M36" altimg="si48.svg"><mml:mrow><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> modules to aggregate high-layers features. Specifically, for the input 2d CT images, we first obtain three high-layers features <inline-formula><mml:math id="M37" altimg="si49.svg"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>= 3, 4, 5 of the VGG16. Then, we use a novel partial decoder <inline-formula><mml:math id="M38" altimg="si50.svg"><mml:mrow><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>
<xref rid="bib0017" ref-type="bibr">[17]</xref>, <xref rid="bib0037" ref-type="bibr">[35]</xref> to aggregate the high-layers features in a paralleled connection. The decoder produces a global location map <inline-formula><mml:math id="M39" altimg="si37.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> = <inline-formula><mml:math id="M40" altimg="si51.svg"><mml:mrow><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, which then serves as an initial guide in the RA module. The details are shown in <xref rid="fig0005" ref-type="fig">Fig. 5</xref>
.<fig id="fig0005"><label>Fig. 5</label><caption><p>Aggregated high-layers features are used to generate a global location map.</p></caption><alt-text id="at0005">Fig. 5</alt-text><graphic xlink:href="gr5_lrg"/></fig></p>
    </sec>
    <sec id="sec0007">
      <label>3.4</label>
      <title>Reverse attention module</title>
      <p id="p0023">In clinical practice, clinicians usually use a two-step operation to segment the lung infection region, first roughly locate the infected region, and then accurately mark these region by examining the local tissue structure. Inspired by this process and <xref rid="bib0024" ref-type="bibr">[24]</xref>, we designed GFNet using five reverse attention modules(RA).</p>
      <p id="p0024">First, the <inline-formula><mml:math id="M41" altimg="si52.svg"><mml:mrow><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> module acts as a rough locator and generates a high-level global map <inline-formula><mml:math id="M42" altimg="si37.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> with unstructured details to provide rough location information for the lung infection region.</p>
      <p id="p0025">Secondly, our structure mine discriminative infection regions in an erasing manner <xref rid="bib0026" ref-type="bibr">[26]</xref>, <xref rid="bib0027" ref-type="bibr">[27]</xref>. Specifically, compared with simply aggregating features from all layers <xref rid="bib0027" ref-type="bibr">[27]</xref>,we propose to adaptively learn the reverse attention in all five layers features. Importing <inline-formula><mml:math id="M43" altimg="si37.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M44" altimg="si38.svg"><mml:msub><mml:mi>f</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> into a RA, meanwhile, under the guidance of <inline-formula><mml:math id="M45" altimg="si29.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula>. The specific approach is as follows: <inline-formula><mml:math id="M46" altimg="si53.svg"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> goes through <inline-formula><mml:math id="M47" altimg="si54.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:math></inline-formula> and reverse to get <inline-formula><mml:math id="M48" altimg="si55.svg"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> with reverse attention weight, and then <inline-formula><mml:math id="M49" altimg="si55.svg"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is multiplied with the feature <inline-formula><mml:math id="M50" altimg="si56.svg"><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the lower layer. Before getting the RA feature, we concatenate Eg with the result multiplied, so as to get <inline-formula><mml:math id="M51" altimg="si57.svg"><mml:msub><mml:mi>R</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math></inline-formula> feature under the guidance of <inline-formula><mml:math id="M52" altimg="si29.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula>. Finally, we add <inline-formula><mml:math id="M53" altimg="si58.svg"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="M54" altimg="si53.svg"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to get lateral output <inline-formula><mml:math id="M55" altimg="si40.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of layer <inline-formula><mml:math id="M56" altimg="si59.svg"><mml:mi>i</mml:mi></mml:math></inline-formula>. For each lateral output <inline-formula><mml:math id="M57" altimg="si40.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, we conduct deep supervised learning. The output <inline-formula><mml:math id="M58" altimg="si30.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M59" altimg="si39.svg"><mml:msub><mml:mi>f</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula> go to the next RA. Repeating the above operation to get <inline-formula><mml:math id="M60" altimg="si34.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> finally. Formula is as follows:<disp-formula id="eq0002"><label>(2)</label><mml:math id="M61" altimg="si60.svg"><mml:mrow><mml:mi>R</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>down</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Where <inline-formula><mml:math id="M62" altimg="si61.svg"><mml:mrow><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mo>·</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> represents concatenate, <inline-formula><mml:math id="M63" altimg="si62.svg"><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:mo>·</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> represents down sampling, followed by two two-dimensional convolution layers with 64 filters. In fact, RA weights <inline-formula><mml:math id="M64" altimg="si55.svg"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> widely adopted design in the salient object detection in computer vision <xref rid="bib0027" ref-type="bibr">[27]</xref>, it is defined as:<disp-formula id="eq0003"><label>(3)</label><mml:math id="M65" altimg="si63.svg"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>Θ</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>Sig</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>Among them <inline-formula><mml:math id="M66" altimg="si64.svg"><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> represents the upsampling operation, <inline-formula><mml:math id="M67" altimg="si65.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula> represents activate function for Sigmoid, <inline-formula><mml:math id="M68" altimg="si66.svg"><mml:mstyle mathvariant="normal"><mml:mi>Θ</mml:mi></mml:mstyle></mml:math></inline-formula> represents the reverse operation of subtracting the input from a matrix of all 1s. The symbol <inline-formula><mml:math id="M69" altimg="si67.svg"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow></mml:math></inline-formula> represents the extension of the single-channel feature map to multi-channels, including the reversion of each channel of the candidate tensors in <xref rid="eq0002" ref-type="disp-formula">Eq. (2)</xref>. The final output of this layer <inline-formula><mml:math id="M70" altimg="si40.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is:<disp-formula id="eq0004"><label>(4)</label><mml:math id="M71" altimg="si68.svg"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>R</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>The details of this process are shown in <xref rid="fig0006" ref-type="fig">Fig. 6</xref>
. It is worth noting that erasure strategies in RA can ultimately refine inaccurate and coarse prediction region into an accurate and complete prediction map.<fig id="fig0006"><label>Fig. 6</label><caption><p>The reverse attention module is used to implicitly learn edge features.</p></caption><alt-text id="at0006">Fig. 6</alt-text><graphic xlink:href="gr6_lrg"/></fig></p>
      <p id="p0026">In <xref rid="bib0017" ref-type="bibr">[17]</xref>, RA module is also used in Res2Net to extract features from layer 5 to layer 3. We differ from them in that here we apply the RA module to each layer and demonstrate that our approach is better in subsequent ablation experiments.</p>
    </sec>
    <sec id="sec0008">
      <label>3.5</label>
      <title>Loss function</title>
      <p id="p0027">We define the loss function <inline-formula><mml:math id="M72" altimg="si69.svg"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> as the combination of the weighted <inline-formula><mml:math id="M73" altimg="si70.svg"><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi></mml:mrow></mml:math></inline-formula> loss function <inline-formula><mml:math id="M74" altimg="si71.svg"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msubsup></mml:math></inline-formula> and the weighted binary cross entropy (BCE) loss function <inline-formula><mml:math id="M75" altimg="si72.svg"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msubsup></mml:math></inline-formula>, namely:<disp-formula id="eq0005"><label>(5)</label><mml:math id="M76" altimg="si73.svg"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">seg</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msubsup><mml:mo linebreak="goodbreak">+</mml:mo><mml:mi>λ</mml:mi><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula>Where <inline-formula><mml:math id="M77" altimg="si74.svg"><mml:mi>λ</mml:mi></mml:math></inline-formula> represents the weight, which was set to 1 in our experiment. The two parts of <inline-formula><mml:math id="M78" altimg="si69.svg"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> provide effective global (image-level) and local (pixel-level) supervision to obtain accurate segmentation effects. Unlike the standard IoU loss function, which is widely used in the segmentation task, the weighted IoU loss function increases the weights at the sample points of the difficult pixel to highlight its importance. In addition, compared with the standard BCE loss function, <inline-formula><mml:math id="M79" altimg="si72.svg"><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:msubsup></mml:math></inline-formula> pays more attention to the difficult pixel sample points rather than assigning the same weight to all pixels. The definitions of these losses are the same as those in <xref rid="bib0033" ref-type="bibr">[31]</xref>, <xref rid="bib0034" ref-type="bibr">[32]</xref>, and their effectiveness has been validated in target detection domain. We adopt deep supervision for the five lateral outputs (<inline-formula><mml:math id="M80" altimg="si75.svg"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) and the global location map <inline-formula><mml:math id="M81" altimg="si37.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula>, each of which is upsampled to the same size as the object-level segmented ground-truth map <inline-formula><mml:math id="M82" altimg="si76.svg"><mml:mi>G</mml:mi></mml:math></inline-formula>. Therefore, the total loss function is:<disp-formula id="eq0006"><label>(6)</label><mml:math id="M83" altimg="si77.svg"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext>total</mml:mtext><mml:mspace width="4.pt"/></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">seg</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:munderover><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></disp-formula>
</p>
    </sec>
  </sec>
  <sec id="sec0009">
    <label>4</label>
    <title>Experiment</title>
    <sec id="sec0010">
      <label>4.1</label>
      <title>Experimental environment and parameter setting</title>
      <p id="p0028">Our model was configured with the PyTorch 1.10.0, cuda 11.6 framework and trained on a single NVIDIA RTX 3080 laptop GPU. Before the training, we uniformly resize all the inputs to 352 <inline-formula><mml:math id="M84" altimg="si78.svg"><mml:mo>×</mml:mo></mml:math></inline-formula> 352, and a multi-scale training method with a scaling ratio of [0.75, 1, 1.25] was used to train the datasets, so as to improve the generalization performance of the model. The network is trained using the Adam optimizer. Set the Epoch to 100 with a batch size of 4 and the learning rate to 1e-4.</p>
    </sec>
    <sec id="sec0011">
      <label>4.2</label>
      <title>Baselines</title>
      <p id="p0029">For the infected region experiment, we applied the GFNet framework of this paper to VGG16. We first identify the best value for <inline-formula><mml:math id="M85" altimg="si79.svg"><mml:mi>n</mml:mi></mml:math></inline-formula> of <inline-formula><mml:math id="M86" altimg="si80.svg"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. And compared with two classic segmentation models in the medical field, including U-Net <xref rid="bib0012" ref-type="bibr">[12]</xref>, U-Net++ <xref rid="bib0013" ref-type="bibr">[13]</xref>, the latest model Inf-Net <xref rid="bib0017" ref-type="bibr">[17]</xref>, and two segmentation models “Few-shot UNet” <xref rid="bib0025" ref-type="bibr">[25]</xref> and “SG-One” <xref rid="bib0042" ref-type="bibr">[39]</xref> based on few-shot learning.</p>
    </sec>
    <sec id="sec0012">
      <label>4.3</label>
      <title>Datasets</title>
      <p id="p0030">Currently, the number of CT images with segmentation labeling is very limited because manually segmenting areas of lung infection is a difficult and time-consuming task, and the outbreak of the disease is in its early stages. To solve this problem, we use a semi-supervised learning strategy to improve GFNet, using a large number of unlabeled CT images to effectively expand the training data. This strategy refers to the method in <xref rid="bib0017" ref-type="bibr">[17]</xref> and is based on the random sampling strategy to gradually expand the unlabeled data of the training dataset. See <xref rid="fig0010" ref-type="fig">Algorithm 1</xref>
<xref rid="bib0017" ref-type="bibr">[17]</xref> for details. Specifically, we generate the pseudo labels for unlabeled CT images using the procedure described in <xref rid="fig0010" ref-type="fig">Algorithm 1</xref>. The resulting CT images with pseudo labels are then utilized to train our model. This semi-supervised approach is simple, it requires no measures to evaluate the labels of predictions, and it has no threshold. Secondly, this strategy can provide better performance than other semi-supervised learning methods and prevent network overfitting. Recent research work <xref rid="bib0036" ref-type="bibr">[34]</xref> has confirmed this conclusion. We use the same training settings as in <xref rid="bib0017" ref-type="bibr">[17]</xref>. We generate pseudo labels for unlabeled CT images using the protocol described in <xref rid="fig0010" ref-type="fig">Algorithm 1</xref>
<xref rid="bib0017" ref-type="bibr">[17]</xref>. The number of randomly selected CT images is set to 5, i.e., K = 5. Now there are 1600 unlabeled images, we need to perform 320 iterations with a batch size of 4. The unlabeled CT images are extracted from the COVID-19 CT Collection <xref rid="bib0035" ref-type="bibr">[33]</xref> dataset, which consists of 20 3D CT volumes from different COVID-19 patients. Fan et al. <xref rid="bib0017" ref-type="bibr">[17]</xref> had extracted 1600 2d CT axial slices from 3D volumes, removed non-lung regions, and constructed an unlabeled training dataset <inline-formula><mml:math id="M87" altimg="si81.svg"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for effective semi supervised segmentation. After obtaining a semi-supervised dataset with pseudo labels, our next training phase consists of two steps: (i) Pre-training on 1600 CT images with pseudo labels, which takes 7.5 h to converge over 100 epochs with a batch size of 4. (ii) Fine tuning on 60 CT images with the ground-truth labels, which takes about 15 min to converge over 100 epochs with a batch size of 4. These 60 CT images were taken from COVID-19 CT Segmentation dataset <xref rid="bib0029" ref-type="bibr">[29]</xref>, which consists of 100 axial CT images collected by the Italian Society of Medical and Interventional Radiology. A radiologist segmented the CT images using different labels to identify lung infections. Although this is the first open-access COVID-19 dataset for lung infection segmentation, it suffers from a small sample size, i.e., only 100 labeled images are available. We employ them as the labeled data <inline-formula><mml:math id="M88" altimg="si82.svg"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which consists of 60 CT images randomly selected as training samples, 20 CT images for validation, and the remaining 20 images for testing. We verify the model’s learning ability on this testing dataset. In addition, we found a public dataset “COVID-CS” <xref rid="bib0030" ref-type="bibr">[30]</xref> and tested the trained model directly on this dataset to verify our model’s generalization ability to “never seen” data. For a fair comparison, we adopt the same training mode and setting for the baseline model.<fig id="fig0010"><label>Algorithm 1</label><caption><p> </p></caption><alt-text id="at0010">Algorithm 1</alt-text><graphic xlink:href="gr10_lrg"/></fig></p>
    </sec>
    <sec id="sec0013">
      <label>4.4</label>
      <title>Evaluation metrics</title>
      <p id="p0031">According to Han et al. <xref rid="bib0005" ref-type="bibr">[5]</xref>, Huang et al. <xref rid="bib0026" ref-type="bibr">[26]</xref>, we used three widely used metrics: Dice similarity coefficient, Sensitivity (Sen.), Specificity (Spec.) and Precision (Prec.). We also introduce three golden indicators from the object detection field, including Structure Measure <xref rid="bib0027" ref-type="bibr">[27]</xref>, Enhanced-alignment Measure <xref rid="bib0028" ref-type="bibr">[28]</xref>, and Mean Absolute Error. In our evaluation, we choose <inline-formula><mml:math id="M89" altimg="si83.svg"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with Sigmoid function as the final prediction <inline-formula><mml:math id="M90" altimg="si84.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula>. The seven metrics used to measure the performance of a model can be expressed as:</p>
      <sec id="sec0014">
        <label>4.4.1</label>
        <title>Dice coefficient</title>
        <p id="p0032">Dice is mainly used to calculate the similarity of two sets, and its definition is as follows:<disp-formula id="eq0007"><label>(7)</label><mml:math id="M91" altimg="si85.svg"><mml:mrow><mml:mspace width="4.pt"/><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mspace width="4.pt"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mo>*</mml:mo></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mi>Seg</mml:mi><mml:mo>∩</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>Seg</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
      </sec>
      <sec id="sec0015">
        <label>4.4.2</label>
        <title>Sensitivity (<inline-formula><mml:math id="M92" altimg="si86.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>)</title>
        <p id="p0033">Sen. reflects the percentage of lung infections that were correctly segmented. It is defined as follows:<disp-formula id="eq0008"><label>(8)</label><mml:math id="M93" altimg="si87.svg"><mml:mrow><mml:mspace width="4.pt"/><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mspace width="4.pt"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>Seg</mml:mi><mml:mo>∩</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p>
      </sec>
      <sec id="sec0016">
        <label>4.4.3</label>
        <title>Specificity (<inline-formula><mml:math id="M94" altimg="si88.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula>)</title>
        <p id="p0034">Spec. reflects the percentage of lung non-infections that were correctly segmented. It is defined as follows:<disp-formula id="eq0009"><label>(9)</label><mml:math id="M95" altimg="si89.svg"><mml:mrow><mml:mspace width="4.pt"/><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>.</mml:mo></mml:mrow><mml:mspace width="4.pt"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>Seg</mml:mi><mml:mo>∪</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mi>T</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>Among them is the pixel set of the entire CT.</p>
      </sec>
      <sec id="sec0017">
        <label>4.4.4</label>
        <title>Structural measure (<inline-formula><mml:math id="M96" altimg="si9.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula>):</title>
        <p id="p0035">This metric is more consistent with the human visual system and is used to measure the structural similarity between the prediction map and the ground-truth mask:<disp-formula id="eq0010"><label>(10)</label><mml:math id="M97" altimg="si90.svg"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>Seg</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">+</mml:mo><mml:mi>α</mml:mi><mml:mo linebreak="goodbreak">×</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>Seg</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Where <inline-formula><mml:math id="M98" altimg="si91.svg"><mml:mi>α</mml:mi></mml:math></inline-formula> is the balance factor used to control object-aware similarity <inline-formula><mml:math id="M99" altimg="si92.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> and region-aware similarity <inline-formula><mml:math id="M100" altimg="si93.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula>. In this paper, we set the same metric value as the original text with the default setting (<inline-formula><mml:math id="M101" altimg="si91.svg"><mml:mi>α</mml:mi></mml:math></inline-formula>=0.5).</p>
      </sec>
      <sec id="sec0018">
        <label>4.4.5</label>
        <title>Enhanced-alignment measure (<inline-formula><mml:math id="M102" altimg="si10.svg"><mml:msubsup><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>)</title>
        <p id="p0036">This is a recently proposed metric that measures both local and global similarity between two binary graphs. The formula is as follows:<disp-formula id="eq0011"><label>(11)</label><mml:math id="M103" altimg="si94.svg"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:munderover><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>Where w and h represent the width and height of ground-truth map G, and (x, y) represents the coordinates of each pixel in G. The symbol <inline-formula><mml:math id="M104" altimg="si95.svg"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> is an enhanced alignment matrix. We get a set of <inline-formula><mml:math id="M105" altimg="si96.svg"><mml:msub><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> value by thresholding the prediction <inline-formula><mml:math id="M106" altimg="si84.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula> with thresholds 0 to 255 to obtain a binary mask. In our experiment, we reported the mean of <inline-formula><mml:math id="M107" altimg="si96.svg"><mml:msub><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> under all thresholds.</p>
      </sec>
      <sec id="sec0019">
        <label>4.4.6</label>
        <title>Mean absolute error (<inline-formula><mml:math id="M108" altimg="si28.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula>)</title>
        <p id="p0037">This metric measures the pixel-level error between <inline-formula><mml:math id="M109" altimg="si84.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M110" altimg="si76.svg"><mml:mi>G</mml:mi></mml:math></inline-formula>, and is defined as:<disp-formula id="eq0012"><label>(12)</label><mml:math id="M111" altimg="si97.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>w</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>h</mml:mi></mml:munderover><mml:mrow><mml:mo>|</mml:mo><mml:mi>Seg</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p>
      </sec>
    </sec>
    <sec id="sec0020">
      <label>4.5</label>
      <title>The determination of the optimal value of n</title>
      <p id="p0038">We first conducted experiments in different <inline-formula><mml:math id="M112" altimg="si79.svg"><mml:mi>n</mml:mi></mml:math></inline-formula> on the “COVID-19 CT Segmentation” dataset <xref rid="bib0029" ref-type="bibr">[29]</xref>. The results can be seen from <xref rid="fig0007" ref-type="fig">Fig. 7</xref>
. Some metrics, such as <inline-formula><mml:math id="M113" altimg="si25.svg"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M114" altimg="si26.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M115" altimg="si98.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M116" altimg="si10.svg"><mml:msubsup><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>,will increase with the number of fusion feature layers increasing. So we set the best value of n to 5, that is, the final output is <inline-formula><mml:math id="M117" altimg="si99.svg"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. Through this step experiment, we proved that the results combinated by all the five lateral output will have the best performance.<fig id="fig0007"><label>Fig. 7</label><caption><p>The value of n corresponds to the data change of [<inline-formula><mml:math id="M118" altimg="si1.svg"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula>].</p></caption><alt-text id="at0007">Fig. 7</alt-text><graphic xlink:href="gr7_lrg"/></fig></p>
    </sec>
    <sec id="sec0021">
      <label>4.6</label>
      <title>Verify the validity of data sets with pseudo labels</title>
      <p id="p0039">Our training process is divided into two steps: first, pre-training on 1600 training sets with pseudo labels, and then fine-tuning with 60 images with ground-truth labels. In this part, we first verify the validity of the pseudo-label dataset to ensure that it is helpful to the learning process of the model. The results are shown in <xref rid="tbl0002" ref-type="table">Table 2</xref>
. It can be seen from the above results that there is a certain gap between the test result after training with only the training set with pseudo-label and the test result after fine-tuning with ground-truth label, indicating that training with only the pseudo label data set is not enough. However, if only ground-truth labels are used for training (“0+60”), due to the small sample size, the final result is inferior to pseudo-label data set in some evaluation metrics such as Spe.. Therefore, we refer to the semi-supervised training method in Inf-Net <xref rid="bib0017" ref-type="bibr">[17]</xref>, which firstly used pseudo-label data set for pre-training, and then used images with ground-truth labels for fine-tuning, so as to ensure sufficient training sample size and achieve the best performance.<table-wrap position="float" id="tbl0002"><label>Table 2</label><caption><p>1600 training sets with pseudo-labels were trained and tested directly on the “COVID-19 CT Segmentation” dataset <xref rid="bib0029" ref-type="bibr">[29]</xref> to verify the validity of the dataset.</p></caption><alt-text id="at0012">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Method</th><th align="left" valign="top">Dice</th><th align="left" valign="top">Sen.</th><th align="left" valign="top">Spe.</th><th align="left" valign="top"><inline-formula><mml:math id="M119" altimg="si7.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M120" altimg="si8.svg"><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left" valign="top">MAE</th></tr></thead><tbody><tr><td valign="top">U-Net</td><td valign="top">0.682</td><td valign="top">0.607</td><td valign="top">0.972</td><td valign="top">0.710</td><td valign="top">0.842</td><td valign="top">0.084</td></tr><tr><td valign="top">U-Net+</td><td valign="top">0.584</td><td valign="top">0.608</td><td valign="top">0.845</td><td valign="top">0.625</td><td valign="top">0.796</td><td valign="top">0.117</td></tr><tr><td valign="top">Inf-Net</td><td valign="top">0.689</td><td valign="top">0.630</td><td valign="top">0.970</td><td valign="top">0.733</td><td valign="top">0.853</td><td valign="top">0.070</td></tr><tr><td valign="top">GFnet(ours)</td><td valign="top">0.697</td><td valign="top">0.613</td><td valign="top">0.976</td><td valign="top">0.723</td><td valign="top">0.858</td><td valign="top">0.069</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec0022">
      <label>4.7</label>
      <title>Experimental results and analysis</title>
      <sec id="sec0023">
        <label>4.7.1</label>
        <title>Quantitative analysis</title>
        <p id="p0040">In order to compare the performance of infected region segmentation, the existing segmentation models as U-Net, U-Net++ and Inf-Net were added. In addition, we have added two algorithms for few-shot learning field: Few-shot U-Net and SG-One. The quantitative results in datasets <xref rid="bib0029" ref-type="bibr">[29]</xref> and <xref rid="bib0030" ref-type="bibr">[30]</xref> are shown in <xref rid="tbl0003" ref-type="table">Tables 3</xref>
and <xref rid="tbl0004" ref-type="table">4</xref>
. For each model, we used the same training method. We first conducted experiments to observe the influence of training sets of different sizes on the performance of the models. The specific approach is: we divide 1600 pseudo-label training sets into different sizes: [0,400,800,1200,1600]. We train the model on training sets of different sizes. Then we fine-tune models on 60 CT images with the ground-truth labels. In general, the experimental results of the traditional medical segmentation models U-Net and U-Net++ are mediocre. The GFNet proposed in this paper performs the best in many metrics. For the few-shot U-Net, which is based on U-Net and uses an online learning paradigm to further improve the segmentation ability of traditional U-Net for COVID-19. Its performance is improved over traditional U-Net. But for SG-One, another few-shot learning method, it was only moderately effective. Compared with the latest COVID-19 segmentation model and our GFNet, the experimental results of these two methods are not outstanding. Under the influence of training sets of different sizes, the performance of existing methods is gradually enhanced with the increase of data sets. We think this is because as the training set increases, more and more different knowledge is learned by the model. For GFNet, with the increase of training set, the performance of GFNet is slightly improved. This shows that GFNet not only has a good learning ability, but also can achieve good performance when training samples are small. This is because “Edge-guidance” in GFNet is a module with strong robustness and universality. It can learn the boundary features of the target region at a fast speed and with a small sample size, and expand it to the segmentation result. In the dataset2 <xref rid="bib0030" ref-type="bibr">[30]</xref>, our GFNet continues to perform best on almost every metric. This means that our GFNet performs better on “never seen” dataset. The universality of Eg to boundary features in different data sets is verified again. Indicating that it has greater generalization ability overall.<table-wrap position="float" id="tbl0003"><label>Table 3</label><caption><p>Quantitative results of segmentation of infected region on the “COVID-19 CT Segmentation” dataset <xref rid="bib0029" ref-type="bibr">[29]</xref>. The table shows the effect of training sets of different sizes on the performance of each model. The best of each evaluation metric is marked in <bold>bold</bold> and the second best in <italic>italic</italic>.</p></caption><alt-text id="at0013">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Method</th><th align="left" valign="top">Training set size (Pseudo label training + Ground-truth label Finetuning)</th><th align="left" valign="top">Dice</th><th align="left" valign="top">Sen.</th><th align="left" valign="top">Spec.</th><th align="left" valign="top"><inline-formula><mml:math id="M121" altimg="si9.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M122" altimg="si10.svg"><mml:msubsup><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></th><th align="left" valign="top">MAE</th></tr></thead><tbody><tr><td/><td valign="top"><inline-formula><mml:math id="M123" altimg="si11.svg"><mml:mrow><mml:mn>0</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.632</td><td valign="top">0.653</td><td valign="top">0.917</td><td valign="top">0.762</td><td valign="top"><inline-formula><mml:math id="M124" altimg="si12.svg"><mml:mrow><mml:mn>0.880</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.083</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M125" altimg="si13.svg"><mml:mrow><mml:mn>400</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.651</td><td valign="top">0.702</td><td valign="top">0.937</td><td valign="top"><inline-formula><mml:math id="M126" altimg="si14.svg"><mml:mrow><mml:mn>0.765</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.892</td><td valign="top">0.076</td></tr><tr><td valign="top">U-Net</td><td valign="top"><inline-formula><mml:math id="M127" altimg="si15.svg"><mml:mrow><mml:mn>800</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.675</td><td valign="top"><inline-formula><mml:math id="M128" altimg="si16.svg"><mml:mrow><mml:mn>0.693</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M129" altimg="si17.svg"><mml:mrow><mml:mn>0.930</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.764</td><td valign="top">0.894</td><td valign="top">0.078</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M130" altimg="si18.svg"><mml:mrow><mml:mn>1200</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.682</td><td valign="top">0.687</td><td valign="top">0.949</td><td valign="top">0.763</td><td valign="top">0.892</td><td valign="top"><inline-formula><mml:math id="M131" altimg="si19.svg"><mml:mrow><mml:mn>0.078</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M132" altimg="si20.svg"><mml:mrow><mml:mn>1600</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.691</td><td valign="top">0.720</td><td valign="top"><italic>0.965</italic></td><td valign="top">0.775</td><td valign="top">0.891</td><td valign="top">0.076</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M133" altimg="si11.svg"><mml:mrow><mml:mn>0</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.645</td><td valign="top">0.659</td><td valign="top">0.921</td><td valign="top">0.759</td><td valign="top">0.881</td><td valign="top">0.079</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M134" altimg="si13.svg"><mml:mrow><mml:mn>400</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.662</td><td valign="top">0.673</td><td valign="top">0.934</td><td valign="top"><inline-formula><mml:math id="M135" altimg="si21.svg"><mml:mrow><mml:mn>0.769</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.899</td><td valign="top">0.078</td></tr><tr><td valign="top">Few-shot U-Net</td><td valign="top">800+60</td><td valign="top">0.683</td><td valign="top"><inline-formula><mml:math id="M136" altimg="si22.svg"><mml:mrow><mml:mn>0.695</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.945</td><td valign="top">0.772</td><td valign="top">0.901</td><td valign="top">0.075</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M137" altimg="si18.svg"><mml:mrow><mml:mn>1200</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.701</td><td valign="top">0.713</td><td valign="top">0.946</td><td valign="top">0.762</td><td valign="top">0.898</td><td valign="top">0.076</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M138" altimg="si20.svg"><mml:mrow><mml:mn>1600</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.717</td><td valign="top">0.720</td><td valign="top">0.952</td><td valign="top">0.775</td><td valign="top">0.904</td><td valign="top">0.073</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M139" altimg="si11.svg"><mml:mrow><mml:mn>0</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.594</td><td valign="top">0.589</td><td valign="top">0.882</td><td valign="top">0.643</td><td valign="top">0.793</td><td valign="top">0.129</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M140" altimg="si13.svg"><mml:mrow><mml:mn>400</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.611</td><td valign="top">0.603</td><td valign="top">0.890</td><td valign="top">0.658</td><td valign="top">0.812</td><td valign="top">0.114</td></tr><tr><td valign="top">SG-One</td><td valign="top"><inline-formula><mml:math id="M141" altimg="si15.svg"><mml:mrow><mml:mn>800</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.625</td><td valign="top"><inline-formula><mml:math id="M142" altimg="si23.svg"><mml:mrow><mml:mn>0.616</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M143" altimg="si24.svg"><mml:mrow><mml:mn>0.913</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.671</td><td valign="top">0.824</td><td valign="top">0.098</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M144" altimg="si18.svg"><mml:mrow><mml:mn>1200</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.632</td><td valign="top">0.639</td><td valign="top">0.927</td><td valign="top">0.685</td><td valign="top">0.821</td><td valign="top">0.087</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M145" altimg="si20.svg"><mml:mrow><mml:mn>1600</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.640</td><td valign="top">0.635</td><td valign="top">0.936</td><td valign="top">0.693</td><td valign="top">0.834</td><td valign="top">0.079</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M146" altimg="si11.svg"><mml:mrow><mml:mn>0</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.607</td><td valign="top">0.692</td><td valign="top">0.898</td><td valign="top">0.681</td><td valign="top">0.767</td><td valign="top">0.125</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M147" altimg="si13.svg"><mml:mrow><mml:mn>400</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.603</td><td valign="top">0.642</td><td valign="top">0.894</td><td valign="top">0.633</td><td valign="top">0.726</td><td valign="top">0.145</td></tr><tr><td valign="top">U-Net+</td><td valign="top"><inline-formula><mml:math id="M148" altimg="si15.svg"><mml:mrow><mml:mn>800</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.611</td><td valign="top">0.618</td><td valign="top">0.940</td><td valign="top">0.685</td><td valign="top">0.827</td><td valign="top">0.090</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M149" altimg="si18.svg"><mml:mrow><mml:mn>1200</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.625</td><td valign="top">0.640</td><td valign="top">0.930</td><td valign="top">0.702</td><td valign="top">0.830</td><td valign="top">0.098</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M150" altimg="si20.svg"><mml:mrow><mml:mn>1600</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.623</td><td valign="top">0.638</td><td valign="top">0.935</td><td valign="top">0.693</td><td valign="top">0.835</td><td valign="top">0.076</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M151" altimg="si11.svg"><mml:mrow><mml:mn>0</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.699</td><td valign="top">0.715</td><td valign="top">0.928</td><td valign="top">0.782</td><td valign="top">0.862</td><td valign="top">0.078</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M152" altimg="si13.svg"><mml:mrow><mml:mn>400</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.727</td><td valign="top">0.721</td><td valign="top">0.960</td><td valign="top"><bold>0.793</bold></td><td valign="top">0.901</td><td valign="top">0.065</td></tr><tr><td valign="top">Inf-Net</td><td valign="top"><inline-formula><mml:math id="M153" altimg="si15.svg"><mml:mrow><mml:mn>800</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.739</td><td valign="top">0.716</td><td valign="top">0.961</td><td valign="top">0.781</td><td valign="top">0.896</td><td valign="top">0.064</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M154" altimg="si18.svg"><mml:mrow><mml:mn>1200</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.741</td><td valign="top">0.728</td><td valign="top">0.960</td><td valign="top"><italic>0.788</italic></td><td valign="top">0.906</td><td valign="top">0.064</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M155" altimg="si20.svg"><mml:mrow><mml:mn>1600</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.743</td><td valign="top"><bold>0.739</bold></td><td valign="top">0.963</td><td valign="top">0.779</td><td valign="top">0.908</td><td valign="top">0.062</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M156" altimg="si11.svg"><mml:mrow><mml:mn>0</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.739</td><td valign="top"><italic>0.731</italic></td><td valign="top">0.944</td><td valign="top">0.767</td><td valign="top">0.919</td><td valign="top">0.064</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M157" altimg="si13.svg"><mml:mrow><mml:mn>400</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.745</td><td valign="top">0.725</td><td valign="top">0.959</td><td valign="top">0.768</td><td valign="top">0.921</td><td valign="top"><italic>0.061</italic></td></tr><tr><td valign="top">GFNet(ours)</td><td valign="top"><inline-formula><mml:math id="M158" altimg="si15.svg"><mml:mrow><mml:mn>800</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top">0.748</td><td valign="top">0.724</td><td valign="top">0.964</td><td valign="top">0.766</td><td valign="top">0.921</td><td valign="top"><italic>0.061</italic></td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M159" altimg="si18.svg"><mml:mrow><mml:mn>1200</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top"><italic>0.750</italic></td><td valign="top">0.721</td><td valign="top"><bold>0.966</bold></td><td valign="top">0.764</td><td valign="top"><italic>0.922</italic></td><td valign="top">0.062</td></tr><tr><td/><td valign="top"><inline-formula><mml:math id="M160" altimg="si20.svg"><mml:mrow><mml:mn>1600</mml:mn><mml:mo linebreak="goodbreak">+</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula></td><td valign="top"><bold>0.755</bold></td><td valign="top">0.729</td><td valign="top"><bold>0.966</bold></td><td valign="top">0.776</td><td valign="top"><bold>0.926</bold></td><td valign="top"><bold>0.059</bold></td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl0004"><label>Table 4</label><caption><p>Quantitative results of segmentation of infected region on the dataset of “COVID-CS” <xref rid="bib0030" ref-type="bibr">[30]</xref>. The best segmentation results are shown in <bold>bold</bold>.</p></caption><alt-text id="at0014">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Method</th><th align="left" valign="top">Dice</th><th align="left" valign="top">Sen.</th><th align="left" valign="top">Spec.</th><th align="left" valign="top"><inline-formula><mml:math id="M161" altimg="si9.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M162" altimg="si10.svg"><mml:msubsup><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></th><th align="left" valign="top">MAE</th></tr></thead><tbody><tr><td valign="top">U-Net</td><td valign="top">0.602</td><td valign="top">0.665</td><td valign="top">0.964</td><td valign="top">0.719</td><td valign="top">0.883</td><td valign="top">0.050</td></tr><tr><td valign="top">Few-shot U-Net</td><td valign="top">0.611</td><td valign="top"><bold>0.668</bold></td><td valign="top">0.964</td><td valign="top">0.725</td><td valign="top">0.870</td><td valign="top">0.051</td></tr><tr><td valign="top">SG-One</td><td valign="top">0.596</td><td valign="top">0.581</td><td valign="top">0.979</td><td valign="top">0.721</td><td valign="top">0.892</td><td valign="top">0.056</td></tr><tr><td valign="top">U-Net+</td><td valign="top">0.485</td><td valign="top">0.617</td><td valign="top">0.946</td><td valign="top">0.650</td><td valign="top">0.801</td><td valign="top">0.077</td></tr><tr><td valign="top">Inf-Net</td><td valign="top">0.615</td><td valign="top">0.565</td><td valign="top">0.963</td><td valign="top">0.732</td><td valign="top">0.849</td><td valign="top">0.038</td></tr><tr><td valign="top">GFNet(ours)</td><td valign="top"><bold>0.663</bold></td><td valign="top">0.605</td><td valign="top"><bold>0.981</bold></td><td valign="top"><bold>0.743</bold></td><td valign="top"><bold>0.895</bold></td><td valign="top"><bold>0.033</bold></td></tr></tbody></table></table-wrap></p>
        <p id="p0041">In addition, we also show the training and computational costs of each model in <xref rid="tbl0005" ref-type="table">Table 5</xref>
. In real-world applications, the accuracy of a deep learning model is important. Not only that, its training cost, test speed and other performance are also taken seriously. As can be seen from the table, on a training set of 1600 images, our GFNet is slightly slower than U-Net and Inf-Net in terms of training time. This is due to the relative complexity of our model. However, it is worth mentioning that GFNet can achieve good performance only by relying on a small number of training sets, which greatly saves the training time of the model and has great potential. U-Net++ has a very long training time, because when we were training U-Net++, on the same GPU, the batchsize was set to be smaller in case of insufficient video memory. In terms of test speed, we counted the time taken for a test set of 20 images. Our GFNet is second only to U-Net in testing speed by a small margin. To sum up, in real world case decision making, our GFNet is fully capable of such tasks if it is put into application under time constraints or with few training samples. If there is enough time or a large training sample, our GFNet can be sufficiently trained to achieve the best results.<table-wrap position="float" id="tbl0005"><label>Table 5</label><caption><p>The table shows the size of each model and the computational cost for training and testing. The training time was conducted on the training set of 1600 images. The testing time was conducted on a test set of 20 images.</p></caption><alt-text id="at0015">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Method</th><th align="left" valign="top">Backbone</th><th align="left" valign="top">Param</th><th align="left" valign="top">FLOP</th><th align="left" valign="top">Training time</th><th align="left" valign="top">Testing speed</th></tr></thead><tbody><tr><td valign="top">U-Net</td><td valign="top">VGG16</td><td valign="top">7.9M</td><td valign="top">38.116G</td><td valign="top">7.2h</td><td valign="top">1.51s</td></tr><tr><td valign="top">U-Net+</td><td valign="top">VGG16</td><td valign="top">9.2M</td><td valign="top">65.938G</td><td valign="top">26.7h</td><td valign="top">3.93s</td></tr><tr><td valign="top">SG-One</td><td valign="top">VGG16</td><td valign="top">19.0M</td><td valign="top">45.916G</td><td valign="top">7.3h</td><td valign="top">1.77s</td></tr><tr><td valign="top">Inf-Net</td><td valign="top">Res2Net</td><td valign="top">33.1M</td><td valign="top">13.922G</td><td valign="top">5.3h</td><td valign="top">1.92s</td></tr><tr><td valign="top">GFNet(ours)</td><td valign="top">VGG16</td><td valign="top">18.1M</td><td valign="top">50.827G</td><td valign="top">7.5h</td><td valign="top">1.63s</td></tr><tr><td valign="top">GFNet(6)(ours)</td><td valign="top">VGG16</td><td valign="top">27.4M</td><td valign="top">49.537G</td><td valign="top">7.1h</td><td valign="top">1.69s</td></tr><tr><td valign="top">GFNet(4)(ours)</td><td valign="top">VGG16</td><td valign="top">10.9M</td><td valign="top">59.391G</td><td valign="top">9.8h</td><td valign="top">1.55s</td></tr></tbody></table></table-wrap></p>
        <p id="p0042">Finally, we also examine the effect of the number of the parameters on the GFNet performance. To be specific: we changed the number of parameters by adding and removing one layer from the original GFNet (5 layers). We named the changed models GFNet (6) and GFNet (4). We examine the performance of these two models, and the detailed results are shown in <xref rid="tbl0006" ref-type="table">Table 6</xref>
. From the perspective of model performance, when we add or reduce the number of GFNet layers, the performance of the model is degraded. Generally speaking, the higher the number of parameters of the model, the better the performance will be, but the performance of GFNet (6) is slightly reduced. We suspect that this is because the boundary details of COVID-19 are already very difficult to extract. As the number of layers increases, many details are lost. For deeper structures, simple stacking of layers will lead to network degradation <xref rid="bib0043" ref-type="bibr">[40]</xref>. The last, we show the training and test time of GFNet (6) and GFNet (4) in <xref rid="tbl0005" ref-type="table">Table 5</xref>.<table-wrap position="float" id="tbl0006"><label>Table 6</label><caption><p>Three GFNets which have different number of parameters.their performance in dataset “1600+60”.</p></caption><alt-text id="at0016">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Method</th><th align="left" valign="top">Dice</th><th align="left" valign="top">Sen.</th><th align="left" valign="top">Spec.</th><th align="left" valign="top"><inline-formula><mml:math id="M163" altimg="si9.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M164" altimg="si10.svg"><mml:msubsup><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></th><th align="left" valign="top">MAE</th></tr></thead><tbody><tr><td valign="top">GFNet(6)</td><td valign="top">0.741</td><td valign="top">0.713</td><td valign="top">0.961</td><td valign="top">0.758</td><td valign="top">0.912</td><td valign="top">0.064</td></tr><tr><td valign="top">GFNet(4)</td><td valign="top">0.723</td><td valign="top">0.700</td><td valign="top">0.958</td><td valign="top">0.747</td><td valign="top">0.910</td><td valign="top">0.067</td></tr><tr><td valign="top">GFNet</td><td valign="top">0.755</td><td valign="top">0.729</td><td valign="top">0.966</td><td valign="top">0.776</td><td valign="top">0.926</td><td valign="top">0.059</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec0024">
        <label>4.7.2</label>
        <title>Quantitative analysis</title>
        <p id="p0043">The segmentation results in different datasets of lung infection are shown in <xref rid="fig0008" ref-type="fig">Figs. 8</xref>
and <xref rid="fig0009" ref-type="fig">9</xref>
. It is easy to see that our GFNet is significantly better than the baselines. Specifically, the segmentation results generated by it are close to the ground-truth map, and there are fewer tissue regions wrongly segmented. In contrast, the results given by U-Net, U-Net++ are not satisfactory because some results have a large amount of normal tissue that has not been segmented. Although Inf-Net is much better than both, there are still many regions where the boundaries are blurred. Our GFNet demarcates the region with very clear and precise boundaries.<fig id="fig0008"><label>Fig. 8</label><caption><p>Comparison of visual effects of segmentation of infected regions on the “COVID-19 CT Segmentation” dataset <xref rid="bib0029" ref-type="bibr">[29]</xref>.</p></caption><alt-text id="at0008">Fig. 8</alt-text><graphic xlink:href="gr8_lrg"/></fig><fig id="fig0009"><label>Fig. 9</label><caption><p>Comparison of visual effects of segmentation of infected regions on the “COVID-CS” dataset <xref rid="bib0030" ref-type="bibr">[30]</xref>.</p></caption><alt-text id="at0009">Fig. 9</alt-text><graphic xlink:href="gr9_lrg"/></fig></p>
      </sec>
    </sec>
    <sec id="sec0025">
      <label>4.8</label>
      <title>Ablation experiments</title>
      <p id="p0044">In this Section, we conducted some experiments to verify the core module and steps of GFNet (Edge-guidance module, number of RA module usage (i.e., the best value of <inline-formula><mml:math id="M165" altimg="si59.svg"><mml:mi>i</mml:mi></mml:math></inline-formula> in,<inline-formula><mml:math id="M166" altimg="si55.svg"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M167" altimg="si40.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (<inline-formula><mml:math id="M168" altimg="si59.svg"><mml:mi>i</mml:mi></mml:math></inline-formula> = 1, 2, 3, 4, 5)), as shown in <xref rid="tbl0007" ref-type="table">Table 7</xref>
. The best result of each metric is indicated in red. (The results of the fusion module are given in <xref rid="fig0007" ref-type="fig">Fig. 7</xref>.<table-wrap position="float" id="tbl0007"><label>Table 7</label><caption><p>Ablation experiments of GFNet. The best segmentation results are shown in <bold>bold</bold>.</p></caption><alt-text id="at0017">Table 7</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">No.</th><th colspan="7" align="left">BLOCKS<hr/></th><th align="left" valign="top"><inline-formula><mml:math id="M169" altimg="si25.svg"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M170" altimg="si26.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M171" altimg="si27.svg"><mml:mrow><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M172" altimg="si9.svg"><mml:msub><mml:mi>S</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M173" altimg="si10.svg"><mml:msubsup><mml:mi>E</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M174" altimg="si28.svg"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula></th></tr><tr><th align="left" valign="top"/><th align="left" valign="top">Backbone (VGG16)</th><th align="left" valign="top"><inline-formula><mml:math id="M175" altimg="si29.svg"><mml:mrow><mml:mi>E</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M176" altimg="si30.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M177" altimg="si31.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M178" altimg="si32.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M179" altimg="si33.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"><inline-formula><mml:math id="M180" altimg="si34.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula></th><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/><th align="left" valign="top"/></tr></thead><tbody><tr><td valign="top">1</td><td valign="top"><inline-formula><mml:math id="M181" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td/><td/><td/><td/><td valign="top">0.601</td><td valign="top">0.612</td><td valign="top">0.837</td><td valign="top">0.695</td><td valign="top">0.720</td><td valign="top">0.151</td></tr><tr><td valign="top">2</td><td valign="top"><inline-formula><mml:math id="M182" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M183" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td/><td/><td/><td valign="top">0.643</td><td valign="top">0.654</td><td valign="top">0.863</td><td valign="top">0.725</td><td valign="top">0.759</td><td valign="top">0.127</td></tr><tr><td valign="top">3</td><td valign="top"><inline-formula><mml:math id="M184" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td valign="top"><inline-formula><mml:math id="M185" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td/><td/><td valign="top">0.639</td><td valign="top">0.635</td><td valign="top">0.878</td><td valign="top">0.734</td><td valign="top">0.792</td><td valign="top">0.085</td></tr><tr><td valign="top">4</td><td valign="top"><inline-formula><mml:math id="M186" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M187" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M188" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td/><td/><td valign="top">0.681</td><td valign="top">0.667</td><td valign="top">0.921</td><td valign="top">0.757</td><td valign="top">0.856</td><td valign="top">0.076</td></tr><tr><td valign="top">5</td><td valign="top"><inline-formula><mml:math id="M189" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M190" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td valign="top"><inline-formula><mml:math id="M191" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td/><td valign="top">0.710</td><td valign="top">0.672</td><td valign="top">0.944</td><td valign="top">0.776</td><td valign="top">0.872</td><td valign="top">0.069</td></tr><tr><td valign="top">6</td><td valign="top"><inline-formula><mml:math id="M192" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M193" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td valign="top"><inline-formula><mml:math id="M194" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td valign="top">0.725</td><td valign="top">0.698</td><td valign="top">0.951</td><td valign="top">0.791</td><td valign="top">0.890</td><td valign="top">0.065</td></tr><tr><td valign="top">7</td><td valign="top"><inline-formula><mml:math id="M195" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M196" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td/><td valign="top"><inline-formula><mml:math id="M197" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td valign="top">0.737</td><td valign="top"><bold>0.715</bold></td><td valign="top">0.962</td><td valign="top">0.792</td><td valign="top">0.899</td><td valign="top">0.063</td></tr><tr><td valign="top">8</td><td valign="top"><inline-formula><mml:math id="M198" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td valign="top"><inline-formula><mml:math id="M199" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td/><td/><td/><td/><td valign="top"><inline-formula><mml:math id="M200" altimg="si35.svg"><mml:mi>✓</mml:mi></mml:math></inline-formula></td><td valign="top"><bold>0.748</bold></td><td valign="top">0.714</td><td valign="top"><bold>0.965</bold></td><td valign="top"><bold>0.798</bold></td><td valign="top"><bold>0.915</bold></td><td valign="top"><bold>0.061</bold></td></tr></tbody></table></table-wrap></p>
      <sec id="sec0026">
        <label>4.8.1</label>
        <title>Effectiveness of edge-guidance module</title>
        <p id="p0045">As can be seen from No.1 and No.2, No.3 and No.4, after the addition of Edge-guidance module, many metrics have significantly improved.</p>
      </sec>
      <sec id="sec0027">
        <label>4.8.2</label>
        <title>Effectiveness of RA module</title>
        <p id="p0046">As can be seen from No.1 and No.3, after the RA module is added, many metrics have greatly improved.</p>
      </sec>
      <sec id="sec0028">
        <label>4.8.3</label>
        <title>Effectiveness of the combination of edge-guidance module and RA module</title>
        <p id="p0047">As can be seen from No. 3 and No. 4, when the combination of Eg and RA module is used, many indicators are improved again.</p>
      </sec>
      <sec id="sec0029">
        <label>4.8.4</label>
        <title>Usage times of RA module</title>
        <p id="p0048">In the paper <xref rid="bib0017" ref-type="bibr">[17]</xref>, RA module was used three times, it is not rigorous enough, because we think RA module is a process of fusing high-level information with low-level information. So, as the features start at the deepest <inline-formula><mml:math id="M201" altimg="si30.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> and backward to the <inline-formula><mml:math id="M202" altimg="si34.svg"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>,the information at each layer can be utilized. From No. 4 to No. 8, we can see that with the increase of RA module, the experimental results become better and better.</p>
      </sec>
    </sec>
    <sec id="sec0030">
      <label>4.9</label>
      <title>Deficiencies and future prospects</title>
      <p id="p0049">Although our GFNet is significantly better than the traditional methods in two datasets, there are still some deficiencies. For example, in terms of training speed and testing speed, our GFNet is not the fastest. We will continue to improve GFNet in this area. In the future, we will combine GFNet with more advanced models, and improve the existing modules such as Edge-guidance module and the final fusion module.</p>
    </sec>
  </sec>
  <sec id="sec0031">
    <label>5</label>
    <title>Conclusion</title>
    <p id="p0050">In this paper, we propose a COVID-19 lung CT region segmentation network framework named GFNet. The edge guidance module is added on the basis of the reverse attention module and the aggregation module, which can help the model to capture the boundary information on the fuzzy focus boundary effectively, and then make the boundary segmentation result very accurate. Then, based on the idea of feature fusion, the obtained features are fused. Finally, the model can produce an optimal output. Meanwhile, we proved the problem of the usage times of RA module. In Inf-Net, RA module was used in three deep convolution layers. We proved that the experimental results would be better if RA module was used for each layer. In order to solve the problem of the dataset is too small, we use a semi-supervised approach to extend the training dataset to train GFNet and to prevent overfitting. On the existing two datasets, we verify GFNet’s learning ability and generalization ability respectively, and the experiment results show that GFNet is superior to previous models. The qualitative results also show that our segmentation results are more accurate and clear than other models. At the same time, we conducted experiments with each model on training datasets of different sizes. Experimental results show that for other models, increasing the size of the training set will improve the performance of the model. But our GFNet works well when the training set is small. This is because “Edge-guidance” in GFNet is a module with strong robustness and universality. It can learn the boundary features of the target region at a fast speed and with a small sample size, and expand it to the segmentation result. This shows that our GFNet has great potential in image segmentation with fuzzy boundaries facing other similar targets. Finally, we also show the training and computational costs of each model. In our experiment, the training and testing speed of our model is not the fastest, but our model can also achieve good performance when the training set is relatively small. In real world case decision making, our GFNet is fully capable of such tasks if it is put into application under time constraints or with few training samples. Our GFNet can also be sufficiently trained to achieve maximum performance if there is sufficient time or a large training sample. In the future, we will apply the GFNet framework to other medical image segmentation tasks, such as colonoscopic polyps, cells, etc., or image segmentation in other fields. Therefore, our GFNet has great potential to assist healthcare professionals in medical images segmentation.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0052">We declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work, there is no professional or other personal interest of any nature or kind in any product, service and/or company that could be construed as influencing the position presented in, or the review of, the manuscript entitled.</p>
  </sec>
</body>
<back>
  <ref-list id="bib001">
    <title>References</title>
    <ref id="bib0001">
      <label>1</label>
      <mixed-citation publication-type="other" id="sbref0001">WHO, Coronavirus disease 2019 (COVID-19) situation report-81, 2020, (April 10, 2020)[online] Available: <ext-link ext-link-type="uri" xlink:href="https://www.who.int/docs/defaultsource/coronaviruse/situation-reports/20200410-sitrep-81-covid19.pdf?sfvrsn=ca96eb84_2" id="intrrf0002">https://www.who.int/docs/defaultsource/coronaviruse/situation-reports/20200410-sitrep-81-covid19.pdf?sfvrsn=ca96eb84_2</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0002">
      <label>2</label>
      <mixed-citation publication-type="other" id="sbref0002">WHO, Weekly operational updateon COVID-19, 2021, 12 July 2021 <ext-link ext-link-type="uri" xlink:href="https://www.who.int/publications/m/item/weekly-operational-update-on-covid-19-12-july-2021" id="intrrf0003">https://www.who.int/publications/m/item/weekly-operational-update-on-covid-19-12-july-2021</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0003">
      <label>3</label>
      <element-citation publication-type="journal" id="sbref0003">
        <person-group person-group-type="author">
          <name>
            <surname>Lei</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>CT imaging of the 2019 novel coronavirus (2019-NCOV) pneumonia</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <fpage>200236</fpage>
        <pub-id pub-id-type="pmid">32003646</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0004">
      <label>4</label>
      <element-citation publication-type="journal" id="sbref0004">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Artificial intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <fpage>200905</fpage>
        <pub-id pub-id-type="pmid">32191588</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0005">
      <label>5</label>
      <mixed-citation publication-type="other" id="sbref0005">F.S. Han, et al., Lung infection quantification of COVID-19 in CT images with deep learning, (2020). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2003.04655" id="intrrf0004">arXiv:2003.04655</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0006">
      <label>6</label>
      <element-citation publication-type="journal" id="sbref0006">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China</article-title>
        <source>Lancet</source>
        <volume>395</volume>
        <issue>10223</issue>
        <year>2020</year>
        <fpage>497</fpage>
        <lpage>506</lpage>
        <pub-id pub-id-type="pmid">31986264</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0007">
      <label>7</label>
      <element-citation publication-type="journal" id="sbref0007">
        <person-group person-group-type="author">
          <name>
            <surname>Rubin</surname>
            <given-names>G.D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The role of chest imaging in patient management during the COVID-19 pandemic: a multinational consensus statement from the fleischner society</article-title>
        <source>Radiology</source>
        <year>2020</year>
        <fpage>201365</fpage>
        <pub-id pub-id-type="pmid">32255413</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0008">
      <label>8</label>
      <element-citation publication-type="journal" id="sbref0008">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Review of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for COVID-19</article-title>
        <source>IEEE Rev. Biomed. Eng.</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0009">
      <label>9</label>
      <element-citation publication-type="journal" id="sbref0009">
        <person-group person-group-type="author">
          <name>
            <surname>Ng</surname>
            <given-names>M.Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Imaging profile of the COVID-19 infection: radiologic findings and literature review</article-title>
        <source>Radiol. Cardiothorac. Imaging</source>
        <volume>2</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>E200034</fpage>
        <pub-id pub-id-type="pmid">33778547</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0010">
      <label>10</label>
      <element-citation publication-type="journal" id="sbref0010">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A noise-robust framework for automatic segmentation of COVID-19 pneumonia lesions from CT images</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
      </element-citation>
      <note>
        <p>PP(99):1-1</p>
      </note>
    </ref>
    <ref id="bib0011">
      <label>11</label>
      <element-citation publication-type="book" id="sbref0011">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Richer convolutional features for edge detection</part-title>
        <source>CVPR</source>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="bib0012">
      <label>12</label>
      <element-citation publication-type="book" id="sbref0012">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>U-Net: convolutional networks for biomedical image segmentation</part-title>
        <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>
        <year>2015</year>
        <publisher-name>Springer International Publishing</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib0013">
      <label>13</label>
      <element-citation publication-type="journal" id="sbref0013">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Siddiquee</surname>
            <given-names>M.M.R.</given-names>
          </name>
          <name>
            <surname>Tajbakhsh</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>U-Net++: a remote U-Net architecture for medical image segmentation</article-title>
        <source>MICCAI</source>
        <volume>11045 LNCS</volume>
        <year>2018</year>
        <fpage>3</fpage>
        <lpage>11</lpage>
      </element-citation>
    </ref>
    <ref id="bib0014">
      <label>14</label>
      <element-citation publication-type="journal" id="sbref0014">
        <person-group person-group-type="author">
          <name>
            <surname>Jin</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>AI-assisted CT imaging analysis for COVID-19 screening: building and deploying a medical AI system in four weeks</article-title>
        <source>medRxiv</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0015">
      <label>15</label>
      <element-citation publication-type="journal" id="sbref0015">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning-based model for detecting 2019 novel coronavirus pneumonia high-resolution computed tomography</article-title>
        <source>medRxiv</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0016">
      <label>16</label>
      <element-citation publication-type="journal" id="sbref0016">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>X.G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A weakly-supervised framework for COVID-19 classification and lesion localization from chest CT</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
      </element-citation>
      <note>
        <p>PP(99):1-1</p>
      </note>
    </ref>
    <ref id="bib0017">
      <label>17</label>
      <element-citation publication-type="journal" id="sbref0017">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>D.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Inf-Net: automatic COVID-19 lung infection segmentation from CT images</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
      </element-citation>
      <note>
        <p>PP(99):1-1</p>
      </note>
    </ref>
    <ref id="bib0018">
      <label>18</label>
      <element-citation publication-type="journal" id="sbref0018">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>D.W.K.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Joint optic disc and optical structure. cup segmentation based on multi-label deep network and polar transformation</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>37</volume>
        <issue>7</issue>
        <year>2018</year>
        <fpage>1597</fpage>
        <lpage>1605</lpage>
        <pub-id pub-id-type="pmid">29969410</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0019">
      <label>19</label>
      <element-citation publication-type="journal" id="sbref0019">
        <person-group person-group-type="author">
          <name>
            <surname>Gu</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CE-NET: context encoder network for 2D medical image segmentation</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>38</volume>
        <issue>10</issue>
        <year>2019</year>
        <fpage>2281</fpage>
        <lpage>2292</lpage>
        <pub-id pub-id-type="pmid">30843824</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0020">
      <label>20</label>
      <element-citation publication-type="book" id="sbref0020">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Attention guided network for retinal image segmentation</part-title>
        <source>MICCAI</source>
        <year>2019</year>
        <fpage>797</fpage>
        <lpage>805</lpage>
      </element-citation>
    </ref>
    <ref id="bib0021">
      <label>21</label>
      <element-citation publication-type="journal" id="sbref0021">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Weakly supervised segmentation of COVID19 infection with scribble annotation on CT images</article-title>
        <source>Pattern Recognit.</source>
        <volume>122</volume>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="bib0022">
      <label>22</label>
      <element-citation publication-type="journal" id="sbref0022">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Lung segmentation and automatic detection of COVID-19 using radiomic features from chest CT images</article-title>
        <source>Pattern Recognit.</source>
        <volume>119</volume>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="bib0023">
      <label>23</label>
      <element-citation publication-type="journal" id="sbref0023">
        <person-group person-group-type="author">
          <name>
            <surname>Ai</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China: a report of 1014 cases</article-title>
        <source>Radiology</source>
        <volume>2019</volume>
        <year>2020</year>
        <fpage>200642</fpage>
      </element-citation>
    </ref>
    <ref id="bib0024">
      <label>24</label>
      <element-citation publication-type="book" id="sbref0024">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <part-title>Salient attention for salient object detection</part-title>
        <source>ECCV</source>
        <year>2018</year>
        <fpage>234</fpage>
        <lpage>250</lpage>
      </element-citation>
    </ref>
    <ref id="bib0025">
      <label>25</label>
      <element-citation publication-type="journal" id="sbref0025">
        <person-group person-group-type="author">
          <name>
            <surname>Voulodimos</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A few-shot U-Net deep learning model for COVID-19 infected area segmentation in CT images</article-title>
        <source>Sensors</source>
        <volume>21</volume>
        <issue>6</issue>
        <year>2021</year>
        <fpage>2215</fpage>
        <pub-id pub-id-type="pmid">33810066</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0026">
      <label>26</label>
      <element-citation publication-type="journal" id="sbref0026">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Large-scale screening of COVID-19 from the community acquired pneumonia using infection size aware classification</article-title>
        <source>arXiv</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0027">
      <label>27</label>
      <element-citation publication-type="book" id="sbref0027">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>D.-P.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>M.-M.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Borji</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Structure-measure: a new approach way to evaluate foreground maps</part-title>
        <source>ICCV</source>
        <year>2017</year>
        <fpage>4548</fpage>
        <lpage>4557</lpage>
      </element-citation>
    </ref>
    <ref id="bib0028">
      <label>28</label>
      <element-citation publication-type="book" id="sbref0028">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>D.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Enhanced alignment measure for binary foreground map evaluation</part-title>
        <source>IJCAI</source>
        <year>2018</year>
        <fpage>698</fpage>
        <lpage>704</lpage>
      </element-citation>
    </ref>
    <ref id="bib0029">
      <label>29</label>
      <mixed-citation publication-type="other" id="sbref0029">COVID-19 CT segmentation dataset, 2020, <ext-link ext-link-type="uri" xlink:href="https://medicalsegmentation.com/covid19" id="intrrf0005">https://medicalsegmentation.com/covid19</ext-link>, accessed: 2020-04-11.</mixed-citation>
    </ref>
    <ref id="bib0030">
      <label>30</label>
      <element-citation publication-type="journal" id="sbref0030">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Y.H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>JCS: an explainable COVID-19 diagnosis system by joint classification and segmentation</article-title>
        <source>IEEE Trans. Image Process.</source>
        <year>2021</year>
      </element-citation>
      <note>
        <p>PP(99):1-1</p>
      </note>
    </ref>
    <ref id="bib0033">
      <label>31</label>
      <element-citation publication-type="book" id="sbref0033">
        <person-group person-group-type="author">
          <name>
            <surname>Qin</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>BASNet: boundary-aware salient object detection</part-title>
        <source>CVPR</source>
        <year>2019</year>
        <fpage>7479</fpage>
        <lpage>7489</lpage>
      </element-citation>
    </ref>
    <ref id="bib0034">
      <label>32</label>
      <element-citation publication-type="book" id="sbref0034">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <part-title>F3Net: fusion, feedback and focus for salient object detection</part-title>
        <source>AAAI</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0035">
      <label>33</label>
      <element-citation publication-type="journal" id="sbref0035">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Morrison</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Dao</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 image data collection</article-title>
        <source>arXiv</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="bib0036">
      <label>34</label>
      <element-citation publication-type="journal" id="sbref0036">
        <person-group person-group-type="author">
          <name>
            <surname>Mittal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tatarchenko</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Çiçek</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Parting with illusions about deep active learning</article-title>
        <source>arXiv</source>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="bib0037">
      <label>35</label>
      <element-citation publication-type="book" id="sbref0037">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <part-title>Cascaded partial decoder for fast and accurate salient object detection</part-title>
        <source>CVPR</source>
        <year>2019</year>
        <fpage>3907</fpage>
        <lpage>3916</lpage>
      </element-citation>
    </ref>
    <ref id="bib0038">
      <label>36</label>
      <element-citation publication-type="journal" id="sbref0038">
        <person-group person-group-type="author">
          <name>
            <surname>Teixeira</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images</article-title>
        <source>Sensors</source>
        <volume>21</volume>
        <issue>21</issue>
        <year>2021</year>
        <fpage>7116</fpage>
        <pub-id pub-id-type="pmid">34770423</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0039">
      <label>37</label>
      <element-citation publication-type="journal" id="sbref0039">
        <person-group person-group-type="author">
          <name>
            <surname>Oulefki</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic COVID-19 lung infected region segmentation and measurement using CT-scans images</article-title>
        <source>Pattern Recognit.</source>
        <volume>114</volume>
        <year>2021</year>
        <fpage>107747</fpage>
        <pub-id pub-id-type="pmid">33162612</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0040">
      <label>38</label>
      <element-citation publication-type="book" id="sbref0040">
        <person-group person-group-type="author">
          <name>
            <surname>Maghdid</surname>
            <given-names>H.S.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Diagnosing COVID-19 pneumonia from X-ray and CT images using deep learning and transfer learning algorithms</part-title>
        <source>Multimodal Image Exploitation and Learning 2021</source>
        <volume>vol. 11734</volume>
        <year>2021</year>
        <publisher-name>International Society for Optics and Photonics</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib0042">
      <label>39</label>
      <element-citation publication-type="journal" id="sbref0042">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SG-One: similarity guidance network for one-shot semantic segmentation</article-title>
        <source>IEEE Trans. Cybern.</source>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>11</lpage>
      </element-citation>
      <note>
        <p>PP(99)</p>
      </note>
    </ref>
    <ref id="bib0043">
      <label>40</label>
      <element-citation publication-type="journal" id="sbref0043">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Deep residual learning for image recognition</article-title>
        <source>CVPR</source>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>778</lpage>
      </element-citation>
    </ref>
    <ref id="bib0044">
      <label>41</label>
      <element-citation publication-type="journal" id="sbref0044">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for COVID-19 detection based on CT images</article-title>
        <source>Sci. Rep.</source>
        <volume>11</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="pmid">33414495</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0045">
      <label>42</label>
      <mixed-citation publication-type="other" id="sbref0045">M.Z. Alom, et al., COVID_MTNet: COVID-19 detection with multi-task deep learning approaches, (2020). arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2004.03747" id="intrrf0006">2004.03747</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0046">
      <label>43</label>
      <element-citation publication-type="journal" id="sbref0046">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The ensemble deep learning model for novel COVID-19 on CT images</article-title>
        <source>Appl. Soft Comput.</source>
        <volume>98</volume>
        <year>2021</year>
        <fpage>106885</fpage>
        <pub-id pub-id-type="pmid">33192206</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0047">
      <label>44</label>
      <element-citation publication-type="book" id="sbref0047">
        <person-group person-group-type="author">
          <name>
            <surname>Voulodimos</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Deep learning models for COVID-19 infected area segmentation in CT images</part-title>
        <source>The 14th PErvasive Technologies Related to Assistive Environments Conference</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="bib0048">
      <label>45</label>
      <element-citation publication-type="journal" id="sbref0048">
        <person-group person-group-type="author">
          <name>
            <surname>Mu</surname>
            <given-names>N.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Progressive global perception and local polishing network for lung infection segmentation of COVID-19 CT images</article-title>
        <source>Pattern Recognit.</source>
        <volume>120</volume>
        <year>2021</year>
        <fpage>108168</fpage>
        <pub-id pub-id-type="pmid">34305181</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0049">
      <label>46</label>
      <element-citation publication-type="book" id="sbref0049">
        <person-group person-group-type="author">
          <name>
            <surname>Katsamenis</surname>
            <given-names>I.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Transfer learning for COVID-19 pneumonia detection and classification in chest X-ray images</part-title>
        <source>24th Pan-Hellenic Conference on Informatics</source>
        <year>2020</year>
      </element-citation>
    </ref>
  </ref-list>
  <bio>
    <p><bold>Chaodong Fan</bold> received his master...s degree and doctor’s degree from Hunan University in 2011 and 2014 respectively. Mainly engaged in intelligent computing, pattern recognition and intelligent systems, smart grid related fields. Some new intelligent optimization algorithms and image processing technologies, such as molecular dynamics optimization algorithm, spatial section projection histogram and postprocessing strategy based on threshold,are proposed. His research interests include artificial intelligence, image processing and model recognition.</p>
  </bio>
  <bio>
    <p><bold>Zhenhuan Zeng</bold> was born in 1995 in Changsha, Hunan Province, China. He is studying in Xiangtan University for his master...s degree. His main research interests are evolutionary algorithms, and computer vision.</p>
  </bio>
  <bio>
    <p><bold>Leyi Xiao</bold> was born in 1986 in Changde, Hunan, China. She graduated from Hunan University with a doctorate in engineering in 2020. Her research interest include image processing and model recognition, which relate to both the field of computer vision and pattern recognition.</p>
  </bio>
  <bio>
    <p><bold>Qu Xilong</bold> was born in January 1978, graduated from Southwest Jiaotong University with a doctor’s degree in 2006. His research interests include manufacturing informatization, distributed system integration technology, information security technology.</p>
  </bio>
  <sec sec-type="data-availability" id="refdata001a">
    <title>Data availability</title>
    <p id="para9001a">
      <list list-type="simple" id="dacelist0001a">
        <list-item id="rdlistitem0001a">
          <p id="para9002a">Our code has been shared at <ext-link ext-link-type="uri" xlink:href="https://github.com/zengzhenhuan/GFNet" id="intrrf0001a">https://github.com/zengzhenhuan/GFNet</ext-link>.</p>
        </list-item>
      </list>
    </p>
  </sec>
  <fn-group>
    <fn id="d36e3780">
      <label>☆</label>
      <p id="np0001">This work is supported by <funding-source id="gs00001"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100001809</institution-id><institution>Hunan Provincial Natural Science Foundation of China</institution></institution-wrap></funding-source> (No. 2020JJ4587), Guangdong Basic and Applied Basic Research Foundation (No. 2019A1515110423), Degree &amp; Postgraduate Education Reform Project of Hunan Province (No. 2019JGYB115), Scientific Research Project of Hunan Provincial Department of Education (No. 21C0922), Open Fund Project of Fujian Provincial Key Laboratory of Data Intensive Computing (No. BD202004), Open Research Fund of AnHui Key Laboratory of Detection Technology and Energy Saving Devices (No. JCKJ2021B05), Open Fund Project of Vehicle Measurement, Control and Safety Key Laboratory of Sichuan Province (No. QCCK2021-006).</p>
    </fn>
  </fn-group>
</back>
