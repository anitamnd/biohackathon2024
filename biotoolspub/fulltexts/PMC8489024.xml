<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">eNeuro</journal-id>
    <journal-id journal-id-type="iso-abbrev">eNeuro</journal-id>
    <journal-id journal-id-type="hwp">eneuro</journal-id>
    <journal-id journal-id-type="publisher-id">eNeuro</journal-id>
    <journal-title-group>
      <journal-title>eNeuro</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2373-2822</issn>
    <publisher>
      <publisher-name>Society for Neuroscience</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8489024</article-id>
    <article-id pub-id-type="pmid">34518364</article-id>
    <article-id pub-id-type="doi">10.1523/ENEURO.0122-21.2021</article-id>
    <article-id pub-id-type="publisher-id">eN-NWR-0122-21</article-id>
    <article-categories>
      <subj-group subj-group-type="hwp-journal-coll">
        <subject>7</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Research Article: New Research</subject>
        <subj-group>
          <subject>Novel Tools and Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MEYE: Web App for Translational and Real-Time Pupillometry</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5344-5079</contrib-id>
        <name>
          <surname>Mazziotti</surname>
          <given-names>Raffaele</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="FN4" ref-type="author-notes">
          <sup>*</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Carrara</surname>
          <given-names>Fabio</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <xref rid="FN4" ref-type="author-notes">
          <sup>*</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Viglione</surname>
          <given-names>Aurelia</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lupori</surname>
          <given-names>Leonardo</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lo Verde</surname>
          <given-names>Luca</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Benedetto</surname>
          <given-names>Alessandro</given-names>
        </name>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ricci</surname>
          <given-names>Giulia</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sagona</surname>
          <given-names>Giulia</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Amato</surname>
          <given-names>Giuseppe</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pizzorusso</surname>
          <given-names>Tommaso</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <aff id="aff1"><label>1</label>Department of Neuroscience, Psychology, Drug Research and Child Health (NEUROFARBA), <institution>University of Florence</institution>, 50135 Florence, <country>Italy</country></aff>
      <aff id="aff2"><label>2</label>Institute of Neuroscience, <institution>National Research Council</institution>, 1-56124 Pisa, <country>Italy</country></aff>
      <aff id="aff3"><label>3</label>Department of Developmental Neuroscience, <institution>IRCCS Stella Maris Foundation</institution>, 56128 Pisa, <country>Italy</country></aff>
      <aff id="aff4"><label>4</label>Laboratory of Biology BIO@SNS<institution>, Scuola Normale Superiore</institution>, 1-56124 Pisa, <country>Italy</country></aff>
      <aff id="aff5"><label>5</label><institution>Istituto di Scienza e Tecnologia dell’Informazione (ISTI)</institution>, 1-56124 Pisa, <country>Italy</country></aff>
      <aff id="aff6"><label>6</label>Department of Translational Research on New Technologies in Medicine and Surgery, University of Pisa, 56127 Pisa, Italy</aff>
    </contrib-group>
    <author-notes>
      <fn fn-type="other">
        <p>The authors declare no competing financial interests.</p>
      </fn>
      <fn fn-type="con">
        <p>Author contributions: R.M., F.C., G.A., and T.P. designed research; R.M., F.C., A.V., L.L., L.L.V., A.B., G.R., and G.S. performed research; R.M., F.C., L.L., L.L.V., and A.B. analyzed data; R.M., F.C., A.B., G.A., and T.P. wrote the paper.</p>
      </fn>
      <fn fn-type="supported-by">
        <p>This work was partially supported by Horizon 2020 projects AI4EU (under GA Grant 825619) and AI4Media (under GA Grant 951911); funding from the Italian Ministry for university and research Grant MIUR-PRIN 2017HMH8FA; Associazione Italiana per la sindrome di Rett Project (AIRETT) “Validation of Pupillometry as a Biomarker for Rett Syndrome and Related Disorders: Longitudinal Assessment and Relationship with Disease”; Orphan Disease Center University of Pennsylvania Grant MDBR-19-103-CDKL5; and Associazione “CDKL5 - Insieme verso la cura.”</p>
      </fn>
      <fn id="FN4" fn-type="equal">
        <label>*</label>
        <p>R.M. and F.C. contributed equally to this work.</p>
      </fn>
      <corresp id="cor1">Correspondence should be addressed to Raffaele Mazziotti at <email>raffaele.mazziotti@in.cnr.it</email>.</corresp>
    </author-notes>
    <pub-date pub-type="epreprint">
      <day>13</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <season>Sep-Oct</season>
      <year>2021</year>
    </pub-date>
    <volume>8</volume>
    <issue>5</issue>
    <elocation-id>ENEURO.0122-21.2021</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>3</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>7</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>6</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Mazziotti et al.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Mazziotti et al.</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International license</ext-link>, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="ENEURO.0122-21.2021.pdf"/>
    <self-uri xlink:role="icon" xlink:href="ENEURO.0122-21.2021g1.jpg"/>
    <abstract>
      <title>Abstract</title>
      <p>Pupil dynamics alterations have been found in patients affected by a variety of neuropsychiatric conditions, including autism. Studies in mouse models have used pupillometry for phenotypic assessment and as a proxy for arousal. Both in mice and humans, pupillometry is noninvasive and allows for longitudinal experiments supporting temporal specificity; however, its measure requires dedicated setups. Here, we introduce a convolutional neural network that performs online pupillometry in both mice and humans in a web app format. This solution dramatically simplifies the usage of the tool for the nonspecialist and nontechnical operators. Because a modern web browser is the only software requirement, this choice is of great interest given its easy deployment and setup time reduction. The tested model performances indicate that the tool is sensitive enough to detect both locomotor-induced and stimulus-evoked pupillary changes, and its output is comparable to state-of-the-art commercial devices.</p>
    </abstract>
    <kwd-group>
      <kwd>arousal</kwd>
      <kwd>neural network</kwd>
      <kwd>oddball</kwd>
      <kwd>pupillometry</kwd>
      <kwd>virtual reality</kwd>
      <kwd>web app</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>H2020 AI4EU</funding-source>
        <award-id>GA 825619</award-id>
      </award-group>
      <award-group>
        <funding-source>AI4Media</funding-source>
        <award-id>GA 951911</award-id>
      </award-group>
      <award-group>
        <funding-source>Orphan Disease Center University of Pennsylvania</funding-source>
        <award-id>MDBR-19-103-CDKL5</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="4"/>
      <equation-count count="3"/>
      <ref-count count="54"/>
      <page-count count="13"/>
      <word-count count="00"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>September/October 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Significance Statement</title>
    <p>Alteration of pupil dynamics is an important biomarker that can be measured noninvasively and across different species. Although pupil size is driven primarily by light, it can also monitor arousal states and cognitive processes. Here we show an open-source web app that, through deep learning, can perform real-time pupil size measurements in both humans and mice, with accuracy similar to commercial-grade eye trackers. The tool requires no installation, and pupil images can be captured using infrared webcams, opening the possibility of performing pupillometry widely, cost-effectively, and in a high-throughput manner.</p>
  </sec>
  <sec sec-type="intro" id="s2">
    <title>Introduction</title>
    <p>Pupillometry, the measurement of pupil size fluctuations over time, provides useful insights into clinical settings and basic research activity. Light level is the primary determinant of pupil size, although non-light-driven pupil fluctuations, widely assumed as an indicator of arousal through locus coeruleus activity, can be used to index brain state across species (<xref rid="B31" ref-type="bibr">McGinley et al., 2015</xref>; <xref rid="B25" ref-type="bibr">Lee and Margolis, 2016</xref>; <xref rid="B42" ref-type="bibr">Reimer et al., 2016</xref>). Higher cognitive and emotional processes are also able to evoke tonic or phasic pupillary changes, such as attention (<xref rid="B7" ref-type="bibr">Binda et al., 2013a</xref>), memory load (<xref rid="B50" ref-type="bibr">Wierda et al., 2012</xref>), novelty (<xref rid="B3" ref-type="bibr">Angulo-Chavira et al., 2017</xref>; <xref rid="B24" ref-type="bibr">Krebs et al., 2018</xref>; <xref rid="B33" ref-type="bibr">Montes-Lourido et al., 2021</xref>), pain (<xref rid="B15" ref-type="bibr">Connelly et al., 2014</xref>; <xref rid="B5" ref-type="bibr">Azevedo-Santos and DeSantana, 2018</xref>; <xref rid="B12" ref-type="bibr">Charier et al., 2019</xref>), and more general cortical sensory processing (<xref rid="B8" ref-type="bibr">Binda et al., 2013b</xref>; <xref rid="B25" ref-type="bibr">Lee and Margolis, 2016</xref>) in humans and in animal models.</p>
    <p>A growing body of work shows how pupillometry can be used as a possible biomarker for numerous neurologic and psychiatric conditions in early development and adult subjects (<xref rid="B2" ref-type="bibr">Aleman et al., 2004</xref>; <xref rid="B9" ref-type="bibr">Blaser et al., 2014</xref>; <xref rid="B44" ref-type="bibr">Rorick-Kehn et al., 2014</xref>; <xref rid="B17" ref-type="bibr">Frost et al., 2017</xref>; <xref rid="B34" ref-type="bibr">Nyström et al., 2018</xref>; <xref rid="B14" ref-type="bibr">Chougule et al., 2019</xref>; <xref rid="B18" ref-type="bibr">Gajardo et al., 2019</xref>; <xref rid="B36" ref-type="bibr">Oh et al., 2019</xref>, <xref rid="B37" ref-type="bibr">2020</xref>; <xref rid="B4" ref-type="bibr">Artoni et al., 2020</xref>; <xref rid="B11" ref-type="bibr">Burley and van Goozen, 2020</xref>; <xref rid="B23" ref-type="bibr">Iadanza et al., 2020</xref>; <xref rid="B35" ref-type="bibr">Obinata et al., 2020</xref>; <xref rid="B51" ref-type="bibr">Winston et al., 2020</xref>; <xref rid="B16" ref-type="bibr">El Ahmadieh et al., 2021</xref>). Spontaneous and voluntary modulation of pupil fluctuations has also been used to facilitate human–computer interaction in normal subjects (<xref rid="B30" ref-type="bibr">Mathôt et al., 2016</xref>; <xref rid="B6" ref-type="bibr">Beggiato et al., 2018</xref>; <xref rid="B40" ref-type="bibr">Ponzio et al., 2019</xref>) and patients with severe motor disabilities. For example, pupil dynamics is used to assess communication capability in locked-in syndrome, a crucial factor for the determination of a minimally conscious state (<xref rid="B38" ref-type="bibr">Olivia et al., 2013</xref>; <xref rid="B46" ref-type="bibr">Stoll et al., 2013</xref>). Pupillometry is also becoming a valuable tool for child neurology, to facilitate risk assessment in infants. For example, the pupil light reflex (PLR) during infancy seems to predict the later diagnosis and severity of autism spectrum disorders (ASDs; <xref rid="B34" ref-type="bibr">Nyström et al., 2018</xref>). Intriguingly, pupil alterations are also present in several ASD mouse models (<xref rid="B4" ref-type="bibr">Artoni et al., 2020</xref>).</p>
    <p>Pupillometry has several advantages compared with other physiological methods: it is noninvasive and can be performed by nonspecialized personnel on noncollaborative and preverbal subjects (like infants), allowing the design of longitudinal experiments to permit temporal specificity. More importantly, it can be conducted similarly across different species from mice to humans, guaranteeing maximal translatability of the protocols and results (<xref rid="B2" ref-type="bibr">Aleman et al., 2004</xref>; <xref rid="B44" ref-type="bibr">Rorick-Kehn et al., 2014</xref>; <xref rid="B4" ref-type="bibr">Artoni et al., 2020</xref>). Given these assumptions, it is vital to introduce a simple, versatile tool used in a range of settings, from the laboratory to the clinical or even domestic environment. Available open-source methods require complicated steps for the installation and configuration of custom software not suitable for nontechnical operators. Moreover, these tools were tested exclusively in one species [mice (<xref rid="B41" ref-type="bibr">Privitera et al., 2020</xref>), humans (<xref rid="B53" ref-type="bibr">Yiu et al., 2019</xref>)], and none of them were applied in cognitive experiments that usually involve small pupil changes associated with high variability.</p>
    <p>In this work, we have developed a deep learning tool called MEYE, using convolutional neural networks (CNNs) to detect and measure real-time changes in pupil size both in humans and mice in different experimental conditions. Furthermore, the MEYE web app, performs pupil area quantification and blink detection, all within a single network. By embedding artificial intelligence algorithms in a web browser to process real-time webcam streams or videos of the eye, MEYE can be used by nontechnical operators, opening the possibility to perform pupillometry widely, cost-effectively, and in a high-throughput manner. This architecture is resistant to different illumination conditions, allowing the design of basic neuroscience experiments in various experimental settings, such as behavior coupled with electrophysiology or imaging such as two-photon microscopy. To describe the performance of the MEYE web app in different settings, we tested the app in both mice and humans. In mice, we recorded both running speed and pupil size during visual and auditory stimulation (AS). In humans, we tested MEYE capabilities to detect the PLR. Furthermore, we performed a visual oddball paradigm (<xref rid="B26" ref-type="bibr">Liao et al., 2016</xref>; <xref rid="B1" ref-type="bibr">Aggius-Vella et al., 2020</xref>; <xref rid="B28" ref-type="bibr">LoTemplio et al., 2021</xref>), comparing pupil size and eye position measurements obtained from MEYE with one of the most used commercial eye-tracker systems: the EyeLink 1000. Finally, we released a dataset of 11,897 eye images that can be used to train other artificial intelligence tools.</p>
  </sec>
  <sec sec-type="materials|methods" id="s3">
    <title>Materials and Methods</title>
    <sec id="s3A">
      <title>Datasets</title>
      <p>For this study, we collected a dataset (<xref rid="F1" ref-type="fig">Fig. 1<italic toggle="yes">A</italic></xref>) composed of 11,897 grayscale images of human (4285) and mouse (7612) eyes. The majority of the pictures are of mouse eyes during head fixation sessions (5061 sessions) in a dark environment using infrared (IR; 850 nm) light sources. In this environment, the pupil is darker than the rest of the image. We also collected mouse eyes [two-photon imaging mice (2P mice), 2551] during two-photon Ca<sup>2+</sup> imaging. In this particular condition, the pupil is inverted in color and tends to be brighter than the iris. Finally, we acquired images of human eyes in IR light (4285 eyes) during virtual reality (VR) experiments (wearing a headset for virtual reality), using an endoscopic camera (<ext-link xlink:href="http://www.misumi.com.tw/" ext-link-type="uri">www.misumi.com.tw/</ext-link>). The dataset contains 1596 eye blinks, 841 images in the mouse, and 755 photographs in the human datasets. Five human raters segmented the pupil in all pictures (one per image), using custom labeling scripts implemented in MATLAB or Python by manual placement of an ellipse or polygon over the pupil area. Raters flagged blinks using the same code.</p>
      <fig position="float" id="F1" fig-type="figure">
        <label>Figure 1.</label>
        <caption>
          <p>Dataset, CNN architecture, and performances. <bold><italic toggle="yes">A</italic></bold>, Examples of images taken from the dataset. The first image depicts a head-fixed mouse with dark pupils, the second one is a head-fixed mouse with a bright pupil, during two-photon microscope sessions. The last image is a human eye taken during experiments wearing virtual reality goggles. <bold><italic toggle="yes">B</italic></bold>, The 64 examples of data augmentation fed to CNN. The images are randomly rotated, cropped, flipped (horizontally or vertically), and changed in brightness/contrast/sharpness. <bold><italic toggle="yes">C</italic></bold>, CNN architecture with an encoder–decoder “hourglass” shape. The encoder part comprises a sequence of convolutional layers. Starting from the last encoder output, the decoder part iteratively upsamples and fuses feature maps with corresponding encoder maps to produce the output pixel map. The pixel probability map and eye/blink probabilities are computed by applying the sigmoid activation to the network outputs in an element-wise manner.</p>
        </caption>
        <graphic xlink:href="ENEURO.0122-21.2021_f001" position="float"/>
      </fig>
    </sec>
    <sec id="s3B">
      <title>CNN architecture</title>
      <p>The CNN model (<xref rid="F1" ref-type="fig">Fig. 1<italic toggle="yes">C</italic></xref>) takes 128 × 128 grayscale images as input and produces the following three outputs: (1) a 128 × 128 probability map of each pixel belonging to the pupil; (2) the probability the image contains an eye; and (3) the probability the image depicts a blinking eye. We evaluated three architectures: two were based on DeepLabv3+ (<xref rid="B13" ref-type="bibr">Chen et al., 2018</xref>), a family of image segmentation models that use atrous convolutions and spatial pyramid pooling, which are known to improve robustness to scale changes. The two models differ for the CNN backbone adopted, respectively, ResNet-50 (<xref rid="B20" ref-type="bibr">He et al., 2016</xref>) and MobileNet V3 (<xref rid="B22" ref-type="bibr">Howard et al., 2019</xref>) CNNs. The third evaluated model is a specialized variant of the U-Net architecture (<xref rid="B43" ref-type="bibr">Ronneberger et al., 2015</xref>), a widely used CNN in image segmentation tasks. The model has an encoder–decoder “hourglass” architecture; the encoder part comprises a sequence of convolutional layers with ReLU activation and 2 × 2 maximum pooling operation, each halving the spatial resolution of feature maps at every layer; this produces a sequence of feature maps of diminishing spatial dimensions that provides both spatially local information and global context for the subsequent steps. Starting from the last encoder output, the decoder part iteratively upsamples and fuses feature maps with corresponding encoder maps, using convolutional layers, to produce the output pixel map. All convolutional layers have 16 3 × 3 kernels and pad their input to obtain output of the same shape. Convolutional layer upsampling and downsampling were changed by a factor of 2 (<xref rid="F1" ref-type="fig">Fig. 1<italic toggle="yes">C</italic></xref>). In all the tested architectures, eye and blink probabilities are predicted by an additional branch that applies global average pooling and a two-output fully connected layer to the bottleneck feature map. The pixel probability map and eye/blink probabilities are computed by applying the sigmoid activation to the network outputs element wise. Among the tested architectures, we chose to adopt the UNet variant in this work, as we observed it provided the best tradeoff in terms of speed and segmentation quality (for further information see: <ext-link xlink:href="https://github.com/fabiocarrara/meye/wiki/MEYE-Models" ext-link-type="uri">https://github.com/fabiocarrara/meye/wiki/MEYE-Models</ext-link>).</p>
    </sec>
    <sec id="s3C">
      <title>Augmentation, training, and validation</title>
      <p>We randomly split the dataset into training, validation, and test subsets following a 70%/20%/10% split. We performed strong data augmentation during the training phase by applying random rotation, random cropping, random horizontal and vertical flipping, and random brightness/contrast/sharpness changes; images were resized to 128 × 128 before feeding them to the CNN (<xref rid="F1" ref-type="fig">Fig. 1<italic toggle="yes">B</italic></xref>).</p>
      <p>For validation and test images, we used a 128 × 128 crop centered on the pupil. We computed the binary cross-entropy for all outputs (pixels and eye/blink logits) and took the sum as the loss function to minimize. The network was trained with the AdaBelief optimizer (<xref rid="B54" ref-type="bibr">Zhuang et al., 2020</xref>) for 750 epochs with a learning rate of 0.001. The best performing snapshot on the validation set was selected and evaluated on the test set.</p>
    </sec>
    <sec id="s3D">
      <title>MEYE: web browser tool</title>
      <p>We built a web app for pupillometry on recorded or live-captured videos harnessing a CNN segmentation model as the core component. The trained models have been converted to a web-friendly format using <italic toggle="yes">TensorFlow.js</italic>, thus enabling predictions on the user machine using a web browser.</p>
      <p>This choice greatly facilitates the deployment and reduces setup time, as a modern web browser is the only minimum requirement. Once loaded, an Internet connection is not mandatory, as no data leaves the user’s browser, and all the processing is performed on the user’s machine. This implies that performance greatly depends on the user’s hardware; if available, hardware (graphics processing unit (GPU)] acceleration is exploited automatically by <italic toggle="yes">TensorFlow.js</italic>. In our tests, a modern laptop shipping an Intel(R) Core(TM) i7-9750H 2.60 GHz CPU and an Intel(R) UHD Graphics 630 GPU can process up to 28 frames/s (fps).</p>
      <p>The web app also offers additional features that facilitate the recording process, such as the following: processing of prerecorded videos or real-time video streams captured via webcam; ROI placement via user-friendly web user interface (UI; drag and drop) and automatic repositioning following tracked pupil center; embedded tunable preprocessing (image contrast/brightness/gamma adjustment and color inversion) and postprocessing (map thresholding and refinement via mathematical morphology); support for registering trigger events; live plotting of pupil area and blink probability; and data export in CSV format including pupil area, blink probability, eye position, and trigger channels.</p>
    </sec>
    <sec id="s3E">
      <title>Behavioral experiments on mice</title>
      <sec id="s3E1">
        <title>Animal handling</title>
        <p>Mice were housed in a controlled environment at 22°C with a standard 12 h light/dark cycle. During the light phase, a constant illumination &lt;40 lux from fluorescent lamps was maintained. Food (standard diet, 4RF25 GLP Certificate, Mucedola) and water were available <italic toggle="yes">ad libitum</italic> and were changed weekly. Open-top cages (36.5 × 20.7 × 14 cm; 26.7 × 20.7 × 14 cm for up to five adult mice; or 42.5 × 26.6 × 15.5 cm for up to eight adult mice) with wooden dust-free bedding were used. All the experiments were conducted following the directives of the European Community Council and approved by the Italian Ministry of Health (1225/2020-PR). All necessary efforts were made to minimize both stress and the number of animals used. The subjects used in this work were three female 3-month-old C57BL/6J mice for the auditory stimulation and five male 2-month-old mice for the VR experiment.</p>
      </sec>
      <sec id="s3E2">
        <title>Surgery</title>
        <p>The mouse was deeply anesthetized using isoflurane (3% induction, 1.5% maintenance). Then it was mounted on a stereotaxic frame through the use of ear bars. Prilocaine was used as a local anesthetic for the acoustic meatus. The eyes were treated with a dexamethasone-based ophthalmic ointment (Tobradex, Alcon Novartis) to prevent cataract formation and keep the cornea moist. Body temperature was maintained at 37°C using a heating pad monitored by a rectal probe. Respiration rate and response to toe pinch were checked periodically to maintain an optimal level of anesthesia. Subcutaneous injection of lidocaine (2%) was performed before scalp removal. The skull surface was carefully cleaned and dried, and a thin layer of cyanoacrylate was poured over the exposed skull to attach a custom-made head post that was composed of a 3D-printed base equipped with a glued set screw (12 mm long, M4 thread; catalog #SS4MS12, Thorlabs). The implant was secured to the skull using cyanoacrylate and UV curing dental cement (Fill Dent, Bludental). At the end of the surgical procedure, the mice recovered in a heated cage. After 1 h, mice were returned to their home cage. Paracetamol was used in the water as antalgic therapy for 3 d. We waited 7 d before performing head-fixed pupillometry to provide sufficient time for the animal to recover.</p>
      </sec>
      <sec id="s3E3">
        <title>Head fixation</title>
        <p>In the awake mouse head fixation experiments, we used a modified version of the apparatus proposed by <xref rid="B45" ref-type="bibr">Silasi et al. (2016)</xref>, equipped with a 3D-printed circular treadmill (diameter, 18 cm). Components are listed in <xref rid="T1" ref-type="table">Table 1</xref>. A locking ball socket mount (TRB1/M) was secured to an aluminum breadboard (MB2020/M) using two optical posts (TR150/M-P5) and a right-angle clamp (RA90/M-P5). The circular treadmill was blocked between the base plate pillar rod and the optical post through a ball-bearing element (BU4041, BESYZY) to allow the spinning of the disk with low effort. To couple the head-fixing thread on the mouse to the locking ball, an ER025 post was modified by retapping one end of it with M4 threads to fit the ball and socket mount. Velocity was detected using an optical mouse under the circular treadmill. Pupillometry was performed using a USB camera (oCam-5CRO-U, Withrobot) equipped with a 25 mm M12 lens connected to a Jetson AGX Xavier Developer Kit (NVIDIA) running a custom Python3 script (30 fps). The Jetson hardware was connected with an Arduino UNO microcontroller board through GPIO (general purpose input/output) digital connection. The Arduino UNO managed the auditory stimuli through a speaker (3 inch; model W3-1364SA, Tang Band Speaker).</p>
        <table-wrap position="float" id="T1">
          <label>Table 1</label>
          <caption>
            <p>Head fixation apparatus components (thorlabs.com)</p>
          </caption>
          <table frame="hsides" rules="none">
            <col align="left" valign="top" span="1"/>
            <col align="left" valign="top" span="1"/>
            <col align="left" valign="top" span="1"/>
            <col align="left" valign="top" span="1"/>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Part number</th>
                <th align="left" rowspan="1" colspan="1">Description</th>
                <th align="left" rowspan="1" colspan="1">Quantity</th>
                <th align="left" rowspan="1" colspan="1">Price (€)</th>
              </tr>
            </thead>
            <tbody valign="top">
              <tr>
                <td align="left" rowspan="1" colspan="1">TRB1/M</td>
                <td align="left" rowspan="1" colspan="1">Locking ball and socket mount M4</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">55.83</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">TR150/M-P5</td>
                <td align="left" rowspan="1" colspan="1">Optical post M4–M6, 150 mm, 5 pack</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">29.97</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">RA90/M-P5</td>
                <td align="left" rowspan="1" colspan="1">Right-angle clamp</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">45.7</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">MB2020/M</td>
                <td align="left" rowspan="1" colspan="1">Aluminum breadboard</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">72.3</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">RS075P/M</td>
                <td align="left" rowspan="1" colspan="1">Pedestal pillar post</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">21.63</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SS4MS12</td>
                <td align="left" rowspan="1" colspan="1">Set screws 12 mm long, M4</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">5.61</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">AP4M3M</td>
                <td align="left" rowspan="1" colspan="1">Adaptor M4-M3</td>
                <td rowspan="1" colspan="1">5</td>
                <td rowspan="1" colspan="1">1.91</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ER025</td>
                <td align="left" rowspan="1" colspan="1">Cage assembly rod</td>
                <td rowspan="1" colspan="1">5</td>
                <td rowspan="1" colspan="1">4.73</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">SS6MS12</td>
                <td align="left" rowspan="1" colspan="1">Set screws 12 mm long, M6</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">5.55</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">CF038C-P5</td>
                <td align="left" rowspan="1" colspan="1">Clamping fork</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">46.49</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Total</td>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">289.72</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="s3E4">
        <title>Behavioral procedures</title>
        <p>Mice were handled for 5 min each day during the week preceding the experiments; then, they were introduced gradually to head fixation for an increasing amount of time for 5 d. During days 1 and 2, we performed two sessions of 10 min of head fixation, one in the morning and one in the afternoon. On day 3, we performed one session of 20 min; on day 4, 30 min; and on day 5, 35 min. Each recording started with 5 min of habituation. We exposed the animal to auditory stimuli during the last day. During each head fixation session, a curved monitor (24 inches; model CF390, Samsung) was placed in front of the animal (distance, 13 cm) showing a uniform gray with a mean luminance of 8.5 cd/m<sup>2</sup>. The frequency of tone 1 was 3000 Hz, and of tone 2, 4000 Hz, both at 70 dB, a duration of 20 s, and an interstimulus interval of 120 s. Virtual reality was composed of a γ-linearized procedural virtual corridor with episodic visual stimulation written in C# and Unity. The virtual corridor was composed of sine-wave gratings at different orientations (wall at 0°; floor at 90°), and spatial frequencies (from 0.06 to 0.1 cycles/°). The position of the animal in the virtual corridor was updated using an optical mouse connected to the circular treadmill. The episodic visual stimulus consisted of a square wave grating patch of 55° (in width and height) of visual space in the binocular portion of the visual field. The grating parameters were as follows: luminance, 8.5 cd/m<sup>2</sup>; orientation, 0°; contrast, 90%; spatial frequency, 0.1 cycles/°; drifting, 0.5 cycle/s.</p>
      </sec>
      <sec id="s3E5">
        <title>Data analysis</title>
        <p>Data has been analyzed using Python 3. All tracks were loaded, and blink removal was applied using the blink detector embedded in MEYE. Blink epochs were filled using linear interpolation and median filtering (0.5 s). Spearman ρ rank-order correlation was performed using the function <italic toggle="yes">corr</italic> from Python library <italic toggle="yes">pingouin</italic> (<xref rid="B47" ref-type="bibr">Vallat, 2018</xref>). The <italic toggle="yes">z</italic>-score was obtained for each trial using the formula <inline-formula id="IE1"><mml:math id="i1" display="inline" overflow="scroll"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mtext>baseline</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>baseline</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, where and <inline-formula id="IE2"><mml:math id="i2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mtext>baseline</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE3"><mml:math id="i3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mtext>baseline</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> were respectively the average and the SD of the baseline. To evaluate whether event-related transients (ERTs) amplitude was significantly different from baseline, a two-way repeated-measures ANOVA was computed on each time sample using the <italic toggle="yes">pingouin</italic> function <italic toggle="yes">rm_anova. Post hoc</italic> analyses and multiple-comparison <italic toggle="yes">p</italic> value correction were conducted using the function <italic toggle="yes">pairwise_t tests</italic> from <italic toggle="yes">pingouin</italic>. For both pupil size and velocity, we compared each time sample after sensory stimulation with the average value of the baseline, adjusting the <italic toggle="yes">p</italic> values using Benjamini–Hochberg FDR correction. For the behavioral state analysis, locomotion activity was identified using a threshold algorithm. We tagged as moving all the samples in which velocity was ≥10% with respect to the maximal speed of the animal. Paired <italic toggle="yes">t</italic> tests between behavioral states were performed using the <italic toggle="yes">function t</italic> test from <italic toggle="yes">pingouin</italic>. Comparison of eyes movements was conducted normalizing (range between −1 and 1) data from both setups, upsampling MEYE data from 15 to 1000 fps using linear interpolation, and then calculating the mean absolute error (MAE), which was performed using the Python function <italic toggle="yes">mean_absolute_error</italic> from the library <italic toggle="yes">sklearn</italic>.</p>
      </sec>
    </sec>
    <sec id="s3F">
      <title>Behavioral experiments on humans</title>
      <sec id="s3F6">
        <title>PLR</title>
        <p>Pupillometry has been performed using a MacBook Pro (Retina, 13 inch, Early 2015, Intel Core i5 dual-core 2.7 GHz, 8GB of RAM, Intel Iris Graphics 6100 card, 1536 MB) running the MEYE application on Firefox (84.0). The tool is able to compute online pupil size quantification, plotting the instantaneous pupil area and saving the results on file. Furthermore, the tool accepts four independent manual push-button triggers (keys “T” or “Y” on the keyboard). This feature allowed us to annotate stimulation events. A USB IR webcam (model Walfront5k3psmv97x, Walfront) equipped with a Varifocal 6–22 mm M12 objective (catalog #149129, Sodial) was used to acquire images of the eye. The camera was equipped with six IR LEDs to illuminate the eye uniformly, optimizing contrast between the iris and the pupil. Photic stimulation was delivered using an Arduino Due (Arduino) microcontroller connected via USB to the notebook and programmed to emulate a keyboard. The Arduino emulates a keyboard (using the <italic toggle="yes">keyboard. h</italic> library) to send event triggers to MEYE in the form of keystroke events. The microcontroller drives a stripe of four LEDs (WS2813, WorldSemi) using the <italic toggle="yes">FastLED. h</italic> library, flashing bright white light for 500 ms with an interstimulus interval of 5 s (see <xref rid="F3" ref-type="fig">Fig. 3<italic toggle="yes">A</italic></xref>). The subject sat in front of a monitor screen (24 inches; model CF390, Samsung) at a distance of 60 cm, with the head stabilized by a chin rest and instructed to maintain fixation on a small dot presented in the center of the screen for the whole duration of the recording (57 s). A total of 10 flash stimuli have been presented through the strip of LEDs mounted above the screen.</p>
      </sec>
      <sec id="s3F7">
        <title>Oddball paradigm corecordings</title>
        <p>To compare the performances shown by the CNN system with that of a state-of-the-art commercial software, we coregistered pupillometry using MEYE and an EyeLink 1000 system, while nine participants (three males, six females; average age, 28.78 years) executed an oddball paradigm. The experiment was conducted in a quiet, dark room. The participant sat in front of a monitor screen (88 × 50 cm) at a distance of 100 cm, with their head stabilized by a chin rest. The viewing was binocular. Stimuli were generated with the PsychoPhysics Toolbox routines (<xref rid="B10" ref-type="bibr">Brainard, 1997</xref>; <xref rid="B39" ref-type="bibr">Pelli, 1997</xref>) for MATLAB (MATLAB r2010a, MathWorks) and presented on a γ-calibrated PROPixx DLP LED projector (VPixx Technologies) with a resolution of 1920 × 1080 pixels, and a refresh rate of 120 Hz. Pupil diameter was monitored at 1 kHz with an EyeLink 1000 system (SR Research) with an infrared camera mounted below the screen and recording from the right eye. The participant was instructed to maintain fixation on a small dot (0.5°) presented in the center of the screen for the whole duration of the recording (300 s). In this study, the visual stimuli consisted of the appearance of a high-probability stimulus (80% of times) defined as “Standard” and a lower probability stimulus (20% of times) defined as “Target.” The Standard stimulus consisted of a 100% contrast-modulated annular grating (mean luminance, 25 cd/m<sup>2</sup>), horizontally oriented, with a spatial frequency of 0.5 cpd, and with inner and outer diameters of 1.5° and 5°, respectively. The edges of the annulus were smoothed by convolving the stimulus with a Gaussian mask (σ = 0.5°). The Target stimulus had the same parameters as the Standard stimulus except that the orientation that was 45° (see <xref rid="F4" ref-type="fig">Fig. 4<italic toggle="yes">A</italic></xref>). The presentation duration of each trial, either the Standard (0°) or Target (45°) trial, was 200 ms with an intertrial interval between two consecutive trials of 2800 ms. The phases of both the Target and the Standard stimuli were randomized across trials. The participants were instructed to press a button for a Target stimulus and not to respond for a Standard stimulus. The <italic toggle="yes">z</italic>-scored ERTs were computed as described for mice. The correlation was performed by taking the amplitude of the peaks of the Targets for each subject using both EyeLink and MEYE, and then performing the Spearman ρ rank-order correlation between the two measures.</p>
      </sec>
      <sec id="s3F8">
        <title>Eye movement corecordings</title>
        <p>For eye-tracking recording, we used both the MEYE tool and EyeLink 1000, as described above. In the smooth pursuit condition, a small dot (0.5°) moved on the screen horizontally, changing direction every 20° of the visual field with a constant velocity of 8°/s. In the saccades condition, every 2.5 s the small dot abruptly changes position horizontally with a span of 20°.</p>
      </sec>
      <sec id="s3F9">
        <title>Offline video analysis</title>
        <p>The MP4 videos were loaded into MEYE, and the parameters were chosen by visually inspecting the quality of the pupillometry. Threshold values were 0.25, 0.15, and 0.5, with morphology FALSE, TRUE, TRUE for “human,” “mouse,” and “2P-mouse” videos respectively. Once the video analysis was completed, the CSV file was loaded into Python. Blink removal and linear interpolation were applied, then the track was plotted using the Python library <italic toggle="yes">matplotlib</italic>.</p>
      </sec>
    </sec>
    <sec sec-type="data-availability" id="s3G">
      <title>Data availability</title>
      <p>The code and web app are freely available on Github: <ext-link xlink:href="http://github.com/fabiocarrara/meye" ext-link-type="uri">github.com/fabiocarrara/meye</ext-link>. MEYE is available at: <ext-link xlink:href="http://www.pupillometry.it" ext-link-type="uri">www.pupillometry.it</ext-link>. MEYE wiki is available at: <ext-link xlink:href="https://github.com/fabiocarrara/meye/wiki" ext-link-type="uri">https://github.com/fabiocarrara/meye/wiki</ext-link>. The dataset is available on: <ext-link xlink:href="https://doi.org/10.5281/zenodo.4488164" ext-link-type="uri">https://doi.org/10.5281/zenodo.4488164</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s4">
    <title>Results</title>
    <sec id="s4A">
      <title>Pupillometry in head-fixed mice</title>
      <p>We tested our CNN-based pupillometer in two behavioral experiments involving locomotion-induced and stimulus-evoked pupillary changes. Pupil size was simultaneously recorded with running speed from head-fixed mice free to run on a circular treadmill (<xref rid="F2" ref-type="fig">Fig. 2<italic toggle="yes">A</italic></xref>). We used two different stimulation protocols: AS, and visual stimulation while the animal is freely exploring VR. The VR experiment included an initial period of habituation in which the animal navigated inside a virtual corridor for 5 min. After this period, a square visual stimulus was presented in the binocular portion of the visual field (duration, 20 s; interstimulus interval, 120 s, 10 times; <xref rid="F3" ref-type="fig">Fig. 3<italic toggle="yes">A</italic>,<italic toggle="yes">B</italic></xref>). The AS experiment was conducted with the same structure as the VR experiment and using auditory stimulus previously used to induce a defensive behavior detectable as a pupillary and behavioral response (<xref rid="B52" ref-type="bibr">Xiong et al., 2015</xref>; <xref rid="B49" ref-type="bibr">Wang et al., 2019</xref>; <xref rid="B21" ref-type="bibr">Hersman et al., 2020</xref>; <xref rid="B27" ref-type="bibr">Li et al., 2021</xref>). An initial period of habituation was followed by auditory stimulation using two tones (tone 1, 3 kHz; tone 2, 4 kHz; duration, 20 s; interstimulus interval, 120 s; <xref rid="F2" ref-type="fig">Fig. 2<italic toggle="yes">A</italic>,<italic toggle="yes">B</italic></xref>). We first set out to evaluate whether CNN can detect event-related pupil transients (i.e., ERTs) because of sensory stimulation in the VR. We averaged pupil size and running velocity during a 15 s temporal window centered on visual stimulation (<xref rid="F3" ref-type="fig">Fig. 3<italic toggle="yes">C</italic></xref>). We detected a significant pupillary dilation after the onset of the visual stimulus and no changes in evoked locomotion (pupil: <italic toggle="yes">p</italic> &lt; 0.001, <italic toggle="yes">post hoc</italic> test; stimulus duration, 1.2–2.7 s; adjusted <italic toggle="yes">p</italic> values &lt; 0.05, two-way repeated-measures ANOVA; velocity: <italic toggle="yes">p</italic> = 0.96, two-way repeated-measures ANOVA). This ERT is an orienting-related pupil response and a proxy of the arousal change because of stimulus detection (<xref rid="B48" ref-type="bibr">Wang and Munoz, 2015</xref>; <xref rid="B33" ref-type="bibr">Montes-Lourido et al., 2021</xref>). In the AS experiment, we also found an increase in pupil size, but dilation was associated with a significant increase in stimulus-induced locomotor activity (pupil: <italic toggle="yes">p</italic> &lt; 0.001, <italic toggle="yes">post hoc</italic> test; stimulus duration, 1–10 s; adjusted <italic toggle="yes">p</italic> values &lt; 0.05, two-way repeated-measures ANOVA; velocity: <italic toggle="yes">p</italic> &gt; 0.01, <italic toggle="yes">post hoc</italic> test; stimulus duration, 3–5.3 s; adjusted <italic toggle="yes">p</italic> values &lt; 0.05; two-way repeated-measures ANOVA; <xref rid="F2" ref-type="fig">Fig. 2<italic toggle="yes">C</italic></xref>). Finally, we calculated pupil size during baseline periods before sensory stimulation both in the VR and the AS experiments. We found that during locomotion the pupil was significantly larger than in stationary periods both in the AS (<italic toggle="yes">p</italic> &lt; 0.01, paired <italic toggle="yes">t</italic> test; <xref rid="F2" ref-type="fig">Fig. 2<italic toggle="yes">D</italic></xref>) and in the VR (<italic toggle="yes">p</italic> &lt; 0.01, paired <italic toggle="yes">t</italic> test; <xref rid="F3" ref-type="fig">Fig. 3<italic toggle="yes">D</italic></xref>) experiments.</p>
      <fig position="float" id="F2" fig-type="figure">
        <label>Figure 2.</label>
        <caption>
          <p>Pupillometry in head-fixed mice. <bold><italic toggle="yes">A</italic></bold>, Setup for head-fixed pupillometry in the awake mouse. The head of the mouse is fixed to a custom-made metal arm equipped with a 3D-printed circular treadmill to monitor running behavior. In the meantime, pupillometry is performed using CNN. <bold><italic toggle="yes">B</italic></bold>, The average fluctuations of pupillometry and velocity in all experimental mice. Dashed pink and yellow areas represent the onset and duration of auditory stimuli. Evoked peaks in both pupil size (blue line) and velocity (green line) are clearly noticeable during auditory stimulation. <bold><italic toggle="yes">C</italic></bold>, Average event-related transients for both pupil size and velocity. Colored areas represent stimulus onset and duration. Red areas in the top part of the plot represent statistically significant data points. <bold><italic toggle="yes">D</italic></bold>, Sensibility of the system to detect locomotor-induced arousal fluctuations. Average pupil size is significantly affected by the behavioral states of the animal. During running epochs (Moving) the pupil is significantly more dilated than during the resting state (Stationary). ***<italic toggle="yes">p</italic> &gt; 0.001.</p>
        </caption>
        <graphic xlink:href="ENEURO.0122-21.2021_f002" position="float"/>
      </fig>
      <fig position="float" id="F3" fig-type="figure">
        <label>Figure 3.</label>
        <caption>
          <p>Pupillometry and VR in head-fixed mice. <bold><italic toggle="yes">A</italic></bold>, Setup for head-fixed pupillometry in VR in the awake mouse, showing the habituation phase (left), in which only the virtual corridor is shown, and the stimulation phase, in which a visual stimulus appears above the virtual corridor, in the binocular portion of the visual field. <bold><italic toggle="yes">B</italic></bold>, The average fluctuations of pupillometry and velocity in all experimental mice. Dashed gray areas represent the onset and duration of auditory stimuli. <bold><italic toggle="yes">C</italic></bold>, Average event-related transients for both pupil size and velocity. Colored areas represent stimulus onset and duration. Red areas in the top part of the plot represent statistically significant data points. <bold><italic toggle="yes">D</italic></bold>, Sensibility of the system to detect locomotor-induced arousal fluctuations. Average pupil size is significantly affected by the behavioral states of the animal. During running epochs (Moving), the pupil is significantly more dilated than during the resting state (Stationary). ***<italic toggle="yes">p</italic> &gt; 0.001.</p>
        </caption>
        <graphic xlink:href="ENEURO.0122-21.2021_f003" position="float"/>
      </fig>
      <p>These results demonstrate that CNN pupillometry can detect the mouse locomotion-induced and stimulus-evoked pupillary changes and can be used to monitor behavioral state change during head fixation experiments.</p>
    </sec>
    <sec id="s4B">
      <title>Web browser application to perform real-time pupillometry experiments</title>
      <p>To greatly expand the use of our CNN-based pupillometer, we implemented the CNN in a web browser (MEYE; <xref rid="F4" ref-type="fig">Fig. 4<italic toggle="yes">B</italic></xref>), and we tested whether it could also be used in humans. To test this possibility, we designed a simple experiment aimed to measure PLR evoked by brief flashes of light on the human eye. The experiment included 10 flash events with an interstimulus interval of 5 s (<xref rid="F4" ref-type="fig">Fig. 4<italic toggle="yes">C</italic></xref>, dashed vertical lines). The results showed a clear light-induced modulation of pupil size in correspondence with each flash onset. Aligning and averaging all the traces along with the events, PLR can be quantified in both the raw trace (change from baseline, 44.53 ± 0.67%) and <italic toggle="yes">z</italic>-scored trace (SD from baseline, 14.59 ± 2.05; <xref rid="F3" ref-type="fig">Fig. 3<italic toggle="yes">D</italic>,<italic toggle="yes">E</italic></xref>). To detect whether it is possible to measure cognitively driven pupil signals using the reliable MEYE tool, we performed pupillometry while participants executed an oddball task, a commonly used paradigm for cognitive and attentional measurement. This task is based on the principle by which pupil dilation is stronger in response to rare stimuli and can be used as a physiological marker for the detection of deviant stimuli (<xref rid="B26" ref-type="bibr">Liao et al., 2016</xref>). This experiment has been conducted by recording the same eye using both the MEYE tool and an EyeLink 1000 system. According to Google Scholar, the EyeLink 1000 system is one of the most used eye trackers in psychology, psychophysics, and neuroscience, with &gt;17,000 scientific publications mentioning this tool. During the oddball experiment, the subject was instructed to maintain fixation on a small dot presented in the center of the screen, pushing a button only when the Target stimulus appears on the screen and not responding to the Standard stimulus (<xref rid="F5" ref-type="fig">Fig. 5<italic toggle="yes">A</italic></xref>). Averaging and comparing the responses to Standard and Target gratings result in significantly stronger pupil dilation for the Target stimulus than the Standard stimulus, which is detected by both of the recording systems (MEYE: <italic toggle="yes">p</italic> &lt; 0.001, paired <italic toggle="yes">t</italic> test; EyeLink: <italic toggle="yes">p</italic> &lt; 0.001, paired <italic toggle="yes">t</italic> test; <xref rid="F5" ref-type="fig">Fig. 5<italic toggle="yes">B</italic>,<italic toggle="yes">C</italic></xref>). No differences have been found for the responses evoked by the Target stimulus between the MEYE tool and the EyeLink system (<italic toggle="yes">p</italic> = 0.327, paired <italic toggle="yes">t</italic> test; <xref rid="F4" ref-type="fig">Fig. 4<italic toggle="yes">B</italic></xref>, inset). Moreover, the single-subject pupillary evoked amplitudes show a significant positive correlation between the two techniques (ρ = 0.88, <italic toggle="yes">p</italic> = 0.01, Spearman correlation) with &gt;75% of the variability explained by the linear model. Pupil size is known to covary with eye position in video-based measurements (<xref rid="B19" ref-type="bibr">Hayes and Petrov, 2016</xref>), producing foreshortening of the pupillary image because the camera is fixed but the eye rotates. To overcome this issue, there are several possible solutions, as follows: the simplest one requires constant fixation throughout each trial, but, if this requirement cannot be satisfied (e.g., in sentence reading), the position of the pupil at each sample can be used to correct and mitigate the estimation error. Thus, we decided to quantify the agreement between positional outputs provided by MEYE and EyeLink for horizontal eye movements. We designed the following two tasks: in the first task, a dot smoothly traveled horizontally on the screen from left to right and vice versa at a velocity of 8°/s and spanning 20°, producing slow and smooth pursuit eye movements. In the other experiment, a dot jumped every 5 s from one position to the other (spanning 20°), producing large, fast, and abrupt saccades. Results (<xref rid="F4" ref-type="fig">Fig. 4<italic toggle="yes">D</italic></xref>) show that smooth pursuit movements generate a sinusoidal change of position with a good agreement between both systems (MAE, 0.04). The second task, inducing saccades, produces a slightly larger error (MAE, 0.073). This error is mainly because of the much lower sampling rate of MEYE (MEYE, 15 fps; EyeLink, 1000 fps). This means that even if MEYE provides the exact positional information for each sample, it has a lower performance in adequately describing fast eye movements, such as saccades. Thus, MEYE provides the data required for <italic toggle="yes">post hoc</italic> correction of pupil measures, although it should be used with caution for measuring saccades.</p>
      <fig position="float" id="F4" fig-type="figure">
        <label>Figure 4.</label>
        <caption>
          <p>Web browser pupillometry experiment. <bold><italic toggle="yes">A</italic></bold>, Experimental setup for running the PLR stimulation and in the meantime performing pupillometric recordings. The PC is connected to the Internet by running an instance of the MEYE tool in the web browser. A USB camera, equipped with an IR light source, is focused on the eye of the subject. The photic stimulus is delivered using an LED array driven by an Arduino Due microcontroller board. The Arduino Due board is connected to the PC, emulating a keyboard and sending keystroke stimulus triggers to the MEYE tool. <bold><italic toggle="yes">B</italic></bold>, A picture of MEYE GUI. The subject during the recording is visualized as a streaming video. An ROI is used to locate the eye, and a preview of the estimation of the pupil is superimposed on the image of the subject. The GUI allows to set different parameters of postprocessing (map thresholding and refinement via mathematical morphology). <bold><italic toggle="yes">C</italic></bold>, Raw trace of the experiment (blue). Dashed lines locate the onset of flash stimuli. The green rectangles locate the onset and duration of blinks. The samples corresponding to blinks are removed and linearly interpolated (in red). <bold><italic toggle="yes">D</italic></bold>, Average event-related transient to flash stimulation in raw values. After the onset of the stimulus (dashed line), a strong constriction of the pupil is observed (44.53%). <bold><italic toggle="yes">E</italic></bold>, The <italic toggle="yes">z</italic>-score of the average event-related transient seen in <bold><italic toggle="yes">D</italic></bold>. The average nadir amplitude is 14.59 SDs from baseline.</p>
        </caption>
        <graphic xlink:href="ENEURO.0122-21.2021_f004" position="float"/>
      </fig>
      <fig position="float" id="F5" fig-type="figure">
        <label>Figure 5.</label>
        <caption>
          <p>Cognitively driven pupillary changes. <bold><italic toggle="yes">A</italic></bold>, Visual oddball procedure. The participant is instructed to fixate a small red dot placed at the center of the screen and to push a button only when the Target visual stimulus appears. <bold><italic toggle="yes">B</italic></bold>, Average pupil waveforms. Average pupil response to Standard and Target stimuli for both the MEYE tool (blue, left) and the EyeLink system (red, right). The inset represents comparison between the evoked response to the Target stimulus in both setups. <bold><italic toggle="yes">C</italic></bold>, Average pupil response. Difference between the Standard and Target stimuli recording using the MEYE tool (top) and the EyeLink system (middle). The bottom graph represents the correlation between MEYE and EyeLink data. <bold><italic toggle="yes">D</italic></bold>, Eye movement data. Comparison between the MEYE tool (blue) and EyeLink system (red) during smooth pursuit task (top) and saccades (bottom).</p>
        </caption>
        <graphic xlink:href="ENEURO.0122-21.2021_f005" position="float"/>
      </fig>
    </sec>
    <sec id="s4C">
      <title>Web browser application to perform pupillometry on videos</title>
      <p>MEYE can also be used as an offline tool to analyze pupillometry videos in various file formats, depending on the video codec installed in the web browser. To demonstrate the feasibility of performing pupillometry on videos captured in a variety of situations and in both mice and humans, we sampled three videos with a duration of 40 s from different experiments carried in our laboratory. Each video can be loaded as a demonstration in the web app to reproduce the same plots seen in <xref rid="F6" ref-type="fig">Figure 6</xref>. Three conditions were analyzed. The first condition can be found by pressing the “Mouse” button in the DEMO section of the graphical UI (GUI). It depicts a head-fixed mouse running on a circular treadmill under IR illumination and watching a uniform gray screen at 10 cd/m<sup>2</sup> (<xref rid="F6" ref-type="fig">Fig. 6<italic toggle="yes">A</italic></xref>). The second is a mouse under a two-photon microscope (button “2P mouse”), walking on a cylindrical treadmill and showing clear dilation events because of locomotion (<xref rid="F6" ref-type="fig">Fig. 6<italic toggle="yes">B</italic></xref>). The third is found by pressing the button “Human,” starting 40 s of footage of a human subject wearing VR goggles projecting a uniform gray at 15 cd/m<sup>2</sup> (<xref rid="F6" ref-type="fig">Fig. 6<italic toggle="yes">C</italic></xref>). These results show that offline pupillometry can be performed in various conditions and in both mice and humans.</p>
      <fig position="float" id="F6" fig-type="figure">
        <label>Figure 6.</label>
        <caption>
          <p>Offline movies analysis. <bold><italic toggle="yes">A</italic></bold>, Awake head-fixed mouse running on a treadmill, recorded for 40 s. The gray area represents a blink, and the trace of the blink is removed and linearly interpolated (red line). <bold><italic toggle="yes">B</italic></bold>, Awake mouse during two-photon calcium imaging. Here a brighter pupil is clearly visible with respect to <bold><italic toggle="yes">A</italic></bold>. Blinking epochs are removed and linearly interpolated. <bold><italic toggle="yes">C</italic></bold>, Pupillometry performed on a human subject, with a higher blinking rate with respect to mice. In all figures, the inset images represent the ROIs.</p>
        </caption>
        <graphic xlink:href="ENEURO.0122-21.2021_f006" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s5">
    <title>Discussion</title>
    <p>In this work, we demonstrated that MEYE is a sensitive tool that can be used to study pupil dynamics in both humans and mice. Furthermore, by providing eye position, MEYE allows <italic toggle="yes">post hoc</italic> control of the possible effects of eye movements on pupil measures (<xref rid="B19" ref-type="bibr">Hayes and Petrov, 2016</xref>). MEYE can detect both locomotion-induced and stimulus-evoked pupil changes with a peak latency of 1 s in a variety of conditions: mice with black pupils in normal illumination conditions; and mice with bright pupils resulting from laser infrared illumination. This flexibility allows the use of MEYE in combination with two-photon, wide-field imaging, and electrophysiological techniques widely adopted in awake or anesthetized mice. Furthermore, MEYE can be used to design stand-alone experiments using cost-effective hardware with performance comparable with that of state-of-the-art commercial software. In this experiment, we used a USB webcam with a varifocal objective that allows focal adjustment concentrated on the eye. The cost of the imaging equipment is &lt;50€ (<xref rid="T2" ref-type="table">Tables 2</xref>, <xref rid="T3" ref-type="table">3</xref>) and requires no knowledge of coding to set up. The flashing stimulus apparatus requires a basic understanding of Arduino boards and can be assembled at a price of &lt;50€. The overall cost of the apparatus is &lt;100€. Our code can be used in two different ways, to satisfy many needs. One way relies on the stand-alone web browser tool, which allows running MEYE on almost any device, from scientific workstations to notebooks or even smartphones. The other way uses a dedicated Python script running the CNN locally on a workstation. This latter case is suited for experiments with specific requirements, like high and stable frame rate or online processing of pupil size in which on-the-fly pupil computer interaction is required.</p>
    <table-wrap position="float" id="T2">
      <label>Table 2</label>
      <caption>
        <p>Hardware for PLR</p>
      </caption>
      <table frame="hsides" rules="none">
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">Part no.</th>
            <th align="left" rowspan="1" colspan="1">Description</th>
            <th align="left" rowspan="1" colspan="1">Quantity</th>
            <th align="left" rowspan="1" colspan="1">Price (€)</th>
            <th align="left" rowspan="1" colspan="1">Store</th>
            <th align="left" rowspan="1" colspan="1">Manufacturer</th>
          </tr>
        </thead>
        <tbody valign="top">
          <tr>
            <td align="left" rowspan="1" colspan="1">Walfront5k3psmv97x</td>
            <td align="left" rowspan="1" colspan="1">USB webcam</td>
            <td align="char" char="." rowspan="1" colspan="1">1</td>
            <td align="char" char="." rowspan="1" colspan="1">33.48</td>
            <td align="left" rowspan="1" colspan="1">Amazon</td>
            <td align="left" rowspan="1" colspan="1">Walfront</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">149129</td>
            <td align="left" rowspan="1" colspan="1">Varifocal M12 lens</td>
            <td align="char" char="." rowspan="1" colspan="1">1</td>
            <td align="char" char="." rowspan="1" colspan="1">12.03</td>
            <td align="left" rowspan="1" colspan="1">Amazon</td>
            <td align="left" rowspan="1" colspan="1">Sodial</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">A000062</td>
            <td align="left" rowspan="1" colspan="1">Microcontroller Arduino Due</td>
            <td align="char" char="." rowspan="1" colspan="1">1</td>
            <td align="char" char="." rowspan="1" colspan="1">35</td>
            <td align="left" rowspan="1" colspan="1">Arduino store</td>
            <td align="left" rowspan="1" colspan="1">Arduino</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">1312</td>
            <td align="left" rowspan="1" colspan="1">4 NeoPixel RGB LEDs</td>
            <td align="char" char="." rowspan="1" colspan="1">1</td>
            <td align="char" char="." rowspan="1" colspan="1">6.53</td>
            <td align="left" rowspan="1" colspan="1">Adafruit</td>
            <td align="left" rowspan="1" colspan="1">Adafruit</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">Total amount</td>
            <td align="left" rowspan="1" colspan="1"/>
            <td align="left" rowspan="1" colspan="1"/>
            <td align="char" char="." rowspan="1" colspan="1">87.04</td>
            <td align="left" rowspan="1" colspan="1"/>
            <td align="left" rowspan="1" colspan="1"/>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap position="float" id="T3">
      <label>Table 3</label>
      <caption>
        <p>Statistical table</p>
      </caption>
      <table frame="hsides" rules="none">
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <col align="left" valign="top" span="1"/>
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">Figure</th>
            <th align="left" rowspan="1" colspan="1">Type of test</th>
            <th align="left" rowspan="1" colspan="1">Statistical data</th>
          </tr>
        </thead>
        <tbody valign="top">
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F1" ref-type="fig">Figure 1</xref>
            </td>
            <td align="left" rowspan="1" colspan="1">No statistical tests</td>
            <td align="left" rowspan="1" colspan="1"/>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F2" ref-type="fig">Figure 2<italic toggle="yes">C</italic></xref>
            </td>
            <td align="left" rowspan="1" colspan="1">Two-way repeated-measures ANOVA</td>
            <td align="left" rowspan="1" colspan="1">Pupil: <italic toggle="yes">F</italic> = 10.56, <italic toggle="yes">p</italic> &lt; 0.001, ng<sup>2</sup> = 0.43<break/>Velocity: <italic toggle="yes">F</italic> = 1.47, <italic toggle="yes">p</italic> &lt; 0.01, ng<sup>2</sup> = 0.2</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F2" ref-type="fig">Figure 2<italic toggle="yes">D</italic></xref>
            </td>
            <td align="left" rowspan="1" colspan="1">Parametric paired <italic toggle="yes">t</italic> test</td>
            <td align="left" rowspan="1" colspan="1"><italic toggle="yes">T</italic> = −8.4395, <italic toggle="yes">p</italic> &lt; 0.01, BF<sub>10</sub> = 14.1</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F3" ref-type="fig">Figure 3<italic toggle="yes">C</italic></xref>
            </td>
            <td align="left" rowspan="1" colspan="1">Two-way repeated-measures ANOVA</td>
            <td align="left" rowspan="1" colspan="1">Pupil: <italic toggle="yes">F</italic> = 7.42, <italic toggle="yes">p</italic> &lt; 0.001, ng<sup>2</sup> = 0.42<break/>Velocity: <italic toggle="yes">F</italic> = 0.75, <italic toggle="yes">p</italic> &lt; 0.96, ng<sup>2</sup> = 0.1)</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F3" ref-type="fig">Figure 3<italic toggle="yes">D</italic></xref>
            </td>
            <td align="left" rowspan="1" colspan="1">Parametric paired <italic toggle="yes">t</italic> test</td>
            <td align="left" rowspan="1" colspan="1"><italic toggle="yes">T</italic> = −11.1, <italic toggle="yes">p</italic> &lt; 0.001, BF<sub>10</sub> = 77.5</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F5" ref-type="fig">Figure 5<italic toggle="yes">C</italic></xref>
              <xref rid="F1" ref-type="fig">1</xref>
            </td>
            <td align="left" rowspan="1" colspan="1">Parametric paired <italic toggle="yes">t</italic> test</td>
            <td align="left" rowspan="1" colspan="1"><italic toggle="yes">T</italic> = −4.64, <italic toggle="yes">p</italic> &lt; 0.001, BF<sub>10</sub> = 62.9</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F5" ref-type="fig">Figure 5<italic toggle="yes">C</italic></xref>
              <xref rid="F2" ref-type="fig">2</xref>
            </td>
            <td align="left" rowspan="1" colspan="1">Parametric paired <italic toggle="yes">t</italic> test</td>
            <td align="left" rowspan="1" colspan="1"><italic toggle="yes">T</italic> = −5.41, <italic toggle="yes">p</italic> &lt; 0.001, BF<sub>10</sub> = 204.03</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F5" ref-type="fig">Figure 5<italic toggle="yes">C</italic></xref>
              <xref rid="F3" ref-type="fig">3</xref>
            </td>
            <td align="left" rowspan="1" colspan="1">Spearman correlation</td>
            <td align="left" rowspan="1" colspan="1"><italic toggle="yes">r</italic> = 0.9; 95% CI = [0.46, 0.98], <italic toggle="yes">r</italic><sup>2</sup> = 0.78, <italic toggle="yes">p</italic> &lt; 0.01, power = 0.9</td>
          </tr>
          <tr>
            <td align="left" rowspan="1" colspan="1">
              <xref rid="F6" ref-type="fig">Figure 6</xref>
            </td>
            <td align="left" rowspan="1" colspan="1">No statistical tests</td>
            <td align="left" rowspan="1" colspan="1"/>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="TF1">
          <p>BF, Bayes factor.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <sec id="s5A">
      <title>Comparison with other methods</title>
      <p>Valid open-source and commercial alternatives exist, but most of them are dedicated to gazing tracking and/or pupillometry. Commercial options are costly (<ext-link xlink:href="https://tobii.com" ext-link-type="uri">https://tobii.com</ext-link>, <ext-link xlink:href="https://sr-research.com" ext-link-type="uri">https://sr-research.com</ext-link>, <ext-link xlink:href="https://neuroptics.com" ext-link-type="uri">https://neuroptics.com</ext-link>), whereas open-source code requires programming knowledge and most open-source alternatives are explicitly dedicated to one species (<xref rid="B53" ref-type="bibr">Yiu et al., 2019</xref>; <xref rid="B41" ref-type="bibr">Privitera et al., 2020</xref>). One study (<xref rid="B41" ref-type="bibr">Privitera et al., 2020</xref>) assessed pupil dilation in mice through DeepLabCut (<xref rid="B29" ref-type="bibr">Mathis et al., 2018</xref>), a technique for 3D markerless pose estimation based on transfer learning. This approach, albeit powerful, is conceptually different since it is trained on user-defined key points instead of on using the entire pupil to perform semantic segmentation. The former technique is more suited to track and locate arbitrary objects on an image, while the latter technique is focused on a more precise quantification of even small changes of the object area since pixelwise segmentation masks are refined iteratively using local and global contexts.</p>
      <p>We compared our architecture with the first stage of DeepLabCut, which implements an image segmentation task (leaving out keypoint extraction), which is in common with our pipeline.</p>
      <p>For a fair comparison, we trained the image segmentation networks used in DeepLabCut on our dataset. <xref rid="T4" ref-type="table">Table 4</xref> shows that our architecture can achieve a higher number of frames per second and a superior segmentation performance (higher dice coefficient) with respect to the DeepLabCut models.</p>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>Comparison among CNN models</p>
        </caption>
        <table frame="hsides" rules="none">
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <col align="left" valign="top" span="1"/>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">CNN model</th>
              <th align="left" rowspan="1" colspan="1">Train/test set</th>
              <th align="left" rowspan="1" colspan="1">mDice</th>
              <th align="left" rowspan="1" colspan="1">FPS (Web<italic toggle="yes"><sup>a</sup></italic>)</th>
              <th align="left" rowspan="1" colspan="1">FPS (Keras<italic toggle="yes"><sup>b</sup></italic>)</th>
              <th align="left" rowspan="1" colspan="1">FLOPS</th>
              <th align="left" rowspan="1" colspan="1">Parameters, <italic toggle="yes">n</italic></th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" rowspan="1" colspan="1">mini-UNet</td>
              <td align="left" rowspan="1" colspan="1">Human/mouse eyes</td>
              <td rowspan="1" colspan="1">84.0%</td>
              <td rowspan="1" colspan="1">23.2</td>
              <td rowspan="1" colspan="1">45.2</td>
              <td rowspan="1" colspan="1">0.2G</td>
              <td align="left" rowspan="1" colspan="1">0.03 M</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DeepLabv3+/ResNet-50</td>
              <td align="left" rowspan="1" colspan="1">ImageNet + finetune on<break/>human/mouse eyes</td>
              <td rowspan="1" colspan="1">80.1%</td>
              <td rowspan="1" colspan="1">&lt;1</td>
              <td rowspan="1" colspan="1">28.7</td>
              <td rowspan="1" colspan="1">14.1G</td>
              <td align="left" rowspan="1" colspan="1">26.8 M</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">DeepLabv3+/Lite-MobileNet-<break/>V3-Small</td>
              <td align="left" rowspan="1" colspan="1">ImageNet + fine-tune on<break/>human/mouse eyes</td>
              <td rowspan="1" colspan="1">69.0%</td>
              <td rowspan="1" colspan="1">18.8</td>
              <td rowspan="1" colspan="1">34.8</td>
              <td rowspan="1" colspan="1">0.3G</td>
              <td align="left" rowspan="1" colspan="1">1.1 M</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="TF2">
            <p><italic toggle="yes"><sup>a</sup></italic>Dell Laptop: Intel(R) Core(TM) i7-9750H CPU @ 2.60 GHz; Intel UHD graphics 630 GPU; TensorFlow.js; backend: WebGL Browser: Microsoft edge 90.0.818.56.</p>
          </fn>
          <fn id="TF3">
            <p><italic toggle="yes"><sup>b</sup></italic>Ubuntu PC: Intel(R) Core(TM) i9-9900K CPU @ 3.60 GHz; GeForce RTX 2080 Ti GPU; Python 3.6.9 + TensorFlow 2.4.1. FLOPS, FLoating point Operations Per Second; G, gigaFLOPS; M, mefaFLOPS.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="s5B">
      <title>Possible preclinical and clinical applications</title>
      <p>The possible contribution of the web app technology resides in its portability: no software needs to be manually installed, and configuration is minimal. Only a clear IR image of the subject’s eye is required. The performances of the tool are dependent on the host computer, but it runs at &gt;10 fps in most of the machines tested. This advantage is particularly useful for settings with limited resources and space or for educational purposes. Web browser-embedded pupillometry will also be crucial for human scientific research, and clinical and preventive medicine. It would also be a promising tool in the recently growing field of telemedicine, given its minimal setup that can run on an average notebook computer or even on a smartphone, and that it allows possible large-scale recruitment of subjects directly in their own homes. This greatly facilitates infant, psychiatric, and motor-impaired patients’ compliance, particularly for longitudinal research designs. We also released an open-source database of eyes composed of &gt;11,000 images in various settings: head-fixed mice (black pupil); head-fixed two-photon imaging mice (white pupil); and human eyes. This dataset will grow over time to introduce new species and new use cases to increase, update, and strengthen MEYE performance. An updated list of planned and executed developments of MEYE can be found in the “Future Development” section of the GitHub Wiki. The possible scenarios can be further expanded in the future, because of the dynamic nature of CNN. It can be updated from the source, providing instantaneous updates on each computer running an instance of the program. Our hope is to create a community that refines and consolidates pupillometric performances to produce a tool that can be applied in different environments.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <p>We thank NVIDIA for donation of the Jetson AGX Xavier Developer Kit. We also thank Dr. Viviana Marchi, Dr. Grazia Rutigliano, and Dr. Carlo Campagnoli for critical reading of the manuscript.</p>
  </ack>
  <ref-list content-type="nameDate">
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><string-name><surname>Aggius-Vella</surname><given-names>E</given-names></string-name>, <string-name><surname>Gori</surname><given-names>M</given-names></string-name>, <string-name><surname>Animali</surname><given-names>S</given-names></string-name>, <string-name><surname>Campus</surname><given-names>C</given-names></string-name>, <string-name><surname>Binda</surname><given-names>P</given-names></string-name> (<year>2020</year>) <article-title>Non-spatial skills differ in the front and rear peri-personal space</article-title>. <source>Neuropsychologia</source>
<volume>147</volume>:<fpage>107619</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107619</pub-id>
<?supplied-pmid 32898519?><pub-id pub-id-type="pmid">32898519</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><string-name><surname>Aleman</surname><given-names>TS</given-names></string-name>, <string-name><surname>Jacobson</surname><given-names>SG</given-names></string-name>, <string-name><surname>Chico</surname><given-names>JD</given-names></string-name>, <string-name><surname>Scott</surname><given-names>ML</given-names></string-name>, <string-name><surname>Cheung</surname><given-names>AY</given-names></string-name>, <string-name><surname>Windsor</surname><given-names>EAM</given-names></string-name>, <string-name><surname>Furushima</surname><given-names>M</given-names></string-name>, <string-name><surname>Michael Redmond</surname><given-names>T</given-names></string-name>, <string-name><surname>Bennett</surname><given-names>J</given-names></string-name>, <string-name><surname>Palczewski</surname><given-names>K</given-names></string-name>, <string-name><surname>Cideciyan</surname><given-names>AV</given-names></string-name> (<year>2004</year>) <article-title>Impairment of the transient pupillary light reflex in Rpe65<sup>−/−</sup> mice and humans with leber congenital amaurosis</article-title>. <source>Invest Ophthalmol Vis Sci</source>
<volume>45</volume>:<fpage>1259</fpage>–<lpage>1271</lpage>. <pub-id pub-id-type="doi">10.1167/iovs.03-1230</pub-id>
<?supplied-pmid 15037595?><pub-id pub-id-type="pmid">15037595</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><string-name><surname>Angulo-Chavira</surname><given-names>AQ</given-names></string-name>, <string-name><surname>García</surname><given-names>O</given-names></string-name>, <string-name><surname>Arias-Trejo</surname><given-names>N</given-names></string-name> (<year>2017</year>) <article-title>Pupil response and attention skills in Down syndrome</article-title>. <source>Res Dev Disabil</source>
<volume>70</volume>:<fpage>40</fpage>–<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1016/j.ridd.2017.08.011</pub-id>
<?supplied-pmid 28888155?><pub-id pub-id-type="pmid">28888155</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><string-name><surname>Artoni</surname><given-names>P</given-names></string-name>, <string-name><surname>Piffer</surname><given-names>A</given-names></string-name>, <string-name><surname>Vinci</surname><given-names>V</given-names></string-name>, <string-name><surname>LeBlanc</surname><given-names>J</given-names></string-name>, <string-name><surname>Nelson</surname><given-names>CA</given-names></string-name>, <string-name><surname>Hensch</surname><given-names>TK</given-names></string-name>, <string-name><surname>Fagiolini</surname><given-names>M</given-names></string-name> (<year>2020</year>) <article-title>Deep learning of spontaneous arousal fluctuations detects early cholinergic defects across neurodevelopmental mouse models and patients</article-title>. <source>Proc Natl Acad Sci U S A</source>
<volume>117</volume>:<fpage>23298</fpage>–<lpage>23303</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1820847116</pub-id>
<?supplied-pmid 31332003?><pub-id pub-id-type="pmid">31332003</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><string-name><surname>Azevedo-Santos</surname><given-names>IF</given-names></string-name>, <string-name><surname>DeSantana</surname><given-names>JM</given-names></string-name> (<year>2018</year>) <article-title>Pain measurement techniques: spotlight on mechanically ventilated patients</article-title>. <source>J Pain Res</source>
<volume>11</volume>:<fpage>2969</fpage>–<lpage>2980</lpage>. <pub-id pub-id-type="doi">10.2147/JPR.S151169</pub-id>
<?supplied-pmid 30538536?><pub-id pub-id-type="pmid">30538536</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><string-name><surname>Beggiato</surname><given-names>M</given-names></string-name>, <string-name><surname>Hartwich</surname><given-names>F</given-names></string-name>, <string-name><surname>Krems</surname><given-names>J</given-names></string-name> (<year>2018</year>) <article-title>Using smartbands, pupillometry and body motion to detect discomfort in automated driving</article-title>. <source>Front Hum Neurosci</source>
<volume>12</volume>:<fpage>338</fpage>. <pub-id pub-id-type="doi">10.3389/fnhum.2018.00338</pub-id>
<?supplied-pmid 30319372?><pub-id pub-id-type="pmid">30319372</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><string-name><surname>Binda</surname><given-names>P</given-names></string-name>, <string-name><surname>Pereverzeva</surname><given-names>M</given-names></string-name>, <string-name><surname>Murray</surname><given-names>SO</given-names></string-name> (<year>2013a</year>) <article-title>Attention to bright surfaces enhances the pupillary light reflex</article-title>. <source>J Neurosci</source>
<volume>33</volume>:<fpage>2199</fpage>–<lpage>2204</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3440-12.2013</pub-id>
<?supplied-pmid 23365255?><pub-id pub-id-type="pmid">23365255</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><string-name><surname>Binda</surname><given-names>P</given-names></string-name>, <string-name><surname>Pereverzeva</surname><given-names>M</given-names></string-name>, <string-name><surname>Murray</surname><given-names>SO</given-names></string-name> (<year>2013b</year>) <article-title>Pupil constrictions to photographs of the sun</article-title>. <source>J Vis</source>
<volume>13</volume>(<issue>6</issue>):8, <fpage>1</fpage>–<lpage>9</lpage>. <?supplied-pmid 23685391?><pub-id pub-id-type="pmid">23685391</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><string-name><surname>Blaser</surname><given-names>E</given-names></string-name>, <string-name><surname>Eglington</surname><given-names>L</given-names></string-name>, <string-name><surname>Carter</surname><given-names>AS</given-names></string-name>, <string-name><surname>Kaldy</surname><given-names>Z</given-names></string-name> (<year>2014</year>) <article-title>Pupillometry reveals a mechanism for the autism spectrum disorder (ASD) advantage in visual tasks</article-title>. <source>Sci Rep</source>
<volume>4</volume>:<fpage>4301</fpage>. <pub-id pub-id-type="doi">10.1038/srep04301</pub-id>
<?supplied-pmid 24603348?><pub-id pub-id-type="pmid">24603348</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><string-name><surname>Brainard</surname><given-names>DH</given-names></string-name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source>
<volume>10</volume>:<fpage>433</fpage>–<lpage>436</lpage>. <?supplied-pmid 9176952?><pub-id pub-id-type="pmid">9176952</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><string-name><surname>Burley</surname><given-names>DT</given-names></string-name>, <string-name><surname>van Goozen</surname><given-names>SHM</given-names></string-name> (<year>2020</year>) <article-title>Pupil response to affective stimuli: a biomarker of early conduct problems in young children</article-title>. <source>J Abnorm Child Psychol</source>
<volume>48</volume>:<fpage>693</fpage>–<lpage>701</lpage>. <pub-id pub-id-type="doi">10.1007/s10802-020-00620-z</pub-id>
<?supplied-pmid 31982978?><pub-id pub-id-type="pmid">31982978</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><string-name><surname>Charier</surname><given-names>D</given-names></string-name>, <string-name><surname>Vogler</surname><given-names>M-C</given-names></string-name>, <string-name><surname>Zantour</surname><given-names>D</given-names></string-name>, <string-name><surname>Pichot</surname><given-names>V</given-names></string-name>, <string-name><surname>Martins-Baltar</surname><given-names>A</given-names></string-name>, <string-name><surname>Courbon</surname><given-names>M</given-names></string-name>, <string-name><surname>Roche</surname><given-names>F</given-names></string-name>, <string-name><surname>Vassal</surname><given-names>F</given-names></string-name>, <string-name><surname>Molliex</surname><given-names>S</given-names></string-name> (<year>2019</year>) <article-title>Assessing pain in the postoperative period: analgesia Nociception Index<sup>TM</sup> versus pupillometry</article-title>. <source>Br J Anaesth</source>
<volume>123</volume>:<fpage>e322</fpage>–<lpage>e327</lpage>. <pub-id pub-id-type="doi">10.1016/j.bja.2018.09.031</pub-id>
<?supplied-pmid 30915996?><pub-id pub-id-type="pmid">30915996</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="book"><string-name><surname>Chen</surname><given-names>L-C</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Papandreou</surname><given-names>G</given-names></string-name>, <string-name><surname>Schroff</surname><given-names>F</given-names></string-name>, <string-name><surname>Adam</surname><given-names>H</given-names></string-name> (<year>2018</year>) <part-title>Encoder-decoder with atrous separable convolution for semantic image segmentation</part-title>. In: <source>Computer Vision - ECCV 2018 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer International</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><string-name><surname>Chougule</surname><given-names>PS</given-names></string-name>, <string-name><surname>Najjar</surname><given-names>RP</given-names></string-name>, <string-name><surname>Finkelstein</surname><given-names>MT</given-names></string-name>, <string-name><surname>Kandiah</surname><given-names>N</given-names></string-name>, <string-name><surname>Milea</surname><given-names>D</given-names></string-name> (<year>2019</year>) <article-title>Light-Induced Pupillary Responses in Alzheimer’s Disease</article-title>. <source>Front Neurol</source>
<volume>10</volume>:<fpage>360</fpage>. <pub-id pub-id-type="doi">10.3389/fneur.2019.00360</pub-id>
<?supplied-pmid 31031692?><pub-id pub-id-type="pmid">31031692</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><string-name><surname>Connelly</surname><given-names>MA</given-names></string-name>, <string-name><surname>Brown</surname><given-names>JT</given-names></string-name>, <string-name><surname>Kearns</surname><given-names>GL</given-names></string-name>, <string-name><surname>Anderson</surname><given-names>RA</given-names></string-name>, <string-name><surname>St Peter</surname><given-names>SD</given-names></string-name>, <string-name><surname>Neville</surname><given-names>KA</given-names></string-name> (<year>2014</year>) <article-title>Pupillometry: a non-invasive technique for pain assessment in paediatric patients</article-title>. <source>Arch Dis Child</source>
<volume>99</volume>:<fpage>1125</fpage>–<lpage>1131</lpage>. <pub-id pub-id-type="doi">10.1136/archdischild-2014-306286</pub-id>
<?supplied-pmid 25187497?><pub-id pub-id-type="pmid">25187497</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><string-name><surname>El Ahmadieh</surname><given-names>TY</given-names></string-name>, <string-name><surname>Bedros</surname><given-names>N</given-names></string-name>, <string-name><surname>Stutzman</surname><given-names>SE</given-names></string-name>, <string-name><surname>Nyancho</surname><given-names>D</given-names></string-name>, <string-name><surname>Venkatachalam</surname><given-names>AM</given-names></string-name>, <string-name><surname>MacAllister</surname><given-names>M</given-names></string-name>, <string-name><surname>Ban</surname><given-names>VS</given-names></string-name>, <string-name><surname>Dahdaleh</surname><given-names>NS</given-names></string-name>, <string-name><surname>Aiyagari</surname><given-names>V</given-names></string-name>, <string-name><surname>Figueroa</surname><given-names>S</given-names></string-name>, <string-name><surname>White</surname><given-names>JA</given-names></string-name>, <string-name><surname>Batjer</surname><given-names>HH</given-names></string-name>, <string-name><surname>Bagley</surname><given-names>CA</given-names></string-name>, <string-name><surname>Olson</surname><given-names>DM</given-names></string-name>, <string-name><surname>Aoun</surname><given-names>SG</given-names></string-name> (<year>2021</year>) <article-title>Automated pupillometry as a triage and assessment tool in patients with traumatic brain injury</article-title>. <source>World Neurosurg</source>
<volume>145</volume>:<fpage>e163</fpage>–<lpage>e169</lpage>. <pub-id pub-id-type="doi">10.1016/j.wneu.2020.09.152</pub-id><pub-id pub-id-type="pmid">33011358</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><string-name><surname>Frost</surname><given-names>S</given-names></string-name>, <string-name><surname>Robinson</surname><given-names>L</given-names></string-name>, <string-name><surname>Rowe</surname><given-names>CC</given-names></string-name>, <string-name><surname>Ames</surname><given-names>D</given-names></string-name>, <string-name><surname>Masters</surname><given-names>CL</given-names></string-name>, <string-name><surname>Taddei</surname><given-names>K</given-names></string-name>, <string-name><surname>Rainey-Smith</surname><given-names>SR</given-names></string-name>, <string-name><surname>Martins</surname><given-names>RN</given-names></string-name>, <string-name><surname>Kanagasingam</surname><given-names>Y</given-names></string-name> (<year>2017</year>) <article-title>Evaluation of cholinergic deficiency in preclinical Alzheimer’s disease using pupillometry</article-title>. <source>J Ophthalmology</source>
<volume>2017</volume>:<fpage>7935406</fpage>. <pub-id pub-id-type="doi">10.1155/2017/7935406</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><string-name><surname>Gajardo</surname><given-names>AIJ</given-names></string-name>, <string-name><surname>Madariaga</surname><given-names>S</given-names></string-name>, <string-name><surname>Maldonado</surname><given-names>PE</given-names></string-name> (<year>2019</year>) <article-title>Autonomic nervous system assessment by pupillary response as a potential biomarker for cardiovascular risk: a pilot study</article-title>. <source>J Clin Neurosci</source>
<volume>59</volume>:<fpage>41</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1016/j.jocn.2018.11.015</pub-id><pub-id pub-id-type="pmid">30448298</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><string-name><surname>Hayes</surname><given-names>TR</given-names></string-name>, <string-name><surname>Petrov</surname><given-names>AA</given-names></string-name> (<year>2016</year>) <article-title>Mapping and correcting the influence of gaze position on pupil size measurements</article-title>. <source>Behav Res Methods</source>
<volume>48</volume>:<fpage>510</fpage>–<lpage>527</lpage>. <pub-id pub-id-type="doi">10.3758/s13428-015-0588-x</pub-id>
<?supplied-pmid 25953668?><pub-id pub-id-type="pmid">25953668</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="book"><string-name><surname>He</surname><given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Ren</surname><given-names>S</given-names></string-name>, <string-name><surname>Sun</surname><given-names>J</given-names></string-name> (<year>2016</year>) <part-title>Identity mappings in deep residual networks</part-title>. In: <source>Computer vision — ECCV 2016: 14th European Conference, Amsterdam, the Netherlands, October 11-14, 2016, Proceedings. Part VII</source>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer International</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><string-name><surname>Hersman</surname><given-names>S</given-names></string-name>, <string-name><surname>Allen</surname><given-names>D</given-names></string-name>, <string-name><surname>Hashimoto</surname><given-names>M</given-names></string-name>, <string-name><surname>Brito</surname><given-names>SI</given-names></string-name>, <string-name><surname>Anthony</surname><given-names>TE</given-names></string-name> (<year>2020</year>) <article-title>Stimulus salience determines defensive behaviors elicited by aversively conditioned serial compound auditory stimuli</article-title>. <source>Elife</source>
<volume>9</volume>:<fpage>e53803</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.53803</pub-id><pub-id pub-id-type="pmid">32216876</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="book"><string-name><surname>Howard</surname><given-names>A</given-names></string-name>, <string-name><surname>Sandler</surname><given-names>M</given-names></string-name>, <string-name><surname>Chen</surname><given-names>B</given-names></string-name>, <string-name><surname>Wang</surname><given-names>W</given-names></string-name>, <string-name><surname>Chen</surname><given-names>L-C</given-names></string-name>, <string-name><surname>Tan</surname><given-names>M</given-names></string-name>, <string-name><surname>Chu</surname><given-names>G</given-names></string-name>, <string-name><surname>Vasudevan</surname><given-names>V</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Pang</surname><given-names>R</given-names></string-name>, <string-name><surname>Adam</surname><given-names>H</given-names></string-name>, <string-name><surname>Le</surname><given-names>Q</given-names></string-name> (<year>2019</year>) <part-title>Searching for MobileNetV3</part-title>. In: <source>2019 IEEE/CVF international conference on computer vision (ICCV)</source>. <publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>IEEE</publisher-name>. <pub-id pub-id-type="doi">10.1109/ICCV.2019.00140</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><string-name><surname>Iadanza</surname><given-names>E</given-names></string-name>, <string-name><surname>Goretti</surname><given-names>F</given-names></string-name>, <string-name><surname>Sorelli</surname><given-names>M</given-names></string-name>, <string-name><surname>Melillo</surname><given-names>P</given-names></string-name>, <string-name><surname>Pecchia</surname><given-names>L</given-names></string-name>, <string-name><surname>Simonelli</surname><given-names>F</given-names></string-name>, <string-name><surname>Gherardelli</surname><given-names>M</given-names></string-name> (<year>2020</year>) <article-title>Automatic detection of genetic diseases in pediatric age using pupillometry</article-title>. <source>IEEE Access</source>
<volume>8</volume>:<fpage>34949</fpage>–<lpage>34961</lpage>. <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2973747</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><string-name><surname>Krebs</surname><given-names>RM</given-names></string-name>, <string-name><surname>Park</surname><given-names>HRP</given-names></string-name>, <string-name><surname>Bombeke</surname><given-names>K</given-names></string-name>, <string-name><surname>Boehler</surname><given-names>CN</given-names></string-name> (<year>2018</year>) <article-title>Modulation of locus coeruleus activity by novel oddball stimuli</article-title>. <source>Brain Imaging Behav</source>
<volume>12</volume>:<fpage>577</fpage>–<lpage>584</lpage>. <pub-id pub-id-type="doi">10.1007/s11682-017-9700-4</pub-id>
<?supplied-pmid 28271441?><pub-id pub-id-type="pmid">28271441</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><string-name><surname>Lee</surname><given-names>CR</given-names></string-name>, <string-name><surname>Margolis</surname><given-names>DJ</given-names></string-name> (<year>2016</year>) <article-title>Pupil dynamics reflect behavioral choice and learning in a Go/NoGo tactile decision-making task in mice</article-title>. <source>Front Behav Neurosci</source>
<volume>10</volume>:<fpage>200</fpage>. <pub-id pub-id-type="doi">10.3389/fnbeh.2016.00200</pub-id>
<?supplied-pmid 27847470?><pub-id pub-id-type="pmid">27847470</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><string-name><surname>Liao</surname><given-names>H-I</given-names></string-name>, <string-name><surname>Yoneya</surname><given-names>M</given-names></string-name>, <string-name><surname>Kidani</surname><given-names>S</given-names></string-name>, <string-name><surname>Kashino</surname><given-names>M</given-names></string-name>, <string-name><surname>Furukawa</surname><given-names>S</given-names></string-name> (<year>2016</year>) <article-title>Human pupillary dilation response to deviant auditory stimuli: effects of stimulus properties and voluntary attention</article-title>. <source>Front Neurosci</source>
<volume>10</volume>:<fpage>43</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2016.00043</pub-id>
<?supplied-pmid 26924959?><pub-id pub-id-type="pmid">26924959</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><string-name><surname>Li</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wei</surname><given-names>J-X</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>G-W</given-names></string-name>, <string-name><surname>Huang</surname><given-names>JJ</given-names></string-name>, <string-name><surname>Zingg</surname><given-names>B</given-names></string-name>, <string-name><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name><surname>Tao</surname><given-names>HW</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>LI</given-names></string-name> (<year>2021</year>) <article-title>Corticostriatal control of defense behavior in mice induced by auditory looming cues</article-title>. <source>Nat Commun</source>
<volume>12</volume>:<fpage>1040</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-21248-7</pub-id>
<?supplied-pmid 33589613?><pub-id pub-id-type="pmid">33589613</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><string-name><surname>LoTemplio</surname><given-names>S</given-names></string-name>, <string-name><surname>Silcox</surname><given-names>J</given-names></string-name>, <string-name><surname>Federmeier</surname><given-names>KD</given-names></string-name>, <string-name><surname>Payne</surname><given-names>BR</given-names></string-name> (<year>2021</year>) <article-title>Inter- and intra-individual coupling between pupillary, electrophysiological, and behavioral responses in a visual oddball task.</article-title>
<source>Psychophysiology</source>
<volume>58</volume>:<fpage>e13758</fpage>. <pub-id pub-id-type="doi">10.1111/psyp.13758</pub-id>
<?supplied-pmid 33347634?><pub-id pub-id-type="pmid">33347634</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><string-name><surname>Mathis</surname><given-names>A</given-names></string-name>, <string-name><surname>Mamidanna</surname><given-names>P</given-names></string-name>, <string-name><surname>Cury</surname><given-names>KM</given-names></string-name>, <string-name><surname>Abe</surname><given-names>T</given-names></string-name>, <string-name><surname>Murthy</surname><given-names>VN</given-names></string-name>, <string-name><surname>Mathis</surname><given-names>MW</given-names></string-name>, <string-name><surname>Bethge</surname><given-names>M</given-names></string-name> (<year>2018</year>) <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>. <source>Nat Neurosci</source>
<volume>21</volume>:<fpage>1281</fpage>–<lpage>1289</lpage>. <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
<?supplied-pmid 30127430?><pub-id pub-id-type="pmid">30127430</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><string-name><surname>Mathôt</surname><given-names>S</given-names></string-name>, <string-name><surname>Melmi</surname><given-names>J-B</given-names></string-name>, <string-name><surname>van der Linden</surname><given-names>L</given-names></string-name>, <string-name><surname>Van der Stigchel</surname><given-names>S</given-names></string-name> (<year>2016</year>) <article-title>The mind-writing pupil: a human-computer interface based on decoding of covert attention through pupillometry.</article-title>
<source>PLoS One</source>
<volume>11</volume>:<fpage>e0148805</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0148805</pub-id>
<?supplied-pmid 26848745?><pub-id pub-id-type="pmid">26848745</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><string-name><surname>McGinley</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Vinck</surname><given-names>M</given-names></string-name>, <string-name><surname>Reimer</surname><given-names>J</given-names></string-name>, <string-name><surname>Batista-Brito</surname><given-names>R</given-names></string-name>, <string-name><surname>Zagha</surname><given-names>E</given-names></string-name>, <string-name><surname>Cadwell</surname><given-names>CR</given-names></string-name>, <string-name><surname>Tolias</surname><given-names>AS</given-names></string-name>, <string-name><surname>Cardin</surname><given-names>JA</given-names></string-name>, <string-name><surname>McCormick</surname><given-names>DA</given-names></string-name> (<year>2015</year>) <article-title>Waking state: rapid variations modulate neural and behavioral responses</article-title>. <source>Neuron</source>
<volume>87</volume>:<fpage>1143</fpage>–<lpage>1161</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.012</pub-id>
<?supplied-pmid 26402600?><pub-id pub-id-type="pmid">26402600</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><string-name><surname>Montes-Lourido</surname><given-names>P</given-names></string-name>, <string-name><surname>Kar</surname><given-names>M</given-names></string-name>, <string-name><surname>Kumbam</surname><given-names>I</given-names></string-name>, <string-name><surname>Sadagopan</surname><given-names>S</given-names></string-name> (<year>2021</year>) <article-title>Pupillometry as a reliable metric of auditory detection and discrimination across diverse stimulus paradigms in animal models</article-title>. <source>Sci Rep</source>
<volume>11</volume>:<fpage>3108</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-82340-y</pub-id>
<?supplied-pmid 33542266?><pub-id pub-id-type="pmid">33542266</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><string-name><surname>Nyström</surname><given-names>P</given-names></string-name>, <string-name><surname>Gliga</surname><given-names>T</given-names></string-name>, <string-name><surname>Nilsson Jobs</surname><given-names>E</given-names></string-name>, <string-name><surname>Gredebäck</surname><given-names>G</given-names></string-name>, <string-name><surname>Charman</surname><given-names>T</given-names></string-name>, <string-name><surname>Johnson</surname><given-names>MH</given-names></string-name>, <string-name><surname>Bölte</surname><given-names>S</given-names></string-name>, <string-name><surname>Falck-Ytter</surname><given-names>T</given-names></string-name> (<year>2018</year>) <article-title>Enhanced pupillary light reflex in infancy is associated with autism diagnosis in toddlerhood</article-title>. <source>Nat Commun</source>
<volume>9</volume>:<fpage>1678</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-03985-4</pub-id>
<?supplied-pmid 29735992?><pub-id pub-id-type="pmid">29735992</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><string-name><surname>Obinata</surname><given-names>H</given-names></string-name>, <string-name><surname>Yokobori</surname><given-names>S</given-names></string-name>, <string-name><surname>Shibata</surname><given-names>Y</given-names></string-name>, <string-name><surname>Takiguchi</surname><given-names>T</given-names></string-name>, <string-name><surname>Nakae</surname><given-names>R</given-names></string-name>, <string-name><surname>Igarashi</surname><given-names>Y</given-names></string-name>, <string-name><surname>Shigeta</surname><given-names>K</given-names></string-name>, <string-name><surname>Matsumoto</surname><given-names>H</given-names></string-name>, <string-name><surname>Aiyagari</surname><given-names>V</given-names></string-name>, <string-name><surname>Olson</surname><given-names>DM</given-names></string-name>, <string-name><surname>Yokota</surname><given-names>H</given-names></string-name> (<year>2020</year>) <article-title>Early automated infrared pupillometry is superior to auditory brainstem response in predicting neurological outcome after cardiac arrest</article-title>. <source>Resuscitation</source>
<volume>154</volume>:<fpage>77</fpage>–<lpage>84</lpage>. <pub-id pub-id-type="doi">10.1016/j.resuscitation.2020.06.002</pub-id>
<?supplied-pmid 32531404?><pub-id pub-id-type="pmid">32531404</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><string-name><surname>Oh</surname><given-names>AJ</given-names></string-name>, <string-name><surname>Amore</surname><given-names>G</given-names></string-name>, <string-name><surname>Sultan</surname><given-names>W</given-names></string-name>, <string-name><surname>Asanad</surname><given-names>S</given-names></string-name>, <string-name><surname>Park</surname><given-names>JC</given-names></string-name>, <string-name><surname>Romagnoli</surname><given-names>M</given-names></string-name>, <string-name><surname>La Morgia</surname><given-names>C</given-names></string-name>, <string-name><surname>Karanjia</surname><given-names>R</given-names></string-name>, <string-name><surname>Harrington</surname><given-names>MG</given-names></string-name>, <string-name><surname>Sadun</surname><given-names>AA</given-names></string-name> (<year>2019</year>) <article-title>Pupillometry evaluation of melanopsin retinal ganglion cell function and sleep-wake activity in pre-symptomatic Alzheimer’s disease</article-title>. <source>PLoS One</source>
<volume>14</volume>:<fpage>e0226197</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0226197</pub-id><pub-id pub-id-type="pmid">31821378</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><string-name><surname>Oh</surname><given-names>AJ</given-names></string-name>, <string-name><surname>Amore</surname><given-names>G</given-names></string-name>, <string-name><surname>Sultan</surname><given-names>W</given-names></string-name>, <string-name><surname>Asanad</surname><given-names>S</given-names></string-name>, <string-name><surname>Park</surname><given-names>JC</given-names></string-name>, <string-name><surname>Romagnoli</surname><given-names>M</given-names></string-name>, <string-name><surname>La Morgia</surname><given-names>C</given-names></string-name>, <string-name><surname>Karanjia</surname><given-names>R</given-names></string-name>, <string-name><surname>Harrington</surname><given-names>MG</given-names></string-name>, <string-name><surname>Sadun</surname><given-names>AA</given-names></string-name> (<year>2020</year>) <article-title>Correction: pupillometry evaluation of melanopsin retinal ganglion cell function and sleep-wake activity in pre-symptomatic Alzheimer’s disease</article-title>. <source>PLoS One</source>
<volume>15</volume>:<fpage>e0230061</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0230061</pub-id>
<?supplied-pmid 32107500?><pub-id pub-id-type="pmid">32107500</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="book"><string-name><surname>Olivia</surname><given-names>C</given-names></string-name>, <string-name><surname>Josef</surname><given-names>S</given-names></string-name>, <string-name><surname>Camille</surname><given-names>C</given-names></string-name>, <string-name><surname>Christof</surname><given-names>K</given-names></string-name>, <string-name><surname>Steven</surname><given-names>L</given-names></string-name>, <string-name><surname>Wolfgang</surname><given-names>E</given-names></string-name> (<year>2013</year>) <part-title>The use of pupil dilation to communicate with locked-in syndrome patients</part-title>. In: <source>ACNS-2013 Australasian Cognitive Neuroscience Society Conference</source>. <publisher-loc>Callaghan, Australia</publisher-loc>: <publisher-name>Australasian Cognitive Neuroscience Society</publisher-name>. <pub-id pub-id-type="doi">10.3389/conf.fnhum.2013.212.00126</pub-id> .</mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><string-name><surname>Pelli</surname><given-names>DG</given-names></string-name> (<year>1997</year>) <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spat Vis</source>
<volume>10</volume>:<fpage>437</fpage>–<lpage>442</lpage>. <?supplied-pmid 9176953?><pub-id pub-id-type="pmid">9176953</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><string-name><surname>Ponzio</surname><given-names>F</given-names></string-name>, <string-name><surname>Villalobos</surname><given-names>AEL</given-names></string-name>, <string-name><surname>Mesin</surname><given-names>L</given-names></string-name>, <string-name><surname>de’Sperati</surname><given-names>C</given-names></string-name>, <string-name><surname>Roatta</surname><given-names>S</given-names></string-name> (<year>2019</year>) <article-title>A human-computer interface based on the “voluntary” pupil accommodative response</article-title>. <source>Int J Hum Comput Stud</source>
<volume>126</volume>:<fpage>53</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1016/j.ijhcs.2019.02.002</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><string-name><surname>Privitera</surname><given-names>M</given-names></string-name>, <string-name><surname>Ferrari</surname><given-names>KD</given-names></string-name>, <string-name><surname>von Ziegler</surname><given-names>LM</given-names></string-name>, <string-name><surname>Sturman</surname><given-names>O</given-names></string-name>, <string-name><surname>Duss</surname><given-names>SN</given-names></string-name>, <string-name><surname>Floriou-Servou</surname><given-names>A</given-names></string-name>, <string-name><surname>Germain</surname><given-names>P-L</given-names></string-name>, <string-name><surname>Vermeiren</surname><given-names>Y</given-names></string-name>, <string-name><surname>Wyss</surname><given-names>MT</given-names></string-name>, <string-name><surname>De Deyn</surname><given-names>PP</given-names></string-name>, <string-name><surname>Weber</surname><given-names>B</given-names></string-name>, <string-name><surname>Bohacek</surname><given-names>J</given-names></string-name> (<year>2020</year>) <article-title>A complete pupillometry toolbox for real-time monitoring of locus coeruleus activity in rodents</article-title>. <source>Nat Protoc</source>
<volume>15</volume>:<fpage>2301</fpage>–<lpage>2320</lpage>. <pub-id pub-id-type="doi">10.1038/s41596-020-0324-6</pub-id>
<?supplied-pmid 32632319?><pub-id pub-id-type="pmid">32632319</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><string-name><surname>Reimer</surname><given-names>J</given-names></string-name>, <string-name><surname>McGinley</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Rodenkirch</surname><given-names>C</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Q</given-names></string-name>, <string-name><surname>McCormick</surname><given-names>DA</given-names></string-name>, <string-name><surname>Tolias</surname><given-names>AS</given-names></string-name> (<year>2016</year>) <article-title>Pupil fluctuations track rapid changes in adrenergic and cholinergic activity in cortex</article-title>. <source>Nat Commun</source>
<volume>7</volume>:<fpage>13289</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms13289</pub-id>
<?supplied-pmid 27824036?><pub-id pub-id-type="pmid">27824036</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><string-name><surname>Ronneberger</surname><given-names>O</given-names></string-name>, <string-name><surname>Fischer</surname><given-names>P</given-names></string-name>, <string-name><surname>Brox</surname><given-names>T</given-names></string-name> (<year>2015</year>) <part-title>U-Net: convolutional networks for biomedical image segmentation</part-title>. In: <source>Medical image computing and computer-assisted intervention – MICCAI 2015</source>, pp <fpage>234</fpage>–<lpage>241</lpage>. <publisher-loc>Cham, Switzerland</publisher-loc>: <publisher-name>Springer International</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><string-name><surname>Rorick-Kehn</surname><given-names>LM</given-names></string-name>, <string-name><surname>Witcher</surname><given-names>JW</given-names></string-name>, <string-name><surname>Lowe</surname><given-names>SL</given-names></string-name>, <string-name><surname>Gonzales</surname><given-names>CR</given-names></string-name>, <string-name><surname>Weller</surname><given-names>MA</given-names></string-name>, <string-name><surname>Bell</surname><given-names>RL</given-names></string-name>, <string-name><surname>Hart</surname><given-names>JC</given-names></string-name>, <string-name><surname>Need</surname><given-names>AB</given-names></string-name>, <string-name><surname>McKinzie</surname><given-names>JH</given-names></string-name>, <string-name><surname>Statnick</surname><given-names>MA</given-names></string-name>, <string-name><surname>Suico</surname><given-names>JG</given-names></string-name>, <string-name><surname>McKinzie</surname><given-names>DL</given-names></string-name>, <string-name><surname>Tauscher-Wisniewski</surname><given-names>S</given-names></string-name>, <string-name><surname>Mitch</surname><given-names>CH</given-names></string-name>, <string-name><surname>Stoltz</surname><given-names>RR</given-names></string-name>, <string-name><surname>Wong</surname><given-names>CJ</given-names></string-name> (<year>2014</year>) <article-title>Determining pharmacological selectivity of the kappa opioid receptor antagonist LY2456302 using pupillometry as a translational biomarker in rat and human</article-title>. <source>Int J Neuropsychopharmacol</source>
<volume>18</volume>:<fpage>pyu036</fpage>. <pub-id pub-id-type="doi">10.1093/ijnp/pyu036</pub-id><pub-id pub-id-type="pmid">25637376</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="journal"><string-name><surname>Silasi</surname><given-names>G</given-names></string-name>, <string-name><surname>Xiao</surname><given-names>D</given-names></string-name>, <string-name><surname>Vanni</surname><given-names>MP</given-names></string-name>, <string-name><surname>Chen</surname><given-names>ACN</given-names></string-name>, <string-name><surname>Murphy</surname><given-names>TH</given-names></string-name> (<year>2016</year>) <article-title>Intact skull chronic windows for mesoscopic wide-field imaging in awake mice</article-title>. <source>J Neurosci Methods</source>
<volume>267</volume>:<fpage>141</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.04.012</pub-id>
<?supplied-pmid 27102043?><pub-id pub-id-type="pmid">27102043</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><string-name><surname>Stoll</surname><given-names>J</given-names></string-name>, <string-name><surname>Chatelle</surname><given-names>C</given-names></string-name>, <string-name><surname>Carter</surname><given-names>O</given-names></string-name>, <string-name><surname>Koch</surname><given-names>C</given-names></string-name>, <string-name><surname>Laureys</surname><given-names>S</given-names></string-name>, <string-name><surname>Einhäuser</surname><given-names>W</given-names></string-name> (<year>2013</year>) <article-title>Pupil responses allow communication in locked-in syndrome patients</article-title>. <source>Curr Biol</source>
<volume>23</volume>:<fpage>R647</fpage>–<lpage>R648</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2013.06.011</pub-id>
<?supplied-pmid 23928079?><pub-id pub-id-type="pmid">23928079</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><string-name><surname>Vallat</surname><given-names>R</given-names></string-name> (<year>2018</year>) <article-title>Pingouin: statistics in Python</article-title>. <source>J Open Source Softw</source>
<volume>3</volume>:<fpage>1026</fpage>. <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="journal"><string-name><surname>Wang</surname><given-names>C-A</given-names></string-name>, <string-name><surname>Munoz</surname><given-names>DP</given-names></string-name> (<year>2015</year>) <article-title>A circuit for pupil orienting responses: implications for cognitive modulation of pupil size</article-title>. <source>Curr Opin Neurobiol</source>
<volume>33</volume>:<fpage>134</fpage>–<lpage>140</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2015.03.018</pub-id>
<?supplied-pmid 25863645?><pub-id pub-id-type="pmid">25863645</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><string-name><surname>Wang</surname><given-names>H</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J</given-names></string-name>, <string-name><surname>Xu</surname><given-names>X</given-names></string-name>, <string-name><surname>Sun</surname><given-names>W-J</given-names></string-name>, <string-name><surname>Chen</surname><given-names>X</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>F</given-names></string-name>, <string-name><surname>Luo</surname><given-names>M-H</given-names></string-name>, <string-name><surname>Liu</surname><given-names>C</given-names></string-name>, <string-name><surname>Guo</surname><given-names>Y</given-names></string-name>, <string-name><surname>Xie</surname><given-names>W</given-names></string-name>, <string-name><surname>Zhong</surname><given-names>H</given-names></string-name>, <string-name><surname>Bai</surname><given-names>T</given-names></string-name>, <string-name><surname>Tian</surname><given-names>Y</given-names></string-name>, <string-name><surname>Mao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Ye</surname><given-names>C</given-names></string-name>, <string-name><surname>Tao</surname><given-names>W</given-names></string-name>, <string-name><surname>Li</surname><given-names>J</given-names></string-name>, <string-name><surname>Farzinpour</surname><given-names>Z</given-names></string-name>, <string-name><surname>Li</surname><given-names>J</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J-N</given-names></string-name>, <string-name><surname>Wang</surname><given-names>K</given-names></string-name>, <string-name><surname>He</surname><given-names>J</given-names></string-name>, <string-name><surname>Chen</surname><given-names>L</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z</given-names></string-name> (<year>2019</year>) <article-title>Direct auditory cortical input to the lateral periaqueductal gray controls sound-driven defensive behavior</article-title>. <source>PLoS Biol</source>
<volume>17</volume>:<fpage>e3000417</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000417</pub-id>
<?supplied-pmid 31469831?><pub-id pub-id-type="pmid">31469831</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><string-name><surname>Wierda</surname><given-names>SM</given-names></string-name>, <string-name><surname>van Rijn</surname><given-names>H</given-names></string-name>, <string-name><surname>Taatgen</surname><given-names>NA</given-names></string-name>, <string-name><surname>Martens</surname><given-names>S</given-names></string-name> (<year>2012</year>) <article-title>Pupil dilation deconvolution reveals the dynamics of attention at high temporal resolution</article-title>. <source>Proc Natl Acad Sci U S A</source>
<volume>109</volume>:<fpage>8456</fpage>–<lpage>8460</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1201858109</pub-id>
<?supplied-pmid 22586101?><pub-id pub-id-type="pmid">22586101</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="journal"><string-name><surname>Winston</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>A</given-names></string-name>, <string-name><surname>Rand</surname><given-names>CM</given-names></string-name>, <string-name><surname>Dunne</surname><given-names>EC</given-names></string-name>, <string-name><surname>Warner</surname><given-names>JJ</given-names></string-name>, <string-name><surname>Volpe</surname><given-names>LJ</given-names></string-name>, <string-name><surname>Pigneri</surname><given-names>BA</given-names></string-name>, <string-name><surname>Simon</surname><given-names>D</given-names></string-name>, <string-name><surname>Bielawiec</surname><given-names>T</given-names></string-name>, <string-name><surname>Gordon</surname><given-names>SC</given-names></string-name>, <string-name><surname>Vitez</surname><given-names>SF</given-names></string-name>, <string-name><surname>Charnay</surname><given-names>A</given-names></string-name>, <string-name><surname>Joza</surname><given-names>S</given-names></string-name>, <string-name><surname>Kelly</surname><given-names>K</given-names></string-name>, <string-name><surname>Panicker</surname><given-names>C</given-names></string-name>, <string-name><surname>Rizvydeen</surname><given-names>S</given-names></string-name>, <string-name><surname>Niewijk</surname><given-names>G</given-names></string-name>, <string-name><surname>Coleman</surname><given-names>C</given-names></string-name>, <string-name><surname>Scher</surname><given-names>BJ</given-names></string-name>, <string-name><surname>Reed</surname><given-names>DW</given-names></string-name>, <etal>et al</etal> (<year>2020</year>) <article-title>Pupillometry measures of autonomic nervous system regulation with advancing age in a healthy pediatric cohort</article-title>. <source>Clin Auton Res</source>
<volume>30</volume>:<fpage>43</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1007/s10286-019-00639-3</pub-id><pub-id pub-id-type="pmid">31555934</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><string-name><surname>Xiong</surname><given-names>XR</given-names></string-name>, <string-name><surname>Liang</surname><given-names>F</given-names></string-name>, <string-name><surname>Zingg</surname><given-names>B</given-names></string-name>, <string-name><surname>Ji</surname><given-names>X-Y</given-names></string-name>, <string-name><surname>Ibrahim</surname><given-names>LA</given-names></string-name>, <string-name><surname>Tao</surname><given-names>HW</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>LI</given-names></string-name> (<year>2015</year>) <article-title>Auditory cortex controls sound-driven innate defense behaviour through corticofugal projections to inferior colliculus</article-title>. <source>Nat Commun</source>
<volume>6</volume>:<fpage>7224</fpage>. <pub-id pub-id-type="doi">10.1038/ncomms8224</pub-id>
<?supplied-pmid 26068082?><pub-id pub-id-type="pmid">26068082</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><string-name><surname>Yiu</surname><given-names>Y-H</given-names></string-name>, <string-name><surname>Aboulatta</surname><given-names>M</given-names></string-name>, <string-name><surname>Raiser</surname><given-names>T</given-names></string-name>, <string-name><surname>Ophey</surname><given-names>L</given-names></string-name>, <string-name><surname>Flanagin</surname><given-names>VL</given-names></string-name>, <string-name><surname>Zu Eulenburg</surname><given-names>P</given-names></string-name>, <string-name><surname>Ahmadi</surname><given-names>S-A</given-names></string-name> (<year>2019</year>) <article-title>DeepVOG: open-source pupil segmentation and gaze estimation in neuroscience using deep learning</article-title>. <source>J Neurosci Methods</source>
<volume>324</volume>:<fpage>108307</fpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.05.016</pub-id>
<?supplied-pmid 31176683?><pub-id pub-id-type="pmid">31176683</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="journal"><string-name><surname>Zhuang</surname><given-names>J</given-names></string-name>, <string-name><surname>Tang</surname><given-names>T</given-names></string-name>, <string-name><surname>Ding</surname><given-names>Y</given-names></string-name>, <string-name><surname>Tatikonda</surname><given-names>S</given-names></string-name>, <string-name><surname>Dvornek</surname><given-names>N</given-names></string-name>, <string-name><surname>Papademetris</surname><given-names>X</given-names></string-name>, <string-name><surname>Duncan</surname><given-names>JS</given-names></string-name> (<year>2020</year>) <article-title>AdaBelief optimizer: adapting stepsizes by the belief in observed gradients</article-title>. <source>arXiv</source>: 2010.07468.</mixed-citation>
    </ref>
  </ref-list>
  <sec sec-type="synthesis-author-response" id="s6">
    <title>Synthesis</title>
    <boxed-text position="float">
      <p>Reviewing Editor: Siegrid Löwel, University of Goettingen</p>
      <p>Decisions are customarily a result of the Reviewing Editor and the peer reviewers coming together and discussing their recommendations until a consensus is reached. When revisions are invited, a fact-based synthesis statement explaining their decision and outlining what is needed to prepare a revision will be listed below. The following reviewer(s) agreed to reveal their identity: Nikolaos Aggelopoulos, Janelle Pakan.</p>
    </boxed-text>
    <p>The authors detail a method for pupil diameter analysis and pupil tracking that is open-source, economically implemented and included a user friendly web-based app. The paper is generally well written although there are some semantics that should be addressed to not be misleading in the impact of the proof of principle results, which are not required to be of high impact in this case but simply accurate uses of the applied method. While the developed methods are not specifically novel in their end goal, they seem to perform well in comparison to other expensive commercially available options in humans. An unbiased comparison between their method and the leading open source tool for this type of analysis (DeepLabCut) would seem warranted. A more detailed discussion of the image parameters vital/sufficient/necessary for success are also needed to appeal to the more novice user that such a web-based app is targeted towards and likely most beneficial for.</p>
    <p>The manuscript provides open source pupil diameter and tracking analysis tools including a user friendly web app. The advances in the field are not dramatic as methods for pupil analysis are already well established using commercial systems and other open source tools (e.g. DeepLabCut). The performance of the tool is compared to the performance of Eyelink and is found to be comparable but at a lower cost. It is also argued that it is a better tool than DeepLabCut, because it is directly measuring pupil size rather than performing semantic segmentation. The prime advantage of the described tool seems to be the economical methods, ease of user friendly interface, and portability allowing pupillometry also in clinical contexts with simply the use of a mobile phone. While this is not a traditional scientific advancement of the field, it may be quite a useful resource for a more general user without extensive programming experience and/or available resources.</p>
    <p>In addition, the authors report an increase in running velocity of mice running on a circular disk during the presentation of an auditory stimulus. While there is not much novelty in observing pupil dilation after auditory stim/locomotion (this has been examined in behaving mice quite extensively, and much more rigorously), claiming novelty of the running speed increases after auditory stimulation would need a much more detailled investigation here. Is this a kind of startle response? We therefore suggest that the authors clarify the semantics of some of their conclusions in this regard. They can then cite the appropriate studies that have also shown similar results directly in the methods for clarity. The authors mention some in the intro, but not exhaustively. Instead of trying to make a novel point here, we would say better to just cite the previous literature and say they could reproduce these effects with their analysis tools. Also N number is quite low to be able to really draw significant new conclusions anyway. In fact, we are not sure if the authors can show the more subtle changes in pupil diameter that come with only arousal and no locomotion in mice with MEYE. They have not shown this directly here. Locomotion induced changes are very dramatic and can generally be detected even with rudimentary image analysis tools (of course the web app interface is still a nice development for the community). Please add a discussion of the importance of this finding for the new tool.</p>
    <p>Statistics comments</p>
    <p>The statistics involved:</p>
    <p>(a) measures of pupil dilation during the presentation of a sound using a permutation test. It is not clear what the permutation test compares. Pupil area at time t1 versus at time t2? Some more detail would be useful.</p>
    <p>(b) measures of pupil size with or without locomotion. In this case a t-test was used. It is again insufficiently clear what measurements were compared.</p>
    <p>(c) Pearson/Spearman correlations of data from the same dataset analysed with the MEYE method vs using Eyelink. Some more detail would again be useful. While the Methods section refers to the use of the Spearman correlation, the Results section reports a Pearson correlation test used to compare pupil dilation to running speed (e.g. Fig 2C) and further down in comparing pupil dilation measured with MEYE vs as measured with Eylink (lines 305-310).</p>
    <p>Software comments:</p>
    <p>Brief testing was carried out on pupil data collected in the reviewers labs (from two-photon imaging in mice). The processing was relative fast and the graphical output well presented and informative. At first, the lab’s format (.avi) was not accepted, even though these files are opened by many other programs (e.g. ImageJ, VLC, generic windows viewers etc) - a more flexible/comprehensive input format would make this more useful to a general user, which seems to be the strength of the web app. After convertion to mp4 and loading of the test images, most of the perceptually ‘easier’ data was extracted reasonably well - although not quite as nice as in the manuscript. Perhaps tips on optimizing may be useful in the software or as an easily accessible help link. The more difficult samples generally failed. The reviewer’s lab has had success with these samples through custom pipelines, however, these do inevitably involve extensive manual intervention to fully optimize. The lab did not find the thresholding controls particularly powerful for optimization in the web app. In future versions, these could be expanded and improved upon. Ideally, users could also tweak their images within the app itself - but this could be a future development.</p>
    <p>In the lab’s testing, because of the presence of an IR light that reflects in the eye in their set-up, the lab often has a small second circular reflection outside of the pupil range. MEYE often recognized this circular area and included it in the pupil region - often creating a region of ‘two pupils’ or a 8-shaped region of interest. It would seem to us to be trivial to include parameters or implement constraints (even user toggled) that would estimate the max pupil size or to train in which only a single ‘connected’ area should be recognized - there should be no need for two separate regions (unconnected) to be identified.</p>
    <p>Finally, it was observed that the tool does not perform well at low contrast situations. The threshold optimization scale could perform better if the image contrast could be adjusted. This might be possible in a pre-processing stage that could be implemented for broader functionality. We suggest that the MEYE tool is improved to offer greater flexibility for image editing and file options, e.g. avi, which is a common format in science labs.</p>
    <p>Conflict of interest:</p>
    <p>Please clarify whether the tool described is also part of a commercial product, i.e. in addition to the free version, is there a commercial version with greater functionality marketed?</p>
    <p>Further comments:</p>
    <p>1. Line 189 describes the tone presentation as 10 seconds, however, Fig 2C scale suggests the tone was 20 seconds in duration. Please explain the discrepancy - this is important as it also related to the comment below. Also related, line 267 lists interstimulus interval but not stimulus duration. It would be good to include both here in the results.</p>
    <p>2. The pupil changes - particularly in Fig 2C are presented as relatively fast on the presented scale, with 20 seconds of auditory stimulation - which is quite a long stimulation period. Line 262-263 discuss an aim as ‘real-time’ pupil analysis, therefore, evidence of faster pupil fluctuations (i.e. data presented on a more reasonable scale in comparison to velocity changes) would be necessary to appreciate the limits of the ‘real-time’ capacity for pupil monitoring.</p>
    <p>3. Lines 274-275: this statement is a bit misleading as a simple untrained stimulus is not quite the same ‘cognitive and emotional processing’ as a trained stimulus-reward association in relation to a behavioural choice paradigm for instance. Please tone down this statement - it is tangential to the point anyway and not required to be convincing of the pupil readout as linked to behavioural state changes (as the authors note in the lines following this). It represent a change in arousal/behavioural state in response to either the sensory stimulation and/or locomotion (this was not specifically separated here in the data presentation (i.e. tone with no locomotion - so one can not tell which) - beyond this is unclear and cognitive/emotional processes can not be inferred with this protocol.</p>
    <p>4. Line 278. The use of spontaneous pupil changes here is problematic. Spontaneous changes occur when the animal is stationary and small pupil changes are then correlated to arousal levels. This is in contrast to behavioural state changes (with associated changes in arousal) - including locomotion, that result in larger pupil changes. This is what is reported - responses in relation to locomotion - but these are not spontaneous. If the authors are calling them spontaneous because of the lack of sensory stimulation, this is misleading. Locomotion itself is a stimulus the affects pupil diameter so is not considered spontaneous. If the authors can show small fluctuations in pupil diameter during stationary periods then these would be the ‘spontaneous’ changes in arousal (see details in Vinck et al., 2015, https://pubmed.ncbi.nlm.nih.gov/25892300/).</p>
    <p>5. Lines 281-282. As mentioned above, the authors did not separate the locomotion effects from the auditory stimulus presentation. This does not seem to be the point of the study, so is likely not necessary, but the statement that evoked versus state changes can be monitored based on this evidence is misleading. If they want to separate these parameter they need to have sufficient data where the tone was presented and mice where stationary. Otherwise, please revise to be more general in the conclusion and not try to separate arousal/state change due to locomotion from sensory evoked responses.</p>
    <p>6. Line 333: presumably this is in reference to Figure 5A-C (not Fig 4). The two-photon mouse data should be elaborated on. Why was this performed, what are the differences and caveats. Fig 5B appears to result in a ‘smoother’ pupil diameter change. This is presumably because of some parameter of the images and not a real change in pupil behaviour. We are happy to see this data included, as there is a large field that many find this useful, but we would suggest the authors devote a paragraph to this analysis and describe remaining potential pitfalls/conditions of use.</p>
    <p>7. We appreciate the discussion of DeepLabCut as an alternative analysis method, but a demonstration of this analysis in comparison to MEYE would be very informative. Indeed DeepLabCut requires some additional knowledge and the development of the web app offers a very simple and accessible interface - but one would need to know that MEYE at least approaches the efficiency of DLC (similar to EyeLink). This should be a relatively trivial test to perform.</p>
    <p>8. Although the authors undoubtedly want to present their best case scenario data - it would be useful from the user perspective (since the aim is clearly to appeal to a general user) to give examples or a further discussion regarding when MEYE may fail and how to optimize image collection to avoid this.</p>
    <p>9. We are missing a clear indication of specific points for future development for the MEYE. For an open-source tool, a discussion of this can be very helpful for the community and lead to greater advancements of mutually beneficial tool development.</p>
    <p>10. We are also missing a discussion about the speed of the pupil changes - particularly in relation to neural signals. There are many mentions of the link to cognitive processes, but a small statement with regard to the time scales is warranted - likely in the discussion.</p>
    <p>11. The text in Figure 1 appears as low quality at the current figure size - perhaps this is just the PDF, but Figure 2 seems fine. Please check.</p>
    <p>12. Line 99: ‘Upsampling and downsampling operations have factor 2’. This is a bit unclear as written. Could the authors either link to Fig. 1C here or rephrase this sentence for clarity.</p>
    <p>13. Line 290: likely should be ‘flash’ not ‘flashes’</p>
    <p>14. The paragraphs towards the end get very lengthy. May I suggest creating more paragraph breaks/sections for ease of reading.</p>
    <p>15. Please expand the section on data analysis. Preferably phrase it in the following way Spearman ρ rank-order (or Pearson?) correlation was performed using the function “x” in pingouin.corr, etc. Please clarify if Spearman or Pearson correlation was used. Please expand on what was being compared using the Z-score, i.e. which data points against which other data points. We believe an ANOVA would have been useful for the time series analyses but perhaps we do not understand what is being compared.</p>
    <p>16. With regard to the other text, please use the past tense for the work presented, the equipment used and tests carried out. Reserve the present tense only in describing characteristics of commercial products e.g. line 127 “GPU that can process up to 28 fps” and such. Please do not use the present tense in other contexts. E.g. Line 177: Velocity &lt;b&gt;was&lt;/b&gt; monitored ... Line 187: a curved monitor &lt;b&gt;was&lt;/b&gt; placed, Line 214: Photic stimulation &lt;b&gt;was&lt;/b&gt; delivered ... and so on. There are many other instances where “is” should be replaced with “was”.</p>
    <p>17. We are confused by the statement on line 99: ‘Upsampling and downsampling operations have factor 2’. It is probably made worse by the use of the present tense where the past tense should have been used. Perhaps a better phrasing would be that convolutional layer upsampling and downsampling (in the CNN they used) was by a factor of 2. As it is written, it rather gives the impression that all convolution up/downsampling by CNNs is by a factor of 2.</p>
  </sec>
  <sec sec-type="synthesis-author-response" id="s7">
    <title>Author Response</title>
    <p>POINT-TO-POINT RESPONSE</p>
    <p>The authors detail a method for pupil diameter analysis and pupil tracking that is open-source, economically implemented and included a user friendly web-based app. The paper</p>
    <p>is generally well written although there are some semantics that should be addressed to not</p>
    <p>be misleading in the impact of the proof of principle results, which are not required to be of</p>
    <p>high impact in this case but simply accurate uses of the applied method. While the</p>
    <p>developed methods are not specifically novel in their end goal, they seem to perform well in</p>
    <p>comparison to other expensive commercially available options in humans.</p>
    <p>An unbiased comparison between their method and the leading open source tool for this</p>
    <p>type of analysis (DeepLabCut) would seem warranted.</p>
    <p>Thank you for raising this point.</p>
    <p>We could not perform a direct comparison with DeepLabCut (DLC) in an end-to-end fashion,</p>
    <p>as available pre-trained models do not perform well on our multi-specie scenario, and our</p>
    <p>dataset is incompatible with and not directly convertible to DLC’s expected label format.</p>
    <p>In addition, DLC is trained using user-defined key points and therefore it involves different</p>
    <p>steps (respect to the UNET) for the identification of the size and location of an object. Our</p>
    <p>11k images dataset is not suitable for direct DLC training, and this means that the two</p>
    <p>models should be trained with datasets with different labeling. To overcome this limitation in</p>
    <p>performance comparison, we adopted the “DeepLab” CNNs</p>
    <p>(https://arxiv.org/pdf/1606.00915.pdf), which are semantic segmentation networks at the</p>
    <p>core of DLC. These models are similar to UNET in terms of structure and can be trained</p>
    <p>exactly with the same dataset allowing a direct performance comparison. Thus, we trained</p>
    <p>and compared different versions of DeepLab with our UNET based pupillometer. We added</p>
    <p>a detailed description of the performances of MEYE and DeepLab in the Github Wiki</p>
    <p>(https://github.com/fabiocarrara/meye/wiki/MEYE-Models). We found our model to offer a</p>
    <p>better trade-off between speed (in FPS) and segmentation performance (in Dice Coefficient).</p>
    <p>These data have been included in Table 4.</p>
    <p>However, since the MEYE backend model can be dynamically changed (through the</p>
    <p>MODELS dropdown menu), we integrated DeepLab into the MEYE webapp. In this way the</p>
    <p>user can choose between the UNET (our developed CNN) or the DeepLab (the standard</p>
    <p>DLC CNN) models.</p>
    <p>A more detailed discussion of the image parameters vital/sufficient/necessary for success</p>
    <p>are also needed to appeal to the more novice user that such a web-based app is targeted</p>
    <p>towards and likely most beneficial for.</p>
    <p>We created a section of the Github Wiki named “Hardware, Image Parameters and Common</p>
    <p>Issues", with all the details about tested hardware and acquisition parameters.</p>
    <p>https://github.com/fabiocarrara/meye/wiki/Hardware,-Image-Parameters-and-Common Issues</p>
    <p>The manuscript provides open source pupil diameter and tracking analysis tools including a</p>
    <p>user friendly web app. The advances in the field are not dramatic as methods for pupil</p>
    <p>analysis are already well established using commercial systems and other open source tools</p>
    <p>(e.g. DeepLabCut). The performance of the tool is compared to the performance of Eyelink</p>
    <p>and is found to be comparable but at a lower cost. It is also argued that it is a better tool than</p>
    <p>DeepLabCut, because it is directly measuring pupil size rather than performing semantic segmentation.The prime advantage of the described tool seems to be the economical</p>
    <p>methods, ease of user friendly interface, and portability allowing pupillometry also in clinical</p>
    <p>contexts with simply the use of a mobile phone. While this is not a traditional scientific</p>
    <p>advancement of the field, it may be quite a useful resource for a more general user without</p>
    <p>extensive programming experience and/or available resources.</p>
    <p>In addition, the authors report an increase in running velocity of mice running on a circular</p>
    <p>disk during the presentation of an auditory stimulus. While there is not much novelty in</p>
    <p>observing pupil dilation after auditory stim/locomotion (this has been examined in behaving</p>
    <p>mice quite extensively, and much more rigorously), claiming novelty of the running speed</p>
    <p>increases after auditory stimulation would need a much more detailed investigation here.</p>
    <p>Is this a kind of startle response?</p>
    <p>We therefore suggest that the authors clarify the semantics of some of their conclusions in</p>
    <p>this regard. They can then cite the appropriate studies that have also shown similar results</p>
    <p>directly in the methods for clarity. The authors mention some in the intro, but not</p>
    <p>exhaustively. Instead of trying to make a novel point here, we would say better to just cite</p>
    <p>the previous literature and say they could reproduce these effects with their analysis tools.</p>
    <p>We definitely agree that our goal was not to discover new effects, but rather use published</p>
    <p>ones to test our pupillometry system. The auditory stimulus that we employed is similar to</p>
    <p>the conditioned stimulus used in published fear conditioning papers. The saliency of the tone</p>
    <p>and the absolute volume can induce defensive sound-induced flight behavior on mice in an</p>
    <p>intensity-dependent manner. We eliminated any claim about the novelty of this finding</p>
    <p>because other works describing these phenomena exist. For this reason, we added a brief</p>
    <p>discussion in the results section and appropriate references explaining the results (line 314-</p>
    <p>317).</p>
    <p>Papers with auditory stimulus &gt;=20sec:</p>
    <p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4154579/</p>
    <p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6596193/</p>
    <p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5651384/</p>
    <p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5098193/</p>
    <p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1142462/</p>
    <p>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6715724/</p>
    <p>Also N number is quite low to be able to really draw significant new conclusions anyway. In</p>
    <p>fact, we are not sure if the authors can show the more subtle changes in pupil diameter that</p>
    <p>come with only arousal and no locomotion in mice with MEYE. They have not shown this</p>
    <p>directly here. Locomotion induced changes are very dramatic and can generally be detected</p>
    <p>even with rudimentary image analysis tools (of course the web app interface is still a nice</p>
    <p>development for the community). Please add a discussion of the importance of this finding</p>
    <p>for the new tool.</p>
    <p>Thank you for prompting us to test this possible limitation of the tool. We added a new figure</p>
    <p>with an entire new experiment employing visual stimulation (N=5). In this experiment, we</p>
    <p>show an event-related transient of the pupil in absence of significant locomotor activity. This</p>
    <p>demonstrates that the tool is able to detect subtle fluctuation in arousal (orienting response)</p>
    <p>given by stimulus detection in mice (line 321-324).Statistics comments</p>
    <p>The statistics involved:</p>
    <p>(a) measures of pupil dilation during the presentation of a sound using a permutation test. It</p>
    <p>is not clear what the permutation test compares. Pupil area at time t1 versus at time t2?</p>
    <p>Some more detail would be useful.</p>
    <p>We changed the permutation test used here with a two-way repeated-measures ANOVA as</p>
    <p>suggested in comment 15 and added the requested details in the manuscript methods</p>
    <p>section (line 221-227).</p>
    <p>(b) measures of pupil size with or without locomotion. In this case a t-test was used. It is</p>
    <p>again insufficiently clear what measurements were compared.</p>
    <p>We measured the average pupil size during stationary or moving periods. Moving periods</p>
    <p>are defined as the animal moving at a velocity &gt;=10% of its maximal speed during the entire</p>
    <p>session. We added this detail in the manuscript methods (line 228-230).</p>
    <p>(c) Pearson/Spearman correlations of data from the same dataset analysed with the MEYE</p>
    <p>method vs using Eyelink. Some more detail would again be useful. While the Methods</p>
    <p>section refers to the use of the Spearman correlation, the Results section reports a Pearson</p>
    <p>correlation test used to compare pupil dilation to running speed (e.g. Fig 2C) and further</p>
    <p>down in comparing pupil dilation measured with MEYE vs as measured with Eylink (lines</p>
    <p>305-310).</p>
    <p>We addressed this problem performing Spearman correlation in all the cases as stated in the</p>
    <p>methods section. We also expanded the method section related to this aspect (line 283-</p>
    <p>286).</p>
    <p>Software comments:</p>
    <p>Brief testing was carried out on pupil data collected in the reviewers labs (from two-photon</p>
    <p>imaging in mice). The processing was relatively fast and the graphical output well presented</p>
    <p>and informative. At first, the lab’s format (.avi) was not accepted, even though these files are</p>
    <p>opened by many other programs (e.g. ImageJ, VLC, generic windows viewers etc) - a more</p>
    <p>flexible/comprehensive input format would make this more useful to a general user, which</p>
    <p>seems to be the strength of the web app.</p>
    <p>We are really pleased to hear that reviewers tried MEYE, thank you very much.</p>
    <p>We agree that support for the ‘.avi’ files could improve the usability of the web app to a</p>
    <p>general user (we also use the .avi format for 2P experiments) but unfortunately such a</p>
    <p>feature it’s impossible for now to implement due to HTML5 development policies specifically</p>
    <p>not supporting video playback for the .avi format. We addressed this problem in two ways:</p>
    <p>first, we provided an extensively commented python notebook for offline analysis and</p>
    <p>postprocessing of .avi videos. Second, we added a link on the GUI to an online avi to MP4</p>
    <p>converter. We hope that in the future HTML5 could also support .avi video playback,</p>
    <p>however, many other high quality video formats are supported by HTML5, see</p>
    <p>https://en.wikipedia.org/wiki/HTML5_video for a list of all the supported file formats.</p>
    <p>We added a discussion about this limitation in the section ‘Hardware, Image Parameters and</p>
    <p>Common Issues’ in the Github Wiki</p>
    <p>After convertion to mp4 and loading of the test images, most of the perceptually ‘easier’ data</p>
    <p>was extracted reasonably well - although not quite as nice as in the manuscript. Perhaps tips</p>
    <p>on optimizing may be useful in the software or as an easily accessible help link. The more difficult samples generally failed. The reviewer’s lab has had success with these samples</p>
    <p>through custom pipelines, however, these do inevitably involve extensive manual</p>
    <p>intervention to fully optimize.</p>
    <p>We improved the image optimization in three different ways: first, we introduced in the Web</p>
    <p>app a PREPROCESSING section, a series of image enhancement tools to better control</p>
    <p>image parameters. The user can change these parameters and easily find the best condition</p>
    <p>to achieve correct pupil size measurement. Please note that in our hands another possible</p>
    <p>solution for difficult samples of 2P experiments is to invert image colors, a manipulation that</p>
    <p>now is available in the GUI as a checkbox. Second, In the LINKS section of the MEYE GUI</p>
    <p>we created a link to the manual section called “Hardware, Image Parameters and common</p>
    <p>issues", in which the best hardware setup and image optimization are explained in detail</p>
    <p>(https://github.com/fabiocarrara/meye/wiki/Hardware,-Image-Parameters-and-Common Issues). Finally, we introduced a new selectable model based on DeepLab CNN that can be</p>
    <p>used interchangeably with the original UNET CNN.</p>
    <p>The lab did not find the thresholding controls particularly powerful for optimization in the web</p>
    <p>app. In future versions, these could be expanded and improved upon. Ideally, users could</p>
    <p>also tweak their images within the app itself - but this could be a future development.</p>
    <p>As stated in the previous point, we added new image preprocessing tools in the GUI: image</p>
    <p>color inversion, brightness, gamma, and contrast controls. Moreover, we plan to add</p>
    <p>customizable mathematical morphology as post processing tools.</p>
    <p>In the lab’s testing, because of the presence of an IR light that reflects in the eye in their setup, the lab often has a small second circular reflection outside of the pupil range. MEYE</p>
    <p>often recognized this circular area and included it in the pupil region - often creating a region</p>
    <p>of ‘two pupils’ or a 8-shaped region of interest. It would seem to us to be trivial to include</p>
    <p>parameters or implement constraints (even user toggled) that would estimate the max pupil</p>
    <p>size or to train in which only a single ‘connected’ area should be recognized - there should</p>
    <p>be no need for two separate regions (unconnected) to be identified.</p>
    <p>Thanks for pointing this out. To solve this problem we introduced in MEYE GUI the</p>
    <p>Morphology option (available in the PREDICTION section). This box should be checked to</p>
    <p>exclude possibly identified secondary areas. The exclusion is based on size, only the biggest</p>
    <p>area is retained. We would like to add an entire set of morphology tools that can be manually</p>
    <p>toggled to improve pupil detection, even if our goal is to improve the model until we reach a</p>
    <p>level of optimal performance in each situation.</p>
    <p>Finally, it was observed that the tool does not perform well at low contrast situations. The</p>
    <p>threshold optimization scale could perform better if the image contrast could be adjusted.</p>
    <p>This might be possible in a pre-processing stage that could be implemented for broader</p>
    <p>functionality. We suggest that the MEYE tool is improved to offer greater flexibility for image</p>
    <p>editing and file options, e.g. avi, which is a common format in science labs.</p>
    <p>We added brightness and contrast controls in the app, with the corresponding descriptions in</p>
    <p>the Github Wiki.</p>
    <p>Conflict of interest:</p>
    <p>Please clarify whether the tool described is also part of a commercial product, i.e. in addition</p>
    <p>to the free version, is there a commercial version with greater functionality marketed?</p>
    <p>The code is provided with the General Public License version 3 (GPLv3). It is a copyleft license granting open-source permissions for users including the right to download, make</p>
    <p>changes, redistribute and modify copies of the software. Moreover, GPLv3 states that</p>
    <p>modified versions of the program require making the changed source code available to</p>
    <p>users, under the GPLv3 terms.</p>
    <p>Further comments:</p>
    <p>1. Line 189 describes the tone presentation as 10 seconds, however, Fig 2C scale suggests</p>
    <p>the tone was 20 seconds in duration. Please explain the discrepancy - this is important as it</p>
    <p>also related to the comment below. Also related, line 267 lists interstimulus interval but not</p>
    <p>stimulus duration. It would be good to include both here in the results.</p>
    <p>We corrected the typo in the text and added the requested details.</p>
    <p>2. The pupil changes - particularly in Fig 2C are presented as relatively fast on the presented</p>
    <p>scale, with 20 seconds of auditory stimulation - which is quite a long stimulation period. Line</p>
    <p>262-263 discuss an aim as ‘real-time’ pupil analysis, therefore, evidence of faster pupil</p>
    <p>fluctuations (i.e. data presented on a more reasonable scale in comparison to velocity</p>
    <p>changes) would be necessary to appreciate the limits of the ‘real-time’ capacity for pupil</p>
    <p>monitoring.</p>
    <p>We adjusted the time scale of Fig 2C plots to 15 sec (5 pre and 10 post-stimulus) time</p>
    <p>window and reduced median filtering to better appreciate faster pupil and locomotor</p>
    <p>fluctuations.</p>
    <p>3. Lines 274-275: this statement is a bit misleading as a simple untrained stimulus is not</p>
    <p>quite the same ‘cognitive and emotional processing’ as a trained stimulus-reward association</p>
    <p>in relation to a behavioural choice paradigm for instance. Please tone down this statement -</p>
    <p>it is tangential to the point anyway and not required to be convincing of the pupil readout as</p>
    <p>linked to behavioural state changes (as the authors note in the lines following this). It</p>
    <p>represent a change in arousal/behavioural state in response to either the sensory stimulation</p>
    <p>and/or locomotion (this was not specifically separated here in the data presentation (i.e. tone</p>
    <p>with no locomotion - so one can not tell which) - beyond this is unclear and</p>
    <p>cognitive/emotional processes can not be inferred with this protocol.</p>
    <p>We modified the sentence accordingly. In addition we changed the article referenced here,</p>
    <p>from an article involving rewarded stimuli to papers involving orienting response pupil-related</p>
    <p>dilations.</p>
    <p>4. Line 278. The use of spontaneous pupil changes here is problematic. Spontaneous</p>
    <p>changes occur when the animal is stationary and small pupil changes are then correlated to</p>
    <p>arousal levels. This is in contrast to behavioural state changes (with associated changes in</p>
    <p>arousal) - including locomotion, that result in larger pupil changes. This is what is reported -</p>
    <p>responses in relation to locomotion - but these are not spontaneous. If the authors are</p>
    <p>calling them spontaneous because of the lack of sensory stimulation, this is misleading.</p>
    <p>Locomotion itself is a stimulus the affects pupil diameter so is not considered spontaneous. If</p>
    <p>the authors can show small fluctuations in pupil diameter during stationary periods then</p>
    <p>these would be the ‘spontaneous’ changes in arousal (see details in Vinck et al., 2015,</p>
    <p>https://pubmed.ncbi.nlm.nih.gov/25892300/).</p>
    <p>We agree that it is very important to test our system also with smaller pupil changes typical</p>
    <p>of pupil modulation during stationary periods. Therefore, although our previous data showed</p>
    <p>that in humans MEYE was capable of detecting pupil fluctuation induced by an oddball paradigm in the absence of movement, we decided to test this possibility also in the mouse.</p>
    <p>In Figure 3 we show the results of an experiment in which the mouse navigates a virtual</p>
    <p>reality corridor for 5 minutes. After this period a squared visual stimulus was presented in the</p>
    <p>binocular portion of the visual field. We detected a significant pupillary dilation after the onset</p>
    <p>of the visual stimulus, and no changes in evoked locomotion, suggesting that our system can</p>
    <p>capture subtle pupil size modulation unrelated to locomotion. Please note that to avoid</p>
    <p>confusion in the terminology we eliminated any reference to spontaneous pupil changes.</p>
    <p>5. Lines 281-282. As mentioned above, the authors did not separate the locomotion effects</p>
    <p>from the auditory stimulus presentation. This does not seem to be the point of the study, so</p>
    <p>is likely not necessary, but the statement that evoked versus state changes can be</p>
    <p>monitored based on this evidence is misleading. If they want to separate these parameter</p>
    <p>they need to have sufficient data where the tone was presented and mice where stationary.</p>
    <p>Otherwise, please revise to be more general in the conclusion and not try to separate</p>
    <p>arousal/state change due to locomotion from sensory evoked responses.</p>
    <p>The referee is correct that one possibility to see the sensitivity of MEYE for pupil fluctuations</p>
    <p>unrelated to locomotion is to separately evaluate pupil size in stationary trials. However, the</p>
    <p>number of trials in the auditory experiment was not sufficient to have enough stationary</p>
    <p>trials. However, as described in the previous point we have performed another experiment</p>
    <p>using visual cues that do not elicit stimulus-induced locomotion. This experiment clearly</p>
    <p>shows that our system is able to reveal stimulus evoked pupillary dilation in absence of</p>
    <p>significant stimulus induced increment of velocity.</p>
    <p>6. Line 333: presumably this is in reference to Figure 5A-C (not Fig 4). The two-photon</p>
    <p>mouse data should be elaborated on. Why was this performed, what are the differences and</p>
    <p>caveats. Fig 5B appears to result in a ‘smoother’ pupil diameter change. This is presumably</p>
    <p>because of some parameter of the images and not a real change in pupil behaviour. We are</p>
    <p>happy to see this data included, as there is a large field that many find this useful, but we</p>
    <p>would suggest the authors devote a paragraph to this analysis and describe remaining</p>
    <p>potential pitfalls/conditions of use.</p>
    <p>MEYE can also be used as an offline tool to analyze pupillometry videos in various file</p>
    <p>formats, depending on the video codec installed in the web browser. To demonstrate the</p>
    <p>feasibility to perform pupillometry on videos captured in a variety of situations in both mice</p>
    <p>and humans, we provide examples of pupil size quantifications in fig.5 (now fig.6). The B</p>
    <p>panel depicts a 40 seconds pupil trace of a mouse in a 2-photon microscopy experiment</p>
    <p>while recording visual cortical activity. Concerning the “smoothness” of the B panel trace,</p>
    <p>this could have been partly due to the different y-scale used. In the new figure (fig. 6) we</p>
    <p>now report fA and B panels with the same scales on the plot axes. Differences between the</p>
    <p>traces could also be due to the fact that each video was recorded using different cameras</p>
    <p>and setups (different luminance level, IR illumination, image resolution, type of head fixation</p>
    <p>and treadmill used), and with different habituation of the mouse to the setup, making the</p>
    <p>comparison between the different conditions hard to do. The videos were sampled from</p>
    <p>different experiments with no sensory stimulation, but locomotion and other movements</p>
    <p>could be present. For instance, we noticed that the eye of the animal in the 2P recording</p>
    <p>(panel B) was less mobile than in the condition of panel A, possibly contributing to the</p>
    <p>smoothness of the trace.</p>
    <p>7. We appreciate the discussion of DeepLabCut as an alternative analysis method, but a demonstration of this analysis in comparison to MEYE would be very informative. Indeed</p>
    <p>DeepLabCut requires some additional knowledge and the development of the web app</p>
    <p>offers a very simple and accessible interface - but one would need to know that MEYE at</p>
    <p>least approaches the efficiency of DLC (similar to EyeLink). This should be a relatively trivial</p>
    <p>test to perform.</p>
    <p>We could not perform a direct comparison with DeepLabCut (DLC) in an end-to-end fashion,</p>
    <p>as available pretrained models do not perform well as-is on our multi-specie scenario, and</p>
    <p>our dataset is incompatible with and not directly convertible to DLC’s expected label format.</p>
    <p>In addition, DLC is trained using user-defined key points and therefore it involves different</p>
    <p>steps (respect to the UNET) for the identification of the size and location of an object. Our</p>
    <p>11k images dataset is not suitable for direct DLC training, and this means that the two</p>
    <p>models should be trained with datasets with different labeling. To overcome this limitation in</p>
    <p>performance comparison, we adopted the “DeepLab” CNNs</p>
    <p>(https://arxiv.org/pdf/1606.00915.pdf), which are semantic segmentation networks at the</p>
    <p>core of DLC. These models are similar to UNET in terms of structure and can be trained</p>
    <p>exactly with the same dataset allowing a direct performance comparison. Thus, we trained</p>
    <p>and compared different versions of DeepLab with our UNET based pupillometer. We added</p>
    <p>a detailed description of the performances of MEYE and DLC in the Github Wiki</p>
    <p>(https://github.com/fabiocarrara/meye/wiki/MEYE-Models). We found our model to offer a</p>
    <p>better trade-off between speed (in FPS) and segmentation performance (in Dice Coefficient).</p>
    <p>These data have been included in Table 4 (line:435-439).</p>
    <p>However, since the MEYE backend model can be dynamically changed (through the</p>
    <p>MODELS dropdown menu), we integrated DeepLab into the MEYE web app. In this way, the</p>
    <p>user can choose between the UNET (our developed CNN) or the DeepLab (the standard</p>
    <p>DLC CNN) models.</p>
    <p>8. Although the authors undoubtedly want to present their best case scenario data - it would</p>
    <p>be useful from the user perspective (since the aim is clearly to appeal to a general user) to</p>
    <p>give examples or a further discussion regarding when MEYE may fail and how to optimize</p>
    <p>image collection to avoid this.</p>
    <p>We added in the Github Wiki a section describing the best practice to avoid common issues</p>
    <p>and pitfalls, moreover, we added to the web app the possibility to preprocess the image</p>
    <p>parameters to increase the flexibility of MEYE. We also underscored that IR light is</p>
    <p>necessary for accurate pupil measurements. Another point that is worth mentioning here is</p>
    <p>that the CNN implemented in a web app is capable of learning. Therefore, MEYE can be</p>
    <p>updated over time by adding new examples in the training dataset in which the actual model</p>
    <p>fails. We plan to perform periodic updates of the model that are able to refine and improve</p>
    <p>the actual performances of the CNN over time.</p>
    <p>9. We are missing a clear indication of specific points for future development for the MEYE.</p>
    <p>For an open-source tool, a discussion of this can be very helpful for the community and lead</p>
    <p>to greater advancements of mutually beneficial tool development.</p>
    <p>Thank you for this comment. We added a link in the Discussion to a page in the Github Wiki</p>
    <p>titled “Future Developments” in which we delineate our main future perspective.</p>
    <p>10. We are also missing a discussion about the speed of the pupil changes - particularly in</p>
    <p>relation to neural signals. There are many mentions of the link to cognitive processes, but a</p>
    <p>small statement with regard to the time scales is warranted - likely in the discussion.We added a statement regarding time scales in the discussion (line 405-406).</p>
    <p>11. The text in Figure 1 appears as low quality at the current figure size - perhaps this is just</p>
    <p>the PDF, but Figure 2 seems fine. Please check.</p>
    <p>We uploaded an image at a higher resolution.</p>
    <p>12. Line 99: ‘Upsampling and downsampling operations have factor 2’. This is a bit unclear</p>
    <p>as written. Could the authors either link to Fig. 1C here or rephrase this sentence for clarity.</p>
    <p>We rephrased the sentence.</p>
    <p>13. Line 290: likely should be ‘flash’ not ‘flashes’</p>
    <p>The typo has been corrected.</p>
    <p>14. The paragraphs towards the end get very lengthy. May I suggest creating more</p>
    <p>paragraph breaks/sections for ease of reading.</p>
    <p>We split the discussion into three paragraphs.</p>
    <p>15. Please expand the section on data analysis. Preferably phrase it in the following way</p>
    <p>Spearman ρ rank-order (or Pearson?) correlation was performed using the function “x” in</p>
    <p>pingouin.corr, etc. Please clarify if Spearman or Pearson correlation was used. Please</p>
    <p>expand on what was being compared using the Z-score, i.e. which data points against which</p>
    <p>other data points. We believe an ANOVA would have been useful for the time series</p>
    <p>analyses but perhaps we do not understand what is being compared.</p>
    <p>We clarified in the text that we employed only Spearman ρ rank-order correlation.</p>
    <p>We also added a description of what comparison we carried out using Z-score.</p>
    <p>As suggested, we changed the permutation tests with two-way repeated-measures ANOVA</p>
    <p>in the time series analysis.</p>
    <p>16. With regard to the other text, please use the past tense for the work presented, the</p>
    <p>equipment used and tests carried out. Reserve the present tense only in describing</p>
    <p>characteristics of commercial products e.g. line 127 “GPU that can process up to 28 fps” and</p>
    <p>such. Please do not use the present tense in other contexts. E.g. Line 177: Velocity was</p>
    <p>monitored ... Line 187: a curved monitor was placed, Line 214: Photic stimulation was</p>
    <p>delivered ... and so on. There are many other instances where “is” should be replaced with</p>
    <p>"was”.</p>
    <p>We corrected the errors.</p>
    <p>17. We are confused by the statement on line 99: ’Upsampling and downsampling</p>
    <p>operations have factor 2’. It is probably made worse by the use of the present tense where</p>
    <p>the past tense should have been used. Perhaps a better phrasing would be that</p>
    <p>convolutional layer upsampling and downsampling (in the CNN they used) was by a factor of</p>
    <p>2. As it is written, it rather gives the impression that all convolution up/downsampling by</p>
    <p>CNNs is by a factor of 2.</p>
    <p>We rephrased the sentence</p>
  </sec>
</back>
