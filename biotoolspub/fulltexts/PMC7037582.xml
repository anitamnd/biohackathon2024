<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ444 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEga1 jpg ?>
<?FILEmmc1 docx ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?FILEsi10 svg ?>
<?FILEsi11 svg ?>
<?FILEsi12 svg ?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7037582</article-id>
    <article-id pub-id-type="publisher-id">S2001-0370(19)30318-6</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2020.01.013</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>C-RNNCrispr: Prediction of CRISPR/Cas9 sgRNA activity using convolutional and recurrent neural networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Zhang</surname>
          <given-names>Guishan</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>Dai</surname>
          <given-names>Zhiming</given-names>
        </name>
        <email>daizhim@mail.sysu.edu.cn</email>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="af015" ref-type="aff">c</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au015">
        <name>
          <surname>Dai</surname>
          <given-names>Xianhua</given-names>
        </name>
        <email>issdxh@mail.sysu.edu.cn</email>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af020" ref-type="aff">d</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
    </contrib-group>
    <aff id="af005"><label>a</label>School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou 510006, China</aff>
    <aff id="af010"><label>b</label>School of Data and Computer Science, Sun Yat-sen University, Guangzhou 510006, China</aff>
    <aff id="af015"><label>c</label>Guangdong Province Key Laboratory of Big Data Analysis and Processing, Sun Yat-sen University, Guangzhou 510006, China</aff>
    <aff id="af020"><label>d</label>Southern Marine Science and Engineering Guangdong Laboratory, Zhuhai, China</aff>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding authors at: School of Data and Computer Science, Sun Yat-sen University, Guangzhou 510006, China (Z. Dai); School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou 510006, China (X. Dai). <email>daizhim@mail.sysu.edu.cn</email><email>issdxh@mail.sysu.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>12</day>
      <month>2</month>
      <year>2020</year>
    </pub-date>
    <volume>18</volume>
    <fpage>344</fpage>
    <lpage>354</lpage>
    <history>
      <date date-type="received">
        <day>6</day>
        <month>8</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>20</day>
        <month>12</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>1</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 The Authors</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="CC BY-NC-ND" xlink:href="http://creativecommons.org/licenses/by-nc-nd/4.0/">
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="graphical" id="ab005">
      <title>Graphical abstract</title>
      <fig id="f0035" position="anchor">
        <graphic xlink:href="ga1"/>
      </fig>
    </abstract>
    <abstract id="ab010">
      <p>CRISPR/Cas9 is a hot genomic editing tool, but its success is limited by the widely varying target efficiencies among different single guide RNAs (sgRNAs). In this study, we proposed C-RNNCrispr, a hybrid convolutional neural networks (CNNs) and bidirectional gate recurrent unit network (BGRU) framework, to predict CRISPR/Cas9 sgRNA on-target activity. C-RNNCrispr consists of two branches: sgRNA branch and epigenetic branch. The network receives the encoded binary matrix of sgRNA sequence and four epigenetic features as inputs, and produces a regression score. We introduced a transfer learning approach by using small-size datasets to fine-tune C-RNNCrispr model that were pre-trained from benchmark dataset, leading to substantially improved predictive performance. Experiments on commonly used datasets showed C-RNNCrispr outperforms the state-of-the-art methods in terms of prediction accuracy and generalization. Source codes are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Peppags/C_RNNCrispr" id="ir005">https://github.com/Peppags/C_RNNCrispr</ext-link>.</p>
    </abstract>
    <kwd-group id="kg005">
      <title>Keywords</title>
      <kwd>CRISPR/Cas9</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>Bidirectional gate recurrent unit network</kwd>
      <kwd>sgRNA</kwd>
      <kwd>On-target</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0005">CRISPR/Cas9 originated from bacterial immune system has been adopted for a promising genomic editing tool in recent years <xref rid="b0005" ref-type="bibr">[1]</xref>. It consists of two components: a nuclease activity-carrying Cas9 protein and a specificity-programming single guide RNA (sgRNA) <xref rid="b0010" ref-type="bibr">[2]</xref>, <xref rid="b0015" ref-type="bibr">[3]</xref>. Recognition and cleavage work via complementarity of a ~20-bp sequence within the sgRNA to the genome target region flanked by a 3′ NGG protospacer adjacent motif (PAM) based on Watson-Crick base pairing <xref rid="b0005" ref-type="bibr">[1]</xref>. The success of CRISPR/Cas9 system for genome engineering of prokaryotic hosts largely depends on sgRNA activity. However, different activities among various sgRNAs still represent a significant limitation, leading to inconsistent target efficiency <xref rid="b0020" ref-type="bibr">[4]</xref>. Moreover, the specific features that determine sgRNA activity remain largely unexplored. Therefore, accurate prediction of sgRNA activity would facilitate the design of sgRNAs by maximizing aimed activity at the desired target site while minimizing off-target cleavage <xref rid="b0025" ref-type="bibr">[5]</xref>.</p>
    <p id="p0010">Numerous computational methods for sgRNA activity prediction have been developed based on different rules. Existing tools fall into three classes, namely alignment-based, hypothesis-driven and learning-based methods <xref rid="b0030" ref-type="bibr">[6]</xref>. Alignment-based tools aligned the sgRNA from the given genome by locating PAM. For example, CRISPRdirect performs sgRNA selection based on investigating the entire genome for perfect matches with the candidate target sequence and their seed sequence flanking the PAM <xref rid="b0035" ref-type="bibr">[7]</xref>. Hypothesis driven-based tools score the sgRNA mainly considering the contribution of specific genome context factors. For instance, ECRISP ranks sgRNAs by taking into account on-target specificity and the number of off-targets <xref rid="b0040" ref-type="bibr">[8]</xref>. Machine learning-based methods predict the sgRNA cleavage efficacy based on training a model by integrating different features affecting the efficiency. For example, sgRNA Designer considers the position of the target site relative to the transcription start site and position within the protein when evaluating the efficacy of candidate sgRNAs <xref rid="b0045" ref-type="bibr">[9]</xref>. sgRNA Scorer unravels both locus accessibility and sequence composition of the sgRNA that are important in determining sgRNA target efficacy <xref rid="b0050" ref-type="bibr">[10]</xref>. However, the accuracy of machine learning-based tools varies widely among the constructed features <xref rid="b0055" ref-type="bibr">[11]</xref>. Moreover, the hand-crafted features may result in redundancy, further leading to the poor prediction results. Therefore, machine learning-based methods have obvious drawbacks, e.g. requiring expert domain knowledge and showing low generalization. To learn more about the computational methods used to facilitate the process of CRISPR/Cas9 sgRNA target efficacy prediction, we refer readers to <xref rid="b0030" ref-type="bibr">[6]</xref>, <xref rid="b0060" ref-type="bibr">[12]</xref> for a comprehensive reading.</p>
    <p id="p0015">More recently, deep learning <xref rid="b0065" ref-type="bibr">[13]</xref> is another exciting and promising approach being applied in the genomics field. It is a variation of machine learning that uses neural networks to automatically extract novel features from input data. Deep learning has made impressive advances in areas such as computer vision <xref rid="b0070" ref-type="bibr">[14]</xref> and natural language processing (NLP) <xref rid="b0075" ref-type="bibr">[15]</xref>. Besides, deep learning-based methods which are mainly based on convolutional neural networks (CNNs) are attractive solutions for CRISPR sgRNA target efficacy prediction problems. Currently, several attractive strategies have been explored for this issue. To the best of our knowledge, Seq-deepCpf1 is the first published deep learning method to predict CRISPR/Cpf1 gRNA on-target activity. It used CNN to extract features from the input gRNA sequence <xref rid="b0080" ref-type="bibr">[16]</xref>. DeepCRISPR has been successfully applied for predicting CRISPR/Cas9 sgRNA on-target knockout efficiency and whole genome off-target profiles by incorporating deep convolutionary neural network (DCDNN)-based auto-encoder as well as CNN <xref rid="b0025" ref-type="bibr">[5]</xref>. DeepCas9 used CNN to automatically learn the sequence determinants and predict the activities of sgRNAs across multiple species genomes <xref rid="b0085" ref-type="bibr">[17]</xref>. These three models all used CNNs to extract features from the input genomic sequence. Overall, they are superior to machine learning-based tools in prediction accuracy.</p>
    <p id="p0020">CNNs are multilayer architectures where the successive layers are designed to learn abstract features, until the last layer produces an output value. They use weight-sharing strategy to capture local patterns in data such as sequences, which is analogous to taking the position weight matrix of a motif and scanning it across the DNA sequence <xref rid="b0090" ref-type="bibr">[18]</xref>. CNNs perform well when some spatially invariant patterns of the inputs are expected. But they are restricted to learn the local patterns. Recurrent neural networks (RNN), particularly based on long short-term memory network (LSTM) <xref rid="b0095" ref-type="bibr">[19]</xref> and gated recurrent unit (GRU) <xref rid="b0100" ref-type="bibr">[20]</xref>, have been designed for sequential or time-series data <xref rid="b0105" ref-type="bibr">[21]</xref>. The hidden layers of RNN are regarded as memory states which can retain information from previous sequence and be updated at each step. Several tools have been introduced in the literature to demonstrate the synergistic improvements of CNN-RNN models due to the complementary in their modeling capability. SPEID achieved competitive performance using types of epigenetic data for enhancer-promoter interaction prediction in a unified CNN-RNN model <xref rid="b0110" ref-type="bibr">[22]</xref>. DeeperBind added a LSTM layer to learn the dependencies between sequence features identified by CNN, further improving the prediction of protein binding specificity <xref rid="b0115" ref-type="bibr">[23]</xref>. DanQ, a CNN combined bi-directional LSTM (BLSTM) framework, has recently been introduced to quantify function of DNA sequences by incorporating the motifs and a complex regulatory grammar between the motifs <xref rid="b0120" ref-type="bibr">[24]</xref>. Pan et al. proposed a hybrid CNN-BLSTM based iDeepS to concurrently identify the binding sequence and structure motifs from RNA sequences <xref rid="b0125" ref-type="bibr">[25]</xref>.</p>
    <p id="p0025">The previous success of CNN-RNN in bioinformatics motivated us to extend its applications to CRISPR/Cas9 sgRNA on-target activity prediction. In this work, we introduced C-RNNCrispr, a hybrid architecture combining CNN with bidirectional GRU (BGRU), to predict sgRNA cleavage efficacy. The intuition of this hybrid architecture is to use CNN for feature extraction while using the BGRU to model sequential dependencies of sgRNA features. C-RNNCrispr contains two branches: sgRNA branch and epigenetic branch, respectively being used to extract sgRNA and epigenetic features. Particularly, we first represented sgRNA sequences and its related epigenetic features by one-hot encoding, which transforms the inputs into two 4 × 23 binary matrices for subsequent convolution operations. Second, the encoded sgRNA and epigenetic matrices were respectively fed into sgRNA branch and epigenetic branch for abstract features extraction. Third, the outputs of these two branches were integrated by element-wise multiplication. Finally, the outputs of the merged layer were fed into a linear regression layer to grade sgRNA cleavage efficiency. Besides, we proposed a transfer learning strategy to address the small-size sample problem. To be specific, we first pre-trained the proposed C-RNNCrispr on the benchmark dataset. Subsequently, we fine-tuned the pre-trained C-RNNCrispr on small-size cell-line datasets to predict sgRNA on-target activity. Experiments results showed that C-RNNCrispr consistently surpasses other available state-of-the-art prediction methods.</p>
  </sec>
  <sec id="s0010">
    <label>2</label>
    <title>Methods</title>
    <sec id="s0015">
      <label>2.1</label>
      <title>Data resources</title>
      <p id="p0030">We utilized the publicly available datasets packaged by Chuai et al. <xref rid="b0025" ref-type="bibr">[5]</xref>, available at <ext-link ext-link-type="uri" xlink:href="https://github.com/bm2-lab/DeepCRISPR" id="ir010">https://github.com/bm2-lab/DeepCRISPR</ext-link>. It contains a benchmark dataset and four cell-line independent datasets. For benchmark dataset, each observation in the data contains a 23-nt sgRNA sequence and a binary class label indicating the high-efficiency and low-efficiency sgRNAs. There are 180,512 unique sgRNA sequences in this dataset. We used this dataset to build our C-RNNCrispr model. HCT116 and HELA datasets were generated in human HCT116 and Hela cells <xref rid="b0130" ref-type="bibr">[26]</xref>, HEK293T dataset generated in human Hek293t cell was original published in <xref rid="b0050" ref-type="bibr">[10]</xref>. HL60 dataset generated in human Hl60 cell was original published in <xref rid="b0135" ref-type="bibr">[27]</xref>. Each observation contains a sgRNA sequence and the measured knockout efficacy. After removing the redundancy, the number of datasets HCT116, HEK293T, HELA and hl60 was 4239, 2333, 8101 and 2076, respectively.</p>
      <p id="p0035">The above datasets were processed by Chuai et al. <xref rid="b0025" ref-type="bibr">[5]</xref>. In their study, four epigenetic features of each sgRNA sequence was obtained from ENCODE <xref rid="b0140" ref-type="bibr">[28]</xref>, including CTCF binding information obtained from ChIP-Seq assay, H3K4me3 information from ChIP-Seq assay, chromatin accessibility information from DNase-Seq assay, and DNA methylation information from RRBS assay. Each epigenetic feature was denoted by a symbolic sequence with length of 23, with notations “A” and “N” meaning the present and absent of the epigenetic feature at a particular base position of DNA regions.</p>
      <p id="p0040">The binary cleavage efficacy was obtained by converting the measured sgRNA efficacy using a log-fold change of 1 as the cutoff. The high-activity sgRNAs were denoted by 1 and the low-activity ones were represented by 0. Besides, numerical cleavage efficacy was defined by applying a collaborative filtering-based data normalization method <xref rid="b0145" ref-type="bibr">[29]</xref>. To be specific, a matrix <inline-formula><mml:math id="M1" altimg="si2.svg"><mml:mrow><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:math></inline-formula> was formulated where each row represented the experiments and each column denoted one gRNA. Normalized numerical sgRNA cleavage efficacy value was defined as<disp-formula id="e0005"><label>(1)</label><mml:math id="M2" altimg="si3.svg"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="italic">nor</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="italic">mn</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">-</mml:mo><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="italic">row</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="italic">col</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="italic">all</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M3" altimg="si4.svg"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="italic">mn</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denoted the <italic>n-th</italic> sgRNA in the <italic>m-th</italic> experiments. <inline-formula><mml:math id="M4" altimg="si5.svg"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="italic">row</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M5" altimg="si6.svg"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="italic">col</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M6" altimg="si7.svg"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="italic">all</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represented the mean values for each row, the mean values for each column and the mean values of <inline-formula><mml:math id="M7" altimg="si2.svg"><mml:mrow><mml:mi mathvariant="normal">Y</mml:mi></mml:mrow></mml:math></inline-formula>, respectively. For each sgRNA, the log<sub>2</sub>-fold change was calculated. Subsequently, sgRNAs within each gene were ranked by carrying out the rank-based normalization method proposed by Doench et al. <xref rid="b0045" ref-type="bibr">[9]</xref>. The resulting normalized ranks were averaged across cell types and rescaled into 0–1. Here, 1 means the successful on-target cleavage efficacy. Each observation in the above four cell-line datasets contained the 23-nt sgRNA sequence, chromosome, start site, end site, strand, four types of symbolic epigenetic features, normalized numerical cleavage efficacy and the binary efficiency. We used these datasets to evaluate and compare the proposed C-RNNCrispr with several current deep learning and existing machine learning prediction methods.</p>
    </sec>
    <sec id="s0020">
      <label>2.2</label>
      <title>Sequence encoding</title>
      <p id="p0045">The input sequence should be numerically encoded before being fed into deep learning models. Several methods have been proposed to represent the input sequence, such as one-hot encoding and k-mer embedding computed by word2vec <xref rid="b0150" ref-type="bibr">[30]</xref>. For one-hot encoding representation, the input sequence is represented by a 4 × L matrix where 4 is the size of nucleotides vocabulary (A, C, G and T) and L is the length of the sequence. Each position in the sequence is related to a vector of length four with a single non-zero element corresponding to the nucleotide in that position. Specifically, the nucleotides A, C, G and T are encoded as four one-hot vectors <inline-formula><mml:math id="M8" altimg="si8.svg"><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="M9" altimg="si9.svg"><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="M10" altimg="si10.svg"><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and<inline-formula><mml:math id="M11" altimg="si11.svg"><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>. Notably, one-hot encoding simply transforms DNA sequences into images with binary values rather than real continuous-values for each pixel with four channels corresponding to A, C, G and T, which may lead to restrictive effects on the performance. When using the k-mer embedding, the input sequence is split into overlapped k-mers of length k using a sliding window with stride <italic>s</italic>. Subsequently, each k-mer in the obtained sequence is mapped into a <italic>d</italic>-dimensional vector using word2vec method <xref rid="b0150" ref-type="bibr">[30]</xref>. Word2vec is an unsupervised learning algorithm which maps k-mers from the vocabulary to vectors of real numbers in a low-dimensional space. The value of k-mer length and stride can be determined by model training. We refer readers to the original publication for details <xref rid="b0150" ref-type="bibr">[30]</xref>. A recent study showed that using k-mer embedding to represent input sequence gained superiority of model performance than one-hot encoding <xref rid="b0155" ref-type="bibr">[31]</xref>. However, the improvement in performance comes at the cost of the training time for sequence modeling. Overall, sequence representations denoted by one-hot encoding are sparse, high-dimensional and hardcoded, whereas k-mer embedding representations are dense, relatively low-dimensional, and learned from data.</p>
      <p id="p0050">It is noteworthy that one-hot encoding has been adapted in several previous methods on sgRNA on-target activity prediction <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0085" ref-type="bibr">[17]</xref>. For fair comparison, we chose this approach to encode sgRNA sequence with 23 nucleotides in length. Therefore, a 1-by-23 nucleotide sequence was denoted by a 4 × 23 binary matrix. Four kinds of epigenetic features (mentioned in <xref rid="s0015" ref-type="sec">Section 2.1</xref>) were analyzed in this work. We used a one-dimensional binary vector to encode each of epigenetic feature. The presence of the specific epigenetic feature at a particular position is represented by 1 while its absence is denoted by 0. As a consequence, four epigenetic features are denoted by a 4 (types of epigenetic features) × 23 (sequence length) binary matrix. The encoded sgRNA and epigenetic features binary matrices were subsequently fed into C-RNNCrispr-based model for training and testing.</p>
    </sec>
    <sec id="s0025">
      <label>2.3</label>
      <title>C-RNNCrispr model</title>
      <p id="p0055">A convolutional neural network (CNN) <xref rid="b0160" ref-type="bibr">[32]</xref> is a type of deep, feed-forward artificial neural network that can capture the hierarchical spatial representations, thus avoiding laborious manual feature engineering. Recurrent neural network (RNN) <xref rid="b0165" ref-type="bibr">[33]</xref> is a variation of deep neural networks. Unlike CNN, RNN has an internal state that is updated as the network reads the input sequence. This internal memory allows RNN to capture interactions between the elements along the sequence, and is thus widely used in the field of NLP <xref rid="b0170" ref-type="bibr">[34]</xref>. For details of CNN and RNN, see <xref rid="s0110" ref-type="sec">Supplementary Note</xref>. CNN excels at capturing local patterns in sequence data by using weight-sharing strategy but it fails at learning sequential correlations. Inversely, RNN achieves excellent performance for sequential modelling while fails to derive features in parallel. Many studies support the idea that the combination of CNN and RNN can achieve better performance <xref rid="b0120" ref-type="bibr">[24]</xref>, <xref rid="b0175" ref-type="bibr">[35]</xref>, <xref rid="b0180" ref-type="bibr">[36]</xref>. To be specific, the convolutional modules stage scans the sequence using a series of 1D convolutional filter to capture sequence patterns. The following RNN stage is used for learning complex high-level relationships by considering the orientations and spatial relationships between the motifs. Inspired by these studies, we proposed a unified CNN-RNN framework to predict CRISPR/Cas9 sgRNA on-target activity. <xref rid="f0005" ref-type="fig">Fig. 1</xref> and the following description give a summary of the basic architectural structure of C-RNNCrispr used.<fig id="f0005"><label>Fig. 1</label><caption><p>An overview of C-RNNCrispr architecture. We trained the C-RNNCrispr from scratch on the benchmark dataset. Then, we fine-tuned the well-trained pre-trained C-RNNCrispr model on small-size datasets.</p></caption><graphic xlink:href="gr1"/></fig></p>
      <p id="p0060">As shown in <xref rid="f0005" ref-type="fig">Fig. 1</xref>, C-RNNCrispr consists of two branches, viz. sgRNA branch and epigenetic branch. The sgRNA branch is applied to extract the abstract features of sgRNA sequences, whereas epigenetic branch is necessary to reveal the hidden knowledge of epigenetic information. Note that, the structure of epigenetic branch is similar to sgRNA branch except that a bidirectional gate recurrent unit network (BGRU, a special kind of variants for RNN) layer is absent. For the example of sgRNA branch, it receives a 4 (size of nucleotides vocabulary) × 23 (sequence length) binary matrix as an input. The first layer of the sub-network is a one-dimensional (1D) convolutional layer (conv_1), which consists of an array of 256 filters that convolves with the input sequence. Our rationale for including a convolution layer before BGRU layer is that CNNs achieve excellent performance for extracting sgRNA sequence features while keeping the number of model parameters tractable by applying convolutional operator. Rectified linear units (ReLU) <xref rid="b0185" ref-type="bibr">[37]</xref> is subsequently used to keep only positive filter values and set the remaining to zeros, where <inline-formula><mml:math id="M12" altimg="si12.svg"><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">U</mml:mi><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p id="p0065">The second layer is a local max-pooling layer (pool_1) with window size of 2. It connects with the outputs of previous layer by propagating only the largest output of each kernel with each stride for download sampling.</p>
      <p id="p0070">The third layer is a BGRU layer with dimension of 256. The motivation for adding a BGRU layer is that it is apt at enhancing the relevance between features of the sequences. The outputs of two parallel GRUs are concatenated to obtain our final feature representation containing both the forward and backward information of sgRNA sequence.</p>
      <p id="p0075">Next, the obtained features are followed by four fully connected layers (fc_1, fc_2, fc_3 and fc_4) with the sizes of 256, 128, 64 and 40 with ReLU activations, respectively. We used dropout for model regularization to avoid overfitting. The dropout rate will be given in <xref rid="s0040" ref-type="sec">Section 2.5.1</xref>.</p>
      <p id="p0080">The features of the last fully connected layer of sgRNA and epigenetic branches are integrated using element-wise multiplication operator for features merging. Finally, the outputs of the merged features are fed into a linear regression transformation to make a prediction of sgRNA on-target activity.</p>
    </sec>
    <sec id="s0030">
      <label>2.4</label>
      <title>Experimental settings</title>
      <p id="p0085">The proposed C-RNNCrispr model was implemented using Python 3.6.4 and Keras 2.1.0 (<ext-link ext-link-type="uri" xlink:href="http://keras.io" id="ir015">http://keras.io</ext-link>) with TensorFlow (1.4.0) as the backend. All experiments were carried out on a desktop computer with Intel (R) Core (TM) i7-7800X CPU @ 3.50 GHz, Ubuntu 16.04.5 LTS and 32 GB RAM, as well as two NVIDIA GeForce GTX 1080 Ti GPUs with 11 GB of memory per GPU. We used mean square error (MSE) as the loss function for the regression task. During training process, we applied the RMSprop algorithm <xref rid="b0190" ref-type="bibr">[38]</xref> for stochastic optimization of the objective of the loss function.</p>
    </sec>
    <sec id="s0035">
      <label>2.5</label>
      <title>Implementation of the C-RNNCrispr model</title>
      <sec id="s0040">
        <label>2.5.1</label>
        <title>Model selection and pre-training</title>
        <p id="p0090">Model selection was performed by testing the variants of C-RNNCrispr as summarized in <xref rid="t0005" ref-type="table">Table 1</xref> on the benchmark dataset. The dataset was randomly divided into a training dataset and an independent testing dataset with 80% and 20% classes. Experiments were performed under 5-fold cross-validation in the training phase. During each training test, the training data was randomly split into equal five parts. Among them, four parts were regarded as training dataset, while the remaining one part was taken as testing dataset. <xref rid="s0110" ref-type="sec">Supplementary Table S1</xref> summarizes the number of the training samples, validation samples and testing samples of all the experiments in our study.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>The variants of C-RNNCrispr models compared in this work.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Architecture</th></tr></thead><tbody><tr><td>C-RNNCrispr_std</td><td>Using one convolutional layer with 256 1D filtering kernels of length 5 with dropout layer</td></tr><tr><td>C-RNNCrispr_2conv</td><td>Using two convolutional layers with 256 1D filtering kernels of length 5</td></tr><tr><td>C-RNNCrispr_len7</td><td>Using one convolutional layer with 256 1D filtering kernels of length 7</td></tr><tr><td>C-RNNCrispr_avepool2</td><td>Using average-pooling layer of window size 2</td></tr><tr><td>C-RNNCrispr_ndrop</td><td>Using one convolutional layer with 256 1D filtering kernels of length 5 without using dropout layer</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The descriptions of four C-RNNCrispr variants are relative to C-RNNCrispr_std model descripted in <xref rid="s0025" ref-type="sec">Section 2.3</xref>.</p></fn></table-wrap-foot></table-wrap></p>
        <p id="p0095">It is worth noting that the number of BGRU (1), neurons per layer of CNN (256) and BGRU (256) with recurrent dropout rate of 0.2 of C-RNNCrispr were manually set empirically. We constructed five C-RNNCrispr variants by various parameters: the number of convolution layers, the window size of convolution filters, the window size of pooling layer and the dropout rate. <xref rid="f0005" ref-type="fig">Fig. 1</xref> depicts the standard architecture of C-RNNCrispr (C-RNNCrispr_std). Mini-batch gradient descent was performed for optimization and further reduced the gradient variance during training process. Each experiment was run for 200 epochs, and batch size was set to 256.</p>
        <p id="p0100">Bayesian optimization <xref rid="b0195" ref-type="bibr">[39]</xref> is an automatic tuning method for optimizing the given learning algorithm via modeling the generalization performance. Hyperopt <xref rid="b0200" ref-type="bibr">[40]</xref> and hyperas <xref rid="b0205" ref-type="bibr">[41]</xref> (<ext-link ext-link-type="uri" xlink:href="https://github.com/maxpumperla/hyperas" id="ir020">https://github.com/maxpumperla/hyperas</ext-link>) were used to carry out the Bayesian optimization created with Keras. We used the benchmark data to train models with different hyperparameters (i.e., dropout out rate, activation function, batch size and epoch) suggested iteratively by Bayesian optimization. The selection was applied over the following set of parameters: dropout coefficient (0.2, 0.3, 0.4, 0.5), activation function (‘ReLU’, ‘ELU’, ‘LeakyReLU’), batch size (128, 256, 512) and epoch (100, 200). The training data and test data were generated in the same way in <xref rid="s0040" ref-type="sec">Section 2.5.1</xref>. After enough iterations, we took the best hyperparameters that showed the minimum average validation loss as the final parameters of the model. The hyperparameters were as follows: activation function: ‘ReLU’; batch size: 256; epoch: 200. The dropout rates were 0.2 (keeping 80% of the connections) and 0.3 for specific layers of C-RNNCrispr. Specifically, the dropout rate following the layers of max_pooling layer, GBRU, and four fully connected layers of sgRNA branch was set to 0.2, 0.3, 0.3, 0.2, 0.2 and 0.2, respectively. Similarly, the dropout rate following the max_pooling layer and four fully connected layers of epigenetic branch was set to 0.3, 0.2, 0.3, 0.3 and 0.2, respectively. The dropout rate following the multiply layer was 0.2. We then carried out the final hyperparameters to pre-train C-RNNCrispr model again from scratch on benchmark dataset under 5-fold cross-validation.</p>
      </sec>
      <sec id="s0045">
        <label>2.5.2</label>
        <title>Transfer learning</title>
        <p id="p0105">Transfer learning is the process of migration of trained model parameters to a new model to help train the new model. Previous studies in computer vision have demonstrated that deep models with better performance are learned via transfer learning from large scale datasets to other datasets of limited scales <xref rid="b0210" ref-type="bibr">[42]</xref>, <xref rid="b0215" ref-type="bibr">[43]</xref>. Motivated by these studies, we investigated four transfer learning strategies (i.e., fine tune, frozen CNN, frozen BGRU and frozen FC) of borrowing information from benchmark dataset, and determined the optimal one for testing new cell line. For complete details see <xref rid="t0010" ref-type="table">Table 2</xref>.<table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>Four transfer learning strategies for C-RNNCrispr model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Strategy</th><th>Transfer learning procedure</th></tr></thead><tbody><tr><td>Fine tune</td><td>Only the weights in the last two fully connected layers of sgRNA and epigenetic branches as well as the last fully connected layer of C-RNNCrispr are trainable</td></tr><tr><td>Frozen CNN</td><td>Freeze the weights of CNN layers</td></tr><tr><td>Frozen BGRU</td><td>Freeze the weights of BGRU layer</td></tr><tr><td>Frozen FC</td><td>Freeze the weights of fully connected layers</td></tr></tbody></table></table-wrap></p>
        <p id="p0110">We compared C-RNNCrispr with these four transfer learning strategies and determined the one which achieved the best performance in terms of Spearman correlation and area under the ROC curve (AUROC) as the final transfer learning strategy for the following analysis. Four cell line datasets were used under 5-fold cross-validation for performance evaluation. The training data and testing data for each cell line were constructed in the same way as described in <xref rid="s0040" ref-type="sec">Section 2.5.1</xref>. We first pre-trained C-RNNCrispr from scratch on the benchmark dataset under 5-fold cross-validation. Next, we applied the above transfer learning strategies on small-size cell-line datasets to evaluate and compare the predictive performance. Here, we provide a detailed description of fine tune strategy. Besides the last two fully connected layers of sgRNA branch and epigenetic branch as well as the element-wise multiplication and the last fully connected layers of C-RNNCrispr, all the layers of these two branches were frozen. After borrowing weights of the pre-trained C-RNNCrispr base network, we fine-tuned C-RNNCrispr to minimize the MSE loss function using the RMSprop optimizer for small-size cell lines. Through fine tune, C-RNNCrispr could effectively prevent overfitting when applying for small-size datasets. For any given cell line of interest, the training process is described as below:<list list-type="simple" id="l0005"><list-item id="o0005"><label>(1)</label><p id="p0115">Pre-train C-RNNCrispr from scratch on benchmark dataset for 200 epochs.</p></list-item><list-item id="o0010"><label>(2)</label><p id="p0120">Freeze the convolution, BGRU, max-pooling layers, the first two fully connected layers of sgRNA branch. On the other hand, freeze the convolution, max-pooling layers and the first two fully connected layers of the epigenetic branch.</p></list-item><list-item id="o0015"><label>(3)</label><p id="p0125">Train the last two fully connected layers of both the above two branches, the element-wise multiplication layer and the last fully connected layer of C-RNNCrispr with training data from cell line of interest for another 200 epochs.</p></list-item><list-item id="o0020"><label>(4)</label><p id="p0130">Evaluate C-RNNCrispr model on the test data.</p></list-item></list></p>
      </sec>
    </sec>
    <sec id="s0050">
      <label>2.6</label>
      <title>Settings of other methods</title>
      <p id="p0135">We ran the Python code of Seq_deepCpf1 (downloaded from Github at <ext-link ext-link-type="uri" xlink:href="https://github.com/MyungjaeSong/Paired-Library" id="ir025">https://github.com/MyungjaeSong/Paired-Library</ext-link>) using the same data and training process. It is noteworthy that the input of Seq_deepCpf1 is a 4-by-34 binary matrix. We changed the input shape of Seq_deepCpf1 model into 4-by-23 into to match the size of the data in this work. We used the benchmark dataset to pre-train the model. For fair comparison, we only fine-tuned the weights parameters in the last two layers (1681 free parameters) for cell line-specific prediction.</p>
      <p id="p0140">The source R code of DeepCas9 can be downloaded from Github at <ext-link ext-link-type="uri" xlink:href="https://github.com/lje00006/DeepCas9" id="ir030">https://github.com/lje00006/DeepCas9</ext-link>. We constructed DeepCas9 model following the description in <xref rid="b0085" ref-type="bibr">[17]</xref> and trained it using the same training and validation data in Python. For fair compared with other deep learning-based methods, we also applied transfer learning for DeepCas9 (DeepCas9 plus transfer learning). Specifically, we used fine tune by only training the top two fully connected layers (80769 free parameters) of DeepCas9 architecture. The source code of DeepCRISPR were run by getting from <ext-link ext-link-type="uri" xlink:href="https://github.com/bm2-lab/DeepCRISPR" id="ir035">https://github.com/bm2-lab/DeepCRISPR</ext-link>. The performance of sgRNA Designer and sgRNA Scorer were taken from Chuai et al. <xref rid="b0025" ref-type="bibr">[5]</xref>.</p>
    </sec>
    <sec id="s0055">
      <label>2.7</label>
      <title>Performance measures</title>
      <p id="p0145">In order to evaluate the performance of C-RNNCrispr, we used Spearman correlation coefficient between efficiency scores and predicted scores. We used Spearman correlation because it is more robust to outliers compared with Pearson correlation coefficient <xref rid="b0220" ref-type="bibr">[44]</xref>. In addition, it was adopted in previous sgRNA activity prediction studies <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0045" ref-type="bibr">[9]</xref>, <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0085" ref-type="bibr">[17]</xref>. We also calculated AUROC to comprehensively quantify the overall predictive model performance of C-RNNCrispr. The value of AUROC is in [0, 1], where 1 equates to a successful performance. In this study, we applied 0.5 AUROC as the baseline.</p>
    </sec>
  </sec>
  <sec id="s0060">
    <label>3</label>
    <title>Results</title>
    <sec id="s0065">
      <label>3.1</label>
      <title>Performance comparisons for different architectures under 5-fold cross-validation on benchmark dataset</title>
      <p id="p0150">We compared the performance of different model architectures trained on benchmark dataset under 5-fold cross-validation in <xref rid="t0015" ref-type="table">Table 3</xref>. Some interesting conclusions can be extracted: first, amongst the compared architectures, C-RNNCrispr_std achieved the best performance for predicting sgRNA on-target activity with mean Spearman correlation and AUROC values of 0.877 and 0.976, respectively. Second, CNN with one convolutional layer (C-RNNCrispr_std) surpassed that with two convolutional layers (C-RNNCrispr_2conv). Third, the performance of C-RNNCrispr_std was a little superior than C-RNNCrispr_len7 and C-RNNCrispr_avepool2. Finally, we noticed that C-RNNCrispr_std outperformed C-RNNCrispr_ndrop, which is expected since dropout regularization contributes to the prevention or mitigation of overfitting. Together, these results demonstrated that C-RNNCrispr_std achieves the best generalization performance amongst these architectures. Therefore, we chose C-RNNCrispr_std for the following experiments.<table-wrap position="float" id="t0015"><label>Table 3</label><caption><p>Performance comparisons amongst different architectures under 5-fold cross-validation on benchmark dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Spearman correlation</th><th>AUROC</th></tr></thead><tbody><tr><td>C-RNNCrispr_std</td><td><bold>0.877</bold><inline-formula><mml:math id="M13" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.062</bold></td><td><bold>0.976</bold><inline-formula><mml:math id="M14" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.002</bold></td></tr><tr><td>C-RNNCrispr_2conv</td><td>0.833<inline-formula><mml:math id="M15" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.000</td><td>0.972<inline-formula><mml:math id="M16" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.003</td></tr><tr><td>C-RNNCrispr_len7</td><td>0.833<inline-formula><mml:math id="M17" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.000</td><td>0.975<inline-formula><mml:math id="M18" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.004</td></tr><tr><td>C-RNNCrispr_avepool2</td><td>0.833<inline-formula><mml:math id="M19" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.000</td><td>0.974<inline-formula><mml:math id="M20" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.005</td></tr><tr><td>C-RNNCrispr_ndrop</td><td>0.833<inline-formula><mml:math id="M21" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.001</td><td>0.969<inline-formula><mml:math id="M22" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.001</td></tr></tbody></table><table-wrap-foot><fn><p>Note: Performance is shown as mean ± standard deviation. This representation also applies to <xref rid="t0020" ref-type="table">Table 4</xref>. The best performance (as measured by each metric) across different architectures is highlighted in bold for clarification. These highlights also apply to <xref rid="t0020" ref-type="table">Table 4</xref>, <xref rid="t0030" ref-type="table">Table 6</xref>, <xref rid="s0110" ref-type="sec">Supplementary Tables 2, 3 and 4</xref>.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="s0070">
      <label>3.2</label>
      <title>Efficacy of CNN and BGRU</title>
      <p id="p0155">Next, we evaluated the effectiveness of CNN and BGRU for our C-RNNCrispr on sgRNA activity prediction. First, we verified the efficacy of the convolution stage by proposing a variant deep architecture (without CNN) getting rid of the convolutional layers and max-pooling layers of sgRNA branch from full C-RNNCrispr model. Then we compared this variant architecture with C-RNNCrispr under 5-fold cross-validation on benchmark dataset. The training data and test data were generated identically to the way described in <xref rid="s0040" ref-type="sec">Section 2.5.1</xref>. As expected, we discovered that removing convolutional layer of sgRNA branch leads to 0.046 and 0.001 decrease on average of Spearman correlation and AUROC values, respectively. Second, we performed experiments on another variant of C-RNNCrispr by removing the BGRU layer in the sgRNA branch (without BGRU). In this case, we observed Spearman correlation and AUROC score became less compared with full C-RNNCrispr, with 0.060 and 0.034 decline on average (<xref rid="t0020" ref-type="table">Table 4</xref>). For the sake of clarity, C-RNNCrispr achieved the highest Spearman correlation and AUROC values amongst these architectures.<table-wrap position="float" id="t0020"><label>Table 4</label><caption><p>Performance comparison among C-RNNCrispr and its two variant architectures (i.e., without CNN and without BGRU) on benchmark dataset under 5-fold cross-validation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Spearman correlation</th><th>AUROC</th></tr></thead><tbody><tr><td>C-RNNCrispr</td><td><bold>0.877</bold><inline-formula><mml:math id="M23" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.062</bold></td><td><bold>0.976</bold><inline-formula><mml:math id="M24" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula><bold>0.002</bold></td></tr><tr><td>without CNN</td><td>0.831<inline-formula><mml:math id="M25" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.001</td><td>0.971<inline-formula><mml:math id="M26" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.002</td></tr><tr><td>without BGRU</td><td>0.817<inline-formula><mml:math id="M27" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.001</td><td>0.942<inline-formula><mml:math id="M28" altimg="si1.svg"><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:math></inline-formula>0.004</td></tr></tbody></table></table-wrap></p>
      <p id="p0160">In view of the above two aspects, convolutional operations were capable of extracting the abstract features of sgRNA and epigenetic sequences. Besides, BGRU stage was indispensable in our architecture for capturing the sequence dependencies of sgRNA. We conclude that combination of CNN and BGRU can boost the power of C-RNNCrispr for sgRNA activity prediction.</p>
    </sec>
    <sec id="s0075">
      <label>3.3</label>
      <title>Effect of transfer learning on small-sample data learning</title>
      <p id="p0165">In this section, we focus on analyzing how the general feature representations from the C-RNNCrispr base network can be transferred and help the small-sample cell lines data learning for sgRNA target efficacy prediction. For this purpose, we compared the performance of the above mentioned transfer learning schemes (the basic training process was introduced in <xref rid="s0045" ref-type="sec">Section 2.5.2</xref>). The experiments were performed under 5-fold cross-validation on the above four small-size cell-line datasets. The training data and testing data for each cell line were constructed identically to the way described in <xref rid="s0055" ref-type="sec">Section 2.7</xref>. As shown in <xref rid="f0010" ref-type="fig">Fig. 2</xref>, it is clear that amongst the transfer learning strategies, fine tune clearly outperformed others. For example, using benchmark data to pre-train C-RNNCrispr, we obtained the Spearman correlation of 0.727, 0.648, 0.672 and 0.630 on HCT116 dataset for fine tune, frozen CNN, frozen BGRU and frozen FC, respectively. <xref rid="s0110" ref-type="sec">Supplementary Table S2</xref> shows the results of AUROC values based on these transfer learning schemes. As can be seen, fine tune strategy performed as well as, or even slightly better than the other transfer learning strategies on datasets HCT116 and HEK293T, with values of 0.937 and 0.976, respectively. Therefore, we applied fine tune strategy on small-size cell line datasets to boost the predictive performance.<fig id="f0010"><label>Fig. 2</label><caption><p>Performance comparison of different transfer learning strategies for sgRNA activity prediction on four cell line-specific training data under 5-fold cross-validation.</p></caption><graphic xlink:href="gr2"/></fig></p>
      <p id="p0170">In order to further verify the advantage of transfer learning (i.e. via fine tune) with benchmark dataset on small-size cell line data, we compared the results between using cell line-specific training data to train from scratch and using fine-tune strategy. Intuitively, there was a significant improvement for each cell line in terms of these two evaluation criteria (<xref rid="f0015" ref-type="fig">Fig. 3</xref>). For example, the standard training method yield Spearman correlation values of 0.285, 0.117, 0.287, 0.338, and 0.443 on HCT116, HEK293T, HELA, HL60 and total datasets, much smaller than 0.727, 0.806, 0.702, 0.624, and 0.682 obtained from fine tune strategy. The superior performance of fine tune for modeling power of C-RNNCrispr is clear.<fig id="f0015"><label>Fig. 3</label><caption><p>Performance comparison of C-RNNCrispr training from scratch and transfer learning via fine tune for each cell line data by 5-fold cross-validation.</p></caption><graphic xlink:href="gr3"/></fig></p>
    </sec>
    <sec id="s0080">
      <label>3.4</label>
      <title>Comparison with current algorithms</title>
      <p id="p0175">We next compared our C-RNNCrispr with current algorithms using sgRNA and epigenetic data. Prior to this, we briefly comment on some comparisons of C-RNNCrispr and other existing deep learning-based methods for sgRNA on-target activity prediction (see <xref rid="t0025" ref-type="table">Table 5</xref>). First, both Seq_deepCpf1 and DeepCas9 performed based on 1D convolution model (1D CNN) considering only sgRNA sequence composition. DeepCRISPR and C-RNNCrispr performed by incorporating both sgRNA sequence and epigenetic data. Second, DeepCRISPR used two dimensional CNN (2D CNN) while C-RNNCrispr used hybrid 1D CNN and BGRU. Third, besides DeepCas9, all methods used transfer learning technique. Thus, we only compared with DeepCRISPR among these methods when considering both sgRNA and epigenetic data. Moreover, we also compared C-RNNCrispr with two current machine learning-based tools, including CRISPR Designer and sgRNA Scorer.<table-wrap position="float" id="t0025"><label>Table 5</label><caption><p>Existing deep learning-based methods for sgRNA on-target activity prediction.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Model</th><th>Sequence</th><th>Training mode</th><th>Reference</th></tr></thead><tbody><tr><td>Seq_deepCpf1</td><td>1D CNN</td><td>sgRNA</td><td>Transfer learning</td><td><xref rid="b0080" ref-type="bibr">[16]</xref></td></tr><tr><td>DeepCRISPR</td><td>2D CNN</td><td>sgRNA + Epi</td><td>Transfer learning</td><td><xref rid="b0025" ref-type="bibr">[5]</xref></td></tr><tr><td>DeepCas9</td><td>1D CNN</td><td>sgRNA</td><td>From scratch</td><td><xref rid="b0085" ref-type="bibr">[17]</xref></td></tr><tr><td>C-RNNCrispr</td><td>1D CNN-BGRU</td><td>sgRNA + Epi</td><td>Transfer learning</td><td>–</td></tr></tbody></table><table-wrap-foot><fn><p>Note: sgRNA + Epi, sequence features and epigenetic features.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0180">The above four cell line datasets were applied under 5-fold cross-validation for performance evaluation. The training data and testing data for each cell line were built in the same way as described in <xref rid="s0040" ref-type="sec">Section 2.5.1</xref>. <xref rid="f0020" ref-type="fig">Fig. 4</xref>a shows the Spearman correlations of compared methods and C-RNNCrispr. In general, deep learning-based model outperformed the machine learning-based tools. To be specific, our C-RNNCrispr gained superiority to other methods in three cell lines and total dataset, whereas for dataset HEK293T, it performed lower than DeepCRISPR. As depicted in <xref rid="f0020" ref-type="fig">Fig. 4</xref>b, it is clear that C-RNNCrispr outperformed other methods in terms of AUROC scores. Detailed Spearman correlation and AUROC scores for individual datasets are provided in <xref rid="s0110" ref-type="sec">Supplementary Table S3</xref>. We conclude that C-RNNCrispr is competitive against other existing methods.<fig id="f0020"><label>Fig. 4</label><caption><p>Performance comparison of C-RNNCrispr and other learning-based prediction models on different testing cell line data under 5-fold cross-validation.</p></caption><graphic xlink:href="gr4"/></fig></p>
      <p id="p0185">To further evaluate the generalization capability of the proposed method, we trained C-RNNCrispr using leave-one-cell-line-out procedure and made comparisons with the above mentioned methods. The training and testing data for each cell line were constructed followed the procedure illustrated in <xref rid="s0040" ref-type="sec">Section 2.5.1</xref>. In the training stage, for a cell line of interest to be predicted, we used the training data from other three cell lines. In the testing stage, we evaluated the model on the test data of the given cell line of interest. Taking leave-HCT116-out as an example, we trained the model by incorporating training data from HEK293T, HELA, and HL60 cell lines (lacking training data from HCT116), and evaluated the model on HCT116 test data. Intuitively, we observed that our C-RNNCrispr achieve the best performance (<xref rid="f0025" ref-type="fig">Fig. 5</xref>). Specifically, compared with the next-best DeepCRISPR, C-RNNCrispr showed the superior performance in all cell line datasets except for HCT116. C-RNNCrispr achieved mean Spearman correlation of 0.692, which was 0.286 higher than DeepCRISPR. In addition, C-RNNCrispr clearly outperformed other models in all cell line datasets in terms of AUROC. For more details, see <xref rid="s0110" ref-type="sec">Supplementary Table S4</xref>. These observations suggest that C-RNNCrispr gained the superiority of generalizability for sgRNA activity prediction.<fig id="f0025"><label>Fig. 5</label><caption><p>Performance comparison of C-RNNCrispr and other learning-based prediction models on different testing cell line data with a leave-one-cell-out procedure.</p></caption><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="s0085">
      <label>3.5</label>
      <title>Case studies</title>
      <p id="p0190">From the above investigation, we have observed that our C-RNNCrispr showed satisfactory performance for sgRNA activity prediction. Previous machine learning-based methods demonstrated that sgRNA sequence composition is critical for the cleavage efficiency <xref rid="b0225" ref-type="bibr">[45]</xref>, <xref rid="b0230" ref-type="bibr">[46]</xref>. The first case study was conducted to investigate whether C-RNNCrispr can effectively predict sgRNA activity using sgRNA sequence composition features. For this purpose, we evaluated the prediction ability of C-RNNCrispr by incorporating only sgRNA sequence features. We only used the sgRNA branch to evaluate the sgRNA activity. We compared C-RNNCrispr with three deep learning-based models (i.e., DeepCRISPR, Seq_deepCpf1 and DeepCas9) for predicting sgRNA activity on four cell line datasets. Notably, the inputs of Seq_deepCpf1 and DeepCas9 only include the sgRNA sequence. For fair comparison, we applied DeepCRISPR using sgRNA sequence only. In addition, Seq_deepCpf1, DeepCRISPR and C-RNNCrispr applied fine-tuning of the benchmark dataset pre-trained model on cell line of interest dataset (see <xref rid="t0025" ref-type="table">Table 5</xref>). More concretely, Seq_deepCpf1 only updated the weights parameters in the last two layers of a benchmark dataset pre-trained CNN model on small sample cell line of interest datasets. Analogously, DeepCRISPR used the encoder part of the DCDNN-based model as the pre-trained model. C-RNNCrispr applied the fine-tune strategy (Section 2.5.3) for improving the performance. DeepCas9 trained the model from scratch. To make a fair comparison, we retrained it by applying transfer learning, namely DeepCas9 + transfer learning. Specifically, all layers except the last two fully connected layers were fine tune on small sample cell line of interest. The training data and test data for each cell line were generated in the same way as described in <xref rid="s0080" ref-type="sec">Section 3.4</xref>.</p>
      <p id="p0195">We note that our C-RNNCrispr consistently outperforms the other methods in terms of Spearman correlation (see <xref rid="t0030" ref-type="table">Table 6</xref>). On average, C-RNNCrispr shows a Spearman correlation value of 0.663, with 0.026 higher than the second best Seq_deepCpf1. Besides, C-RNNCrispr also presents better performance in terms of AUROC than other models except for dataset HCT116, convincing us that C-RNNCrispr is more powerful for sgRNA activity prediction. We also note that, with the help of transfer learning, DeepCas9 plus fine tune surpasses DeepCas9 training from scratch. This observation is in accordance with the conclusion of <xref rid="s0075" ref-type="sec">Section 3.3</xref>. Taken together, our C-RNNCrispr can effectively predict sgRNA activity using sequence composition information only.<table-wrap position="float" id="t0030"><label>Table 6</label><caption><p>Performance comparisons amongst five deep learning models based on target sequence composition on four cell line datasets under 5-fold cross-validation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>HCT116</th><th>HEK293T</th><th>HELA</th><th>HL60</th><th>Average</th></tr></thead><tbody><tr><td colspan="6">(a) Spearman correlation</td></tr><tr><td>C-RNNCrispr</td><td><bold>0.724</bold></td><td><bold>0.665</bold></td><td><bold>0.685</bold></td><td><bold>0.577</bold></td><td><bold>0.663</bold></td></tr><tr><td>Seq_deepCpf1</td><td>0.672</td><td><bold>0.665</bold></td><td>0.651</td><td>0.558</td><td>0.637</td></tr><tr><td>DeepCRISPR</td><td>0.650</td><td>0.035</td><td>0.510</td><td>0.200</td><td>0.349</td></tr><tr><td>DeepCas9</td><td>0.603</td><td>−0.116</td><td>0.418</td><td>0.118</td><td>0.256</td></tr><tr><td>DeepCas9 + TF</td><td>0.683</td><td>0.572</td><td>0.675</td><td>0.495</td><td>0.606</td></tr><tr><td colspan="6">  </td></tr><tr><td colspan="6">(b) AUROC</td></tr><tr><td>C-RNNCrispr</td><td>0.934</td><td><bold>0.978</bold></td><td><bold>0.925</bold></td><td><bold>0.936</bold></td><td><bold>0.943</bold></td></tr><tr><td>Seq_deepCpf1</td><td><bold>0.939</bold></td><td><bold>0.978</bold></td><td>0.921</td><td>0.928</td><td>0.942</td></tr><tr><td>DeepCRISPR</td><td>0.887</td><td>0.474</td><td>0.788</td><td>0.584</td><td>0.683</td></tr><tr><td>DeepCas9</td><td>0.784</td><td>0.470</td><td>0.677</td><td>0.535</td><td>0.617</td></tr><tr><td>DeepCas9 + TF</td><td>0.902</td><td>0.905</td><td>0.902</td><td>0.887</td><td>0.899</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The top table records Spearman correlation values while the bottom one records AUROC values. DeepCas9 + TF: DeepCas9 + transfer learning. The performance of DeepCRISPR is take from <xref rid="b0025" ref-type="bibr">[5]</xref>.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0200">The second case study was conducted to reveal the biological insights into the sgRNA on-target activity prediction. Using the method in a previous study <xref rid="b0235" ref-type="bibr">[47]</xref>, we developed a heuristic to interpret our C-RNNCrispr network by visualizing the importance of all possible nucleotides and their corresponding epigenetic features at different locations. Briefly, we generated special sequences denoting the presence of the nucleotide and epigenetic features at a specific position and respectively fed them into the sgRNA branch and epigenetic branch of the well trained C-RNNCrispr model, subsequently took the outputs for visualization. For complete details see <xref rid="s0110" ref-type="sec">Supplementary Note</xref>. <xref rid="f0030" ref-type="fig">Fig. 6</xref> shows the importance of all four nucleotides and epigenetic features at different positions. This allowed us to reveal general patterns of CRISPR-mediated DNA editing and make a number of observations. The positions adjacent to the PAM are more crucial than the PAM-distal region for sgRNA activity prediction. This is consistent with previous observations that perfect base-pairing with 10–12 bp immediately upstream the PAM (PAM-proximal) determines Cas9 specificity, whereas multiple PAM-distal mismatches can be tolerated <xref rid="b0240" ref-type="bibr">[48]</xref>. In agreement with previous studies that presence of an A or T nucleotide at position 20 (1 bp adjacent to PAM) increased the proportion of indel <xref rid="b0245" ref-type="bibr">[49]</xref>, we found the presence of A or T is favored at this position. We also noted that position 17 (immediately 5′ of the cleavage site) is the most important. The presence of a C nucleotide is informative at this position since the cut site usually resides 3 bp upstream the PAM. The homopolymers (a run of two or more identical nucleotides) are found to be favored at position 17–19, which coincides with a previous finding that the presence of homopolymers adjacent to the cut site increased the proportion of deletions <xref rid="b0245" ref-type="bibr">[49]</xref>. Most of the top epigenetic features were obtained by convolving the middle region of the input matrix. Opening-chromatin information of Dnase is found to be favored at 3 bp upstream of the PAM, which is in accord with a previous study that considering the target site accessibility can boost the predictive performance of gRNA activity <xref rid="b0080" ref-type="bibr">[16]</xref>. It has a general preference for Dnase while relative avoidance of DNA methylation (H3K4me3) for high sgRNA efficiency. The same observation was also obtained by Chuai et al. <xref rid="b0025" ref-type="bibr">[5]</xref>.<fig id="f0030"><label>Fig. 6</label><caption><p>Visualization of the importance of different nucleotides and epigenetic features at different positions for our C-RNNCrispr. The colors represent the contribution of the position-specific nucleotide and epigenetic features to determining an efficient sgRNA. The nucleotides and epigenetic features are arranged vertically, whereas the positions of the sequence are placed horizontally.</p></caption><graphic xlink:href="gr6"/></fig></p>
    </sec>
  </sec>
  <sec id="s0090">
    <label>4</label>
    <title>Discussion</title>
    <p id="p0205">In this study, we introduced C-RNNCrispr, a hybrid CNN-BGRU based model for CRISPR/Cas9 sgRNA activity prediction. An intuition of using a combination of CNN and BGRU is to use CNN for feature extraction and apply BGRU for modeling sequential dependencies of sgRNA features. Experimental results on publicly available datasets show C-RNNCrispr can adaptively learn sequence characteristics of sgRNA and epigenetic features; thereby avoiding manual feature extraction.</p>
    <p id="p0210">Numerous studies have demonstrated the synergistic improvements of unified CNN-RNN models in virtue of the complementary in their modeling capability, such as DeeperBind <xref rid="b0115" ref-type="bibr">[23]</xref> and DanQ <xref rid="b0120" ref-type="bibr">[24]</xref>. However, there is no consensus on the relative superiority between CNNs and RNNs for sequential data. Though RNNs are popular for natural language data with strong long-range dependencies, there are some studies reported that CNNs perform as well as RNNs <xref rid="b0250" ref-type="bibr">[50]</xref>, <xref rid="b0255" ref-type="bibr">[51]</xref>. For instance, Zhuang et al. proposed a simple CNN for predicting enhancer-promoter interactions with DNA sequence data, which performs equally well with hybrid CNN-RNN model <xref rid="b0260" ref-type="bibr">[52]</xref>. They used only moderately large sample sizes of the training data. It is perhaps that there is no strong long-range dependency in DNA sequence data in their study, possibly too subtle to be detected. If so, it would suggest unnecessary to use RNNs.</p>
    <p id="p0215">Previous studies in computer vision have demonstrated that CNN transfer learning from ImageNet to other datasets of limited scales contributed to better performing deep models <xref rid="b0215" ref-type="bibr">[43]</xref>, <xref rid="b0265" ref-type="bibr">[53]</xref>. We have explored four transfer learning strategies (<xref rid="s0075" ref-type="sec">Section 3.3</xref>) to fine-tune the benchmark dataset pre-trained C-RNNCrispr model on small sample cell line datasets. Each model gained clearly better predictive results than the naive way. This result is expected given that there may be commonalities among cell line-specific sequence features. Therefore, fine tune is ideal for borrowing information from other cell lines for the task of predicting sgRNA activity when training data was limited.</p>
    <p id="p0220">Compared with several state-of-the-art learning-based tools, we found that our C-RNNCrispr coupled with a fine tune strategy integrating data from benchmark dataset would perform competitively against other methods. Moreover, we noticed that even given the sgRNA sequence only, C-RNNCrispr still surpassed amongst the compared deep learning-based models. This result implies that C-RNNCrispr can represent and capture nontrivial patterns or relationship between sequence information of sgRNA sequences, it is due to the proposed CNN-BGRU based model combines the advantages of CNN for capturing local patterns of sequences and BGRU for modeling sequential dependencies. Note that, the architectures that combine CNN with BGRU indeed provide performance over the CNN model. However, the improvement in accuracy comes at the expense of the increased computational cost. We ran our experiments on an Ubuntu server with two NVIDIA GeForce GTX 1080 Ti GPUs with 11 GB of memory per GPU. Typical running time of each experiment for model training was 1 h for the variant without BGRU (see <xref rid="s0070" ref-type="sec">Section 3.2</xref>), 9.8 h for the variant without CNN and almost 5.5 h for the C-RNNCrispr network including CNN and GBRU modules (see details in <xref rid="s0110" ref-type="sec">Supplementary Table S5</xref>). Because BGRU are expensive for processing long sequence, but 1D converts are cheap, it can be a good idea to use 1D convert as a preprocessing step before a BGRU, shortening the sequence and extracting useful representations for the BGRU to process. Considering that sgRNA sequence is more important than its corresponding epigenetic features, thus, we used no BGRU for the epigenetic branch to reduce the computational cost.</p>
    <p id="p0225">Although C-RNNCrispr has improved the performance for sgRNA activity prediction and become an advantageous approach, there are still several avenues of interest to investigate. Our future work will focus on three areas. One area is about exploring other deep learning-based frameworks and exploiting methods for optimal hyperparameters selection, which may yield better performance. Notably, there are certain characteristic differences between biological sequence and image data of computer vision, the technical details of optimizing parameters determination may differ. The second area is about expanding the feature space. Currently, we only use sgRNA sequence data and four epigenetic features including CTCF binding, H3K4me3, chromatin accessibility as well as DNA methylation. Other informative features such as cutting positions, physicochemical property and RNA fold score can be exploited to boost the predictive power. The third area is sgRNA off-target site prediction. The limitation of the current study is that C-RNNCrispr can only be used for sgRNA on-target activity prediction. Note that, CRISPR can tolerate mismatches in sgRNA-DNA at various positions in a sequence-dependent manner, leading to off-target mutations <xref rid="b0270" ref-type="bibr">[54]</xref>, <xref rid="b0275" ref-type="bibr">[55]</xref>. Therefore, this issue should be critically resolved and completely avoided when applying CRISPR/Cas9 gene editing to clinical applications. Various tools have been proposed to predict off-target score by considering the positions of the mismatches to the guide sequence (MIT score <xref rid="b0270" ref-type="bibr">[54]</xref> and CFD score <xref rid="b0045" ref-type="bibr">[9]</xref>, etc.). Subsequently, two machine learning-based methods Elevation <xref rid="b0280" ref-type="bibr">[56]</xref> and CRISTA <xref rid="b0285" ref-type="bibr">[57]</xref> expand the feature set, including features such as sgRNA secondary structure, genomic location for off-target prediction. Zhang et al. proposed an ensemble learning framework to predict the off-target activities. It found ensemble learning using AdaBoost outperformed other individual off-target predictive tools and adding PhyloP can enhance the predictive capabilities <xref rid="b0290" ref-type="bibr">[58]</xref>. It was not until 2018 that deep learning-based methods have been integrated for CRISPR off-target prediction, such as DeepCRISPR and CNN_std <xref rid="b0295" ref-type="bibr">[59]</xref>. The above two CNN-based models showed good performance in off-target activity prediction. To learn more about the computational methods used to facilitate the process of CRISPR/Cas9 sgRNA off-target activity prediction, we refer readers to <xref rid="b0060" ref-type="bibr">[12]</xref>, <xref rid="b0300" ref-type="bibr">[60]</xref>, <xref rid="b0305" ref-type="bibr">[61]</xref> for details. Integrating the sgRNA off-target site prediction and our on-target activity prediction is worth of generating for providing more comprehensive guidance for optimal sgRNAs selection. These are interesting topics deserve to be explored in the future.</p>
  </sec>
  <sec id="s0095">
    <label>5</label>
    <title>Conclusion</title>
    <p id="p0230">In this study, we present C-RNNCrispr, a unified CNN-BGRU architecture for CRISPR/Cas9 sgRNA on-target activity prediction. We applied a CNN to automatically learn the abstract features of sgRNA and four epigenetic features (i.e., CTCF binding, H3K4me3, chromatin accessibility and DNA methylation). On the other hand, we used BGRU to model the sequential dependencies of sgRNA features. Compared with three deep learning based models (i.e., Seq_deepCpf1, DeepCRISPR and DeepCas9) and two machine learning-based models (e.g. sgRNA Designer and sgRNA Scorer), C-RNNCrispr can effectively learn the features of sgRNA sequence and epigenetic features. We also introduced a transfer learning strategy to boost the predictive power of C-RNNCrispr in dealing with small-size datasets. Experimental results on the published datasets indicated that the effectiveness of our C-RNNCrispr for CRISPR/Cas9 sgRNA cleavage efficacy prediction.</p>
  </sec>
  <sec id="s0100">
    <title>Conflicts of interest</title>
    <p id="p0235">None declared.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <label>1</label>
      <element-citation publication-type="journal" id="h0005">
        <person-group person-group-type="author">
          <name>
            <surname>Jinek</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Chylinski</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fonfara</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Hauer</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Doudna</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Charpentier</surname>
            <given-names>E.</given-names>
          </name>
        </person-group>
        <article-title>A programmable dual-RNA-guided DNA endonuclease in adaptive bacterial immunity</article-title>
        <source>Science</source>
        <volume>337</volume>
        <year>2012</year>
        <fpage>816</fpage>
        <lpage>821</lpage>
        <pub-id pub-id-type="pmid">22745249</pub-id>
      </element-citation>
    </ref>
    <ref id="b0010">
      <label>2</label>
      <element-citation publication-type="journal" id="h0010">
        <person-group person-group-type="author">
          <name>
            <surname>Mali</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Esvelt</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>Aach</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Guell</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>DiCarlo</surname>
            <given-names>J.E.</given-names>
          </name>
        </person-group>
        <article-title>RNA-guided human genome engineering via Cas9</article-title>
        <source>Science</source>
        <volume>339</volume>
        <year>2013</year>
        <fpage>823</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="pmid">23287722</pub-id>
      </element-citation>
    </ref>
    <ref id="b0015">
      <label>3</label>
      <element-citation publication-type="journal" id="h0015">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Guan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Improved sgRNA design in bacteria via genome-wide activity profiling</article-title>
        <source>Nucleic Acids Res</source>
        <volume>46</volume>
        <year>2018</year>
        <fpage>7052</fpage>
        <lpage>7069</lpage>
        <pub-id pub-id-type="pmid">29982721</pub-id>
      </element-citation>
    </ref>
    <ref id="b0020">
      <label>4</label>
      <element-citation publication-type="journal" id="h0020">
        <person-group person-group-type="author">
          <name>
            <surname>Moreno-Mateos</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Vejnar</surname>
            <given-names>C.E.</given-names>
          </name>
          <name>
            <surname>Beaudoin</surname>
            <given-names>J.D.</given-names>
          </name>
          <name>
            <surname>Fernandez</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Mis</surname>
            <given-names>E.K.</given-names>
          </name>
          <name>
            <surname>Khokha</surname>
            <given-names>M.K.</given-names>
          </name>
        </person-group>
        <article-title>CRISPRscan: designing highly efficient sgRNAs for CRISPR-Cas9 targeting in vivo</article-title>
        <source>Nat Methods</source>
        <volume>12</volume>
        <year>2015</year>
        <fpage>982</fpage>
        <lpage>988</lpage>
        <pub-id pub-id-type="pmid">26322839</pub-id>
      </element-citation>
    </ref>
    <ref id="b0025">
      <label>5</label>
      <element-citation publication-type="journal" id="h0025">
        <person-group person-group-type="author">
          <name>
            <surname>Chuai</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Xue</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>DeepCRISPR: optimized CRISPR guide RNA design by deep learning</article-title>
        <source>Genome Biol</source>
        <volume>19</volume>
        <year>2018</year>
        <fpage>80</fpage>
        <pub-id pub-id-type="pmid">29945655</pub-id>
      </element-citation>
    </ref>
    <ref id="b0030">
      <label>6</label>
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chuai</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Benchmarking CRISPR on-target sgRNA design</article-title>
        <source>Brief Bioinform</source>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="b0035">
      <label>7</label>
      <element-citation publication-type="journal" id="h0035">
        <person-group person-group-type="author">
          <name>
            <surname>Naito</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hino</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Bono</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Ui-Tei</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>CRISPRdirect: software for designing CRISPR/Cas guide RNA with reduced off-target sites</article-title>
        <source>Bioinformatics</source>
        <volume>31</volume>
        <year>2015</year>
        <fpage>1120</fpage>
        <lpage>1123</lpage>
        <pub-id pub-id-type="pmid">25414360</pub-id>
      </element-citation>
    </ref>
    <ref id="b0040">
      <label>8</label>
      <element-citation publication-type="journal" id="h0040">
        <person-group person-group-type="author">
          <name>
            <surname>Heigwer</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kerr</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Boutros</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>E-CRISP: fast CRISPR target site identification</article-title>
        <source>Nat Methods</source>
        <volume>11</volume>
        <year>2014</year>
        <fpage>122</fpage>
        <pub-id pub-id-type="pmid">24481216</pub-id>
      </element-citation>
    </ref>
    <ref id="b0045">
      <label>9</label>
      <element-citation publication-type="journal" id="h0045">
        <person-group person-group-type="author">
          <name>
            <surname>Doench</surname>
            <given-names>J.G.</given-names>
          </name>
          <name>
            <surname>Fusi</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Sullender</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hegde</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Vaimberg</surname>
            <given-names>E.W.</given-names>
          </name>
          <name>
            <surname>Donovan</surname>
            <given-names>K.F.</given-names>
          </name>
        </person-group>
        <article-title>Optimized sgRNA design to maximize activity and minimize off-target effects of CRISPR-Cas9</article-title>
        <source>Nat Biotechnol</source>
        <volume>34</volume>
        <year>2016</year>
        <fpage>184</fpage>
        <pub-id pub-id-type="pmid">26780180</pub-id>
      </element-citation>
    </ref>
    <ref id="b0050">
      <label>10</label>
      <element-citation publication-type="journal" id="h0050">
        <person-group person-group-type="author">
          <name>
            <surname>Chari</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Mali</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Moosburner</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Church</surname>
            <given-names>G.M.</given-names>
          </name>
        </person-group>
        <article-title>Unraveling CRISPR-Cas9 genome engineering parameters via a library-on-library approach</article-title>
        <source>Nat Methods</source>
        <volume>12</volume>
        <year>2015</year>
        <fpage>823</fpage>
        <lpage>826</lpage>
        <pub-id pub-id-type="pmid">26167643</pub-id>
      </element-citation>
    </ref>
    <ref id="b0055">
      <label>11</label>
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Hinz</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Laughery</surname>
            <given-names>M.F.</given-names>
          </name>
          <name>
            <surname>Wyrick</surname>
            <given-names>J.J.</given-names>
          </name>
        </person-group>
        <article-title>Nucleosomes inhibit Cas9 endonuclease activity in vitro</article-title>
        <source>Biochemistry</source>
        <volume>54</volume>
        <year>2015</year>
        <fpage>7063</fpage>
        <lpage>7066</lpage>
        <pub-id pub-id-type="pmid">26579937</pub-id>
      </element-citation>
    </ref>
    <ref id="b0060">
      <label>12</label>
      <element-citation publication-type="journal" id="h0060">
        <person-group person-group-type="author">
          <name>
            <surname>Wilson</surname>
            <given-names>L.O.W.</given-names>
          </name>
          <name>
            <surname>O'Brien</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>D.C.</given-names>
          </name>
        </person-group>
        <article-title>The current state and future of CRISPR-Cas9 gRNA design tools</article-title>
        <source>Front Pharmacol</source>
        <volume>9</volume>
        <year>2018</year>
        <fpage>749</fpage>
        <pub-id pub-id-type="pmid">30050439</pub-id>
      </element-citation>
    </ref>
    <ref id="b0065">
      <label>13</label>
      <element-citation publication-type="journal" id="h0065">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <volume>521</volume>
        <year>2015</year>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="b0070">
      <label>14</label>
      <mixed-citation publication-type="other" id="h0070">Zamir AR, Sax A, Shen W, Guibas LJ, Malik J, Savarese S, editors. Taskonomy: disentangling task transfer learning. Proceedings of the IEEE conference on computer vision and pattern recognition; 2018.</mixed-citation>
    </ref>
    <ref id="b0075">
      <label>15</label>
      <mixed-citation publication-type="other" id="h0075">Lopez MM, Kalita J. Deep learning applied to NLP. arXiv preprint arXiv:170303091 2017.</mixed-citation>
    </ref>
    <ref id="b0080">
      <label>16</label>
      <element-citation publication-type="journal" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>H.K.</given-names>
          </name>
          <name>
            <surname>Min</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Jung</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>J.W.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning improves prediction of CRISPR-Cpf1 guide RNA activity</article-title>
        <source>Nat Biotechnol</source>
        <volume>36</volume>
        <year>2018</year>
        <fpage>239</fpage>
        <lpage>241</lpage>
        <pub-id pub-id-type="pmid">29431740</pub-id>
      </element-citation>
    </ref>
    <ref id="b0085">
      <label>17</label>
      <element-citation publication-type="journal" id="h0085">
        <person-group person-group-type="author">
          <name>
            <surname>Xue</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of CRISPR sgRNA activity using a deep convolutional neural network</article-title>
        <source>J Chem Inf Model</source>
        <year>2018</year>
      </element-citation>
    </ref>
    <ref id="b0090">
      <label>18</label>
      <element-citation publication-type="journal" id="h0090">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Huss</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Abid</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mohammadi</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Torkamani</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Telenti</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>A primer on deep learning in genomics</article-title>
        <source>Nat Genet</source>
        <volume>51</volume>
        <year>2019</year>
        <fpage>12</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="pmid">30478442</pub-id>
      </element-citation>
    </ref>
    <ref id="b0095">
      <label>19</label>
      <mixed-citation publication-type="other" id="h0095">Sundermeyer M, Schlüter R, Ney H, editors. LSTM neural networks for language modeling. Thirteenth annual conference of the international speech communication association; 2012.</mixed-citation>
    </ref>
    <ref id="b0100">
      <label>20</label>
      <mixed-citation publication-type="other" id="h0100">Chung J, Gulcehre C, Cho K, Bengio Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:14123555 2014.</mixed-citation>
    </ref>
    <ref id="b0105">
      <label>21</label>
      <element-citation publication-type="book" id="h0105">
        <person-group person-group-type="author">
          <name>
            <surname>Goodfellow</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <chapter-title>Deep learning</chapter-title>
        <year>2016</year>
        <publisher-name>MIT press</publisher-name>
      </element-citation>
    </ref>
    <ref id="b0110">
      <label>22</label>
      <mixed-citation publication-type="other" id="h0110">Singh S, Yang Y, Poczos B, Ma J. Predicting enhancer-promoter interaction from genomic sequence with deep neural networks. bioRxiv 2018;085241.</mixed-citation>
    </ref>
    <ref id="b0115">
      <label>23</label>
      <mixed-citation publication-type="other" id="h0115">Hassanzadeh HR, Wang MD, editors. DeeperBind: enhancing prediction of sequence specificities of DNA binding proteins. IEEE international conference on bioinformatics &amp; biomedicine; 2017.</mixed-citation>
    </ref>
    <ref id="b0120">
      <label>24</label>
      <element-citation publication-type="journal" id="h0120">
        <person-group person-group-type="author">
          <name>
            <surname>Quang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences</article-title>
        <source>Nucleic Acids Res</source>
        <volume>44</volume>
        <year>2016</year>
        <fpage>e107-e</fpage>
        <pub-id pub-id-type="pmid">27084946</pub-id>
      </element-citation>
    </ref>
    <ref id="b0125">
      <label>25</label>
      <element-citation publication-type="journal" id="h0125">
        <person-group person-group-type="author">
          <name>
            <surname>Pan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Rijnbeek</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>H.B.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of RNA-protein sequence and structure binding preferences using deep convolutional and recurrent neural networks</article-title>
        <source>BMC Genomics</source>
        <volume>19</volume>
        <year>2018</year>
        <fpage>511</fpage>
        <pub-id pub-id-type="pmid">29970003</pub-id>
      </element-citation>
    </ref>
    <ref id="b0130">
      <label>26</label>
      <element-citation publication-type="journal" id="h0130">
        <person-group person-group-type="author">
          <name>
            <surname>Hart</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Chandrashekhar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Aregger</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Steinhart</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>K.R.</given-names>
          </name>
          <name>
            <surname>MacLeod</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>High-resolution CRISPR screens reveal fitness genes and genotype-specific cancer liabilities</article-title>
        <source>Cell</source>
        <volume>163</volume>
        <year>2015</year>
        <fpage>1515</fpage>
        <lpage>1526</lpage>
        <pub-id pub-id-type="pmid">26627737</pub-id>
      </element-citation>
    </ref>
    <ref id="b0135">
      <label>27</label>
      <element-citation publication-type="journal" id="h0135">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Sabatini</surname>
            <given-names>D.M.</given-names>
          </name>
          <name>
            <surname>Lander</surname>
            <given-names>E.S.</given-names>
          </name>
        </person-group>
        <article-title>Genetic screens in human cells using the CRISPR-Cas9 system</article-title>
        <source>Science</source>
        <volume>343</volume>
        <year>2014</year>
        <fpage>80</fpage>
        <lpage>84</lpage>
        <pub-id pub-id-type="pmid">24336569</pub-id>
      </element-citation>
    </ref>
    <ref id="b0140">
      <label>28</label>
      <element-citation publication-type="journal" id="h0140">
        <person-group person-group-type="author">
          <name>
            <surname>Consortium EP</surname>
          </name>
        </person-group>
        <article-title>The ENCODE (ENCyclopedia of DNA elements) project</article-title>
        <source>Science</source>
        <volume>306</volume>
        <year>2004</year>
        <fpage>636</fpage>
        <lpage>640</lpage>
        <pub-id pub-id-type="pmid">15499007</pub-id>
      </element-citation>
    </ref>
    <ref id="b0145">
      <label>29</label>
      <mixed-citation publication-type="other" id="h0145">Badaro G, Hajj H, El-Hajj W, Nachman L, editors. A hybrid approach with collaborative filtering for recommender systems. 2013 9th International wireless communications and mobile computing conference (IWCMC); 2013 1–5 July 2013.</mixed-citation>
    </ref>
    <ref id="b0150">
      <label>30</label>
      <element-citation publication-type="journal" id="h0150">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sutskever</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Distributed representations of words and phrases and their compositionality</article-title>
        <source>Adv Neural Inf Process Systems</source>
        <volume>26</volume>
        <year>2013</year>
        <fpage>3111</fpage>
        <lpage>3119</lpage>
      </element-citation>
    </ref>
    <ref id="b0155">
      <label>31</label>
      <element-citation publication-type="journal" id="h0155">
        <person-group person-group-type="author">
          <name>
            <surname>Trabelsi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chaabane</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ben-Hur</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive evaluation of deep learning architectures for prediction of DNA/RNA sequence binding specificities</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <year>2019</year>
        <fpage>i269</fpage>
        <lpage>i277</lpage>
        <pub-id pub-id-type="pmid">31510640</pub-id>
      </element-citation>
    </ref>
    <ref id="b0160">
      <label>32</label>
      <element-citation publication-type="journal" id="h0160">
        <person-group person-group-type="author">
          <name>
            <surname>Cun</surname>
            <given-names>Y.L.</given-names>
          </name>
          <name>
            <surname>Boser</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Denker</surname>
            <given-names>J.S.</given-names>
          </name>
          <name>
            <surname>Howard</surname>
            <given-names>R.E.</given-names>
          </name>
          <name>
            <surname>Habbard</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Jackel</surname>
            <given-names>L.D.</given-names>
          </name>
        </person-group>
        <article-title>Handwritten digit recognition with a back-propagation network</article-title>
        <source>Adv Neural Inf Process Systems</source>
        <volume>2</volume>
        <year>1990</year>
        <fpage>396</fpage>
        <lpage>404</lpage>
      </element-citation>
    </ref>
    <ref id="b0165">
      <label>33</label>
      <mixed-citation publication-type="other" id="h0165">Graves A, Mohamed A-r, Hinton G, editors. Speech recognition with deep recurrent neural networks. 2013 IEEE international conference on acoustics, speech and signal processing; 2013: IEEE.</mixed-citation>
    </ref>
    <ref id="b0170">
      <label>34</label>
      <element-citation publication-type="journal" id="h0170">
        <person-group person-group-type="author">
          <name>
            <surname>Hirschberg</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Manning</surname>
            <given-names>C.D.</given-names>
          </name>
        </person-group>
        <article-title>Advances in natural language processing</article-title>
        <source>Science</source>
        <volume>349</volume>
        <year>2015</year>
        <fpage>261</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="pmid">26185244</pub-id>
      </element-citation>
    </ref>
    <ref id="b0175">
      <label>35</label>
      <element-citation publication-type="journal" id="h0175">
        <person-group person-group-type="author">
          <name>
            <surname>You</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>C.K.</given-names>
          </name>
        </person-group>
        <article-title>Relative CNN-RNN: learning relative atmospheric visibility from images</article-title>
        <source>IEEE Trans Image Process</source>
        <volume>28</volume>
        <year>2019</year>
        <fpage>45</fpage>
        <lpage>55</lpage>
        <pub-id pub-id-type="pmid">30028702</pub-id>
      </element-citation>
    </ref>
    <ref id="b0180">
      <label>36</label>
      <element-citation publication-type="journal" id="h0180">
        <person-group person-group-type="author">
          <name>
            <surname>Liang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Hong</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Combining convolutional neural network with recursive neural network for blood cell image classification</article-title>
        <source>IEEE Access</source>
        <volume>6</volume>
        <year>2018</year>
        <fpage>36188</fpage>
        <lpage>36197</lpage>
      </element-citation>
    </ref>
    <ref id="b0185">
      <label>37</label>
      <mixed-citation publication-type="other" id="h0185">Krizhevsky A, Sutskever I, Hinton GE, editors. ImageNet classification with deep convolutional neural networks. International conference on neural information processing systems; 2012.</mixed-citation>
    </ref>
    <ref id="b0190">
      <label>38</label>
      <mixed-citation publication-type="other" id="h0190">Tieleman T, Hinton G. Lecture 6.5-rmsprop: divide the gradient by a running average of its recent magnitude. COURSERA: neural networks for machine learning 2012;4:26-31.</mixed-citation>
    </ref>
    <ref id="b0195">
      <label>39</label>
      <mixed-citation publication-type="other" id="h0195">J. Snoek H. Larochelle R.P. Adams editors. Practical Bayesian optimization of machine learning algorithms international conference on neural information processing systems 2012</mixed-citation>
    </ref>
    <ref id="b0200">
      <label>40</label>
      <element-citation publication-type="journal" id="h0200">
        <person-group person-group-type="author">
          <name>
            <surname>Bergstra</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Komer</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Eliasmith</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Dan</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>D.D.</given-names>
          </name>
        </person-group>
        <article-title>Hyperopt: a Python library for model selection and hyperparameter optimization</article-title>
        <source>Comput Sci Discov</source>
        <volume>8</volume>
        <year>2015</year>
        <object-id pub-id-type="publisher-id">014008</object-id>
      </element-citation>
    </ref>
    <ref id="b0205">
      <label>41</label>
      <mixed-citation publication-type="other" id="h0205">Pumperla M. Keras+ Hyperopt: a very simple wrapper for convenient hyperparameter optimization. 2016.</mixed-citation>
    </ref>
    <ref id="b0210">
      <label>42</label>
      <element-citation publication-type="journal" id="h0210">
        <person-group person-group-type="author">
          <name>
            <surname>Shin</surname>
            <given-names>H.C.</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>H.R.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Nogues</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</article-title>
        <source>IEEE Trans Med Imaging</source>
        <volume>35</volume>
        <year>2016</year>
        <fpage>1285</fpage>
        <lpage>1298</lpage>
        <pub-id pub-id-type="pmid">26886976</pub-id>
      </element-citation>
    </ref>
    <ref id="b0215">
      <label>43</label>
      <mixed-citation publication-type="other" id="h0215">Zhou B, Garcia AL, Xiao J, Torralba A, Oliva A, editors. Learning deep features for scene recognition using places database. International conference on neural information processing systems; 2014.</mixed-citation>
    </ref>
    <ref id="b0220">
      <label>44</label>
      <element-citation publication-type="journal" id="h0220">
        <person-group person-group-type="author">
          <name>
            <surname>Mukaka</surname>
            <given-names>M.M.</given-names>
          </name>
        </person-group>
        <article-title>Statistics corner: a guide to appropriate use of correlation coefficient in medical research</article-title>
        <source>Malawi Med J</source>
        <volume>24</volume>
        <year>2012</year>
        <fpage>69</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="pmid">23638278</pub-id>
      </element-citation>
    </ref>
    <ref id="b0225">
      <label>45</label>
      <element-citation publication-type="journal" id="h0225">
        <person-group person-group-type="author">
          <name>
            <surname>Rahman</surname>
            <given-names>M.K.</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>M.S.</given-names>
          </name>
        </person-group>
        <article-title>CRISPRpred: a flexible and efficient tool for sgRNAs on-target activity prediction in CRISPR/Cas9 systems</article-title>
        <source>PLoS ONE</source>
        <volume>12</volume>
        <year>2017</year>
        <object-id pub-id-type="publisher-id">e0181943</object-id>
      </element-citation>
    </ref>
    <ref id="b0230">
      <label>46</label>
      <mixed-citation publication-type="other" id="h0230">Chen L, Wang SP, Zhang YH, Li JR, Xing ZH, Yang J, et al. Identify key sequence features to improve CRISPR sgRNA efficacy. IEEE Access 2017; PP:26582-89.</mixed-citation>
    </ref>
    <ref id="b0235">
      <label>47</label>
      <element-citation publication-type="journal" id="h0235">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Jankovic</surname>
            <given-names>B.R.</given-names>
          </name>
          <name>
            <surname>Bajic</surname>
            <given-names>V.B.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Poly(A) motif prediction using spectral latent features from human DNA sequences</article-title>
        <source>Bioinformatics</source>
        <volume>29</volume>
        <year>2013</year>
        <fpage>i316</fpage>
        <lpage>i325</lpage>
        <pub-id pub-id-type="pmid">23813000</pub-id>
      </element-citation>
    </ref>
    <ref id="b0240">
      <label>48</label>
      <element-citation publication-type="journal" id="h0240">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Bikard</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Marraffini</surname>
            <given-names>L.A.</given-names>
          </name>
        </person-group>
        <article-title>RNA-guided editing of bacterial genomes using CRISPR-Cas systems</article-title>
        <source>Nat Biotechnol</source>
        <volume>31</volume>
        <year>2013</year>
        <fpage>233</fpage>
        <lpage>239</lpage>
        <pub-id pub-id-type="pmid">23360965</pub-id>
      </element-citation>
    </ref>
    <ref id="b0245">
      <label>49</label>
      <element-citation publication-type="journal" id="h0245">
        <person-group person-group-type="author">
          <name>
            <surname>Leenay</surname>
            <given-names>R.T.</given-names>
          </name>
          <name>
            <surname>Aghazadeh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hiatt</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Tse</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>T.L.</given-names>
          </name>
          <name>
            <surname>Apathy</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Large dataset enables prediction of repair after CRISPR-Cas9 editing in primary T cells</article-title>
        <source>Nat Biotechnol</source>
        <volume>37</volume>
        <year>2019</year>
        <fpage>1034</fpage>
        <lpage>1037</lpage>
        <pub-id pub-id-type="pmid">31359007</pub-id>
      </element-citation>
    </ref>
    <ref id="b0250">
      <label>50</label>
      <element-citation publication-type="journal" id="h0250">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Exploiting sequence-based features for predicting enhancer–promoter interactions</article-title>
        <source>Bioinformatics</source>
        <volume>33</volume>
        <year>2017</year>
        <fpage>i252</fpage>
        <lpage>i260</lpage>
        <pub-id pub-id-type="pmid">28881991</pub-id>
      </element-citation>
    </ref>
    <ref id="b0255">
      <label>51</label>
      <mixed-citation publication-type="other" id="h0255">Bai S, Kolter JZ, Koltun V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:180301271 2018.</mixed-citation>
    </ref>
    <ref id="b0260">
      <label>52</label>
      <element-citation publication-type="journal" id="h0260">
        <person-group person-group-type="author">
          <name>
            <surname>Zhuang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>A simple convolutional neural network for prediction of enhancer-promoter interactions with DNA sequence data</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="b0265">
      <label>53</label>
      <mixed-citation publication-type="other" id="h0265">Razavian AS, Azizpour H, Sullivan J, Carlsson S. CNN features off-the-shelf: an astounding baseline for recognition. 2014.</mixed-citation>
    </ref>
    <ref id="b0270">
      <label>54</label>
      <element-citation publication-type="journal" id="h0270">
        <person-group person-group-type="author">
          <name>
            <surname>Hsu</surname>
            <given-names>P.D.</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Weinstein</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Ran</surname>
            <given-names>F.A.</given-names>
          </name>
          <name>
            <surname>Konermann</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Agarwala</surname>
            <given-names>V.</given-names>
          </name>
        </person-group>
        <article-title>DNA targeting specificity of RNA-guided Cas9 nucleases</article-title>
        <source>Nat Biotechnol</source>
        <volume>31</volume>
        <year>2013</year>
        <fpage>827</fpage>
        <lpage>832</lpage>
        <pub-id pub-id-type="pmid">23873081</pub-id>
      </element-citation>
    </ref>
    <ref id="b0275">
      <label>55</label>
      <element-citation publication-type="journal" id="h0275">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bae</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H.R.</given-names>
          </name>
        </person-group>
        <article-title>Digenome-seq: genome-wide profiling of CRISPR-Cas9 off-target effects in human cells</article-title>
        <source>Nat Methods</source>
        <volume>12</volume>
        <year>2015</year>
        <fpage>237</fpage>
        <lpage>243</lpage>
        <pub-id pub-id-type="pmid">25664545</pub-id>
      </element-citation>
    </ref>
    <ref id="b0280">
      <label>56</label>
      <element-citation publication-type="journal" id="h0280">
        <person-group person-group-type="author">
          <name>
            <surname>Listgarten</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Weinstein</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kleinstiver</surname>
            <given-names>B.P.</given-names>
          </name>
          <name>
            <surname>Sousa</surname>
            <given-names>A.A.</given-names>
          </name>
          <name>
            <surname>Joung</surname>
            <given-names>J.K.</given-names>
          </name>
          <name>
            <surname>Crawford</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of off-target activities for the end-to-end design of CRISPR guide RNAs</article-title>
        <source>Nat Biomed Eng</source>
        <volume>2</volume>
        <year>2018</year>
        <fpage>38</fpage>
        <lpage>47</lpage>
        <pub-id pub-id-type="pmid">29998038</pub-id>
      </element-citation>
    </ref>
    <ref id="b0285">
      <label>57</label>
      <element-citation publication-type="journal" id="h0285">
        <person-group person-group-type="author">
          <name>
            <surname>Abadi</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yan</surname>
            <given-names>W.X.</given-names>
          </name>
          <name>
            <surname>Amar</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Mayrose</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>A machine learning approach for predicting CRISPR-Cas9 cleavage efficiencies and patterns underlying its mechanism of action</article-title>
        <source>PLoS Comput Biol</source>
        <volume>13</volume>
        <year>2017</year>
        <object-id pub-id-type="publisher-id">e1005807</object-id>
      </element-citation>
    </ref>
    <ref id="b0290">
      <label>58</label>
      <element-citation publication-type="journal" id="h0290">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>K.C.</given-names>
          </name>
        </person-group>
        <article-title>Synergizing CRISPR/Cas9 off-target predictions for ensemble insights and practical applications</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <year>2019</year>
        <fpage>1108</fpage>
        <lpage>1115</lpage>
        <pub-id pub-id-type="pmid">30169558</pub-id>
      </element-citation>
    </ref>
    <ref id="b0295">
      <label>59</label>
      <element-citation publication-type="journal" id="h0295">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>K.-C.</given-names>
          </name>
        </person-group>
        <article-title>Off-target predictions in CRISPR-Cas9 gene editing using deep learning</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <year>2018</year>
        <fpage>i656</fpage>
        <lpage>i663</lpage>
        <pub-id pub-id-type="pmid">30423072</pub-id>
      </element-citation>
    </ref>
    <ref id="b0300">
      <label>60</label>
      <element-citation publication-type="journal" id="h0300">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>An overview and metanalysis of machine and deep learning-based CRISPR gRNA design tools</article-title>
        <source>RNA Biol</source>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="b0305">
      <label>61</label>
      <element-citation publication-type="journal" id="h0305">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Computational approaches for effective CRISPR guide RNA design and evaluation</article-title>
        <source>Comput Struct Biotechnol J</source>
        <volume>18</volume>
        <year>2020</year>
        <fpage>35</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="pmid">31890142</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="s0110" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary data</title>
    <p id="p0250">The following are the Supplementary data to this article:<supplementary-material content-type="local-data" id="m0005"><caption><title>Supplementary data 1</title></caption><media xlink:href="mmc1.docx"/></supplementary-material></p>
  </sec>
  <ack id="ak005">
    <title>Acknowledgement</title>
    <p>This research was funded by the <funding-source id="gp005">National Natural Science Foundation of China</funding-source> Grant No. 61872396, U1611265 and 61872395, and also by <funding-source id="gp010">Pearl River Nova Program of Guangzhou</funding-source>, China Grant No. 201710010044.</p>
  </ack>
  <fn-group>
    <fn id="s0105" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0245">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csbj.2020.01.013" id="ir040">https://doi.org/10.1016/j.csbj.2020.01.013</ext-link>.</p>
    </fn>
  </fn-group>
</back>
