<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9477532</article-id>
    <article-id pub-id-type="pmid">35920769</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac509</article-id>
    <article-id pub-id-type="publisher-id">btac509</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Genome Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Identification of bacteriophage genome sequences with representation learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4921-0936</contrib-id>
        <name>
          <surname>Bai</surname>
          <given-names>Zeheng</given-names>
        </name>
        <aff><institution>Division of Health Medical Intelligence, Human Genome Center, The Institute of Medical Science, The University of Tokyo</institution>, Minato-ku, Tokyo 108-8639, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Zhang</surname>
          <given-names>Yao-zhong</given-names>
        </name>
        <aff><institution>Division of Health Medical Intelligence, Human Genome Center, The Institute of Medical Science, The University of Tokyo</institution>, Minato-ku, Tokyo 108-8639, <country country="JP">Japan</country></aff>
        <xref rid="btac509-cor1" ref-type="corresp"/>
        <!--yaozhong@ims.u-tokyo.ac.jp-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Miyano</surname>
          <given-names>Satoru</given-names>
        </name>
        <aff><institution>Division of Health Medical Intelligence, Human Genome Center, The Institute of Medical Science, The University of Tokyo</institution>, Minato-ku, Tokyo 108-8639, <country country="JP">Japan</country></aff>
        <aff><institution>M&amp;D Data Science Center, Tokyo Medical and Dental University</institution>, Tokyo 113-8510, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yamaguchi</surname>
          <given-names>Rui</given-names>
        </name>
        <aff><institution>Division of Health Medical Intelligence, Human Genome Center, The Institute of Medical Science, The University of Tokyo</institution>, Minato-ku, Tokyo 108-8639, <country country="JP">Japan</country></aff>
        <aff><institution>Division of Cancer Systems Biology, Aichi Cancer Center Research Institute</institution>, Nagoya 464-8681, <country country="JP">Japan</country></aff>
        <aff><institution>Division of Cancer Informatics, Nagoya University Graduate School of Medicine</institution>, Nagoya 466-8560, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fujimoto</surname>
          <given-names>Kosuke</given-names>
        </name>
        <aff><institution>Division of Metagenome Medicine, Human Genome Center, The Institute of Medical Science, The University of Tokyo</institution>, Minato-ku, Tokyo 108-8639, <country country="JP">Japan</country></aff>
        <aff><institution>Collaborative Research Institute for Innovative Microbiology, The University of Tokyo</institution>, Bunkyo-ku, Tokyo 113-8657, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Uematsu</surname>
          <given-names>Satoshi</given-names>
        </name>
        <aff><institution>Division of Metagenome Medicine, Human Genome Center, The Institute of Medical Science, The University of Tokyo</institution>, Minato-ku, Tokyo 108-8639, <country country="JP">Japan</country></aff>
        <aff><institution>Collaborative Research Institute for Innovative Microbiology, The University of Tokyo</institution>, Bunkyo-ku, Tokyo 113-8657, <country country="JP">Japan</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2989-308X</contrib-id>
        <name>
          <surname>Imoto</surname>
          <given-names>Seiya</given-names>
        </name>
        <aff><institution>Division of Health Medical Intelligence, Human Genome Center, The Institute of Medical Science, The University of Tokyo</institution>, Minato-ku, Tokyo 108-8639, <country country="JP">Japan</country></aff>
        <aff><institution>Collaborative Research Institute for Innovative Microbiology, The University of Tokyo</institution>, Bunkyo-ku, Tokyo 113-8657, <country country="JP">Japan</country></aff>
        <xref rid="btac509-cor1" ref-type="corresp"/>
        <!--imoto@hgc.jp-->
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac509-cor1">To whom correspondence should be addressed. Email: <email>yaozhong@ims.u-tokyo.ac.jp</email> or <email>imoto@hgc.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-08-03">
      <day>03</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>18</issue>
    <fpage>4264</fpage>
    <lpage>4270</lpage>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>11</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>20</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>01</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>05</day>
        <month>8</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Â© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac509.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Bacteriophages/phages are the viruses that infect and replicate within bacteria and archaea, and rich in human body. To investigate the relationship between phages and microbial communities, the identification of phages from metagenome sequences is the first step. Currently, there are two main methods for identifying phages: database-based (alignment-based) methods and alignment-free methods. Database-based methods typically use a large number of sequences as references; alignment-free methods usually learn the features of the sequences with machine learning and deep learning models.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose INHERIT which uses a deep representation learning model to integrate both database-based and alignment-free methods, combining the strengths of both. Pre-training is used as an alternative way of acquiring knowledge representations from existing databases, while the BERT-style deep learning framework retains the advantage of alignment-free methods. We compare INHERIT with four existing methods on a third-party benchmark dataset. Our experiments show that INHERIT achieves a better performance with the F1-score of 0.9932. In addition, we find that pre-training two species separately helps the non-alignment deep learning model make more accurate predictions.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The codes of INHERIT are now available in: <ext-link xlink:href="https://github.com/Celestial-Bai/INHERIT" ext-link-type="uri">https://github.com/Celestial-Bai/INHERIT</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Ministry of Education, Culture, Sports, Science, and Technology of Japan</institution>
          </institution-wrap>
        </funding-source>
        <award-id>21H03538</award-id>
        <award-id>21K19495</award-id>
        <award-id>22H00477</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Japan Society for the Promotion of Science</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001691</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>JSPS KAKENHI</institution>
          </institution-wrap>
        </funding-source>
        <award-id>JP21K12104</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Japan Agency for Medical Research and Development</institution>
            <institution-id institution-id-type="DOI">10.13039/100009619</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>21fk0108619h0001</award-id>
        <award-id>21ae0121048h0001</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Uehara Memorial Foundation </institution>
            <institution-id institution-id-type="DOI">10.13039/100008732</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Bacteriophages (phages for short) are the viruses that infect the bacteria and archaea, and are rich in human body (<xref rid="btac509-B12" ref-type="bibr">Edwards and Rohwer, 2005</xref>; <xref rid="btac509-B14" ref-type="bibr">Fuhrman, 1999</xref>; <xref rid="btac509-B41" ref-type="bibr">Reyes <italic toggle="yes">et al.</italic>, 2012</xref>; <xref rid="btac509-B42" ref-type="bibr">Rodriguez-Valera <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btac509-B43" ref-type="bibr">Rohwer and Thurber, 2009</xref>). To study the role of phages in the microbial community in the human body, we need first to identify phages from the metagenome nucleotide sequences (<xref rid="btac509-B13" ref-type="bibr">Fang <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac509-B31" ref-type="bibr">Marquet <italic toggle="yes">et al.</italic>, 2020</xref>). Using a method that can precisely distinguish between phages and bacteria can help researchers study phages more efficiently. Many methods have been proposed to identify phages, such as VIBRANT (<xref rid="btac509-B22" ref-type="bibr">Kieft et al., 2020</xref>), VirSorter2 (<xref rid="btac509-B16" ref-type="bibr">Guo <italic toggle="yes">et al</italic>., 2021</xref>), Seeker (<xref rid="btac509-B2" ref-type="bibr">Auslander <italic toggle="yes">et al</italic>., 2020</xref>) and DeepVirFinder (<xref rid="btac509-B40" ref-type="bibr">Ren <italic toggle="yes">et al</italic>., 2020</xref>). We categorize them into two groups: database-based (alignment-based) methods (VIBRANT and VirSorter2), and alignment-free methods(Seeker and DeepVirFinder) . Both types have their advantages and disadvantages, and they are complementary. Database-based approaches are commonly based on multiple sequence alignment (<xref rid="btac509-B5" ref-type="bibr">Chatzou <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac509-B11" ref-type="bibr">Edgar and Batzoglou, 2006</xref>; <xref rid="btac509-B19" ref-type="bibr">Hyatt <italic toggle="yes">et al.</italic>, 2010</xref>) with Profile Hidden Markov Models (<xref rid="btac509-B10" ref-type="bibr">Eddy, 1998</xref>), which can achieve good prediction performance. However, such prediction speed is generally limited by alignment. Alignment-free methods usually can predict fast. However, subject to the training process of the machine learning and deep learning models, we need to balance the amount of phage and bacteria data (<xref rid="btac509-B20" ref-type="bibr">Japkowicz and Stephen, 2002</xref>), which affects the amount of information obtained. The introduction of them can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods Section S1.1</xref>.</p>
    <p>In proposing the MSA Transformer, <xref rid="btac509-B39" ref-type="bibr">Rao <italic toggle="yes">et al.</italic> (2021)</xref> demonstrated that pre-trained Transformer-based models can have comparable performance to HMM Profiles and are even better in some cases. That indicates the core of the database-based approaches, HMM Profiles, can be realized for a similar purpose by representation learning (). Thus we can use the pre-train-fine-tune paradigm (<xref rid="btac509-B29" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac509-B30" ref-type="bibr">Mao, 2020</xref>;<xref rid="btac509-B3" ref-type="bibr"> Bengio et al., 2013</xref>; <xref rid="btac509-B17" ref-type="bibr">Gururangan et al., 2020</xref>; <xref rid="btac509-B38" ref-type="bibr">Radford et al., 2018</xref>; <xref rid="btac509-B48" ref-type="bibr">Zhang et al., 2018</xref>) to combine the above two methods.</p>
    <p>Here we propose INHERIT: <bold>I</bold>dentificatio<bold>N</bold> of bacteriop<bold>H</bold>ag<bold>E</bold>s using deep <bold>R</bold>epresentat<bold>I</bold>on model with pre-<bold>T</bold>raining. It also means our model âinheritsâ the characteristics from both database-based and alignment-free methods. The code of INHERIT is now available at <ext-link xlink:href="https://github.com/Celestial-Bai/INHERIT" ext-link-type="uri">https://github.com/Celestial-Bai/INHERIT</ext-link>. We show that using the representation learning framework can improve deep learning models, and INHERIT also achieves the best performance in our tests.</p>
    <p>The main contributions of our paper can be summarized as follows:
</p>
    <list list-type="order">
      <list-item>
        <p>We proposed INHERIT, an integrated model based on the DNA sequence language model: DNABERT, with two pre-trained models as references. It reaches the best performance compared with four existing state-of-the-art approaches: VIBRANT, VirSorter2, Seeker and DeepVirFinder. INHERIT outperforms them with the highest F1-score of 0.9932 in our test.</p>
      </list-item>
      <list-item>
        <p>We used an independent pre-training strategy to deal with the data imbalance issue of bacteria and phages. We found that this strategy can help the deep learning framework make more accurate predictions for both species.</p>
      </list-item>
    </list>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>INHERIT is a model with DNABERT as the backbone and uses two pre-trained models as references (see the pipeline in <xref rid="btac509-F1" ref-type="fig">Fig.Â 1A</xref>). DNABERT is an extension of BERT (<xref rid="btac509-B8" ref-type="bibr">Devlin et al., 2018</xref>) on nucleotide sequences. The structure of BERT model contains of 12 Transformer encoders (<xref rid="btac509-B46" ref-type="bibr">Vaswani et al., 2017</xref>), and Transformer is a neural network composed mainly of multi-head self-attention. Multi-head self-attention is a mechanism that can be expressed with (<xref rid="btac509-B46" ref-type="bibr">Vaswani et al., 2017</xref>):
<disp-formula id="E17"><mml:math id="M17" display="block" overflow="scroll"><mml:mtable columnspacing="0px" columnalign="right center left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>M</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mi>V</mml:mi><mml:mo>)</mml:mo><mml:mo>Â </mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>Â </mml:mo><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup><mml:mo>Â </mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:mi>where</mml:mi><mml:mo>Â </mml:mo><mml:mo>Â </mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>Â </mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>Â </mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:mo>Â·</mml:mo><mml:msup><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>Q</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>Â·</mml:mo><mml:msup><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>K</mml:mi></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>â¤</mml:mo></mml:msup></mml:mrow><mml:msqrt><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msqrt></mml:mfrac><mml:mo>)</mml:mo><mml:mi>V</mml:mi><mml:mo>Â·</mml:mo><mml:msup><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>V</mml:mi></mml:msup><mml:mo>}</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
    <p>Where <inline-formula id="IE9001"><mml:math id="IM9001" display="inline" overflow="scroll"><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mi>V</mml:mi></mml:math></inline-formula> are the vectors obtained by multiplying the three learned vectors with the last hidden states. <inline-formula id="IE9002"><mml:math id="IM9002" display="inline" overflow="scroll"><mml:msup><mml:mtable columnspacing="0px" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mi>Q</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mtable columnspacing="0px" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mi>K</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:msup><mml:msub><mml:mi>W</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>V</mml:mi></mml:msup><mml:mtable columnspacing="0px" columnalign="right center left"><mml:mtr><mml:mtd><mml:mfenced><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mo>â¯</mml:mo><mml:mo>,</mml:mo><mml:mo>Â </mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math></inline-formula> and <inline-formula id="IE9003"><mml:math id="IM9003" display="inline" overflow="scroll"><mml:msup><mml:mi>W</mml:mi><mml:mi>O</mml:mi></mml:msup></mml:math></inline-formula> are all learned matrices. <inline-formula id="IE9004"><mml:math id="IM9004" display="inline" overflow="scroll"><mml:mi>h</mml:mi></mml:math></inline-formula> is the number of attention heads, and <inline-formula id="IE9005"><mml:math id="IM9005" display="inline" overflow="scroll"><mml:msub><mml:mi>d</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> is the dimension of <inline-formula id="IE9006"><mml:math id="IM9006" display="inline" overflow="scroll"><mml:mi>K</mml:mi></mml:math></inline-formula>. A detailed introduction of DNABERT can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods Section S1.1</xref>. We fine-tune them simultaneously to identify the metagenome sequences. The following will introduce the structure of INHERIT and the datasets we use.</p>
    <fig position="float" id="btac509-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Panel (<bold>A</bold>) shows the overall model architecture and explains how the label class is predicted given trained model parameters. In predicting the label class of a given 500âbp nucleotide fragment, it is first encoded into tokens as the k-mer inputs. Then, they will be converted to the embeddings for two fine-tuned DNABERTs through the embedding layer independently. These embeddings are then fed into the fine-tuned Transformer encoders. The representations of the (CLS) of the two DNABERTs are extracted and then classified to generate four outputs. Eventually, INHERIT predicts the class from the four outputs of DNABERTs with a dense layer. Panel (<bold>B</bold>) shows how the parameter weights are assigned through pre-training and fine-tuning. In pre-training, two randomly initialized DNABERTs pre-train independently with two datasets of different sizes and species. After that, the pre-trained weights in the lower non-linear-probing layers are first used for the model initialization. Then in the fine-tuning step, all model parameters, including those of the two pre-trained DNABERTs, are fine-tuned together with a balanced training set</p>
      </caption>
      <graphic xlink:href="btac509f1" position="float"/>
    </fig>
    <sec>
      <title>2.1 Model architecture and pipeline</title>
      <p>Here we show the architecture of INHERIT in <xref rid="btac509-F1" ref-type="fig">FigureÂ 1A</xref> and how parameter weights are assigned and transferred in the pre-training and fine-tuning process.</p>
      <p>The sequences are split into several 500âbp-long segments as the input of INHERIT. When this sequence is not divisible by 500, we will use the head of this sequence to complement its end until it is divisible, which keeps the same with Seeker (<xref rid="btac509-B2" ref-type="bibr">Auslander et al., 2020</xref>). For each segment, it will be encoded to tokens as k-mer inputs. According to the previous work (<xref rid="btac509-B21" ref-type="bibr">Ji <italic toggle="yes">et al.</italic>, 2021</xref>), we use 6-mer as input to DNABERT. Those tokens will be generated to embeddings for two fine-tuned DNABERTs through each embedding layer separately. Then each embedding is fed to each transformer encoder (<xref rid="btac509-B46" ref-type="bibr">Vaswani <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac509-B8" ref-type="bibr">Devlin et al., 2018</xref>). Same with BERT (<xref rid="btac509-B8" ref-type="bibr">Devlin <italic toggle="yes">et al.</italic>, 2018</xref>), the representations of the â[CLS]â token is extracted and generate two outputs for each fine-tuned DNABERT through a dense layer (<xref rid="btac509-B47" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic>, 2020</xref>). INHERIT eventually predicts the label class based on the four outputs with a dense layer. The prediction of the whole sequence is the average of predictions of all segments, which we call the âscoreâ of the sequence.</p>
      <p>Here we used the pre-train-fine-tune paradigm to build INHERIT (see the process of pre-training and fine-tuning INHERIT in <xref rid="btac509-F1" ref-type="fig">Fig. 1B</xref>). To deal with the information bias that may be caused by data imbalance (<xref rid="btac509-B45" ref-type="bibr">Thabtah <italic toggle="yes">et al.</italic>, 2020</xref>), we pre-trained bacteria and phages independently. The number of bacteria we have known is much larger than the number of phages, and the length of bacteria is also longer (<xref rid="btac509-B4" ref-type="bibr">Chanishvili <italic toggle="yes">et al.</italic>, 2001</xref>). If we want the pre-training set to carry a considerable amount of data, the segments belonging to the bacteria will be bound to be much more than those belonging to the phages. If we combine bacteria and phages in one pre-trained model, the model will learn much more about bacteria than phages. Therefore, we prepared two pre-trained models for INHERIT. Here, we used Masked Language Modeling (<xref rid="btac509-B8" ref-type="bibr">Devlin <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac509-B34" ref-type="bibr">Naseem <italic toggle="yes">et al.</italic>, 2021</xref>) as the pre-training task, which is the same as <xref rid="btac509-B21" ref-type="bibr">Ji <italic toggle="yes">et al.</italic> (2021)</xref>. For the detailed settings of the pre-training, please refer to <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods Section S1.2</xref>.</p>
      <p>Fine-tuning is very similar to the traditional training strategy (<xref rid="btac509-B9" ref-type="bibr">Dodge <italic toggle="yes">et al.</italic>, 2020</xref>). The difference between them is that traditionally we randomly initialize the deep learning model before we start to train the model. However, we will transfer most of the pre-trained model weights as the initialization before we start to fine-tune the deep learning framework. Here, we transferred the weights of non-linear-probing layers of the two pre-trained models to initialize INHERIT. The linear layers of INHERIT were still randomly initialized because we could not transfer the weights from the pre-trained models. All model parameters were fine-tuned together with a balanced training set. For hyperparameters and platforms of fine-tuning, please see <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods Section S1.2</xref>.</p>
    </sec>
    <sec>
      <title>2.2 Datasets</title>
      <sec>
        <title>2.2.1 Pre-training sets</title>
        <p>To make pre-trained models carry as much biological information as possible, we pre-trained bacteria and phages separately and did not balance the size of the two pre-training sets. For the bacteria pre-training set, we used ncbi-genome-download (<ext-link xlink:href="https://github.com/kblin/ncbi-genome-download" ext-link-type="uri">https://github.com/kblin/ncbi-genome-download</ext-link>) to obtain the complete bacteria genome sequence from the NCBI FTP. We used the command: ncbi-genome-download-formats fasta âassembly-levels complete bacteria. All of those bacteria sequences were high quality, and we called them âbacteria assembliesâ. We randomly sampled 4124 bacteria sequences from them because of the physical memory limitation. However, these 4124 sequences can generate 15â975â346 segments, and the dataset size is large enough. We could not obtain the phage sequence data in the same way for the phage pre-training set. Since phage sequences could not be found and downloaded directly in the NCBI FTP like the bacteria sequences, we directly searched for the keyword âphageâ on NCBI, downloaded all sequences longer than 500âbp, and checked all of them manually. We also referred to the phage sequences used by Seeker and VIBRANT and finally generated a pre-training set containing 26â920 phage sequences. It did not include the phage sequences in the test and validation sets to prevent overfitting. These phage sequences can generate 1â750â662 segments, and the size is still large for a phage dataset.</p>
      </sec>
      <sec>
        <title>2.2.2 Training set and validation set for fine-tuning</title>
        <p>For bacteria during fine-tuning, we randomly selected 260 bacteria sequences that were not in the pre-training and test sets but bacteria assemblies. Two hundred seventeen bacteria sequences were used as the training set, generating 718â879 segments, and the remaining 43 were used as the validation set, generating 188â149 segments. However, we did not have as many sequences to choose from for phages, so we selected 10â574 phage sequences from the pre-training set that possessed a quality comparable to the bacteria assemblies, generating 718â663 segments. We also chose 2643 sequences not in the pre-training set as the validation set, generating 186â121 segments.</p>
      </sec>
      <sec>
        <title>2.2.3 Test set for comparisons</title>
        <p>The test set we use is one of the third-party benchmark tests previously proposed by <xref rid="btac509-B18" ref-type="bibr">Ho <italic toggle="yes">et al.</italic> (2021)</xref> for virus identification methods, called the RefSeq test set. Since our method identifies phages and not other viruses, we only used data related to phages. The RefSeq test set contains 710 bacteria sequences and 1028 phage sequences. However, since there are 19 bacteria sequences removed from NCBI RefSeq database, we used the rest of them, including 691 bacteria sequences and 1028 phage sequences, to examine the performance of the methods on phage identification. It should be added that, in that article (<xref rid="btac509-B18" ref-type="bibr">Ho <italic toggle="yes">et al.</italic>, 2021</xref>), the authors split the sequences in this test set into 1âkb to 15âkb segments on average and predict the results and calculate metrics on segment level to make a benchmark test. However, since we consider INHERIT to determine whether a sequence is a phage or not in applications, we compared INHERIT with other existing methods on the sequence level. For experimental details and results, please refer to Section 3.1.</p>
        <p>We have posted the accessions of the sequences used in each dataset and the sources we obtain in Supplementary Table S1.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Experiments</title>
    <sec>
      <title>3.1 Benchmarking INHERIT with VIBRANT, VirSorter2, Seeker, DeepVirFinder</title>
      <p>In this section, we compared INHERIT with four state-of-the-art methods: VIBRANT, VirSorter2, Seeker and DeepVirFinder, which are the representatives of database-based and alignment-free methods. We used a third-party benchmark dataset to conduct experiments and analyses.</p>
      <sec>
        <title>3.1.1 Experimental setups</title>
        <p><bold>Baselines:</bold> Until our work is completed, INHERIT is the only model that integrates the features of both database-based and alignment-free approaches. Thus, we chose two representatives which have achieved state-of-the-art from each of the two methods, VIBRANT (<xref rid="btac509-B22" ref-type="bibr">Kieft <italic toggle="yes">et al.</italic>, 2020</xref>) and VirSorter2 (<xref rid="btac509-B16" ref-type="bibr">Guo <italic toggle="yes">et al.</italic>, 2021</xref>); and Seeker (<xref rid="btac509-B2" ref-type="bibr">Auslander <italic toggle="yes">et al.</italic>, 2020</xref>) and DeepVirFinder (<xref rid="btac509-B40" ref-type="bibr">Ren <italic toggle="yes">et al.</italic>, 2020</xref>), to compare it with INHERIT. To ensure the comparison is as fair as possible, we used each methodâs default commands for predictions as much as possible, and we set corresponding rules for some methods to ensure that the prediction formats of each method are as consistent as possible. Detailed settings for each method can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Methods Section S1.3</xref>.</p>
        <p><bold>Evaluation metrics:</bold> The evaluation metrics we chose are:
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
Â <disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mtext>TPR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
Â <disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
Â <disp-formula id="E4"><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mtext>score</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>Â·</mml:mo><mml:mtext>TP</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>Â·</mml:mo><mml:mtext>TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext>FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and AUROC (Area Under the Receiver Operating Characteristic curve) and AUPRC (Area Under the Precision-Recall Curve). In this article, TP is the number of phage sequences successfully identified as phages, while FP is the number of bacteria sequences incorrectly identified as phages. TN is the number of bacteria sequences successfully identified as bacteria and FN is the number of phage sequences incorrectly identified as bacteria. AUROC and AUPRC are calculated based on the score of each sequence and the real value (phages are recorded as 1 and bacteria as 0). Here we calculate all evaluation metrics by scikit-learn v1.0.2 (<xref rid="btac509-B36" ref-type="bibr">Pedregosa <italic toggle="yes">et al.</italic>, 2011</xref>).</p>
        <p><bold>Prediction results:</bold> The predictions of VIBRANT, VirSorter2, Seeker, DeepVirFinder and INHERIT for all the sequences in the test set can be seen in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref>.</p>
      </sec>
      <sec>
        <title>3.1.2 Result and analysis</title>
        <p><bold>INHERIT performs better than existing methods:</bold> From the results (see <xref rid="btac509-T1" ref-type="table">TableÂ 1</xref>), compared to VIBRANT, VirSorter2, Seeker and DeepVirFinder, INHERIT performs better than other existing methods. Moreover, the overall performance of INHERIT is an order of magnitude more precise. Even if we use the default threshold of 0.5, the recall of INHERIT does not differ much from that of VIBRANT. Significantly, the high F1-score of INHERIT proves that INHERIT performs well enough when we use the default parameters and is competent for most application scenarios. From the <italic toggle="yes">P</italic>-values of the DeLong Test (<xref rid="btac509-B7" ref-type="bibr">DeLong <italic toggle="yes">et al.</italic>, 1988</xref>) of every two methods, the ROC curves for each of the two approaches are statistically significantly different. Therefore, the high AUROC and AUPRC indicate that INHERIT can distinguish between phages and bacteria better.</p>
        <table-wrap position="float" id="btac509-T1">
          <label>Table 1.</label>
          <caption>
            <p>The benchmark results of VIBRANT, VirSorter2, Seeker, DeepVirFinder and INHERIT</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Model</th>
                <th rowspan="1" colspan="1">Precision</th>
                <th rowspan="1" colspan="1">Recall</th>
                <th rowspan="1" colspan="1">Accuracy</th>
                <th rowspan="1" colspan="1">F1-score</th>
                <th rowspan="1" colspan="1">AUROC</th>
                <th rowspan="1" colspan="1">AUPRC</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">VIBRANT</td>
                <td rowspan="1" colspan="1">0.9541</td>
                <td rowspan="1" colspan="1">0.9903</td>
                <td rowspan="1" colspan="1">0.9656</td>
                <td rowspan="1" colspan="1">0.9718</td>
                <td rowspan="1" colspan="1">0.9595</td>
                <td rowspan="1" colspan="1">0.9751</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">VirSorter2</td>
                <td rowspan="1" colspan="1">0.9685</td>
                <td rowspan="1" colspan="1">0.9893</td>
                <td rowspan="1" colspan="1">0.9728</td>
                <td rowspan="1" colspan="1">0.9787</td>
                <td rowspan="1" colspan="1">0.9932</td>
                <td rowspan="1" colspan="1">0.9971</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Seeker</td>
                <td rowspan="1" colspan="1">0.9264</td>
                <td rowspan="1" colspan="1">0.8453</td>
                <td rowspan="1" colspan="1">0.8674</td>
                <td rowspan="1" colspan="1">0.8840</td>
                <td rowspan="1" colspan="1">0.9382</td>
                <td rowspan="1" colspan="1">0.9605</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">DeepVirFinder</td>
                <td rowspan="1" colspan="1">0.8992</td>
                <td rowspan="1" colspan="1">0.8502</td>
                <td rowspan="1" colspan="1">0.8534</td>
                <td rowspan="1" colspan="1">0.8740</td>
                <td rowspan="1" colspan="1">0.8971</td>
                <td rowspan="1" colspan="1">0.9435</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">
                  <bold>INHERIT</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9884</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9981</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9919</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9932</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9996</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>0.9997</bold>
                </td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic toggle="yes">Note</italic>: For all methods, we used the default commands for predictions. This can make our benchmarking as fair as possible.Â Values corresponding to best performance are bolded.Â </p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p><bold>The performance of INHERIT is less affected by the length of nucleotide sequences:</bold> We find the performance of INHERIT is less sensitive to the length of genome sequences compared with other methods. We divided the phage and bacteria into five length intervals, each based on the quartiles of lengths in the test set. The length intervals are according to the following rules: âminimumâfirst quartileâ, âfirst quartileâmedianâ, âmedianâthird quartileâ, âthird quartileâmaximum of non-outliersâ, âthe maximum of non-outliersâoutliersâ. For phage sequences, there are âless than 42â000âbpâ, â42â000â51â000âbpâ, â51â500â91â600âbpâ and âgreater than 91â600âbpâ. For bacteria sequences, there are âless than 2â788â000âbpâ, â2â788â000â4â107â500âbpâ, â4â107â500â4â938â000âbpâ, âgreater than 4â938â000âbpâ. We calculated the true positive rates of VIBRANT, VirSorter2, Seeker, DeepVirFinder and INHERIT in the four intervals of phage and the true negative rates in the four intervals of bacteria, respectively. That allows exploring whether these five methods perform consistently and robustly at different sequence lengths for both phages and bacteria. We can find that INHERIT is the only method to maintain the true positive rate of 0.99 and the true negative rate of 0.97 in different intervals (see <xref rid="btac509-F2" ref-type="fig">Fig.Â 2</xref>). It does not always perform the best in every interval, but it is more robust than other models. For example, the true negative rate of VIBRANT in the â2â788â000â4â107â500âbpâ interval is 0.994, which is better than INHERIT (0.977), but its performance in the â4â938â000â8â000â000âbpâ interval is only 0.767.</p>
        <fig position="float" id="btac509-F2">
          <label>Fig. 2.</label>
          <caption>
            <p>The barplot of the true positive rates and true negative rates at different intervals. Panel (<bold>A</bold>) shows the true positive rates for each method at different length intervals. Panel (<bold>B</bold>) shows the true negative rate of each method at different length intervals</p>
          </caption>
          <graphic xlink:href="btac509f2" position="float"/>
        </fig>
        <p><bold>INHERIT has appropriate prediction speed:</bold> INHERIT is still a model based on a deep learning framework, so it does not take as long a time to predict as database-based approaches. We calculated the average time required for VIBRANT, VirSorter2, Seeker, DeepVirFinder and INHERIT to predict phage sequences and bacteria sequences in the test set. The results (see <xref rid="btac509-T2" ref-type="table">TableÂ 2</xref>) show that the predictions of VIBRANT and VirSorter2 take much more time than Seeker and INHERIT. From our experiment, VIBRANT and VirSorter2 even cannot predict the whole bacteria test set at once. That indicates that even though database-based methods perform better than alignment-free methods, they consume a long time to predict and are more sensitive to dataset size. However, INHERIT has high performance and predicts the second fastest among VIBRANT, VirSorter, Seeker and DeepVirFinder. Although INHERIT takes a longer time to predict than Seeker, it takes much less time than DeepVirFinder. DeepVirFinder selects different models for prediction based on the length of the target sequence. In our experiments, we offered the same environment to INHERIT, Seeker and DeepVirFinder, but DeepVirFinder is still much slower than the other two methods, even if it is based on a convolutional neural network (<xref rid="btac509-B26" ref-type="bibr">Lecun <italic toggle="yes">et al.</italic>, 1998</xref>; <xref rid="btac509-B35" ref-type="bibr">OâShea and Nash, 2015</xref>). We conjecture that it may be because it consumes much time in the process of DNA sequence encoding and model selection. INHERIT uses one model to make predictions for each sequence regardless of the length, showing that INHERIT can give accurate predictions in an appropriate time budget.</p>
        <table-wrap position="float" id="btac509-T2">
          <label>Table 2.</label>
          <caption>
            <p>The average time (second) required for VIBRANT, VirSorter2, Seeker, DeepVirFinder, DNABERT and INHERIT to predict phage sequences and bacteria sequences in the test set</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Sequence</th>
                <th rowspan="1" colspan="1">VIBRANT</th>
                <th rowspan="1" colspan="1">VirSorter2</th>
                <th rowspan="1" colspan="1">Seeker</th>
                <th rowspan="1" colspan="1">DeepVirFinder</th>
                <th rowspan="1" colspan="1">DNABERT</th>
                <th rowspan="1" colspan="1">INHERIT</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Bacteria</td>
                <td rowspan="1" colspan="1">565.7887</td>
                <td rowspan="1" colspan="1">1256.7568</td>
                <td rowspan="1" colspan="1">
                  <bold>1.6068</bold>
                </td>
                <td rowspan="1" colspan="1">344.4573</td>
                <td rowspan="1" colspan="1">53.7031</td>
                <td rowspan="1" colspan="1">67.3556</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Phage</td>
                <td rowspan="1" colspan="1">19.1440</td>
                <td rowspan="1" colspan="1">29.1797</td>
                <td rowspan="1" colspan="1">
                  <bold>0.1139</bold>
                </td>
                <td rowspan="1" colspan="1">16.2840</td>
                <td rowspan="1" colspan="1">2.7851</td>
                <td rowspan="1" colspan="1">3.0127</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn2">
              <p><italic toggle="yes">Note</italic>: The time of each model implies their average running time (second) on predicting each bacterium and phage. The average length of bacteria samples on the test set is 3â950â500âbp, while the average length of phage samples on the test set is 75â800âbp.Â Values corresponding to fastest speed are bolded.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
    <sec>
      <title>3.2 Ablation study</title>
      <p>We used the validation set to do an additional experiment for considering ablation. In this section, we discuss how INHERITâs pre-training strategy and deep learning structure affect performance.</p>
      <p>There are two main differences between INHERIT and the backbone: DNABERT. First, we used a strategy to pre-train two species independently. To use pre-training to provide references and solve imbalanced bacterial and phage datasets, we pre-trained bacteria and phages separately. In addition, we utilized two pre-trained models and have them fine-tuned simultaneously. This model structure has twice the number of parameters as DNABERT. To explore whether pre-training would help deep learning frameworks improve performance, we made an ablation study on the validation set. In training INHERIT, we first trained two pre-trained models on two separate datasets of different sizes and species. Then, the weights of these two pre-trained models were used as initialization while fine-tuning simultaneously on the training set. Therefore, we chose to compare an INHERIT that does not use any pre-trained models and randomly initializes two DNABERTs and fine-tuned them on the training set simultaneously. We denote it as INHERIT (w/o pre-train). Further, since we used two DNABERTs, INHERIT is about twice as large as DNABERT in terms of the number of parameters. Therefore, we also trained a DNABERT to reflect the effect of the number of parameters on the model performance. All models used the same hyperparameters as INHERIT during training.</p>
      <p>We tested all modelsâ performance on the validation set. Here we evaluated all three models on both sequence and segment levels because the validation set is balanced on the segment level while severely imbalanced on the sequence level (43 bacteria and 2643 phages). The differences in performance reflected by different metrics may be biased when we evaluate them on the sequence level. Therefore, we have attached the confusion matrix of the three models on the level of both sequence and segment in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S3</xref>. Based on the results (see <xref rid="btac509-T3" ref-type="table">TableÂ 3</xref>), INHERIT improves on the excellent performance of DNABERT in all metrics and performs better than INHERIT (w/o pre-train) in most of the metrics on sequence level and performs the best on a segment level. Meanwhile, although we trained on segment level, the better performance on the segment level, the more accurate the prediction of the whole genome sequence. The increase in the number of parameters helps INHERIT, allowing it to outperform DNABERT. However, pre-training can give INHERIT a further boost in very high accuracy without increasing the number of parameters. We also made DeLong Test for these three models with each other, and these three curves are statistically significant. Compared to INHERIT (w/o pre-train), INHERIT significantly improves the F1-score from 0.9611 to 0.9943 on the sequence level and from 0.8788 to 0.9542 on the segment level. This reflects that the independent pre-training strategy can help the deep learning framework to distinguish better and make more accurate predictions on both species. Compared with DNABERT, both two differences help INHERIT to make better performance.</p>
      <table-wrap position="float" id="btac509-T3">
        <label>Table 3.</label>
        <caption>
          <p>The comparison among DNABERT (without pre-training, i.e. training from scratch), INHERIT (without pre-training) and INHERIT</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Precision</th>
              <th rowspan="1" colspan="1">Recall</th>
              <th rowspan="1" colspan="1">Accuracy</th>
              <th rowspan="1" colspan="1">F1-score</th>
              <th rowspan="1" colspan="1">AUROC</th>
              <th rowspan="1" colspan="1">AUPRC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Sequence level</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td colspan="7" rowspan="1">
                <hr/>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">âDNABERT</td>
              <td rowspan="1" colspan="1">0.9992</td>
              <td rowspan="1" colspan="1">0.9224</td>
              <td rowspan="1" colspan="1">0.9229</td>
              <td rowspan="1" colspan="1">0.9593</td>
              <td rowspan="1" colspan="1">0.9751</td>
              <td rowspan="1" colspan="1">0.9996</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">âINHERIT (w/o pre-train)</td>
              <td rowspan="1" colspan="1">
                <bold>0.9996</bold>
              </td>
              <td rowspan="1" colspan="1">0.9255</td>
              <td rowspan="1" colspan="1">0.9263</td>
              <td rowspan="1" colspan="1">0.9611</td>
              <td rowspan="1" colspan="1">0.9839</td>
              <td rowspan="1" colspan="1">0.9997</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>âINHERIT</bold>
              </td>
              <td rowspan="1" colspan="1">0.9992</td>
              <td rowspan="1" colspan="1">
                <bold>0.9894</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9888</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9943</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9971</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>1.0000</bold>
              </td>
            </tr>
            <tr>
              <td colspan="7" rowspan="1">
                <hr/>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Segment level</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td colspan="7" rowspan="1">
                <hr/>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">âDNABERT</td>
              <td rowspan="1" colspan="1">0.8767</td>
              <td rowspan="1" colspan="1">0.8809</td>
              <td rowspan="1" colspan="1">0.8792</td>
              <td rowspan="1" colspan="1">0.8788</td>
              <td rowspan="1" colspan="1">0.9508</td>
              <td rowspan="1" colspan="1">0.9507</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">âINHERIT (w/o pre-train)</td>
              <td rowspan="1" colspan="1">0.9044</td>
              <td rowspan="1" colspan="1">0.8951</td>
              <td rowspan="1" colspan="1">0.9008</td>
              <td rowspan="1" colspan="1">0.8997</td>
              <td rowspan="1" colspan="1">0.9530</td>
              <td rowspan="1" colspan="1">0.9422</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>âINHERIT</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9436</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9650</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9539</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9542</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9875</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.9882</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <p><italic toggle="yes">Note</italic>: We use the validation set to evaluate the performance of the three models. Here, we evaluate all three models on sequence level and segment because the validation set is balanced on the segment level while severely imbalanced on sequence level.Â Values corresponding to best performance are bolded.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Pre-training independently will allow the deep learning framework to give predictions closer to the true values. We plotted boxplots of the scores given by DNABERT, INHERIT (w/o pre-train) and INHERIT for phage and bacteria samples in the validation set on sequence level (see <xref rid="btac509-F3" ref-type="fig">Fig.Â 3</xref>). For the vast majority of samples, the score generated by INHERIT is closer to the true value (1 for phages and 0 for bacteria). For example, for phage MH576962, DNABERT gives a score of 0.4707 and INHERIT (w/o pre-train) gives a score of 0.4968. Neither model classify it correctly. However, the score from INHERIT is 0.8182. Compared to the other two models, INHERIT classifies it correctly and significantly improves the score more toward 1. That means that independent pre-training provides great help for deep learning frameworks to give more correct predictions on both species. This point even stands for the samples where none of the three predict correctly or all of the three predict correctly. For example, for phage HQ906662, the DNABERT, INHERIT (w/o pre-train) and INHERIT prediction scores were 0.4209, 0.2908 and 0.4547, respectively; for bacterium NZ_CP018197, the DNABERT, INHERIT (w/o pre-train) and INHERIT prediction scores were 0.2237, 0.1210 and 0.0621, respectively.</p>
      <fig position="float" id="btac509-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>The boxplots of the predictions of DNABERT, INHERIT (w/o pre-train) and INHERIT in the validation set. For phage sequences, the closer the model prediction is to 1, the better. While for bacteria, the closer the model prediction is to 0, the better. Panel (<bold>A</bold>) is the boxplot of the scores of DNABERT, INHERIT (w/o pre-train) and INHERIT for the phage sequences in the validation set. Panel (<bold>B</bold>) is the boxplot of the scores of DNABERT, INHERIT (w/o pre-train) and INHERIT for the bacteria sequences in the validation set</p>
        </caption>
        <graphic xlink:href="btac509f3" position="float"/>
      </fig>
      <p>We also tested the prediction speed of DNABERT. From the results (see <xref rid="btac509-T2" ref-type="table">TableÂ 2</xref>), the average prediction speed of DNABERT is 53.7031âs for bacteria sequences and 2.7851âs for phage sequences in the test set. It is slightly faster than INHERIT. That is because INHERIT has a more complex structure than DNABERT. However, INHERIT can still give predictions faster than other methods.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusions</title>
    <p>In this work, we proposed INHERIT, an integrated method that combines both database-based and alignment-free approaches under a unified deep representation learning framework. It uses two pre-trained models as references and keeps the features of alignment-free methods by the deep learning structure. On a third-party benchmark dataset, we compared the proposed method with VIBRANT, VirSorter2, Seeker and DeepVirFinder, representing database-based methods and alignment-free methods. We demonstrate that INHERIT can achieve better performance than the four existing methods in all metrics. In particular, INHERIT improves the F1-score from 0.9787 to 0.9932. Meanwhile, we also prove that using an independent pre-training strategy can make deep learning models make better predictions on both species.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac509_Supplementary_Data</label>
      <media xlink:href="btac509_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The super-computing resource was provided by the Human Genome Center, Institute of Medical Science, The University of Tokyo (<ext-link xlink:href="https://gc.hgc.jp/en/" ext-link-type="uri">https://gc.hgc.jp/en/</ext-link>). Y.Z.âs appreciation also goes to Google who provided GCP based on the partnership between UTokyo and Google.</p>
    <sec>
      <title>Funding</title>
      <p>This study was supported by the Ministry of Education, Culture, Sports, Science, and Technology of Japan (21H03538 to S.I.; 21K19495 and 22H00477 to S.U.), Japan Society for the Promotion of Science (to Y.Z., JSPS KAKENHI Grant Number JP21K12104), the Japan Agency for Medical Research and Development (AMED) (21fk0108619h0001 to S.U. and 21ae0121048h0001 to K.F.) and the Uehara Memorial Foundation (to S.I.).</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>The data underlying this article are available in NCBI Nucleotide database at <ext-link xlink:href="https://www.ncbi.nlm.nih.gov/nuccore/" ext-link-type="uri">https://www.ncbi.nlm.nih.gov/nuccore/</ext-link>, and the accessions of the sequences used in each dataset can be found in online supplementary materials.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac509-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Andersson</surname><given-names>D.I.</given-names></string-name>, <string-name><surname>Hughes</surname><given-names>D.</given-names></string-name></person-group> (<year>2010</year>) <article-title>Antibiotic resistance and its cost: is it possible to reverse resistance?</article-title> Â <source>Nat. Rev. Microbiol</source>., <volume>8</volume>, <fpage>260</fpage>â<lpage>271</lpage>.<pub-id pub-id-type="pmid">20208551</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Auslander</surname><given-names>N.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) <article-title>Seeker: alignment-free identification of bacteriophage genomes by deep learning</article-title>. <source>Nucleic Acids Res</source>., <volume>48</volume>, <fpage>e121</fpage>.<pub-id pub-id-type="pmid">33045744</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bengio</surname><given-names>Y.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2013</year>) <article-title>Representation learning: a review and new perspectives</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell</source>., <volume>35</volume>, <fpage>1798</fpage>â<lpage>1828</lpage>.<pub-id pub-id-type="pmid">23787338</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chanishvili</surname><given-names>N.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2001</year>) <article-title>Phages and their application against drug-resistant bacteria</article-title>. <source>J. Chem. Technol. Biotechnol</source>., <volume>76</volume>, <fpage>689</fpage>â<lpage>699</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chatzou</surname><given-names>M.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2016</year>) <article-title>Multiple sequence alignment modeling: methods and applications</article-title>. <source>Brief. Bioinform</source>., <volume>17</volume>, <fpage>1009</fpage>â<lpage>1023</lpage>.<pub-id pub-id-type="pmid">26615024</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davies</surname><given-names>J.</given-names></string-name>, <string-name><surname>Davies</surname><given-names>D.</given-names></string-name></person-group> (<year>2010</year>) <article-title>Origins and evolution of antibiotic resistance</article-title>. <source>Microbiol. Mol. Biol. Rev</source>., <volume>74</volume>, <fpage>417</fpage>â<lpage>433</lpage>.<pub-id pub-id-type="pmid">20805405</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>DeLong</surname><given-names>E.R.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>1988</year>) <article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title>. <source>Biometrics</source>, <volume>44</volume>, <fpage>837</fpage>â<lpage>845</lpage>.<pub-id pub-id-type="pmid">3203132</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Devlin</surname><given-names>J.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2018</year>) Bert: pre-training of deep bidirectional transformers for language understanding. <italic toggle="yes">arXiv preprint arXiv:1810.04805</italic>. <pub-id pub-id-type="doi">10.48550/arXiv.1810.04805</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac509-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Dodge</surname><given-names>J.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. <italic toggle="yes">arXiv preprint arXiv:2002.06305.</italic> Â <pub-id pub-id-type="doi">10.48550/arXiv.2002.06305</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac509-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eddy</surname><given-names>S.R.</given-names></string-name></person-group> (<year>1998</year>) <article-title>Profile hidden Markov models</article-title>. <source>Bioinformatics</source>, <volume>14</volume>, <fpage>755</fpage>â<lpage>763</lpage>.<pub-id pub-id-type="pmid">9918945</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edgar</surname><given-names>R.C.</given-names></string-name>, <string-name><surname>Batzoglou</surname><given-names>S.</given-names></string-name></person-group> (<year>2006</year>) <article-title>Multiple sequence alignment</article-title>. <source>Curr. Opin. Struct. Biol</source>., <volume>16</volume>, <fpage>368</fpage>â<lpage>373</lpage>.<pub-id pub-id-type="pmid">16679011</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Edwards</surname><given-names>R.A.</given-names></string-name>, <string-name><surname>Rohwer</surname><given-names>F.</given-names></string-name></person-group> (<year>2005</year>) <article-title>Viral metagenomics</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>3</volume>, <fpage>504</fpage>â<lpage>510</lpage>.<pub-id pub-id-type="pmid">15886693</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fang</surname><given-names>Z.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2019</year>) <article-title>PPR-Meta: a tool for identifying phages and plasmids from metagenomic fragments using deep learning</article-title>. <source>GigaScience</source>, <volume>8</volume>, <fpage>giz066</fpage>.<pub-id pub-id-type="pmid">31220250</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fuhrman</surname><given-names>J.A.</given-names></string-name></person-group> (<year>1999</year>) <article-title>Marine viruses and their biogeochemical and ecological effects</article-title>. <source>Nature</source>, <volume>399</volume>, <fpage>541</fpage>â<lpage>548</lpage>.<pub-id pub-id-type="pmid">10376593</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Golkar</surname><given-names>Z.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2014</year>) <article-title>Bacteriophage therapy: a potential solution for the antibiotic resistance crisis</article-title>. <source>J. Infect. Dev. Ctries</source>., <volume>8</volume>, <fpage>129</fpage>â<lpage>136</lpage>.<pub-id pub-id-type="pmid">24518621</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Guo</surname><given-names>J.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2021</year>) <article-title>VirSorter2: a multi-classifier, expert-guided approach to detect diverse DNA and RNA viruses</article-title>. <source>Microbiome</source>, <volume>9</volume>, <fpage>1</fpage>â<lpage>13</lpage>.<pub-id pub-id-type="pmid">33388088</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B17">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gururangan</surname><given-names>S.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) Donât stop pretraining: adapt language models to domains and tasks. <italic toggle="yes">arXiv preprint arXiv:2004.10964</italic>. <pub-id pub-id-type="doi">10.48550/arXiv.2004.10964</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac509-B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ho</surname><given-names>S.F.S.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2021</year>) Comprehensive benchmarking of tools to identify phages in metagenomic shotgun sequencing data. <italic toggle="yes">bioRxiv</italic>. https://doi.org/10.1101/2021.04.12.438782.</mixed-citation>
    </ref>
    <ref id="btac509-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hyatt</surname><given-names>D.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2010</year>) <article-title>Prodigal: prokaryotic gene recognition and translation initiation site identification</article-title>. <source>BMC Bioinformatics</source>, <volume>11</volume>, <fpage>119</fpage>â<lpage>111</lpage>.<pub-id pub-id-type="pmid">20211023</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Japkowicz</surname><given-names>N.</given-names></string-name>, <string-name><surname>Stephen</surname><given-names>S.</given-names></string-name></person-group> (<year>2002</year>) <article-title>The class imbalance problem: a systematic study</article-title>. <source>IDA</source>, <volume>6</volume>, <fpage>429</fpage>â<lpage>449</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ji</surname><given-names>Y.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2021</year>) <article-title>DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>2112</fpage>â<lpage>2120</lpage>.<pub-id pub-id-type="pmid">33538820</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kieft</surname><given-names>K.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) <article-title>Vibrant: automated recovery, annotation and curation of microbial viruses, and evaluation of viral community function from genomic sequences</article-title>. <source>Microbiome</source>, <volume>8</volume>, <fpage>1</fpage>â<lpage>23</lpage>.<pub-id pub-id-type="pmid">31901242</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kortright</surname><given-names>K.E.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2019</year>) <article-title>Phage therapy: a renewed approach to combat antibiotic-resistant bacteria</article-title>. <source>Cell Host Microbe</source>., <volume>25</volume>, <fpage>219</fpage>â<lpage>232</lpage>.<pub-id pub-id-type="pmid">30763536</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kutter</surname><given-names>E.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2010</year>) <article-title>Phage therapy in clinical practice: treatment of human infections</article-title>. <source>Curr. Pharm. Biotechnol</source>., <volume>11</volume>, <fpage>69</fpage>â<lpage>86</lpage>.<pub-id pub-id-type="pmid">20214609</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laxminarayan</surname><given-names>R.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2013</year>) <article-title>Antibiotic resistance-the need for global solutions</article-title>. <source>Lancet. Infect. Dis</source>., <volume>13</volume>, <fpage>1057</fpage>â<lpage>1098</lpage>.<pub-id pub-id-type="pmid">24252483</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lecun</surname><given-names>Y.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>1998</year>) <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc. IEEE</source>, <volume>86</volume>, <fpage>2278</fpage>â<lpage>2324</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lessa</surname><given-names>F.C.</given-names></string-name></person-group> Â <etal>et al</etal>; <collab>Emerging Infections Program C. difficile Surveillance Team</collab>. (<year>2015</year>) <article-title>Burden of clostridium difficile infection in the United States</article-title>. <source>N. Engl. J. Med</source>., <volume>372</volume>, <fpage>2369</fpage>â<lpage>2370</lpage>.<pub-id pub-id-type="pmid">26061850</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lin</surname><given-names>D.M.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2017</year>) <article-title>Phage therapy: an alternative to antibiotics in the age of multi-drug resistance</article-title>. <source>World J. Gastrointest. Pharmacol. Ther</source>., <volume>8</volume>, <fpage>162</fpage>â<lpage>173</lpage>.<pub-id pub-id-type="pmid">28828194</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>P.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2021</year>) Pre-train, prompt, and predict: a systematic survey of prompting methods in natural language processing. <italic toggle="yes">arXiv preprint arXiv:2107.13586</italic>. <pub-id pub-id-type="doi">10.48550/arXiv.2107.13586</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac509-B30">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mao</surname><given-names>H.H.</given-names></string-name></person-group> (<year>2020</year>) A survey on self-supervised pre-training for sequential transfer learning in neural networks. <italic toggle="yes">arXiv preprint arXiv:2007.00800</italic>. https://doi.org/10.48550/arXiv.2007.00800.</mixed-citation>
    </ref>
    <ref id="btac509-B31">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Marquet</surname><given-names>M.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) What the phage: a scalable workflow for the identification and analysis of phage sequences. <italic toggle="yes">bioRxiv</italic>. <pub-id pub-id-type="doi">10.1101/2020.07.24.219899</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac509-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mirzaei</surname><given-names>M.K.</given-names></string-name>, <string-name><surname>Maurice</surname><given-names>C.F.</given-names></string-name></person-group> (<year>2017</year>) <article-title>MÃ©nage trois in the human gut: interactions between host, bacteria and phages</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>15</volume>, <fpage>397</fpage>â<lpage>408</lpage>.<pub-id pub-id-type="pmid">28461690</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Munita</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Arias</surname><given-names>C.A.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Mechanisms of antibiotic resistance</article-title>. <source>Microbiol. Spectr</source>., <volume>4</volume>, <fpage>4</fpage>â<lpage>2</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naseem</surname><given-names>U.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2021</year>) <article-title>A comprehensive survey on word representation models: from classical to state-of-the-art word representation language models</article-title>. <source>Trans. Asian Low-Resour. Lang. Inf. Process</source>., <volume>20</volume>, <fpage>1</fpage>â<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B35">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>OâShea</surname><given-names>K.</given-names></string-name>, <string-name><surname>Nash</surname><given-names>R.</given-names></string-name></person-group> (<year>2015</year>) An introduction to convolutional neural networks. <italic toggle="yes">arXiv preprint arXiv:1511.08458</italic>. <pub-id pub-id-type="doi">10.48550/arXiv.1511.08458</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac509-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname><given-names>F.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2011</year>) <article-title>Scikit-learn: machine learning in python</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2825</fpage>â<lpage>2830</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pires</surname><given-names>D.P.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) <article-title>Current challenges and future opportunities of phage therapy</article-title>. <source>FEMS Microbiol. Rev</source>., <volume>44</volume>, <fpage>684</fpage>â<lpage>700</lpage>.<pub-id pub-id-type="pmid">32472938</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B38">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Radford</surname><given-names>A.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2018</year>) <italic toggle="yes">Improving Language Understanding by Generative Pre-Training</italic>. <ext-link xlink:href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" ext-link-type="uri">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="btac509-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rao</surname><given-names>R.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2021</year>) Msa transformer. In: Marina,M. and Tong,Z. (eds) <source><italic toggle="yes">Proceedings of Machine Learning Research,</italic></source> Vol. 139, PMLR, pp. <fpage>8844</fpage>â<lpage>8856</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ren</surname><given-names>J.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) <article-title>Identifying viruses from metagenomic data using deep learning</article-title>. <source>Quant. Biol</source>., <volume>8</volume>, <fpage>64</fpage>â<lpage>77</lpage>.<pub-id pub-id-type="pmid">34084563</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Reyes</surname><given-names>A.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2012</year>) <article-title>Going viral: next-generation sequencing applied to phage populations in the human gut</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>10</volume>, <fpage>607</fpage>â<lpage>617</lpage>.<pub-id pub-id-type="pmid">22864264</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rodriguez-Valera</surname><given-names>F.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2009</year>) <article-title>Explaining microbial population genomics through phage predation</article-title>. <source>Nat. Rev. Microbiol</source>., <volume>7</volume>, <fpage>828</fpage>â<lpage>836</lpage>.<pub-id pub-id-type="pmid">19834481</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rohwer</surname><given-names>F.</given-names></string-name>, <string-name><surname>Thurber</surname><given-names>R.V.</given-names></string-name></person-group> (<year>2009</year>) <article-title>Viruses manipulate the marine environment</article-title>. <source>Nature</source>, <volume>459</volume>, <fpage>207</fpage>â<lpage>212</lpage>.<pub-id pub-id-type="pmid">19444207</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sulakvelidze</surname><given-names>A.</given-names></string-name></person-group> (<year>2005</year>) <article-title>Phage therapy: an attractive option for dealing with antibiotic-resistant bacterial infections</article-title>. <source>Drug Discov. Today</source>., <volume>10</volume>, <fpage>807</fpage>â<lpage>809</lpage>.<pub-id pub-id-type="pmid">15970258</pub-id></mixed-citation>
    </ref>
    <ref id="btac509-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thabtah</surname><given-names>F.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) <article-title>Data imbalance in classification: experimental evaluation</article-title>. <source>Inf. Sci</source>., <volume>513</volume>, <fpage>429</fpage>â<lpage>441</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2017</year>) <article-title>Attention is all you need</article-title>. In: <source>Advances in Neural Information Processing Systems</source>, Curran Associates, Inc., pp. <fpage>5998</fpage>â<lpage>6008</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B47">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wolf</surname><given-names>T.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2020</year>) Transformers: State-of-the-art natural language processing. In: <italic toggle="yes">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</italic>, Association for Computational Linguistics, pp. <fpage>38</fpage>â<lpage>45</lpage>.</mixed-citation>
    </ref>
    <ref id="btac509-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>D.</given-names></string-name></person-group> Â <etal>et al</etal> (<year>2018</year>) <article-title>Network representation learning: a survey</article-title>. <source>IEEE Trans. Big Data</source>, <volume>6</volume>, <fpage>3</fpage>â<lpage>28</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
