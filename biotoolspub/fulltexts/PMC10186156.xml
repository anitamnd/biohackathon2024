<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10186156</article-id>
    <article-id pub-id-type="pmid">34358294</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab576</article-id>
    <article-id pub-id-type="publisher-id">btab576</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Predicting correlated outcomes from molecular data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6498-4801</contrib-id>
        <name>
          <surname>Rauschenberger</surname>
          <given-names>Armin</given-names>
        </name>
        <xref rid="btab576-cor1" ref-type="corresp"/>
        <aff><institution>Luxembourg Centre for Systems Biomedicine (LCSB), University of Luxembourg</institution>, 4362 Esch-sur-Alzette, <country country="LU">Luxembourg</country></aff>
        <!--armin.rauschenberger@uni.lu-->
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3977-7469</contrib-id>
        <name>
          <surname>Glaab</surname>
          <given-names>Enrico</given-names>
        </name>
        <aff><institution>Luxembourg Centre for Systems Biomedicine (LCSB), University of Luxembourg</institution>, 4362 Esch-sur-Alzette, <country country="LU">Luxembourg</country></aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab576-cor1">To whom correspondence should be addressed. <email>armin.rauschenberger@uni.lu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-08-06">
      <day>06</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>06</day>
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>21</issue>
    <fpage>3889</fpage>
    <lpage>3895</lpage>
    <history>
      <date date-type="received">
        <day>01</day>
        <month>4</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>14</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="editorial-decision">
        <day>03</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>05</day>
        <month>8</month>
        <year>2021</year>
      </date>
      <date date-type="corrected-typeset">
        <day>25</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab576.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Multivariate (multi-target) regression has the potential to outperform univariate (single-target) regression at predicting correlated outcomes, which frequently occur in biomedical and clinical research. Here we implement multivariate lasso and ridge regression using stacked generalization.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Our flexible approach leads to predictive and interpretable models in high-dimensional settings, with a single estimate for each input–output effect. In the simulation, we compare the predictive performance of several state-of-the-art methods for multivariate regression. In the application, we use clinical and genomic data to predict multiple motor and non-motor symptoms in Parkinson’s disease patients. We conclude that stacked multivariate regression, with our adaptations, is a competitive method for predicting correlated outcomes.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The R package <monospace>joinet</monospace> is available on GitHub (<ext-link xlink:href="https://github.com/rauschenberger/joinet" ext-link-type="uri">https://github.com/rauschenberger/joinet</ext-link>) and <sc>cran</sc> (<ext-link xlink:href="https://cran.r-project.org/package=joinet" ext-link-type="uri">https://cran.r-project.org/package=joinet</ext-link>).</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Luxembourg National Research Fund</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Centre for Excellence in Research on Parkinson’s disease</institution>
          </institution-wrap>
        </funding-source>
        <award-id>11651464</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>European Union’s Horizon 2020 research and innovation programme</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2020-314</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Michael J. Fox Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000864</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="7"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>For clinical diagnosis and prognosis, multinomial (multiclass) classification is often more relevant than binary classification (<xref rid="btab576-B1" ref-type="bibr">Biesheuvel <italic toggle="yes">et al.</italic>, 2008</xref>; <xref rid="btab576-B8" ref-type="bibr">de Jong <italic toggle="yes">et al.</italic>, 2019</xref>), because it exploits and provides more information. Similarly, multivariate (multi-target) as compared to univariate (single-target) classification or prediction might often be more clinically relevant. Classifying patients into diseases that are not mutually exclusive, for example, requires a multivariate approach (cf. <xref rid="btab576-B26" ref-type="bibr">Vega, 2021</xref>). <xref rid="btab576-B23" ref-type="bibr">Teixeira-Pinto <italic toggle="yes">et al.</italic> (2009)</xref> explain why many applications involve multiple outputs rather than a single output: ‘lack of consensus on the most important [output]’, ‘desire to demonstrate effectiveness on [multiple outputs]’ and ‘disease complexity is often [better characterized by multiple outputs]’. Recent applications with multiple outputs include predicting mental illness and criminal behaviour of soldiers (<xref rid="btab576-B20" ref-type="bibr">Rosellini <italic toggle="yes">et al.</italic>, 2017</xref>), predicting various conditions of anaesthesia patients (<xref rid="btab576-B28" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2019</xref>) and predicting clinical outcomes after severe injury (<xref rid="btab576-B6" ref-type="bibr">Christie <italic toggle="yes">et al.</italic>, 2019</xref>). Although multiple outputs are commonly available, they are not commonly used for predictive modelling.</p>
    <p>In a prediction problem with multiple outputs, which may represent different symptoms of the same disease, we could fit one <italic toggle="yes">univariate</italic> regression for each output, or one <italic toggle="yes">multivariate</italic> regression for all outputs. Exploiting the correlation among outputs, multivariate regression potentially improves the prediction of the output(s) of interest. <xref rid="btab576-B27" ref-type="bibr">Waegeman <italic toggle="yes">et al.</italic> (2019)</xref> describes the use of stacked generalization (also known as ‘stacking’) for multivariate regression (<xref rid="btab576-B30" ref-type="bibr">Wolpert, 1992</xref>; <xref rid="btab576-B4" ref-type="bibr">Breiman and Friedman, 1997</xref>). Here we adapt this approach to ridge and lasso regression, which are generalized by elastic net regression (<xref rid="btab576-B32" ref-type="bibr">Zou and Hastie, 2005</xref>), in order to estimate interpretable and predictive models in high-dimensional settings. As an implementation, we provide the package <monospace>joinet</monospace> for the R statistical computing environment.</p>
    <p>Stacked multivariate regression involves multiple univariate regressions in two layers. In the base layer, we regress each output on all inputs, and in the meta layer, we regress each output on all cross-validated linear predictors from the base layer. Since combining linear predictors is equivalent to combining estimated coefficients, we construct a single estimate for each input–output effect (<xref rid="btab576-B19" ref-type="bibr">Rauschenberger <italic toggle="yes">et al.</italic>, 2021</xref>). Compared to multiple univariate regressions, stacked multivariate regression increases the predictive performance, while maintaining model interpretability. Thus, the proposed approach shares the benefits of ensemble learning methods in terms of predictivity without their usual limitations in terms of providing uninterpretable ‘black box’ models.</p>
    <p><xref rid="btab576-B11" ref-type="bibr">Friedman <italic toggle="yes">et al.</italic> (2010)</xref> have implemented elastic net regression (<xref rid="btab576-B32" ref-type="bibr">Zou and Hastie, 2005</xref>) for many univariate families and the multivariate Gaussian family (R package <monospace>glmnet</monospace>). We extend this implementation to multivariate outputs from the Gaussian, binomial and Poisson families through stacked generalization. Alternative multivariate predictive methods include multivariate adaptive regression splines (<xref rid="btab576-B10" ref-type="bibr">Friedman, 1991</xref>, R package <monospace>earth</monospace>), sparse partial least squares (<xref rid="btab576-B7" ref-type="bibr">Chung and Keles, 2010</xref>, R package <monospace>spls</monospace>), multivariate regression with covariance estimation (<xref rid="btab576-B21" ref-type="bibr">Rothman <italic toggle="yes">et al.</italic>, 2010</xref>, R package <monospace>MRCE</monospace>), regularized multivariate regression for identifying master predictors (<xref rid="btab576-B17" ref-type="bibr">Peng <italic toggle="yes">et al.</italic>, 2010</xref>, R package <monospace>remMap</monospace>), multivariate random forest (<xref rid="btab576-B22" ref-type="bibr">Segal and Xiao, 2011</xref>, R package <monospace>MultivariateRandomForest</monospace>), signal extraction for sparse multivariate regression (<xref rid="btab576-B12" ref-type="bibr">Luo and Qi, 2017</xref>, R package <monospace>SiER</monospace>), multivariate cluster elastic net (<xref rid="btab576-B18" ref-type="bibr">Price and Sherwood, 2017</xref>, R package <monospace>mcen</monospace>), Gaussian process modelling (<xref rid="btab576-B2" ref-type="bibr">Bostanabad <italic toggle="yes">et al.</italic>, 2018</xref>, R package <monospace>GPM</monospace>) and regularized multi-task learning (<xref rid="btab576-B5" ref-type="bibr">Cao <italic toggle="yes">et al.</italic>, 2019</xref>, R package <monospace>RMTL</monospace>). Furthermore, <xref rid="btab576-B31" ref-type="bibr">Xing <italic toggle="yes">et al.</italic> (2020)</xref> also implemented multi-task prediction using stacking (R package <monospace>MTPS</monospace>), but their approach does not meet our objective to increase the predictive performance without decreasing model interpretability (see Section 5).</p>
    <p>There are different prediction types for multivariate regression. When modelling two binary events, for example, we might want to predict the marginal probability of an event, i.e. <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> or <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, the joint probability of both events, i.e. <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, or the conditional probability of one event given the other event, i.e. <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> or <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. <xref rid="btab576-B29" ref-type="bibr">Wilkinson <italic toggle="yes">et al.</italic> (2021)</xref> illustrate marginal, joint and conditional (marginal or joint) prediction in the context of species distribution modelling. We focus on <italic toggle="yes">marginal</italic> prediction, i.e. exploiting the correlation between outputs to improve the prediction of each output separately. For modelling multiple binary outputs, however, <italic toggle="yes">joint</italic> prediction might be more relevant (<xref rid="btab576-B9" ref-type="bibr">Dudbridge, 2020</xref>), i.e. modelling the correlation between events to predict the simultaneous occurrence of multiple events. See <xref rid="btab576-B15" ref-type="bibr">Martin <italic toggle="yes">et al.</italic> (2021)</xref> for a comparison of approaches to marginal and joint prediction of multiple binary outputs.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p>Let the <italic toggle="yes">n </italic>×<italic toggle="yes"> p</italic> matrix <bold><italic toggle="yes">X</italic></bold> denote the inputs (e.g. clinical or molecular data), and let the <italic toggle="yes">n </italic>×<italic toggle="yes"> q</italic> matrix <bold><italic toggle="yes">Y</italic></bold> denote the outputs (e.g. multiple clinical measures), where <italic toggle="yes">n</italic> is the sample size, <italic toggle="yes">p</italic> is the number of inputs and <italic toggle="yes">q</italic> is the number of outputs. We will use the inputs (independent variables) to predict the outputs (dependent variables). Let <italic toggle="yes">i</italic> in <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> index the samples, <italic toggle="yes">j</italic> in <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> index the inputs, and <italic toggle="yes">k</italic> and<italic toggle="yes"> l</italic> in <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> index the outputs. For sample <italic toggle="yes">i</italic>, the entries <italic toggle="yes">X<sub>ij </sub></italic>and<italic toggle="yes"> Y<sub>ik</sub></italic> represent input <italic toggle="yes">j</italic> and output <italic toggle="yes">k</italic>, respectively. We allow for high-dimensional settings <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>≫</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and for outputs generated from different univariate distributions (Gaussian, binomial, Poisson).</p>
    <p>In the base layer, we regress each output on all inputs <bold><italic toggle="yes">X</italic></bold>. For any sample <italic toggle="yes">i</italic> and output <italic toggle="yes">k</italic>, the base model equals
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the link function (identity, logit, log), <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the unknown intercept and <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi></mml:mrow><mml:mi mathvariant="italic">k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊺</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are the unknown slopes. The slope <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the effect of input <italic toggle="yes">j</italic> on the linear predictor for output <italic toggle="yes">k</italic>. We estimate the <italic toggle="yes">q</italic> base models by maximizing the penalized likelihoods, under lasso <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> or ridge <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> regularization (<xref rid="btab576-B32" ref-type="bibr">Zou and Hastie, 2005</xref>), which render sparse or dense models, respectively.</p>
    <p>For each output, we tune the regularization parameter by <italic toggle="yes">k</italic>-fold cross-validation. Let the <italic toggle="yes">n </italic>×<italic toggle="yes"> q</italic> matrix <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>cv</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represent the cross-validated linear predictors (‘out-of-fold’), where <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>cv</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the entry for sample <italic toggle="yes">i</italic> and output <italic toggle="yes">k</italic>:
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>cv</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mo>κ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mo>κ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the superscript <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mo>−</mml:mo><mml:mo>κ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> indicates that the regression coefficients are estimated without using the fold including sample <italic toggle="yes">i</italic>. Using <italic toggle="yes">cross-validated</italic> rather than <italic toggle="yes">fitted</italic> linear predictors reduces leakage of information from the outputs to the inputs for the meta models (level-one data).</p>
    <p>In the meta layer, we regress each output on all linear predictors <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>cv</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. For any sample <italic toggle="yes">i</italic> and output <italic toggle="yes">k</italic>, the meta model equals
<disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mo>ω</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mo>ω</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>cv</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ω</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the unknown intercept and <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">ω</mml:mi></mml:mrow><mml:mi mathvariant="italic">k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mo>ω</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ω</mml:mo></mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊺</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are the unknown slopes. The slope <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ω</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represents the effect of the cross-validated linear predictor for output <italic toggle="yes">l</italic> from the base model on the linear predictor for output <italic toggle="yes">k</italic> in the meta model. We estimate the <italic toggle="yes">q</italic> meta models under lasso <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> regularization to avoid overfitting.</p>
    <p>In many applications, it is reasonable to assume that all pairwise combinations of outputs (e.g. different measures for disease severity) are <italic toggle="yes">positively</italic> correlated, i.e. <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ρ</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for all <italic toggle="yes">k</italic> and<italic toggle="yes"> l</italic> in <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, potentially after additive inverse transformations of some outputs, i.e. <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>·</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for some <italic toggle="yes">k</italic> in <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. We then impose non-negativity constraints on the slopes of the meta models, i.e. <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for all <italic toggle="yes">k</italic> and<italic toggle="yes"> l</italic> in <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. Non-negativity constraints have proven useful in the case of strongly positively correlated predictors according to extensive simulation studies (<xref rid="btab576-B3" ref-type="bibr">Breiman, 1996</xref>).</p>
    <p>Given the estimated coefficients, we typically want to predict the outputs for previously unseen samples. The linear predictors of the meta learners combine the linear predictors of the base learners. For sample <italic toggle="yes">i</italic> and output <italic toggle="yes">k</italic>, the linear predictor equals
<disp-formula id="E4"><mml:math id="M4" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mo>η</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>⋆</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mo>⋆</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>p</mml:mi></mml:munderover><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>⋆</mml:mo></mml:msubsup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo> </mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mo>⋆</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>⋆</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Hence, <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mo>⋆</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> are the initial and final estimated effects of input <italic toggle="yes">j</italic> on the linear predictor for output <italic toggle="yes">k</italic>, respectively, meaning that the final models have the same intuitive interpretation as the initial models in terms of input–output effects. For each input, stacking exchanges information among the estimated effects on the outputs, such that the final estimated effect on <italic toggle="yes">one</italic> output linearly combines the initial estimated effects on <italic toggle="yes">all</italic> outputs (<xref rid="btab576-F1" ref-type="fig">Fig. 1</xref>).</p>
    <fig position="float" id="btab576-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>To estimate the effect of input <italic toggle="yes">j</italic> on the linear predictor for output <italic toggle="yes">k</italic>, we first estimate the effects of input <italic toggle="yes">j</italic> on the linear predictor for each output (base layer) and then estimate the effects of all cross-validated linear predictors on the linear predictor for output <italic toggle="yes">k</italic> (meta layer)</p>
      </caption>
      <graphic xlink:href="btab576f1" position="float"/>
    </fig>
    <p>Next, we consider two extensions to make stacked multivariate regression more generally applicable.</p>
    <p>The first extension concerns <italic toggle="yes">input–output</italic> relationships: In some applications, we might want to exploit different inputs for modelling different outputs. For example, one group of inputs might be relevant for all outputs, but another group of inputs might only be relevant for some outputs. Let the <italic toggle="yes">p </italic>×<italic toggle="yes"> q</italic> matrix <bold><italic toggle="yes">W</italic></bold> indicate which inputs (rows) are relevant for which outputs (columns). Specifically, let the entry in the j<italic toggle="yes">th</italic> row and the <italic toggle="yes">k</italic>th column indicate whether the <italic toggle="yes">j</italic>th input may be used for modelling the <italic toggle="yes">k</italic>th output in the base layer, with <italic toggle="yes">W<sub>jk</sub></italic> = 0 meaning ‘no’ and <italic toggle="yes">W<sub>jk</sub></italic> = 1 meaning ‘yes’. If an input may not be used for modelling an output (<italic toggle="yes">W<sub>jk</sub></italic> = 0), the corresponding coefficient in the base model is set to zero (<inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). In the meta layer, however, each output is regressed on all cross-validated linear predictors from the base layer. This means that even if the univariate prediction for output <italic toggle="yes">k</italic> may not depend on input <italic toggle="yes">j</italic> (base model), the multivariate prediction for output <italic toggle="yes">k</italic> might still depend on input <italic toggle="yes">j</italic> (meta model).</p>
    <p>The second extension concerns <italic toggle="yes">output–output</italic> relationships: If all outputs are positively correlated, non-negativity constraints can help to introduce stability (see above). If some outputs are negatively correlated, however, we need to choose between not using any constraints and using non-negativity and non-positivity constraints. Let the <italic toggle="yes">q </italic>×<italic toggle="yes"> q</italic> matrix <bold><italic toggle="yes">V</italic></bold> represent these contraints, where the entry in the <italic toggle="yes">l</italic>th row and the <italic toggle="yes">k</italic>th column indicates how the <italic toggle="yes">l</italic>th output may be used for modelling the <italic toggle="yes">k</italic>th output, with <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> meaning ‘non-positive effect’, <italic toggle="yes">V<sub>lk</sub></italic> = 0 meaning ‘no effect’, <italic toggle="yes">V<sub>lk</sub></italic> = 1 meaning ‘non-negative effect’ and a missing value meaning ‘any effect’. The resulting constraints are <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for a non-positive effect, <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for no effect and <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>ω</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for a non-negative effect. While the diagonal elements of the matrix <bold><italic toggle="yes">V</italic></bold> equal one, the off-diagonal elements may represent known or estimated relationships between outputs. Tentatively, we could check whether the Spearman correlation coefficient (between outputs <italic toggle="yes">l</italic> and<italic toggle="yes"> k</italic>) is significantly negative (<inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>), insignificant (<italic toggle="yes">V<sub>lk</sub></italic> = 0) or significantly positive (<italic toggle="yes">V<sub>lk</sub></italic> = 1) at the 5% level.</p>
  </sec>
  <sec>
    <title>3 Simulation</title>
    <p>We report the simulation study using the <sc>ademp</sc> framework (<xref rid="btab576-B16" ref-type="bibr">Morris <italic toggle="yes">et al.</italic>, 2019</xref>):
</p>
    <list list-type="simple">
      <list-item>
        <p>• <bold>Aims:</bold> In this simulation study, we compare the (marginal) predictive performance of different approaches to multivariate regression.</p>
      </list-item>
      <list-item>
        <p>• <bold>Data-generating mechanisms:</bold> We repeatedly simulate data for <italic toggle="yes">n</italic> samples, <italic toggle="yes">p</italic> inputs, and <italic toggle="yes">q</italic> outputs, namely the <italic toggle="yes">n</italic> × <italic toggle="yes">p</italic> matrix <bold><italic toggle="yes">X</italic></bold> (inputs), the <italic toggle="yes">p</italic> × <italic toggle="yes">q</italic> matrix <bold><italic toggle="yes">B</italic></bold> (effects) and the <italic toggle="yes">n</italic> × <italic toggle="yes">q</italic> matrix <bold><italic toggle="yes">Y</italic></bold> (outputs), with a fixed random seed for reproducibility.</p>
      </list-item>
      <list-item>
        <p>  For the <italic toggle="yes">inputs</italic>, we simulate the <italic toggle="yes">n </italic>×<italic toggle="yes"> p</italic> matrix <bold><italic toggle="yes">X</italic></bold> from a multivariate Gaussian distribution with the constant mean 0 and the constant correlation <italic toggle="yes">ρ<sub>x</sub></italic>, where <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mo>ρ</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. For the <italic toggle="yes">effects</italic>, we simulate the <italic toggle="yes">p </italic>×<italic toggle="yes"> q</italic> matrix <bold><italic toggle="yes">B</italic></bold> from a multivariate Gaussian distribution with the constant mean 0 and the constant correlation <italic toggle="yes">ρ<sub>b</sub></italic>, where <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mo>ρ</mml:mo></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. In each column, we leave the <italic toggle="yes">m</italic> largest values unchanged, and set the <italic toggle="yes">p−m</italic> smallest values equal to 0, where <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>m</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>. Then the entry in row <italic toggle="yes">j</italic> and column <italic toggle="yes">k</italic> of <bold><italic toggle="yes">B</italic></bold> indicates whether input <italic toggle="yes">j</italic> affects output <italic toggle="yes">k</italic>, where <italic toggle="yes">j</italic> in <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">k</italic> in <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. For the <italic toggle="yes">outputs</italic>, we calculate the <italic toggle="yes">n </italic>×<italic toggle="yes"> q</italic> linear predictor matrix <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">XB</mml:mi></mml:mrow></mml:math></inline-formula>, column-standardize <bold><italic toggle="yes">H</italic></bold> and simulate the <italic toggle="yes">n </italic>×<italic toggle="yes"> q</italic> error matrix <bold><italic toggle="yes">E</italic></bold> from a standard Gaussian distribution. We then obtain the <italic toggle="yes">n </italic>×<italic toggle="yes"> q</italic> matrix <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>0.8</mml:mn></mml:mrow></mml:msqrt><mml:mi mathvariant="bold-italic">H</mml:mi><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>0.2</mml:mn></mml:mrow></mml:msqrt><mml:mi mathvariant="bold-italic">E</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      </list-item>
      <list-item>
        <p>  We considered low-dimensional settings, sparse high-dimensional settings and dense high-dimensional settings. While the low-dimensional settings involve <italic toggle="yes">p </italic>=<italic toggle="yes"> </italic>10 inputs with <italic toggle="yes">m </italic>=<italic toggle="yes"> </italic>5 effects on each output, the sparse and dense high-dimensional settings involve <italic toggle="yes">p </italic>=<italic toggle="yes"> </italic>500 inputs with <italic toggle="yes">m </italic>=<italic toggle="yes"> </italic>10 or <italic toggle="yes">m </italic>=<italic toggle="yes"> </italic>100 effects on each output, respectively. All settings involve <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>10100</mml:mn></mml:mrow></mml:math></inline-formula> samples and <italic toggle="yes">q </italic>=<italic toggle="yes"> </italic>3 outputs. We also varied the correlation between inputs and the correlation between effects. The correlation between inputs <italic toggle="yes">ρ<sub>x</sub></italic> takes values in <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.3</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, and the correlation between effects <italic toggle="yes">ρ<sub>b</sub></italic> takes values in <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. This leads to <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>=</mml:mo><mml:mn>27</mml:mn></mml:mrow></mml:math></inline-formula> settings in total, with various degrees of correlation between outputs.</p>
      </list-item>
      <list-item>
        <p>• <bold>Estimands/targets:</bold> We trained and validated the models with <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> samples, and tested the models with <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo> </mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></inline-formula> samples, with 10 repetitions for each setting. This means that models are tested on previously unseen samples (‘holdout’). In each of the 270 iterations (27 settings times 10 repetitions), let <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">0</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">0</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the training data, and let <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">1</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">1</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denote the testing data. For each model, we compare its predicted outputs (<inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">1</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>) with the true outputs (<inline-formula id="IE58"><mml:math id="IM58" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">1</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>).</p>
      </list-item>
      <list-item>
        <p>• <bold>Methods:</bold> We compared the proposed method (<monospace>joinet</monospace>) with one univariate method (<monospace>glmnet</monospace>) and eleven multivariate methods (<monospace>glmnet</monospace>, <monospace>earth</monospace>, <monospace>spls</monospace>, <monospace>MRCE</monospace>, <monospace>remMap</monospace>, <monospace>MultivariateRandomForest</monospace>, <monospace>SiER</monospace>, <monospace>mcen</monospace>, <monospace>GPM</monospace>, <monospace>RMTL</monospace>, <monospace>MTPS</monospace>).</p>
      </list-item>
      <list-item>
        <p>  For standard univariate and multivariate regression (<monospace>glmnet</monospace>) and the base learners of stacked multivariate regression (<monospace>joinet</monospace>, <monospace>MTPS</monospace>), we used lasso regularization in the low-dimensional and the sparse high-dimensional settings, and ridge regularization in the dense high-dimensional settings.</p>
      </list-item>
      <list-item>
        <p> We aimed at comparable hyperparameter optimization, but this is too computationally expensive for three methods (<monospace>MultivariateRandomForest</monospace>, <monospace>SiER</monospace>, <monospace>mcen</monospace>). For internal cross-validation, we used the same 10 folds (<monospace>glmnet</monospace>, <monospace>joinet</monospace>, <monospace>mcen</monospace>), other 10 folds (<monospace>spls</monospace>, <monospace>MRCE</monospace>,<monospace> MTPS</monospace>, <monospace>remMap</monospace>, <monospace>RMTL</monospace>), because the implementations let the user choose the number of folds but not the fold identifiers, or 3 folds (<monospace>SiER</monospace>) due to the computational expense. We performed grid searches, specifically for <monospace>glmnet</monospace> and <monospace>joinet</monospace>: data-dependent sequence of 100 regularization parameters; <monospace>spls</monospace>: number of hidden components in <inline-formula id="IE59"><mml:math id="IM59" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> and thresholding parameter in <inline-formula id="IE60"><mml:math id="IM60" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>0.9</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> (i.e. 10 × 10); <monospace>MRCE</monospace>: both penalty parameters in <inline-formula id="IE61"><mml:math id="IM61" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> (i.e. 11 × 11); <monospace>remMap</monospace>: both penalty parameters in <inline-formula id="IE62"><mml:math id="IM62" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mn>5</mml:mn></mml:msup><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> (i.e. 11 × 11); <monospace>mcen</monospace>: sequence of five penalty parameters, one possible cluster and cluster parameter in <inline-formula id="IE63"><mml:math id="IM63" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1.1</mml:mn><mml:mo>,</mml:mo><mml:mn>2.1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>5.1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> (i.e. 5 × 6); and <monospace>RMTL</monospace>: both penalty parameters in <inline-formula id="IE64"><mml:math id="IM64" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> (i.e. 12 × 12). For <monospace>MultivariateRandomForest</monospace>, we used 100 trees, 5 features for each split and 5 samples for each node. For <monospace>MTPS</monospace>, we chose cross-validation residual stacking (<xref rid="btab576-B31" ref-type="bibr">Xing <italic toggle="yes">et al.</italic>, 2020</xref>), either ridge or lasso regression for the base learners (see above), and regression trees for the meta learner (<xref rid="btab576-B31" ref-type="bibr">Xing <italic toggle="yes">et al.</italic>, 2020</xref>), but a potential limitation is the ‘one-standard-error rule’ for the base learners (see Section 5).</p>
      </list-item>
      <list-item>
        <p>• <bold>Performance measures:</bold> We measured the predictive performance based on the mean squared error of the testing samples, i.e. <inline-formula id="IE65"><mml:math id="IM65" display="inline" overflow="scroll"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>×</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>q</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow/><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, but we divided the mean squared errors of all methods by the mean squared error of prediction by the mean (empty model, intercept-only model). These re-scaled mean squared errors are more comparable between different simulation settings, because 0% means that the predictions are perfect and 100% means that the predictions are as poor as those from prediction by the mean. For each method, we obtained 27 × 10 re-scaled mean squared errors (27 settings, 10 repetitions).</p>
      </list-item>
      <list-item>
        <p>  For each setting and each repetition (27 × 10), we ranked the 12 multivariate methods by the re-scaled mean squared error. According to the mean rank, stacked multivariate regression (<monospace>joinet</monospace>) is among the top three most predictive methods in the low-dimensional settings (<monospace>joinet</monospace>: 2.1, <monospace>glmnet</monospace>: 2.8, <monospace>GPM</monospace>: 3.9), the sparse high-dimensional settings (<monospace>joinet</monospace>: 1.8, <monospace>mcen</monospace>: 3.0, <monospace>spls</monospace>: 3.4) and the dense high-dimensional settings (<monospace>joinet</monospace>: 2.1, <monospace>spls</monospace>: 2.6, <monospace>RMTL</monospace>: 3.2). For each of the 27 settings, we examined the 10 paired differences in re-scaled mean squared error between stacked multivariate regression and univariate regression. According to the one-sided Wilcoxon-signed rank test at the 5% level, stacked multivariate regression is significantly more predictive than univariate regression in 21 settings, namely in 6 low-dimensional, 8 sparse high-dimensional and 7 dense high-dimensional settings. As compared to the other multivariate methods, stacked multivariate regression outperforms univariate regression in more settings (<monospace>joinet</monospace>: 21, <monospace>spls</monospace>: 10, <monospace>RMTL</monospace>: 8, others: <inline-formula id="IE66"><mml:math id="IM66" display="inline" overflow="scroll"><mml:mrow><mml:mo>≤</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula>). <xref rid="btab576-T1" ref-type="table">Table 1</xref> summarizes the re-scaled mean squared errors for each setting and each method (mean over 10 repetitions). We conclude that stacked multivariate regression leads to a competitive predictive performance.
</p>
      </list-item>
    </list>
    <table-wrap position="float" id="btab576-T1">
      <label>Table 1.</label>
      <caption>
        <p>Mean loss of different models (re-scaled mean squared error, mean over 10 repetitions) in low-dimensional (top), sparse high-dimensional (centre) and dense high-dimensional (bottom) settings</p>
      </caption>
      <table frame="hsides" rules="groups">
        <colgroup span="1">
          <col valign="top" align="left" span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
          <col valign="top" align="char" char="." span="1"/>
        </colgroup>
        <thead>
          <tr>
            <th rowspan="1" colspan="1">
              <italic toggle="yes">ρ<sub>x</sub></italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic toggle="yes">ρ<sub>b</sub></italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic toggle="yes">ρ<sub>y</sub></italic>
            </th>
            <th rowspan="1" colspan="1">glmnet<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></th>
            <th rowspan="1" colspan="1">joinet</th>
            <th rowspan="1" colspan="1">glmnet<xref rid="tblfn1" ref-type="table-fn"><sup>b</sup></xref></th>
            <th rowspan="1" colspan="1">earth</th>
            <th rowspan="1" colspan="1">spls</th>
            <th rowspan="1" colspan="1">MRCE</th>
            <th rowspan="1" colspan="1">remMap</th>
            <th rowspan="1" colspan="1">MRF<xref rid="tblfn1" ref-type="table-fn"><sup>c</sup></xref></th>
            <th rowspan="1" colspan="1">SiER</th>
            <th rowspan="1" colspan="1">mcen</th>
            <th rowspan="1" colspan="1">GPM</th>
            <th rowspan="1" colspan="1">RMTL</th>
            <th rowspan="1" colspan="1">MTPS</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">20.6</td>
            <td rowspan="1" colspan="1">
              <underline>20.4</underline>
            </td>
            <td rowspan="1" colspan="1">21.0</td>
            <td rowspan="1" colspan="1">25.2</td>
            <td rowspan="1" colspan="1">21.1</td>
            <td rowspan="1" colspan="1">20.7</td>
            <td rowspan="1" colspan="1">32.7</td>
            <td rowspan="1" colspan="1">52.5</td>
            <td rowspan="1" colspan="1">21.2</td>
            <td rowspan="1" colspan="1">23.4</td>
            <td rowspan="1" colspan="1">21.8</td>
            <td rowspan="1" colspan="1">21.1</td>
            <td rowspan="1" colspan="1">22.7</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.6</td>
            <td rowspan="1" colspan="1">21.0</td>
            <td rowspan="1" colspan="1">21.1</td>
            <td rowspan="1" colspan="1">20.9</td>
            <td rowspan="1" colspan="1">25.4</td>
            <td rowspan="1" colspan="1">21.1</td>
            <td rowspan="1" colspan="1">21.7</td>
            <td rowspan="1" colspan="1">33.9</td>
            <td rowspan="1" colspan="1">41.3</td>
            <td rowspan="1" colspan="1">21.6</td>
            <td rowspan="1" colspan="1">22.9</td>
            <td rowspan="1" colspan="1">
              <underline>20.9</underline>
            </td>
            <td rowspan="1" colspan="1">21.5</td>
            <td rowspan="1" colspan="1">22.7</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">21.7</td>
            <td rowspan="1" colspan="1">
              <underline>21.6</underline>
            </td>
            <td rowspan="1" colspan="1">21.7</td>
            <td rowspan="1" colspan="1">24.2</td>
            <td rowspan="1" colspan="1">21.8</td>
            <td rowspan="1" colspan="1">21.9</td>
            <td rowspan="1" colspan="1">27.2</td>
            <td rowspan="1" colspan="1">38.4</td>
            <td rowspan="1" colspan="1">21.9</td>
            <td rowspan="1" colspan="1">22.3</td>
            <td rowspan="1" colspan="1">21.7</td>
            <td rowspan="1" colspan="1">22.0</td>
            <td rowspan="1" colspan="1">24.1</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.4</td>
            <td rowspan="1" colspan="1">21.6</td>
            <td rowspan="1" colspan="1">
              <underline>21.3</underline>
            </td>
            <td rowspan="1" colspan="1">21.6</td>
            <td rowspan="1" colspan="1">24.0</td>
            <td rowspan="1" colspan="1">21.8</td>
            <td rowspan="1" colspan="1">22.0</td>
            <td rowspan="1" colspan="1">41.5</td>
            <td rowspan="1" colspan="1">44.1</td>
            <td rowspan="1" colspan="1">21.5</td>
            <td rowspan="1" colspan="1">22.9</td>
            <td rowspan="1" colspan="1">21.6</td>
            <td rowspan="1" colspan="1">21.5</td>
            <td rowspan="1" colspan="1">23.3</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.2</td>
            <td rowspan="1" colspan="1">21.6</td>
            <td rowspan="1" colspan="1">21.7</td>
            <td rowspan="1" colspan="1">21.8</td>
            <td rowspan="1" colspan="1">28.2</td>
            <td rowspan="1" colspan="1">21.5</td>
            <td rowspan="1" colspan="1">
              <underline>21.5</underline>
            </td>
            <td rowspan="1" colspan="1">23.6</td>
            <td rowspan="1" colspan="1">47.7</td>
            <td rowspan="1" colspan="1">22.2</td>
            <td rowspan="1" colspan="1">23.7</td>
            <td rowspan="1" colspan="1">21.9</td>
            <td rowspan="1" colspan="1">22.0</td>
            <td rowspan="1" colspan="1">23.3</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.6</td>
            <td rowspan="1" colspan="1">21.0</td>
            <td rowspan="1" colspan="1">21.0</td>
            <td rowspan="1" colspan="1">21.3</td>
            <td rowspan="1" colspan="1">25.4</td>
            <td rowspan="1" colspan="1">22.4</td>
            <td rowspan="1" colspan="1">
              <underline>20.5</underline>
            </td>
            <td rowspan="1" colspan="1">27.9</td>
            <td rowspan="1" colspan="1">33.6</td>
            <td rowspan="1" colspan="1">21.9</td>
            <td rowspan="1" colspan="1">21.1</td>
            <td rowspan="1" colspan="1">21.5</td>
            <td rowspan="1" colspan="1">21.9</td>
            <td rowspan="1" colspan="1">21.4</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">20.9</td>
            <td rowspan="1" colspan="1">20.7</td>
            <td rowspan="1" colspan="1">20.7</td>
            <td rowspan="1" colspan="1">21.6</td>
            <td rowspan="1" colspan="1">21.2</td>
            <td rowspan="1" colspan="1">21.7</td>
            <td rowspan="1" colspan="1">23.9</td>
            <td rowspan="1" colspan="1">41.1</td>
            <td rowspan="1" colspan="1">21.6</td>
            <td rowspan="1" colspan="1">23.1</td>
            <td rowspan="1" colspan="1">
              <underline>20.6</underline>
            </td>
            <td rowspan="1" colspan="1">20.7</td>
            <td rowspan="1" colspan="1">21.4</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">20.8</td>
            <td rowspan="1" colspan="1">
              <underline>20.6</underline>
            </td>
            <td rowspan="1" colspan="1">20.6</td>
            <td rowspan="1" colspan="1">23.4</td>
            <td rowspan="1" colspan="1">21.0</td>
            <td rowspan="1" colspan="1">21.6</td>
            <td rowspan="1" colspan="1">23.6</td>
            <td rowspan="1" colspan="1">37.4</td>
            <td rowspan="1" colspan="1">21.5</td>
            <td rowspan="1" colspan="1">22.9</td>
            <td rowspan="1" colspan="1">20.6</td>
            <td rowspan="1" colspan="1">20.7</td>
            <td rowspan="1" colspan="1">22.1</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">20.7</td>
            <td rowspan="1" colspan="1">20.4</td>
            <td rowspan="1" colspan="1">
              <underline>20.4</underline>
            </td>
            <td rowspan="1" colspan="1">22.3</td>
            <td rowspan="1" colspan="1">21.0</td>
            <td rowspan="1" colspan="1">21.5</td>
            <td rowspan="1" colspan="1">23.2</td>
            <td rowspan="1" colspan="1">32.3</td>
            <td rowspan="1" colspan="1">20.5</td>
            <td rowspan="1" colspan="1">21.2</td>
            <td rowspan="1" colspan="1">20.6</td>
            <td rowspan="1" colspan="1">20.9</td>
            <td rowspan="1" colspan="1">22.1</td>
          </tr>
          <tr>
            <td colspan="16" rowspan="1">
              <hr/>
            </td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">24.7</td>
            <td rowspan="1" colspan="1">
              <underline>22.9</underline>
            </td>
            <td rowspan="1" colspan="1">29.1</td>
            <td rowspan="1" colspan="1">49.2</td>
            <td rowspan="1" colspan="1">26.5</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">41.5</td>
            <td rowspan="1" colspan="1">98.2</td>
            <td rowspan="1" colspan="1">31.4</td>
            <td rowspan="1" colspan="1">27.2</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">30.1</td>
            <td rowspan="1" colspan="1">29.6</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.2</td>
            <td rowspan="1" colspan="1">26.5</td>
            <td rowspan="1" colspan="1">25.5</td>
            <td rowspan="1" colspan="1">29.0</td>
            <td rowspan="1" colspan="1">35.4</td>
            <td rowspan="1" colspan="1">
              <underline>21.8</underline>
            </td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">37.6</td>
            <td rowspan="1" colspan="1">84.0</td>
            <td rowspan="1" colspan="1">38.8</td>
            <td rowspan="1" colspan="1">26.5</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">67.0</td>
            <td rowspan="1" colspan="1">30.2</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">26.6</td>
            <td rowspan="1" colspan="1">26.2</td>
            <td rowspan="1" colspan="1">28.2</td>
            <td rowspan="1" colspan="1">32.5</td>
            <td rowspan="1" colspan="1">37.9</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">49.8</td>
            <td rowspan="1" colspan="1">57.7</td>
            <td rowspan="1" colspan="1">36.6</td>
            <td rowspan="1" colspan="1">
              <underline>25.6</underline>
            </td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">46.8</td>
            <td rowspan="1" colspan="1">29.1</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">28.4</td>
            <td rowspan="1" colspan="1">23.6</td>
            <td rowspan="1" colspan="1">30.6</td>
            <td rowspan="1" colspan="1">48.8</td>
            <td rowspan="1" colspan="1">
              <underline>23.0</underline>
            </td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">47.8</td>
            <td rowspan="1" colspan="1">97.3</td>
            <td rowspan="1" colspan="1">27.2</td>
            <td rowspan="1" colspan="1">30.1</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">32.2</td>
            <td rowspan="1" colspan="1">34.0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.2</td>
            <td rowspan="1" colspan="1">26.2</td>
            <td rowspan="1" colspan="1">
              <underline>24.8</underline>
            </td>
            <td rowspan="1" colspan="1">29.6</td>
            <td rowspan="1" colspan="1">39.6</td>
            <td rowspan="1" colspan="1">34.6</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">42.4</td>
            <td rowspan="1" colspan="1">84.7</td>
            <td rowspan="1" colspan="1">45.5</td>
            <td rowspan="1" colspan="1">26.9</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">65.5</td>
            <td rowspan="1" colspan="1">29.4</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">26.5</td>
            <td rowspan="1" colspan="1">
              <underline>26.4</underline>
            </td>
            <td rowspan="1" colspan="1">30.7</td>
            <td rowspan="1" colspan="1">42.3</td>
            <td rowspan="1" colspan="1">39.4</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">33.3</td>
            <td rowspan="1" colspan="1">59.4</td>
            <td rowspan="1" colspan="1">47.6</td>
            <td rowspan="1" colspan="1">27.6</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">42.0</td>
            <td rowspan="1" colspan="1">30.3</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">27.5</td>
            <td rowspan="1" colspan="1">24.9</td>
            <td rowspan="1" colspan="1">28.3</td>
            <td rowspan="1" colspan="1">26.8</td>
            <td rowspan="1" colspan="1">
              <underline>23.8</underline>
            </td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">41.7</td>
            <td rowspan="1" colspan="1">97.5</td>
            <td rowspan="1" colspan="1">32.7</td>
            <td rowspan="1" colspan="1">28.8</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">28.2</td>
            <td rowspan="1" colspan="1">32.3</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">26.3</td>
            <td rowspan="1" colspan="1">25.4</td>
            <td rowspan="1" colspan="1">27.8</td>
            <td rowspan="1" colspan="1">27.7</td>
            <td rowspan="1" colspan="1">
              <underline>23.9</underline>
            </td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">35.1</td>
            <td rowspan="1" colspan="1">83.1</td>
            <td rowspan="1" colspan="1">32.0</td>
            <td rowspan="1" colspan="1">27.8</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">28.9</td>
            <td rowspan="1" colspan="1">30.0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.6</td>
            <td rowspan="1" colspan="1">
              <underline>25.9</underline>
            </td>
            <td rowspan="1" colspan="1">26.5</td>
            <td rowspan="1" colspan="1">26.5</td>
            <td rowspan="1" colspan="1">31.4</td>
            <td rowspan="1" colspan="1">33.8</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">36.5</td>
            <td rowspan="1" colspan="1">57.2</td>
            <td rowspan="1" colspan="1">34.7</td>
            <td rowspan="1" colspan="1">26.4</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">30.7</td>
            <td rowspan="1" colspan="1">28.4</td>
          </tr>
          <tr>
            <td colspan="16" rowspan="1">
              <hr/>
            </td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">89.2</td>
            <td rowspan="1" colspan="1">89.7</td>
            <td rowspan="1" colspan="1">89.4</td>
            <td rowspan="1" colspan="1">143.3</td>
            <td rowspan="1" colspan="1">89.5</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">99.1</td>
            <td rowspan="1" colspan="1">94.8</td>
            <td rowspan="1" colspan="1">97.4</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">
              <underline>86.9</underline>
            </td>
            <td rowspan="1" colspan="1">89.5</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.7</td>
            <td rowspan="1" colspan="1">27.5</td>
            <td rowspan="1" colspan="1">
              <underline>25.9</underline>
            </td>
            <td rowspan="1" colspan="1">28.4</td>
            <td rowspan="1" colspan="1">80.5</td>
            <td rowspan="1" colspan="1">27.8</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">42.6</td>
            <td rowspan="1" colspan="1">61.1</td>
            <td rowspan="1" colspan="1">29.4</td>
            <td rowspan="1" colspan="1">35.0</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">27.3</td>
            <td rowspan="1" colspan="1">27.8</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">22.3</td>
            <td rowspan="1" colspan="1">22.0</td>
            <td rowspan="1" colspan="1">22.3</td>
            <td rowspan="1" colspan="1">50.4</td>
            <td rowspan="1" colspan="1">
              <underline>21.8</underline>
            </td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">42.0</td>
            <td rowspan="1" colspan="1">37.6</td>
            <td rowspan="1" colspan="1">23.2</td>
            <td rowspan="1" colspan="1">25.2</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">23.1</td>
            <td rowspan="1" colspan="1">22.3</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.4</td>
            <td rowspan="1" colspan="1">89.1</td>
            <td rowspan="1" colspan="1">91.5</td>
            <td rowspan="1" colspan="1">89.5</td>
            <td rowspan="1" colspan="1">165.8</td>
            <td rowspan="1" colspan="1">
              <underline>88.9</underline>
            </td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">99.5</td>
            <td rowspan="1" colspan="1">92.6</td>
            <td rowspan="1" colspan="1">96.1</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">90.0</td>
            <td rowspan="1" colspan="1">99.8</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">28.4</td>
            <td rowspan="1" colspan="1">
              <underline>26.6</underline>
            </td>
            <td rowspan="1" colspan="1">29.5</td>
            <td rowspan="1" colspan="1">73.4</td>
            <td rowspan="1" colspan="1">27.0</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">64.1</td>
            <td rowspan="1" colspan="1">61.7</td>
            <td rowspan="1" colspan="1">28.0</td>
            <td rowspan="1" colspan="1">33.8</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">28.1</td>
            <td rowspan="1" colspan="1">28.0</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.5</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">21.8</td>
            <td rowspan="1" colspan="1">21.8</td>
            <td rowspan="1" colspan="1">21.9</td>
            <td rowspan="1" colspan="1">51.3</td>
            <td rowspan="1" colspan="1">
              <underline>21.6</underline>
            </td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">58.4</td>
            <td rowspan="1" colspan="1">37.4</td>
            <td rowspan="1" colspan="1">23.3</td>
            <td rowspan="1" colspan="1">24.7</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">22.3</td>
            <td rowspan="1" colspan="1">23.4</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.0</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.7</td>
            <td rowspan="1" colspan="1">90.7</td>
            <td rowspan="1" colspan="1">
              <underline>89.9</underline>
            </td>
            <td rowspan="1" colspan="1">91.4</td>
            <td rowspan="1" colspan="1">146.3</td>
            <td rowspan="1" colspan="1">91.8</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">99.3</td>
            <td rowspan="1" colspan="1">90.2</td>
            <td rowspan="1" colspan="1">99.5</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">92.3</td>
            <td rowspan="1" colspan="1">96.8</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.1</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">28.6</td>
            <td rowspan="1" colspan="1">
              <underline>26.5</underline>
            </td>
            <td rowspan="1" colspan="1">29.7</td>
            <td rowspan="1" colspan="1">73.6</td>
            <td rowspan="1" colspan="1">26.8</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">58.0</td>
            <td rowspan="1" colspan="1">62.1</td>
            <td rowspan="1" colspan="1">27.8</td>
            <td rowspan="1" colspan="1">33.0</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">27.7</td>
            <td rowspan="1" colspan="1">30.1</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">0.3</td>
            <td rowspan="1" colspan="1">0.9</td>
            <td rowspan="1" colspan="1">0.8</td>
            <td rowspan="1" colspan="1">22.7</td>
            <td rowspan="1" colspan="1">22.2</td>
            <td rowspan="1" colspan="1">22.8</td>
            <td rowspan="1" colspan="1">47.8</td>
            <td rowspan="1" colspan="1">22.8</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">45.2</td>
            <td rowspan="1" colspan="1">38.3</td>
            <td rowspan="1" colspan="1">23.1</td>
            <td rowspan="1" colspan="1">25.6</td>
            <td rowspan="1" colspan="1">100.0</td>
            <td rowspan="1" colspan="1">
              <underline>22.2</underline>
            </td>
            <td rowspan="1" colspan="1">22.5</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="tblfn1">
          <p><italic toggle="yes">Note</italic>: The first three columns indicate the correlation between inputs (<italic toggle="yes">ρ<sub>x</sub></italic>), the correlation between effects (<italic toggle="yes">ρ<sub>b</sub></italic>) and the resulting mean correlation between outputs (<italic toggle="yes">ρ<sub>y</sub></italic>). The other columns show the predictive performance of a univariate method (<monospace>glmnet</monospace><xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref>), the proposed multivariate method (<monospace>joinet</monospace>) and eleven other multivariate methods (<monospace>glmnet</monospace><xref rid="tblfn1" ref-type="table-fn"><sup>b</sup></xref>, <monospace>earth</monospace>, <monospace>spls</monospace>, <monospace>MRCE</monospace>, <monospace>remMap</monospace>, <monospace>MRF</monospace><xref rid="tblfn1" ref-type="table-fn"><sup>c</sup></xref>, <monospace>SiER</monospace>, <monospace>mcen</monospace>, <monospace>GPM</monospace>, <monospace>RMTL</monospace>, <monospace>MTPS</monospace>). For each setting (row), the colour black indicates which multivariate methods are more predictive than the univariate method (<monospace>glmnet</monospace><xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref>), and the underline indicates the most predictive method, based on the sharp (not rounded) numbers. <sup>a</sup>Univariate linear regression with <monospace>glmnet</monospace>. <sup>b</sup>Multivariate linear regression with <monospace>glmnet</monospace>. <sup>c</sup><monospace>MultivariateRandomForest</monospace>.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>Multivariate regression can be computationally expensive. As compared to <monospace>glmnet</monospace> and <monospace>joinet</monospace>, the mean computation time is shorter for <monospace>earth</monospace>, but about five times longer for <monospace>MRCE</monospace>, <monospace>remMap</monospace> and <monospace>MTPS</monospace>, about ten times longer for <monospace>RMTL</monospace>, <monospace>spls</monospace> and <monospace>mcen</monospace> and above twenty times longer for <monospace>SiER</monospace>, <monospace>GPM</monospace> and <monospace>MultivariateRandomForest</monospace>. The computational efficiency of <monospace>glmnet</monospace> and thereby <monospace>joinet</monospace> stems from regularization paths via coordinate descent (<xref rid="btab576-B11" ref-type="bibr">Friedman <italic toggle="yes">et al.</italic>, 2010</xref>).</p>
    <p>Another advantage of stacked multivariate regression (<monospace>joinet</monospace>) is its flexibility. First, it accepts multivariate outcomes from different families. Some of the alternative methods allow for different families in separate models but not in the same model. The current implementations accept continuous multivariate outcomes (<monospace>glmnet</monospace>, <monospace>spls</monospace>, <monospace>MRCE</monospace>, <monospace>remMap</monospace>, <monospace>MultivariateRandomForest</monospace>, <monospace>SiER</monospace>, <monospace>GPM</monospace>), either continuous or binary multivariate outcomes (<monospace>earth</monospace>, <monospace>mcen</monospace>, <monospace>RMTL</monospace>), or continuous and binary multivariate outcomes (<monospace>joinet</monospace>, <monospace>MTPS</monospace>). Second, it accepts missing values in the outcomes. An alternative would be to impute them by chained equations (<xref rid="btab576-B25" ref-type="bibr">van Buuren and Groothuis-Oudshoorn, 2011</xref>, R package <monospace>mice</monospace>) for the training data.</p>
  </sec>
  <sec>
    <title>4 Application</title>
    <p>We illustrate the application and assess the performance of stacked multivariate regression by analysing data from a clinical cohort study on Parkinson’s disease, which is part of the Parkinson’s Progression Markers Initiative (<sc>ppmi</sc>, <xref rid="btab576-B14" ref-type="bibr">Marek <italic toggle="yes">et al.</italic>, 2011</xref>). From clinical or genomic variables measured at baseline, we predict motor and non-motor symptoms measured at three follow-up visits. <xref rid="sup1" ref-type="supplementary-material">Supplementary Table A</xref> includes details on the pre-processing of the clinical and the genomic data.</p>
    <p>The outputs to predict are the (total) scores from the following clinical assessment tools: Montreal Cognitive Assessment (<sc>moca</sc>, adjusted for education), Questionnaire for Impulsive-Compulsive Disorders in Parkinson’s Disease (<sc>quip</sc>), Movement Disorder Society Unified Parkinson’s Disease Rating Scale (<sc>mds-updrs</sc>, ‘off’), Geriatric Depression Scale (<sc>gds</sc>), Scales for Outcomes in Parkinson’s Disease – Autonomic Dysfunction (<sc>scopa</sc>-<sc>aut</sc>), Epworth Sleepiness Scale (<sc>ess</sc>), Benton Judgement of Line Orientation Test (<sc>bjlot</sc>) and Rapid Eye Movement (<sc>rem</sc>) Sleep Behaviour Disorder Questionnaire. We take the additive inverse transform of the <sc>moca</sc> and <sc>bjlot</sc> scores <inline-formula id="IE67"><mml:math id="IM67" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>→</mml:mo><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> to ensure that all minima indicate ‘no symptoms’ and all maxima indicate ‘severe symptoms’ (which should render all pairwise combinations of outputs positively correlated). The inputs consist of 138 clinical variables (dataset 1) or <inline-formula id="IE68"><mml:math id="IM68" display="inline" overflow="scroll"><mml:mrow><mml:mn>17714</mml:mn></mml:mrow></mml:math></inline-formula>  <sc>rna-s</sc>eq gene expression variables reflecting measurements from whole-blood samples (dataset 2). We restrict the analysis to samples with both types of inputs (<italic toggle="yes">n </italic>=<italic toggle="yes"> </italic>242). Our aim is to build the most predictive model for each clinical assessment tool (<sc>m<sc>o</sc>c<sc>a</sc></sc>, <sc>quip</sc>, <sc>updrs</sc>, <sc>gds</sc>, <sc>scopa</sc>, <sc>ess</sc>, <sc>bjlot</sc>, <sc>rem</sc>) and each clinical follow-up visit (first, second, third), i.e. 8 × 3 = 24 prediction problems. (In the following, we use the short terms ‘tool’ and ‘visit’, respectively.) We consider three types of inputs (clinical, genomic, both) and two types of regularization (lasso, ridge), i.e. 3 × 2 = 6 modelling approaches. This leads to 24 × 6 = 144 univariate regression models.</p>
    <p><xref rid="btab576-F2" ref-type="fig">Figure 2</xref> summarizes the correlation between the outputs. Outputs from the same tool at different visits are strongly correlated (left), and outputs from different tools at the same visit are weakly correlated (right). The mean correlation between visits is strongest for <sc>scopa</sc> (Spearman’s <inline-formula id="IE69"><mml:math id="IM69" display="inline" overflow="scroll"><mml:mrow><mml:mo>ρ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.78</mml:mn></mml:mrow></mml:math></inline-formula>), and the mean correlation between tools is strongest for <sc>scopa</sc> and <sc>updrs</sc> (Spearman’s <inline-formula id="IE70"><mml:math id="IM70" display="inline" overflow="scroll"><mml:mrow><mml:mo>ρ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.43</mml:mn></mml:mrow></mml:math></inline-formula>). We suspect that, due to the correlated outputs, stacked multivariate regression has the potential to outperform univariate regression. In our two applications of multivariate regression, we share information among different clinical follow-up visits or between different clinical assessment tools, respectively, reflecting two common settings in clinical data analysis. In both cases, we first regress each output on all inputs, and then combine information from different outputs. In the first case, we support the prediction for one tool and one visit with the same tool at the other visits (‘support from other visits’), and in the second case, we support the prediction for one tool and one visit with another tool at the same visit (‘support from other tool’). Even if an output is not of interest itself, it can still support the prediction of other outputs, functioning as a ‘coaching variable’ (<xref rid="btab576-B24" ref-type="bibr">Tibshirani and Hinton, 1998</xref>).</p>
    <fig position="float" id="btab576-F2">
      <label>Fig. 2.</label>
      <caption>
        <p>Spearman’s rank correlation coefficients. Left: correlation between outputs from the same tool at different visits (averaged across combinations of visits). Right: correlation between outputs from different tools at the same visit (averaged across visits)</p>
      </caption>
      <graphic xlink:href="btab576f2" position="float"/>
    </fig>
    <p>We evaluate the predictive performance of univariate and multivariate regression by nested cross-validation, using <italic toggle="yes">internal</italic> cross-validation for hyperparameter optimization and <italic toggle="yes">external</italic> cross-validation for performance evaluation. While the holdout method would involve a single train-test split, external cross-validation involves multiple train-test splits. We first assign each sample to one external fold (out of five) and one internal fold (out of ten). In each external iteration (out of five), we train (parameter estimation) and validate (hyperparameter optimization) the models with four external folds (80%), and test (performance evaluation) the models with the other external fold (20%). The training and validation phase involves internal cross-validation. In each internal iteration (out of ten), we train the models with nine internal folds (<inline-formula id="IE71"><mml:math id="IM71" display="inline" overflow="scroll"><mml:mrow><mml:mn>80</mml:mn><mml:mo>%</mml:mo><mml:mo>×</mml:mo><mml:mn>90</mml:mn><mml:mo>%</mml:mo><mml:mo>=</mml:mo><mml:mn>72</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>) and keep the other internal fold (<inline-formula id="IE72"><mml:math id="IM72" display="inline" overflow="scroll"><mml:mrow><mml:mn>80</mml:mn><mml:mo>%</mml:mo><mml:mo>×</mml:mo><mml:mn>10</mml:mn><mml:mo>%</mml:mo><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>) for validation. After the last internal iteration in each external iteration, we tune the hyperparameters, and after the last external iteration, we evaluate the performance. This nested cross-validation scheme allows us to repeatedly use 80% of the samples for training and validation, and finally 100% of the samples for testing. We thereby test the methods on previously unseen data.</p>
    <p><xref rid="sup1" ref-type="supplementary-material">Supplementary Figures A and B</xref> show the percentage change in cross-validated mean squared error from univariate to multivariate regression. The loss tends to decrease more strongly for the second visit than for the first and third visits, more under lasso than ridge regularization and more with combined clinical and genomic data than either clinical or genomic data. <xref rid="btab576-F3" ref-type="fig">Figure 3</xref> shows the mean percentage change in mean squared error. In this application, jointly modelling different visits (left) is more beneficial than jointly modelling different tools (right). It is most beneficial to support the prediction of the <sc>moca</sc> score with the <sc>moca</sc> scores at the other visits (left), or to support the prediction of the <sc>updrs</sc> score with the <sc>moca</sc> score at the same visit (right). We also assessed the predictive performance relative to prediction by the mean. <xref rid="btab576-F4" ref-type="fig">Figure 4</xref> shows the percentage change in cross-validated mean squared error from prediction by the mean to prediction by univariate and multivariate regression. The improvement is best for <sc>moca</sc> (above 20%) and worst for <sc>quip</sc> (about 1%). We observe the improvement tends to be larger for multivariate regression than for univariate regression. Overall, these empirical analyses show that stacked multivariate regression can be an effective means to improve the predictive performance as compared to univariate regression, if suitable correlated outcome data is available.</p>
    <fig position="float" id="btab576-F3">
      <label>Fig. 3.</label>
      <caption>
        <p>Percentage change in cross-validated mean squared error from univariate to multivariate regression. Left: we support the prediction for one tool at one visit with the same tool at other visits. Right: we support the prediction for one tool at one visit (row) with another tool at the same visit (column). All values are averaged across visits (1/2/3), regularization methods (lasso/ridge) and data types (clinical/omics/both), i.e. 18 settings</p>
      </caption>
      <graphic xlink:href="btab576f3" position="float"/>
    </fig>
    <fig position="float" id="btab576-F4">
      <label>Fig. 4.</label>
      <caption>
        <p>Percentage change in cross-validated mean squared error from prediction by the mean to univariate (first row) and multivariate regression (second row: support from other visits; third row: support from other tool). All values are averaged across visits (1/2/3), regularization methods (lasso/ridge), data types (clinical/omics/both) and coaching variables (third row only), i.e. 18 settings (first and second row) or 18 × 7 = 126 settings (third row)</p>
      </caption>
      <graphic xlink:href="btab576f4" position="float"/>
    </fig>
  </sec>
  <sec>
    <title>5 Discussion</title>
    <p>Multivariate outputs frequently occur in biomedical and clinical research, because many symptoms and impairments associated with a complex disease cannot be captured by a single number. There are different sources of multivariate outputs: measurements of multiple attributes (e.g. different symptoms), multiple measurements of the same attribute (e.g. repeated measures) or multiple transformations of the same measurement (e.g. identity and logarithm).</p>
    <p>If the outputs are neither too weakly nor too strongly correlated, we expect stacked multivariate regression to be more predictive than univariate regression. As the strength of the signal also matters, we cannot provide any thresholds for the correlation. If the inputs predict one output very badly, this output cannot provide support to the other outputs. And if the inputs predict one output very well, this output does not require support from the other outputs. To find out whether multivariate regression outperforms univariate regression in a specific application, we propose to use the holdout method or cross-validation.</p>
    <p>Although the proposed multivariate model combines predictions from multiple univariate models, it has comparable interpretability to a univariate model, because stacking linear predictors is equivalent to pooling regression coefficients. The proposed method therefore shares the usual advantage of ensemble learning (high predictivity) but not the usual disadvantage (low interpretability). Though another approach to multi-target prediction (<xref rid="btab576-B31" ref-type="bibr">Xing <italic toggle="yes">et al.</italic>, 2020</xref>) can also combine two layers of penalized regression, our approach not only provides (i) a coefficient matrix that links the inputs to the <italic toggle="yes">univariate</italic> predictions and (ii) a coefficient matrix that links the univariate to the multivariate predictions but also a (iii) coefficient matrix that directly links the inputs to the <italic toggle="yes">multivariate</italic> predictions. This facilitates the biological interpretation of the statistical model. If non-linear effects might be important and if the aim is to maximize predictivity regardless of interpretability, however, we recommend the approach from <xref rid="btab576-B31" ref-type="bibr">Xing <italic toggle="yes">et al.</italic> (2020)</xref> with regression trees, quadratic discriminant analysis or <italic toggle="yes">k</italic>-nearest neighbour classification, because the proposed method only estimates linear effects.</p>
    <p>The one-standard-error rule normally renders penalized regression models more parsimonious but not significantly less predictive. We argue, however, that the one-standard-error rule should not be used in stacked multivariate regression. Although it normally does not make models significantly less predictive, it still makes them less predictive (unless there is overfitting). Whereas it affects each output only once in univariate regression, it affects each output multiple times in stacked multivariate regression (once in the meta learner and once in each included base learner). And multiple insignificant decreases in predictivity can sum up to a significant decrease. Therefore, in stacked multivariate regression, the one-standard-error rule might lead to significantly worse predictions.</p>
    <p>It would be interesting to extend stacked multivariate regression to settings with not only many inputs but also many outputs. This would for example be relevant for predicting gene expression values from other molecular data. <xref rid="btab576-B13" ref-type="bibr">Lutz and Bühlmann (2006)</xref> showed that multivariate modelling can outperform univariate modelling even if the number of outputs is high-dimensional. Our approach, however, involves cross-validating two regression models for each output, which would be too computationally expensive in this case.</p>
    <p>The proposed method is of special interest for biomedical research because multiple outputs and high-dimensional inputs are becoming the rule rather than the exception in this domain. Our flexible approach allows for outputs with missing values, output-specific probability distributions and output-specific loss functions. It provides a general framework for prediction problems with multiple outputs and high-dimensional inputs. The R package <monospace>joinet</monospace> is available on GitHub (<ext-link xlink:href="https://github.com/rauschenberger/joinet" ext-link-type="uri">https://github.com/rauschenberger/joinet</ext-link>) and <sc>cran</sc> (<ext-link xlink:href="https://cran.r-project.org/package=joinet" ext-link-type="uri">https://cran.r-project.org/package=joinet</ext-link>).</p>
    <p>
      <boxed-text id="btab576-BOX1" position="float">
        <sec>
          <title>Key points</title>
          <list list-type="bullet">
            <list-item>
              <p>Biomedical prediction problems often include many inputs (e.g. molecular data) and multiple outputs (e.g. clinical data).</p>
            </list-item>
            <list-item>
              <p>Multivariate regression (‘multitasking’) outperforms univariate regression (‘single-tasking’) at predicting correlated outputs.</p>
            </list-item>
            <list-item>
              <p>Stacked multivariate regression leads to predictive and interpretable models in high-dimensional settings.</p>
            </list-item>
          </list>
        </sec>
      </boxed-text>
    </p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab576_Supplementary_Data</label>
      <media xlink:href="btab576_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors are grateful to Zhi Zhang for preparing the sequencing data, to Wessel van Wieringen for helpful discussions, to Sarah Peter for computational support, and to two anonymous reviewers for constructive criticism.</p>
    <sec sec-type="data-availability">
      <title>Data availability</title>
      <p>Data used in the preparation of this article were obtained from the <sc>ppmi</sc> database (<ext-link xlink:href="https://www.ppmi-info.org/data" ext-link-type="uri">https://www.ppmi-info.org/data</ext-link>). The R package <monospace>joinet</monospace> includes the code for the simulation and the application (<ext-link xlink:href="https://github.com/rauschenberger/joinet" ext-link-type="uri">https://github.com/rauschenberger/joinet</ext-link>). We obtained our results using R 4.0.4 with joinet 0.0.8 on a virtual machine (x86_64-redhat-linux-gnu, CentOS Linux 8).</p>
    </sec>
    <sec>
      <title>Funding</title>
      <p>This research was supported by the Luxembourg National Research Fund (<sc>fnr</sc>) as part of the National Centre for Excellence in Research on Parkinson’s disease (<sc>i</sc>1<sc>r</sc>-<sc>bic</sc>-<sc>pfn</sc>-15<sc>ncer</sc>) and the <sc>era</sc>-<sc>n</sc>et <sc>eracos</sc>y<sc>sm</sc>ed <sc>jtc</sc>-2 project <sc>pd</sc>-<sc>s</sc>trat (<sc>inter</sc>/11651464), and from the European Union’s Horizon 2020 research and innovation programme under the grant no. <sc>erapermed</sc> 2020-314 for the project <sc>digi-pd</sc>. <sc>ppmi</sc>—a public-private partnership—is funded by the Michael J. Fox Foundation for Parkinson’s Research and funding partners (<ext-link xlink:href="https://www.ppmi-info.org/fundingpartners" ext-link-type="uri">https://www.ppmi-info.org/fundingpartners</ext-link>).</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: The authors declare that they have no competing interests.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab576-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Biesheuvel</surname><given-names>C.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2008</year>) <article-title>Polytomous logistic regression analysis could be applied more often in diagnostic research</article-title>. <source>J. Clin. Epidemiol</source>., <volume>61</volume>, <fpage>125</fpage>–<lpage>134</lpage>.<pub-id pub-id-type="pmid">18177785</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bostanabad</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Leveraging the nugget parameter for efficient Gaussian process modeling</article-title>. <source>Int. J. Numer. Methods Eng</source>., <volume>114</volume>, <fpage>501</fpage>–<lpage>516</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname><given-names>L.</given-names></string-name></person-group> (<year>1996</year>) <article-title>Stacked regressions</article-title>. <source>Mach. Learn</source>., <volume>24</volume>, <fpage>49</fpage>–<lpage>64</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname><given-names>L.</given-names></string-name>, <string-name><surname>Friedman</surname><given-names>J.H.</given-names></string-name></person-group> (<year>1997</year>) <article-title>Predicting multivariate responses in multiple linear regression</article-title>. <source>J. R. Stat. Soc. Ser. B (Stat. Methodol.)</source>, <volume>59</volume>, <fpage>3</fpage>–<lpage>54</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cao</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>RMTL: an R library for multi-task learning</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>1797</fpage>–<lpage>1798</lpage>.<pub-id pub-id-type="pmid">30256897</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Christie</surname><given-names>S.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Dynamic multi-outcome prediction after injury: applying adaptive machine learning for precision medicine in trauma</article-title>. <source>PLoS One</source>, <volume>14</volume>, <fpage>e0213836</fpage>.<pub-id pub-id-type="pmid">30970030</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chung</surname><given-names>D.</given-names></string-name>, <string-name><surname>Keles</surname><given-names>S.</given-names></string-name></person-group> (<year>2010</year>) <article-title>Sparse partial least squares classification for high dimensional data</article-title>. <source>Stat. Appl. Genet. Mol. Biol</source>., <volume>9</volume>, <fpage>Article 17</fpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Jong</surname><given-names>V.M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Sample size considerations and predictive performance of multinomial logistic prediction models</article-title>. <source>Stat. Med</source>., <volume>38</volume>, <fpage>1601</fpage>–<lpage>1619</lpage>.<pub-id pub-id-type="pmid">30614028</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dudbridge</surname><given-names>F.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Criteria for evaluating risk prediction of multiple outcomes</article-title>. <source>Stat. Methods Med. Res</source>., <volume>29</volume>, <fpage>3492</fpage>–<lpage>3510</lpage>.<pub-id pub-id-type="pmid">32594841</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J.H.</given-names></string-name></person-group> (<year>1991</year>) <article-title>Multivariate adaptive regression splines</article-title>. <source>Ann. Stat</source>., <volume>19</volume>, <fpage>1</fpage>–<lpage>67</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J.H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>Regularization paths for generalized linear models via coordinate descent</article-title>. <source>J. Stat. Softw</source>., <volume>33</volume>, <fpage>1</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">20808728</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luo</surname><given-names>R.</given-names></string-name>, <string-name><surname>Qi</surname><given-names>X.</given-names></string-name></person-group> (<year>2017</year>) <article-title>Signal extraction approach for sparse multivariate response regression</article-title>. <source>J. Multivar. Stat</source>., <volume>153</volume>, <fpage>83</fpage>–<lpage>97</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lutz</surname><given-names>R.W.</given-names></string-name>, <string-name><surname>Bühlmann</surname><given-names>P.</given-names></string-name></person-group> (<year>2006</year>) <article-title>Boosting for high-multivariate responses in high-dimensional linear regression</article-title>. <source>Stat. Sin</source>., <volume>16</volume>, <fpage>471</fpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Marek</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal>; Parkinson Progression Marker Initiative. (<year>2011</year>) <article-title>The Parkinson Progression Marker Initiative (<sc>ppmi</sc>)</article-title>. <source>Progress Neurobiol</source>., <volume>95</volume>, <fpage>629</fpage>–<lpage>635</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Martin</surname><given-names>G.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Clinical prediction models to predict the risk of multiple binary outcomes: a comparison of approaches</article-title>. <source>Stat. Med</source>., <volume>40</volume>, <fpage>498</fpage>–<lpage>517</lpage>.<pub-id pub-id-type="pmid">33107066</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Morris</surname><given-names>T.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Using simulation studies to evaluate statistical methods</article-title>. <source>Stat. Med</source>., <volume>38</volume>, <fpage>2074</fpage>–<lpage>2102</lpage>.<pub-id pub-id-type="pmid">30652356</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peng</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer</article-title>. <source>Ann. Appl. Stat</source>., <volume>4</volume>, <fpage>53</fpage>–<lpage>77</lpage>.<pub-id pub-id-type="pmid">24489618</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Price</surname><given-names>B.S.</given-names></string-name>, <string-name><surname>Sherwood</surname><given-names>B.</given-names></string-name></person-group> (<year>2017</year>) <article-title>A cluster elastic net for multivariate regression</article-title>. <source>J. Mach. Learn. Res</source>., <volume>18</volume>, <fpage>1</fpage>–<lpage>39</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rauschenberger</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Predictive and interpretable models via the stacked elastic net</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>2012</fpage>–<lpage>2016</lpage>.<pub-id pub-id-type="pmid">32437519</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rosellini</surname><given-names>A.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Using self-report surveys at the beginning of service to develop multi-outcome risk models for new soldiers in the US Army</article-title>. <source>Psychol. Med</source>., <volume>47</volume>, <fpage>2275</fpage>–<lpage>2287</lpage>.<pub-id pub-id-type="pmid">28374665</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rothman</surname><given-names>A.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>Sparse multivariate regression with covariance estimation</article-title>. <source>J. Comput. Graph. Stat</source>., <volume>19</volume>, <fpage>947</fpage>–<lpage>962</lpage>.<pub-id pub-id-type="pmid">24963268</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Segal</surname><given-names>M.</given-names></string-name>, <string-name><surname>Xiao</surname><given-names>Y.</given-names></string-name></person-group> (<year>2011</year>) <article-title>Multivariate random forests</article-title>. <source>Wiley Interdiscip. Rev. Data Min. Knowledge Discov</source>., <volume>1</volume>, <fpage>80</fpage>–<lpage>87</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teixeira-Pinto</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) <article-title>Statistical approaches to modeling multiple outcomes in psychiatric studies</article-title>. <source>Psychiatric Ann</source>., <volume>39</volume>, <fpage>729</fpage>–<lpage>735</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tibshirani</surname><given-names>R.</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G.</given-names></string-name></person-group> (<year>1998</year>) <article-title>Coaching variables for regression and classification</article-title>. <source>Stat. Comput</source>., <volume>8</volume>, <fpage>25</fpage>–<lpage>33</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Buuren</surname><given-names>S.</given-names></string-name>, <string-name><surname>Groothuis-Oudshoorn</surname><given-names>K.</given-names></string-name></person-group> (<year>2011</year>) <article-title>mice: multivariate imputation by chained equations in R</article-title>. <source>J. Stat. Softw</source>., <volume>45</volume>, <fpage>1</fpage>–<lpage>67</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vega</surname><given-names>C.</given-names></string-name></person-group> (<year>2021</year>) <article-title>From Hume to Wuhan: an epistemological journey on the problem of induction in COVID-19 machine learning models and its impact upon medical research</article-title>. <source>IEEE Access</source>, <volume>9</volume>, <fpage>97243</fpage>–<lpage>97250</lpage>.<pub-id pub-id-type="pmid">34812399</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Waegeman</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Multi-target prediction: a unifying view on problems and methods</article-title>. <source>Data Min. Knowledge Discov</source>., <volume>33</volume>, <fpage>293</fpage>–<lpage>324</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>L.Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Multi-outcome predictive modelling of anesthesia patients</article-title>. <source>J. Biomed. Res</source>., <volume>33</volume>, <fpage>430</fpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilkinson</surname><given-names>D.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Defining and evaluating predictions of joint species distribution models</article-title>. <source>Methods Ecol. Evol</source>., <volume>12</volume>, <fpage>394</fpage>–<lpage>404</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolpert</surname><given-names>D.H.</given-names></string-name></person-group> (<year>1992</year>) <article-title>Stacked generalization</article-title>. <source>Neural Netw</source>., <volume>5</volume>, <fpage>241</fpage>–<lpage>259</lpage>.</mixed-citation>
    </ref>
    <ref id="btab576-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xing</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Simultaneous prediction of multiple outcomes using revised stacking algorithms</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>65</fpage>–<lpage>72</lpage>.<pub-id pub-id-type="pmid">31263871</pub-id></mixed-citation>
    </ref>
    <ref id="btab576-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname><given-names>H.</given-names></string-name>, <string-name><surname>Hastie</surname><given-names>T.</given-names></string-name></person-group> (<year>2005</year>) <article-title>Regularization and variable selection via the elastic net</article-title>. <source>J. R. Stat. Soc. Ser. B (Stat. Methodol.)</source>, <volume>67</volume>, <fpage>301</fpage>–<lpage>320</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
