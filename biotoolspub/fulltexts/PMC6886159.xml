<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6886159</article-id>
    <article-id pub-id-type="publisher-id">3085</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3085-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Virtual Grid Engine: a simulated grid engine environment for large-scale supercomputers</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ito</surname>
          <given-names>Satoshi</given-names>
        </name>
        <address>
          <email>sito@hgc.jp</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yadome</surname>
          <given-names>Masaaki</given-names>
        </name>
        <address>
          <email>yadome@ims.u-tokyo.ac.jp</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nishiki</surname>
          <given-names>Tatsuo</given-names>
        </name>
        <address>
          <email>nishiki.tatsuo@jp.fujitsu.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ishiduki</surname>
          <given-names>Shigeru</given-names>
        </name>
        <address>
          <email>sishi@jp.fujitsu.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Inoue</surname>
          <given-names>Hikaru</given-names>
        </name>
        <address>
          <email>inoue-hikaru@jp.fujitsu.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yamaguchi</surname>
          <given-names>Rui</given-names>
        </name>
        <address>
          <email>ruiy@ims.u-tokyo.ac.jp</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Miyano</surname>
          <given-names>Satoru</given-names>
        </name>
        <address>
          <email>miyano@ims.u-tokyo.ac.jp</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2151 536X</institution-id><institution-id institution-id-type="GRID">grid.26999.3d</institution-id><institution>The Institute of Medical Science, The University of Tokyo, </institution></institution-wrap>Shirokanedai 4-6-1, Minato-ku, Tokyo, 108-8639 Japan </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1789 4688</institution-id><institution-id institution-id-type="GRID">grid.418251.b</institution-id><institution>Frontier Computing Center, Fujitsu Limited, </institution></institution-wrap>Higashishinbashi1-5-2, Minato-ku, Tokyo, 105-7123 Japan </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>2</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>2</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 16</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. XH was an author of one of the papers in this supplement but was not involved in the process of its selection and review. No other competing interests were declared.</issue-sponsor>
    <elocation-id>591</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Supercomputers have become indispensable infrastructures in science and industries. In particular, most state-of-the-art scientific results utilize massively parallel supercomputers ranked in TOP500. However, their use is still limited in the bioinformatics field due to the fundamental fact that the asynchronous parallel processing service of Grid Engine is not provided on them. To encourage the use of massively parallel supercomputers in bioinformatics, we developed middleware called <italic>Virtual Grid Engine</italic>, which enables software pipelines to automatically perform their tasks as MPI programs.</p>
      </sec>
      <sec>
        <title>Result</title>
        <p id="Par2">We conducted basic tests to check the time required to assign jobs to workers by VGE. The results showed that the overhead of the employed algorithm was 246 microseconds and our software can manage thousands of jobs smoothly on the K computer. We also tried a practical test in the bioinformatics field. This test included two tasks, the split and BWA alignment of input FASTQ data. 25,055 nodes (2,000,440 cores) were used for this calculation and accomplished it in three hours.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">We considered that there were four important requirements for this kind of software, non-privilege server program, multiple job handling, dependency control, and usability. We carefully designed and checked all requirements. And this software fulfilled all the requirements and achieved good performance in a large scale analysis.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>High performance computing</kwd>
      <kwd>Grid engine</kwd>
      <kwd>TOP500</kwd>
      <kwd>MPI</kwd>
      <kwd>Python</kwd>
    </kwd-group>
    <conference xlink:href="http://orienta.ugr.es/bibm2018/">
      <conf-name>IEEE International Conference on Bioinformatics and Biomedicine 2018</conf-name>
      <conf-loc>Madrid, Spain</conf-loc>
      <conf-date>3-6 December 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p>The use of supercomputers in bioinformatics has become common with the unprecedented increase in the amount of biomedical data, e.g., DNA sequence data, and also demands of complex data analysis using multiple software tools. The growth of data size has been due to drastic improvements of measurement devices in the last decade. For DNA sequence data, the speed of data generation and reduction of the cost were over-exponential due to the development of so-called Next-Generation Sequencers (NGSs) [<xref ref-type="bibr" rid="CR1">1</xref>].</p>
    <p>DNA analyses of variant diseases have been estimated to require tens of thousands of sample analyses [<xref ref-type="bibr" rid="CR2">2</xref>]. Furthermore, the size of sample such as read length and coverage tends to be larger rapidly. However, only a few studies have utilized massively parallel supercomputers ranked in TOP500 [<xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR6">6</xref>]. One of the main reasons is lack of <italic>Grid Engine</italic> (GE) services, e.g., <italic>Sun Grid Engine</italic> and <italic>Univa Grid Engine</italic>, on most of those supercomputers; the use of GE-like service is currently almost a prerequisite for large-scale biological data analyses..</p>
    <p>Most software and programs that run on such TOP500-like supercomputers are paralleled using Message Passing Interface (MPI)[<xref ref-type="bibr" rid="CR7">7</xref>], wherein all subprocesses work synchronously. On the other hand, array jobs, automatically paralleled subprocesses of software pipelines by GE are asynchronous. Therefore, the GE conflicts with MPI-based systems from the perspective of the job-filling factor.</p>
    <p>Here, the MPI parallelization of software pipelines requires expert knowledge and experience. It is necessary for the MPI parallelization of software pipelines to use C or Fortran language wrapper programs or to commission High Performance Computing (HPC) experts to overwrite them fundamentally, which will be difficult for users.</p>
    <p>Recently, Cloud-base systems, such as Amazon Web Services (AWS), have been popular in NGS data analysis [<xref ref-type="bibr" rid="CR8">8</xref>]. Cloud computing services are very useful for small laboratories and companies that do not have computational resources. However, they still require significant costs for large-scale analyses [<xref ref-type="bibr" rid="CR9">9</xref>]. In addition, there are still several problems to be overcome, such as data transfer time, data corruption checking, and data security management.</p>
    <p>From the perspective of HPC, DRAGEN [<xref ref-type="bibr" rid="CR10">10</xref>] achieved drastic acceleration of the GATK pipeline [<xref ref-type="bibr" rid="CR11">11</xref>]. The hardware implementation of all the processes in GATK using FPGA caused this great acceleration. This approach is the ultimate software-tuning technique. On the other hand, it makes it quite difficult to improve the implemented workflows. GATK is one of the most popular pipelines for germline mutation calling, so this tuning is extremely efficient for it.</p>
    <p>However, there is a great variety of target variants for NGS data analyses for each study, and it is inevitable for algorithms and pipelines to be designed for the study. Therefore, general software pipelines still have merits in many studies and massively paralleled supercomputers are useful for accelerating their analyses.</p>
    <p>In this study, we developed MPI-based middleware named <italic>Virtual Grid Engine</italic> (VGE) that enables software pipelines based on GE system to run on massively parallel supercomputers.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <sec id="Sec3">
      <title>Goal and requirements</title>
      <p>Supercomputers are always used by many users. Thousands of jobs from users will be submitted to the system. Required system resource such as calculation time, necessary nodes, memory size, etc., are varies from job to job. Therefore, job management system (JMS) that controls the assignment of jobs efficiently is inevitable for large scale supercomputers.</p>
      <p>There are some JMS and GE-like tools. <italic>Sun Grid Engine</italic>, <italic>Univa Grid Engine</italic>, and <italic>TORQUE</italic> [<xref ref-type="bibr" rid="CR12">12</xref>] are pure GE systems. <italic>GNU parallel</italic> enables users to employ commands or software in parallel [<xref ref-type="bibr" rid="CR13">13</xref>]. While those GE-like tools are useful, it is a problem that not all of the supercomputer system do not implement them. That has prevented to analyze biological data on those massively parallel supercomputer systems.</p>
      <p>GE is a kind of server service program that works with JMS. JMS is one of the core service programs of supercomputers, so that a user cannot install or change it on a target machine. Thus, it is extremely hard for bioinformatics pipelines to work on a supercomputer that does not equip any GE.</p>
      <p>Our goal is to perform user pipelines efficiently on supercomputers on which GE has not been installed. VGE is the first application for this field, which must be a user program and act like a server service. To achieve this objective, there are the following four requirements: 1. Non-privilege server program VGE must be a user program and also works as a server service program. In order to utilize thousands of CPUs, it must be a MPI parallel program, and also provide a job submission mechanism for user pipelines. 2. Multiple job handling VGE must accept a number of jobs (many samples, multiple type analyses, etc.) simultaneously. Most MPI programs running on large-scale supercomputers often use thousands to tens of thousands of processes; thus running a job with a small number of processes a dozen times is inefficient. 3. Dependency control VGE must handle dependencies among tasks. Tasks in pipelines often contain dependencies, meaning that a certain task must wait for the former task to be accomplished. 4. Usability VGE must be friendly for medical and bioinformatics researchers. For example, file formats and programming languages must be widely used in this research area, and the modification of user pipelines for using VGE must be minimal. This is the most important point.It must be noted that software that does not satisfy this requirement will have severe problems in their dissemination.</p>
    </sec>
    <sec id="Sec4">
      <title>Algorithms</title>
      <p>To satisfy the most important requirement of the system, that is, the fourth requirement, VGE was written in Python. We also employed the <italic>MPI4PY</italic> package for the MPI environment of Python [<xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR16">16</xref>]. The system uses a Master-Worker model, which enables it to accept jobs with different processing details or different number of processes concurrently. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the concept of the VGE system. The VGE system consists of two programs: a Master-Worker type MPI paralleled main program (VGE: Virtual Grid Engine) and a job controller program (VGE job controller) that controls the job information between the VGE and user pipelines.
<fig id="Fig1"><label>Fig. 1</label><caption><p>VGE system</p></caption><graphic xlink:href="12859_2019_3085_Fig1_HTML" id="MO1"/></fig>
</p>
      <p>Job submission is performed by <italic>vge_task()</italic> function in place of GE commands, such as <italic>qsub</italic> of <italic>SGE</italic>. Scripts for submission and the number of array jobs are passed to <italic>vge_task()</italic> as arguments. <italic>vge_task()</italic> then sends the job information to VGE using socket communication.</p>
      <p>The information is stored in the main job pool of VGE on the Python shared object. The VGE master process, rank 0 of VGE MPI processes, plays this role, assigns registered jobs in the main pool to sleeping workers for execution, then waits for the completion signal from workers.</p>
      <p>The VGE job controller uses two types of communication: the socket communication of the <italic>vge_task()</italic> function, and the Python <italic>shared object</italic> for the main job pool. User pipelines, the VGE job controller, and the VGE maser process must be executed on the same physical computer unit (master node) to allow these two types of communication.</p>
      <p>The key point in the master-worker algorithm is the overhead cost for assigning jobs to workers. The master node executes three processes: the user pipeline, VGE job controller, and VGE master process. However, the former two processes only perform the registration of jobs to VGE, and their calculation loads are negligible.</p>
      <p>Therefore, the computational cost of the VGE master for assigning jobs to workers is critical for the performance of this system. Its cost is closely related to the access frequency to the main job pool and its size.</p>
      <p>To overcome these two problems, the VGE master creates two local job pools in itself. One pool (the second job pool) is used for reducing the access frequency by copying all jobs from the main job pool to it at a certain time (the blue dashed line in Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The other pool (the first job pool) extracts jobs equal to the number of VGE workers from the second job pool (the red arrow in Fig. <xref rid="Fig1" ref-type="fig">1</xref>).</p>
      <p>By assigning jobs from the first job pool, VGE reduces the size of the job pool to access and minimize its overhead.</p>
    </sec>
  </sec>
  <sec id="Sec5" sec-type="results">
    <title>Results</title>
    <p>In this section, we conducted several tests to check the performance of VGE. As described above, the VGE basic performance depends on the overhead time for assigning jobs. Therefore, we first checked this using an elementary code with a number of array jobs. Then, we performed a large-scale analysis test using practical data.</p>
    <sec id="Sec6">
      <title>Overhead measurements</title>
      <p>Here, we conducted basic tests to check the time required to assign jobs to workers by VGE. The test code comprised only 120 seconds of sleep. The numbers of array jobs were 10,000 (Case 1) and 100,000 (Case 2). The numerical environment was 2000 nodes of the K computer [<xref ref-type="bibr" rid="CR17">17</xref>]. Table <xref rid="Tab1" ref-type="table">1</xref> shows the specification of the K computer. VGE used a node for the master process, so the number of workers was 1999.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Specifications of the K computer</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">CPU</td><td align="left">SPARC64 VIIfx 2.0 GHz 8 cores/socket</td></tr><tr><td align="left">RAM</td><td align="left">16 GB/node (2GB/core)</td></tr><tr><td align="left">Node</td><td align="left">82,944</td></tr><tr><td align="left">Capability</td><td align="left">10,510 TFlops</td></tr></tbody></table></table-wrap>
</p>
      <p>Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the start time of each job. In Case 1 (Fig. <xref rid="Fig2" ref-type="fig">2</xref>), workers executed five or six jobs. The red points indicate an ideal situation where the overhead is equal to zero. It is clear that the result in this figure shows this step, so the VGE master smoothly assigned jobs to workers.
<fig id="Fig2"><label>Fig. 2</label><caption><p>The results of VGE job assignments. (<bold>a</bold>) Case1 10,000 jobs (<bold>b</bold>) Case2 100,000 jobs</p></caption><graphic xlink:href="12859_2019_3085_Fig2_HTML" id="MO2"/></fig>
</p>
      <p>The larger the number of jobs, the bigger the overhead. In Case 2 (Fig. <xref rid="Fig2" ref-type="fig">2</xref>), the number of jobs was ten times of Case 1, but the assignment was still performed smoothly. In Case 2, the ideal elapsed time for execution was 6120 seconds and the measured time for execution was 6145 seconds. Thus, the total VGE overhead was approximately 24.6 seconds and the assignment overhead for one job to a worker was 246 microseconds. This is sufficiently small to handle a large number of jobs.</p>
    </sec>
    <sec id="Sec7">
      <title>Simultaneous analyses of many samples</title>
      <p>In this test, we focused on the massively parallel analysis of multiple samples simultaneously. The numerical environment was the K computer, the same as in the previous section. The test code included two tasks, the split and BWA [<xref ref-type="bibr" rid="CR18">18</xref>] alignment of input FASTQ data [<xref ref-type="bibr" rid="CR19">19</xref>], which is standard in high-throughput sequencer data analysis. The details of the input data are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. Here, we used 25,055 nodes (200,440 cores) for this calculation and accomplished it in three hours.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Details of sample data</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">Number of samples</td><td align="left">14</td></tr><tr><td align="left">Data type</td><td align="left">Whole-genome sequencing (WGS)</td></tr><tr><td align="left">Data format</td><td align="left">FASTQ</td></tr><tr><td align="left">Read length</td><td align="left">152 bp, paired-end</td></tr><tr><td align="left">Total size</td><td align="left">4.2 TByte, 3.8 Tbp</td></tr></tbody></table></table-wrap>
</p>
      <p>This result indicates that VGE has enough capability for controlling thousands of workers and handling multiple samples simultaneously.</p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="discussion">
    <title>Discussion</title>
    <sec id="Sec9">
      <title>Performance and usability</title>
      <p>We defined four requirements of VGE in “<xref rid="Sec3" ref-type="sec">Goal and requirements</xref>” section. The first requirement (Non-privilege server program) has been described in “<xref rid="Sec4" ref-type="sec">Algorithms</xref>” and “<xref rid="Sec5" ref-type="sec">Results</xref>” section. Thus we focused on the other requirements in this part.</p>
      <p>The second requirement is handling multiple tasks. Figure <xref rid="Fig3" ref-type="fig">3</xref>a shows a short extraction of the job-submitting script used in “<xref rid="Sec7" ref-type="sec">Simultaneous analyses of many samples</xref>” section. Fourteen samples were named Data 0 to Data 13, respectively, and their tasks of FASTQ data division and BWA alignment were written in simple_pipeline.py.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Sources of the scripts used for multiple sample analyses (short extraction). (<bold>a</bold>) job-script. (<bold>b</bold>) pipeline-script (simple_pipeline.py). (<bold>c</bold>) command-script</p></caption><graphic xlink:href="12859_2019_3085_Fig3_HTML" id="MO3"/></fig>
</p>
      <p>A line starting with “simple_pipeline.py” corresponded to one sample analysis. In this example, fourteen sample jobs were submitted to VGE independently. In this way, VGE accepts multiple job submissions at once. Of course, different pipelines can also be submitted simultaneously.</p>
      <p>Here, we focus on the results of job assignment and filling to workers. We used the same pipeline and data used in “<xref rid="Sec7" ref-type="sec">Simultaneous analyses of many samples</xref>” section. The only difference was the number of workers. In this case, we used 5,000 workers which was much less than the number of total jobs. Thus, the workers had to perform the assigned jobs many times.</p>
      <p>Figure <xref rid="Fig4" ref-type="fig">4</xref> shows how the workers executed the jobs based on time. In the first twenty minutes, only a few workers performed jobs and the majority of the others did not work. This is because fourteen pipelines submitted <italic>fastq_splitter</italic>, which contained only two jobs. The results indicate that VGE successfully handled the dependency between tasks.
<fig id="Fig4"><label>Fig. 4</label><caption><p>The results of filling jobs to workers. The colored bars mean that workers performed assigned jobs. The white space means workers waited for assigned jobs. Workers that performed jobs from a coinciding task were given the same color. Colors were used cyclically. The number of workers was 5,000. Figure (<bold>a</bold>) is an enlarged image of Figure (<bold>b</bold>) framed by a light green box. (1) Workers performed <italic>fastq_splitter</italic> tasks. The number of these tasks were 28, so that most of workers were sleepingt. (2) The first three tasks of <italic>bwa_align</italic> were assigned to workers and started at almost the same time. There were still sleeping workers because the other <italic>fastq_splitter</italic> had been calculating. (3) These workers first performed jobs belonging to a red-colored task and accomplished them. Then, they immediately started the next jobs belonging to a yellow-colored task. The complicated results shown in Figure (a) indicate that each worker worked independently and continuously despite the computational costs of each task being quite different</p></caption><graphic xlink:href="12859_2019_3085_Fig4_HTML" id="MO4"/></fig>
</p>
      <p>After this task, the FASTQ files were split into thousands of files that were aligned with BWA by all the workers. The number of split files was much larger than that of workers, so all workers continued to perform their jobs. From this figure, it can be concluded that the assignment was tightly arranged; thus, job management of VGE was very effective in a real case.</p>
      <p>The third requirement is dependency control among tasks. Python is an interpreter language and performs a process per line. Using its characteristics, VGE controls task dependencies by tuning the task writing order in a script.</p>
      <p>Figure <xref rid="Fig3" ref-type="fig">3</xref> shows a short extraction of simple_pipeline.py used in “<xref rid="Sec7" ref-type="sec">Simultaneous analyses of many samples</xref>” section. It consists of two tasks, the division of input FASTQ files (<italic>fastq_splitter</italic>) and alignment of decomposed files by BWA (<italic>bwa_align</italic>). Here, <italic>bwa_align</italic> must wait for completion of <italic>fastq_splitter</italic> task.</p>
      <p>As described in “<xref rid="Sec4" ref-type="sec">Algorithms</xref>” section, job submission to VGE is performed by using <italic>vge_task()</italic> function. It is clear from Fig. <xref rid="Fig3" ref-type="fig">3</xref> that the simple_pipeline.py contains two tasks; the former is <italic>fastq_splitter</italic> and the latter is <italic>bwa_align</italic>.</p>
      <p>The <italic>vge_task()</italic> written on the eighth line in Fig. <xref rid="Fig3" ref-type="fig">3</xref> handles <italic>fastq_splitter</italic> task, and the process is accomplished after finishing the division of FASTQ files by VGE workers. Therefore, the <italic>vge_task()</italic> that is written later and corresponds to <italic>bwa_align</italic> does not submit to VGE before its accomplishment of the first task. The dependencies among tasks will be controlled by the order of tasks written in a script in this manner.</p>
      <p>The final requirement is friendliness to medical and bioinformatics researchers. Pipelines using VGE consist of three parts: describing the concrete contents of tasks (hereinafter referred to as <italic>command-script</italic>), denoting the flow of the pipelines (<italic>pipeline-script</italic>), and submitting tasks to VGE (<italic>job-script</italic>) (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).</p>
      <p>The <italic>command-script</italic> and the <italic>pipeline-script</italic> can be written either in the same file or in independent files. The <italic>command-script</italic> can also be described in shell script. Therefore, legacy scripts can be used on VGE. However, development of scripts from scratch is also possible as researchers in this field are familiar with coding in Python.</p>
      <p>On the other hand, <italic>pipeline-script</italic> must be written in Python. However, only <italic>vge_task()</italic> needs to be described. <italic>vge_task()</italic> requires three arguments: <sc>COMMAND</sc>, <sc>MAX_TASK</sc>, and <sc>BASENAME_FOR_OUTPUT</sc>. These arguments indicate the task name defined in <italic>command-script</italic>, the number of workers neccesary for the task (in short, the number of array jobs), and the unique ID (arbitral strings) used for log files, respectively. The value assignments are very clear, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref> (5-7 lines).</p>
      <p>As discussed, VGE is very straightforward and friendly software for users.</p>
    </sec>
    <sec id="Sec10">
      <title>Issues associated with distributed file systems</title>
      <p>At the test described in “<xref rid="Sec7" ref-type="sec">Simultaneous analyses of many samples</xref>” section, we encountered an unexpected severe problem. General behavior of a job using VGE is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. There are two intervals before the pipeline starts. First one is the initialization time of the system such as environmental value settings (a), and the other is the initialization time of MPI and VGE. Both intervals are from seconds to a minute in usual, but the total time of them was over 2 hours in the first trial.
<fig id="Fig5"><label>Fig. 5</label><caption><p>General behavior of a job using VGE</p></caption><graphic xlink:href="12859_2019_3085_Fig5_HTML" id="MO5"/></fig>
</p>
      <p>This problem had observed first in this large-scale computation test, or it never appeared at smaller scale tests such as two thousand nodes. Therefore, VGE didn’t mainly cause this problem. We carefully investigated the cause of this problem with K computer operating team. According to the result of this investigation, both initialization intervals ((a) and (b)) took 1 hour respectively. In the system initialization interval (a), the operating system and JMS do various processes such as assigning of nodes, but we found that the file system became overload.</p>
      <p>In this study, we mainly used the K computer that is one of the biggest supercomputers in the world. Of course, it equips a very large storage. Its size is over 30 PB thus traditional storage systems cannot handle such a huge storage. The K computer employs Fujitsu Exabyte File System [<xref ref-type="bibr" rid="CR20">20</xref>] that was based on Lustre [<xref ref-type="bibr" rid="CR21">21</xref>].</p>
      <p>Lustre is one of distributed file systems. Lustre family file systems consist of three parts, one is physical disks, another is object storage servers (OSS), and the other is metadata servers (MDS). Thousands of physical disks and OSSs are used in Lustre family system, but the number of MDS is usually small. Therefore, MDS may become a bottleneck of Lustre family systems.</p>
      <p>According to the investigation, we found that the job sent too much requests (e.g., make files, remove files, etc.) to the MDSs of FEFS at VGE launching. The observed value was over 20,000 per second. Applicable value of request to MDSs is 1300 per second, so that it was an extremely high value. The requests was caused by making log files of each workers. VGE workers make each log files in which the received task information and worker status are stored. The number of these files is proportional to the number of workers, thus we can’t find this problem in the previous tests. To avoid this problem, we made log files for VGE workers using only 1 process before MPI launched VGE.</p>
      <p>Figure <xref rid="Fig6" ref-type="fig">6</xref> shows FEFS structure in the K computer. The unique characteristics is that the whole file system consists of two layer: Global file system (GFS) and Local file system (LFS). Each system is complete as an independent file system. Programs and data that is required for a job send from GFS to LFS by the job management system (staging function) through the data transfer network. Thus user jobs are not affected by the others miscellaneous works on the login nodes.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Fujitsu Exabyte File System (FEFS) [<xref ref-type="bibr" rid="CR20">20</xref>]</p></caption><graphic xlink:href="12859_2019_3085_Fig6_HTML" id="MO6"/></fig>
</p>
      <p>The initialization time of MPI and VGE (b) was also related to MDS. In this initialization period, the system proceeds MPI startup and loads python modules that VGE imports. Each MPI process loads python module files respectively so that the requests to MDSs become very high in large scale tests. This problem is widely known in the dynamic linking library research field [<xref ref-type="bibr" rid="CR22">22</xref>]. To avoid this problem, you may use tools that improve a library loading performance. In this study, we prepared python main system and all modules on the local disks of all calculation nodes. Since VGE master and workers didn’t access to the files of python on the FEFS, we reduced the number of access to the MDSs at this initialization intervals.</p>
      <p>These issues were occurred in the large-scale computation of multiple sample analysis, so that you may consider that it is a very particular situation. However, it may occur in all types of bioinformatics analysis. As described before, one sample data size become quite large and it has already become over 5TB in the state-of-the-art studies. In such cases, typical protocols of sequence analysis hold potential risks for file systems.</p>
    </sec>
  </sec>
  <sec id="Sec11" sec-type="conclusion">
    <title>Conclusion</title>
    <p>In this study, we developed MPI-based middleware named <italic>Virtual Grid Engine</italic> (VGE), that employs the Master-Worker algorithm and provides grid engine services. It achieved extremely low overhead costs in large-scale computation. In the test calculation on the K computer, we accomplished alignments of 4.2 TB, 3.7 Tbp FASTQ data in three hours, and the results indicate that this will contribute to the rapid analysis of multiple large-scale samples. We found problems related to distributed file systems in the large scale computation. These problems are usually hard to recognize and solve for bioinformaticians. We successfully overcame them by collaborating with the K computer operating team.</p>
  </sec>
  <sec id="Sec12">
    <title>Availability and requirements</title>
    <p><bold>Project name:</bold> Virtual Grid Engine <bold>Project home page:</bold><ext-link ext-link-type="uri" xlink:href="https://github.com/SatoshiITO/VGE">https://github.com/SatoshiITO/VGE</ext-link><bold>Operating system(s):</bold> Linux <bold>Programming language:</bold> Python <bold>Other requirements:</bold> MPI4PY 2.0.0 or higher, MPICH or OpenMPI 2.0 or higher<bold>License:</bold> MIT license<bold>Any restrictions to use by non-academics:</bold> no</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>FEFS</term>
        <def>
          <p>Fujitsu exabyte file system</p>
        </def>
      </def-item>
      <def-item>
        <term>GE</term>
        <def>
          <p>Grid engine</p>
        </def>
      </def-item>
      <def-item>
        <term>GFS</term>
        <def>
          <p>Global file system</p>
        </def>
      </def-item>
      <def-item>
        <term>JMS</term>
        <def>
          <p>Job management system</p>
        </def>
      </def-item>
      <def-item>
        <term>LFS</term>
        <def>
          <p>Local file system</p>
        </def>
      </def-item>
      <def-item>
        <term>MDS</term>
        <def>
          <p>Meta data server</p>
        </def>
      </def-item>
      <def-item>
        <term>NGS</term>
        <def>
          <p>Next-generation sequencer</p>
        </def>
      </def-item>
      <def-item>
        <term>OSS</term>
        <def>
          <p>Object storage server</p>
        </def>
      </def-item>
      <def-item>
        <term>VGE</term>
        <def>
          <p>Virtual grid engine</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We greatly appreciate Soichiro Suzuki of R-CCS, Riken, Japan for creating a visualization tool for VGE log files. We are grateful to the K computer operating team for investigating FEFS related problems. The authors thank Georg Tremmel of The Institute of Medical Science, The University of Tokyo for creating a qualitative figure.</p>
    <sec id="d29e999">
      <title>About this supplement</title>
      <p>This article has been published as part of <italic>BMC Bioinformatics Volume 20 Supplement 16, 2019: Selected articles from the IEEE BIBM International Conference on Bioinformatics &amp; Biomedicine (BIBM) 2018: bioinformatics and systems biology</italic>. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-16.">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-16.</ext-link></p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>SI wrote the manuscript text. SI and MY contributed in conception and design of study, conducting of experiments, analysing the results and proofreading of paper. SI, MY, TN, SI, and HI developed the software. RY and SM contributed in design of study and proofreading of paper. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This research was supported by MEXT as a “Priority Issue on Post-K computer” (Integrated Computational Life Science to Support Personalized and Preventive Medicine) (Project ID: hp150265, hp160219, hp170227, hp180198, hp190158). Publication costs were also funded by this grant. Part of this research covered in this paper was supported by MEXT’s program for the Development and Improvement for the Next-Generation Ultra-High-Speed Computer System under its Subsidies for Operating Specific Advanced Large Research Facilities. This work was also partly supported by the Department of HPC Support, Research Organization for Information Science &amp; Technology (RIST), under the User Support Program of the K computer. Part of the supercomputing resource was provided by Human Genome Center (The University of Tokyo). No funding body played a role in the design of the study, analysis and interpretation of data, or in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>VGE is freely available from <ext-link ext-link-type="uri" xlink:href="https://github.com/SatoshiITO/VGE">https://github.com/SatoshiITO/VGE</ext-link></p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <mixed-citation publication-type="other">The Cost of Sequencing a Human Genome. 2018. <ext-link ext-link-type="uri" xlink:href="https://www.genome.gov/sequencingcosts/">https://www.genome.gov/sequencingcosts/</ext-link>. Accessed 25 Aug 2018.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zuk</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Schaffner</surname>
            <given-names>SF</given-names>
          </name>
          <name>
            <surname>Samocha</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Do</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hechter</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kathiresan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Daly</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Neale</surname>
            <given-names>BM</given-names>
          </name>
          <name>
            <surname>Sunyaev</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Lander</surname>
            <given-names>ES</given-names>
          </name>
        </person-group>
        <article-title>Searching for missing heritability: Designing rare variant association studies</article-title>
        <source>Proc Nat Acad Sci</source>
        <year>2014</year>
        <volume>111</volume>
        <issue>4</issue>
        <fpage>455</fpage>
        <lpage>64</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1322563111</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <mixed-citation publication-type="other">TOP, 500 Project. 2018. <ext-link ext-link-type="uri" xlink:href="https://www.top500.org/.">https://www.top500.org/.</ext-link>. Accessed 25 Aug 2018.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McNally</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Dorn</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Gerald</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Foster</surname>
            <given-names>IT</given-names>
          </name>
          <name>
            <surname>Golbus</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Dellefave-Castillo</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Pesce</surname>
            <given-names>LL</given-names>
          </name>
          <name>
            <surname>Puckelwartz</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Day</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Cappola</surname>
            <given-names>TP</given-names>
          </name>
          <name>
            <surname>Nelakuditi</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Supercomputing for the parallelization of whole genome analysis</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>11</issue>
        <fpage>1508</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu071</pub-id>
        <pub-id pub-id-type="pmid">24526712</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <mixed-citation publication-type="other">Ito S, Shiraishi Y, Shimamura T, Chiba K, Miyano S. High performance computing of a fusion gene detection pipeline on the k computer. In: 2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM): 2015. p. 1441–7. 10.1109/BIBM.2015.7359888.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>TAMADA</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>SHIMAMURA</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>YAMAGUCHI</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>IMOTO</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>NAGASAKI</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>MIYANO</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Sign: Large-scale gene network estimation environment for high performance computing</article-title>
        <source>Genome Informa</source>
        <year>2011</year>
        <volume>25</volume>
        <issue>1</issue>
        <fpage>40</fpage>
        <lpage>52</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <mixed-citation publication-type="other">Message Passing Interface Forum. 2018. <ext-link ext-link-type="uri" xlink:href="https://www.mpi-forum.org/.">https://www.mpi-forum.org/.</ext-link>. Accessed 25 Aug 2018.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Shanrong</given-names>
          </name>
          <name>
            <surname>Watrous</surname>
            <given-names>Kirk</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Chi</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Baohong</given-names>
          </name>
        </person-group>
        <article-title>Cloud Computing for Next-Generation Sequencing Data Analysis</article-title>
        <source>Cloud Computing - Architecture and Applications</source>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shringarpure</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Carroll</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>De La Vega</surname>
            <given-names>FM</given-names>
          </name>
          <name>
            <surname>Bustamante</surname>
            <given-names>CD</given-names>
          </name>
        </person-group>
        <article-title>Inexpensive and highly reproducible cloud-based variant calling of 2535 human genomes</article-title>
        <source>PLOS ONE</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>6</issue>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0129277</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <mixed-citation publication-type="other">Miller N, G. Farrow E, Gibson M, K. Willig L, Twist G, Yoo B, Marrs T, Corder S, Krivohlavek L, Walter A, Petrikin J, Saunders C, Thiffault I, Soden S, Smith L, Dinwiddie D, Herd S, Cakici J, Catreux S, Kingsmore S. A 26-hour system of highly sensitive whole genome sequencing for emergency management of genetic diseases. Genome Med. 2015; 7. 10.1186/s13073-015-0221-8.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jia</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Nan</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Transcriptome analysis of three critical periods of ovarian development in yellow river carp (cyprinus carpio)</article-title>
        <source>Theriogenology</source>
        <year>2018</year>
        <volume>105</volume>
        <fpage>15</fpage>
        <lpage>26</lpage>
        <pub-id pub-id-type="doi">10.1016/j.theriogenology.2017.08.027</pub-id>
        <pub-id pub-id-type="pmid">28923703</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Staples</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Torque resource manager</article-title>
        <source>Proceedings of the 2006 ACM/IEEE Conference on Supercomputing. SC ’06</source>
        <year>2006</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Tange O. GNU Parallel 20150322 (’Hellwig’). GNU Parallel is a general parallelizer to run multiple serial command line programs in parallel without changing them. 2015. 10.5281/zenodo.16303.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dalcin</surname>
            <given-names>LD</given-names>
          </name>
          <name>
            <surname>Paz</surname>
            <given-names>RR</given-names>
          </name>
          <name>
            <surname>Kler</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Cosimo</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Parallel distributed computing using Python</article-title>
        <source>Adv Water Res</source>
        <year>2011</year>
        <volume>34</volume>
        <fpage>1124</fpage>
        <lpage>39</lpage>
        <pub-id pub-id-type="doi">10.1016/j.advwatres.2011.04.013</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dalcín</surname>
            <given-names>Lisandro</given-names>
          </name>
          <name>
            <surname>Paz</surname>
            <given-names>Rodrigo</given-names>
          </name>
          <name>
            <surname>Storti</surname>
            <given-names>Mario</given-names>
          </name>
          <name>
            <surname>D’Elía</surname>
            <given-names>Jorge</given-names>
          </name>
        </person-group>
        <article-title>MPI for Python: Performance improvements and MPI-2 extensions</article-title>
        <source>Journal of Parallel and Distributed Computing</source>
        <year>2008</year>
        <volume>68</volume>
        <issue>5</issue>
        <fpage>655</fpage>
        <lpage>662</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpdc.2007.09.005</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dalcin</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Paz</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Storti</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Mpi for python</article-title>
        <source>J Paral Distrib Comput</source>
        <year>2005</year>
        <volume>65</volume>
        <issue>9</issue>
        <fpage>1108</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jpdc.2005.03.010</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <mixed-citation publication-type="other">K Computer. 2018. <ext-link ext-link-type="uri" xlink:href="https://www.r-ccs.riken.jp/en/k-computer/system.">https://www.r-ccs.riken.jp/en/k-computer/system.</ext-link>. Accessed 25 Aug 2018.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Durbin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Fast and accurate short read alignment with Burrows Wheeler transform</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>14</issue>
        <fpage>1754</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp324</pub-id>
        <pub-id pub-id-type="pmid">19451168</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <mixed-citation publication-type="other">FASTQ Format Specification. 2018. <ext-link ext-link-type="uri" xlink:href="http://maq.sourceforge.net/fastq.shtml.">http://maq.sourceforge.net/fastq.shtml.</ext-link>. Accessed 25 Aug 2018.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sakai</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sumimoto</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kurokawa</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>High-performance and highly reliable file system for the k computer</article-title>
        <source>Fujitsu Sci Tech J</source>
        <year>2012</year>
        <volume>48</volume>
        <issue>1</issue>
        <fpage>302</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <mixed-citation publication-type="other">The Lustre Filesystem. 2019. <ext-link ext-link-type="uri" xlink:href="http://lustre.org/.">http://lustre.org/.</ext-link>. Accessed 20 Mar 2019.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boito</surname>
            <given-names>FZ</given-names>
          </name>
          <name>
            <surname>Inacio</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Bez</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Navaux</surname>
            <given-names>POA</given-names>
          </name>
          <name>
            <surname>Dantas</surname>
            <given-names>MAR</given-names>
          </name>
          <name>
            <surname>Denneulin</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A checkpoint of research on parallel i/o for high-performance computing</article-title>
        <source>ACM Comput Surv</source>
        <year>2018</year>
        <volume>51</volume>
        <issue>2</issue>
        <fpage>23</fpage>
        <lpage>12335</lpage>
        <pub-id pub-id-type="doi">10.1145/3152891</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
