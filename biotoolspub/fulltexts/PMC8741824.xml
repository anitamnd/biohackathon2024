<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8741824</article-id>
    <article-id pub-id-type="publisher-id">3858</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-021-03858-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>OpenWeedLocator (OWL): an open-source, low-cost device for fallow weed detection</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Coleman</surname>
          <given-names>Guy</given-names>
        </name>
        <address>
          <email>guy.coleman@sydney.edu.au</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Salter</surname>
          <given-names>William</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Walsh</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.1013.3</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 834X</institution-id><institution>School of Life and Environmental Sciences, Sydney Institute of Agriculture, </institution><institution>The University of Sydney, </institution></institution-wrap>Brownlow Hill, NSW Australia </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>170</elocation-id>
    <history>
      <date date-type="received">
        <day>9</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>3</day>
        <month>12</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">The use of a fallow phase is an important tool for maximizing crop yield potential in moisture limited agricultural environments, with a focus on removing weeds to optimize fallow efficiency. Repeated whole field herbicide treatments to control low-density weed populations is expensive and wasteful. Site-specific herbicide applications to low-density fallow weed populations is currently facilitated by proprietary, sensor-based spray booms. The use of image analysis for fallow weed detection is an opportunity to develop a system with potential for in-crop weed recognition. Here we present OpenWeedLocator (OWL), an open-source, low-cost and image-based device for fallow weed detection that improves accessibility to this technology for the weed control community. A comprehensive GitHub repository was developed, promoting community engagement with site-specific weed control methods. Validation of OWL as a low-cost tool was achieved using four, existing colour-based algorithms over seven fallow fields in New South Wales, Australia. The four algorithms were similarly effective in detecting weeds with average precision of 79% and recall of 52%. In individual transects up to 92% precision and 74% recall indicate the performance potential of OWL in fallow fields. OWL represents an opportunity to redefine the approach to weed detection by enabling community-driven technology development in agriculture.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Plant sciences</kwd>
      <kwd>Engineering</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000980</institution-id>
            <institution>Grains Research and Development Corporation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">In Australian large-scale conservation cropping systems, where growing season rainfall generally limits crop yields, fallow phases are incorporated in rotations to conserve soil moisture for subsequent crops<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. These phases provide an opportunity for improved weed and disease control<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup> and nutrient conservation<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. To maximise stored soil moisture growers prioritise maintaining weed free fallows<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, which frequently results in repeated applications of whole-field herbicide treatments to low density (&lt; 1.0 plant 10 m<sup>−2</sup>) weed populations. The development of reflectance and fluorescence-based weed detection technologies that enabled site-specific weed control (SSWC) of low weed densities in fallow fields, began in the early 1980s<sup><xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR11">11</xref></sup> (for an overview see review by Peteinatos et al.<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>). As all living plants in fallows are considered weeds, these detection systems use spectral filters and photodiode sensors to detect chlorophyll fluorescence<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. For over two decades sensor-based weed detection has been used in the development of spot-spraying systems that are now widely used for fallow weed control by Australian growers<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>. These spot-spraying systems can effectively control low density weed populations to realise weed control savings of up to 90%<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>.</p>
    <p id="Par3">The effective application of site-specific treatments has enabled a more efficient approach to fallow weed control and created interest in the development of this approach for in-crop use. In addressing the threat of herbicide resistant weed populations in their production systems, Australian growers have been reducing in-crop weed densities through the diligent use of diverse weed control treatments<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Low in-crop weed densities have increased the interest in specifically targeting these weeds to achieve similar savings in weed control inputs as those realised with fallow SSWC. However, sensor-based fallow weed detection technologies are only suitable for detecting growing plants, with little opportunity for further development to discriminate between crop and weed plants<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. The use of digital, visual spectrum imagery has long been identified as an approach to collect the type and quantity of data required for accurate discrimination between crop and weed plants<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>. Imaging sensors offer detailed data streams with standard, visible spectrum digital cameras providing three channels (red, green and blue [RGB] images) of spatial and spectral intensity information. The richer data collected by these systems can be used for the more challenging task of in-crop weed recognition, with substantial research efforts focussed on developing this opportunity for large-scale systems<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>.</p>
    <p id="Par4">The introduction of durable, small-scale, low-cost computing and digital camera systems has created the potential to develop simple algorithm-based weed detection systems for fallow weed control in large-scale cropping systems. The Raspberry Pi is an example of a low-cost single board computer that was developed as a teaching resource to promote computer science in schools<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. When coupled with a digital camera, the Raspberry Pi can be used in simple computer vision related tasks, including fallow and in-crop weed detection. For fallow weed detection, where differences between the target weed and soil or stubble background are clear, simple plant colour-based detection methods may be sufficient for weed detection. A number of studies have developed weed detection algorithms based on specific plant features such as colour<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>, shape<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>, texture<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup> or a combination of these features<sup><xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR29">29</xref></sup>. Importantly, non-machine learning algorithms typically have lower computational requirements and perform faster on less powerful processors, such as the Raspberry Pi, improving the likelihood of real-time operation in large-scale cropping systems<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. Critically, the computational requirements of the algorithm determine the framerate and hence the real-time capability of the device. Coupling low-cost hardware with an open-source and community led approach to software development provides an opportunity to rapidly progress the development of these technologies for weed detection in cropping systems. Similar approaches have been effective in industries including medical research<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, autonomous vehicles<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, machine learning<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> and implemented by software companies such as Microsoft<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
    <p id="Par5">Image-based weed recognition for SSWC represents a fundamental shift in the approach to weed control for large-scale growers in Australia. A practical understanding of the limitations and realistic opportunities of this new approach by growers and the wider weed control community is an important aspect of the effective development and use of this new technology. Growers are generally regarded as Bayesian learners, where an understanding of technology is best achieved through practical “hands-on” use<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. The general objective of this research was to develop and validate an open-source and low-cost option for weed detection in fallow fields, with potential for upgrades and improvement with future software and hardware innovations. Specifically, we aimed to (1) develop the OpenWeedLocator (OWL) as a low-cost image-based weed detection system; (2) advance weed control industry understanding and familiarity in the use of digital image-based weed detection systems; and (3) validate the baseline efficacy of OWL using colour-based algorithms for fallow weed detection.</p>
    <p id="Par6">In the following section, OWL configuration and the parameters under which the design process was guided are described. The validation of the OWL for fallow weed detection using colour-based algorithms and the implications of the device for SSWC are discussed in further sections. All software, hardware designs and a guide to build the OWL are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/geezacoleman/OpenWeedLocator">https://github.com/geezacoleman/OpenWeedLocator</ext-link>.</p>
  </sec>
  <sec id="Sec2">
    <title>OWL configuration and system development</title>
    <sec id="Sec3">
      <title>Establishing design parameters and configuring OWL for weed detection</title>
      <p id="Par7">Five key design parameters for the OWL units were identified that facilitated the development of OWL units within the scope of improving community understanding and familiarity with use of imaging and algorithm-based weed detection systems. Specifically, the OWL platform needed to include (1) low-cost and accessible “off-the-shelf” hardware components; (2) simple designs with minimal use of specialised electrical tools; (3) 3D printable enclosures and mounts for accessible and customisable production; (4) modular and simple image-based software for ease of contribution and explanation; and (5) validated performance using simple and existing colour-based algorithms in fallow scenarios at relevant levels for weed detection, addressed in the algorithm assessment component of this research.</p>
      <p id="Par8">The image processing components of OWL consist of a Raspberry Pi 4 (Raspberry Pi Foundation, Cambridge, UK) 8 GB computer coupled to a Raspberry Pi HQ Camera with a Sony IMX477 CMOS sensor. This camera provides a maximum sensor resolution of 4056 × 3040 pixels with a 7.9 mm optical format and a rolling shutter. Images are resized to 416 × 320 pixels to ensure high processing throughput on the Raspberry Pi platform. The HQ camera connects to the Raspberry Pi using the camera serial interface (CSI) cable and port, whilst operating the camera is completed with the inbuilt picamera Python API. A 6 mm focal length C/CS lens (Raspberry Pi Foundation) was used with the camera, providing a 1 m horizontal field of view (FOV) on the ground at an operational height of 0.82 m above the soil surface. Camera settings, including white balance, exposure and shutter speed, remained automatic as default, whilst focus was set manually during setup of the system. Based on the detection outputs from the selected algorithm, the pixel coordinates of each detection determine the allocation to one of four 25 cm wide zones covering the 1 m on-ground FOV. A unique general purpose input/output (GPIO) pin is assigned to each zone, which is activated for a specified duration if the weed is detected within that zone. A generic and low-cost relay control board enables the low current, low voltage GPIO signal to drive higher power devices including, but not limited to, water and hydraulic solenoids for targeted weed control, such as spot spraying or site-specific tillage (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The system is powered by a 12 V DC input with a voltage regulator (POLOLU-4091; Pololu Corporation, Las Vegas, NV, USA) providing 5 V power to the Raspberry Pi and associated components. Although optional for the purposes of weed control, we included a real time clock (RTC) module (ADA3386; Adafruit Industries, New York, NY, USA), a buzzer and several LEDs in our test system for timekeeping, system alerts and monitoring system status, respectively. The Python code and detailed installation instructions are provided in the OWL open-source repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/geezacoleman/OpenWeedLocator">https://github.com/geezacoleman/OpenWeedLocator</ext-link>).<fig id="Fig1"><label>Figure 1</label><caption><p>Overview of the OpenWeedLocator (OWL) (<bold>a</bold>) software and (<bold>b</bold>) hardware, which combines weed detection with an actionable output. Detection is achieved with a Raspberry Pi 4 8 GB and HQ camera with actuation achieved using 12 V relays on the relay control board. A real time clock (RTC) module is used for accurate timekeeping. A 12 V DC source is required to power the system, with a voltage regulator providing 5 V power for the Raspberry Pi computer. A six-pin weatherproof connector is used to connect the OWL unit to the 12 V power supply and to connect the relays to four external devices. The buzzer and LEDs provide status information.</p></caption><graphic xlink:href="41598_2021_3858_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>System development and an open-source platform for community engagement</title>
      <p id="Par9">OWL enables weed detection and potential targeting by integrating the within-image location of each detected weed with a defined channel (GPIO pin) on the Raspberry Pi and subsequently a relay on the connected relay board. To address the first three design parameters, four OWL units were assembled using simple, “off-the-shelf” and low-cost items, with an approximate cost of AU$400 per unit. The critical components in the system, namely the Raspberry Pi, camera, voltage regulator and relay control board are easily accessible due to their extensive use in other industries. Activating an OWL unit requires connection to a 12 V DC power source, a voltage commonly available on most vehicles and powered farm equipment. The design is simple to assemble with minimal soldering or specialist tools required. It is also modular, with the ability to replace individual components and maintain functionality without changes to form or software. For example, given the use of generic GPIO-based trigger of the relay control board, the Raspberry Pi could be swapped with other embedded computers such as the Jetson Nano or Jetson Xavier for access to more powerful processing without substantial changes to software or hardware. Internally, all electrical connections are made using terminal blocks and press fit connections. The OWL enclosure and all mounting parts are 3D printable and all 3D model files are freely available for use and customisation (<ext-link ext-link-type="uri" xlink:href="https://www.tinkercad.com/things/3An6a3MtL9C">https://www.tinkercad.com/things/3An6a3MtL9C</ext-link>). An important aspect of using widely accessible components is the existing widespread support by the ‘maker’ community with respect to more general issues and upgrades of Raspberry Pi hardware and Python software. These resources improve problem solving availability for end users with reduced risk of obsolescence.</p>
      <p id="Par10">The fourth design parameter, the modularity, customisability and interpretability of the OWL detection and actuation software, was the focus of software development. The Python language-based software enables the selection of all tested algorithms (see “<xref rid="Sec8" ref-type="sec">Methods</xref>”), adjustment of threshold parameters, minimum detection size requirements (in pixels) and other features, such as frame saving for dataset development, video recording and visualisation of the detection process. The incorporation of new algorithms is fundamental to ongoing development. The modular design of the software allows new algorithms to only require an image as an input and return a grayscale image as an output. This allows further improvements or additions of new algorithms to be made easily without restructuring of the code base.</p>
      <p id="Par11">The OWL system is supported by an open-source software repository to create a pathway for feedback and ongoing development, whilst providing a practical device on which to learn about image-based weed detection. Specific instructions and extensive guides are available for self-guided assembly, with the widely used platform GitHub selected as an avenue to engage with community feedback and development whilst providing accessibility to the code and instructions (<ext-link ext-link-type="uri" xlink:href="https://github.com/geezacoleman/OpenWeedLocator">https://github.com/geezacoleman/OpenWeedLocator</ext-link>). The online platform also supports logging of issues and tracking of changes over time with the release of new software versions as improvements are incorporated.</p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Validation of OWL with colour-based weed detection algorithms</title>
    <p id="Par12">The four algorithms used to validate OWL, namely excess green (ExG), normalized ExG (NExG), hue saturation value (HSV), and a combined ExG and HSV (ExHSV), performed equally well across the seven fields on the low-cost, Raspberry Pi-based OWL hardware, with no statistical differences found for either the precision or the recall (P &gt; 0.05) (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). The mean recall—the percentage of true weeds detected—was 52.2 ± 5.1% (mean ± SEM) whilst the mean precision—the proportion of detections that were correct—was 78.8 ± 3.6% (mean ± SEM). Across the seven transects, median recall rates for ExG, NExG, HSV and ExHSV were 68.1, 45.5, 47.0 and 47.7%, respectively. Median precision values were 70.2, 90.6, 96.6 and 91.1%, respectively. Although no algorithm clearly outperformed the others, ExG appeared more sensitive to weed detection, albeit with reduced precision. HSV and ExHSV appeared to have lower rates of false positive detections (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). In five of the seven transects, the maximum recall was observed with the ExG algorithm (Table <xref rid="Tab1" ref-type="table">1</xref>). In all five daylight transects, ExHSV had the maximum precision, whilst HSV had precision of 100% in both night-time, artificially illuminated transects. The normalised ExG algorithm (NExG) did not outperform the other three algorithms in any of the transects for any performance metric, with complete loss of detection under NIGHT2 conditions using the parameters tested.<fig id="Fig2"><label>Figure 2</label><caption><p>Comparison of weed detection performance metrics precision and recall across ExG, NExG, HSV and ExHSV algorithms. Values presented are based on all seven field sites visited to indicate variability. Boxplots present the median and interquartile range with the boxes, and the range and outlier points (if more than 1.5 times the interquartile range) with the lines and points.</p></caption><graphic xlink:href="41598_2021_3858_Fig2_HTML" id="MO2"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Summary of algorithm performance across seven field test sites during the day (n = 5) and night (n = 2) with artificial lighting using precision and recall.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Algorithm</th><th align="left" colspan="2">ExG</th><th align="left" colspan="2">NExG</th><th align="left" colspan="2">HSV</th><th align="left" colspan="2">ExHSV</th></tr><tr><th align="left"><italic>Location</italic></th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th><th align="left">Precision (%)</th><th align="left">Recall (%)</th></tr></thead><tbody><tr><td align="left"><italic>HEN1</italic></td><td char="." align="char">18.0</td><td char="." align="char"><bold>94.7</bold></td><td align="left">27.7</td><td align="left">75.8</td><td char="." align="char">22.6</td><td char="." align="char">76.3</td><td char="." align="char"><bold>37.9</bold></td><td char="." align="char">71.6</td></tr><tr><td align="left"><italic>HEN2</italic></td><td char="." align="char">46.5</td><td char="." align="char"><bold>73.2</bold></td><td align="left">90.2</td><td align="left">53.3</td><td char="." align="char">87.7</td><td char="." align="char">50.5</td><td char="." align="char"><bold>94.4</bold></td><td char="." align="char">54.4</td></tr><tr><td align="left"><italic>WAG1</italic></td><td char="." align="char">91.8</td><td char="." align="char"><bold>73.8</bold></td><td align="left">95.1</td><td align="left">47.8</td><td char="." align="char">96.6</td><td char="." align="char">39.9</td><td char="." align="char"><bold>99.1</bold></td><td char="." align="char">47.7</td></tr><tr><td align="left"><italic>WAG2</italic></td><td char="." align="char">70.2</td><td char="." align="char"><bold>68.1</bold></td><td align="left">90.9</td><td align="left">36.1</td><td char="." align="char">91.0</td><td char="." align="char">48.8</td><td char="." align="char"><bold>91.1</bold></td><td char="." align="char">43.4</td></tr><tr><td align="left"><italic>COB1</italic></td><td char="." align="char">98.0</td><td char="." align="char">39.3</td><td align="left">98.0</td><td align="left">20.2</td><td char="." align="char"><bold>100</bold></td><td char="." align="char"><bold>47.0</bold></td><td char="." align="char"><bold>100</bold></td><td char="." align="char">23.1</td></tr><tr><td align="left"><italic>NIGHT1</italic></td><td char="." align="char">64.5</td><td char="." align="char"><bold>48.9</bold></td><td align="left">47.7</td><td align="left">42.5</td><td char="." align="char"><bold>100</bold></td><td char="." align="char">23.8</td><td char="." align="char">80.6</td><td char="." align="char">34.7</td></tr><tr><td align="left"><italic>NIGHT2</italic></td><td char="." align="char">96.4</td><td char="." align="char">62.7</td><td align="left">–</td><td align="left">–</td><td char="." align="char"><bold>100</bold></td><td char="." align="char">35.7</td><td char="." align="char">90.6</td><td char="." align="char"><bold>76.6</bold></td></tr></tbody></table><table-wrap-foot><p>The highest result for each performance metric within each field is bolded.</p></table-wrap-foot></table-wrap></p>
    <p id="Par13">Across the seven fields, the performance of the weed detection algorithms varied substantially (Table <xref rid="Tab1" ref-type="table">1</xref>). The minimum precision for all algorithms tested was found in field HEN1 whilst the maximum precision was found in WAG1. HEN1 had substantial canola stubble present, which under strong sunlight conditions resulted in bright white reflections and frequent false positive detections (indicated by mean precision of 26.6 ± 4.3%). WAG1 on the other hand had a high weed density, sparser lupin stubble and red soil, resulting in a reduced rate of false positive detections (indicated by mean precision of 95.65 ± 1.5%).</p>
    <p id="Par14">Given the colour-based nature of the algorithms, small annual sowthistle (<italic>Sonchus oleraceus</italic>) that was grey-green or purple-green in colour was not well detected (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Similarly, the thin leaves of small rigid ryegrass (<italic>Lolium rigidum</italic>) and other grass weeds, including volunteer wheat (<italic>Triticum aestivum</italic>) and barley (<italic>Hordeum vulgare</italic>), were poorly detected and often missed. Small, medium and large broadleaf weeds with strong green colouring, including wild radish (<italic>Raphanus raphanistrum</italic>), sowthistle, volunteer canola (<italic>Brassica napus</italic>), volunteer faba bean (<italic>Vicia faba</italic>), billygoat weed (<italic>Ageratum conyzoides</italic>) and catsear (<italic>Hypochaeris radiata</italic>), were confidently detected, however, obstruction by heavy stubble increased the likelihood of the weed being missed.<fig id="Fig3"><label>Figure 3</label><caption><p>Representative weeds that were either correctly detected (green) or missed (red) at each of the seven field sites based on the ExHSV algorithm. Images of weeds shown were taken directly from concurrent video collected with a Samsung S8 phone camera and have not been rescaled, suggesting relative size is accurate.</p></caption><graphic xlink:href="41598_2021_3858_Fig3_HTML" id="MO3"/></fig></p>
    <p id="Par15">Frame rates were recorded to assess the processing demand of each algorithm and the likelihood of real-time operation. This is important for OWL, given the relatively limited processing power of the Raspberry Pi. HSV had the highest framerate of 35.3 FPS (P &lt; 0.01), indicating that it had the lowest processing requirements. ExG had the second highest framerate of 22.6 FPS (P &lt; 0.01), whilst ExHSV and NExG were the slowest of the algorithms with framerates of 15.4 FPS and 16.6 FPS, respectively.</p>
  </sec>
  <sec id="Sec6">
    <title>Discussion</title>
    <p id="Par16">OWL capitalises on recent developments of low cost and small form factor computing systems, digital imaging sensors, so-called “maker” hardware and open-source software packages. The original Raspberry Pi computer was released in 2012 with the explicit focus of teaching basic computer science to ‘young people’ and igniting interest in programming<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Technological advancements to the Raspberry Pi system in the years since, including more powerful processors, increased memory, and networking capabilities, have allowed for increasingly complex projects to be developed using the system. This has led to a large online community of “makers”, who have found a multitude of uses for the Raspberry Pi, ranging from environmental monitoring<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> to cloud computing infrastructure<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, and who, with the help of platforms such as GitHub and StackOverflow, can provide support for hardware and software related issues and improve the development process<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The Raspberry Pi was an obvious choice for preliminary development of OWL, due to its widespread availability, low-cost, educational roots, interfacing options (through GPIO pins) and computational power, as well as existing and extensive online support communities. Coupling the GPIO pins of the Raspberry Pi with a relay control board makes identifiable interactions between the camera input, image-based detection output and actuation. The use of the OWL for SSWC requires coupling with external actuators such as solenoids for spot spraying or targeted tillage<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. The OWL platform combines simple assembly and software designs with practical fallow field detection outcomes.</p>
    <p id="Par17">OWL represents a novel opportunity for community-driven development of weed recognition capability using existing ‘off-the-shelf’ hardware and simple yet effective image-based algorithms. The combination of the OWL device, supporting documentation and repository create a channel for practical education of key image-based weed detection and actuation concepts for growers and the wider weed control community. The topic is of particular importance now, given the emergence of image-based in-crop weed recognition technologies. OWL has been designed as a community focussed educational platform that will grow over time with initial baseline validation performed in the present research. The platform relies heavily on the principles of first- and second-order learning<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. The term “first-order learning" refers to the education of growers, where users of a new technology learn with hands-on experience, in this case how image-based weed detection systems work by building and using an OWL unit. Early exposure to novel precision agricultural technologies in this manner has been shown to be strongly correlated with the adoption of new precision agricultural tools<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. The term “second-order learning” refers to the education of SSWC technology developers based upon feedback from users and specific user needs. This has also been coined the “learning by using” approach<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. The open-source availability facilitates such an approach, allowing continual development of OWL, weed recognition technologies more generally, and ongoing educational opportunities for the broader weed control community. Similar open-source approaches to software and hardware development have been used successfully in other industries, including machine learning<sup><xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup>, medical sciences<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>, scientific imaging<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup> and autonomous driving<sup><xref ref-type="bibr" rid="CR48">48</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup> to improve development speed, adoption of technology, reproducibility and error management<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup>. In agriculture, practical implementations of grower-ready open-source systems are more limited, though include AgOpenGPS<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> and FarmOS<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, and now OWL. The GitHub platform selected for engagement with growers enables both the dissemination of instructions in an easy-to-read format and facilitates community contribution through licensed replication, adjustment and change tracking and has been successfully used in the transition of closed source to open source software<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. The approach is in line with similar open-source dataset and code publications such DeepWeeds<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, pybonirob<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> and OpenSourceOV<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>.</p>
    <p id="Par18">The algorithms chosen to validate the baseline in field performance of OWL represent widely used methods of colour segmentation<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> or vegetation index generation<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Despite the challenging scenarios and low cost of hardware, maximum rates of precision of 100% and recall of 94.7% were recorded by ExHSV/HSV and ExG algorithms, respectively, demonstrating clear potential for the use of image-based weed detection in fallow systems. Precision and recall means across all fields were lower than expected, likely a result of established limitations of using colour-only algorithms in highly complex and diverse environmental conditions with variable weed colour<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>. Based on qualitative assessment of weed characteristics, it is likely that variable weed appearance, in particular stressed, purple-green annual sowthistle and thin grass weeds on diverse soil and stubble backgrounds, contributed to the reduced performance of colour-only algorithms. Factors such as image blur, resulting from slow shutter speeds and the rolling shutter of the Raspberry Pi HQ camera are likely to have contributed to low recall. High image blur results in the green pixels from small or thin-leaved weeds being averaged with neighbouring background pixels, resulting in missed detections and poorer performance of colour-based algorithms<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. Immediate improvements would likely be observed with more advanced global shutter cameras and brighter, more uniform illumination for faster shutter speeds<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. Deep learning algorithms have been found to be more tolerant to blur, lighting and colour variability<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, though would likely run too slow on Raspberry Pi devices and require large image datasets for training and generalization. Exploiting the colour differences between growing weeds and background is a simple method to validate the OWL system. Whilst substantial advances in useability of deep learning systems continue to be made, supporting these algorithms increases both the complexity and cost of the system, with a requirement for more powerful embedded computers. Nonetheless, the modular nature of the OWL system allows future versions to utilise more powerful processors running more advanced algorithms.</p>
    <p id="Par19">Whilst no differences (P &gt; 0.05) were found among algorithms when compared across all seven transects, trends in performance and algorithm variability suggested field-scale differences in performance would be likely. ExG appeared more sensitive to weed detection than the other algorithms, which would result in fewer weeds being missed. The low variability in precision of ExHSV, coupled with the ability to refine sensitivity in two colour spaces suggests it is a better option for large-scale weed detection in environments where weeds are large and green. Similarly, Kawamura et al. found combining HSV and ExG features improved performance of a machine learning model over other models trained on HSV alone<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. Using ExG instead of NExG in the composite ExHSV algorithm may be advantageous based on the results presented here, however, others have found non-normalized RGB chromatic coordinates to be highly variable<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, resulting in poorer performance. Additionally, managing bright reflection from white stubble is critical in environments with bright sunlight, where the use of specular reflection management approaches such as that developed by Morgand and Tamaazousti<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>, would likely reduce false positives in heavy stubble conditions. Nevertheless, under current settings it appears that the precision of ExHSV offers a reduced risk of excessive false positives in stubble. Further analyses with more field trials in defined environments would improve our ability to confidently determine the most effective algorithm, however, the adjustment of colour-based algorithms to suit individual environmental circumstances is a well-known drawback of these approaches<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. This may be required for consistently effective use as a fallow weed control tool. The benefit of OWL is that there are opportunities to include additional reduced sensitivity options when used under brightly sunlit, heavy stubble conditions, such as those at field site HEN1, where precision across all algorithms is substantially reduced. Whilst the very high precision recorded for all algorithms at the WAG1 field site is encouraging, the result is likely due to the high density of weeds, where the frame is already filled by green objects with few opportunities for false positives. Based on the experiences of other industries, the use of image-based weed detection is likely to expand as a result of community engagement from open-source availability, leading to rapid progression in SSWC for fallow crop production systems. Importantly, the detection of green plants does not limit the OWL to fallow spraying. Inter-row use of the device for weed control in wide row crops or the site-specific application of weed control, fertiliser, desiccants and irrigation in crops may also be viable uses, demonstrating the wide-ranging potential of the device.</p>
    <p id="Par20">On embedded devices such as the Raspberry Pi, the processing speed, as measured here as the framerate of each algorithm, is an important metric to determine maximum possible forward speed for real-time use. The low framerates at which ExHSV and NExG run highlight the increased computational demand of these algorithms compared to HSV and ExG. HSV ran at the highest framerate, which is likely due to the binary output image (black and white only) not requiring an additional computationally expensive adaptive threshold. This finding is contrary to Woebbecke et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, where HSV was found to be more computationally expensive, however, the evaluation in that study was based purely on defined thresholds rather than a combination of both defined and adaptive thresholds. Previous methods of green-based differentiation have employed Otsu’s thresholding<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, where the appropriate threshold value is determined algorithmically based on image content. In large-scale, fallow scenarios where weeds are infrequent, relying solely on adaptive thresholds such as this may result in false positives when no clear green signal is provided by the selected algorithm. The combination of defined and adaptive thresholds in ExG, NExG and ExHSV was used in this study to better determine weed presence where weeds are infrequent. It is highly unlikely that framerate is a limiting factor for OWL, with real-time operation in large-scale systems (forward speed dependent) observed at framerates above 6 FPS for other systems<sup><xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR53">53</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup>, with current commercial systems operating between 16 and 17 FPS for forward speeds between 2.67 and 6.67 m s<sup>−1</sup><sup><xref ref-type="bibr" rid="CR64">64</xref>,<xref ref-type="bibr" rid="CR65">65</xref></sup>. The performance of algorithms in the field is likely also dependent on the ambient lighting conditions, which in turn influence the blurriness of the video feed. Optimising these factors and measuring impacts will be important in determining the most effective weed detection algorithm for fallow scenarios.</p>
  </sec>
  <sec id="Sec7">
    <title>Conclusion</title>
    <p id="Par21">The development of lower cost, smaller form factor and higher power computing is generating opportunities to deploy more accessible weed recognition technologies and embrace the potential for education and engagement that open-source software and hardware brings. The validation of OWL presented here has immediate applications as a low-cost image-based fallow weed detection device for large-scale crop production systems. The open-source and community-driven nature of the system enables ongoing development and opportunities to further increase the complexity of detection possibilities and reliability by upgrading the algorithms, embedded computer and camera hardware, and system settings. The results presented here of colour-based algorithms in seven separate field transects under both day (full sun, overcast) and night (artificial lighting) demonstrate the baseline potential for the OWL unit, with individual field performance at levels equivalent to other fallow detection systems. The high precision of the ExHSV combination algorithm suggests it may be relevant for use with large weeds in stubble, where the green signal is strong and false positives are undesirable. In contrast, the higher recall of the ExG algorithm suggests it may be better applied detecting smaller weeds and reducing misses. OWL sets an open-source path for the weed control industry, to assist in the affordable, site-specific and effective control of weeds in a variety of scenarios.</p>
  </sec>
  <sec id="Sec8">
    <title>Methods</title>
    <sec id="Sec9">
      <title>Field data collection</title>
      <p id="Par22">Video data were collected in cropping fields with varying weed plant morphology and density, and different background conditions of soil colour, stubble and lighting for the validation of OWL using four separate colour-based algorithms. The collection of data was designed to replicate the in-field video environment of the OWL unit as closely as practicable. Landowners provided field access and permission to record videos in each field transect. Videos were collected using a handheld apparatus, which enabled concurrent collection of video data from two Raspberry Pi HQ cameras. Algorithms were selected via a rotary switch for each transect. Both computer-camera pairs were powered with a 12 V battery and a 5 V, 5 A voltage regulator (POLOLU-4091; Pololu Corporation). Recorded videos as well as average frame rates were stored on the onboard 64 GB micro-SD card and offloaded after each data collection. Real time clock modules (ADA3386; Adafruit Industries) ensured accurate timestamps of recorded videos. Five transects were recorded in daylight at five distinct field locations (Fig. <xref rid="Fig4" ref-type="fig">4</xref>), with a further two sites (including one used for daylight collection) used for collection of video data under artificial illumination with a Stedi C-4 Black Edition LED Light Cube. The 40 W light provides 4,200 Lm with a colour temperature of 5700 K and similar field of illumination to the FOV of the camera. The field sites represented likely use cases for fallow weed control, including canola, barley and wheat stubble and tilled soil at six locations in southern New South Wales, Australia (Table <xref rid="Tab2" ref-type="table">2</xref>). Transects of 50 m were traversed by walking at a target speed of approximately 4 km h<sup>−1</sup>, with the walking time for each transect recorded to determine true average speed (Table <xref rid="Tab2" ref-type="table">2</xref>). Based on the 1 m FOV of the camera each transect covered a total area of 50 m<sup>2</sup>, which was used to calculate weed density. The authors undertook in situ visual inspection of growth stage and identification of the weeds growing in the field at each of the sites where videos were collected. As all weeds identified are commonly occurring species with recognizable features no plant specimens were collected for subsequent formal identification (Table <xref rid="Tab2" ref-type="table">2</xref>).<fig id="Fig4"><label>Figure 4</label><caption><p>Representative images of the variable background and lighting conditions for seven image collection scenarios, (<bold>a</bold>) HEN1, (<bold>b</bold>) HEN2, (<bold>c</bold>) NIGHT1, (<bold>d</bold>) NIGHT2, (<bold>e</bold>) WAG1, (<bold>f</bold>) WAG2 and (<bold>g</bold>) COB1, used to evaluate the performance of colour-based weed detection.</p></caption><graphic xlink:href="41598_2021_3858_Fig4_HTML" id="MO4"/></fig><table-wrap id="Tab2"><label>Table 2</label><caption><p>Summary of field locations, weed species, background conditions, weed growth stage range and image collection speeds (n = 5) in fields used for video data collection and analysis.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Field ID</th><th align="left">Location</th><th align="left">Coordinates</th><th align="left">Light conditions</th><th align="left">Background</th><th align="left">Weeds present</th><th align="left">Weed density (plants m<sup>−2</sup>)</th><th align="left">Weed growth stages</th><th align="left">Average speed (m s<sup>−1</sup> ± SE)</th></tr></thead><tbody><tr><td align="left">HEN1</td><td align="left">Henty, NSW</td><td align="left">− 35.517102, 147.034436</td><td align="left">Clear, morning full sun</td><td align="left">Canola stubble, red–orange soil</td><td align="left">annual sowthistle (<italic>Sonchus oleraceus</italic>), volunteer canola (<italic>Brassica napus</italic>), annual ryegrass (<italic>Lolium rigidum</italic>), volunteer faba bean (<italic>Vicia faba</italic>)</td><td char="." align="char">3.1</td><td align="left">2-leaf to flowering</td><td char="±" align="char">1.14 ± 0.02</td></tr><tr><td align="left">HEN2</td><td align="left">Henty, NSW</td><td align="left">− 35.517102, 147.034436</td><td align="left">Clear, afternoon full sun</td><td align="left">Heavy wheat stubble, red soil</td><td align="left">Volunteer wheat (<italic>Triticum aestivum</italic>), annual sowthistle, annual ryegrass</td><td char="." align="char">9.3</td><td align="left">2-leaf to late tillering</td><td char="±" align="char">1.16 ± 0.01</td></tr><tr><td align="left">WAG1</td><td align="left">Wagga Wagga, NSW</td><td align="left">− 35.056986, 147.351146</td><td align="left">Clear, morning full sun</td><td align="left">Lupin stubble, red–orange soil</td><td align="left">Volunteer narrowleaf lupins (<italic>Lupinus angustifolius</italic>), annual sowthistle</td><td char="." align="char">18.7</td><td align="left">2-leaf to flowering</td><td char="±" align="char">1.14 ± 0.01</td></tr><tr><td align="left">WAG2</td><td align="left">Wagga Wagga, NSW</td><td align="left">− 35.056986, 147.351146</td><td align="left">Clear, morning full sun</td><td align="left">Grazed barley stubble</td><td align="left">Volunteer barley (<italic>Hordeum vulgare</italic>), annual sowthistle</td><td char="." align="char">3.3</td><td align="left">2-leaf to 8-leaf</td><td char="±" align="char">1.24 ± 0.03</td></tr><tr><td align="left">COB1</td><td align="left">Cobbitty, NSW</td><td align="left">− 34.021914, 150.662655</td><td align="left">Overcast</td><td align="left">Dark brown soil, freshly tilled, no soil cover</td><td align="left">Wild radish (<italic>Raphanus raphanistrum</italic>), fumitory (<italic>Fumaria officinalis</italic>), large crabgrass (<italic>Digitaria sanguinalis</italic>), billygoat weed (<italic>Ageratum conyzoides</italic>), stagger weed (<italic>Stachys arvensis</italic>)</td><td char="." align="char">9.8</td><td align="left">Cotyledon to 6-leaf</td><td char="±" align="char">1.07 ± 0.01</td></tr><tr><td align="left">NIGHT1</td><td align="left">Culcairn, NSW</td><td align="left">− 35.667692, 147.036800</td><td align="left">Night</td><td align="left">Canola stubble</td><td align="left">annual sowthistle, khaki weed (<italic>Alternanthera pungens</italic>), awnless barnyardgrass (<italic>Echinocloa colona</italic>), annual ryegrass, common catsear (<italic>Hypochaeris radicata</italic>)</td><td char="." align="char">9.6</td><td align="left">2-leaf to flowering</td><td char="±" align="char">1.23 ± 0.01</td></tr><tr><td align="left">NIGT2</td><td align="left">Cobbitty, NSW</td><td align="left">− 34.021914, 150.662655</td><td align="left">Night</td><td align="left">Dark brown soil, freshly tilled, no soil cover</td><td align="left">Wild radish, fumitory, large crabgrass, billygoat weed, stagger weed</td><td char="." align="char">7.8</td><td align="left">Cotyledon to 6-leaf</td><td char="±" align="char">0.83 ± 0.01</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec10">
      <title>In field validation and evaluation of weed detection algorithms</title>
      <p id="Par23">Four colour-based algorithms that exploited the greenness of weeds for detection were used to validate OWL hardware. The algorithms were selected based on ease of implementation, alignment with human colour perception<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> and use in weed detection and vegetation segmentation<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR66">66</xref>–<xref ref-type="bibr" rid="CR68">68</xref></sup>: (1) raw excess green (ExG); (2) normalised excess green (NExG); (3) hue, saturation, value (HSV). A combined ExG and HSV (4) algorithm (ExHSV) (Fig. <xref rid="Fig5" ref-type="fig">5</xref>) was implemented to manage the sensitivity of ExG to expected changes in brightness in field conditions. Algorithm indices were calculated on a frame-by-frame basis by splitting each image into colour channels. Thresholds were then applied in conjunction with morphological operations to remove image noise. Remaining areas were marked as positive detections. All image processing software was written in Python 3.6<sup><xref ref-type="bibr" rid="CR69">69</xref></sup> and completed using OpenCV<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, NumPy<sup><xref ref-type="bibr" rid="CR71">71</xref></sup> and imutils<sup><xref ref-type="bibr" rid="CR72">72</xref></sup> in addition to inbuilt libraries.<fig id="Fig5"><label>Figure 5</label><caption><p>Overview of the frame-by-frame analysis process. Each 416 × 320 image is split into either red, green and blue (RGB) or hue, saturation and value (HSV) channels, and the ExG, NExG, ExHSV or HSV algorithm applied. A defined threshold is applied to the processed image followed by an adaptive threshold on the result (except HSV which is already binary) followed by contour detection and the generation of minimum enclosing rectangles for weed centre calculation.</p></caption><graphic xlink:href="41598_2021_3858_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par24">For (1) ExG, the algorithm is adapted from Woebbecke et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ExG= 2G-R-B$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>G</mml:mi><mml:mo>-</mml:mo><mml:mi>R</mml:mi><mml:mo>-</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2021_3858_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where G, R and B represent the raw pixel intensities for the green, red and blue channels, respectively of the digital camera image. The raw channel intensities may be influenced by environmental lighting conditions, which can be minimised by normalising individual channel values by the sum of all channels:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r=\frac{R}{R+G+B}, g= \frac{G}{R+G+B}, b= \frac{B}{R+G+B}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>G</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>B</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2021_3858_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par25">The resultant values from 0 to 1 were scaled by 255. The (2) NExG algorithm was calculated using the normalized channel intensities:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$NExG=2g-r-b$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mi>N</mml:mi><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>g</mml:mi><mml:mo>-</mml:mo><mml:mi>r</mml:mi><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math><graphic xlink:href="41598_2021_3858_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par26">On the resultant grayscale image within the threshold bounds of both ExG and NExG, an adaptive threshold was applied generating a binary (black and white only) masking image.</p>
      <p id="Par27">For (3) HSV, the colour space was converted from the RGB colour space to HSV using inbuilt OpenCV functions, with thresholds applied to each of the channels (Table <xref rid="Tab3" ref-type="table">3</xref>). The minimum and maximum values for each threshold were manually selected to minimise over sensitivity, whilst maximising the number of true positives. The combined (4) ExHSV algorithm required a value to be both within the NExG threshold and the HSV binary region.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Threshold parameters used for each of the four algorithms, where relevant.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="3">Parameters</th><th align="left" colspan="6">Day</th><th align="left" colspan="6">Night</th></tr><tr><th align="left" colspan="2">ExG/NExG</th><th align="left" colspan="2">ExHSV</th><th align="left" colspan="2">HSV</th><th align="left" colspan="2">ExG/NExG</th><th align="left" colspan="2">ExHSV</th><th align="left" colspan="2">HSV</th></tr><tr><th align="left">Min</th><th align="left">Max</th><th align="left">Min</th><th align="left">Max</th><th align="left">Min</th><th align="left">Max</th><th align="left">Min</th><th align="left">Max</th><th align="left">Min</th><th align="left">Max</th><th align="left">Min</th><th align="left">Max</th></tr></thead><tbody><tr><td align="left">ExG</td><td align="left">13</td><td align="left">200</td><td char="." align="char">13</td><td align="left">200</td><td align="left">–</td><td align="left">–</td><td align="left">29</td><td align="left">200</td><td char="." align="char">29</td><td align="left">200</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">Hue</td><td align="left">–</td><td align="left">–</td><td char="." align="char">30</td><td align="left">92</td><td align="left">35</td><td align="left">84</td><td align="left">–</td><td align="left">–</td><td char="." align="char">30</td><td align="left">92</td><td align="left">45</td><td align="left">80</td></tr><tr><td align="left">Saturation</td><td align="left">–</td><td align="left">–</td><td char="." align="char">4</td><td align="left">250</td><td align="left">10</td><td align="left">220</td><td align="left">–</td><td align="left">–</td><td char="." align="char">10</td><td align="left">250</td><td align="left">75</td><td align="left">200</td></tr><tr><td align="left">Value</td><td align="left">–</td><td align="left">–</td><td char="." align="char">15</td><td align="left">250</td><td align="left">50</td><td align="left">200</td><td align="left">–</td><td align="left">–</td><td char="." align="char">60</td><td align="left">250</td><td align="left">46</td><td align="left">240</td></tr><tr><td align="left">Object size (pixels)</td><td align="left">10</td><td align="left">–</td><td char="." align="char">10</td><td align="left">–</td><td align="left">10</td><td align="left">–</td><td align="left">10</td><td align="left">–</td><td char="." align="char">10</td><td align="left">–</td><td align="left">10</td><td align="left">–</td></tr></tbody></table><table-wrap-foot><p>Values represent pixel intensities for zero-indexed 8-bit arrays with a range of 0–255. Pixel values that did not sit within the ranges were excluded, hence leaving only green pixels as the detected object. Separate thresholds were used for the day and night videos. A minimum object size was implemented to reduce noise and is based on the area of each detected object. Values were selected manually to optimize algorithm performance.</p></table-wrap-foot></table-wrap></p>
      <p id="Par28">For the resultant binary images, the within-image coordinates and size of each detection were returned for allocation to specific activation zones (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). The frame rates for each algorithm were recorded over five separate transects each 60 s in duration to ensure consistency in reporting.</p>
    </sec>
    <sec id="Sec11">
      <title>Video analysis of algorithm performance</title>
      <p id="Par29">Videos collected by the handheld video apparatus at a resolution of 416 × 320 pixels were analysed using standard, desktop computers on a frame-by-frame basis. Each frame was processed using the respective algorithms and threshold settings (Table <xref rid="Tab3" ref-type="table">3</xref>), whereby detections were displayed as red boxes (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). A separate, high-definition video of the transect was used to count all weeds within the field of view for the ground-truth data. True and false positives were recorded by comparison with the high-definition video. Algorithm performance was measured by calculating recall (Eq. <xref rid="Equ4" ref-type="">4</xref>) and precision (Eq. <xref rid="Equ5" ref-type="">5</xref>). Recall refers to the proportion of weeds detected when compared with all those present in the transect. Precision refers to the proportion of detections that were correct.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Recall= \frac{True\; Positives}{Total\; Weeds\; Present}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.277778em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.277778em"/><mml:mi>W</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.277778em"/><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2021_3858_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Precision= \frac{True\; Positives}{True\; Positives+False\; Positives}$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.277778em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.277778em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.277778em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2021_3858_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec12">
      <title>Statistical analysis</title>
      <p id="Par30">A one-way analysis of variance (ANOVA) was used to compare means of precision, recall and framerates across all fields implemented in RStudio<sup><xref ref-type="bibr" rid="CR73">73</xref>,<xref ref-type="bibr" rid="CR74">74</xref></sup>. The Shapiro–Wilkes test (P &gt; 0.05) was used to test for normality. Precision data were transformed using a fifth power due to a negative skew in the distribution. Homogeneity of variance for recall and the transformed precision data were assessed with the Bartlett (P &gt; 0.05) and Fligner–Killeen tests (P &gt; 0.05). Data were visualised with ggplot2<sup><xref ref-type="bibr" rid="CR75">75</xref></sup> in RStudio. Pair-wise comparisons of framerates were made with the Agricolae package<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>. Illustrative figures were composed in Adobe Illustrator (v 24.4.1; Adobe Inc., San Jose, CA, USA).</p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to thank all growers and agronomists involved in the data collection process. The project was funded by the Grains Research and Development Corporation (GRDC) and the National Landcare Program Smart Farming Partnerships.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>G.C. constructed the first prototype device. G.C. and W.T.S. constructed the second prototype device. G.C. and W.T.S. developed the methodology, collected and analysed image data and prepared the figures. M.W. advised on methodology development. All authors contributed equally to writing, editing and reviewing of the manuscript and approved the final version.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par31">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Verburg</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Bond</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Hunt</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>Fallow management in dryland agriculture: Explaining soil water accumulation using a pulse paradigm</article-title>
        <source>F. Crop. Res.</source>
        <year>2012</year>
        <volume>130</volume>
        <fpage>68</fpage>
        <lpage>79</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thomas</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Titmarsh</surname>
            <given-names>GW</given-names>
          </name>
          <name>
            <surname>Freebairn</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Radford</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>No-tillage and conservation farming practices in grain growing areas of Queensland—A review of 40 years of development</article-title>
        <source>Aust. J. Exp. Agric.</source>
        <year>2007</year>
        <volume>47</volume>
        <fpage>887</fpage>
        <lpage>898</lpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ortiz-monasterio</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Lobell</surname>
            <given-names>DB</given-names>
          </name>
        </person-group>
        <article-title>Remote sensing assessment of regional yield losses due to sub-optimal planting dates and fallow period weed management</article-title>
        <source>F. Crop. Res.</source>
        <year>2007</year>
        <volume>101</volume>
        <fpage>80</fpage>
        <lpage>87</lpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roget</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Venn</surname>
            <given-names>NR</given-names>
          </name>
          <name>
            <surname>Rovira</surname>
            <given-names>AD</given-names>
          </name>
        </person-group>
        <article-title>Reduction of Rhizoctonia root rot of direct-drilled wheat by short-term chemical fallow</article-title>
        <source>Aust. J. Exp. Agric.</source>
        <year>1987</year>
        <volume>27</volume>
        <fpage>425</fpage>
        <lpage>430</lpage>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Dunsford, K., Nuttall, J., Armstrong, R. &amp; O’Leary, G. Yield benefits of fallow to high value crops. In <italic>Cells to Satellites: Proceedings of the 2019 Agronomy Australia Conference</italic> (ed. Pratley, J.) 1–4 (2019).</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hunt</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Kirkegaard</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>Re-evaluating the contribution of summer fallow rain to wheat yield in southern Australia</article-title>
        <source>Crop Pasture Sci.</source>
        <year>2011</year>
        <volume>62</volume>
        <fpage>915</fpage>
        <lpage>929</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fernandez</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A study of the effect of the interaction between site-specific conditions, residue cover and weed control on water storage during fallow</article-title>
        <source>Agric. Water Manag.</source>
        <year>2008</year>
        <volume>95</volume>
        <fpage>1028</fpage>
        <lpage>1040</lpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shearer</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>PT</given-names>
          </name>
        </person-group>
        <article-title>Selective application of post-emergence herbicides using photoelectrics</article-title>
        <source>Trans. ASAE</source>
        <year>1991</year>
        <volume>34</volume>
        <fpage>1661</fpage>
        <lpage>1666</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Felton, W. L., Doss, A. F., Nash, P. G. &amp; McCloy, K. R. A microprocessor controlled technology to selectively spot spray weeds. In <italic>Automated Agriculture for the 21st Century: Proceedings of the 1991 Symposium</italic> 427–432 (American Society of Agricultural Engineers, 1991).</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Visser, R. &amp; Timmermans, A. Weed-It: A new selective weed control system. In <italic>Optics in Agriculture, Forestry and Biological Processing II</italic> (eds. Meyer, G. E. &amp; DeShazer, J. A.) 120–129 (SPIE, 1996). doi:10.1117/12.262852.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haggar</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Stent</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Isaac</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A prototype hand-held patch sprayer for killing weeds, activated by spectral differences in crop/weed canopies</article-title>
        <source>J. Agric. Eng. Res.</source>
        <year>1983</year>
        <volume>28</volume>
        <fpage>349</fpage>
        <lpage>358</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peteinatos</surname>
            <given-names>GG</given-names>
          </name>
          <name>
            <surname>Weis</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Andújar</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rueda Ayala</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Gerhards</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Potential use of ground-based sensor technologies for weed detection</article-title>
        <source>Pest Manag. Sci.</source>
        <year>2014</year>
        <volume>70</volume>
        <fpage>190</fpage>
        <lpage>199</lpage>
        <?supplied-pmid 24203911?>
        <pub-id pub-id-type="pmid">24203911</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Mccarthy, C., Rees, S. &amp; Baillie, C. Machine vision-based weed spot sprying: A review and where next for sugarcane? In <italic>32nd Annual Conference of the Australian Society of Sugar Cane Technologists</italic> vol. 32 424–432 (Australian Society of Sugar Cane Technologists, 2010).</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">SPAA. <italic>SPAA precision Ag fact sheet: Weed sensing</italic>. <ext-link ext-link-type="uri" xlink:href="https://www.spaa.com.au/pdf/456_9056_SPAA_fact_sheet_(Weed_Sensing)_A4.pdf">https://www.spaa.com.au/pdf/456_9056_SPAA_fact_sheet_(Weed_Sensing)_A4.pdf</ext-link> (2016).</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Timmermann</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Gerhards</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kühbauch</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>The economic impact of site-specific weed control</article-title>
        <source>Precis. Agric.</source>
        <year>2003</year>
        <volume>4</volume>
        <fpage>249</fpage>
        <lpage>260</lpage>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peterson</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Collavo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ovejero</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Shivrain</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Walsh</surname>
            <given-names>MJ</given-names>
          </name>
        </person-group>
        <article-title>The challenge of herbicide resistance around the world: A current summary</article-title>
        <source>Pest Manag. Sci.</source>
        <year>2018</year>
        <volume>74</volume>
        <fpage>2246</fpage>
        <lpage>2259</lpage>
        <?supplied-pmid 29222931?>
        <pub-id pub-id-type="pmid">29222931</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thompson</surname>
            <given-names>JF</given-names>
          </name>
          <name>
            <surname>Stafford</surname>
            <given-names>JV</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>PCHH</given-names>
          </name>
        </person-group>
        <article-title>Potential for automatic weed detection and selective herbicide application</article-title>
        <source>Crop Prot.</source>
        <year>1991</year>
        <volume>10</volume>
        <fpage>254</fpage>
        <lpage>259</lpage>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Woebbecke</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Meyer</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Von Bargen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mortensen</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Color indices for weed identification under various soil, residue, and lighting conditions</article-title>
        <source>Trans. Am. Soc. Agric. Eng.</source>
        <year>1995</year>
        <volume>38</volume>
        <fpage>259</fpage>
        <lpage>269</lpage>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>A review on weed detection using ground-based machine vision and image processing techniques</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2019</year>
        <volume>158</volume>
        <fpage>226</fpage>
        <lpage>240</lpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Severance</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Eben Upton: Raspberry Pi</article-title>
        <source>Computer</source>
        <year>2013</year>
        <volume>46</volume>
        <fpage>14</fpage>
        <lpage>16</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Esau</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine vision smart sprayer for spot-application of agrochemical in wild blueberry fields</article-title>
        <source>Precis. Agric.</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>770</fpage>
        <lpage>788</lpage>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burks</surname>
            <given-names>TF</given-names>
          </name>
          <name>
            <surname>Shearer</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Gates</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Donohue</surname>
            <given-names>KD</given-names>
          </name>
        </person-group>
        <article-title>Backpropagation neural network design and evaluation for classifying weed species using color image texture</article-title>
        <source>Trans. ASAE</source>
        <year>2000</year>
        <volume>43</volume>
        <fpage>1029</fpage>
        <lpage>1037</lpage>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>WS</given-names>
          </name>
          <name>
            <surname>Slaughter</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Giles</surname>
            <given-names>DK</given-names>
          </name>
        </person-group>
        <article-title>Robotic weed control system for tomatoes</article-title>
        <source>Precis. Agric.</source>
        <year>1999</year>
        <volume>1</volume>
        <fpage>95</fpage>
        <lpage>113</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Woebbecke</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Meyer</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Von Bargen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Mortensen</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Shape features for identifying young weeds using image analysis</article-title>
        <source>Trans. ASAE</source>
        <year>1995</year>
        <volume>38</volume>
        <fpage>271</fpage>
        <lpage>281</lpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Development of color co-occurrence matrix based machine vision algorithms for wild blueberry fields</article-title>
        <source>Appl. Eng. Agric.</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>315</fpage>
        <lpage>323</lpage>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tian</surname>
            <given-names>LF</given-names>
          </name>
          <name>
            <surname>Reid</surname>
            <given-names>JF</given-names>
          </name>
        </person-group>
        <article-title>Development of a precision sprayer for site-specific weed management</article-title>
        <source>Trans. ASAE</source>
        <year>1999</year>
        <volume>42</volume>
        <fpage>893</fpage>
        <lpage>900</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Golzarian</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Frick</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>Classification of images of wheat, ryegrass and brome grass species at early growth stages using principal component analysis</article-title>
        <source>Plant Methods</source>
        <year>2011</year>
        <volume>7</volume>
        <fpage>28</fpage>
        <?supplied-pmid 21943349?>
        <pub-id pub-id-type="pmid">21943349</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kavdir</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Discrimination of sunflower, weed and soil by artificial neural networks</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2004</year>
        <volume>44</volume>
        <fpage>153</fpage>
        <lpage>160</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burgos-Artizzu</surname>
            <given-names>XP</given-names>
          </name>
          <name>
            <surname>Ribeiro</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Guijarro</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pajares</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Real-time image processing for crop/weed discrimination in maize fields</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2011</year>
        <volume>75</volume>
        <fpage>337</fpage>
        <lpage>346</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chechliński</surname>
            <given-names>Ł</given-names>
          </name>
          <name>
            <surname>Siemiątkowska</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Majewski</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A system for weeds and crops identification—reaching over 10 fps on Raspberry Pi with the usage of MobileNets, DenseNet and custom modifications</article-title>
        <source>Sensors</source>
        <year>2019</year>
        <volume>19</volume>
        <fpage>3787</fpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tufail</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of tobacco crop based on machine learning for a precision agricultural sprayer</article-title>
        <source>IEEE Access</source>
        <year>2021</year>
        <volume>9</volume>
        <fpage>23814</fpage>
        <lpage>23825</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pearce</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Return on investment for open source scientific hardware development</article-title>
        <source>Sci. Public Policy</source>
        <year>2016</year>
        <volume>43</volume>
        <fpage>192</fpage>
        <lpage>195</lpage>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Agarwal</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Ford multi-AV seasonal dataset</article-title>
        <source>Int. J. Rob. Res.</source>
        <year>2020</year>
        <volume>39</volume>
        <fpage>1367</fpage>
        <lpage>1376</lpage>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pereira</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>The need for open source software in machine learning</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>2443</fpage>
        <lpage>2466</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kochhar</surname>
            <given-names>PS</given-names>
          </name>
          <name>
            <surname>Kalliamvakou</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Nagappan</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Zimmermann</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bird</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Moving from closed to open source: Observations from six transitioned projects to GitHub</article-title>
        <source>IEEE Trans. Softw. Eng.</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1109/tse.2019.2937025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Watcharaanantapong</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Timing of precision agriculture technology adoption in US cotton production</article-title>
        <source>Precis. Agric.</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>427</fpage>
        <lpage>446</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ferdoush</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Wireless sensor network system design using Raspberry Pi and Arduino for environmental monitoring applications</article-title>
        <source>Procedia Comput. Sci.</source>
        <year>2014</year>
        <volume>34</volume>
        <fpage>103</fpage>
        <lpage>110</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Tso, F. P., White, D. R., Jouet, S., Singer, J. &amp; Pezaros, D. P. The Glasgow raspberry Pi cloud: A scale model for cloud computing infrastructures. In <italic>33rd International Conference on Distributed Computing Systems Workshops</italic> 108–112 (IEEE, 2013). 10.1109/ICDCSW.2013.25.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Vasilescu, B., Filkov, V. &amp; Serebrenik, A. StackOverflow and GitHub: Associations between software development and crowdsourced knowledge. In <italic>International Conference on Social Computing (SocialCom)</italic> 188–195 (IEEE, 2013). 10.1109/SocialCom.2013.35.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Walsh</surname>
            <given-names>MJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Tillage based, site-specific weed control for conservation cropping systems</article-title>
        <source>Weed Technol.</source>
        <year>2020</year>
        <pub-id pub-id-type="doi">10.1017/wet.2020.34</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adler</surname>
            <given-names>PS</given-names>
          </name>
          <name>
            <surname>Clark</surname>
            <given-names>KB</given-names>
          </name>
        </person-group>
        <article-title>Behind the learning curve: A sketch of the learning process</article-title>
        <source>Manage. Sci.</source>
        <year>1991</year>
        <volume>37</volume>
        <fpage>267</fpage>
        <lpage>281</lpage>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rosenberg</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <source>Inside the Black Box: Technology and Economics</source>
        <year>1982</year>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Abadi, M. <italic>et al.</italic> Tensorflow: A system for large-scale machine learning. In <italic>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</italic> vol. 101 265–283 (USENIX Association, 2016).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Paszke, A. <italic>et al.</italic> Pytorch: An imperative style, high-performance deep learning library. In <italic>Advances in Neural Information Processing Systems 32</italic> (eds. Wallach, H. et al.) 8026–8037 (2019).
</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tournier</surname>
            <given-names>JD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MRtrix3: A fast, flexible and open software framework for medical image processing and visualisation</article-title>
        <source>Neuroimage</source>
        <year>2019</year>
        <volume>202</volume>
        <fpage>116137</fpage>
        <?supplied-pmid 31473352?>
        <pub-id pub-id-type="pmid">31473352</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fischl</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>FreeSurfer</article-title>
        <source>Neuroimage</source>
        <year>2012</year>
        <volume>62</volume>
        <fpage>774</fpage>
        <lpage>781</lpage>
        <?supplied-pmid 22248573?>
        <pub-id pub-id-type="pmid">22248573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schindelin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rueden</surname>
            <given-names>CT</given-names>
          </name>
          <name>
            <surname>Hiner</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Eliceiri</surname>
            <given-names>KW</given-names>
          </name>
        </person-group>
        <article-title>The ImageJ ecosystem: An open platform for biomedical image analysis</article-title>
        <source>Mol. Reprod. Dev.</source>
        <year>2015</year>
        <volume>82</volume>
        <fpage>518</fpage>
        <lpage>529</lpage>
        <?supplied-pmid 26153368?>
        <pub-id pub-id-type="pmid">26153368</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Kato, S. <italic>et al.</italic> Autoware on board: Enabling autonomous vehicles with embedded systems. In <italic>Proceedings 9th ACM/IEEE International Conference Cyber-Physical Systems ICCPS 2018</italic> 287–296 (2018). 10.1109/ICCPS.2018.00035.</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goldfain</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>AutoRally: An open platform for aggressive autonomous driving</article-title>
        <source>IEEE Control Syst.</source>
        <year>2019</year>
        <volume>39</volume>
        <fpage>26</fpage>
        <lpage>55</lpage>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Challet</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>YL</given-names>
          </name>
        </person-group>
        <article-title>Microscopic model of software bug dynamics: Closed source versus open source</article-title>
        <source>Int. J. Reliab. Qual. Saf. Eng.</source>
        <year>2005</year>
        <volume>12</volume>
        <fpage>521</fpage>
        <lpage>534</lpage>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Tischler, B. AgOpenGPS. <italic>GitHub</italic><ext-link ext-link-type="uri" xlink:href="https://github.com/farmerbriantee/AgOpenGPS">https://github.com/farmerbriantee/AgOpenGPS</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Stenta, M. FarmOS. <italic>GitHub</italic><ext-link ext-link-type="uri" xlink:href="https://github.com/farmos/farmos">https://github.com/farmos/farmos</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Olsen</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepWeeds: A multiclass weed species image dataset for deep learning</article-title>
        <source>Sci. Rep.</source>
        <year>2019</year>
        <volume>9</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="pmid">30626917</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chebrolu</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Agricultural robot dataset for plant classification, localization and mapping on sugar beet fields</article-title>
        <source>Int. J. Rob. Res.</source>
        <year>2017</year>
        <volume>36</volume>
        <fpage>1045</fpage>
        <lpage>1052</lpage>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brodribb</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Carriqui</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Delzon</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lucani</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Optical measurement of stem xylem vulnerability</article-title>
        <source>Plant Physiol.</source>
        <year>2017</year>
        <volume>174</volume>
        <fpage>2054</fpage>
        <lpage>2061</lpage>
        <?supplied-pmid 28684434?>
        <pub-id pub-id-type="pmid">28684434</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>HD</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>XH</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Color image segmentation: Advances and prospects</article-title>
        <source>Pattern Recognit.</source>
        <year>2001</year>
        <volume>34</volume>
        <fpage>2259</fpage>
        <lpage>2281</lpage>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamuda</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Glavin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>A survey of image processing techniques for plant extraction and segmentation in the field</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2016</year>
        <volume>125</volume>
        <fpage>184</fpage>
        <lpage>199</lpage>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Maize and weed classification using color indices with support vector data description in outdoor fields</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2017</year>
        <volume>141</volume>
        <fpage>215</fpage>
        <lpage>222</lpage>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>El-Desouki</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CMOS image sensors for high speed applications</article-title>
        <source>Sensors</source>
        <year>2009</year>
        <volume>9</volume>
        <fpage>430</fpage>
        <lpage>444</lpage>
        <?supplied-pmid 22389609?>
        <pub-id pub-id-type="pmid">22389609</pub-id>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sapkota</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Thomasson</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Bagavathiannan</surname>
            <given-names>MV</given-names>
          </name>
        </person-group>
        <article-title>Influence of image quality and light consistency on the performance of convolutional neural networks for weed mapping</article-title>
        <source>Remote Sens.</source>
        <year>2021</year>
        <volume>13</volume>
        <fpage>2140</fpage>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kawamura</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Asai</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yasuda</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Soisouvanh</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Phongchanmixay</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Discriminating crops/weeds in an upland rice field from UAV images with the SLIC-RF algorithm</article-title>
        <source>Plant Prod. Sci.</source>
        <year>2021</year>
        <volume>24</volume>
        <fpage>198</fpage>
        <lpage>215</lpage>
      </element-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <mixed-citation publication-type="other">Morgand, A. &amp; Tamaazousti, M. Generic and real-time detection of specular reflections in images. In <italic>2014 International Conference on Computer Vision Theory and Applications (VISAPP)</italic> vol. 1 274–282 (IEEE, 2014).</mixed-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <mixed-citation publication-type="other">Milioto, A., Lottes, P. &amp; Stachniss, C. Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in CNNs. in <italic>Proceedings - IEEE International Conference on Robotics and Automation</italic> 2229–2235 (IEEE, 2018). 10.1109/ICRA.2018.8460962.</mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Martin, S. Harvesting AI: Startup’s weed recognition for herbicides grows yield for farmers. <italic>NVIDIA Blog</italic><ext-link ext-link-type="uri" xlink:href="https://blogs.nvidia.com/blog/2021/04/05/bilberry-weed-recognition-ai-grows-yield-for-farmers/">https://blogs.nvidia.com/blog/2021/04/05/bilberry-weed-recognition-ai-grows-yield-for-farmers/</ext-link> (2021).</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Calvert</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Whinney</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Azghadi</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Robotic spot spraying of Harrisia cactus (Harrisia martinii) in grazing pastures of the Australian rangelands</article-title>
        <source>Plants</source>
        <year>2021</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>18</lpage>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Kazmi, W., Garcia-Ruiz, F. J., Nielsen, J., Rasmussen, J. &amp; Jørgen Andersen, H. Detecting creeping thistle in sugar beet fields using vegetation indices. <italic>Comput. Electron. Agric.</italic><bold>112</bold>, 10–19 (2015).</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guijarro</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic segmentation of relevant textures in agricultural images</article-title>
        <source>Comput. Electron. Agric.</source>
        <year>2011</year>
        <volume>75</volume>
        <fpage>75</fpage>
        <lpage>83</lpage>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <mixed-citation publication-type="other">Hamuda, E., Mc Ginley, B., Glavin, M. &amp; Jones, E. Automatic crop detection under field conditions using the HSV colour space and morphological operations. <italic>Comput. Electron. Agric.</italic><bold>133</bold>, 97–107 (2017).</mixed-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <mixed-citation publication-type="other">Van Rossum, G. &amp; Drake, F. L. <italic>Python 3 Reference Manual</italic>. (CreateSpace, 2009).</mixed-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <mixed-citation publication-type="other">Bradski, G. The OpenCV Library. <italic>Dr. Dobb’s J. Softw. Tools</italic> (2000).</mixed-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harris</surname>
            <given-names>CR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Array programming with NumPy</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>585</volume>
        <fpage>357</fpage>
        <lpage>362</lpage>
        <?supplied-pmid 32939066?>
        <pub-id pub-id-type="pmid">32939066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <mixed-citation publication-type="other">Rosebrock, A. I just open sourced my personal imutils package: A series of OpenCV convenience functions. <italic>PyImageSearch</italic><ext-link ext-link-type="uri" xlink:href="https://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/">https://www.pyimagesearch.com/2015/02/02/just-open-sourced-personal-imutils-package-series-opencv-convenience-functions/</ext-link> (2015).</mixed-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <mixed-citation publication-type="other">RStudio Team. RStudio: Integrated Development Environment for R. (2015).</mixed-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <mixed-citation publication-type="other">R Core Team. R: A Language and Environment for Statistical Computing. <ext-link ext-link-type="uri" xlink:href="https://www.r-project.org/">https://www.r-project.org/</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Wickham</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <source>ggplot2: Elegant Graphics for Data Analysis</source>
        <year>2016</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <mixed-citation publication-type="other">de Mendiburu, F. Package ‘agricolae’. Statistical procedures for agricultural research. <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/agricolae/agricolae.pdf">https://cran.r-project.org/web/packages/agricolae/agricolae.pdf</ext-link> (2021).
</mixed-citation>
    </ref>
  </ref-list>
</back>
