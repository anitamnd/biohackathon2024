<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brief Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">bib</journal-id>
    <journal-title-group>
      <journal-title>Briefings in Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1467-5463</issn>
    <issn pub-type="epub">1477-4054</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8921635</article-id>
    <article-id pub-id-type="pmid">35022651</article-id>
    <article-id pub-id-type="doi">10.1093/bib/bbab550</article-id>
    <article-id pub-id-type="publisher-id">bbab550</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Problem Solving Protocol</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CRISP: a deep learning architecture for GC × GC–TOFMS contour ROI identification, simulation and analysis in imaging metabolomics</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3916-9949</contrib-id>
        <name>
          <surname>Mathema</surname>
          <given-names>Vivek Bhakta</given-names>
        </name>
        <!--vivek_mathema@hotmail.com-->
        <aff><institution>Metabolomics and Systems Biology, Department of Biochemistry, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
        <aff><institution>Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Duangkumpha</surname>
          <given-names>Kassaporn</given-names>
        </name>
        <!--kassaporn@yahoo.com-->
        <aff><institution>Metabolomics and Systems Biology, Department of Biochemistry, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
        <aff><institution>Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wanichthanarak</surname>
          <given-names>Kwanjeera</given-names>
        </name>
        <!--kwanjeeraw@gmail.com-->
        <aff><institution>Metabolomics and Systems Biology, Department of Biochemistry, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
        <aff><institution>Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jariyasopit</surname>
          <given-names>Narumol</given-names>
        </name>
        <!--narumoljariyasopit@gmail.com-->
        <aff><institution>Metabolomics and Systems Biology, Department of Biochemistry, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
        <aff><institution>Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dhakal</surname>
          <given-names>Esha</given-names>
        </name>
        <!--esha.dhakal@gmail.com-->
        <aff><institution>Metabolomics and Systems Biology, Department of Biochemistry, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
        <aff><institution>Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sathirapongsasuti</surname>
          <given-names>Nuankanya</given-names>
        </name>
        <!--nuankanya.sat@mahidol.edu-->
        <aff><institution>Section of Translational Medicine, Faculty of Medicine Ramathibodi Hospital, Mahidol University</institution>, Bangkok, <country country="TH">Thailand</country></aff>
        <aff><institution>Research Network of NANOTEC – MU Ramathibodi on Nanomedicine</institution>, Bangkok, <country country="TH">Thailand</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kitiyakara</surname>
          <given-names>Chagriya</given-names>
        </name>
        <!--kitiyakc@yahoo.com-->
        <aff><institution>Department of Medicine, Faculty of Medicine, Ramathibodi Hospital</institution>, Rama VI Rd., Ratchathewi, Bangkok 10400, <country country="TH">Thailand</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sirivatanauksorn</surname>
          <given-names>Yongyut</given-names>
        </name>
        <!--yongyut.sir@mahidol.ac.th-->
        <aff><institution>Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9461-8597</contrib-id>
        <name>
          <surname>Khoomrung</surname>
          <given-names>Sakda</given-names>
        </name>
        <!--sakda.kho@mahidol.edu-->
        <aff><institution>Metabolomics and Systems Biology, Department of Biochemistry, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
        <aff><institution>Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital, Mahidol University</institution>, Bangkok 10700, <country country="TH">Thailand</country></aff>
        <aff><institution>Center of Excellence for Innovation in Chemistry (PERCH-CIC), Faculty of Science, Mahidol University</institution>, Bangkok, <country country="TH">Thailand</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Corresponding author. Sakda Khoomrung, Metabolomics and Systems Biology, Department of Biochemistry, Faculty of Medicine Siriraj Hospital, Mahidol University, Bangkok 10700, Thailand; Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital, Mahidol University, Bangkok 10700, Thailand; Center of Excellence for Innovation in Chemistry (PERCH-CIC), Faculty of Science, Mahidol University, Bangkok, Thailand; Tel.: <phone>+66 24195510</phone>. E-mail: <email>sakda.kho@mahidol.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-01-11">
      <day>11</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <issue>2</issue>
    <elocation-id>bbab550</elocation-id>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>19</day>
        <month>11</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>11</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bbab550.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Two-dimensional gas chromatography–time-of-flight mass spectrometry (GC × GC–TOFMS) provides a large amount of molecular information from biological samples. However, the lack of a comprehensive compound library or customizable bioinformatics tool is currently a challenge in GC × GC–TOFMS data analysis. We present an open-source deep learning (DL) software called contour regions of interest (ROI) identification, simulation and untargeted metabolomics profiler (CRISP). CRISP integrates multiple customizable deep neural network architectures for assisting the semi-automated identification of ROIs, contour synthesis, resolution enhancement and classification of GC × GC–TOFMS-based contour images. The approach includes the novel aggregate feature representative contour (AFRC) construction and stacked ROIs. This generates an unbiased contour image dataset that enhances the contrasting characteristics between different test groups and can be suitable for small sample sizes. The utility of the generative models and the accuracy and efficacy of the platform were demonstrated using a dataset of GC × GC–TOFMS contour images from patients with late-stage diabetic nephropathy and healthy control groups. CRISP successfully constructed AFRC images and identified over five ROIs to create a deepstacked dataset. The high fidelity, 512 × 512-pixels generative model was trained as a generator with a Fréchet inception distance of &lt;47.00. The trained classifier achieved an AUROC of &gt;0.96 and a classification accuracy of &gt;95.00% for datasets with and without column bleed. Overall, CRISP demonstrates good potential as a DL-based approach for the rapid analysis of 4-D GC × GC–TOFMS untargeted metabolite profiles by directly implementing contour images. CRISP is available at <ext-link xlink:href="https://github.com/vivekmathema/GCxGC-CRISP" ext-link-type="uri">https://github.com/vivekmathema/GCxGC-CRISP</ext-link>.</p>
    </abstract>
    <abstract abstract-type="graphical">
      <title>Graphical Abstract</title>
      <p>
        <fig position="float" id="ga1">
          <label>Graphical Abstract</label>
          <graphic xlink:href="bbab550fx1" position="float"/>
        </fig>
      </p>
      <p>The CRISP software, which combines multiple deep learning models for the interpretation of two-dimensional gas chromatography–time-of-flight mass spectrometry (GC × GC–TOFMS) by directly processing contour images and assisting with the biological interpretation of results.</p>
    </abstract>
    <kwd-group>
      <kwd>chronic kidney disease</kwd>
      <kwd>imaging metabolomics</kwd>
      <kwd>deep learning</kwd>
      <kwd>bioinformatics</kwd>
      <kwd>GC × GC–TOF</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Mahidol University</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004156</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R016420001</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Research Council of Thailand</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004704</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Mahidol University</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004156</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>NRCT5-TRG63009-03</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Faculty of Medicine Ramathibodi Hospital Mahidol University</institution>
          </institution-wrap>
        </funding-source>
        <award-id>CF_62006</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Science and Technology Development Agency</institution>
            <institution-id institution-id-type="DOI">10.13039/501100004192</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>P-13-00505</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="17"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec3">
    <title>Introduction</title>
    <p>Gas chromatography–mass spectrometry (GC–MS) is one of the most widely used techniques for metabolic profiling of body fluids because of its high sensitivity, excellent separation performance and availability of databases. It serves as a molecular imaging apparatus and has been used for various medical applications such as metabolic disorder profiling and biomarker discovery for type 2 diabetes mellitus, end-stage renal disease (ESRD) and chronic kidney disease (CKD) [<xref rid="ref1" ref-type="bibr">1–4</xref>]. Comprehensive two-dimensional gas chromatography–time-of-flight mass spectrometry (GC × GC–TOFMS) is a significant advancement over traditional GC–MS, improving the separation power and identification of molecule constituents in the spatial dimensions while adding a second retention time dimension [<xref rid="ref5" ref-type="bibr">5</xref>]. GC × GC–TOFMS, when coupled with a high-resolution mass spectrometer, enables detailed analysis of biological fluids with better peak separation and segregation of molecular intensities [<xref rid="ref6" ref-type="bibr">6</xref>, <xref rid="ref7" ref-type="bibr">7</xref>]. The technique has proved to be extremely effective in evaluating trace components in complex mixtures such as human plasma [<xref rid="ref4" ref-type="bibr">4</xref>, <xref rid="ref8" ref-type="bibr">8</xref>]. In particular, a contour image obtained from GC × GC–TOFMS defines the ionized molecular features of high-dimensional chromatogram data as a multichannel two-dimensional image that can be used for rapid analysis of the aggregate sample characteristics [<xref rid="ref9" ref-type="bibr">9</xref>]. However, limited bioinformatic tools, a lack of standardized protocols for data analysis, and inadequate mass spectral libraries are currently the major challenges faced in GC × GC–TOFMS data analysis. The complex nature of GC × GC–TOFMS and relatively expensive running costs make repeated sample runs difficult for large database generation and analysis. The conventional methods of metabolomics data analysis are also time-consuming and require multiple manual data preprocessing and analysis steps [<xref rid="ref10" ref-type="bibr">10</xref>]. Furthermore, slight unpredictable shifting in the two-dimensional retention times of each metabolite during chromatographic separation makes it difficult to precisely identify peaks or screen samples using conventional analysis [<xref rid="ref3" ref-type="bibr">3</xref>]. Given these challenges, advanced computational approaches are necessary for GC × GC–TOFMS data analysis.</p>
    <p>Our recent review explored possible applications of deep learning (DL), a branch of artificial intelligence, in metabolomics research [<xref rid="ref11" ref-type="bibr">11–13</xref>]. DL, particularly in the field of medical technology, has revolutionized the diagnostic aspect of several diseases [<xref rid="ref11" ref-type="bibr">11</xref>, <xref rid="ref14" ref-type="bibr">14</xref>]. DL techniques are now used to predict early stages of cancer as well as to simulate and <italic toggle="yes">de novo</italic> synthesize various types of omics data [<xref rid="ref15" ref-type="bibr">15</xref>, <xref rid="ref16" ref-type="bibr">16</xref>]. Generative adversarial networks (GANs) and convolutional neural networks (CNNs) have been applied in several areas of omics data analysis, including high fidelity sparse sample data simulation [<xref rid="ref17" ref-type="bibr">17</xref>], gene expression simulation [<xref rid="ref18" ref-type="bibr">18</xref>], single nucleotide polymorphism-based classification [<xref rid="ref19" ref-type="bibr">19</xref>, <xref rid="ref20" ref-type="bibr">20</xref>], genome-wide association studies [<xref rid="ref21" ref-type="bibr">21</xref>, <xref rid="ref22" ref-type="bibr">22</xref>] and alphafold protein folding prediction [<xref rid="ref23" ref-type="bibr">23</xref>]. Typically, a GAN is a DL model that can learn and generate entirely new data with the same statistical distribution as its corresponding training dataset. [<xref rid="ref24" ref-type="bibr">24</xref>, <xref rid="ref25" ref-type="bibr">25</xref>] A CNN is the class of deep neural network most commonly used to analyze image features [<xref rid="ref26" ref-type="bibr">26</xref>]. A region of interest (ROI) refers to any region within the GC × GC–TOFMS contour image with contrasting features that could be used to classify the corresponding sample. Previously, non-negative matrix factorization was used for unsupervised direct GC × GC–TOFMS contour classification [<xref rid="ref27" ref-type="bibr">27</xref>], but the method was unable to identify multiple ROIs within contour data and its classifier could not be customized. Furthermore, DL has not yet been used for automated multi-ROI identification, simulation, and untargeted profiling of GC × GC–TOFMS contour data.</p>
    <p>Here, we present the open-source cross-platform software contour ROI identification, simulation and untargeted metabolomics profiler (CRISP). We demonstrate the potential utility of this integrated DL approach for classifying GC × GC–TOFMS contour images of late-stage diabetic nephropathy and healthy control samples. The CRISP software can also be used to assist in the rapid screening of GC × GC–TOFMS contour image data.</p>
  </sec>
  <sec id="sec4">
    <title>Material and methods</title>
    <sec id="sec5">
      <title>Participants and plasma samples</title>
      <p>Participants were enrolled by nephrologists at Ramathibodi Hospital, Mahidol University, Thailand. Written informed consent was obtained from the participants before the start of the study. The study was approved by the Ethical Clearance Committee on Human Rights Related to Research Involving Human Subjects, Faculty of Medicine, Ramathibodi Hospital, Mahidol University (COA. MURA2014/369), and all methods were carried out in accordance with the Declaration of Helsinki.</p>
      <p>The samples were categorized into a healthy control group (CON; <italic toggle="yes">N</italic> = 20) or a group consisting of ESRD patients on hemodialysis or continuous ambulatory peritoneal dialysis for more than three months with diabetes mellitus (ESRD/DM, <italic toggle="yes">N</italic> = 20). The control group consisted of both male and female subjects with normal renal function, normal urinalysis, and no history of diabetes. Subjects with co-morbidities such as hypertension and those on medication for cardiovascular disease or cancer were excluded from the study.</p>
    </sec>
    <sec id="sec6">
      <title>Chemical standards and reagents</title>
      <p>Hexane, methanol (MeOH), methoxyamine hydrochloride (MeOX), N-methyl-N-(trimethylsilyl)-trifluoroacetamide (MSTFA) + 1% chlorotrimethylsilane (TMCS) and N-tert-butyldimethylsilyl-N-methyltrifluoroacetamide (MTBSTFA) were purchased from Sigma-Aldrich (St. Louis, MO, USA). The stable isotope-labeled internal standards (IS) DL-alanine-3,3,3-d<sub>3</sub> and L-phenylalanine-1-C<sub>13</sub> were purchased from Sigma-Aldrich (St. Louis, MO, USA) and Cambridge Isotope Laboratories, Inc. (Frontage Rd, MA, USA), respectively. Pyridine was purchased from Tokyo Chemical Industry, Inc. (Tokyo, Japan). MeOX solution (15 μg/μL in pyridine) was freshly prepared before analysis.</p>
    </sec>
    <sec id="sec7">
      <title>Sample preparation and GC × GC–TOFMS analysis</title>
      <p>The sample preparation for GC × GC–TOFMS analysis was adapted from a previous protocol [<xref rid="ref28" ref-type="bibr">28</xref>] with minor modifications. In brief, 100 μL of plasma sample was precipitated in 900 μL of pre-cooled 90% aqueous MeOH containing ISs of DL-alanine-3,3,3-d<sub>3</sub> and L-phenylalanine-1-C<sub>13</sub> at 20 ng/μL. The mixed solution was left at −20 °C for 1 h and centrifuged at 19 600 g (4 °C) for 10 min. After centrifugation, 200 μL of the supernatant was transferred to a new Eppendorf tube (1.5 mL) and then completely dried in a Centrivap concentrator (Labconco) at 65 °C (~2 h). The sample was kept at −20 °C until analysis. The dried sample was derivatized by methoximation followed by trimethylsilylation (TMS). Briefly, 30 μL of (15 μg/μL) MeOX in pyridine was added to the dried sample, sonicated at 25 °C for 3 min, and incubated at room temperature for 16 h. Subsequently, the mixture was mixed with 30 μL of MSTFA with 1% TMCS and sonicated for 3 min at room temperature. The mixture was incubated at 70 °C for 1 h and transferred into a GC vial for GC × GC–TOFMS analysis. The pooled sample was used as the quality control [<xref rid="ref29" ref-type="bibr">29</xref>] and was prepared by combining 200 μL of supernatant from each sample and performing the same protocol for sample derivatization described above. The pooled samples were distributed along the run order (every 15 samples).</p>
      <p>The derivatized samples were analyzed by GC × GC–TOFMS (Pegasus 4D HRT, Leco Corp. Inc.). The first GC column was a non-polar Rxi-5sil MS column (30 m length, 0.25 mm ID, and 0.25 μM film thickness, Restek, Bellefonte, PA, USA). The second column was a Rxi-17sil MS column (1 m length, 0.25 mm ID, and 0.25 μM film thickness, Restek, Bellefonte, PA, USA). One microliter of the derivatized sample was injected into the GC × GC–TOFMS using a split ratio of 1:20 and an inlet temperature of 250 °C. The GC × GC–TOFMS oven temperature was initially set at 50 °C (5 min hold), and ramped to 180 °C at 25 °C/min (1 min hold), to 220 °C at 10 °C/min (1 min hold), to 260 °C at 15 °C/min, and to 300 °C at 15 °C/min (4 min hold). The modulator period was set to 4 s, with hot and cold pulse durations of 0.8 and 1.20 s, respectively. Helium was used as a carrier gas with a flow rate of 1 mL/min. The GC × GC–TOFMS contour images were acquired using ChromaTOF (Version: 5.50, Leco Corp. for Windows) and then processed by the CRISP software. All samples were run at least twice to obtain contour image data with both low and high column bleeding to evaluate the ability of CRISP to handle dataset variation. The GC × GC–TOFMS contour images generated from the ChromaTOF were used as source images for the development and validation of the CRISP software (Supplementary <xref rid="sup1" ref-type="supplementary-material">Figure S1</xref>). Two sets of contour images with high and low column bleed were created by running the same samples 10 months later to evaluate the analytical performance potential of CRISP in the presence of experimental instrumental variation. Validation datasets were created by randomly selecting approximately 15% of the original GC × GC–TOFMS contour images from each group.</p>
    </sec>
    <sec id="sec8">
      <title>Software architecture</title>
      <p>CRISP consists of four major modules (<xref rid="f1" ref-type="fig">Figure 1A–D</xref>): contrasting feature identification (<xref rid="f1" ref-type="fig">Figure 1A</xref>), contour simulation (<xref rid="f1" ref-type="fig">Figure 1B</xref>), resolution enhancement (<xref rid="f1" ref-type="fig">Figure 1C</xref>) and transfer learning-based GC × GC–TOFMS contour image classification (<xref rid="f1" ref-type="fig">Figure 1D</xref>). CRISP takes GC × GC–TOFMS contour images generated from the ChromaTOF as input data for training or classification and produces report files and watermarked contour images indicating the inference results for unknown samples. The first module of CRISP identifies and enhances the contrasting features between different groups (<xref rid="f1" ref-type="fig">Figure 1A</xref>). It is followed by the second module that simulates contour images (<xref rid="f1" ref-type="fig">Figure 1B</xref>) and the third module that improves the contour image resolution (<xref rid="f1" ref-type="fig">Figure 1C</xref>) to train a classifier for the inference of unknown samples (<xref rid="f1" ref-type="fig">Figure 1D</xref>) and maximize classification efficacy. Although the data generated from GC × GC–TOFMS are multidimensional, the contour images provide a direct glance into the overall feature space of the sample content. Thus, we explored the possibility of profiling all the contour plots with respect to their holistic features, which forms the basis of CRISP’s untargeted metabolite profiling. For contour feature-based untargeted metabolite profiling while avoiding spatial-dimension complexity, CRISP processes the GC × GC–TOFMS contour images in the four modules, described in detail below.</p>
      <fig position="float" id="f1">
        <label>Figure 1</label>
        <caption>
          <p>Software architecture. CRISP has four major components. (<bold>A</bold>) The ROI and deepstacking module take contour data input and identifies the ROIs. It then stacks the ROIs using feature detecting CNNs to prepare the datasets. (<bold>B</bold>) The integrated GAN takes the pre-processed data and trains generative models to synthesize high resolution contour image data within the given distribution of the source dataset. (<bold>C</bold>) The contour super-resolution module helps improve the contour image quality prior to use in DNN classifier training. (<bold>D</bold>) The CNN classifier and inference module is customized for 244 × 244 to 512 × 512 pixels input resolutions. It implements multiple transfer learning architectures for training on the contour datasets produced by (<bold>B</bold>) and (<bold>C</bold>). The trained CNN classifier can be used for the subsequent inference of unknown contour profiling. CNN, convolutional neural network; GAN, generative adversarial network; ROI, region of interest.</p>
        </caption>
        <graphic xlink:href="bbab550f1" position="float"/>
      </fig>
    </sec>
    <sec id="sec9">
      <title>Module I: Aggregate feature representative contour and ROI stacking</title>
      <p>CRISP introduces the concept of aggregate feature representative contours (AFRCs) for creating a single representative GC × GC–TOFMS contour image for each study group, regardless of differences in sample size among the study groups. The first module directly uses GC × GC–TOFMS contour images from the ChromaTOF (Leco, MI, USA) as input. The software provides an option to manually select a single ROI (<xref rid="f2" ref-type="fig">Figure 2A</xref>) or construct an AFRC image (<xref rid="f2" ref-type="fig">Figure 2B</xref>) for stacking multiple ROIs algorithmically to construct a contour image dataset (<xref rid="f2" ref-type="fig">Figure 2C</xref>). In addition, a single ROI can be selected from the whole contour image for the inclusion of all features. In most cases, the discriminating features are relatively small and dispersed across the entire contour plot. To amplify these sparse contrasting features and minimize manual selection bias, the module uses a novel extraction procedure to construct a single AFRC image for each group. The AFRC image construction approach provides an algorithmic means of representing generalized GC × GC–TOFMS contour feature of a study group. The process is unsupervised and does not discriminate between noise and signal content, resulting in an unbiased representative contour image for each group in the dataset. The AFRC is based on the accumulation of features from the cyclic-ordered rotation of high-resolution contour image content captured at a fixed viewpoint for further foreground–background segmentation. The sequence of image data at a fixed frames per second (FPS) rate is generated by running the source contour images in an iterative manner. Thus, a video is temporarily created using the contour images as frames, and a single aggregate feature of the video content is extracted as an AFRC image. The feature extraction process includes a weight accumulation factor <italic toggle="yes">α</italic> that specifies the amount of information that an AFRC image retains of its previous input image during a cyclic run at a constant FPS. Because the function in OpenCV (<ext-link xlink:href="https://opencv.org/" ext-link-type="uri">https://opencv.org/</ext-link>) by default supports multi-channel image data, each channel is processed independently, preserving the color intensity information in the contour image data. The function is expressed as follows:<disp-formula id="deqn01"><label>(1)</label><tex-math notation="LaTeX" id="DmEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{align*} \mathrm{afrc}\left(x,y\right)\leftarrow &amp;\left(1-\alpha \right)\times \mathrm{afrc}\left(x,y\right)\nonumber\\ &amp;+\alpha \cdot \mathrm{sources}\left(x,y\right)\mathrm{if}\kern0.17em \mathrm{content}\left(x,y\right)\ne 0 \end{align*}\end{document}</tex-math></disp-formula>
Here, sources denotes the set of multichannel input image frames in a cyclic order at a given fixed frame rate, afrc denotes the destination accumulator AFRC image, which has the same number of channels as the input image, and parameter <italic toggle="yes">α</italic> is as described above. The content condition requires the input contour image data to exist.</p>
      <fig position="float" id="f2">
        <label>Figure 2</label>
        <caption>
          <p>ROI and deepstacking module. This CRISP module is designed to preprocess contour image data for enriched feature dataset construction that can be used by the downstream generative or classifier modules. (<bold>A</bold>) Manual selection of a single ROI to construct a contour dataset. (<bold>B</bold>) Construction of single AFRC image for each study group. (<bold>C</bold>) Semi-automatic identification of multiple ROIs between two classes of contours using VGG16-based feature computation method with example of five identified ROIs (m<sub>1</sub> – m<sub>5</sub>) stacked algorithmically in ascending order of feature similarity scores to produce a deepstacked dataset. AFRC, aggregate feature representative contour; ROI, region of interest; VGG16, visual geometry group-16.</p>
        </caption>
        <graphic xlink:href="bbab550f2" position="float"/>
      </fig>
      <p>This process creates a single AFRC that captures the general features of the entire contour content belonging to the group (<xref rid="f2" ref-type="fig">Figure 2B</xref>). The AFRC content is proportionally influenced by the frequency of the contour feature content and automatically suppresses any outlier features during construction. These AFRCs are subsequently processed using a novel stacking approach to create the feature-enhanced GC × GC–TOFMS contour dataset. The CRISP, for the first time in metabolomics, introduces the concept of feature-enriched GC × GC–TOFMS contour image dataset by stacking regions with major differences between the study classes. The CNN-based model is utilized to compute similarity between two AFRCs due to their ability to recognize complex patterns in images. In brief, the features of the AFRCs for any two groups are compared <italic toggle="yes">via</italic> a fixed size scanning window along the first dimension of retention time. A high performance CNN-based image feature extraction model (VGG16) [<xref rid="ref26" ref-type="bibr">26</xref>, <xref rid="ref30" ref-type="bibr">30–32</xref>] was implemented using the default architecture to evaluate the differences in each window and compute the similarity scores. According to these scores, the corresponding ROIs for all source contours are sorted in ascending order and stacked to create a contrasting feature-enhanced dataset called a ‘deepstacked’ dataset (<xref rid="f2" ref-type="fig">Figure 2C</xref>). CRISP stacks the first five ROIs by default with the minimum similarity scores to ensure at least three major ROIs are incorporated in the deepstacked database. In addition to the standard VGG filter, the user can experiment with several alternative similarity metrics such as the Hamming distance [<xref rid="ref33" ref-type="bibr">33</xref>], PSNR [<xref rid="ref34" ref-type="bibr">34</xref>], Fréchet inception distance (FID) [<xref rid="ref35" ref-type="bibr">35</xref>] and SSIM [<xref rid="ref36" ref-type="bibr">36</xref>] for ROI comparison. This dataset can either be directly sent to the classifier or processed by the GAN synthesizer module.</p>
    </sec>
    <sec id="sec10">
      <title>Module II: GAN training and contour synthesis</title>
      <p>The GAN module consists of the core engine used to generate synthetic GC × GC–TOFMS contour data based on a limited number of true samples (<xref rid="f3" ref-type="fig">Figure 3A</xref>). CRISP uses a modified version of the efficient quadratic potential (QP)-GAN [<xref rid="ref37" ref-type="bibr">37</xref>]. This GAN architecture minimizes the vanishing gradient and Lipschitz constraint, in contrast to previous GAN algorithms. [<xref rid="ref37" ref-type="bibr">37</xref>, <xref rid="ref38" ref-type="bibr">38</xref>] The model ignores probability divergence and directly converges the probability distributions into source sample distributions, iterating to achieve a min–max optimization with respect to generator loss.<disp-formula id="deqn02"><label>(2)</label><tex-math notation="LaTeX" id="DmEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{align*} G,T=&amp;\arg{\min}_G\;\arg{\max}_T{E}_{x\sim p(x)}\left[\log\;\sigma \left(T(x)\right)\right]\nonumber \\ &amp;+{E}_{x=G(z);z\sim q(z)}\left[\log \left(1-\sigma \left(T(x)\right)\right)\right] \end{align*}\end{document}</tex-math></disp-formula></p>
      <fig position="float" id="f3">
        <label>Figure 3</label>
        <caption>
          <p>Integrated GAN module. This CRISP module consists of a customizable integrated generative model based on a QP-GAN [<xref rid="ref37" ref-type="bibr">37</xref>]. (<bold>A</bold>) The dataset containing deepstacked or original full-frame source contour images is supplied to the GAN network, which trains the generator to synthesize corresponding high resolution (256 × 256–512 × 512 pixels) synthetic contours within the source data distribution. Here, ƒL(x) represents FID or a similar scoring metric to compute the similarity of synthesized contour during simulation with the true contours. Once the generator is sufficiently trained, it is able to synthesize numerous contour images independently. Changes in (<bold>B</bold>) FID (<bold>C</bold>) model loss and (<bold>D</bold>) qScore values for synthesized contours images during GAN training. (<bold>E</bold>) Manipulation of the latent space <italic toggle="yes">Z</italic>-vector for intensity variation during contour synthesis. Content inside the dotted circle represents the core engine of the integrated PQ-GAN networks. GAN, generative adversarial network; GC × GC–TOFMS, gas chromatography time of flight mass spectrometry; QP-GAN, quadratic potential generative adversarial network; FID, Fréchet inception distance.</p>
        </caption>
        <graphic xlink:href="bbab550f3" position="float"/>
      </fig>
      <p>For a fixed <italic toggle="yes">T</italic>, the goal of <italic toggle="yes">G</italic> can be represented as<disp-formula id="deqn03"><label>(3)</label><tex-math notation="LaTeX" id="DmEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$ G=\arg{\min}_G\;{E}_{x=G(z),z\sim p(x)}\left[\log \left(1-\sigma \left(T(x)\right)\right)\right] $$\end{document}</tex-math></disp-formula>Here, <italic toggle="yes">G</italic> and <italic toggle="yes">T</italic> represent the generator and trainer, respectively, and <italic toggle="yes">σ</italic> is a sigmoid function such that <italic toggle="yes">σ</italic> (<italic toggle="yes">x</italic>) = 1/(1 + <italic toggle="yes">e</italic><sup>–<italic toggle="yes">x</italic></sup>) has a loss function of –log <italic toggle="yes">σ</italic> (<italic toggle="yes">T</italic>(<italic toggle="yes">x</italic>)). Furthermore, <italic toggle="yes">Z</italic>, argmax and argmin represent the latent space vector, maxima and minima of functions <italic toggle="yes">G</italic> or <italic toggle="yes">T</italic> for any given input dataset [<xref rid="ref37" ref-type="bibr">37</xref>]. The QP-GAN adjusts the loss of generator <italic toggle="yes">G</italic> for better optimization instead of focusing on the original min–max game. The FID metric is computed to evaluate the distance between the feature vectors of the simulated and source images to assess the quality of contour image synthesis. [<xref rid="ref35" ref-type="bibr">35</xref>, <xref rid="ref37" ref-type="bibr">37</xref>, <xref rid="ref39" ref-type="bibr">39</xref>, <xref rid="ref40" ref-type="bibr">40</xref>] Lower FID scores indicate better quality in the synthesized contour images. In the synthesizer training step, the batch size for FID score evaluation can be set and the vector shape can be customized for random noise input. CRISP introduces a novel qScore to estimate the quality of synthesized contour based on image sharpness. The qScore matric evaluates the synthesized image blurriness with respect to the source contour images. For a synthetic contour image, this is calculated as qScore = sigmoid (<inline-formula><tex-math notation="LaTeX" id="ineq01">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\nabla}^2$\end{document}</tex-math></inline-formula> (image)), which is the value obtained from a sigmoid function applied to the output of a Laplacian (<inline-formula><tex-math notation="LaTeX" id="ineq02">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\nabla}^2)$\end{document}</tex-math></inline-formula> operator. [<xref rid="ref41" ref-type="bibr">41</xref>, <xref rid="ref42" ref-type="bibr">42</xref>] A qScore of 0.18 indicates that the sharpnesses of synthesized and source images are approximately equal. These indicators and other customization features provide real-time visualizations of the FIDs (<xref rid="f3" ref-type="fig">Figure 3B</xref>), generator (G Loss) and discriminator (D Loss) loss functions (<xref rid="f3" ref-type="fig">Figure 3C</xref>), qScores (<xref rid="f3" ref-type="fig">Figure 3D</xref>) and a user-defined preview of a simulation image grid for visually inspecting the GAN model training status. The contour synthesizer is a well-trained generator <italic toggle="yes">G</italic> that can synthesize image-wise unique contours within the given data distribution and can be customized to obtain various intensity levels for each synthetic contour (<xref rid="f3" ref-type="fig">Figure 3E</xref>). This was done by proportionally manipulating the latent space-associated vector <italic toggle="yes">Z</italic> for each noise input in the trained synthesizer. The GAN module consists of multiple image preprocessing and augmentation features that include contour sharpening, blurring, noise/denoising, erosion, dilation, distortion, brightness, contrast, and edge-enhancement filters for both model training and synthesis. Once properly trained, the synthesizer can be used independently to generate random contour images within the distribution of the training dataset (<xref rid="f3" ref-type="fig">Figure 3A</xref>). The full and deepstacked contour images were constructed using their corresponding trained synthesizers to generate datasets that are ten times larger than the datasets of the original source images.</p>
    </sec>
    <sec id="sec11">
      <title>Module III: Super-resolution network</title>
      <p>CRISP implements a cascading residual network (CARN)-based super-resolution CNN to enhance the resolution of the relatively low-resolution contour images [<xref rid="ref43" ref-type="bibr">43</xref>]. The contour image synthesized by the GAN module can be optionally processed by the CARN module to enhance image resolution without compromising output quality and speed (<xref rid="f4" ref-type="fig">Figure 4A</xref>). This super-resolution model was reported to outperform SelNet, DRCN and SRDenseNet in terms of computational cost and performance [<xref rid="ref43" ref-type="bibr">43</xref>]. The CARN network is trained on high-quality GC × GC–TOFMS contour images to optimize the super-resolution model and hence improve the quality of the synthesized contour images. Because GC × GC–TOFMS contour images resemble a mosaic or have a plasma-like appearance, a custom simulated high-quality contour-like dataset was constructed to train the contour-specific super-resolution network (<xref rid="f4" ref-type="fig">Figure 4B</xref>). The CARN module employs the L1-loss as a model loss function, which is the mean of the absolute difference between the predicted and true resolution images during training. The L1-loss was reported to yield better convergence and performance than the regular L2-loss, which is the mean-squared error [<xref rid="ref43" ref-type="bibr">43</xref>]. CRISP also introduces the novel qScore ratio to view the real-time changes in image sharpness during training. This score is computed as the difference in image sharpness of the high- and low-resolution contour images divided by the sharpness of the high-resolution contour image during CARN model training. A qScore ratio of zero indicates approximately equal levels of sharpness in a super-resolution enhanced image and its corresponding original high resolution source image.</p>
      <fig position="float" id="f4">
        <label>Figure 4</label>
        <caption>
          <p>Contour super-resolution module. This CRISP module is based on CARN [<xref rid="ref43" ref-type="bibr">43</xref>] to improve contour image resolution. (<bold>A</bold>) The architecture of CARN, showing the multiple layers of network responsible for image resolution improvement. (<bold>B</bold>) The super-resolution network is specifically trained using high quality contour-like features similar to those in the simulated dataset. The network is trained until the (<bold>C</bold>) model loss and (<bold>D</bold>) qScore ratio are sufficiently low. The trained model can be used independently to improve the quality of contours synthesized by the generator module. CARN, cascading residual network, HR, high resolution; LR, low resolution, GAN, generative adversarial network.</p>
        </caption>
        <graphic xlink:href="bbab550f4" position="float"/>
      </fig>
    </sec>
    <sec id="sec12">
      <title>Module IV: Classification network (transfer learning-based contour classifier)</title>
      <p>The final CRISP module uses customizable CNN models capable of increasing image input size from 128 × 128 to 512 × 512 pixels for transfer learning-based classification of the GC × GC–TOFMS contour image data. The dataset made from the original, synthesized or combined contour images can be supplied to this module for training the classifier CNNs. The CNNs available in CRISP for transfer learning consist of VGG16 [<xref rid="ref26" ref-type="bibr">26</xref>], VGG19 [<xref rid="ref44" ref-type="bibr">44</xref>], Inception V3 [<xref rid="ref45" ref-type="bibr">45</xref>], RasNet, [<xref rid="ref46" ref-type="bibr">46</xref>] and DenseNet [<xref rid="ref47" ref-type="bibr">47</xref>], which enable high-fidelity contour feature classification [<xref rid="ref44" ref-type="bibr">44</xref>, <xref rid="ref48" ref-type="bibr">48</xref>]. To maximize the feature content input to train the classifier models, the input resolution was increased to 512 × 512 pixels instead of the default 244 × 244 pixels. This enables the models to process more information from the source contour images (<xref rid="f5" ref-type="fig">Figure 5A</xref>). To simplify the interpretation and optimize CRISP, VGG16 was used as the default transfer learning-based CNN model for contour image classification. The transfer learning module of CRISP has two submodules.</p>
      <fig position="float" id="f5">
        <label>Figure 5</label>
        <caption>
          <p>CNN classifier and inference module. The deep CNN is the core engine of CRISP and utilizes transfer learning for feature extraction and classification on the pre-processed dataset. (<bold>A</bold>) Multilayer architecture of the transfer learning-based CNN responsible for feature computation and scoring. The transfer learning model’s (<bold>B</bold>) loss (<bold>C</bold>) accuracy, and (<bold>D</bold>) AUROC for evaluating performance of the CNN classifier. (<bold>E</bold>) A conventional PCA analysis plot for the GC × GC–TOFMS data showing the contrasting features between the ESRD and CON groups. AUROC, area under the receiver operating characteristic; CNN, convolutional neural network; GC × GC–TOFMS, two-dimensional gas chromatography time-of-flight mass spectrometry.</p>
        </caption>
        <graphic xlink:href="bbab550f5" position="float"/>
      </fig>
      <p>The first submodule is the classifier trainer, which implements transfer learning using an input contour dataset and can handle multiple classes. The datasets are separated into training and validation sets for evaluating both the training and validation accuracies, model losses, and area under the receiver operating characteristic (AUROC) values. In particular, the AUROC metric quantifies the overall performance of the classifier model in terms of sensitivity and specificity. The training submodule has a built-in image augmentation option, which performs additional multiple random image augmentation operations (e.g., image shearing, skewing and distortion) to increase diversity in the training dataset.</p>
      <p>The second submodule is contour inference, and it is the final process in the CRISP software. This submodule uses the trained classifier model to infer unknown contour samples at a pre-defined confidence threshold (set to 85%) and predict their classes. Prior to inference, the preprocessing of the contour image input for whole or deepstacked images should be matched to the type of dataset used to train the classifier model. To interpret results, a customizable report file is generated containing the class prediction and the corresponding contour images tagged with prediction scores. The classification accuracies of conventional machine learning classifiers (decision trees, K-nearest neighbors, random forests, support vector machines, linear regression and simple artificial neural networks) and Keras-based standard DL models (<ext-link xlink:href="https://keras.io" ext-link-type="uri">https://keras.io</ext-link>) were compared with the classification accuracy of CRISP. The conventional classifiers were computed using the Scikit-learn machine learning package [<xref rid="ref49" ref-type="bibr">49</xref>]. The conventional approach of principal component analysis (PCA) was used to validate the raw GC × GC–TOFMS datasets before they were used to optimize the DL software [<xref rid="ref50" ref-type="bibr">50</xref>].</p>
    </sec>
    <sec id="sec13">
      <title>Model configuration, logging and updates</title>
      <p>CRISP has many settings for GC × GC–TOFMS contour ROI extraction, deepstacking, simulation, resolution enhancement and contour classification. In addition, each trained model has its training history configuration, which includes iteration counts, model loss, accuracies, FIDs, AUROCs and other metrics. The settings needed to run each module can be stored as a plain text configuration file to restore, edit, or directly execute the program using the graphical user interface (GUI) or command-line interface. All model weights and associated ROI and AFRC settings can be stored so that the same ROIs can be used for the inference of similar contour images in future. A plain-text summary of information including the model type, input dimension, trained iteration/epoch, source dataset location and loss function is stored along with the trained weights to provide a quick overview of the model history. The platform can also store logs of most activity to assist error tracking and troubleshooting. Likewise, the updated or pre-trained models for each module, which are stored in Google drive or on an HTTP server, can be manually downloaded and used through a built-in feature of CRISP. Interested researchers can also submit their custom trained models or datasets with a proper description and be listed for download on the CRISP official web repository after manual review.</p>
    </sec>
    <sec id="sec14">
      <title>Computational hardware and DL software platform</title>
      <p>The construction and computation of all CRISP DL models were completed using an Intel core i9 processor with an NVIDIA RTX3070 series CUDA-core compatible graphics processing unit with 8 Gb VRAM and 32 GB system onboard DDR5 RAM. The entire cross-platform compatible software was written in Python and TensorFlow-backend Keras application programming interface. [<xref rid="ref51" ref-type="bibr">51</xref>, <xref rid="ref52" ref-type="bibr">52</xref>] The GUI was designed using PyQt5. Pre-trained base weights for the CNN models (VGG16, VGG19, ResNet50, DenseNet and InceptionV3) used in the transfer learning-based classifier were downloaded from the official Keras website (<ext-link xlink:href="https://keras.io/api/applications/#available-models" ext-link-type="uri">https://keras.io/api/applications/#available-models</ext-link>). CRISP features both GUI and command line interface architecture for novice to advanced level customization. The updates and trained DL models of CRISP can be directly downloaded from its GUI interface.</p>
    </sec>
  </sec>
  <sec id="sec15">
    <title>Results and discussion</title>
    <p>Initially, to develop CRISP, we used 15 contour images from both the ESRD/DM and CON groups to train the CRISP architecture. We used 3–5 images depending upon the quality of the datasets from both groups for model validation. Furthermore, we tested how the variation in analysis affects the model performance by repeating the analysis for GC × GC–TOF measurements using the same source samples (<italic toggle="yes">N</italic> = 20 for each group) after ten months had passed.</p>
    <sec id="sec16">
      <title>ROIs, AFRC and deepstacking</title>
      <p>The first module of CRISP was designed to take the GC × GC–TOFMS contour images of samples directly from the ChromaTOF and construct the feature-enhanced deepstacked dataset. The contour images from the ESRD/DM and CON groups were separately processed to generate AFRC (<xref rid="f2" ref-type="fig">Figure 2B</xref>) images and the five most contrasting ROIs were identified to construct a deepstacked dataset (<xref rid="f2" ref-type="fig">Figure 2C</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S2–S4</xref>). Although contrasting features are often small and distributed throughout contour images, CRISP was able to identify major ROIs that were consistent with the overall differences observed from conventional metabolomics data analysis. For instance, the ROIs indicated regions with aberrantly high levels of metabolites such as maltitol, mannitol, sorbitol and dulcitol, which were the most contrasting metabolites observed in the PCA results of the ESRD/DM and CON groups (<xref rid="f5" ref-type="fig">Figure 5E</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S11</xref>). In contrast to processing an entire contour image or a single ROI (<xref rid="f2" ref-type="fig">Figure 2A</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref>), the construction of a deepstacked dataset for each group enabled the features to be enhanced by algorithmic means while minimizing manual selection bias (<xref rid="f2" ref-type="fig">Figure 2B and C</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S3 and S4</xref>). It was possible for the minor ROIs that might have been otherwise ignored during manual selection to be proportionately represented in the AFRC images for each class. The effect of any existing outlier contour image features was likely diminished because of their low occurrence among the total samples during AFRC construction. The creation of an AFRC image for ROI identification and deepstack database construction might be a suitable way to enhance features in small to medium contour image datasets. Column bleed and a slight shift in retention time are the most common issues that affect data quality and analysis in gas chromatography [<xref rid="ref53" ref-type="bibr">53–55</xref>]. Experiments involving contour dataset construction using low, high and mixed levels of column bleed exhibited a similar pattern of AFRC construction. In each case, CRISP was able to handle data variation without compromising the identification of key features in GC × GC–TOFMS contour images (<xref rid="f6" ref-type="fig">Figure 6A and B</xref>). Subsequently, a deepstacked dataset was constructed by incorporating the top five ROIs for the enhancement of contrasting features that could be sent to downstream CRISP modules to improve their classifier performance. The feature-enhanced deepstacked dataset constructed using mixed column bleed samples was used in this case because it represented the highest number of samples without compromising the key features of each test group.</p>
      <fig position="float" id="f6">
        <label>Figure 6</label>
        <caption>
          <p>Comparison of ROIs and deepstacking for samples with different amounts of column bleeding. The HCB and LCB GC × GC–TOFMS contour datasets were obtained by analyzing the same samples with the same experimental setup again after an interval of 10 months. (<bold>A</bold>) The ARFC image and ROI deepstacking for the HCB (top), MIX (middle) and LCB (bottom) datasets using the original GC × GC–TOFMS contour images. The deepstacked feature map is constructed by stacking the top five least similar scoring ROIs. (<bold>B</bold>) Full sliding-window feature similarity scores for multiple ROIs based on VGG16 filter for low, high and combined column bleeding contour datasets using the default settings. Classification AUROCs for classifiers trained on full and deepstacked contours for the (<bold>C</bold>) high resolution (512 × 512 pixels) and (<bold>D</bold>) low resolution (244 × 244 pixels) trained models to compare classifier performance at different levels of column bleeding and model resolution. LCB, low column bleed; HCB, high column bleed; MIX, a mixture of LCB and HCB.</p>
        </caption>
        <graphic xlink:href="bbab550f6" position="float"/>
      </fig>
    </sec>
    <sec id="sec17">
      <title>GC × GC–TOFMS contour image simulation</title>
      <p>Generative DL models have the architectural advantage of being able to learn complex features from training datasets and synthesize completely new data with the same distribution. [<xref rid="ref24" ref-type="bibr">24</xref>, <xref rid="ref56" ref-type="bibr">56</xref>] This CRISP module was designed to use the feature-enhanced datasets generated by the previous module to train a generator for creating synthetic GC × GC–TOFMS contour images. The synthesizer module of CRISP facilitated the generation of a 10× increase in the number of similar contour images containing randomly induced variation within the data distribution for each group (<xref rid="f3" ref-type="fig">Figure 3A</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S5 and S6</xref>). The QP-GAN is more suitable for the CRISP synthesizer module than other GANs because of its implementation of a Lipschitz constraint [<xref rid="ref38" ref-type="bibr">38</xref>] on the discriminator to prevent the vanishing gradient, retain small source data features and prevent model collapse [<xref rid="ref37" ref-type="bibr">37</xref>, <xref rid="ref38" ref-type="bibr">38</xref>]. This model is more powerful than previous GANs (e.g., WGAN, LSGAN and SGAN) because the synthesized images can be scaled up to 512 × 512 pixels with decent quality, in contrast to the 128 × 128–256 × 256 pixels limitations of previous GANs [<xref rid="ref37" ref-type="bibr">37</xref>]. Although there is currently no gold standard for directly measuring the quality of GAN-synthesized images, the highly effective FID scores provided unbiased estimates of the contour image likeliness relative to the source datasets [<xref rid="ref35" ref-type="bibr">35</xref>]. The synthesizer was able to simulate contours with an FID of &lt;47.00 (<xref rid="f3" ref-type="fig">Figure 3B</xref>) while converging the losses of the generator and discriminator (<xref rid="f3" ref-type="fig">Figure 3C</xref>), which correlated with observed decent-quality contours for a relatively small source dataset (<xref rid="f3" ref-type="fig">Figure 3C</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S6</xref>). The variation introduced by combining the original samples with synthetic images provided a reasonably large pool of samples for training the CRISP classifier module. The custom-trained QP-GAN synthesizer generated images with a resolution of 512 × 512 pixels, which meant that nearly four times the amount of contour image details could be sent to the downstream classifier module when compared with the default input of 244 × 244 pixels (<xref rid="f3" ref-type="fig">Figure 3A</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S6</xref>). The implementation of QP-GAN may facilitate proper training with a limited sample size, unlike most generative models in clinical research that require large image datasets. [<xref rid="ref57" ref-type="bibr">57</xref>, <xref rid="ref58" ref-type="bibr">58</xref>] The experimental qScore was approximately 0.165, indicating that contour images were synthesized that were similar to the source images in terms of image sharpness (<xref rid="f3" ref-type="fig">Figure 3D</xref>). The qScores together with the FIDs indicate the qualitative traits for feature similarity and image quality during GC × GC–TOFMS contour simulation. The real-time graphs for FIDs, model loss convergence and qScores, calculated for batches of simulated images during the training session, indicated a trend highly similar to the source contour images simulated during model training (<xref rid="f3" ref-type="fig">Figure 3B–D</xref>). Unlike traditional GANs [<xref rid="ref17" ref-type="bibr">17</xref>], the CRISP provides customizable <italic toggle="yes">Z</italic>-vector enabled manipulation of the features during contour synthesis (<xref rid="f3" ref-type="fig">Figure 3E</xref>), which can simulate contour images with different intensities, which is analogous to the concentration variation in true GC × GC–TOFMS contour data. Because GAN models are relatively hard to train, the real-time indicators are crucial not only to adjust the model hyperparameters but also for minimizing the loss of time and computational resources [<xref rid="ref25" ref-type="bibr">25</xref>, <xref rid="ref37" ref-type="bibr">37</xref>]. CRISP’s optional image-enhancement module uses a CARN-based DL super-resolution model for fast and accurate improvement of synthesized contour image quality (<xref rid="f4" ref-type="fig">Figure 4A and B</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S7 and S8</xref>) [<xref rid="ref43" ref-type="bibr">43</xref>]. The custom dataset was able to properly train the super-resolution model based on the observed model loss (<xref rid="f4" ref-type="fig">Figure 4C</xref>) and qScore ratios (<xref rid="f4" ref-type="fig">Figure 4D</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S7</xref>). Although the use of super-resolution slightly enhances the quality of the synthesized contours, no significant improvements were observed during classifier training. This may be a result of the features of the dataset, quality of the synthesized image, and efficacy of the classifier, which was able to detect contrasting features without the need for super-resolution. However, this function provides users the option to train the model and implement super-resolution on synthesized images for custom datasets.</p>
    </sec>
    <sec id="sec18">
      <title>Classifier training and inference</title>
      <p>The performance of the CRISP classifier model is affected by image resolution because a larger input shape can hold more contour feature information during training (<xref rid="f6" ref-type="fig">Figure 6C and D</xref>). However, training at a higher resolution is memory intensive, time consuming, and limited by the architecture of the CNNs. The customizable GUI feature of CRISP classifier (Supplementary <xref rid="sup1" ref-type="supplementary-material">Figure S9</xref>) and inference (Supplementary <xref rid="sup1" ref-type="supplementary-material">Figure S10</xref>) modules makes it easier to train and predict classes, respectively. The 512 × 512 pixels high resolution classifier model trained on the 10× simulated dataset yields promising improvements in model performance within the first 200 epochs (<xref rid="f5" ref-type="fig">Figure 5B–D</xref>). The result is in agreement with the PCA score plot (from conventional data analysis; <xref rid="f5" ref-type="fig">Figure 5E</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary S11</xref>) and confirms the difference between the ESRD/DM and healthy control groups. The clear increases in classification accuracy obtained by CRISP (<xref rid="TB1" ref-type="table">Table 1</xref>) with respect to the accuracy obtained by the conventional histogram-based pixel approach indicates the potential of the transfer learning models in contour image-based metabolomics profiling.</p>
      <table-wrap position="float" id="TB1">
        <label>Table 1</label>
        <caption>
          <p>Comparison of CRISP classification accuracies with conventional histogram-based image profilers and artificial intelligence models</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">S.N</th>
              <th align="left" rowspan="1" colspan="1">Model</th>
              <th colspan="2" align="left" rowspan="1">Image histogram-based accuracy<xref rid="tblfn1" ref-type="table-fn"><sup>a</sup></xref></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">244 × 244 pixels</th>
              <th align="left" rowspan="1" colspan="1">512 × 512 pixels</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="1" colspan="1">1</td>
              <td align="left" rowspan="1" colspan="1">Direct Tree Classifier</td>
              <td align="left" rowspan="1" colspan="1">84.14 ± 0.53%</td>
              <td align="left" rowspan="1" colspan="1">86.00 + 0.45%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">2</td>
              <td align="left" rowspan="1" colspan="1">Extra Tree Classifier</td>
              <td align="left" rowspan="1" colspan="1">84.54 ± 0.63%</td>
              <td align="left" rowspan="1" colspan="1">84.00 ± 0.76%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">3</td>
              <td align="left" rowspan="1" colspan="1">K-Nearest Neighbors</td>
              <td align="left" rowspan="1" colspan="1">70.00 ± 0.45%</td>
              <td align="left" rowspan="1" colspan="1">70.0 ± 0.54%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">4</td>
              <td align="left" rowspan="1" colspan="1">Random Forest</td>
              <td align="left" rowspan="1" colspan="1">64.17 ± 1.32%</td>
              <td align="left" rowspan="1" colspan="1">67.37 ± 1.22%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">5</td>
              <td align="left" rowspan="1" colspan="1">Supervised Vector Machine</td>
              <td align="left" rowspan="1" colspan="1">62.00 ± 0.35%</td>
              <td align="left" rowspan="1" colspan="1">65.0 ± 0.75%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">6</td>
              <td align="left" rowspan="1" colspan="1">Linear Regression</td>
              <td align="left" rowspan="1" colspan="1">62.00 ± 0.35%</td>
              <td align="left" rowspan="1" colspan="1">30 ± 0.45%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">7</td>
              <td align="left" rowspan="1" colspan="1">Simple artificial neural network (ANN)</td>
              <td align="left" rowspan="1" colspan="1">62.00 ± 0.36%</td>
              <td align="left" rowspan="1" colspan="1">68.0 ± 0.55%</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">8</td>
              <td align="left" rowspan="1" colspan="1">State-of-art CNNs (Keras DCNN, DenseNet, VGG19/VGG16,InceptionV3)</td>
              <td align="left" rowspan="1" colspan="1">≥94.00%</td>
              <td align="left" rowspan="1" colspan="1">N/A<sup>b</sup></td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">9</td>
              <td align="left" rowspan="1" colspan="1">CRISP</td>
              <td align="left" rowspan="1" colspan="1">96.45 ± 0.77%</td>
              <td align="left" rowspan="1" colspan="1">&gt;98.45%</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><sup>a</sup>Full-size contour for the mixed column bleed dataset was used for computing classification accuracy. For fetching GC × GC–TOFMS contour image histogram data to machine learning models, input images were resized to 244 × 244 or 512 × 512 pixels and a three-dimensional color histogram was extracted from the HSV color space. The values were normalized and flattened to a one-dimensional feature vector which was subjected to different machine learning classifiers. A train-test-split ratio of 85:15 was applied and each classifier model was tested to fit corresponding models and prediction accuracy was computed.</p>
          </fn>
          <fn id="tblfn2">
            <p><sup>b</sup>The default input shape for state-of-art CNN is 244 × 244 pixels while CRISP can take up to 512 × 512 pixels RGB image. All neural network and DL models were run for at least 500 epochs to compute prediction accuracies. CNN, convolutional neural networks; GC × GC–TOFMS, two-dimensional gas chromatography time-of-flight mass spectrometry; HSV, hue saturation value.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Furthermore, column bleeding, retention time shift and spectrum noise are common problems in GC–MS-based metabolomics analysis [<xref rid="ref55" ref-type="bibr">55</xref>]. To address these issues related to experimental variation, the source samples were re-evaluated under the same measurement conditions to obtain GC × GC–TOFMS contour datasets with high and low amounts of column bleed. The CRISP classifier model uses an aggregate feature of the contour images, and hence the slightly different contour images obtained from both two experiments conducted 10 months apart had no effect on classifier performance. The full and deepstacked contour datasets derived from the high, low and mixed column bleed source samples all had an AUROC of more than 95.00% (<xref rid="f7" ref-type="fig">Figure 7C</xref>) for the 512 × 512 pixels high resolution classifier models, indicating the relatively good performance of the trained classifier models compared to the 244 × 244 pixels low-resolution classifier models with AUROC values of less than 95%. For the 244 × 244 pixels low resolution dataset, the model trained on the mixed samples dataset had performed better (&gt;85.00% AUROC) than the high or low column bleed datasets (&gt;80.00% AUROC) within 500 epochs. The improvement in performance of the low-resolution trained model on only the mixed dataset might be due to the combined larger sample size (<xref rid="f6" ref-type="fig">Figure 6C</xref>). Thus, the mixed datasets provide a suitable option for incorporating maximum diversity during simulation and classifier training. The 512 × 512 pixels classifier trained on the full and deepstacked datasets had substantially lower model loss (<xref rid="f7" ref-type="fig">Figure 7A</xref>) and higher performance (<xref rid="f7" ref-type="fig">Figure 7B–D</xref>) than the default 244 × 244 pixels-trained model on the non-simulated contour datasets. Similarly, the 512 × 512 pixels classifier trained on the full and deepstacked datasets had substantially lower model loss (<xref rid="f8" ref-type="fig">Figure 8A</xref>) and higher performance (<xref rid="f8" ref-type="fig">Figure 8B–D</xref>) than the default 244 × 244 pixels-trained model on the 10× simulated contour datasets. The classifiers trained with the deepstacked and 10× simulated datasets were able to consistently achieve higher or similar levels of classification performance (<xref rid="f7" ref-type="fig">Figures 7B and </xref><xref rid="f8" ref-type="fig">8B</xref>) and model accuracy (<xref rid="f7" ref-type="fig">Figures 7C and </xref><xref rid="f8" ref-type="fig">8C</xref>) within one fifth the training epochs used for the non-simulated contour image datasets. The model performance observed for the 512 × 512 pixels deepstacked-trained models exhibited substantial increases in discrimination capacities of classifiers. These models could achieve an AUROC of &gt;0.96 and accuracy of &gt;96.00% within the first 100 epochs, which supports the potential of the CRISP’s ROI-stacking approach to efficiently train contour classifier models with a small sample size. The removal of similar regions among test groups and inclusion of contrasting ROIs during feature enhancement could have mitigated the potential decrease in classifier model efficacy. The 10× increase in the GC × GC–TOFMS training data size obtained by the CRISP synthesizer could potentially cause a reduction in model overfitting and compensate for the lack of a large source dataset. Even though CRISP tries to compensate for issues related to model overfitting by using a 10× simulated dataset, the current limitation in original GC × GC–TOFMS training data size may exert some level of influence on the actual performance of the model. The contour features that could be used as input to classifier models were much smaller in the default CNN models (<xref rid="f5" ref-type="fig">Figure 5A</xref>), with an input shape of 244 × 244 pixels needed to gain good performance. Low-resolution input contour images fundamentally mean information could be lost from the start, which could have affected the classifier performance (<xref rid="f7" ref-type="fig">Figures 7B and </xref><xref rid="f8" ref-type="fig">8B</xref>) for the 244 × 244 pixels contours regardless of the differences among the sample classes and transfer learning models used. The deepstacked simulated dataset in combination with a larger input resolution of 512 × 512 pixels achieved the best performance, with an AUROC of &gt;0.95 and accuracy of &gt;96.00%, within the first 100 epochs, which indicates the potential of the CRISP’s approach to efficiently training a contour classifier model with a small available sample size. The training of a classifier with a larger number of source contour images and the corresponding simulated datasets might increase the accuracy of the models.</p>
      <fig position="float" id="f7">
        <label>Figure 7</label>
        <caption>
          <p>Comparison of classifier performance in terms of (<bold>A</bold>) model loss (<bold>B</bold>) classification AUROC, (<bold>C</bold>) classification accuracy, and (<bold>D</bold>) validation accuracy for GC × GC–TOFMS contour image datasets made using full and deepstacked contour image datasets and trained using a VGG16-based customized transfer learning technique. The high resolution (512 × 512 pixels) and default low resolution (244 × 244 pixels) transfer learning models were separately trained for 500 epochs on the corresponding dataset and trendlines of different indicators are plotted. Shaded regions around the trendlines indicate fluctuation in the data points during training. AUROC, area under the receiver operating characteristic; GC × GC–TOFMS, two-dimensional gas chromatography time-of-flight mass spectrometry.</p>
        </caption>
        <graphic xlink:href="bbab550f7" position="float"/>
      </fig>
      <fig position="float" id="f8">
        <label>Figure 8</label>
        <caption>
          <p>Comparison of classifier performance in terms of (<bold>A</bold>) model loss (<bold>B</bold>) classification AUROC, (<bold>C</bold>) classification accuracy, and (<bold>D</bold>) validation accuracy for the GC × GC–TOFMS contour image dataset, constructed using 10× simulated full and deepstacked contour image dataset and trained using a VGG16-based customized transfer learning technique. The high resolution (512 × 512 pixels) and default low resolution (244 × 244 pixels) transfer learning models were separately trained for 100 epochs on the corresponding dataset and the trendlines of the different indicators are shown. Shaded regions around the trendlines indicate fluctuation in the data points during training. AUROC, area under the receiver operating characteristic; GC × GC–TOFMS, two-dimensional gas chromatography time-of-flight mass spectrometry.</p>
        </caption>
        <graphic xlink:href="bbab550f8" position="float"/>
      </fig>
      <p>CRISP could be used to rapidly screen GC × GC–TOFMS contours for pathogen-related disease and healthy control groups if substantial differences appear in the metabolite contents. Some pathogens are known to produce an aberrant and often unidentified collection of metabolites as disease fingerprints in host specimens [<xref rid="ref59" ref-type="bibr">59</xref>], which could be holistically profiled by CRISP based on their GC × GC–TOFMS contour image. Most of the core DL engines of CRISP are based on image analysis using modified versions of state-of-the-art CNNs. Thus, significant changes in the GC × GC–TOFMS contour profiles could even allow CRISP to predict potential pathogens if the proper training datasets and protocols are provided. Likewise, CRISP is equipped with many options for conducting numerous combinations of experiments that are beyond the scope of the current study. The novel approach of CRISP demonstrates the potential of integrated DL in untargeted GC × GC–TOFMS metabolite profiling that directly implements contour images.</p>
    </sec>
  </sec>
  <sec id="sec19">
    <title>Summary</title>
    <p>The CRISP software uses an integrated DL approach for untargeted GC × GC–TOFMS contour profiling and was evaluated using an in-house GC × GC–TOFMS contour image datasets. The novel approach of AFRC construction combined with ROI stacking helps enhance contour image features for contour profiling in an unbiased manner. The synthesizer module enables the use of a small dataset, highlighting the potential of CRISP to facilitate DL model training for GC × GC–TOFMS datasets with relatively few samples. The fully operational GUI with real-time graphs and model configuration storage features make CRISP easy to operate and allows changes to be tracked. Even though a limited contour dataset and lack of well-established protocols are few limitations of the current version, CRISP may provide profiling scheme for GC × GC–TOFMS data that is an alternative to existing conventional methods.</p>
    <boxed-text id="box01" position="float">
      <sec id="sec22z">
        <title>Key Points</title>
        <list list-type="bullet">
          <list-item>
            <p>First direct use of contour images in a deep learning approach for GC × GC–TOFMS untargeted metabolite profiling is reported.</p>
          </list-item>
          <list-item>
            <p>The aggregate feature representative contour image, which automatically identifies contrasting regions between any two groups of GC × GC–TOFMS contour images, is introduced.</p>
          </list-item>
          <list-item>
            <p>Because of the holistic feature analysis, column bleed and other experimentation variation have little or no effect on contrasting feature identification and classification efficacy.</p>
          </list-item>
          <list-item>
            <p>A fully operational user interface with real-time indicators and single software pipeline for contrasting feature detection, simulation and classification of GC × GC–TOFMS contour images is provided.</p>
          </list-item>
        </list>
      </sec>
    </boxed-text>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>Revision_Supplimentary_information_bbab550</label>
      <media xlink:href="revision_supplimentary_information_bbab550.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="bbab550-ack">
    <title>Acknowledgements</title>
    <p>This research was conducted under the postdoctoral research sponsorship of Mahidol University to VBM. S.K. was supported by a Chalermphrakiat Grant, Faculty of Medicine, Siriraj Hospital. The Center of Excellence for Innovation in Chemistry (PERCH-CIC), Ministry of Higher Education, Science, Research and Innovation Thailand, is gratefully acknowledged. We also would like to acknowledge Dr Jonathan Robinson and Dr Pattipong Wisanpitayakorn for their comments and constructive discussions. We thank Kimberly Moravec, PhD, from Edanz (<ext-link xlink:href="https://www.edanz.com/ac" ext-link-type="uri">https://www.edanz.com/ac</ext-link>) for editing a draft of this manuscript.</p>
  </ack>
  <sec sec-type="data-availability" id="sec20">
    <title>Data availability</title>
    <p>The source code of CRISP along with information for installation and operation manuals is available in a GitHub repository (<ext-link xlink:href="https://github.com/vivekmathema/GCxGC-CRISP" ext-link-type="uri">https://github.com/vivekmathema/GCxGC-CRISP</ext-link>). The original in-house GC × GC–TOFMS contour images and associated raw data files used for construction and optimization of CRISP will be made available upon reasonable request. The information about CRISP is also made available at <ext-link xlink:href="http://metsysbio.com/CRISP_HTML/crisp_webinfo.html" ext-link-type="uri">http://metsysbio.com/CRISP_HTML/crisp_webinfo.html</ext-link>.</p>
  </sec>
  <sec id="sec23">
    <title>Funding</title>
    <p>Mahidol University (Grant no. (IO) R016420001 to S.K.); the National Research Council of Thailand (NRCT); Mahidol University (Grant no. NRCT5-TRG63009-03 to N.J.); the Research Network NANOTEC Program (RNN-Ramathibodi Hospital); the Faculty of Medicine Ramathibodi Hospital Mahidol University (Grant no. CF_62006 to N.S. and C.K.). C.K. was also supported by a grant from Ramathibodi Hospital and the National Science and Technology Development Agency (NSTDA) (Grant no. P-13-00505), Bangkok, Thailand.</p>
  </sec>
  <notes id="bio3">
    <p><bold>Vivek B. Mathema</bold> has a PhD in bioclinical sciences and is currently a researcher at the Siriraj Metabolomics and Phenomics Center, Faculty of Medicine Siriraj Hospital. Research interest include metabolomics, bioinformatics, oncology, parasitology and molecular epidemiology.</p>
    <p><bold>Kassaporn Duangkumpha</bold> received a PhD in Medical Biochemistry and Molecular Biology. Currently, she is a researcher with an interest in metabolomics for biomedical research.</p>
    <p><bold>Kwanjeera Wanichthanarak</bold> has a PhD in Bioscience. Her main areas of research are systems biology and bioinformatics. She is currently a senior researcher at the Siriraj Metabolomics and Phenomics Center.</p>
    <p><bold>Narumol Jariyasopit</bold> Narumol Jariyasopit has a PhD in Chemistry. Her research focuses on Mass spectrometry based-metabolomics.</p>
    <p><bold>Esha Dhakal</bold> (M. Sc.) is a graduate student at the department of Biochemistry, Faculty of Medicine Siriraj Hospital. Her research interest includes mass spectrometry and biochemistry of diseases.</p>
    <p><bold>Nuankanya Sathirapongsasuti (MD, PhD)</bold> is an instructor at the Section of Translational Medicine, Faculty of Medicine Ramathibodi Hospital. Research interest includes human diseases and health related organisms from multi-omics perspective.</p>
    <p><bold>Chagriya Kitiyakara (MD)</bold> is a professor of Medicine at Faculty of Medicine Ramathibodi Hospital. His research interests are biomarkers and metabolomics in kidney diseases.</p>
    <p><bold>Yongyut Sirivatanauksorn (MD, PhD)</bold> is a director of the Siriraj Metabolomics and Phenomics Center, and the founding president of the Thailand Metabolomics Society. His research interests are Hepatopancretobiliary cancer and liver transplantation.</p>
    <p><bold>Sakda Khoomrung</bold> (PhD in Chemistry) is an instructor at the Faculty of Medicine Siriraj Hospital. His research interest is on the development of tools in metabolomics and systems biology to support translational research. </p>
  </notes>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="ref1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Han</surname>  <given-names>L-D</given-names></string-name>, <string-name><surname>Xia</surname>  <given-names>J-F</given-names></string-name>, <string-name><surname>Liang</surname>  <given-names>Q-L</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Plasma esterified and non-esterified fatty acids metabolic profiling using gas chromatography–mass spectrometry and its application in the study of diabetic mellitus and diabetic nephropathy</article-title>. <source>Anal. Chim. Acta</source>  <year>2011</year>;<volume>689</volume>:<fpage>85</fpage>–<lpage>91</lpage>.<pub-id pub-id-type="pmid">21338761</pub-id></mixed-citation>
    </ref>
    <ref id="ref2">
      <label>2.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Yuzawa</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Inaguma</surname>  <given-names>D</given-names></string-name></person-group>. <source>Possible Biomarkers for Diabetic Kidney Disease. Diabetic Kidney Disease</source>.
<publisher-name>Springer</publisher-name>, <fpage>47</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tian</surname>  <given-names>TF</given-names></string-name>, <string-name><surname>Wang</surname>  <given-names>SY</given-names></string-name>, <string-name><surname>Kuo</surname>  <given-names>TC</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Web server for peak detection, baseline correction, and alignment in two-dimensional gas chromatography mass spectrometry-based metabolomics data</article-title>. <source>Anal Chem</source>  <year>2016</year>;<volume>88</volume>:<fpage>10395</fpage>–<lpage>403</lpage>.<pub-id pub-id-type="pmid">27673369</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Winnike</surname>  <given-names>JH</given-names></string-name>, <string-name><surname>Wei</surname>  <given-names>X</given-names></string-name>, <string-name><surname>Knagge</surname>  <given-names>KJ</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Comparison of GC-MS and GCxGC-MS in the analysis of human serum samples for biomarker discovery</article-title>. <source>J Proteome Res</source>  <year>2015</year>;<volume>14</volume>:<fpage>1810</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">25735966</pub-id></mixed-citation>
    </ref>
    <ref id="ref5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tranchida</surname>  <given-names>PQ</given-names></string-name>, <string-name><surname>Franchina</surname>  <given-names>FA</given-names></string-name>, <string-name><surname>Dugo</surname>  <given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Comprehensive two-dimensional gas chromatography-mass spectrometry: recent evolution and current trends</article-title>. <source>Mass Spectrom Rev</source>  <year>2016</year>;<volume>35</volume>:<fpage>524</fpage>–<lpage>34</lpage>.<pub-id pub-id-type="pmid">25269651</pub-id></mixed-citation>
    </ref>
    <ref id="ref6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname>  <given-names>Z</given-names></string-name>, <string-name><surname>Huang</surname>  <given-names>H</given-names></string-name>, <string-name><surname>Reim</surname>  <given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Optimizing gas chromatography mass spectrometry for robust tissue, serum and urine metabolite profiling</article-title>. <source>Talanta</source>  <year>2017</year>;<volume>165</volume>:<fpage>685</fpage>–<lpage>91</lpage>.<pub-id pub-id-type="pmid">28153317</pub-id></mixed-citation>
    </ref>
    <ref id="ref7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Welke</surname>  <given-names>JE</given-names></string-name>, <string-name><surname>Zini</surname>  <given-names>CA</given-names></string-name></person-group>. <article-title>Comprehensive two-dimensional gas chromatography for analysis of volatile compounds in foods and beverages</article-title>. <source>J Braz Chem</source>  <year>2011</year>;<volume>22</volume>:<fpage>609</fpage>–<lpage>22</lpage>.</mixed-citation>
    </ref>
    <ref id="ref8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Menendez-Carreno</surname>  <given-names>M</given-names></string-name>, <string-name><surname>Steenbergen</surname>  <given-names>H</given-names></string-name>, <string-name><surname>Janssen</surname>  <given-names>HG</given-names></string-name></person-group>. <article-title>Development and validation of a comprehensive two-dimensional gas chromatography-mass spectrometry method for the analysis of phytosterol oxidation products in human plasma</article-title>. <source>Anal Bioanal Chem</source>  <year>2012</year>;<volume>402</volume>:<fpage>2023</fpage>–<lpage>32</lpage>.<pub-id pub-id-type="pmid">21972006</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kates</surname>  <given-names>LN</given-names></string-name>, <string-name><surname>Richards</surname>  <given-names>PI</given-names></string-name>, <string-name><surname>Sandau</surname>  <given-names>CD</given-names></string-name></person-group>. <article-title>The application of comprehensive two-dimensional gas chromatography to the analysis of wildfire debris for ignitable liquid residue</article-title>. <source>Forensic Sci Int</source>  <year>2020</year>;<volume>310</volume>:<fpage>110256</fpage>.<pub-id pub-id-type="pmid">32229064</pub-id></mixed-citation>
    </ref>
    <ref id="ref10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wanichthanarak</surname>  <given-names>K</given-names></string-name>, <string-name><surname>Jeamsripong</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Pornputtapong</surname>  <given-names>N</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Accounting for biological variation with linear mixed-effects modelling improves the quality of clinical metabolomics data</article-title>. <source>Comput Struct Biotechnol J</source>  <year>2019</year>;<volume>17</volume>:<fpage>611</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">31110642</pub-id></mixed-citation>
    </ref>
    <ref id="ref11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sen</surname>  <given-names>P</given-names></string-name>, <string-name><surname>Lamichhane</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Mathema</surname>  <given-names>VB</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Deep learning meets metabolomics: a methodological perspective</article-title>. <source>Briefings in Bioinformatics</source>  <year>2020</year>;<volume>22</volume>(2):1531–42.</mixed-citation>
    </ref>
    <ref id="ref12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shah</surname>  <given-names>P</given-names></string-name>, <string-name><surname>Kendall</surname>  <given-names>F</given-names></string-name>, <string-name><surname>Khozin</surname>  <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Artificial intelligence and machine learning in clinical development: a translational perspective</article-title>. <source>NPJ Digit Med</source>  <year>2019</year>;<volume>2</volume>:<fpage>69</fpage>.<pub-id pub-id-type="pmid">31372505</pub-id></mixed-citation>
    </ref>
    <ref id="ref13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pomyen</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Wanichthanarak</surname>  <given-names>K</given-names></string-name>, <string-name><surname>Poungsombat</surname>  <given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Deep metabolome: applications of deep learning in metabolomics</article-title>. <source>Comput Struct Biotechnol J</source>  <year>2020</year>;<volume>18</volume>:<fpage>2818</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">33133423</pub-id></mixed-citation>
    </ref>
    <ref id="ref14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Miotto</surname>  <given-names>R</given-names></string-name>, <string-name><surname>Wang</surname>  <given-names>F</given-names></string-name>, <string-name><surname>Wang</surname>  <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Deep learning for healthcare: review, opportunities and challenges</article-title>. <source>Brief Bioinform</source>  <year>2018</year>;<volume>19</volume>:<fpage>1236</fpage>–<lpage>46</lpage>.<pub-id pub-id-type="pmid">28481991</pub-id></mixed-citation>
    </ref>
    <ref id="ref15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Méndez-Lucio</surname>  <given-names>O</given-names></string-name>, <string-name><surname>Baillif</surname>  <given-names>B</given-names></string-name>, <string-name><surname>Clevert</surname>  <given-names>D-A</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>De novo generation of hit-like molecules from gene expression signatures using artificial intelligence</article-title>. <source>Nat Commun</source>  <year>2020</year>;<volume>11</volume>:<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">31911652</pub-id></mixed-citation>
    </ref>
    <ref id="ref16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Jain</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Eng</surname>  <given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>A deep learning system for differential diagnosis of skin diseases</article-title>. <source>Nat Med</source>  <year>2020</year>;<volume>26</volume>:<fpage>900</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">32424212</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Zhou</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Liu</surname>  <given-names>X</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Wasserstein GAN-based small-sample augmentation for new-generation artificial intelligence: a case study of cancer-staging data in biology</article-title>. <source>Engineering</source>  <year>2019</year>;<volume>5</volume>:<fpage>156</fpage>–<lpage>63</lpage>.</mixed-citation>
    </ref>
    <ref id="ref18">
      <label>18.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ghahramani</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Watt</surname>  <given-names>FM</given-names></string-name>, <string-name><surname>Luscombe</surname>  <given-names>NM</given-names></string-name></person-group>. <article-title>Generative adversarial networks uncover epidermal regulators and predict single cell perturbations</article-title>. <source>bioRxiv</source>  <year>2018</year>;<fpage>262501</fpage>. doi: <pub-id pub-id-type="doi">10.1101/2625019</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Wang</surname>  <given-names>D</given-names></string-name>, <string-name><surname>He</surname>  <given-names>F</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Phenotype prediction and genome-wide association study using deep convolutional neural network of soybean</article-title>. <source>Front Genet</source>  <year>2019</year>;<volume>10</volume>:<fpage>1091</fpage>.<pub-id pub-id-type="pmid">31824557</pub-id></mixed-citation>
    </ref>
    <ref id="ref20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Montaez</surname>  <given-names>CAC</given-names></string-name>, <string-name><surname>Fergus</surname>  <given-names>P</given-names></string-name>, <string-name><surname>Montaez</surname>  <given-names>AC</given-names></string-name>  <etal>et al.</etal></person-group>  <part-title>Deep learning classification of polygenic obesity using genome wide association study SNPs</part-title>. In: <source>2018 International Joint Conference on Neural Networks (IJCNN)</source>. <year>2018</year>, p. <fpage>1</fpage>–<lpage>8</lpage>.
<publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eraslan</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Arloth</surname>  <given-names>J</given-names></string-name>, <string-name><surname>Martins</surname>  <given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>DeepWAS: directly integrating regulatory information into GWAS using deep learning supports master regulator MEF2C as risk factor for major depressive disorder</article-title>. <source>PLoS Comput Biol</source>  <year>2016</year>;<volume>16</volume>:<fpage>069096</fpage>.</mixed-citation>
    </ref>
    <ref id="ref22">
      <label>22.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Pan</surname>  <given-names>D</given-names></string-name>, <string-name><surname>Huang</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Zeng</surname>  <given-names>A</given-names></string-name>  <etal>et al.</etal></person-group>  <part-title>Early diagnosis of Alzheimer’s disease based on deep learning and GWAS</part-title>. In: <source>International Workshop on Human Brain and Artificial Intelligence</source>. <year>2019</year>, p. <fpage>52</fpage>–<lpage>68</lpage>.
<publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wei</surname>  <given-names>G-W</given-names></string-name></person-group>. <article-title>Protein structure prediction beyond AlphaFold</article-title>. <source>Nat Mach</source>  <year>2019</year>;<volume>1</volume>:<fpage>336</fpage>–<lpage>7</lpage>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Goodfellow</surname>  <given-names>I</given-names></string-name>, <string-name><surname>Pouget-Abadie</surname>  <given-names>J</given-names></string-name>, <string-name><surname>Mirza</surname>  <given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Generative adversarial nets</article-title>. <source>Advances in neural information processing systems</source>  <year>2014</year>;<volume>2</volume>:2672–80.</mixed-citation>
    </ref>
    <ref id="ref25">
      <label>25.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Liu</surname>  <given-names>H</given-names></string-name>, <string-name><surname>Gu</surname>  <given-names>X</given-names></string-name>, <string-name><surname>Samaras</surname>  <given-names>D</given-names></string-name></person-group>. <part-title>Wasserstein Gan with quadratic transport cost</part-title>. In: <source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source>. IEEE, <year>2019</year>, p. <fpage>4832</fpage>–<lpage>41</lpage>.</mixed-citation>
    </ref>
    <ref id="ref26">
      <label>26.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Ul Hassan MJNDnhniep-nv</collab></person-group>. <source>Vgg16 Convolutional Network for Classification and Detection</source>, <year>2018</year>. <ext-link xlink:href="https://neurohive.io/en/popular-networks/vgg16/" ext-link-type="uri">https://neurohive.io/en/popular-networks/vgg16/</ext-link>.</mixed-citation>
    </ref>
    <ref id="ref27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zushi</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Hashimoto</surname>  <given-names>S</given-names></string-name></person-group>. <article-title>Direct classification of GC x GC-analyzed complex mixtures using non-negative matrix factorization-based feature extraction</article-title>. <source>Anal Chem</source>  <year>2018</year>;<volume>90</volume>:<fpage>3819</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">29443502</pub-id></mixed-citation>
    </ref>
    <ref id="ref28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jiye</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Trygg</surname>  <given-names>J</given-names></string-name>, <string-name><surname>Gullberg</surname>  <given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Extraction and GC/MS analysis of the human blood plasma metabolome</article-title>. <source>Anal Chem</source>  <year>2005</year>;<volume>77</volume>:<fpage>8086</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">16351159</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Khoomrung</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Nookaew</surname>  <given-names>I</given-names></string-name>, <string-name><surname>Sen</surname>  <given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Metabolic profiling and compound-class identification reveal alterations in serum triglyceride levels in mice immunized with human vaccine adjuvant alum</article-title>. <source>J Proteome Res</source>  <year>2020</year>;<volume>19</volume>:<fpage>269</fpage>–<lpage>78</lpage>.<pub-id pub-id-type="pmid">31625748</pub-id></mixed-citation>
    </ref>
    <ref id="ref30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rangarajan</surname>  <given-names>AK</given-names></string-name>, <string-name><surname>Purushothaman</surname>  <given-names>R</given-names></string-name></person-group>. <article-title>Disease classification in eggplant using pre-trained vgg16 and msvm</article-title>. <source>Sci Rep</source>  <year>2020</year>;<volume>10</volume>:<fpage>1</fpage>–<lpage>11</lpage>.<pub-id pub-id-type="pmid">31913322</pub-id></mixed-citation>
    </ref>
    <ref id="ref31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name>Yamashita R, Nishio M, Do RKG</string-name>, <etal>et al.</etal></person-group>  <article-title>Convolutional neural networks: an overview and application in radiology</article-title>. <source>Insights into imaging</source>  <year>2018</year>;<volume>9</volume>:611–629.</mixed-citation>
    </ref>
    <ref id="ref32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yuan</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Ning</surname>  <given-names>H</given-names></string-name>, <string-name><surname>Lu</surname>  <given-names>X</given-names></string-name></person-group>. <article-title>Bio-inspired representation learning for visual attention prediction</article-title>. <source>IEEE Trans Cybern</source>  <year>2019</year>;<volume>51</volume>:3562. doi: <pub-id pub-id-type="doi">10.1109/TCYB.2019.2931735</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Norouzi</surname>  <given-names>M</given-names></string-name>, <string-name><surname>Fleet</surname>  <given-names>DJ</given-names></string-name>, <string-name><surname>Salakhutdinov</surname>  <given-names>RR</given-names></string-name></person-group>. <article-title>Hamming distance metric learning</article-title>. <source>Adv Neural Inform Proc Syst</source>  <year>2012</year>;<fpage>1061</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="ref34">
      <label>34.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Korhonen</surname>  <given-names>J</given-names></string-name>, <string-name><surname>You</surname>  <given-names>J</given-names></string-name></person-group>. <part-title>Peak Signal-to-Noise Ratio Revisited: Is Simple Beautiful?</part-title> In: <source>2012 Fourth International Workshop on Quality of Multimedia Experience</source>.
<publisher-name>IEEE</publisher-name>, <year>2012</year>, <fpage>37</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="ref35">
      <label>35.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mathiasen</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Hvilshøj</surname>  <given-names>F</given-names></string-name></person-group>. <source>Backpropagating through Fréchet Inception Distance</source>. arXiv preprint arXiv:2009.14075. <year>2020</year>. <ext-link xlink:href="https://arxiv.org/abs/2009.14075" ext-link-type="uri">https://arxiv.org/abs/2009.14075</ext-link>.</mixed-citation>
    </ref>
    <ref id="ref36">
      <label>36.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hore</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Ziou</surname>  <given-names>D</given-names></string-name></person-group>. <part-title>Image quality metrics: PSNR vs. SSIM</part-title>. In: <source>20th international conference on pattern recognition</source>. <year>2010</year>, p. <fpage>2366</fpage>–<lpage>9</lpage>.
<publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref37">
      <label>37.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Su</surname>  <given-names>J</given-names></string-name></person-group>. <source>GAN-QP: A Novel GAN Framework Without Gradient Vanishing and Lipschitz Constraint</source>, arXiv preprint arXiv:1811.07296. <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="ref38">
      <label>38.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhou</surname>  <given-names>Z</given-names></string-name>, <string-name><surname>Liang</surname>  <given-names>J</given-names></string-name>, <string-name><surname>Song</surname>  <given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>  <part-title>Lipschitz generative adversarial nets</part-title>. In: <source>International Conference on Machine Learning</source>. <year>2019</year>, p. <fpage>7584</fpage>–<lpage>93</lpage>.
<publisher-name>PMLR</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref39">
      <label>39.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Heusel</surname>  <given-names>M</given-names></string-name>, <string-name><surname>Ramsauer</surname>  <given-names>H</given-names></string-name>, <string-name><surname>Unterthiner</surname>  <given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>  <source>Gans Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</source>. San Diego, CA, USA: Neural Information Processing Systems (NIPS), <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Borji</surname>  <given-names>AJCV</given-names></string-name>, <string-name><surname>Understanding</surname>  <given-names>I</given-names></string-name></person-group>. <article-title>Pros and cons of Gan evaluation measures</article-title>. <source>Comput Vis Image Underst</source>  <year>2019</year>;<volume>179</volume>:<fpage>41</fpage>–<lpage>65</lpage>.</mixed-citation>
    </ref>
    <ref id="ref41">
      <label>41.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bansal</surname>  <given-names>R</given-names></string-name>, <string-name><surname>Raj</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Choudhury</surname>  <given-names>T</given-names></string-name></person-group>. <part-title>Blur image detection using Laplacian operator and open-CV</part-title>. <source>In: 2016 International Conference System Modeling &amp; Advancement in Research Trends (SMART)</source>. <year>2016</year>, p. <fpage>63</fpage>–<lpage>7</lpage>.
<publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>  <given-names>X</given-names></string-name></person-group>. <article-title>Laplacian operator-based edge detectors</article-title>. <source>IEEE transactions on pattern analysis and machine intelligence (TPAMI)</source>  <year>2007</year>;<volume>29</volume>:<fpage>886</fpage>–<lpage>90</lpage>.</mixed-citation>
    </ref>
    <ref id="ref43">
      <label>43.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Ahn</surname>  <given-names>N</given-names></string-name>, <string-name><surname>Kang</surname>  <given-names>B</given-names></string-name>, <string-name><surname>Sohn</surname>  <given-names>K-A</given-names></string-name></person-group>. <part-title>Fast, accurate, and lightweight super-resolution with cascading residual network</part-title>. <source>In: Proceedings of the European Conference on Computer Vision (ECCV)</source>. European Conference on Computer Vision (ECCV), <year>2018</year>, p. <fpage>252</fpage>–<lpage>68</lpage>.</mixed-citation>
    </ref>
    <ref id="ref44">
      <label>44.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wen</surname>  <given-names>L</given-names></string-name>, <string-name><surname>Li</surname>  <given-names>X</given-names></string-name>, <string-name><surname>Li</surname>  <given-names>X</given-names></string-name>, <etal>et al.</etal></person-group>  <part-title>A new transfer learning based on VGG-19 network for fault diagnosis</part-title>. <source>In: 2019 IEEE 23rd International Conference on Computer Supported Cooperative Work in Design (CSCWD)</source>. <year>2019</year>, p. <fpage>205</fpage>–<lpage>9</lpage>.
<publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref45">
      <label>45.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Szegedy</surname>  <given-names>C</given-names></string-name>, <string-name><surname>Vanhoucke</surname>  <given-names>V</given-names></string-name>, <string-name><surname>Ioffe</surname>  <given-names>S</given-names></string-name>  <etal>et al.</etal></person-group>  <part-title>Rethinking the inception architecture for computer vision</part-title>. <source>In: Proceedings of the IEEE conference on computer vision and pattern recognition</source>. Piscataway, New Jersey, United States: IEEE, <year>2016</year>, p. <fpage>2818</fpage>–<lpage>26</lpage>.</mixed-citation>
    </ref>
    <ref id="ref46">
      <label>46.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>He</surname>  <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname>  <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname>  <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>  <part-title>Deep residual learning for image recognition</part-title>. <source>In: Proceedings of the IEEE conference on computer vision and pattern recognition</source>. Piscataway, New Jersey, United States: IEEE, <year>2016</year>, p. <fpage>770</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="ref47">
      <label>47.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Huang</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Liu</surname>  <given-names>Z</given-names></string-name>, <string-name><surname>Van Der Maaten</surname>  <given-names>L</given-names></string-name></person-group>, et al. <part-title>Densely connected convolutional networks</part-title>. <source>In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2017</year>, p. <fpage>4700</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="ref48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Qassim</surname>  <given-names>H</given-names></string-name>, <string-name><surname>Verma</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Feinzimer</surname>  <given-names>D</given-names></string-name></person-group>. <part-title>Compressed residual-VGG16 CNN model for big data places image recognition</part-title>. <source>In: 2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)</source>. <year>2018</year>, p. <fpage>169</fpage>–<lpage>75</lpage>.
<publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="ref49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pedregosa</surname>  <given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname>  <given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Scikit-learn: machine learning in python</article-title>. <source>the Journal of Machine Learning research (JMLR)</source>  <year>2011</year>;<volume>12</volume>:<fpage>2825</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="ref50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gagnebin</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Jaques</surname>  <given-names>DA</given-names></string-name>, <string-name><surname>Rudaz</surname>  <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Exploring blood alterations in chronic kidney disease and haemodialysis using metabolomics</article-title>. <source>Scientific reports</source>  <year>2020</year>;<volume>10</volume>:<fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">31913322</pub-id></mixed-citation>
    </ref>
    <ref id="ref51">
      <label>51.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Abadi</surname>  <given-names>M</given-names></string-name>, <string-name><surname>Barham</surname>  <given-names>P</given-names></string-name>, <string-name><surname>Chen</surname>  <given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>  <part-title>Tensorflow: a system for large-scale machine learning</part-title>. In: <source>12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)</source>. USA: USENIX Association, <year>2016</year>, p. <fpage>265</fpage>–<lpage>83</lpage>.</mixed-citation>
    </ref>
    <ref id="ref52">
      <label>52.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ketkar</surname>  <given-names>N</given-names></string-name></person-group>. <part-title>Introduction to keras</part-title>. In: <source>Deep Learning with Python</source>.
<publisher-name>Springer</publisher-name>, <year>2017</year>, <fpage>97</fpage>–<lpage>111</lpage>.</mixed-citation>
    </ref>
    <ref id="ref53">
      <label>53.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rood</surname>  <given-names>D</given-names></string-name></person-group>. <source>The Troubleshooting and Maintenance Guide for Gas Chromatographers</source>.
<publisher-name>John Wiley &amp; Sons</publisher-name>, <year>2007</year>.</mixed-citation>
    </ref>
    <ref id="ref54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xu</surname>  <given-names>X</given-names></string-name>, <string-name><surname>Song</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Zhu</surname>  <given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Simultaneous determination of two acute poisoning rodenticides tetramine and fluoroacetamide with a coupled column in poisoning cases</article-title>. <source>Journal of Chromatography</source>  <year>2008</year>;<volume>876</volume>:<fpage>103</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">18993117</pub-id></mixed-citation>
    </ref>
    <ref id="ref55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dey</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Das</surname>  <given-names>TN</given-names></string-name></person-group>. <article-title>Septum bleed during GC–MS analysis: utility of septa of various makes</article-title>. <source>Journal of chromatographic science (JCS)</source>  <year>2013</year>;<volume>51</volume>:<fpage>117</fpage>–<lpage>21</lpage>.<pub-id pub-id-type="pmid">22781185</pub-id></mixed-citation>
    </ref>
    <ref id="ref56">
      <label>56.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Liu</surname>  <given-names>B</given-names></string-name>, <string-name><surname>Wei</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname>  <given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>  <part-title>Deep neural networks for high dimension, low sample size data</part-title>. In: <source>IJCAI</source>. California, USA: IJCAI (International Joint Conferences on Artificial Intelligence Organization), <year>2017</year>, <fpage>2287</fpage>–<lpage>93</lpage>.</mixed-citation>
    </ref>
    <ref id="ref57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>VanRullen</surname>  <given-names>R</given-names></string-name>, <string-name><surname>Reddy</surname>  <given-names>L</given-names></string-name></person-group>. <article-title>Reconstructing faces from fMRI patterns using deep generative neural networks</article-title>. <source>Communications biology</source>  <year>2019</year>;<volume>2</volume>:<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">30740537</pub-id></mixed-citation>
    </ref>
    <ref id="ref58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname>  <given-names>D</given-names></string-name>, <string-name><surname>Moon</surname>  <given-names>W-J</given-names></string-name>, <string-name><surname>Ye</surname>  <given-names>JC</given-names></string-name></person-group>. <article-title>Assessing the importance of magnetic resonance contrasts using collaborative generative adversarial networks</article-title>. <source>Nat Mach Intell</source>  <year>2020</year>;<volume>2</volume>:<fpage>34</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="ref59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brandt</surname>  <given-names>P</given-names></string-name>, <string-name><surname>Garbe</surname>  <given-names>E</given-names></string-name>, <string-name><surname>Vylkova</surname>  <given-names>S</given-names></string-name></person-group>. <article-title>Catch the wave: metabolomic analyses in human pathogenic fungi</article-title>. <source>PLoS Pathog</source>  <year>2020</year>;<volume>16</volume>:<fpage>e1008757</fpage>.<pub-id pub-id-type="pmid">32817633</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
