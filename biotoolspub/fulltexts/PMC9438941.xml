<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Stat Anal Data Min</journal-id>
    <journal-id journal-id-type="iso-abbrev">Stat Anal Data Min</journal-id>
    <journal-id journal-id-type="doi">10.1002/(ISSN)1932-1872</journal-id>
    <journal-id journal-id-type="publisher-id">SAM</journal-id>
    <journal-title-group>
      <journal-title>Statistical Analysis and Data Mining</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1932-1864</issn>
    <issn pub-type="epub">1932-1872</issn>
    <publisher>
      <publisher-name>Wiley Subscription Services, Inc., A Wiley Company</publisher-name>
      <publisher-loc>Hoboken</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9438941</article-id>
    <article-id pub-id-type="pmid">36061078</article-id>
    <article-id pub-id-type="doi">10.1002/sam.11573</article-id>
    <article-id pub-id-type="publisher-id">SAM11573</article-id>
    <article-categories>
      <subj-group subj-group-type="overline">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Research Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A general iterative clustering algorithm</article-title>
      <alt-title alt-title-type="left-running-head">Lin et al.</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="sam11573-cr-0001" contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>Ziqiang</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-1990-6788</contrib-id>
        <xref rid="sam11573-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="sam11573-cr-0002" contrib-type="author" corresp="yes">
        <name>
          <surname>Laska</surname>
          <given-names>Eugene</given-names>
        </name>
        <xref rid="sam11573-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="sam11573-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="sam11573-curr-0001" ref-type="aff">
          <sup>3</sup>
        </xref>
        <address>
          <email>eugene.laska@nyulangone.org</email>
        </address>
      </contrib>
      <contrib id="sam11573-cr-0003" contrib-type="author">
        <name>
          <surname>Siegel</surname>
          <given-names>Carole</given-names>
        </name>
        <xref rid="sam11573-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="sam11573-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="sam11573-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">Department of Psychiatry</named-content>
      <institution>New York University Langone School of Medicine</institution>
      <city>New York</city>
      <named-content content-type="country-part">NY</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="sam11573-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <named-content content-type="organisation-division">Department of Population Health, Division of Biostatistics</named-content>
      <institution>New York University Langone School of Medicine</institution>
      <city>New York</city>
      <named-content content-type="country-part">NY</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="sam11573-curr-0001">
      <label>
        <sup>3</sup>
      </label>
      <city>One Park Avenue, New York</city>
      <named-content content-type="country-part">NY</named-content>
      <postal-code>10016</postal-code>
      <country country="US">USA</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label><bold>Correspondence</bold><break/>
Eugene Laska, Department of Psychiatry, New York University, One Park Ave, New York, NY, 10016, USA.<break/>
Email: <email>eugene.laska@nyulangone.org</email>
<break/>
</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>31</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <volume>15</volume>
    <issue seq="30">4</issue>
    <issue-id pub-id-type="doi">10.1002/sam.v15.4</issue-id>
    <fpage>433</fpage>
    <lpage>446</lpage>
    <history>
      <date date-type="rev-recd">
        <day>27</day>
        <month>12</month>
        <year>2021</year>
      </date>
      <date date-type="received">
        <day>21</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>1</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <!--&#x000a9; 2022 Wiley Periodicals LLC.-->
      <copyright-statement content-type="article-copyright">© 2022 The Authors. <italic toggle="yes">Statistical Analysis and Data Mining</italic> published by Wiley Periodicals LLC.</copyright-statement>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link> License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non‐commercial and no modifications or adaptations are made.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="file:SAM-15-433.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>The quality of a cluster analysis of unlabeled units depends on the quality of the between units dissimilarity measures. Data‐dependent dissimilarity is more objective than data independent geometric measures such as Euclidean distance. As suggested by Breiman, many data driven approaches are based on decision tree ensembles, such as a random forest (RF), that produce a proximity matrix that can easily be transformed into a dissimilarity matrix. An RF can be obtained using labels that distinguish units with real data from units with synthetic data. The resulting dissimilarity matrix is input to a clustering program and units are assigned labels corresponding to cluster membership. We introduce a general iterative cluster (GIC) algorithm that improves the proximity matrix and clusters of the base RF. The cluster labels are used to grow a new RF yielding an updated proximity matrix, which is entered into the clustering program. The process is repeated until convergence. The same procedure can be used with many base procedures such as the extremely randomized tree ensemble. We evaluate the performance of the GIC algorithm using benchmark and simulated data sets. The properties measured by the Silhouette score are substantially superior to the base clustering algorithm. The GIC package has been released in R: 
<ext-link xlink:href="https://cran.r-project.org/web/packages/GIC/index.html" ext-link-type="uri">https://cran.r‐project.org/web/packages/GIC/index.html</ext-link>.</p>
    </abstract>
    <kwd-group kwd-group-type="author-generated">
      <kwd id="sam11573-kwd-0001">clustering</kwd>
      <kwd id="sam11573-kwd-0002">extremely randomized tree</kwd>
      <kwd id="sam11573-kwd-0003">iterative RF clustering</kwd>
      <kwd id="sam11573-kwd-0004">proximity</kwd>
      <kwd id="sam11573-kwd-0005">random forest</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="funding-0001">
        <funding-source>
          <institution-wrap>
            <institution>National Institute on Alcohol Abuse and Alcoholism
</institution>
            <institution-id institution-id-type="doi">10.13039/100000027</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>PO1AA027057‐01</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="6"/>
      <page-count count="14"/>
      <word-count count="7098"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>August 2022</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.2.0 mode:remove_FC converted:07.10.2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <p content-type="self-citation">
      <mixed-citation publication-type="journal" id="sam11573-cit-1001"><string-name><given-names>Z.</given-names><surname>Lin</surname></string-name>, <string-name><given-names>E.</given-names><surname>Laska</surname></string-name>, <string-name><given-names>C.</given-names><surname>Siegel</surname></string-name>, <article-title>A general iterative clustering algorithm</article-title>, <source>Stat. Anal. Data Min.: ASA Data Sci. J.</source>. <volume>15</volume> (<year>2022</year>), <fpage>433</fpage>–<lpage>446</lpage>. <pub-id pub-id-type="doi">10.1002/sam.11573</pub-id>
</mixed-citation>
    </p>
    <fn-group>
      <fn id="sam11573-note-1002">
        <p><bold>Funding information</bold> National Institute on Alcohol Abuse and Alcoholism, PO1AA027057‐01</p>
      </fn>
    </fn-group>
  </notes>
</front>
<body id="sam11573-body-0001">
  <sec id="sam11573-sec-0001">
    <label>1</label>
    <title>INTRODUCTION</title>
    <p>Finding distinct homogeneous clusters of a sample of units, each with many attributes or features, can clarify complicated heterogeneous relationships. For example, in medicine, an apparent heterogeneous disorder may actually be a combination of several subtype disorders with specific clinical and/or biological features. These features may indicate specific treatment with better outcomes and their identification through cluster analysis fulfills a primary goal of precision medicine. Complex illnesses such as schizophrenia, alcohol use disorder, and PTSD will benefit from the modern ability to handle the large number of biological features that are now collected across multiple domains with increasing scientific and technological sophistication.</p>
    <p>A <italic toggle="yes">cluster</italic> is a group of units that are close to each other and far from units in other clusters, where distance, <italic toggle="yes">proximity or dissimilarity</italic> is a function of the input attributes or features. There are two critical elements in a cluster analysis. The underlying distance between all pairs of units is a core ingredient. The second is a structured algorithm for finding a partition of the units into separate groups that maximizes an objective function of dissimilarities. The overarching goal of the methods we introduce here is to improve cluster analysis, but the vehicle to accomplish it is to improve methods for obtaining distance measures. Historically, these were defined by data independent geometric measures such as Euclidean distance. However, experience has taught that defining similarity metrics based on formulaic assumption on data structures whose complexities are not well understood can lead to meaningless results. Data‐dependent dissimilarity (DDD) [<xref rid="sam11573-bib-0022" ref-type="bibr">22</xref>], in contrast, provides a more principled measure of dissimilarity than does data independent geometric models. Much work has been done to develop and refine DDD measures.</p>
    <p>In this communication, we introduce an approach that iterates between DDDs obtained from decision tree based supervised classifiers and the resulting new clusters. These in turn are used to obtain new estimates of DDD leading to new clusters until convergence. After providing background information in the Methods section, we describe our new approach, the general iterative cluster (GIC) algorithm. We illustrate the method using Breiman's random forest [<xref rid="sam11573-bib-0006" ref-type="bibr">6</xref>] and Geurts, Ernst, and Wehenkel's extremely randomized trees (ERT) [<xref rid="sam11573-bib-0017" ref-type="bibr">17</xref>] as base DDDs and partitioning around medoids (PAM) [<xref rid="sam11573-bib-0021" ref-type="bibr">21</xref>, <xref rid="sam11573-bib-0031" ref-type="bibr">31</xref>] as the classification algorithm. The data sets on which the clustering algorithm are compared include real‐world data sets, which are part of the set of benchmarks commonly used in cluster analysis research, as well as randomly generated simulation data.</p>
  </sec>
  <sec id="sam11573-sec-0002">
    <label>2</label>
    <title>METHODS</title>
    <sec id="sam11573-sec-0003">
      <label>2.1</label>
      <title>Background</title>
      <p>A machine learning/data mining clustering task consists of identifying clusters from a data set of unlabeled units and their features. There is no ground truth. It is usually assumed that the training data set is comprised of random and independent samples from a fixed and unknown probability distribution over the set of all possible feature vectors. The most common cluster objective function is the average dissimilarity between each unit in the cluster and the center of the cluster. The widely used <italic toggle="yes">k‐means algorithm</italic> utilizes the centroid and the <italic toggle="yes">k‐medoids algorithm</italic> [<xref rid="sam11573-bib-0021" ref-type="bibr">21</xref>, <xref rid="sam11573-bib-0031" ref-type="bibr">31</xref>] utilizes the medoid as the center. The medoid is the member of a cluster whose mean dissimilarity from all other members in the cluster is a minimum. It is generally believed that it is easier to interpret clusters determined by the k‐medoids centers than those arising from a k‐means analysis. Because it is based on means, the k‐means approach is more vulnerable to outliers than is k‐medoids. The input to the k‐medoids algorithm is an arbitrary dissimilarity matrix, whose elements are not required to satisfy the geometric distance metric conditions. Commonly used formula‐based <italic toggle="yes">dissimilarity</italic> functions are Euclidean, Manhattan, Angular Distance, Hamming, Cosine Similarity, and the Huffman Code. Milligan [<xref rid="sam11573-bib-0026" ref-type="bibr">26</xref>] conducted a Monte Carlo study of 30 internal criterion measures for cluster analysis, and Hubalek [<xref rid="sam11573-bib-0019" ref-type="bibr">19</xref>] used 20 of the 43 similarity measures he collected for cluster analyses on mushroom data.</p>
      <p>Recognizing the limitations of formula defined dissimilarity functions such as those listed above, a considerable number of methods have been proposed to produce DDD measures. One important way is based on a simple but elegant idea. Split the data into subsets using a specific decision rule at each node in a decision tree. Pairs of units that follow the same pathway down the tree to the terminal nodes or leaves are similar with respect to the decision rule. The proximity of a pair of units is the fraction of times a common path, defined in many possible ways, is followed. Then the square root of one minus the proximity is a dissimilarity measure.</p>
      <sec id="sam11573-sec-0004">
        <label>2.1.1</label>
        <title>Random forest</title>
        <p>Many approaches to obtain DDD are variants or derivatives of the random forest (RF) algorithm introduced by Breiman [<xref rid="sam11573-bib-0006" ref-type="bibr">6</xref>], who built on Amit and Geman's [<xref rid="sam11573-bib-0002" ref-type="bibr">2</xref>] contributions on geometric feature selection, Ho′s [<xref rid="sam11573-bib-0019" ref-type="bibr">19</xref>] work on random methods, and Dietterich's [<xref rid="sam11573-bib-0013" ref-type="bibr">13</xref>] random split selection approach. An RF is an ensemble of individual trees [<xref rid="sam11573-bib-0006" ref-type="bibr">6</xref>, <xref rid="sam11573-bib-0042" ref-type="bibr">42</xref>] grown to obtain a classifier based on bootstrapped samples of labeled data on a sample of units. In the process a data driven proximity matrix is produced. A decision tree is grown from a bootstrap sample of units. At each node, a random subset of features is selected and an optimal splitting threshold determined, based on a criterion that maximizes a measure of node purity such as the degree to which units in the child nodes belong to a single class. A widely used criterion is the Gini impurity index [<xref rid="sam11573-bib-0008" ref-type="bibr">8</xref>]. The splitting process continues until an unpruned tree is grown. Replicate trees are grown following the same rules on independent bootstrap samples. Breiman [<xref rid="sam11573-bib-0006" ref-type="bibr">6</xref>] proposed that the proximity between units is the fraction of trees in which both members are in the same terminal node. Bicego and Escolano [<xref rid="sam11573-bib-0005" ref-type="bibr">5</xref>] performed an empirical evaluation of four RF learning schemes examining alternative forest parametrizations, distances, and clustering algorithms.</p>
        <p>Our approach applies to any classification algorithm based on an ensemble of decision trees that utilize labels at nodes and produce a proximity matrix.</p>
      </sec>
      <sec id="sam11573-sec-0005">
        <label>2.1.2</label>
        <title>Generalized RF</title>
        <p>An ensemble of trees grown to obtain a classifier is a generalized RF that includes
<list list-type="alpha-upper" id="sam11573-list-0001"><list-item id="sam11573-li-0001"><p>an initialization label from which the classification process begins, for example, artificial or random labels for sample units;</p></list-item><list-item id="sam11573-li-0002"><p>rules at each node for growing a decision tree and a stopping rule for deciding when to discontinue splitting;</p></list-item><list-item id="sam11573-li-0003"><p>a rule for defining similarity between units.</p></list-item></list>
</p>
        <p>The similarity matrix is turned into a dissimilarity matrix and used in a clustering program, such as k‐medoids.</p>
      </sec>
      <sec id="sam11573-sec-0006">
        <label>2.1.3</label>
        <title>Example initialization approaches for start‐up labels</title>
        <p>For decision trees ensembles designed for classification, unit labels are required to grow a tree. There are several ways of starting the process. One is to introduce an auxiliary sample. Labels identify whether a unit's data are from the original or the auxiliary sample. Breiman [<xref rid="sam11573-bib-0006" ref-type="bibr">6</xref>] and Breiman and Cutler's [<xref rid="sam11573-bib-0007" ref-type="bibr">7</xref>] approach, which we call RFC, is to randomly produce synthetic feature data from a reference distribution obtained by sampling from the product of empirical marginal distributions of the sample data. The motive is to reduce the between tree dependency. Shi and Horvath [<xref rid="sam11573-bib-0033" ref-type="bibr">33</xref>] proposed alternatives. In <italic toggle="yes">AddCl1</italic>, synthetic data are generated by randomly sampling from the product of empirical marginal distributions of the variables. In <italic toggle="yes">AddCl2</italic>, synthetic data are obtained by randomly sampling from the hyper‐rectangle that contains the observed data. Siegel and colleagues [<xref rid="sam11573-bib-0035" ref-type="bibr">35</xref>] proposed “purposeful” clustering in which the auxiliary data are a sample from a separate population related to the purpose of the clustering. In their search for subtypes of PTSD for war fighters, they used an auxiliary set of data of healthy controls who were war fighters. In the initial iteration step, an RF was grown to distinguish these individuals from individuals with PTSD. Another approach is to assign labels to the units by any strategy. Dalleau and colleagues [<xref rid="sam11573-bib-0011" ref-type="bibr">11</xref>] proposed <italic toggle="yes">AddCl3</italic>, which randomly assigns a label for each unit. Yet another strategy is to apply a geometric measure such as Euclidean distance on the part of the feature vector that is numeric, enter the resulting between unit distances into a cluster program and use the resulting cluster membership as labels for the initial iteration of the decision tree ensemble.</p>
      </sec>
      <sec id="sam11573-sec-0007">
        <label>2.1.4</label>
        <title>Example approaches to forming a decision tree with labels</title>
        <p>A decision tree can include all subjects or a randomly chosen bootstrap sample. All available features can be considered for determining a splitting rule at each node or a random subset can be selected. Among candidate features at a node, the threshold on which to perform a split is usually chosen so as to optimize a criterion such as the Gini impurity index. Another approach is to find the linear combination of features that optimize the impurity index at each node. In classical RF, a random set of features is selected and the one with a threshold that produces the best Gini impurity index is used. At each node in an extremely random tree (ERT) introduced by Geurts et al. [<xref rid="sam11573-bib-0017" ref-type="bibr">17</xref>], a splitting threshold is randomly selected for each of a randomly selected subset of features at each node. The feature used is the one whose split produces the best value of the Gini impurity index.</p>
      </sec>
      <sec id="sam11573-sec-0008">
        <label>2.1.5</label>
        <title>Example decision trees without a label based splitting criteria</title>
        <p>In the extreme, forests can be grown that do not require labels at any stage. For example, Breiman [<xref rid="sam11573-bib-0006" ref-type="bibr">6</xref>] and Cutler and Zhao [<xref rid="sam11573-bib-0010" ref-type="bibr">10</xref>] introduced <italic toggle="yes">extreme random splitting</italic>, calling the resulting ensemble a <italic toggle="yes">purely random forest</italic>; both the feature and the location of the cut‐point at every node in every tree in the ensemble is randomly chosen. Fernando and Webb [<xref rid="sam11573-bib-0015" ref-type="bibr">15</xref>] proposed the Centered Forest, which is an unsupervised stochastic forest. At each node in a tree, units are divided into two equal subsets by splitting at the median value of a randomly chosen feature. Recursive splits do not depend on labels. These kinds of forests cannot be used in the GIT we describe in Section <xref rid="sam11573-sec-0012" ref-type="sec">2.2</xref> except for the initial run.</p>
      </sec>
      <sec id="sam11573-sec-0009">
        <label>2.1.6</label>
        <title>Example definitions of similarity</title>
        <p>Many contributions in the literature have been devoted to obtaining better distance measures for input to cluster programs. Breiman [<xref rid="sam11573-bib-0006" ref-type="bibr">6</xref>] used the fraction of trees two units are in the same terminal node. Aryal and colleagues [<xref rid="sam11573-bib-0003" ref-type="bibr">3</xref>] carried out a comparative study of data‐dependent approaches without learning in measuring similarities of data objects. One alternative measure is called Zhu2, [<xref rid="sam11573-bib-0043" ref-type="bibr">43</xref>], which defines similarity as proportional to the average length of the path two units share in their travel down the tree to the terminal node. Another is “Zhu3” [<xref rid="sam11573-bib-0043" ref-type="bibr">43</xref>], which utilizes node weights, defined as the inverse of the number of units that reach the node at every node along the shared path. Ting and colleagues [<xref rid="sam11573-bib-0038" ref-type="bibr">38</xref>] define similarity between a pair of units as the ratio of units in the training set reaching the lowest common ancestor of the pair. These authors used isolation forests (iForest) [<xref rid="sam11573-bib-0025" ref-type="bibr">25</xref>] to obtain pairwise similarity, defined as the probability mass of the smallest region in feature space covering the pair in a hierarchical partitioning of the space into non‐overlapping and non‐empty regions. The dissimilarity between two units across an iForest is the average probability mass in the deepest shared node in a collection of trees.</p>
      </sec>
      <sec id="sam11573-sec-0010">
        <label>2.1.7</label>
        <title>Some examples of generalized RF distance ensembles for clustering</title>
        <p>Kulkarni and Sinha [<xref rid="sam11573-bib-0023" ref-type="bibr">23</xref>] presented a taxonomy of various versions of random forest. Similar in spirit to a probability mixture distribution approach to clustering, Bicego [<xref rid="sam11573-bib-0004" ref-type="bibr">4</xref>] proposed a method based on a set of RFs, each one devoted to modeling one cluster. The RFs are iteratively updated using a k‐means‐like clustering algorithm. Yan et al. [<xref rid="sam11573-bib-0041" ref-type="bibr">41</xref>] proposed a method that randomly probes the vector space of features to detect locally “good” clusters that are subsequently aggregated by spectral clustering [<xref rid="sam11573-bib-0039" ref-type="bibr">39</xref>] to produce what they call cluster forests.</p>
        <p>Several recent applications of ERTs have appeared in the domain of brain tumor segmentation [<xref rid="sam11573-bib-0018" ref-type="bibr">18</xref>, <xref rid="sam11573-bib-0029" ref-type="bibr">29</xref>, <xref rid="sam11573-bib-0036" ref-type="bibr">36</xref>]. Other applications include content‐based image classification [<xref rid="sam11573-bib-0027" ref-type="bibr">27</xref>], image categorization and segmentation [<xref rid="sam11573-bib-0034" ref-type="bibr">34</xref>], and video segmentation [<xref rid="sam11573-bib-0028" ref-type="bibr">28</xref>].</p>
      </sec>
      <sec id="sam11573-sec-0011">
        <label>2.1.8</label>
        <title>Cluster ensembles</title>
        <p>Strehl and Ghosh [<xref rid="sam11573-bib-0037" ref-type="bibr">37</xref>] considered the problem of combining multiple partitions of a set of units into a single consolidated cluster set without reference to their source. For example, using Breiman's approach to obtaining synthetic data, multiple RF runs, called the “ensemble constructor” over the same data set, or a single run over different data sets, are used to produce sets of clusters. Each one is called an “ensemble member” and collectively they are referred to as the “base clusters.” A “consensus function” combines the base clusters to produce an overall consolidated cluster. Alhusain and Hafe [<xref rid="sam11573-bib-0001" ref-type="bibr">1</xref>] used this method to determine underlying population structure based on genetic data. They call their method the random forest cluster ensemble (RFcluE). Clusters are found using k‐means operating on the dissimilarity output of the RF based on Breiman's algorithm after multidimensional scaling [<xref rid="sam11573-bib-0014" ref-type="bibr">14</xref>] is used for transformation to Euclidean space. The overall definition of similarity between any two units is the proportion of times in the ensemble that the pair are assigned to the same cluster. The so‐called co‐association matrix is input to an agglomerative hierarchical clustering algorithm to obtain the final cluster.</p>
      </sec>
    </sec>
    <sec id="sam11573-sec-0012">
      <label>2.2</label>
      <title>The general iterative cluster algorithm</title>
      <p>The GIC algorithm is very general and can be applied to DDD‐based clustering approaches described above except in the purely random case. The simple idea is that new proximity matrices and clusters are obtained iteratively. The GIC algorithm begins by running the underlying or base classification method using an initialization procedure as required to obtain a proximity matrix, followed by running the selected cluster algorithm. Thereafter, each iteration uses the same base classifier followed by the same clustering algorithm. At each step, units are labeled according to the cluster to which they were assigned in the cluster algorithm in the previous iteration. The process continues until convergence. Convergence occurs when the assignment of units to clusters does not change, which corresponds to a proximity matrix that does not change. Techniques such as <italic toggle="yes">AddCl1</italic> or <italic toggle="yes">AddCl2</italic> can be used to generate synthetic data when called for by the base algorithm, but only for the first iteration. Because the results are potentially dependent on the random assignment or the random seed, the procedure is repeated many times to obtain a balanced data set. The approach can be conceptualized as providing improved estimates of the dissimilarity measures, which in turn produces improved clusters.</p>
      <p>One example that can be used as the base classifier is the approach of Shi and Horvath [<xref rid="sam11573-bib-0033" ref-type="bibr">33</xref>]. After the first iteration, the synthetic data are not used again. Another example is to use ERT as the underlying classifier and PAM as the clustering method. We denote this by IERT. In the first and only the first step, either <italic toggle="yes">AddCl1</italic>, <italic toggle="yes">AddCl2</italic>, or <italic toggle="yes">AddCl3</italic> is used to assign labels. In one version of ERT, the square root of the number of features are randomly selected as candidates at each node and cut‐points are randomly selected for each feature. The cut‐point and feature with the best purity index are chosen for the split. The proximity matrix resulting from this ERT is converted to a dissimilarity matrix, which is input to PAM to obtain clusters. Thereafter, at each successive iteration, the ERT ensemble is grown based on unit labels corresponding to the cluster to which the units were assigned in the previous iteration. The process is repeated until convergence.</p>
      <p>As a third example, in the RFcluE method for cluster ensembles [<xref rid="sam11573-bib-0001" ref-type="bibr">1</xref>], after the proximity matrix is first obtained, clusters are found using the GIC algorithm iteration process. The resulting proximity matrices are used by RFcluE as before. Multidimensional scaling is used to transform each one to Euclidean space, which is then passed to a k‐means clustering algorithm.</p>
    </sec>
    <sec id="sam11573-sec-0013">
      <label>2.3</label>
      <title>Real‐world data sets for comparisons of base <styled-content style="fixed-case" toggle="no">DDD</styled-content> and <styled-content style="fixed-case" toggle="no">ICAs</styled-content>
</title>
      <sec id="sam11573-sec-0014">
        <label>2.3.1</label>
        <title>Evaluation using the iris data</title>
        <p>The iris flower data set described by R. A. Fisher in 1936 [<xref rid="sam11573-bib-0016" ref-type="bibr">16</xref>] contains 50 examples of flowers from each of three iris species, setosa, virginica, and versicolor. It is considered one of the standard benchmark data sets for cluster analysis research and perhaps the best‐known database to be found in the pattern recognition literature. Fisher's paper has been cited nearly 20,000 times. Four measures were taken for each flower, sepals length, sepals width, petals length, and petals width. Detailed information about these data can be found at <ext-link xlink:href="https://archive.ics.uci.edu/ml/datasets/iris" ext-link-type="uri">https://archive.ics.uci.edu/ml/datasets/iris</ext-link>.</p>
      </sec>
      <sec id="sam11573-sec-0015">
        <label>2.3.2</label>
        <title>Evaluation using a heart disease data set</title>
        <p>The heart disease data set [<xref rid="sam11573-bib-0012" ref-type="bibr">12</xref>] comes from patients undergoing angiography in a multisite study conducted at the Cleveland Clinic in Cleveland, the Hungarian Institute of Cardiology in Budapest, the Veterans Administration Medical Center in Long Beach, and University Hospitals in Zurich and Basel. It too is considered one of the standard benchmark data sets for cluster analysis research. It is comprised of 120 individuals who have heart disease and 150 who do not. Although many measures were taken on each individual, 13 are considered the “standard” data set including age, gender, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved during exercise, exercise‐induced angina, exercise‐induced ST depression, the slope of the peak exercise ST segment, number of major vessels, and thal. Detailed information can be found at <ext-link xlink:href="https://archive.ics.uci.edu/ml/datasets/Statlog+" ext-link-type="uri">https://archive.ics.uci.edu/ml/datasets/Statlog+</ext-link>(Heart).</p>
      </sec>
      <sec id="sam11573-sec-0016">
        <label>2.3.3</label>
        <title>Evaluation using standard real‐world data sets</title>
        <p>Dalleau et al. [<xref rid="sam11573-bib-0011" ref-type="bibr">11</xref>] studied ERT starting the clustering algorithm with AddCl3. We used the same real‐world data sets they used for empirical evaluations. The size of the sample, the number of features, and the number of labels for each study are given in Table <xref rid="sam11573-tbl-0005" ref-type="table">5</xref>. The data are available on the UCI website <ext-link xlink:href="https://archive.ics.uci.edu/ml/index.php" ext-link-type="uri">https://archive.ics.uci.edu/ml/index.php</ext-link>.</p>
      </sec>
    </sec>
    <sec id="sam11573-sec-0017">
      <label>2.4</label>
      <title>Evaluations using simulated data</title>
      <p>Data sets were simulated for a variety of cases. We used 9 and 49 continuous features and 2, 5, and 10 clusters. A multivariate normal distribution was assumed with means equal to (0.5, −0.5) for two clusters, (0.5, −0.5, 1, −1, 0) for 5 clusters and (0.5, −0.5, 1, −1, 0, 2, −2, 3, −3,5) for 10 clusters. For two clusters, the sample size was 200, for 5 clusters it was 500, and for 10 clusters it was 1000, with 100 units in each cluster. Random vectors were simultaneously generated with the specified marginal means, and the between‐feature correlations were randomly generated from partial correlation. These are derived from specified eigenvalues of the covariance matrices with lower bounds set equal to one. [<xref rid="sam11573-bib-0010" ref-type="bibr">10</xref>, <xref rid="sam11573-bib-0020" ref-type="bibr">20</xref>, <xref rid="sam11573-bib-0024" ref-type="bibr">24</xref>]. A second set of data were produced using the same procedures, but in this simulation, an independent three‐level categorical feature was added to the list of features. The simulations were performed using the r program NORTA [<xref rid="sam11573-bib-0009" ref-type="bibr">9</xref>] (<ext-link xlink:href="https://rdrr.io/github/superdesolator/NORTARA/" ext-link-type="uri">https://rdrr.io/github/superdesolator/NORTARA/</ext-link>).</p>
    </sec>
    <sec id="sam11573-sec-0018">
      <label>2.5</label>
      <title>Example base <styled-content style="fixed-case" toggle="no">DDD</styled-content> ensemble algorithms</title>
      <p>Two base DDD ensemble algorithms and their corresponding GIC algorithms were chosen to illustrate the method. PAM was used to obtain cluster results for RFC and ERT, and their GIC counterparts were labeled IRFC and IERT. It is not our purpose to contrast the base DDD methods but to investigate the degree of improvement in the proximity matrices and the resulting improvement in clusters that are obtained using the iteration process. The number of trees for RFC and IRFC runs was set to 1000, and for ERT and IERT runs it was set to 10,000. All other tunable parameters were set to their default values; the number of features randomly selected at each node for all four methods is the square root of the total number of features. The max depth for RFC and IRFC is reached when all leaves are pure or when all leaves have less than 2 units. The max depth for ERT and IERT is reached when the number of units in a node is one‐third of the number of units in the sample. RFC and IRFC used a bootstrap sample. ERT and IERT used all subjects at each iteration. Since results produced by clustering algorithms are affected by initial values and the random seed, each of the four approaches was run 500 times for the iris and heart disease data. The results shown for these two data sets in Table <xref rid="sam11573-tbl-0001" ref-type="table">1</xref> are the average of these runs and their standard deviations. These taught us how stable are the averages. As a consequence, for the remaining data sets shown in the table, the number of runs for ERT and ERTI was reduced from 500 to 10 based on the extensive computation time required for each run and the standard deviations of the Silhouette scores and the Jaccard Indices used to appraise the GIC algorithm. For the iris and heart disease data, there were little to be gained by additional replication.</p>
      <table-wrap position="float" id="sam11573-tbl-0001" content-type="TABLE">
        <label>TABLE 1</label>
        <caption>
          <p>Silhouette score and Jaccard index for RFC, IRFC, ERT and IERT clustering methods for real‐world data sets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1"/>
              <th colspan="4" align="left" valign="bottom" rowspan="1">Silhouette score</th>
              <th colspan="4" align="left" valign="bottom" rowspan="1">Jaccard Index</th>
            </tr>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Dataset<xref rid="sam11573-note-0001" ref-type="table-fn"><sup>a</sup></xref>
</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">RFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IRFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">ERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">RFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IRFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">ERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IERT</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td colspan="9" align="left" valign="top" rowspan="1">Mean value and standard deviation</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Iris</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.169</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.834</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.023</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.437</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.648</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.706</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.287</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.517</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(150, 4, 3)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.004)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.014)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.003)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.041)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.031)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.025)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.194)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.235)</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Heart disease</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.022</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.407</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.009</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.192</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.474</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.414</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.177</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.179</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(270, 13, 2)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.002)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.068)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.001)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.053)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.037)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.026)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.057)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">(0.063)</td>
            </tr>
            <tr>
              <td colspan="9" align="left" valign="top" rowspan="1">Mean value</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Wisconsin</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.109</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.761</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.505</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.604</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.666</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.796</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.942</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.894</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(699, 9, 2)</td>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Lung</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.055</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.301</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.094</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.274</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.268</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.261</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.095</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.182</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(32, 56, 3)</td>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Breast tissue</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.224</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.709</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.282</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.409</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.331</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.355</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.427</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.431</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(106, 9, 6)</td>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Isolet</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.004</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.187</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.063</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.244</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.156</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.192</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.016</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.039</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(1559, 617, 26)</td>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Parkinson</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.254</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.814</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.254</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.447</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.451</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.446</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.168</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.189</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(768, 8, 2)</td>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Ionosphere</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.122</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.594</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.298</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.496</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.440</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.404</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.513</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.519</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(351, 34, 2)</td>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Segmentation</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.245</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.603</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.295</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.528</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.405</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.386</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.179</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.137</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">(2310, 19, 7)</td>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
              <td align="left" valign="top" rowspan="1" colspan="1"/>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="sam11573-ntgp-0001">
          <fn id="sam11573-note-0001">
            <label>
              <sup>a</sup>
            </label>
            <p>Size of sample, number of features, number of labels.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>There are many implementations of the PAM clustering algorithm. We used the one in the “cluster” package in R [<xref rid="sam11573-bib-0030" ref-type="bibr">30</xref>]. For ERT, we used the Python packages numpy, pandas, and sklearn, and made modifications so that the proximity matrix was able to be accessed.</p>
    </sec>
    <sec id="sam11573-sec-0019">
      <label>2.6</label>
      <title>Indices for appraising the clustering algorithms</title>
      <p>There are many indices in the literature for appraising how good are the result of applying a clustering algorithm. In this section, we describe the two indices we used. Let <italic toggle="yes">C</italic> = {<italic toggle="yes">C</italic>
<sub>1</sub>, <italic toggle="yes">C</italic>
<sub>2</sub>,…, <italic toggle="yes">C</italic>
<sub><italic toggle="yes">m</italic></sub>} be the set of clusters obtained by applying a clustering method where m is the number of clusters. Denote by <italic toggle="yes">n</italic>
<sub><italic toggle="yes">i</italic></sub> the number of units in cluster <italic toggle="yes">C</italic>
<sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">i</italic> = 1, 2,…, <italic toggle="yes">m</italic>. Then <mml:math id="jats-math-1" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math> is the number of units in the entire sample.</p>
      <sec id="sam11573-sec-0020">
        <label>2.6.1</label>
        <title>The Silhouette score</title>
        <p>The Silhouette score [<xref rid="sam11573-bib-0032" ref-type="bibr">32</xref>] is a measure that indicates how close unit i is to members of its own cluster compared with how close it is to units of its nearest neighbor cluster. Suppose there are m clusters. For any data point i in <italic toggle="yes">C</italic>
<sub><italic toggle="yes">i</italic></sub>, let <mml:math id="jats-math-2" display="inline" overflow="scroll"><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math> be the average distance between i and every other point in the same cluster, where <italic toggle="yes">d</italic>(<italic toggle="yes">i</italic>, <italic toggle="yes">j</italic>) is the distance between data point i and data point j. Let <mml:math id="jats-math-3" display="inline" overflow="scroll"><mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="italic">min</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∣</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math> be the smallest average distance between i and all of the data points in each of the other clusters. <italic toggle="yes">b</italic>(<italic toggle="yes">i</italic>) is the average distance between i and members of the nearest cluster. Then the Silhouette score for unit i is <mml:math id="jats-math-4" display="inline" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="italic">max</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math>. A Silhouette score takes values between −1 and 1, and as the value increases, the nearer the unit is to other units in its own cluster and the further it is from units in its nearest neighbor cluster.</p>
      </sec>
      <sec id="sam11573-sec-0021">
        <label>2.6.2</label>
        <title>The Jaccard index</title>
        <p>The Jaccard index [<xref rid="sam11573-bib-0040" ref-type="bibr">40</xref>], sometimes called the Jaccard similarity coefficient, is a measure of the similarity of two partitions, <italic toggle="yes">P</italic>
<sub>1</sub> and <italic toggle="yes">P</italic>
<sub>2</sub> in terms of the proportion of units that are in both partitions. Its formal definition is the number of units in the intersection of <italic toggle="yes">P</italic>
<sub>1</sub> and <italic toggle="yes">P</italic>
<sub>2</sub> divided by the number of units in the union of <italic toggle="yes">P</italic>
<sub>1</sub> and <italic toggle="yes">P</italic>
<sub>2</sub>. Here we will use it to compare a partition based on the true labels to a partition based on a clustering algorithm of the same data set. The Jaccard index is defined to be <mml:math id="jats-math-5" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∣</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math>. A larger value of the index indicates greater similarity between <italic toggle="yes">P</italic>
<sub>1</sub> and <italic toggle="yes">P</italic>
<sub>2</sub>.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sam11573-sec-0022">
    <label>3</label>
    <title>RESULTS</title>
    <sec id="sam11573-sec-0023">
      <label>3.1</label>
      <title>Evaluations using real‐world data</title>
      <p>Table <xref rid="sam11573-tbl-0001" ref-type="table">1</xref> displays the Silhouette scores and the Jaccard indices for the RFC, ERT, and their counterpart, IRFC and IERT iterative clustering method for nine real‐world data sets. For the iris data, the Silhouette score for the IRFC was the highest by far, improving from 0.17 for RFC to a remarkable 0.83. Starting at 0.23, the Silhouette score for ERT also had a very large improvement to 0.44. Figure <xref rid="sam11573-fig-0001" ref-type="fig">1</xref> is a plot in Cartesian coordinates of petal length versus petal width, the top two features found in the RFC and IRFC list of important variables in the RF. It can be seen that for RFC, many virginica (green) dots were labeled setosa (red), while for ERT, many versicolor (blue) dots were labeled virginica (green). The iterations tended to correct these erroneous labels.</p>
      <fig position="float" fig-type="FIGURE" id="sam11573-fig-0001">
        <label>FIGURE 1</label>
        <caption>
          <p>Scatterplots of petal length versus petal width features for the iris data for ground truth and 4 clustering methods. Ground truth clusters are setosa, versicolor and virginica shown in the upper left plot</p>
        </caption>
        <graphic xlink:href="SAM-15-433-g005" position="anchor" id="jats-graphic-1">
          <alt-text>SAM-11573-FIG-0001-c</alt-text>
        </graphic>
      </fig>
      <p>For the heart disease data set, the IRFC improvement over the RFC (0.41 compared with 0.2) as measured by the Silhouette score, just as for the iris data, was remarkable. The same was true but to a lesser extent for ERT (0.01) compared with IERT (0.19). The Jaccard index of the base classifier and the corresponding GIC algorithm were quite similar for both methods. A scatterplot of maximum heart rate achieved and exercise induced ST depression, two of the top three features found in the RF of the RFC and IRFC list of important variables, is displayed in Figure <xref rid="sam11573-fig-0002" ref-type="fig">2</xref>. It can be seen that there was an excess of normal controls with the base RFC, which was corrected by the IRFC method. The ratio of heart disease subjects to normal controls was close to the ground truth using IRFC. As for ERT and IERT, the ratio of heart disease subjects to normal controls is similar to that of IRFC.</p>
      <fig position="float" fig-type="FIGURE" id="sam11573-fig-0002">
        <label>FIGURE 2</label>
        <caption>
          <p>Scatterplots of maximum heart rate achieved versus exercise‐induced ST depression for the heart disease data, for ground truth and 4 clustering methods</p>
        </caption>
        <graphic xlink:href="SAM-15-433-g002" position="anchor" id="jats-graphic-3">
          <alt-text>SAM-11573-FIG-0002-c</alt-text>
        </graphic>
      </fig>
      <p>Looking at all of the data sets as a whole, the Silhouette scores appear to be relatively low for both base approaches. RFC and ERT were relatively close to each other, with a large difference only once, for the Wisconsin data. The GIC improved the Silhouette scores for every data set for both RFC and ERT. The IRFC was larger in eight of the nine data sets and substantially larger four times. The overall effect of the GIC on the Jaccard index was small with one exception: the iris data changed from 0.29 for ERT to 0.52 for IERT.</p>
    </sec>
    <sec id="sam11573-sec-0024">
      <label>3.2</label>
      <title>Evaluation using simulated data sets</title>
      <p>Table <xref rid="sam11573-tbl-0002" ref-type="table">2</xref> displays the Silhouette scores and the Jaccard indices for the RFC, ERT, and their counterparts, IRFC and IERT iterative clustering methods for simulations with 9 and 49 continuous features. Table <xref rid="sam11573-tbl-0003" ref-type="table">3</xref> shows the same information for simulations with 9 and 49 continuous features and 1 independent categorical feature. In every case, the base method produced clusters with very poor Silhouette scores. For both methods, the GIC produced substantial improvements; the largest increment, as for the real‐world data, accrued to RFC. The Jaccard indices, as for the real‐world data, did not have a consistent pattern of change, which in any case, was small.</p>
      <table-wrap position="float" id="sam11573-tbl-0002" content-type="TABLE">
        <label>TABLE 2</label>
        <caption>
          <p>Silhouette score and Jaccard index for RFC, IRFC, ERT and IERT clustering methods in simulated data with 9 and 49 continuous features</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1"/>
              <th colspan="4" align="left" valign="bottom" rowspan="1">Silhouette score</th>
              <th colspan="4" align="left" valign="bottom" rowspan="1">Jaccard index</th>
            </tr>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Number of clusters</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">RFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IRFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">ERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">RFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IRFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">ERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IERT</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td colspan="9" align="left" valign="top" rowspan="1">9 continuous features</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">2</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.016</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.539</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.110</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.182</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.376</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.366</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.493</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.487</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.008</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.356</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.073</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.113</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.143</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.134</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.081</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.068</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">10</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.007</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.323</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.074</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.105</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.165</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.132</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.033</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.037</td>
            </tr>
            <tr>
              <td colspan="9" align="left" valign="top" rowspan="1">49 continuous features</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">2</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.005</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.337</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.032</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.107</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.372</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.374</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.575</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.688</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.003</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.141</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.011</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.048</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.196</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.160</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.111</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.096</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">10</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.018</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.105</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.039</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.097</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.325</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.211</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.036</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.028</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap position="float" id="sam11573-tbl-0003" content-type="TABLE">
        <label>TABLE 3</label>
        <caption>
          <p>Silhouette score and Jaccard index for RFC, IRFC, ERT and IERT clustering methods in simulated data with 9 and 49 continuous features and one categorical feature</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1"/>
              <th colspan="4" align="left" valign="bottom" rowspan="1">Silhouette score</th>
              <th colspan="4" align="left" valign="bottom" rowspan="1">Jaccard index</th>
            </tr>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Number of clusters</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">RFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IRFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">ERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">RFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IRFC</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">ERT</th>
              <th align="char" valign="bottom" rowspan="1" colspan="1">IERT</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td colspan="9" align="left" valign="top" rowspan="1">9 continuous features +1 categorical feature</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">2</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.021</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.500</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.143</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.339</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.385</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.360</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.347</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.370</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.006</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.314</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.109</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.231</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.135</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.128</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.063</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.045</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">10</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.007</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.201</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.177</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.224</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.137</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.103</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.048</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.025</td>
            </tr>
            <tr>
              <td colspan="9" align="left" valign="top" rowspan="1">49 continuous features +1 categorical feature</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">2</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.009</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.291</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.040</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.217</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.543</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.411</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.594</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.384</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">5</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.002</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.136</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.017</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.135</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.220</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.169</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.065</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.272</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">10</td>
              <td align="left" valign="top" rowspan="1" colspan="1">−0.011</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.100</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.079</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.187</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.168</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.219</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.027</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.042</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sam11573-sec-0025">
      <label>3.3</label>
      <title>Convergence of the <styled-content style="fixed-case" toggle="no">GIC</styled-content>
</title>
      <p>It is natural to ask about rate of convergence of the GIC algorithms. Convergence occurs for PAM at an iteration where the medoids and cluster labels for each unit are the same as in the previous iteration. Tables <xref rid="sam11573-tbl-0004" ref-type="table">4</xref> and <xref rid="sam11573-tbl-0005" ref-type="table">5</xref> display the mean value at each iteration and the incremental change from one iteration to the next of the average between units distance respectively for the iris and heart disease data. This is the average of the entries in the proximity matrix. In the iris data, it can be seen that the absolute value of the differences is monotonically decreasing and close to zero by the seventh iteration for the IRFC and almost immediately for the IERT. For the heart disease data, the same pattern of monotonic decrease in the absolute value of the difference also converges to zero.</p>
    </sec>
    <sec id="sam11573-sec-0026">
      <label>3.4</label>
      <title>Choice of number of clusters</title>
      <p>In most applications, the number of clusters is unknown and a common issue is choosing a value to use in the clustering algorithm. Unfortunately, there is no completely satisfactory answer, and as a result, a variety of heuristics have evolved. Clearly, it is desirable to minimize distances between units in a cluster and maximize distances between units in different clusters. A common strategy is to create an objective function that balances the compactness and separation goals, and to choose the number of clusters that provides the maximum over a range of reasonable candidates. We used the Silhouette score as the function and found the maximum over a range of clusters from 2 to 11 in the iris data set. Of particular interest is the relationship between the value of the underlying DDD‐based RF clustering method and its corresponding GIC. We found the maximum for the RFC occurred at six clusters with a Silhouette score of 0.178 and the maximum for the IRFC occurred at 2 clusters with a Silhouette score of 0.966. The Silhouette scores over the range are shown in Figure <xref rid="sam11573-fig-0003" ref-type="fig">3</xref>). Notice that the scale of the ordinates are different in the two plots because of the sizeable difference in the ranges. The ground truth has three clusters with a Silhouette score of 0.76. Figure <xref rid="sam11573-fig-0004" ref-type="fig">4</xref> shows scatter plots of petal width versus petal length using 6 and 2 as input for the number of clusters for the RFC and IRFC respectively. The plot for the ground truth with 3 clusters is shown in the upper left corner of Figure <xref rid="sam11573-fig-0001" ref-type="fig">1</xref>. From the plots, it appears that all three of the candidate number of clusters are visually plausible. If the choice were based entirely on the properties of the Silhouette score, we should use the proximity measures of the IRFC with 2 clusters. It is the subject matter expertise of the botanist that is required to declare that 3 is the true number of clusters.</p>
      <fig position="float" fig-type="FIGURE" id="sam11573-fig-0003">
        <label>FIGURE 3</label>
        <caption>
          <p>Silhouette scores over the range possible cluster numbers for the iris data. Note that the range of the ordinates in the two plots are not the same. The maximum for RFC is 6 and for IRFC is 2</p>
        </caption>
        <graphic xlink:href="SAM-15-433-g001" position="anchor" id="jats-graphic-5">
          <alt-text>SAM-11573-FIG-0003-b</alt-text>
        </graphic>
      </fig>
      <fig position="float" fig-type="FIGURE" id="sam11573-fig-0004">
        <label>FIGURE 4</label>
        <caption>
          <p>Scatterplots of petal length versus petal width for the iris data for the number of clusters that maximized the Silhouette scores, 6 for RFC and 2 for IRFC. Ground truth with 3 clusters are shown in the upper left plot in Figure <xref rid="sam11573-fig-0001" ref-type="fig">1</xref>
</p>
        </caption>
        <graphic xlink:href="SAM-15-433-g003" position="anchor" id="jats-graphic-7">
          <alt-text>SAM-11573-FIG-0004-c</alt-text>
        </graphic>
      </fig>
    </sec>
    <sec id="sam11573-sec-0027">
      <label>3.5</label>
      <title>Choice of initialization labels</title>
      <p>To start the iteration process, unit labels are required to grow the forest. We used the iris data assuming three clusters with three different initialization approaches followed by IRFC to iterate to the final clusters. In the first, synthetic feature data were randomly produced from a reference distribution obtained by sampling from the product of empirical marginal distributions of the sample data [<xref rid="sam11573-bib-0007" ref-type="bibr">7</xref>]. The second method was “purposeful clustering” [<xref rid="sam11573-bib-0035" ref-type="bibr">35</xref>], which used the ground truth as the initial labels. The third method was <italic toggle="yes">AddCl3</italic> [<xref rid="sam11573-bib-0011" ref-type="bibr">11</xref>], which is just a random assignment of labels. Table <xref rid="sam11573-tbl-0006" ref-type="table">6</xref> displays the number of flowers in each cluster and the Silhouette scores for each of the three label initialization approaches and the ground truth. Figure <xref rid="sam11573-fig-0005" ref-type="fig">5</xref> displays the scatter plots of petal width versus petal length for these approaches in Cartesian coordinates. From the plots, it appears that all but <italic toggle="yes">AddCl3</italic> are plausible. This strategy had difficulty distinguishing members of the sartosa and versicolor clusters. The other two approaches were in full agreement with the ground truth identifying the same 50 sartosa irises. Not surprisingly, the purposeful label assignment produced the best Silhouette scores and the random assignment produced the worst. Although the difference in the Silhouette scores is small, it is visual inspection that informs the analyst that <italic toggle="yes">AddCl3</italic> produces an unsatisfactory clustering. These examples suggest that the data analyst should carefully consider the method to use to initialize to labels. The more known information about the ultimate clusters that can be used, the better.</p>
      <fig position="float" fig-type="FIGURE" id="sam11573-fig-0005">
        <label>FIGURE 5</label>
        <caption>
          <p>Scatterplots of petal length versus petal width for the iris data for different initial labeling strategies all assuming 3 clusters. Ground truth clusters are shown in the upper left</p>
        </caption>
        <graphic xlink:href="SAM-15-433-g004" position="anchor" id="jats-graphic-9">
          <alt-text>SAM-11573-FIG-0005-c</alt-text>
        </graphic>
      </fig>
    </sec>
  </sec>
  <sec id="sam11573-sec-0028">
    <label>4</label>
    <title>DISCUSSION</title>
    <p>The real‐world and the simulation examples demonstrate that, at least for these data sets, the new GIC algorithm produces clusters that have superior properties compared with the base method, as measured by substantially higher Silhouette scores. The Jaccard index values from the GIC algorithm were about the same as those from the base method. Though the pattern was replicated for all of the examples we considered, we provide no proof that this will be the case for every data set. Nevertheless, the evidence suggests that an analyst will likely obtain better results in a clustering application by using the GIC algorithm. There are many modifications of RF that have been proposed for estimating similarities. We believe that except for purely random forests that make no use of labels, all of them can be improved by iterating as we have described at the appropriate point in the procedure. How large the improvement will be depends on the base procedure. In the data sets we examined, the degree of improvement in the Silhouette score for RFC was usually considerably greater than for ERT.</p>
    <p>If the distance between every pair of units converges, it follows that the PAM results will converge too. In Tables <xref rid="sam11573-tbl-0004" ref-type="table">4</xref> and <xref rid="sam11573-tbl-0005" ref-type="table">5</xref>, we see that the absolute value of the mean difference between successive iterations decreases monotonically. A monotonically decreasing series of positive values bounded below must converge. We have run the GIC algorithm on many simulated data sets for RFC and ERT in addition to those reported here and all have converged monotonically in absolute value by the eight iteration or so. But it is possible that the algorithm is trapped in a local minimum. Notice that the actual change in the mean pairwise dissimilarity is relatively small, 0.021, 0.007, 0.03, and 0.009, as seen in Tables <xref rid="sam11573-tbl-0004" ref-type="table">4</xref> and <xref rid="sam11573-tbl-0005" ref-type="table">5</xref>. This is likely why the Jaccard index does not change much.</p>
    <table-wrap position="float" id="sam11573-tbl-0004" content-type="TABLE">
      <label>TABLE 4</label>
      <caption>
        <p>Mean pairwise proximity and change in mean pairwise proximity and standard deviation over iterations of IRFC and IERT for the iris data</p>
      </caption>
      <table frame="hsides" rules="groups">
        <col align="left" span="1"/>
        <col align="left" char="(" span="1"/>
        <col align="left" char="(" span="1"/>
        <col align="left" char="(" span="1"/>
        <col align="left" char="(" span="1"/>
        <thead valign="bottom">
          <tr style="border-bottom:solid 1px #000000">
            <th align="left" valign="bottom" rowspan="1" colspan="1"/>
            <th colspan="2" align="left" valign="bottom" rowspan="1">IRFC</th>
            <th colspan="2" align="left" valign="bottom" rowspan="1">IERT</th>
          </tr>
          <tr style="border-bottom:solid 1px #000000">
            <th align="left" valign="bottom" rowspan="1" colspan="1">Iteration</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Mean pairwise distance</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Mean iteration change in pairwise distance</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Mean pairwise distance</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Mean iteration change in pairwise distance</th>
          </tr>
        </thead>
        <tbody valign="top">
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">1</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.717 (0.39)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">‐</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.658 (0.004)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">‐</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">2</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.702 (0.40)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.015 (0.08)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.651 (&lt;0.001)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.007 (0.004)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">3</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.694 (0.41)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.008 (0.05)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.651 (&lt;0.001)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (&lt;0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">4</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.702 (0.70)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.007 (0.04)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.651 (&lt;0.001)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (&lt;0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">5</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.696 (0.41)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.006 (0.04)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.651 (&lt;0.001)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (&lt;0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">6</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.698 (0.41)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.002 (0.04)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.651 (&lt;0.001)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (&lt;0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">7</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.696 (0.41)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.002 (0.04)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.651 (&lt;0.001)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (&lt;0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">8</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.696 (0.41)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (0.04)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.651 (&lt;0.001)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (&lt;0.001)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap position="float" id="sam11573-tbl-0005" content-type="TABLE">
      <label>TABLE 5</label>
      <caption>
        <p>Mean pairwise proximity and change in mean pairwise proximity and standard deviation over iterations of IRFC and IERT for the heart disease data</p>
      </caption>
      <table frame="hsides" rules="groups">
        <col align="left" span="1"/>
        <col align="left" char="(" span="1"/>
        <col align="left" char="(" span="1"/>
        <col align="left" char="(" span="1"/>
        <col align="left" char="(" span="1"/>
        <thead valign="bottom">
          <tr style="border-bottom:solid 1px #000000">
            <th align="left" valign="bottom" rowspan="1" colspan="1"/>
            <th colspan="2" align="left" valign="bottom" rowspan="1">IRFC</th>
            <th colspan="2" align="left" valign="bottom" rowspan="1">IERT</th>
          </tr>
          <tr style="border-bottom:solid 1px #000000">
            <th align="left" valign="bottom" rowspan="1" colspan="1">Iteration</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Mean pairwise distance</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Mean iteration change in pairwise proximity</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Mean pairwise proximity</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Mean iteration change in pairwise proximity</th>
          </tr>
        </thead>
        <tbody valign="top">
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">1</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.80 (0.22)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">‐</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.64(0.01)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">‐</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">2</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.87 (0.17)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.07(0.11)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.62 (0.01)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.02 (0.01)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">3</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.86 (0.18)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.01 (0.09)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.62 (0.01)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.004 (0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">4</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.84 (0.20)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.02 (0.08)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.62 (0.004)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.003 (0.004)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">5</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.82 (0.22)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.02 (0.08)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.63 (0.004)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.001 (0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">6</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.78 (0.24)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.04 (0.08)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.63 (0.004)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.001 (0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">7</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.77 (0.24)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.01 (0.08)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.63 (0.004)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (0.001)</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">8</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.77 (0.24)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">−0.01 (0.07)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.63 (0.004)</td>
            <td align="left" valign="top" rowspan="1" colspan="1">&lt;0.001 (0.001)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <table-wrap position="float" id="sam11573-tbl-0006" content-type="TABLE">
      <label>TABLE 6</label>
      <caption>
        <p>Number of units in clusters and the Silhouette score for IRFC for different initial labels for the iris data</p>
      </caption>
      <table frame="hsides" rules="groups">
        <col align="left" span="1"/>
        <col align="left" span="1"/>
        <col align="left" span="1"/>
        <col align="left" span="1"/>
        <col align="left" span="1"/>
        <thead valign="bottom">
          <tr style="border-bottom:solid 1px #000000">
            <th align="left" valign="bottom" rowspan="1" colspan="1">Initial label method</th>
            <th align="left" valign="bottom" rowspan="1" colspan="1">Setosa</th>
            <th align="left" valign="bottom" rowspan="1" colspan="1">Versicolor</th>
            <th align="left" valign="bottom" rowspan="1" colspan="1">Virginica</th>
            <th align="char" valign="bottom" rowspan="1" colspan="1">Silhouette score</th>
          </tr>
        </thead>
        <tbody valign="top">
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Ground truth</td>
            <td align="left" valign="top" rowspan="1" colspan="1">50</td>
            <td align="left" valign="top" rowspan="1" colspan="1">50</td>
            <td align="left" valign="top" rowspan="1" colspan="1">50</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.759</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Breiman and Cutler</td>
            <td align="left" valign="top" rowspan="1" colspan="1">50</td>
            <td align="left" valign="top" rowspan="1" colspan="1">66</td>
            <td align="left" valign="top" rowspan="1" colspan="1">34</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.834</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">Purposeful clustering</td>
            <td align="left" valign="top" rowspan="1" colspan="1">50</td>
            <td align="left" valign="top" rowspan="1" colspan="1">54</td>
            <td align="left" valign="top" rowspan="1" colspan="1">46</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.861</td>
          </tr>
          <tr>
            <td align="left" valign="top" rowspan="1" colspan="1">
              <italic toggle="yes">AddCl3</italic>
            </td>
            <td align="left" valign="top" rowspan="1" colspan="1">34</td>
            <td align="left" valign="top" rowspan="1" colspan="1">74</td>
            <td align="left" valign="top" rowspan="1" colspan="1">42</td>
            <td align="left" valign="top" rowspan="1" colspan="1">0.753</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>In many applications, it is necessary to have a way to place a new unit into one of the discovered clusters. A new unit may be classified by running its feature vector down the final forest in the iteration. The proximities between the new unit and the medoids are equal to the fraction of terminal nodes they reside in together, or whatever way proximity is measured. The unit is assigned to the class corresponding to the largest of these proximities.</p>
    <p>RF and its many versions are efficient algorithms with considerable capability for handling high‐dimensional data. The iteration method provides an improvement in the generation of similarities and the clusters they produce. There are many properties of the GIC algorithm still to be learned for different data types. The GIC package has been released in R: <ext-link xlink:href="https://cran.r-project.org/web/packages/GIC/index.html" ext-link-type="uri">https://cran.r‐project.org/web/packages/GIC/index.html</ext-link>.</p>
  </sec>
  <sec sec-type="COI-statement" id="sam11573-sec-0030">
    <title>CONFLICT OF INTEREST</title>
    <p>The authors have no conflicts to disclose.</p>
  </sec>
</body>
<back>
  <ack id="sam11573-sec-0029">
    <title>ACKNOWLEDGMENTS</title>
    <p>The authors would like to thank Dr. Charles Marmar for the many stimulating discussions about clustering and classification in mental health applications.</p>
  </ack>
  <sec sec-type="data-availability" id="sam11573-sec-0032">
    <title>DATA AVAILABILITY STATEMENT</title>
    <p>The data that support the findings of this study are openly available in <ext-link xlink:href="https://archive.ics.uci.edu/ml/index.php" ext-link-type="uri" specific-use="software is-supplemented-by">https://archive.ics.uci.edu/ml/index.php</ext-link>.</p>
  </sec>
  <ref-list id="sam11573-bibl-0001" content-type="cited-references">
    <title>REFERENCES</title>
    <ref id="sam11573-bib-0001">
      <label>1</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0001"><string-name><given-names>L.</given-names><surname>Alhusain</surname></string-name> and <string-name><given-names>A. M.</given-names><surname>Hafez</surname></string-name>, <article-title>Cluster ensemble based on random forests for genetic data</article-title>, <source>BioData Min.</source>
<volume>10</volume> (<year>2017</year>), no. <issue>1</issue>, <fpage>1</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">28127402</pub-id></mixed-citation>
    </ref>
    <ref id="sam11573-bib-0002">
      <label>2</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0002"><string-name><given-names>Y.</given-names><surname>Amit</surname></string-name> and <string-name><given-names>D.</given-names><surname>Geman</surname></string-name>, <source>Randomized inquiries about shape: An application to handwritten digit recognition</source>, <publisher-name>Chicago Univ IL Dept of Statistics</publisher-name>, <year>1994</year>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0003">
      <label>3</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0003"><string-name><given-names>S.</given-names><surname>Aryal</surname></string-name>, <string-name><given-names>K. M.</given-names><surname>Ting</surname></string-name>, <string-name><given-names>T.</given-names><surname>Washio</surname></string-name>, and <string-name><given-names>G.</given-names><surname>Haffari</surname></string-name>, <article-title>A comparative study of data‐dependent approaches without learning in measuring similarities of data objects</article-title>, <source>Data Min. Knowl. Disc.</source><volume>34</volume> (<year>2020</year>), no. <issue>1</issue>, <fpage>124</fpage>–<lpage>162</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0004">
      <label>4</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0004"><string-name><given-names>M.</given-names><surname>Bicego</surname></string-name>, “<part-title>K‐random forests: A k‐means style algorithm for random forest clustering</part-title>,” <source>2019 International Joint Conference on Neural Networks (IJCNN)</source>, <publisher-name>IEEE</publisher-name>, <year>2019</year>, pp. <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0005">
      <label>5</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0005"><string-name><given-names>M.</given-names><surname>Bicego</surname></string-name> and <string-name><given-names>F.</given-names><surname>Escolano</surname></string-name>, “<part-title>On learning random forests for random forest‐clustering</part-title>,” <source>2020 25th International Conference on Pattern Recognition (ICPR)</source>, <publisher-name>IEEE</publisher-name>, <year>2021</year>, pp. <fpage>3451</fpage>–<lpage>3458</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0006">
      <label>6</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0006"><string-name><given-names>L.</given-names><surname>Breiman</surname></string-name>, <article-title>Random forests</article-title>, <source>Mach. Learn.</source><volume>45</volume> (<year>2001</year>), no. <issue>1</issue>, <fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0007">
      <label>7</label>
      <mixed-citation publication-type="miscellaneous" id="sam11573-cit-0007"><string-name><given-names>L.</given-names><surname>Breiman</surname></string-name> and <string-name><given-names>A.</given-names><surname>Cutler</surname></string-name>, <article-title><italic toggle="yes">RFtools‐for predicting and understanding data</italic>, Interface'04 Workshop, 2004</article-title>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0008">
      <label>8</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0008"><string-name><given-names>L.</given-names><surname>Breiman</surname></string-name>, <string-name><given-names>J. H.</given-names><surname>Friedman</surname></string-name>, <string-name><given-names>R. A.</given-names><surname>Olshen</surname></string-name>, and <string-name><given-names>C. J.</given-names><surname>Stone</surname></string-name>, <source>Classification and regression trees</source>, <publisher-name>Routledge</publisher-name>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0009">
      <label>9</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0009"><string-name><given-names>H.</given-names><surname>Chen</surname></string-name>, <article-title>Initialization for NORTA: Generation of random vectors with specified marginals and correlations</article-title>, <source>INFORMS J. Comput.</source><volume>13</volume> (<year>2001</year>), no. <issue>4</issue>, <fpage>312</fpage>–<lpage>331</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0010">
      <label>10</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0010"><string-name><given-names>A.</given-names><surname>Cutler</surname></string-name> and <string-name><given-names>G.</given-names><surname>Zhao</surname></string-name>, <article-title>Pert‐perfect random tree ensembles</article-title>, <source>Comput. Sci. Stat.</source>
<volume>33</volume> (<year>2001</year>), <fpage>490</fpage>–<lpage>497</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0011">
      <label>11</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0011"><string-name><given-names>K.</given-names><surname>Dalleau</surname></string-name>, <string-name><given-names>M.</given-names><surname>Couceiro</surname></string-name>, and <string-name><given-names>M.</given-names><surname>Smaïl‐Tabbone</surname></string-name>, “<part-title>Unsupervised extremely randomized trees</part-title>,” <source>Pacific‐Asia Conference on Knowledge Discovery and Data Mining</source>, <publisher-name>Springer</publisher-name>, <year>2018</year>, pp. <fpage>478</fpage>–<lpage>489</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0012">
      <label>12</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0012"><string-name><given-names>R.</given-names><surname>Detrano</surname></string-name>, <string-name><given-names>A.</given-names><surname>Janosi</surname></string-name>, <string-name><given-names>W.</given-names><surname>Steinbrunn</surname></string-name>, <string-name><given-names>M.</given-names><surname>Pfisterer</surname></string-name>, <string-name><given-names>J.‐J.</given-names><surname>Schmid</surname></string-name>, <string-name><given-names>S.</given-names><surname>Sandhu</surname></string-name>, <string-name><given-names>K. H.</given-names><surname>Guppy</surname></string-name>, <string-name><given-names>S.</given-names><surname>Lee</surname></string-name>, and <string-name><given-names>V.</given-names><surname>Froelicher</surname></string-name>, <article-title>International application of a new probability algorithm for the diagnosis of coronary artery disease</article-title>, <source>Am. J. Cardiol.</source><volume>64</volume> (<year>1989</year>), no. <issue>5</issue>, <fpage>304</fpage>–<lpage>310</lpage>.<pub-id pub-id-type="pmid">2756873</pub-id></mixed-citation>
    </ref>
    <ref id="sam11573-bib-0013">
      <label>13</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0013"><string-name><given-names>T. G.</given-names><surname>Dietterich</surname></string-name>, <article-title>An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization</article-title>, <source>Mach. Learn.</source><volume>40</volume> (<year>2000</year>), no. <issue>2</issue>, <fpage>139</fpage>–<lpage>157</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0014">
      <label>14</label>
      <mixed-citation publication-type="miscellaneous" id="sam11573-cit-0014"><string-name><given-names>R. P.</given-names><surname>Duin</surname></string-name> and <string-name><given-names>E.</given-names><surname>Pękalska</surname></string-name>, <article-title><italic toggle="yes">The dissimilarity representation for pattern recognition: A tutorial</italic>. Technical Report, 2009</article-title>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0015">
      <label>15</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0015"><string-name><given-names>T. L.</given-names><surname>Fernando</surname></string-name> and <string-name><given-names>G. I.</given-names><surname>Webb</surname></string-name>, <article-title>SimUSF: An efficient and effective similarity measure that is invariant to violations of the interval scale assumption</article-title>, <source>Data Min. Knowl. Disc.</source>
<volume>31</volume> (<year>2017</year>), no. <issue>1</issue>, <fpage>264</fpage>–<lpage>286</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0016">
      <label>16</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0016"><string-name><given-names>R. A.</given-names><surname>Fisher</surname></string-name>, <article-title>The use of multiple measurements in taxonomic problems</article-title>, <source>Ann. Eugenics</source><volume>7</volume> (<year>1936</year>), no. <issue>2</issue>, <fpage>179</fpage>–<lpage>188</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0017">
      <label>17</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0017"><string-name><given-names>P.</given-names><surname>Geurts</surname></string-name>, <string-name><given-names>D.</given-names><surname>Ernst</surname></string-name>, and <string-name><given-names>L.</given-names><surname>Wehenkel</surname></string-name>, <article-title>Extremely randomized trees</article-title>, <source>Mach. Learn.</source><volume>63</volume> (<year>2006</year>), no. <issue>1</issue>, <fpage>3</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0018">
      <label>18</label>
      <mixed-citation publication-type="miscellaneous" id="sam11573-cit-0018"><string-name><given-names>M.</given-names><surname>Goetz</surname></string-name>, <string-name><given-names>C.</given-names><surname>Weber</surname></string-name>, <string-name><given-names>J.</given-names><surname>Bloecher</surname></string-name>, <string-name><given-names>B.</given-names><surname>Stieltjes</surname></string-name>, <string-name><given-names>H.‐P.</given-names><surname>Meinzer</surname></string-name>, and <string-name><given-names>K.</given-names><surname>Maier‐Hein</surname></string-name>, <article-title><italic toggle="yes">Extremely randomized trees based brain tumor segmentation</italic>, Proc. BRATS Challenge‐MICCAI, 2014, pp. 006–011</article-title>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0019">
      <label>19</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0019"><string-name><given-names>T. K.</given-names><surname>Ho</surname></string-name>, “<part-title>Random decision forests</part-title>,” <source>Proceedings of 3rd International Conference on Document Analysis and Recognition</source>, Vol <volume>1</volume>, <publisher-name>IEEE</publisher-name>, <year>1995</year>, pp. <fpage>278</fpage>–<lpage>282</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0020">
      <label>20</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0020"><string-name><given-names>H.</given-names><surname>Joe</surname></string-name>, <article-title>Generating random correlation matrices based on partial correlations</article-title>, <source>J. Multivar. Anal.</source><volume>97</volume> (<year>2006</year>), no. <issue>10</issue>, <fpage>2177</fpage>–<lpage>2189</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0021">
      <label>21</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0021"><string-name><given-names>L.</given-names><surname>Kaufman</surname></string-name> and <string-name><given-names>P. J.</given-names><surname>Rousseeuw</surname></string-name>, <source>Finding groups in data: An introduction to cluster analysis</source>, Vol <volume>344</volume>, <publisher-name>John Wiley &amp; Sons</publisher-name>, <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0022">
      <label>22</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0022"><string-name><given-names>C. L.</given-names><surname>Krumhansl</surname></string-name>, <article-title>Concerning the applicability of geometric models to similarity data: The interrelationship between similarity and spatial density</article-title>, <source>Psychol. Rev.</source><volume>85</volume> (<year>1978</year>), no. <issue>5</issue>, <fpage>445</fpage>–<lpage>463</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0023">
      <label>23</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0023"><string-name><given-names>V.</given-names><surname>Kulkarni</surname></string-name> and <string-name><given-names>P.</given-names><surname>Sinha</surname></string-name>, <article-title>Random forest classifier: A survey and future research directions</article-title>, <source>Int. J. Adv. Comput</source>
<volume>36</volume> (<year>2013</year>), no. <issue>1</issue>, <fpage>1144</fpage>–<lpage>1156</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0024">
      <label>24</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0024"><string-name><given-names>D.</given-names><surname>Kurowicka</surname></string-name> and <string-name><given-names>R. M.</given-names><surname>Cooke</surname></string-name>, <source>Uncertainty analysis with high dimensional dependence modelling</source>, <publisher-name>John Wiley &amp; Sons</publisher-name>, <publisher-loc>Chichester, West Sussex</publisher-loc>, <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0025">
      <label>25</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0025"><string-name><given-names>F. T.</given-names><surname>Liu</surname></string-name>, <string-name><given-names>K. M.</given-names><surname>Ting</surname></string-name>, and <string-name><given-names>Z.‐H.</given-names><surname>Zhou</surname></string-name>, “<part-title>Isolation forest</part-title>,” <source>2008 Eighth IEEE International Conference on Data Mining</source>, <publisher-name>IEEE</publisher-name>, <year>2008</year>, pp. <fpage>413</fpage>–<lpage>422</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0026">
      <label>26</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0026"><string-name><given-names>G. W.</given-names><surname>Milligan</surname></string-name>, <article-title>A Monte Carlo study of thirty internal criterion measures for cluster analysis</article-title>, <source>Psychometrika</source><volume>46</volume> (<year>1981</year>), no. <issue>2</issue>, <fpage>187</fpage>–<lpage>199</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0027">
      <label>27</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0027"><string-name><given-names>F.</given-names><surname>Moosmann</surname></string-name>, <string-name><given-names>B.</given-names><surname>Triggs</surname></string-name>, and <string-name><given-names>F.</given-names><surname>Jurie</surname></string-name>, “<part-title>Fast discriminative visual codebooks using randomized clustering forests</part-title>,” <source>Twentieth Annual Conference on Neural Information Processing Systems (NIPS'06)</source>, <publisher-name>MIT Press</publisher-name>, <year>2006</year>, pp. <fpage>985</fpage>–<lpage>992</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0028">
      <label>28</label>
      <mixed-citation publication-type="miscellaneous" id="sam11573-cit-0028"><string-name><given-names>F.</given-names><surname>Perbet</surname></string-name>, <string-name><given-names>B.</given-names><surname>Stenger</surname></string-name>, and <string-name><given-names>A.</given-names><surname>Maki</surname></string-name>, <article-title><italic toggle="yes">Random forest clustering and application to video segmentation</italic>. BMVC, Citeseer, 2009, pp. 1–10</article-title>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0029">
      <label>29</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0029"><string-name><given-names>A.</given-names><surname>Pinto</surname></string-name>, <string-name><given-names>S.</given-names><surname>Pereira</surname></string-name>, <string-name><given-names>H.</given-names><surname>Correia</surname></string-name>, <string-name><given-names>J.</given-names><surname>Oliveira</surname></string-name>, <string-name><given-names>D. M.</given-names><surname>Rasteiro</surname></string-name>, and <string-name><given-names>C. A.</given-names><surname>Silva</surname></string-name>, “<part-title>Brain tumour segmentation based on extremely randomized forest with high‐level features</part-title>,” <source>2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>, <publisher-name>IEEE</publisher-name>, <year>2015</year>, pp. <fpage>3037</fpage>–<lpage>3040</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0030">
      <label>30</label>
      <mixed-citation publication-type="miscellaneous" id="sam11573-cit-0030"><collab collab-type="authors">R Core Team</collab>
, <italic toggle="yes">R: A language and environment for statistical computing</italic>. <ext-link xlink:href="https://www.r-project.org/" ext-link-type="uri">https://www.R‐project.org/</ext-link>, 2021.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0031">
      <label>31</label>
      <mixed-citation publication-type="miscellaneous" id="sam11573-cit-0031"><string-name><given-names>L.</given-names><surname>Rousseeuw</surname></string-name> and <string-name><given-names>P.</given-names><surname>Kaufman</surname></string-name>, <article-title><italic toggle="yes">Clustering by means of medoids</italic>, Proc. Stat. Data Anal. Based on the L1 Norm Conf., Neuchatel, Switzerland, 1987, pp. 405–416</article-title>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0032">
      <label>32</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0032"><string-name><given-names>P. J.</given-names><surname>Rousseeuw</surname></string-name>, <article-title>Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</article-title>, <source>J. Comput. Appl. Math.</source><volume>20</volume> (<year>1987</year>), <fpage>53</fpage>–<lpage>65</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0033">
      <label>33</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0033"><string-name><given-names>T.</given-names><surname>Shi</surname></string-name> and <string-name><given-names>S.</given-names><surname>Horvath</surname></string-name>, <article-title>Unsupervised learning with random forest predictors</article-title>, <source>J. Comput. Graph. Stat.</source>
<volume>15</volume> (<year>2006</year>), no. <issue>1</issue>, <fpage>118</fpage>–<lpage>138</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0034">
      <label>34</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0034"><string-name><given-names>J.</given-names><surname>Shotton</surname></string-name>, <string-name><given-names>M.</given-names><surname>Johnson</surname></string-name>, and <string-name><given-names>R.</given-names><surname>Cipolla</surname></string-name>, “<part-title>Semantic texton forests for image categorization and segmentation</part-title>,” <source>2008 IEEE Conference on Computer Vision and Pattern Recognition</source>, <publisher-name>IEEE</publisher-name>, <year>2008</year>, pp. <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0035">
      <label>35</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0035"><string-name><given-names>C. E.</given-names><surname>Siegel</surname></string-name>, <string-name><given-names>E. M.</given-names><surname>Laska</surname></string-name>, <string-name><given-names>Z.</given-names><surname>Lin</surname></string-name>, <string-name><given-names>M.</given-names><surname>Xu</surname></string-name>, <string-name><given-names>D.</given-names><surname>Abu‐Amara</surname></string-name>, <string-name><given-names>M. K.</given-names><surname>Jeffers</surname></string-name>, <string-name><given-names>M.</given-names><surname>Qian</surname></string-name>, <string-name><given-names>N.</given-names><surname>Milton</surname></string-name>, <string-name><given-names>J. D.</given-names><surname>Flory</surname></string-name>, <string-name><given-names>R.</given-names><surname>Hammamieh</surname></string-name>, <string-name><given-names>B. J.</given-names><surname>Daigle</surname><suffix>Jr.</suffix></string-name>, <string-name><given-names>A.</given-names><surname>Gautam</surname></string-name>, <string-name><given-names>K. R.</given-names><surname>Dean</surname></string-name>, <string-name><given-names>V. I.</given-names><surname>Reus</surname></string-name>, <string-name><given-names>O. M.</given-names><surname>Wolkowitz</surname></string-name>, <string-name><given-names>S. H.</given-names><surname>Mellon</surname></string-name>, <string-name><given-names>K. J.</given-names><surname>Ressler</surname></string-name>, <string-name><given-names>R.</given-names><surname>Yehuda</surname></string-name>, <string-name><given-names>K.</given-names><surname>Wang</surname></string-name>, <string-name><given-names>L.</given-names><surname>Hood</surname></string-name>, <string-name><given-names>F. J.</given-names><surname>Doyle</surname><suffix>III</suffix></string-name>, <string-name><given-names>M.</given-names><surname>Jett</surname></string-name>, and <string-name><given-names>C. R.</given-names><surname>Marmar</surname></string-name>, <article-title>Utilization of machine learning for identifying symptom severity military‐related PTSD subtypes and their biological correlates</article-title>, <source>Transl. Psychiatry</source><volume>11</volume> (<year>2021</year>), no. <issue>1</issue>, <fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">33414379</pub-id></mixed-citation>
    </ref>
    <ref id="sam11573-bib-0036">
      <label>36</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0036"><string-name><given-names>M.</given-names><surname>Soltaninejad</surname></string-name>, <string-name><given-names>G.</given-names><surname>Yang</surname></string-name>, <string-name><given-names>T.</given-names><surname>Lambrou</surname></string-name>, <string-name><given-names>N.</given-names><surname>Allinson</surname></string-name>, <string-name><given-names>T. L.</given-names><surname>Jones</surname></string-name>, <string-name><given-names>T. R.</given-names><surname>Barrick</surname></string-name>, <string-name><given-names>F. A.</given-names><surname>Howe</surname></string-name>, and <string-name><given-names>X.</given-names><surname>Ye</surname></string-name>, <article-title>Automated brain tumour detection and segmentation using superpixel‐based extremely randomized trees in FLAIR MRI</article-title>, <source>Int. J. Comput. Assist. Radiol. Surg.</source><volume>12</volume> (<year>2017</year>), no. <issue>2</issue>, <fpage>183</fpage>–<lpage>203</lpage>.<pub-id pub-id-type="pmid">27651330</pub-id></mixed-citation>
    </ref>
    <ref id="sam11573-bib-0037">
      <label>37</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0037"><string-name><given-names>A.</given-names><surname>Strehl</surname></string-name> and <string-name><given-names>J.</given-names><surname>Ghosh</surname></string-name>, <article-title>Cluster ensembles—A knowledge reuse framework for combining multiple partitions</article-title>, <source>J. Mach. Learn. Res.</source>
<volume>3, no. Dec</volume> (<year>2002</year>), <fpage>583</fpage>–<lpage>617</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0038">
      <label>38</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0038"><string-name><given-names>K. M.</given-names><surname>Ting</surname></string-name>, <string-name><given-names>Y.</given-names><surname>Zhu</surname></string-name>, <string-name><given-names>M.</given-names><surname>Carman</surname></string-name>, <string-name><given-names>Y.</given-names><surname>Zhu</surname></string-name>, and <string-name><given-names>Z.‐H.</given-names><surname>Zhou</surname></string-name>, “<part-title>Overcoming key weaknesses of distance‐based neighbourhood methods using a data dependent dissimilarity measure</part-title>,” <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>, <year>2016</year>, pp. <fpage>1205</fpage>–<lpage>1214</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0039">
      <label>39</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0039"><string-name><given-names>U.</given-names><surname>Von Luxburg</surname></string-name>, <article-title>A tutorial on spectral clustering</article-title>, <source>Stat. Comput.</source><volume>17</volume> (<year>2007</year>), no. <issue>4</issue>, <fpage>395</fpage>–<lpage>416</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0040">
      <label>40</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0040"><string-name><given-names>X. L.</given-names><surname>Xie</surname></string-name> and <string-name><given-names>G.</given-names><surname>Beni</surname></string-name>, <article-title>A validity measure for fuzzy clustering</article-title>, <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>13</volume> (<year>1991</year>), no. <issue>8</issue>, <fpage>841</fpage>–<lpage>847</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0041">
      <label>41</label>
      <mixed-citation publication-type="journal" id="sam11573-cit-0041"><string-name><given-names>D.</given-names><surname>Yan</surname></string-name>, <string-name><given-names>A.</given-names><surname>Chen</surname></string-name>, and <string-name><given-names>M. I.</given-names><surname>Jordan</surname></string-name>, <article-title>Cluster forests</article-title>, <source>Comput. Stat. Data Anal.</source><volume>66</volume> (<year>2013</year>), <fpage>178</fpage>–<lpage>192</lpage>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0042">
      <label>42</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0042"><string-name><given-names>Z.‐H.</given-names><surname>Zhou</surname></string-name>, <source>Ensemble methods: Foundations and algorithms</source>, <publisher-name>Chapman and Hall/CRC</publisher-name>, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="sam11573-bib-0043">
      <label>43</label>
      <mixed-citation publication-type="book" id="sam11573-cit-0043"><string-name><given-names>X.</given-names><surname>Zhu</surname></string-name>, <string-name><given-names>C.</given-names><surname>Change Loy</surname></string-name>, and <string-name><given-names>S.</given-names><surname>Gong</surname></string-name>, “<part-title>Constructing robust affinity graphs for spectral clustering</part-title>,” <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, <year>2014</year>, pp. <fpage>1450</fpage>–<lpage>1457</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
