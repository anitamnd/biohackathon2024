<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Oncol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Oncol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Oncol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Oncology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2234-943X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6798642</article-id>
    <article-id pub-id-type="doi">10.3389/fonc.2019.01045</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Oncology</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Superpixel-Based Conditional Random Fields (SuperCRF): Incorporating Global and Local Context for Enhanced Deep Learning in Melanoma Histopathology</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zormpas-Petridis</surname>
          <given-names>Konstantinos</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/484211/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Failmezger</surname>
          <given-names>Henrik</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn002">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/797883/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Raza</surname>
          <given-names>Shan E Ahmed</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Roxanis</surname>
          <given-names>Ioannis</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jamin</surname>
          <given-names>Yann</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/555121/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yuan</surname>
          <given-names>Yinyin</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Division of Radiotherapy and Imaging, The Institute of Cancer Research</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <aff id="aff2"><sup>2</sup><institution>The Royal Marsden NHS Trust</institution>, <addr-line>Surrey</addr-line>, <country>United Kingdom</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Division of Molecular Pathology, The Institute of Cancer Research</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Royal Free London NHS Foundation Trust</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Lei Deng, Jacobi Medical Center, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Sandra Avila, Campinas State University, Brazil; Sara Hosseinzadeh Kassani, University of Saskatchewan, Canada</p>
      </fn>
      <corresp id="c001">*Correspondence: Yinyin Yuan <email>yinyin.yuan@icr.ac.uk</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Cancer Imaging and Image-directed Interventions, a section of the journal Frontiers in Oncology</p>
      </fn>
      <fn fn-type="other" id="fn002">
        <p>†These authors have contributed equally to this work</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>1045</elocation-id>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>6</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2019 Zormpas-Petridis, Failmezger, Raza, Roxanis, Jamin and Yuan.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Zormpas-Petridis, Failmezger, Raza, Roxanis, Jamin and Yuan</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Computational pathology-based cell classification algorithms are revolutionizing the study of the tumor microenvironment and can provide novel predictive/prognosis biomarkers crucial for the delivery of precision oncology. Current algorithms used on hematoxylin and eosin slides are based on individual cell nuclei morphology with limited local context features. Here, we propose a novel multi-resolution hierarchical framework (SuperCRF) inspired by the way pathologists perceive regional tissue architecture to improve cell classification and demonstrate its clinical applications. We develop SuperCRF by training a state-of-art deep learning spatially constrained- convolution neural network (SC-CNN) to detect and classify cells from 105 high-resolution (20×) H&amp;E-stained slides of The Cancer Genome Atlas melanoma dataset and subsequently, a conditional random field (CRF) by combining cellular neighborhood with tumor regional classification from lower resolution images (5, 1.25×) given by a superpixel-based machine learning framework. SuperCRF led to an 11.85% overall improvement in the accuracy of the state-of-art deep learning SC-CNN cell classifier. Consistent with a stroma-mediated immune suppressive microenvironment, SuperCRF demonstrated that (i) a high ratio of lymphocytes to all lymphocytes within the stromal compartment (<italic>p</italic> = 0.026) and (ii) a high ratio of stromal cells to all cells (<italic>p</italic> &lt; 0.0001 compared to <italic>p</italic> = 0.039 for SC-CNN only) are associated with poor survival in patients with melanoma. SuperCRF improves cell classification by introducing global and local context-based information and can be implemented in combination with any single-cell classifier. SuperCRF provides valuable tools to study the tumor microenvironment and identify predictors of survival and response to therapy.</p>
    </abstract>
    <kwd-group>
      <kwd>deep learning</kwd>
      <kwd>machine learning</kwd>
      <kwd>conditional random fields</kwd>
      <kwd>digital pathology</kwd>
      <kwd>cell classification</kwd>
      <kwd>melanoma</kwd>
      <kwd>tumor microenvironment</kwd>
    </kwd-group>
    <counts>
      <fig-count count="3"/>
      <table-count count="2"/>
      <equation-count count="3"/>
      <ref-count count="45"/>
      <page-count count="10"/>
      <word-count count="6621"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Cancer is a highly complex, non-autonomous disease. The interactions between microenvironmental selective pressures and cancer cells dictate how cancer progresses and evolves. Accurate and spatially explicit characterization of the tumor microenvironmental landscape including how cancer cells interact with the extra-cellular matrix and other cellular players such as stromal cells and immune cells within the tumoral niche, is needed to understand the context in which cancer evolves, and may also provide robust predictor of cancer behavior for risk-stratification (<xref rid="B1" ref-type="bibr">1</xref>). More specifically the recent success of cancer immunotherapy including the spectacular response observed in patients with previously incurable melanoma, a highly aggressive form of skin cancer, calls for a better understanding of the cancer-immune interface.</p>
    <p>In the new era of digital pathology, advanced image analysis can objectively, consistently, and quantitatively characterize the different components of the tumor and how they spatially interact, and as a result assist pathologists in tasks such as tumor grading (<xref rid="B2" ref-type="bibr">2</xref>). Algorithms for cell detection and classification are key components of this process. Machine learning, and more recently deep learning algorithms, both exploiting the phenotypic differences in nuclear morphology between each cell type, revolutionized the field yielding significantly better cell detection, segmentation, and classification results (<xref rid="B3" ref-type="bibr">3</xref>–<xref rid="B9" ref-type="bibr">9</xref>).</p>
    <p>However, even state-of-the-art deep learning algorithms can underperform especially in cases where different cell types appear morphologically similar. Current computed pathology tools focus on individual cell nuclei morphology with limited abstract local context features, whereas pathologists incorporate regional tissue architecture (in practice, by zooming in/out), together with cell morphological features to accurately classify cells.</p>
    <p>Here, we hypothesize that robust tumor regional classification from lower resolution images can provide the contextual information that is key to further improve single cell classification algorithms. Our aim is to introduce dependencies on global tissue context and cell neighborhood and enhance learning results for cell classification from deep convolution neural networks (CNNs). Probabilistic graphical models have successfully been applied to improve cell classification in time-lapse imaging by taking into account the temporal context of a cell (<xref rid="B10" ref-type="bibr">10</xref>–<xref rid="B15" ref-type="bibr">15</xref>). Probabilistic graphical models have also been used successfully in histopathology images for pathology detection and segmentation (<xref rid="B16" ref-type="bibr">16</xref>–<xref rid="B19" ref-type="bibr">19</xref>), disease and tissue staging (<xref rid="B20" ref-type="bibr">20</xref>, <xref rid="B21" ref-type="bibr">21</xref>), and nuclei segmentation (<xref rid="B22" ref-type="bibr">22</xref>). In our study, instead of time dependency, we apply graphical models to introduce the spatial context of a cell as additional information to improve single-cell classification. A multi-resolution hierarchical framework was proposed to mirror the way pathologists perceive tumor architecture, and applied to whole-slide images (WSI) hematoxylin and eosin (H&amp;E)-stained slides of melanoma skin cancer (<xref ref-type="fig" rid="F1">Figure 1A</xref>). We demonstrated that our new system is computationally efficient and significantly improves single cell classification. The increased accuracy in cell classification further enabled us to shed new light on the understanding of cancer-immune-stroma interface of melanoma.</p>
    <fig id="F1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Overview of the SuperCRF framework for analyzing H&amp;E-stained pathological images of melanoma. <bold>(A)</bold> Major histological features of melanoma architecture. <bold>(B)</bold> Projection of regional classification results using superpixels from various scales to the 20× magnification for the improvement of single-cell classification. <bold>(C)</bold> Graphical representation of node dependencies (cells and superpixels) across different scales. <bold>(D)</bold> Region classification scheme using a superpixel based machine-learning method in whole-slide images (5× and 1.25× magnification) <bold>(E)</bold> Single-cell classification using a state-of-the-art spatially constrained-convolution neural network (SC-CNN) classifier <bold>(F)</bold> representative results of the SC-CNN cell classifier alone and combined with our SuperCRF system. Note the misclassification of various stromal cells by the SC-CNN, which are corrected by our model.</p>
      </caption>
      <graphic xlink:href="fonc-09-01045-g0001"/>
    </fig>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>Materials and Methods</title>
    <sec>
      <title>Datasets</title>
      <p>In total, 105 full-face, H&amp;E stained section images from formalin-fixed, paraffin-embedded (FFPE) diagnostic blocks of melanoma skin cancer from The Cancer Genome Atlas (TCGA) were used. We scaled all digitized (Aperio ImageScope) histology images to 20, 5, and 1.25× magnification with pixel resolution 0.504, 2.016, and 8.064 μm, respectively, using Bio-Formats (<ext-link ext-link-type="uri" xlink:href="https://www.openmicroscopy.org/bio-formats/">https://www.openmicroscopy.org/bio-formats/</ext-link>). WSIs at 20× magnification (representative size: 30,000 × 30,000 pixels), were split into sub-images (tiles) of 2,000 × 2,000 pixels each, for computational efficiency.</p>
      <p>For the purpose of training and testing the different parts of our system we divided the dataset into sub-datasets, namely single-cell classification dataset, 5× sub-dataset, 1.25× sub-dataset and discovery sub-dataset (<xref rid="T1" ref-type="table">Table 1</xref>, also see <xref ref-type="supplementary-material" rid="SM1">Supplementary Tables 1</xref>–<xref ref-type="supplementary-material" rid="SM4">4</xref>).</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>Summary of the data used to train and test the different parts of the SuperCRF system, as well as study the cancer-immune-stroma interface (also, see <xref ref-type="supplementary-material" rid="SM1">Supplementary Tables 1</xref>–<xref ref-type="supplementary-material" rid="SM4">4</xref>).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Name</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Number of WSIs</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Purpose</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>Single-cell</bold> classification sub-dataset</td>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>8</bold><break/> Training SC-CNN: 3 (348 tiles)<break/> Training SuperCRF: 2 (84 tiles)<break/> Testing: 3 (290 tiles)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Single-cell classification into four categories: cancer cells, lymphocytes, stromal cells, epidermal cells</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>5x</bold> sub-dataset</td>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>16</bold><break/> Training: 10<break/> Testing: 6</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Region classification into five categories: tumor, normal stroma, lymphocyte cluster, normal epidermis, lumen/white space</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>1.25x</bold> sub-dataset</td>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>58</bold><break/> Training: 21<break/> Testing: 37</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Region classification into four categories: tumor, normal stroma, normal epidermis, lumen/white space</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>Discovery</bold> dataset</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>97</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Study of the tumor-stroma interface. To accelerate the analysis, 50 tiles (2,000 × 2,000 pixels) containing tumors were randomly sampled from every whole-slide image (WSI)</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The values are bold for visual (illustration) purposes</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>Single-Cell Classification Using a Spatially Constrained Convolutional Neural Network</title>
      <p>We used a Spatially Constrained Convolutional Neural Network (SC-CNN) (<xref rid="B6" ref-type="bibr">6</xref>) for single cell classification (<xref ref-type="fig" rid="F1">Figure 1E</xref>). SC-CNN uses spatial regression in order to predict the probability of a pixel being the center of the nucleus. The nucleus is classified by a neighboring ensemble predictor (NEP) in conjunction with a standard softmax CNN. We randomly initialized the network's layers as we have found that to perform better than transfer learning from real-world datasets in our experiments with pathological samples.</p>
    </sec>
    <sec>
      <title>Superpixel-Based Tumor Region Classification</title>
      <p>A machine learning superpixel-based framework was implemented in Matlab (<xref rid="B23" ref-type="bibr">23</xref>) to classify tumor tissue regions and was subsequently applied to low resolution (5 and 1.25×) images. Reinhard stain normalization (<xref rid="B24" ref-type="bibr">24</xref>) was applied separately on each of the 5 and 1.25× sub-datasets to account for stain variabilities that could affect the classification (<xref rid="B25" ref-type="bibr">25</xref>).</p>
      <p>Downscaled images were segmented using the simple linear iterative clustering (SLIC) superpixels algorithm (<xref rid="B26" ref-type="bibr">26</xref>), which is designed to provide roughly uniform superpixels. Choosing the optimal number of superpixels is important to ensure that the superpixels capture homogeneous areas and adhere to image boundaries. With our pathologist's input, we visually identified a size of superpixels that met these criteria and chose the number of superpixels automatically based on each image's size (Equation 1).</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>c</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>l</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>g</mml:mi>
                <mml:mtext> </mml:mtext>
                <mml:mrow>
                  <mml:mo stretchy="true">(</mml:mo>
                  <mml:mrow>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>S</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>U</mml:mi>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                  <mml:mo stretchy="true">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>N</italic><sub><italic>i</italic></sub> is the number of superpixels in the <italic>i</italic>th image, <italic>S</italic><sub><italic>i</italic></sub> is the size of image <italic>i</italic> in pixels, and <italic>U</italic> (here <italic>U</italic> = 1,250) is a constant held across all images that defined a desired size of the superpixels. This means, on average, a superpixel occupies an area of approximately 35 × 35 pixels, equivalent to 280 × 280 mm<sup>2</sup>. We identified the superpixels belonging to each area by determining whether their central points fell within the regions annotated by the pathologist.</p>
      <p>Overall, for the 1.25× training sub-dataset, we found 15,477 superpixels belonging in tumor areas, 6,989 in stroma areas, 141 in epidermis and 691 in lumen/white space, while for the 5× training sub-dataset we found 1,193 superpixels belonging in tumor areas, 1,324 in stroma areas, 360 in epidermis, 506 in lymphocyte clusters and 830 in lumen/white space.</p>
      <p>Next, we extracted four types of features, 85 in total, from each superpixel, including seven histogram features (mean values of hue, saturation, and brightness, sum of intensities, contrast, standard deviation, and entropy), and well-established texture features [12 Haralick features (<xref rid="B27" ref-type="bibr">27</xref>), 59 rotation-invariant local binary patterns (RILBP), 7 segmentation-based fractal texture analysis (SFTA) features (<xref rid="B28" ref-type="bibr">28</xref>)]. Features were standardized into <italic>z</italic>-scores. The mean values and standard deviation of the features from the training set were used for the normalization of the test set. A support vector machine (SVM) with a radial basis function (RBF, γ = 1/number_of_features) was trained with these features to classify superpixels into different biologically meaningful categories.</p>
      <p>For the 5× sub-dataset, superpixels were classified into five categories: tumor area, normal stroma, normal epidermis, lymphocytes cluster, and lumen/white space. We increased the penalty in the cost function for the epidermis and lumen/white space classes by a factor of 10 when training the SVM, to account for class imbalance. For the 1.25× sub-dataset superpixels classification consisted of four categories: tumor area, normal stroma, normal epidermis, and lumen/white space. We randomly selected a subset of 5,000 cancer and stroma superpixels and increased the penalty in the cost function for the epidermis and lumen/white space classes by a factor of 10, again to account for class imbalance (<xref ref-type="fig" rid="F1">Figure 1D</xref>).</p>
    </sec>
    <sec>
      <title>SuperCRF</title>
      <p>Single-cell based classification approaches often assign a class label based on the morphology of -individual cells, regardless of their neighboring cells. However, these spatial relationships provide important information that is used by pathologists. Conditional random fields (CRF) are undirected graphical models that represent efficient ways to model dependences, by factorizing the probability density into a specific set of conditional dependence (<xref rid="B29" ref-type="bibr">29</xref>). Therefore, the tumor microenvironment can be modeled by a CRF by introducing nodes for cells and superpixels, as well as edges whenever there is a spatial relationship between nodes.</p>
      <p>We excluded lymphocytes from the CRF assumption that neighboring cells have a higher probability to share the same class labels, since they infiltrate, in an inconsistent manner ranging from sparse to highly dense, in tumor as well as stromal tissue. Therefore, lymphocytes kept their label as assigned by the SC-CNN.</p>
      <p>Let <italic>n</italic> be the total number of cells (besides lymphocytes) in the image and <italic>c</italic><sub><italic>i</italic></sub>∈{<italic>stromal, cancer, epidermis</italic>}, <italic>i</italic> = 1, 2, …, <italic>n</italic> the input labels of the cells as assigned by the SC-CNN. Let <italic>s</italic><sub><italic>i</italic></sub>, be the corresponding superpixel for a cell <italic>c</italic><sub><italic>i</italic></sub> with <italic>s</italic><sub><italic>i</italic></sub>∈{<italic>stromal, cancer, epidermis, white space</italic>} for 1.25× superpixels and <italic>s</italic><sub><italic>i</italic></sub>∈{<italic>stromal, cancer, epidermis, lymphocyte, whitespace</italic>} for 5× superpixels. <bold>x</bold>∈{<bold>c</bold>, <bold>s</bold>} comprises the nodes of the CRF. The CRF assigns output labels <italic>y</italic><sub><italic>i</italic></sub>∈{<italic>stromal, cancer, epidermis, lymphocyte, white space</italic>} based on the input data. The joint probability distribution over input data and output labels, <italic>p</italic>(<italic>y</italic><sub>1</sub>, <italic>y</italic><sub>2</sub>, …, <italic>y</italic><sub><italic>n</italic></sub> ⌊<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>n</italic></sub>) can be modeled by factorizing the probability density into a specific set of conditional dependence relationships (<xref ref-type="fig" rid="F1">Figure 1C</xref>).</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>p</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="true">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>y</mml:mi>
                    <mml:mo stretchy="false">|</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="true">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:munderover accentunder="false" accent="false">
                    <mml:mrow>
                      <mml:mo>∏</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:munderover>
                </mml:mstyle>
                <mml:mi>p</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="true">(</mml:mo>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mtext> </mml:mtext>
                    <mml:mo stretchy="false">|</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mtext> </mml:mtext>
                  </mml:mrow>
                  <mml:mo stretchy="true">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mtext>  </mml:mtext>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>Z</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mtext> exp</mml:mtext>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mstyle displaystyle="true">
                      <mml:mo>∑</mml:mo>
                    </mml:mstyle>
                    <mml:mi>E</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="true">(</mml:mo>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>y</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>,</mml:mo>
                        <mml:mtext> </mml:mtext>
                        <mml:mi>x</mml:mi>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>N</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>,</mml:mo>
                        <mml:mtext> </mml:mtext>
                        <mml:mi>y</mml:mi>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>N</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo stretchy="true">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>Z</italic> is a normalizing constant, w is a weight vector and</p>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M3">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>E</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="true">(</mml:mo>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>x</mml:mi>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>y</mml:mi>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                  <mml:mo stretchy="true">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mtext> </mml:mtext>
                <mml:mstyle displaystyle="true">
                  <mml:mo>∑</mml:mo>
                </mml:mstyle>
                <mml:mo>Φ</mml:mo>
                <mml:mrow>
                  <mml:mo stretchy="true">(</mml:mo>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="true">)</mml:mo>
                </mml:mrow>
                <mml:mo>+</mml:mo>
                <mml:mstyle displaystyle="true">
                  <mml:mo>∑</mml:mo>
                </mml:mstyle>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>ψ</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="true">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>y</mml:mi>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="true">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>defines the energy function of the CRF.</p>
      <p>The node potentials Φ(<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>) represent the evidence that a cell <italic>i</italic>, with the input label <italic>x</italic><sub><italic>i</italic></sub> takes the class label <italic>y</italic><sub><italic>i</italic></sub>. The node potential can be defined as Φ(<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>) = <italic>f</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>)+b, with <inline-formula><mml:math id="M4"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mtext> </mml:mtext></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mtext> </mml:mtext></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <italic>b</italic> representing the bias.</p>
      <p>The edge potentials ψ<sub><italic>c</italic></sub>(<italic>xNi, yNi</italic>) model the probability that neighboring cells take a similar cell label. <italic>N</italic><sub><italic>i</italic></sub> is the neighborhood of cell <italic>i</italic>, defined as all the cells that can be found in a defined distance. The edge potentials are defined as: ψ<sub><italic>c</italic></sub>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>, <italic>xN</italic><sub><italic>i</italic></sub>, <italic>yN</italic><sub><italic>i</italic></sub>) = <italic>f</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>)*<italic>f</italic>(<italic>xN</italic><sub><italic>i</italic></sub>, <italic>yN</italic><sub><italic>i</italic></sub>)+<italic>b</italic>.</p>
      <p>The CRF was trained with stochastic gradient descent and the decoding was applied using loopy belief propagation. The toolbox of M. Schmidt was used to train and decode the CRF (<xref rid="B30" ref-type="bibr">30</xref>).</p>
      <p>The source code for the study is available at Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/Henrik86/SuperCRF">https://github.com/Henrik86/SuperCRF</ext-link>).</p>
    </sec>
    <sec>
      <title>Survival Analysis</title>
      <p>We evaluated the prognosis value of the abundance of stromal cells and location of lymphocytes in our discovery sub-dataset. The ratio of stromal cells to all cells, the ratio of lymphocytes in cancer areas to all lymphocytes, and the ratio of lymphocytes in stroma areas to all lymphocytes were calculated for each patient. Patients were divided into high and low ratio groups, split at the median value of all scores. Patients with a ratio of lymphocytes being high inside the tumor area and low in the stroma were categorized as the “immune infiltration” group whereas patients with a ratio of lymphocytes being low in the tumor area and high in the stroma were categorized as “immune excluded,” based on the recent classification of the main immune phenotypes of anticancer immunity that predict response to immunotherapy (<xref rid="B31" ref-type="bibr">31</xref>). The number of patients belonging to neither of these two groups (high/high <italic>n</italic> = 6 and low/low <italic>n</italic> = 5) was too small to perform the survival analysis. Non-parametric Kaplan-Meier estimation was used to analyze overall survival in 94 patients. Differences between survival estimates were assessed with the log-rank test. Finally, Cox regression models were adjusted, testing for the independent prognostic relevance of our risk scores. To test if Breslow-thickness (the distance between the upper layer of the epidermis and the deepest point of tumor penetration) was contributing to a high ratio of stromal cells, we created a multivariate model containing both stromal cells ratio and Breslow-thickness, as well as two univariate models containing the covariates separately. Pearson's correlation was used to test for linear relation between the two variables.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <sec>
      <title>SuperCRF Improves Accuracy of Cell Classification</title>
      <p>First, we trained the state-of-the-art deep learning method, spatially-constrained CNN (SC-CNN) algorithm, to detect and classify cells in high resolution (20×) WSI into four categories: cancer cells, stroma cells, lymphocytes, and epidermis cells. The SC-CNN network yielded an accuracy of 84.63% over 4,059 cells in the independent test set (<xref rid="T1" ref-type="table">Table 1</xref>, <xref ref-type="supplementary-material" rid="SM5">Supplementary Table 5</xref>). Visual inspection revealed that the majority of false positives were misclassification of stromal and cancer cells as epidermis, which confirmed our initial motive for the incorporation of regional and spatial information to improve classification.</p>
      <p>Subsequently, we trained a conditional random field (CRF) by combining the cellular neighborhood with tumor region classification (cancer area, normal stroma, normal epidermis, lymphocyte cluster, and lumen/white space) from low resolution images (5 and 1.25×, <xref ref-type="fig" rid="F1">Figure 1B</xref>), given by the superpixel-based machine-learning framework. The SLIC superpixels algorithm has previously been shown to be computationally efficient, requiring only 3s on average to segment a single downscaled image of 2,500 × 2,500 pixels using a 2.9 GHz Intel core i7 processor. Performance of classification using individual and various combinations of feature sets was tested and the use of all 85 features, yielded the highest accuracy (<xref rid="B23" ref-type="bibr">23</xref>). It was then applied on the two datasets of 1.25 and 5× magnification (<xref ref-type="fig" rid="F2">Figure 2A</xref>) and achieved high accuracy in regional classification (1.25× sub-dataset: Overall accuracy 97.7% in the training set using 10-fold cross validation and 95.7% in 2,997 superpixels annotated in the 37 images of the independent test set; 5× sub-dataset: Overall accuracy 97.1% in the training set using 10-fold cross validation and 95.2% in 1,798 superpixels annotated in the six images of the independent test set).</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Representative examples of both superpixel and single-cell classification with or without SuperCRF. <bold>(A)</bold> Superpixels-based regional classification on representative whole slide images (5× magnification) of melanoma. Green: tumor area, Red: stroma area, Blue: normal epidermis, Yellow: lymphocyte cluster. <bold>(B)</bold> Representative images showing cell classification using a state-of-the-art spatially constrained-convolution neural network (SC-CNN) and four conditional random fields (CRF) models. Note the mislabeling of many cancer and stromal cells as epidermis cells when using the SC-CNN and the gradual increase in classification accuracy with the best accuracy achieved with the SuperCRF. Green, cancer cells; Red, stromal cells; Blue, lymphocytes; Yellow, epidermis cells.</p>
        </caption>
        <graphic xlink:href="fonc-09-01045-g0002"/>
      </fig>
      <p>To train SuperCRF, we first introduced dependencies on cell neighborhood. Cells were considered neighbors in the CRF, if they were located in a spatial proximity of 15 μm (or 30 pixels), which resulted in an average of 1.3 neighbors per cell. Subsequently, we integrated this local neighborhood with global context by connecting the CRF single-cell nodes to the regional classification results from superpixels. To determine the best configuration, we trained four different CRFs and compared their performance in terms of single-cell classification on a test set, including three samples, 290 tiles and 4,059 single-cell annotations (1,527 cancer cells, 676 lymphocytes, 837 normal epidermis cells, 1,019 stromal cells).</p>
      <p>In detail, for the first CRF we did not use any context classification, just cell neighborhood dependencies, i.e., the only edges of the CRF were between neighboring cells (singleCellCRF). For the second and third CRF we introduced superpixel nodes. Now, single-cell nodes are not only connected to neighboring cells but every single-cell node is also connected to a superpixel node. We trained a CRF for 5× superpixel classification (CRF5×) and 1.25× superpixel classification (CRF1.25×). Furthermore, we trained a CRF in which every single-cell node was connected to two superpixel nodes in 5 and 1.25× resolution (SuperCRF). Already the singleCellCRF (Accuracy: 87.6%, Precision: 89.7%, Recall: 89.5%, <xref rid="T2" ref-type="table">Table 2</xref>) improves the classification accuracy compared to the SC-CNN (84.6%, Precision: 87.6%, Recall: 88.1%, <xref rid="T2" ref-type="table">Table 2</xref>). However, the use of contextual information by the introduction of superpixel nodes, markedly improves the classification metrics (Accuracy 90.8%, Precision: 92.5%, Recall: 91.1%, <xref rid="T2" ref-type="table">Table 2</xref>) for CRF1.25× and (Accuracy 91.7%, Precision: 93%, Recall: 91.3%, <xref rid="T2" ref-type="table">Table 2</xref>) for the CRF5×. The SuperCRF, using nodes from superpixels in both 5 and 1.25× resolution images, as well as the neighboring cells, resulted in the highest classification outcome (Accuracy 96.5%, Precision: 96.4%, Recall: 96.3%, <xref rid="T2" ref-type="table">Table 2</xref>, <xref ref-type="fig" rid="F1">Figures 1F</xref>, <xref ref-type="fig" rid="F2">2B</xref>, <xref ref-type="supplementary-material" rid="SM5">Supplementary Tables 5</xref>–<xref ref-type="supplementary-material" rid="SM9">9</xref>).</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>Evaluation of different conditional random fields (CRF) versions and a state-of-the-art spatially constrained-convolution neural network (SC-CNN) deep learning cell-classifier.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Accuracy (%)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Precision</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Recall</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SC-CNN</td>
              <td valign="top" align="center" rowspan="1" colspan="1">84.63</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8756</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8808</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">singleCellCRF</td>
              <td valign="top" align="center" rowspan="1" colspan="1">87.61</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8973</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.8946</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">CRF1.25×</td>
              <td valign="top" align="center" rowspan="1" colspan="1">90.79</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9248</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9110</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">CRF5×</td>
              <td valign="top" align="center" rowspan="1" colspan="1">91.70</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9298</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.9126</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SuperCRF</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>96.48</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.9644</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.9629</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The values are bold to indicate the highest achieved accuracy, precision and recall</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>SuperCRF's Increased Accuracy of Cell Classification Improves Confidence in Stromal Cell Ratio as a Predictive Feature of Survival in Melanoma</title>
      <p>The crosstalk between cancer cells and stromal cells play an active role in tumor invasion and metastasis, and controlling immune infiltration and is increasingly recognized as a hallmark of cancer (<xref rid="B32" ref-type="bibr">32</xref>). Tumor-stromal cell ratio has been shown to hold prognostic and predictive information in patient with solid tumors (<xref rid="B31" ref-type="bibr">31</xref>, <xref rid="B33" ref-type="bibr">33</xref>, <xref rid="B34" ref-type="bibr">34</xref>). Here, we demonstrate that a high stromal cell ratio is also a predictor of poor prognosis in melanoma using both values derived from the multivariate models of SC-CNN and SuperCRF in our discovery sub-dataset. Yet SuperCRF yields a significantly higher confidence in the predictive value of the stromal cell ratio (SuperCRF: <italic>p</italic> &lt; 0.0001, Coxph-Regression (discretized by median): HR = 4.1, <italic>p</italic> = 0.006; SC-CNN: <italic>p</italic> = 0.039, Coxph-Regression (discretized by median): HR = 2.4, <italic>p</italic> = 0.05, <xref ref-type="fig" rid="F3">Figure 3A</xref>).</p>
      <fig id="F3" position="float">
        <label>Figure 3</label>
        <caption>
          <p>Associations between survival outcomes and SuperCRF-define risk groups in the Cancer Genome Atlas (TCGA) cohorts of patients with melanoma. <bold>(A)</bold> Kaplan-Meier Survival curves for patients in the high-risk group (blue) and low risk group classified by stromal cells ratio derived from SuperCRF (left) and using only the SC-CNN classifier. Note the difference in the p-value using the two methods. <bold>(B)</bold> Kaplan-Meier Survival curves for patients in the high-risk group (blue) and low risk group classified by immune phenotype based on spatial distribution of lymphocytes in different tumor compartments derived from SuperCRF.</p>
        </caption>
        <graphic xlink:href="fonc-09-01045-g0003"/>
      </fig>
      <p>Similar regression coefficients for both stromal cells ratio and Breslow-thickness covariates were observed between the multivariate and the two univariate survival models (1.404 and 0.171, respectively, for the multivariate model and 1.633 and 0.179 for the univariate models) of the SuperCRF. Pearson's correlation showed no correlation between stromal cells ratio and Breslow-thickness (<italic>r</italic> = −0.05), overall indicating that stromal cells ratio is independent to Breslow-thickness.</p>
    </sec>
    <sec>
      <title>Combining Cell and Region Classification: Location of the Immune Infiltrate Is Predictive of Survival in Melanoma</title>
      <p>There is increasing evidence of the value of immune infiltration to provide prognostic information and predictors of response in patient with melanoma [recently reviewed in (<xref rid="B35" ref-type="bibr">35</xref>)]. The spatial compartmentalization of immune cells afforded by our SuperCRF (by the cell and region classification results) was used to define the recently-described main immune phenotypes of anticancer immunity that predict response to immunotherapy (<xref rid="B31" ref-type="bibr">31</xref>). Patients with a classified “immune excluded” phenotype, defined by a low lymphocyte ratio inside the tumor area and high inside the stroma area, was associated with a significantly worse prognosis compared to “inflamed” tumors characterized by a high ratio of lymphocytes inside the tumor and a low ratio inside the stroma (<italic>p</italic> = 0.026, Cox PH –regression: HR = 2.57, <italic>p</italic> = 0.032, <xref ref-type="fig" rid="F3">Figure 3B</xref>). Taken together, our data is consistent with the model of a stroma-mediated immune suppressive microenvironment that exclude T cells from the vicinity of cancer cells.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>In this study, we implemented a framework which fuses traditional machine learning with deep learning to model the way pathologists incorporate large-scale tissue architecture and context across spatial scales, to improve single-cell classification in large whole-section slide images. Using this approach, we demonstrated a marked 11.85% overall improvement in the accuracy of the state-of-art deep learning SC-CNN cell classifier. Also, the similar values of both precision and recall and their simultaneous increase in every step show the unbiased nature of our approach.</p>
    <p>Computational pathology algorithms, typically exploit the inter-cell phenotypic differences for cell classification, yet even state-of-art deep learning algorithms tend to underperform in this task, mainly due to the disproportional numbers of cells sharing similar nuclear morphological features, or due to intra-class diversity, seen for example in tumor stroma (fatty tissue, necrosis, vessels, muscle, fibroblasts, and associated collagen). Whilst computers can quantify morphological differences in a considerably more complex way, pathologists still generally outperform computers in cell classification. An essential reason is that they incorporate key contextual information such as heterogeneous tissue architecture, together with cell morphological features.</p>
    <p>The idea that a cancer cell is dependent on its neighboring cells and global context is comparable to the fundamental concept in landscape ecology that a living population depends on the existing habitats and is not equally spread on the terrain. A particular habitat could favor the development of specific organisms. In practice, landscape ecologists denote the habitats from satellite images and then “ground-truth” them by detailed small-scale sampling of the habitats of interest (<xref rid="B36" ref-type="bibr">36</xref>). This inspired the design of our framework by introducing CRF dependencies between (i) the cells and their neighbors and (ii) the cells and to the global context (i.e., habitats from low resolution captured by the classified superpixels).</p>
    <p>Our proposed framework connects deep learning and classical image processing using probabilistic graphical models. All the information was combined using a CRF graphical model, which have been widely applied in image analysis for pathological images, yet mainly for semantic segmentation (<xref rid="B16" ref-type="bibr">16</xref>, <xref rid="B17" ref-type="bibr">17</xref>, <xref rid="B37" ref-type="bibr">37</xref>, <xref rid="B38" ref-type="bibr">38</xref>). Here, (1) we introduce a new way to capture high-level spatial context using superpixels, (2) propose a new CRF model that introduces dependences over space and across different spatial scales, thereby modeling multiple cells and their associated superpixels simultaneously for more accurate classification, (3) introduce the concept of context-specific CRF modeling, given that the strength of dependence can be variable according to tumor compartments. There is an increasing interest in combining deep learning with different strategies, or “umbrella approaches,” such as the use of traditional machine learning to spatially explicit context used in this study, with the aim to, not only refine and improve the overall existing deep learning network (<xref rid="B17" ref-type="bibr">17</xref>, <xref rid="B39" ref-type="bibr">39</xref>–<xref rid="B41" ref-type="bibr">41</xref>), but also facilitate biological interpretation compared to the “black-box”-like approach of deep-learning-only methods. However, optimizing and inventing new and refined deep learning networks is of equal importance, as during experimentation we observed that the better we made our single-cell classifier baseline, the more effective our SuperCRF approach became.</p>
    <p>We also showed that combining cell classification with the global context given by the region classification (both inherent parts of the SuperCRF architecture) can open new avenues to study the cancer microenvironment from histopathological slides. For example, the spectacular response observed in clinical trials of immunotherapy in patients with incurable melanoma calls for a better understanding of the tumor microenvironment and in particular the cancer-immune-stroma interface. Here, our approach and its ability to look at lymphocytes within their cellular and global context can predict melanoma patient survival and potentially provide biomarker stratification for immunotherapeutic approaches, by identifying the three main types of tumor immunophenotypes including (i) inflamed tumors which are characterized by infiltrated T Cells within the tumor, and associated with a generally good prognosis (ii) immune-excluded tumors, in which T cells are present but prevented to infiltrate the tumor due to stromal interaction, and associated with worse prognosis (and obviously (iii) immune desert tumors). This could also potentially be extended to provide quantitative biomarkers to characterize the immune infiltrating response to immunotherapy. We also demonstrated that in accordance with the immune-excluded phenotype, tumors rich in stromal cells had a marked poorer prognosis in patients with melanoma. With <italic>p</italic>-value lower by two orders of magnitude, our method provide stronger predictive power than by using deep-learning only method for cell classification.</p>
    <p>In the future, we plan to extend our framework and include an upward optimization step for the superpixels which may include additional classes for cells, regions and structures in order to provide a complete characterization of the tumor microenvironment. This may include deriving further classes from higher resolution images as we did for lymphocyte clusters in this study which were difficult to visualize in 1.25× resolution images. Incorporating additional deep learning methods should also be explored to perfect the classification of superpixels, for example by incorporating features extracted from a DCNN or a deep autoencoder, or to provide a potential alternative to superpixels, which may not be appropriate for the characterization of complicated structures, such as glands (<xref rid="B42" ref-type="bibr">42</xref>).</p>
    <p>The primary aim of this study was to demonstrate proof-of-principle that the introduction of global and local context as cell dependencies using a probabilistic graphical model as a post-processing step, like an “umbrella,” can significantly improve the performance of deep learning or classical machine learning cell classifiers based only on cell-morphology and abstract local context information. We chose the SC-CNN architecture as our primary cell classification step due to its state-of-the-art performance in cell detection and classification compared to other well-established deep learning and classical machine learning approaches (<xref rid="B6" ref-type="bibr">6</xref>). Alternatively, other promising deep learning networks could potentially be used including Inception v3 (<xref rid="B43" ref-type="bibr">43</xref>), Inception v4 (<xref rid="B44" ref-type="bibr">44</xref>), or a VGG architecture (<xref rid="B45" ref-type="bibr">45</xref>).</p>
    <p>Overall, our vision is to establish a network which will provide a complete characterization of every component of the tumor microenvironment where all the parts will interact with each other like an ecological landscape. Such system has immense potential and can be virtually transferred to any cancer type, to provide a better understanding of the cancer-immune cell interface, cell-stroma interactions, and predictive biomarkers of response to novel therapies, including immunotherapy, which has radically changed melanoma patient survival.</p>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>Conclusion</title>
    <p>The novel general framework SuperCRF improves cell classification by introducing global and local context-based information much like pathologists do. SuperCRF can be implemented in combination with any single-cell classifier and represent valuable tools to study the cancer-stroma-immune interface, which we used to identify predictors of survival in melanoma patients from conventional H&amp;E stained histopathology.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found here: <ext-link ext-link-type="uri" xlink:href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga">https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga</ext-link>.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>KZ-P, HF, SR, IR, YJ, and YY: substantial contributions to the conception or design of the work; or the method development, analysis, or interpretation of data for the work; drafting the work or revising it critically for important intellectual content; final approval of the version to be published; agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.</p>
    <sec>
      <title>Conflict of Interest</title>
      <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> Cancer Research UK to the Cancer Imaging Centre at ICR, in association with the MRC and Department of Health (England) (C1060/A16464), NHS funding to the NIHR Biomedicine Research Centre and the Clinical Research Facility in Imaging, The Rosetrees Trust (KZ-P). YY acknowledges support by Cancer Research UK (C45982/A21808), Breast Cancer Now (2015NovPR638) and was also supported in part by the Wellcome Trust (105104/Z/14/Z). YJ was a Children with Cancer UK Research Fellow.</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s8">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fonc.2019.01045/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fonc.2019.01045/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <media xlink:href="Table_1.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM2">
      <media xlink:href="Table_2.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM3">
      <media xlink:href="Table_3.XLSX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM4">
      <media xlink:href="Table_4.xlsx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM5">
      <media xlink:href="Table_5.DOCX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM6">
      <media xlink:href="Table_6.DOCX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM7">
      <media xlink:href="Table_7.DOCX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM8">
      <media xlink:href="Table_8.DOCX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM9">
      <media xlink:href="Table_9.DOCX">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gurcan</surname><given-names>MN</given-names></name><name><surname>Boucheron</surname><given-names>L</given-names></name><name><surname>Can</surname><given-names>A</given-names></name><name><surname>Madabhushi</surname><given-names>A</given-names></name><name><surname>Rajpoot</surname><given-names>N</given-names></name><name><surname>Yener</surname><given-names>B</given-names></name></person-group>. <article-title>Histopathological image analysis: a review</article-title>. <source>IEEE Rev. Biomed. Eng.</source> (<year>2009</year>) <volume>2</volume>:<fpage>147</fpage>. <pub-id pub-id-type="doi">10.1109/RBME.2009.2034865</pub-id><?supplied-pmid 20671804?><pub-id pub-id-type="pmid">20671804</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kothari</surname><given-names>S</given-names></name><name><surname>Phan</surname><given-names>JH</given-names></name><name><surname>Stokes</surname><given-names>TH</given-names></name><name><surname>Wang</surname><given-names>MD</given-names></name></person-group>. <article-title>Pathology imaging informatics for quantitative analysis of whole-slide images</article-title>. <source>J Am Med Inform Assoc.</source> (<year>2013</year>) <volume>20</volume>:<fpage>1099</fpage>–<lpage>108</lpage>. <pub-id pub-id-type="doi">10.1136/amiajnl-2012-001540</pub-id><?supplied-pmid 23959844?><pub-id pub-id-type="pmid">23959844</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>TR</given-names></name><name><surname>Kang</surname><given-names>IH</given-names></name><name><surname>Wheeler</surname><given-names>DB</given-names></name><name><surname>Lindquist</surname><given-names>RA</given-names></name><name><surname>Papallo</surname><given-names>A</given-names></name><name><surname>Sabatini</surname><given-names>DM</given-names></name><etal/></person-group>. <article-title>CellProfiler analyst: data exploration and analysis software for complex image-based screens</article-title>. <source>BMC Bioinf.</source><volume>9</volume>:<fpage>482</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-9-482</pub-id><?supplied-pmid 19014601?><pub-id pub-id-type="pmid">19014601</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>Y</given-names></name><name><surname>Failmezger</surname><given-names>H</given-names></name><name><surname>Rueda</surname><given-names>OM</given-names></name><name><surname>Ali</surname><given-names>HR</given-names></name><name><surname>Gräf</surname><given-names>S</given-names></name><name><surname>Chin</surname><given-names>SF</given-names></name><etal/></person-group>. <article-title>Quantitative image analysis of cellular heterogeneity in breast tumors complements genomic profiling</article-title>. <source>Sci. Trans. Med</source>. (<year>2012</year>) <volume>4</volume>:<fpage>157ra143</fpage>. <pub-id pub-id-type="doi">10.1126/scitranslmed.3004330</pub-id><?supplied-pmid 23100629?><pub-id pub-id-type="pmid">23100629</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CL</given-names></name><name><surname>Mahjoubfar</surname><given-names>A</given-names></name><name><surname>Tai</surname><given-names>L-C</given-names></name><name><surname>Blaby</surname><given-names>IK</given-names></name><name><surname>Huang</surname><given-names>A</given-names></name><name><surname>Niazi</surname><given-names>KR</given-names></name><etal/></person-group>. <article-title>Deep learning in label-free cell classification</article-title>. <source>Sci Rep.</source> (<year>2016</year>) <volume>6</volume>:<fpage>21471</fpage>. <pub-id pub-id-type="doi">10.1038/srep21471</pub-id><?supplied-pmid 26975219?><pub-id pub-id-type="pmid">26975219</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sirinukunwattana</surname><given-names>K</given-names></name><name><surname>Raza</surname><given-names>SEA</given-names></name><name><surname>Tsang</surname><given-names>Y-W</given-names></name><name><surname>Snead</surname><given-names>DR</given-names></name><name><surname>Cree</surname><given-names>IA</given-names></name><name><surname>Rajpoot</surname><given-names>NM</given-names></name></person-group>. <article-title>Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</article-title>. <source>IEEE Trans Med Imag.</source> (<year>2016</year>) <volume>35</volume>:<fpage>1196</fpage>–<lpage>206</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2016.2525803</pub-id><?supplied-pmid 26863654?><pub-id pub-id-type="pmid">26863654</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankhead</surname><given-names>P</given-names></name><name><surname>Loughrey</surname><given-names>MB</given-names></name><name><surname>Fernández</surname><given-names>JA</given-names></name><name><surname>Dombrowski</surname><given-names>Y</given-names></name><name><surname>McArt</surname><given-names>DG</given-names></name><name><surname>Dunne</surname><given-names>PD</given-names></name><etal/></person-group>. <article-title>QuPath: open source software for digital pathology image analysis</article-title>. <source>Sci Rep.</source><volume>7</volume>:<fpage>16878</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id><?supplied-pmid 29203879?><pub-id pub-id-type="pmid">29203879</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Khoshdeli</surname><given-names>M</given-names></name><name><surname>Cong</surname><given-names>R</given-names></name><name><surname>Parvin</surname><given-names>B</given-names></name></person-group>. <article-title>Detection of nuclei in H&amp;E stained sections using convolutional neural networks</article-title>. In: <source>Biomedical and Health Informatics (BHI), 2017 IEEE EMBS International Conference on</source>. <publisher-loc>Orlando, FL</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2017</year>). p. <fpage>105</fpage>–<lpage>8</lpage>. <?supplied-pmid 28580455?></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piccinini</surname><given-names>F</given-names></name><name><surname>Balassa</surname><given-names>T</given-names></name><name><surname>Szkalisity</surname><given-names>A</given-names></name><name><surname>Molnar</surname><given-names>C</given-names></name><name><surname>Paavolainen</surname><given-names>L</given-names></name><name><surname>Kujala</surname><given-names>K</given-names></name><etal/></person-group>. <article-title>Advanced cell classifier: user-friendly machine-learning-based software for discovering phenotypes in high-content imaging data</article-title>. <source>Cell Syst</source>. (<year>2017</year>) <volume>4</volume>:<fpage>651</fpage>–<lpage>5</lpage>.e655. <pub-id pub-id-type="doi">10.1016/j.cels.2017.05.012</pub-id><?supplied-pmid 28647475?><pub-id pub-id-type="pmid">28647475</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Held</surname><given-names>M</given-names></name><name><surname>Schmitz</surname><given-names>MH</given-names></name><name><surname>Fischer</surname><given-names>B</given-names></name><name><surname>Walter</surname><given-names>T</given-names></name><name><surname>Neumann</surname><given-names>B</given-names></name><name><surname>Olma</surname><given-names>MH</given-names></name><etal/></person-group>. <article-title>CellCognition: time-resolved phenotype annotation in high-throughput live cell imaging</article-title>. <source>Nat Method.</source> (<year>2010</year>) <volume>7</volume>:<fpage>747</fpage>–<lpage>54</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.1486</pub-id><?supplied-pmid 20693996?><pub-id pub-id-type="pmid">20693996</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>Q</given-names></name><name><surname>Busetto</surname><given-names>AG</given-names></name><name><surname>Fededa</surname><given-names>JP</given-names></name><name><surname>Buhmann</surname><given-names>JM</given-names></name><name><surname>Gerlich</surname><given-names>DW</given-names></name></person-group>. <article-title>Unsupervised modeling of cell morphology dynamics for time-lapse microscopy</article-title>. <source>Nat Methods.</source> (<year>2012</year>) <volume>9</volume>:<fpage>711</fpage>–<lpage>3</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2046</pub-id><?supplied-pmid 22635062?><pub-id pub-id-type="pmid">22635062</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Failmezger</surname><given-names>H</given-names></name><name><surname>Frohlich</surname><given-names>H</given-names></name><name><surname>Tresch</surname><given-names>A</given-names></name></person-group>. <article-title>Unsupervised automated high throughput phenotyping of RNAi time-lapse movies</article-title>. <source>BMC Bioinf.</source> (<year>2013</year>) <volume>14</volume>:<fpage>292</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-14-292</pub-id><?supplied-pmid 24090185?><pub-id pub-id-type="pmid">24090185</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Failmezger</surname><given-names>H</given-names></name><name><surname>Praveen</surname><given-names>P</given-names></name><name><surname>Tresch</surname><given-names>A</given-names></name><name><surname>Frohlich</surname><given-names>H</given-names></name></person-group>. <article-title>Learning gene network structure from time laps cell imaging in RNAi Knock downs</article-title>. <source>Bioinformatics</source>. (<year>2013</year>) <volume>29</volume>:<fpage>1534</fpage>–<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btt179</pub-id><?supplied-pmid 23595660?><pub-id pub-id-type="pmid">23595660</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niederberger</surname><given-names>T</given-names></name><name><surname>Failmezger</surname><given-names>H</given-names></name><name><surname>Uskat</surname><given-names>D</given-names></name><name><surname>Poron</surname><given-names>D</given-names></name><name><surname>Glauche</surname><given-names>I</given-names></name><name><surname>Scherf</surname><given-names>N</given-names></name><etal/></person-group>. <article-title>Factor graph analysis of live cell-imaging data reveals mechanisms of cell fate decisions</article-title>. <source>Bioinformatics.</source> (<year>2015</year>) <volume>31</volume>:<fpage>1816</fpage>–<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btv040</pub-id><?supplied-pmid 25638814?><pub-id pub-id-type="pmid">25638814</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Failmezger</surname><given-names>H</given-names></name><name><surname>Dursun</surname><given-names>E</given-names></name><name><surname>Dumcke</surname><given-names>S</given-names></name><name><surname>Endele</surname><given-names>M</given-names></name><name><surname>Poron</surname><given-names>D</given-names></name><name><surname>Schroeder</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>Clustering of samples with a tree-shaped dependence structure, with an application to microscopic time lapse imaging</article-title>. <source>Bioinformatics.</source> (<year>2018</year>). <pub-id pub-id-type="doi">10.1093/bioinformatics/bty939</pub-id><?supplied-pmid 30452534?><pub-id pub-id-type="pmid">30452534</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimaghaloo</surname><given-names>Z</given-names></name><name><surname>Arnold</surname><given-names>DL</given-names></name><name><surname>Arbel</surname><given-names>T</given-names></name></person-group>. <article-title>Adaptive multi-level conditional random fields for detection and segmentation of small enhanced pathology in medical images</article-title>. <source>Med Image Anal.</source> (<year>2016</year>) <volume>27</volume>:<fpage>17</fpage>–<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2015.06.004</pub-id><?supplied-pmid 26211811?><pub-id pub-id-type="pmid">26211811</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L-C</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group>. <article-title>Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title>. <source>IEEE Trans Pattern Anal Machine Intell.</source> (<year>2018</year>) <volume>40</volume>:<fpage>834</fpage>–<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><?supplied-pmid 28463186?><pub-id pub-id-type="pmid">28463186</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Ping</surname><given-names>W</given-names></name></person-group><article-title>Cancer metastasis detection with neural conditional random field</article-title>. In: <source>1st Conference on Medical Imaging With Deep Learning (MIDL)</source>. Amsterdam (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zanjani</surname><given-names>FG</given-names></name><name><surname>Zinger</surname><given-names>S</given-names></name></person-group><article-title>Cancer detection in histopathology whole-slide images using conditional random fields on deep embedded spaces</article-title>. In: <source>Medical Imaging 2018: Digital Pathology</source>. <publisher-loc>Houston, TX</publisher-loc>: <publisher-name>International Society for Optics and Photonics</publisher-name> (<year>2018</year>). p. <fpage>105810I</fpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rajapakse</surname><given-names>JC</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name></person-group>. <article-title>Staging tissues with conditional random fields</article-title>. In: <source>2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2011</year>). p. <fpage>5128</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1109/IEMBS.2011.6091270</pub-id><?supplied-pmid 22255493?></mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Xue</surname><given-names>D</given-names></name><name><surname>Hu</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>He</surname><given-names>L</given-names></name><etal/></person-group><article-title>Weakly supervised cervical histopathological image classification using multilayer hidden conditional random fields</article-title>. In: <source>International Conference on Information Technologies in Biomedicine</source>. <publisher-loc>Kamień Śląski</publisher-loc>: <publisher-name>Springer</publisher-name> (<year>2019</year>). p. <fpage>209</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-23762-2_19</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paramanandam</surname><given-names>M</given-names></name><name><surname>O'Byrne</surname><given-names>M</given-names></name><name><surname>Ghosh</surname><given-names>B</given-names></name><name><surname>Mammen</surname><given-names>JJ</given-names></name><name><surname>Manipadam</surname><given-names>MT</given-names></name><name><surname>Thamburaj</surname><given-names>R</given-names></name><etal/></person-group>. <article-title>Automated segmentation of nuclei in breast cancer histopathology images</article-title>. <source>PLoS ONE.</source><volume>11</volume>:<fpage>e0162053</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0162053</pub-id><?supplied-pmid 27649496?><pub-id pub-id-type="pmid">27649496</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zormpas-Petridis</surname><given-names>K</given-names></name><name><surname>Failmezger</surname><given-names>H</given-names></name><name><surname>Roxanis</surname><given-names>I</given-names></name><name><surname>Blackledge</surname><given-names>M</given-names></name><name><surname>Jamin</surname><given-names>Y</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name></person-group><article-title>Capturing global spatial context for accurate cell classification in skin cancer histology</article-title>. <source>Comput Pathol Ophth Med Image Anal</source>. (<year>2018</year>) <volume>11039</volume>:<fpage>52</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-00949-6_7</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinhard</surname><given-names>E</given-names></name><name><surname>Adhikhmin</surname><given-names>M</given-names></name><name><surname>Gooch</surname><given-names>B</given-names></name><name><surname>Shirley</surname><given-names>P</given-names></name></person-group><article-title>Color transfer between images</article-title>. <source>IEEE Comp Grap Appl.</source> (<year>2001</year>) <volume>21</volume>:<fpage>34</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1109/38.946629</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>AM</given-names></name><name><surname>Rajpoot</surname><given-names>N</given-names></name><name><surname>Treanor</surname><given-names>D</given-names></name><name><surname>Magee</surname><given-names>D</given-names></name></person-group>. <article-title>A nonlinear mapping approach to stain normalization in digital histopathology images using image-specific color deconvolution</article-title>. <source>IEEE Trans Biomed Eng.</source> (<year>2014</year>) <volume>61</volume>:<fpage>1729</fpage>–<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1109/TBME.2014.2303294</pub-id><?supplied-pmid 24845283?><pub-id pub-id-type="pmid">24845283</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achanta</surname><given-names>R</given-names></name><name><surname>Shaji</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>K</given-names></name><name><surname>Lucchi</surname><given-names>A</given-names></name><name><surname>Fua</surname><given-names>P</given-names></name><name><surname>Süsstrunk</surname><given-names>S</given-names></name></person-group>. <article-title>SLIC superpixels compared to state-of-the-art superpixel methods</article-title>. <source>IEEE Trans Pattern Anal Mach Intell.</source> (<year>2012</year>) <volume>34</volume>:<fpage>2274</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2012.120</pub-id><?supplied-pmid 22641706?><pub-id pub-id-type="pmid">22641706</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haralick</surname><given-names>RM</given-names></name><name><surname>Shanmugam</surname><given-names>K</given-names></name></person-group><article-title>Textural features for image classification</article-title>. <source>IEEE Trans Syst Man Cyber.</source> (<year>1973</year>) <volume>3</volume>:<fpage>610</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.1973.4309314</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Imran</surname><given-names>M</given-names></name><name><surname>Hashim</surname><given-names>R</given-names></name><name><surname>Khalid</surname><given-names>NEA</given-names></name></person-group><article-title>Segmentation-based fractal texture analysis and color layout descriptor for content based image retrieval</article-title>. In: <source>Intelligent Systems Design and Applications (ISDA), 2014 14th International Conference on IEEE.</source><publisher-loc>Okinawa</publisher-loc> (<year>2014</year>). p. <fpage>30</fpage>–<lpage>3</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lafferty</surname><given-names>J</given-names></name><name><surname>McCallum</surname><given-names>A</given-names></name><name><surname>Pereira</surname><given-names>FC</given-names></name></person-group><article-title>Conditional random fields: probabilistic models for segmenting and labeling sequence data</article-title>. In: <source>Proceedings of the 18th International Conference on Machine Learning 2001 (ICML 2001)</source>. <publisher-loc>Williamstown, MA</publisher-loc>: <publisher-name>Morgan Kaufmann Publishers</publisher-name> (<year>2001</year>). p. <fpage>282</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>M</given-names></name></person-group><source>UGM: A Matlab Toolbox for Probabilistic Undirected Graphical Models</source>. (<year>2007</year>). Available online at: <ext-link ext-link-type="uri" xlink:href="http://www.cs.ubc.ca/~schmidtm/Software/UGM.html">http://www.cs.ubc.ca/~schmidtm/Software/UGM.html</ext-link></mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Kruijf</surname><given-names>EM</given-names></name><name><surname>van Nes</surname><given-names>JG</given-names></name><name><surname>van de Velde</surname><given-names>CJ</given-names></name><name><surname>Putter</surname><given-names>H</given-names></name><name><surname>Smit</surname><given-names>VT</given-names></name><name><surname>Liefers</surname><given-names>GJ</given-names></name><etal/></person-group>. <article-title>Tumor-stroma ratio in the primary tumor is a prognostic factor in early breast cancer patients, especially in triple-negative carcinoma patients</article-title>. <source>Breast Cancer Res Treat.</source> (<year>2011</year>) <volume>125</volume>:<fpage>687</fpage>–<lpage>96</lpage>. <pub-id pub-id-type="doi">10.1007/s10549-010-0855-6</pub-id><?supplied-pmid 20361254?><pub-id pub-id-type="pmid">20361254</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanahan</surname><given-names>D</given-names></name><name><surname>Weinberg</surname><given-names>RA</given-names></name></person-group>. <article-title>Hallmarks of cancer: the next generation</article-title>. <source>Cell.</source> (<year>2011</year>) <volume>144</volume>:<fpage>646</fpage>–<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2011.02.013</pub-id><?supplied-pmid 21376230?><pub-id pub-id-type="pmid">21376230</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Liang</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Su</surname><given-names>W</given-names></name></person-group>. <article-title>Association between tumor-stroma ratio and prognosis in solid tumor patients: a systematic review and meta-analysis</article-title>. <source>Oncotarget.</source> (<year>2016</year>) <volume>7</volume>:<fpage>68954</fpage>–<lpage>65</lpage>. <pub-id pub-id-type="doi">10.18632/oncotarget.12135</pub-id><?supplied-pmid 27661111?><pub-id pub-id-type="pmid">27661111</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheer</surname><given-names>R</given-names></name><name><surname>Baidoshvili</surname><given-names>A</given-names></name><name><surname>Zoidze</surname><given-names>S</given-names></name><name><surname>Elferink</surname><given-names>MAG</given-names></name><name><surname>Berkel</surname><given-names>AEM</given-names></name><name><surname>Klaase</surname><given-names>JM</given-names></name><etal/></person-group>. <article-title>Tumor-stroma ratio as prognostic factor for survival in rectal adenocarcinoma: a retrospective cohort study</article-title>. <source>World J Gastrointest Oncol.</source> (<year>2017</year>) <volume>9</volume>:<fpage>466</fpage>–<lpage>74</lpage>. <pub-id pub-id-type="doi">10.4251/wjgo.v9.i12.466</pub-id><?supplied-pmid 29290917?><pub-id pub-id-type="pmid">29290917</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnes</surname><given-names>TA</given-names></name><name><surname>Amir</surname><given-names>E</given-names></name></person-group><article-title>HYPE or HOPE: the prognostic value of infiltrating immune cells in cancer</article-title>. <source>Br J Cancer.</source> (<year>2017</year>) <volume>117</volume>:<fpage>451</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1038/bjc.2017.220</pub-id><pub-id pub-id-type="pmid">28704840</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>JY</given-names></name><name><surname>Gatenby</surname><given-names>RA</given-names></name></person-group>. <article-title>Quantitative clinical imaging methods for monitoring intratumoral evolution</article-title>. <source>Methods Mol Biol.</source> (<year>2017</year>) <volume>1513</volume>:<fpage>61</fpage>–<lpage>81</lpage>. <pub-id pub-id-type="doi">10.1007/978-1-4939-6539-7_6</pub-id><?supplied-pmid 27807831?><pub-id pub-id-type="pmid">27807831</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>LC</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group>. <article-title>Semantic image segmentation with deep convolutional nets and fully connected CRFS</article-title>. In: <source>3rd International Conference on Learning Representations, ICLR 2015</source>. <publisher-loc>San Diego, CA</publisher-loc> (<year>2014</year>). <?supplied-pmid 28463186?></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kokkinos</surname><given-names>I</given-names></name></person-group><article-title>Ubernet: training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</source><publisher-loc>Honolulu, HI</publisher-loc> (<year>2017</year>). p. <fpage>6129</fpage>–<lpage>38</lpage>.</mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnab</surname><given-names>A</given-names></name><name><surname>Zheng</surname><given-names>S</given-names></name><name><surname>Jayasumana</surname><given-names>S</given-names></name><name><surname>Romera-Paredes</surname><given-names>B</given-names></name><name><surname>Larsson</surname><given-names>M</given-names></name><name><surname>Kirillov</surname><given-names>A</given-names></name><etal/></person-group><article-title>Conditional random fields meet deep neural networks for semantic segmentation</article-title>. <source>IEEE Signal Process Mag.</source><volume>35</volume>:<fpage>37</fpage>–<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1109/MSP.2017.2762355</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>Y</given-names></name><name><surname>Kamnitsas</surname><given-names>K</given-names></name><name><surname>Ancha</surname><given-names>S</given-names></name><name><surname>Nanavati</surname><given-names>J</given-names></name><name><surname>Cottrell</surname><given-names>G</given-names></name><name><surname>Criminisi</surname><given-names>A</given-names></name><etal/></person-group><article-title>Autofocus layer for semantic segmentation</article-title>. <source>arXiv:1805.08403</source> (<year>2018</year>). <pub-id pub-id-type="doi">10.1007/978-3-030-00931-1_69</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>AG</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Wachinger</surname><given-names>C</given-names></name></person-group><article-title>Concurrent spatial and channel ‘Squeeze &amp; Excitation' in fully convolutional networks</article-title>. In: <person-group person-group-type="editor"><name><surname>Frangi</surname><given-names>A</given-names></name><name><surname>Schnabel</surname><given-names>J</given-names></name><name><surname>Davatzikos</surname><given-names>C</given-names></name><name><surname>Alberola-López</surname><given-names>C</given-names></name><name><surname>Fichtinger</surname><given-names>G</given-names></name></person-group> editors. <source>Medical Image Computing and Computer Assisted Intervention - MICCAI 2018. Lecture Notes in Computer Science, Vol 11070</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer</publisher-name> (<year>2018</year>). <pub-id pub-id-type="doi">10.1007/978-3-030-00928-1_48</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raza</surname><given-names>SEA</given-names></name><name><surname>Cheung</surname><given-names>L</given-names></name><name><surname>Shaban</surname><given-names>M</given-names></name><name><surname>Graham</surname><given-names>S</given-names></name><name><surname>Epstein</surname><given-names>D</given-names></name><name><surname>Pelengaris</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>Micro-Net: a unified model for segmentation of various objects in microscopy images</article-title>. <source>Med. Image Anal.</source> (<year>2019</year>) <volume>52</volume>:<fpage>160</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2018.12.003</pub-id><?supplied-pmid 30580111?><pub-id pub-id-type="pmid">30580111</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Wojna</surname><given-names>Z</given-names></name></person-group><article-title>Rethinking the inception architecture for computer vision</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. (<year>2016</year>). p. <fpage>2818</fpage>–<lpage>26</lpage>.</mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Alemi</surname><given-names>AA</given-names></name></person-group><article-title>Inception-v4, inception-resnet and the impact of residual connections on learning</article-title>. In: <source>Thirty-First AAAI Conference on Artificial Intelligence.</source><publisher-loc>San Francisco, CA</publisher-loc> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title>. In: <source>3rd International Conference on Learning Representations, ICLR 2015</source>. <publisher-loc>San Diego, CA</publisher-loc> (<year>2014</year>).</mixed-citation>
    </ref>
  </ref-list>
</back>
