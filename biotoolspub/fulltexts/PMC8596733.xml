<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Mol Ecol Resour</journal-id>
    <journal-id journal-id-type="iso-abbrev">Mol Ecol Resour</journal-id>
    <journal-id journal-id-type="doi">10.1111/(ISSN)1755-0998</journal-id>
    <journal-id journal-id-type="publisher-id">MEN</journal-id>
    <journal-title-group>
      <journal-title>Molecular Ecology Resources</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1755-098X</issn>
    <issn pub-type="epub">1755-0998</issn>
    <publisher>
      <publisher-name>John Wiley and Sons Inc.</publisher-name>
      <publisher-loc>Hoboken</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8596733</article-id>
    <article-id pub-id-type="pmid">33950563</article-id>
    <article-id pub-id-type="doi">10.1111/1755-0998.13413</article-id>
    <article-id pub-id-type="publisher-id">MEN13413</article-id>
    <article-categories>
      <subj-group subj-group-type="overline">
        <subject>Special Issue</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>RESOURCE ARTICLES</subject>
        <subj-group subj-group-type="heading">
          <subject>Molecular and Statistical Advances</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Extending approximate Bayesian computation with supervised machine learning to infer demographic history from genetic polymorphisms using DIYABC Random Forest</article-title>
      <alt-title alt-title-type="left-running-head">COLLIN et al.</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="men13413-cr-0001" contrib-type="author">
        <name>
          <surname>Collin</surname>
          <given-names>François‐David</given-names>
        </name>
        <xref rid="men13413-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="men13413-cr-0002" contrib-type="author">
        <name>
          <surname>Durif</surname>
          <given-names>Ghislain</given-names>
        </name>
        <xref rid="men13413-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="men13413-cr-0003" contrib-type="author">
        <name>
          <surname>Raynal</surname>
          <given-names>Louis</given-names>
        </name>
        <xref rid="men13413-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="men13413-cr-0004" contrib-type="author">
        <name>
          <surname>Lombaert</surname>
          <given-names>Eric</given-names>
        </name>
        <xref rid="men13413-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="men13413-cr-0005" contrib-type="author">
        <name>
          <surname>Gautier</surname>
          <given-names>Mathieu</given-names>
        </name>
        <xref rid="men13413-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="men13413-cr-0006" contrib-type="author">
        <name>
          <surname>Vitalis</surname>
          <given-names>Renaud</given-names>
        </name>
        <xref rid="men13413-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="men13413-cr-0007" contrib-type="author">
        <name>
          <surname>Marin</surname>
          <given-names>Jean‐Michel</given-names>
        </name>
        <xref rid="men13413-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="men13413-cr-0008" contrib-type="author" corresp="yes">
        <name>
          <surname>Estoup</surname>
          <given-names>Arnaud</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4357-6144</contrib-id>
        <xref rid="men13413-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <address>
          <email>arnaud.estoup@inrae.fr</email>
        </address>
      </contrib>
    </contrib-group>
    <aff id="men13413-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">IMAG</named-content>
      <institution>Univ Montpellier</institution>
      <institution>CNRS</institution>
      <institution>UMR 5149</institution>
      <city>Montpellier</city>
      <country country="FR">France</country>
    </aff>
    <aff id="men13413-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <named-content content-type="organisation-division">ISA</named-content>
      <institution>INRAE</institution>
      <institution>CNRS</institution>
      <institution>Univ Côte d'Azur</institution>
      <city>Sophia Antipolis</city>
      <country country="FR">France</country>
    </aff>
    <aff id="men13413-aff-0003">
      <label>
        <sup>3</sup>
      </label>
      <named-content content-type="organisation-division">CBGP</named-content>
      <institution>Univ Montpellier</institution>
      <institution>CIRAD</institution>
      <institution>INRAE</institution>
      <institution>Institut Agro</institution>
      <institution>IRD</institution>
      <city>Montpellier</city>
      <country country="FR">France</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label><bold>Correspondence</bold><break/>
Arnaud Estoup, CBGP, Univ Montpellier, CIRAD, INRAE, Institut Agro, IRD, Montpellier, France<break/>
Email: <email>arnaud.estoup@inrae.fr</email>
<break/>
</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>21</day>
      <month>5</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <issue seq="30">8</issue>
    <issue-id pub-id-type="doi">10.1111/men.v21.8</issue-id>
    <issue-title content-type="special-issue-title">MACHINE LEARNING IN MOLECULAR ECOLOGY</issue-title>
    <fpage>2598</fpage>
    <lpage>2613</lpage>
    <history>
      <date date-type="rev-recd">
        <day>29</day>
        <month>3</month>
        <year>2021</year>
      </date>
      <date date-type="received">
        <day>10</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>28</day>
        <month>4</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <!--Copyright &#x000a9; 2021 John Wiley & Sons Ltd-->
      <copyright-statement content-type="article-copyright">© The Authors. <italic toggle="yes">Molecular Ecology Resources</italic> published by John Wiley &amp; Sons Ltd.</copyright-statement>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link> License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited and is not used for commercial purposes.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="file:MEN-21-2598.pdf"/>
    <abstract id="men13413-abs-0001">
      <title>Abstract</title>
      <p>Simulation‐based methods such as approximate Bayesian computation (ABC) are well‐adapted to the analysis of complex scenarios of populations and species genetic history. In this context, supervised machine learning (SML) methods provide attractive statistical solutions to conduct efficient inferences about scenario choice and parameter estimation. The Random Forest methodology (RF) is a powerful ensemble of SML algorithms used for classification or regression problems. Random Forest allows conducting inferences at a low computational cost, without preliminary selection of the relevant components of the ABC summary statistics, and bypassing the derivation of ABC tolerance levels. We have implemented a set of RF algorithms to process inferences using simulated data sets generated from an extended version of the population genetic simulator implemented in DIYABC v2.1.0. The resulting computer package, named DIYABC Random Forest v1.0, integrates two functionalities into a user‐friendly interface: the simulation under custom evolutionary scenarios of different types of molecular data (microsatellites, DNA sequences or SNPs) and RF treatments including statistical tools to evaluate the power and accuracy of inferences. We illustrate the functionalities of DIYABC Random Forest v1.0 for both scenario choice and parameter estimation through the analysis of pseudo‐observed and real data sets corresponding to pool‐sequencing and individual‐sequencing SNP data sets. Because of the properties inherent to the implemented RF methods and the large feature vector (including various summary statistics and their linear combinations) available for SNP data, DIYABC Random Forest v1.0 can efficiently contribute to the analysis of large SNP data sets to make inferences about complex population genetic histories.</p>
    </abstract>
    <kwd-group>
      <kwd id="men13413-kwd-0001">approximate Bayesian computation</kwd>
      <kwd id="men13413-kwd-0100">demographic history</kwd>
      <kwd id="men13413-kwd-0002">model or scenario selection</kwd>
      <kwd id="men13413-kwd-0003">parameter estimation</kwd>
      <kwd id="men13413-kwd-0004">pool‐sequencing</kwd>
      <kwd id="men13413-kwd-0007">population genetics</kwd>
      <kwd id="men13413-kwd-0005">random forest</kwd>
      <kwd id="men13413-kwd-0006">SNP</kwd>
      <kwd id="men13413-kwd-0008">supervised machine learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="funding-0001">
        <funding-source>INRAE scientific division SPE</funding-source>
        <award-id>AAP‐SPE 2016</award-id>
      </award-group>
      <award-group id="funding-0002">
        <funding-source>LabEx NUMEV</funding-source>
        <award-id>NUMEV</award-id>
        <award-id>ANR10‐LABX‐20</award-id>
      </award-group>
      <award-group id="funding-0003">
        <funding-source>French Agence National pour la Recherche (ANR)</funding-source>
        <award-id>SWING ANR‐16‐CE02‐0015‐01</award-id>
        <award-id>GANDHI ANR‐20‐CE02‐0018</award-id>
        <award-id>ABSint ANR‐18‐CE40‐0034</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="3"/>
      <table-count count="2"/>
      <page-count count="0"/>
      <word-count count="12329"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>November 2021</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.0.9 mode:remove_FC converted:17.11.2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <fn-group id="men13413-ntgp-0001">
      <fn id="men13413-note-0001">
        <p>Jean‐Michel Marin and Arnaud Estoup are joint senior authors of this study</p>
      </fn>
    </fn-group>
  </notes>
</front>
<body id="men13413-body-0001">
  <sec id="men13413-sec-0001">
    <label>1</label>
    <title>INTRODUCTION</title>
    <p>To keep pace with a regular increase of genetic data accessible to biologists, computational methodologies for population genetic inference are constantly and rapidly being developed. Simulation‐based likelihood‐free methods such as approximate Bayesian computation (ABC; Beaumont et al., <xref rid="men13413-bib-0005" ref-type="bibr">2002</xref>) represent an elaborate approach to model‐based inference in a Bayesian setting in which model likelihoods are difficult to calculate and must be estimated by massive simulations. Due to their great flexibility, ABC methods are well adapted to the analysis of complex models (hereafter referred to as scenarios) of populations' and species' histories, in which divergence events, population size changes, and genetic admixture or migration events are suspected (reviewed in Beaumont, <xref rid="men13413-bib-0004" ref-type="bibr">2010</xref>; Bertorelle et al., <xref rid="men13413-bib-0006" ref-type="bibr">2010</xref>; Csilléry et al., <xref rid="men13413-bib-0015" ref-type="bibr">2010</xref>). With the advent of next generation sequencing (NGS) technologies, population genetic data sets have drastically grown in size (both in terms of number of genotyped loci and number of genetically characterized populations), so that ABC users are facing two major problems: (i) the simulation of massive numbers of large data sets constituting a so‐called reference table, as required for “traditional” ABC methods, becomes prohibitive without extensive computational resources, and (ii) the substantial increase in the number of nonindependent statistics used to extract information from the genetic data (an issue also valid for non‐NGS data) poses various statistical problems, including the “curse of dimensionality” whereby accuracy of inferences decreases as the number of summary statistics grows (e.g., Beaumont, <xref rid="men13413-bib-0004" ref-type="bibr">2010</xref>). Although much effort has gone into dimensionality reduction and feature selection for ABC (reviewed in Blum et al., <xref rid="men13413-bib-0009" ref-type="bibr">2013</xref>; Estoup et al., <xref rid="men13413-bib-0017" ref-type="bibr">2012</xref>), reducing dimensionality might lead to loss of information if the remaining summaries fail to capture enough information from the data.</p>
    <p>In this context, supervised machine learning (SML) methods provide attractive solutions for statistical inference. SML methods allow predicting new data points through the use of a training set of labeled simulated data examples, for which true response values are known. This data structure is reminiscent of the ABC reference table. The ability of SML methods to use simulation as a stand‐in for observed data is crucial for population genetics applications, where adequately sized data sets with high‐confidence labels are currently hard to obtain. Most interestingly, some SML methods are able to take advantage of high dimensional input and suffer only slightly from the curse of dimensionality (Anderson et al., <xref rid="men13413-bib-0002" ref-type="bibr">2014</xref>; Chen et al., <xref rid="men13413-bib-0013" ref-type="bibr">2013</xref>; Schrider &amp; Kern, <xref rid="men13413-bib-0039" ref-type="bibr">2018</xref>). SML approaches are currently revolutionizing many fields (e.g., Sebastiani, <xref rid="men13413-bib-0040" ref-type="bibr">2002</xref> in text categorization; Libbrecht &amp; Noble, <xref rid="men13413-bib-0026" ref-type="bibr">2015</xref> in genomics; Angermueller et al., <xref rid="men13413-bib-0003" ref-type="bibr">2016</xref> in genomics and cellular imaging), but their use in population genetics inference is still in its infancy (see for example, Chapuis et al., <xref rid="men13413-bib-0012" ref-type="bibr">2020</xref>; Fraimout et al., <xref rid="men13413-bib-0020" ref-type="bibr">2017</xref>; Pybus et al., <xref rid="men13413-bib-0034" ref-type="bibr">2015</xref>; Schrider &amp; Kern, <xref rid="men13413-bib-0038" ref-type="bibr">2016</xref>, <xref rid="men13413-bib-0039" ref-type="bibr">2018</xref>; Sheehan &amp; Song, <xref rid="men13413-bib-0041" ref-type="bibr">2016</xref>; Schrider et al., <xref rid="men13413-bib-0037" ref-type="bibr">2018</xref>; Smith &amp; Carstens, <xref rid="men13413-bib-0042" ref-type="bibr">2020</xref>; Smith et al., <xref rid="men13413-bib-0043" ref-type="bibr">2017</xref>).</p>
    <p>The Random Forest (RF) approach proposed by Breiman (<xref rid="men13413-bib-0010" ref-type="bibr">2001</xref>) is one of the major SML algorithms for classification (e.g., for scenario choice) or regression (e.g., for estimation of continuous parameters). Pudlo et al. (<xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>) recently developed RF algorithms to perform scenario choice from simulated data sets summarized through a large set of statistics, as typically considered in ABC, hence leading to the so‐called ABC‐RF approach. As compared to classical ABC methods, the ABC‐RF approach enables efficient discrimination among scenarios and estimation of the posterior probability of the best scenario, with a lower computational burden. More specifically, ABC‐RF and other ABC methods provide consistent results for analyses based on a large number of simulated data sets, but ABC‐RF outperforms other ABC methods for analyses of multiple complex scenarios based on a smaller (hence more manageable) number of simulated data sets (Fraimout et al., <xref rid="men13413-bib-0020" ref-type="bibr">2017</xref>; Pudlo et al., <xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>). Building on these results, Raynal et al. (<xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>) recently proposed an extension of the RF approach in a (nonparametric) regression setting to characterize the posterior distributions of parameters of interest under a given scenario. As compared to alternative ABC solutions, the RF method of Raynal et al. (<xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>) offers many advantages: (i) a significant improvement in robustness to the choice of summary statistics, (ii) the nonrequirement of any type of tolerance level, and (iii) a good trade‐off between the precision of point estimates of parameters and the accuracy of credible intervals for a given computational burden.</p>
    <p>The workflow for applying any SML methods to population genetic data includes several stages: (i) the simulation of data under one or several evolutionary scenarios, (ii) the encoding of both simulated and real (observed) data as feature vectors (i.e., summary statistics as in ABC), (iii) the training of the algorithm, applying it on new (observed) data point(s), and (iv) assessing its performance in term of prediction through the computation of error and accuracy measurements. Any effort to create self‐contained, efficient, and user‐friendly software packages capable of performing this entire workflow would streamline SML methods and make them more accessible to researchers, including nonspecialist users. To that end, we have implemented in a new computer package a set of RF algorithms to infer population' histories from genetic polymorphisms, building upon an extended version of the population genetics simulator implemented in DIYABC 2.1.0 (Cornuet et al., <xref rid="men13413-bib-0014" ref-type="bibr">2014</xref>). The data correspond to various types of genetic markers: microsatellites, DNA sequences and SNPs, including individual‐sequencing and pool‐sequencing SNP data (Gautier et al., <xref rid="men13413-bib-0021" ref-type="bibr">2013</xref>; Schlötterer et al., <xref rid="men13413-bib-0036" ref-type="bibr">2014</xref>). A large set of summary statistics has also been implemented to improve the extraction of genetic information from SNP data sets. The resulting package, named DIYABC Random Forest v1.0, integrates two functionalities in a user‐friendly interface: the simulation under custom evolutionary scenarios of polymorphism data (summarized into a large set of descriptive statistics) and RF treatments including various statistical tools to evaluate the power and accuracy of RF‐based inferences. Here we describe the main statistical features of DIYABC Random Forest v1.0 and illustrate its potentialities and functionalities for both scenario choice and parameter estimation through the analyses of pseudo‐observed and real data sets corresponding to pool‐sequencing and individual‐sequencing SNP data.</p>
  </sec>
  <sec sec-type="materials-and-methods" id="men13413-sec-0002">
    <label>2</label>
    <title>MATERIALS AND METHODS</title>
    <sec id="men13413-sec-0003">
      <label>2.1</label>
      <title>ABC random forest in the realm of supervised machine learning</title>
      <p>The guiding idea of supervised machine learning (SML) approaches is to use a set of data made of explanatory variables (input) and response values (output), in order to learn the relationship between these two, and hence emit a predicted response value for each new input of interest. More formally, SML methods learn this relationship thanks to a function, <italic toggle="yes">f</italic>, that predicts a response variable, <italic toggle="yes">y</italic>, from a feature vector, <italic toggle="yes">x</italic>, containing <italic toggle="yes">M</italic> input variables, such that <italic toggle="yes">f</italic>(<italic toggle="yes">x</italic>) = <italic toggle="yes">y</italic>. If <italic toggle="yes">y</italic> is a categorical variable (e.g., for scenario choice), one refers to the task as a classification problem, whereas if <italic toggle="yes">y</italic> is a continuous variable one refers to it as a regression problem (e.g., for parameter estimation). In supervised learning, the objective is to optimize <italic toggle="yes">f</italic>:<italic toggle="yes">x→y</italic> using a training set of labelled data (i.e., whose response values are known). The training set includes values of a feature vector which is a multidimensional representation of any data point made up of measurements (or features) taken from it. That is, one assumes to have a set of training data of length <italic toggle="yes">m</italic> of the form {(<italic toggle="yes">x</italic>
<sub>1</sub>, <italic toggle="yes">y</italic>
<sub>1</sub>),…,(<italic toggle="yes">x</italic>
<sub>m</sub>, <italic toggle="yes">y</italic>
<sub>m</sub>)}, where <italic toggle="yes">x</italic> Є R<italic toggle="yes"><sup>M</sup></italic>. A variety of learning algorithms exist which can generate functions that can perform either classification or regression (reviewed in e.g., Schrider &amp; Kern, <xref rid="men13413-bib-0039" ref-type="bibr">2018</xref>).</p>
      <p>In our inferential framework, SML methods learn from simulations which come from one or several generative model(s) (i.e., scenario[s]). A relevant way to obtain benefits from generative scenario simulations is the Bayesian paradigm and therefore the ABC type approach (Beaumont et al., <xref rid="men13413-bib-0005" ref-type="bibr">2002</xref>). Here, the training set is equivalent to the ABC reference table, which includes a given number of data sets that have been simulated for different scenarios using parameter values drawn from prior distributions, each data set being summarized with a set of descriptive statistics. Random forest (RF; Breiman, <xref rid="men13413-bib-0010" ref-type="bibr">2001</xref>) is considered as a major SML algorithm for classification or regression. Briefly, RF aggregates the predictions of a collection of classification trees or regression trees, depending on whether the output is categorical (e.g., the identity of a finite number of compared scenarios) or quantitative (e.g., the simulated values of a parameter of interest). Each tree is built by using the information provided by a bootstrap sample of the training set and manages to capture one part of the dependency between the output and the covariates of the feature vector. Based on these random trees which are individually poor predictors of output, a random forest is built by aggregating the tree predictions in order to increase the predictive performances to a high level of accuracy, mainly due to the variance reduction of predictions compared to an individual tree (Breiman, <xref rid="men13413-bib-0010" ref-type="bibr">2001</xref>). More detail and in‐depth explanation can be found in Pudlo et al. (<xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>), Fraimout et al. (<xref rid="men13413-bib-0020" ref-type="bibr">2017</xref>), Estoup et al. (<xref rid="men13413-bib-0018" ref-type="bibr">2018</xref>) and Marin et al. (<xref rid="men13413-bib-0027" ref-type="bibr">2018</xref>) for scenario choice, and Raynal et al. (<xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>) for parameter estimation. See also Appendix <xref rid="men13413-sup-0001" ref-type="supplementary-material">S3</xref> of Chapuis et al. (<xref rid="men13413-bib-0012" ref-type="bibr">2020</xref>) for a concise overview of the RF algorithms and statistical developments used in the present study and implemented in the computer package DIYABC Random Forest v1.0.</p>
    </sec>
    <sec id="men13413-sec-0004">
      <label>2.2</label>
      <title>Simulation of the training set</title>
      <p>Before performing RF analyses, one needs to generate a training set. The data sets composing the training set can be simulated under different scenarios and sample configurations, using parameter values drawn from prior distributions. Each resulting data set is summarized using a set of descriptive statistics. We formalized scenarios and prior distributions, and computed summary statistics using the “training set simulation” module of the main pipeline of DIYABC Random Forest v1.0, which essentially corresponds to an extended version of the population genetics simulator implemented in DIYABC v2.1.0 (Cornuet et al., <xref rid="men13413-bib-0014" ref-type="bibr">2014</xref>). As in the latter program, DIYABC Random Forest v1.0 allows consideration of complex population histories including any combination of population divergence events, symmetrical or asymmetrical admixture events (but not any continuous gene flow between populations) and changes in past population size, with population samples potentially collected at different times.</p>
      <p>DIYABC Random Forest v1.0 accepts various types of molecular data (microsatellites, DNA sequences, and SNPs) evolving under various mutation models and located on various chromosome types (autosomal, X or Y chromosomes, and mitochondrial DNA) for diploid or haploid individuals. To simulate polymorphic data sets at a given SNP locus, we follow the algorithm proposed by Hudson (<xref rid="men13413-bib-0023" ref-type="bibr">1993</xref>) – cf. <italic toggle="yes">–s 1</italic> option in the program ms associated to Hudson (<xref rid="men13413-bib-0024" ref-type="bibr">2002</xref>). In DIYABC Random Forest v1.0, it is possible to impose a MAF (minimum allele frequency) criterion on both the observed and simulated data sets. For details, see the user manual of DIYABC Random Forest v1.0 (<ext-link xlink:href="https://diyabc.github.io/doc/" ext-link-type="uri">https://diyabc.github.io/doc/</ext-link>).</p>
      <p>In addition to individual‐sequencing SNP data (hereafter IndSeq data), DIYABC Random Forest v1.0 allows the simulation and analyses of pool‐sequencing SNP data (hereafter PoolSeq data), which consist of whole‐genome sequences of pools of tens to hundreds of individual DNAs (Gautier et al., <xref rid="men13413-bib-0021" ref-type="bibr">2013</xref>; Schlötterer et al., <xref rid="men13413-bib-0036" ref-type="bibr">2014</xref>). In practice, the simulation of PoolSeq data consists first in simulating individual SNP genotypes for all individuals in each population pool, and then generating pool read counts from a binomial distribution parameterized with the simulated allele counts (obtained from individual SNP genotypes) and the total pool read coverage (e.g., Hivert et al., <xref rid="men13413-bib-0022" ref-type="bibr">2018</xref>). To account for variation of the total read coverage across SNPs in the observed data set, the coverages across the pools of a given SNP are randomly drawn from the vectors of SNP coverages composing the observed data set. The “synthetic data file generation” module of the program allows the simulation of various types of pseudo‐observed “raw” data sets (i.e., not summarized through statistics) without referring to any (actual) observed data set. In the case of raw PoolSeq data sets, the total coverage within each pool of each SNP is sampled from a Poisson distribution with a mean corresponding to an arbitrary coverage value (e.g., 100X) fixed by the DIYABC Random Forest v1.0 user.</p>
      <p>It is worth noting that, in contrast to any other types of markers treated in DIYABC Random Forest v1.0 (including IndSeq SNPs), PoolSeq SNP data are considered as located on autosomal chromosomes only. A criterion somewhat similar to the MAF was implemented for PoolSeq data: the minimum read count (MRC) which is the minimum number of sequence reads for each alleles of a SNP when pooling the reads overall population samples. For details, see the user manual of DIYABC Random Forest v1.0 (<ext-link xlink:href="https://diyabc.github.io/doc/" ext-link-type="uri">https://diyabc.github.io/doc/</ext-link>).</p>
    </sec>
    <sec id="men13413-sec-0005">
      <label>2.3</label>
      <title>Components of the feature vector</title>
      <p>The feature vector includes a large number of statistics that summarize genetic variation and capture different aspects of gene genealogies and hence various features of molecular patterns generated by selectively neutral population' histories (e.g., Beaumont, <xref rid="men13413-bib-0004" ref-type="bibr">2010</xref>; Cornuet et al., <xref rid="men13413-bib-0014" ref-type="bibr">2014</xref>). For microsatellite and DNA sequence markers, DIYABC Random Forest v1.0 proposes by default the same set of summary statistics as DIYABC v2.1.0 (Cornuet et al., <xref rid="men13413-bib-0014" ref-type="bibr">2014</xref>). These summary statistics describe genetic variation within populations (e.g., numbers of alleles), between pairs (e.g., genetic distances), or per triplets (e.g., coefficients of admixture) of populations, averaged over loci.</p>
      <p>For both IndSeq and PoolSeq SNPs, we have implemented in DIYABC Random Forest v1.0 an extended set (when compared to DIYABC v2.1.0) of summary statistics to more thoroughly describe genetic variation within populations (e.g., proportion of monomorphic loci, heterozygosity, population‐specific <italic toggle="yes">F</italic>
<sub>ST</sub>) and between pair, triplet or quadruplet of populations (e.g., Nei's distance, <italic toggle="yes">F</italic>
<sub>ST</sub>‐related statistics, Patterson's allele‐sharing <italic toggle="yes">f</italic>‐statistics, coefficients of admixture) to describe genetic variation among various population combinations. More specifically, the proportion of monomorphic loci is computed for each population, as well as for each pair and triplet of populations. Mean and variance (over loci) values are computed for all subsequent summary statistics. Heterozygosity is computed for each population and for each pair of populations as (1−<italic toggle="yes">Q</italic>
<sub>1</sub>) and (1−<italic toggle="yes">Q</italic>
<sub>2</sub>), where <italic toggle="yes">Q</italic>
<sub>1</sub> and <italic toggle="yes">Q</italic>
<sub>2</sub> are the probabilities of identity between pairs of genes (Hivert et al., <xref rid="men13413-bib-0022" ref-type="bibr">2018</xref>). <italic toggle="yes">F</italic>
<sub>ST</sub>‐related statistics are computed for each population (i.e., population‐specific <italic toggle="yes">F</italic>
<sub>ST</sub>; Weir &amp; Goudet, <xref rid="men13413-bib-0046" ref-type="bibr">2017</xref>), as well as for each pair, triplet, quadruplet and overall populations (when the data set includes more than four populations), using the method‐of‐moments estimators described in Hivert et al. (<xref rid="men13413-bib-0022" ref-type="bibr">2018</xref>). In addition, we compute Patterson's <italic toggle="yes">f</italic>‐statistics for each triplet (<italic toggle="yes">f</italic>
<sub>3</sub>‐statistics) and quadruplet (<italic toggle="yes">f</italic>
<sub>4</sub>‐statistics) of populations as described in Patterson et al. (<xref rid="men13413-bib-0031" ref-type="bibr">2012</xref>), except for the <italic toggle="yes">f</italic>
<sub>3</sub>‐statistics for PoolSeq read count data which are computed using the unbiased estimator described in Leblois et al. (<xref rid="men13413-bib-0025" ref-type="bibr">2018</xref>). Finally, distance as in Nei (<xref rid="men13413-bib-0030" ref-type="bibr">1972</xref>) is computed for each pair of populations and the coefficient of admixture is computed for each triplet of populations as described in Cornuet et al. (<xref rid="men13413-bib-0014" ref-type="bibr">2014</xref>). For additional details, see the user manual of DIYABC Random Forest v1.0 (<ext-link xlink:href="https://diyabc.github.io/doc/" ext-link-type="uri">https://diyabc.github.io/doc/</ext-link>). An illustration of the feature vector composed of all above summary statistics is given in Table <xref rid="men13413-sup-0001" ref-type="supplementary-material">S1</xref> for the analysis of two example SNP pseudo‐observed data sets.</p>
      <p>For scenario choice, the feature vector can be expanded by values of the <italic toggle="yes">d</italic> axes of a linear discriminant analysis (LDA) processed on the above summary statistics (with <italic toggle="yes">d</italic> equal to the number of scenarios minus 1; Pudlo et al., <xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>). In the same spirit, for parameter estimation, the feature vector can be completed by values of a subset of the <italic toggle="yes">s</italic> axes of a partial least squares regression analysis (PLS) also processed on the above summary statistics (with <italic toggle="yes">s</italic> equal to the number of summary statistics). The number of PLS axes added to the feature vector is determined as the number of PLS axes providing a given fraction of the maximum amount of variance explained by all PLS axes (i.e., 95% by default, but this parameter can be adjusted).</p>
    </sec>
    <sec id="men13413-sec-0006">
      <label>2.4</label>
      <title>Prediction using random forest</title>
      <p>We used the "random forest analyses" module of the main pipeline of the software DIYABC Random Forest v1.0 to perform RF analyses (i.e. predictions) on a given target data set. For scenario choice, the outcome of the first step of RF computation is a classification vote for each scenario which represents the number of times a scenario is selected in a forest of <italic toggle="yes">n</italic> trees. The scenario with the highest classification vote corresponds to the scenario best suited to the target data set among the set of compared scenarios. This first RF predictor is good enough to select the most likely scenario but not to derive directly the associated posterior probabilities. A second analytical step based on a second random forest in regression is necessary to provide an estimation of the posterior probability of the best‐supported scenario (Pudlo et al., <xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>). Raynal et al. (<xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>) extended the RF approach to estimate the posterior distributions of parameters of interest in a given scenario. Their approach requires the derivation of a new RF for each component of interest of the parameter vector. Practitioners of Bayesian inference often report the posterior mean, posterior variance or posterior quantiles, rather than the full posterior distribution, since the former are easier to interpret than the latter. We implemented the methodologies detailed in Raynal et al. (<xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>) to provide estimations of the posterior mean, variance, median (i.e., 50% quantile) as well as 5% and 95% quantiles (and hence 90% credibility interval) of each parameter of interest. The posterior distribution of each parameter of interest was obtained using importance weights following the work by Meinshausen (<xref rid="men13413-bib-0028" ref-type="bibr">2006</xref>) on quantile regression forests.</p>
    </sec>
    <sec id="men13413-sec-0007">
      <label>2.5</label>
      <title>Assessing the quality of predictions</title>
      <p>For scenario choice and parameter estimation, DIYABC Random Forest v1.0 allows evaluating the robustness of inferences. Because the level of errors on scenario choice and accuracy of parameter estimation may substantially differ depending on the location of an observed data set in the prior data space, prior‐based indicators are poorly relevant, aside from their use to select the best classification method and possibly a set of highly informative components of the feature vector. Therefore, in addition to global (i.e., prior) error/accuracy corresponding to prediction quality measures computed over the entire data space, it is crucial to compute local (i.e., posterior) error/accuracy conditionally on the observed data set, corresponding to prediction quality exactly at the position of the observed data set. For scenario choice, the global prior errors, including the confusion matrix (i.e., the contingency table of the true and predicted classes for each example in the training set) and the mean misclassification error rate, were computed using the out‐of‐bag (a.k.a. out‐of‐bootstrap) training data as a free test data set. The out‐of‐bag data set corresponds to the data of the training set that were not selected when creating the different tree bootstrap samples and is hence equivalent to using an independent test data set (Breiman, <xref rid="men13413-bib-0010" ref-type="bibr">2001</xref>; Pudlo et al., <xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>; Raynal et al., <xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>). Using the out‐of‐bag prediction method for estimating global and local error/accuracy measures is computationally efficient as this approach makes use of the data sets already present in the training set and hence avoids the computationally costly simulations (especially for large SNP data sets) of additional test data sets. Chapuis et al. (<xref rid="men13413-bib-0012" ref-type="bibr">2020</xref>) highlighted that the local (posterior) error for scenario choice can be computed as 1 minus the posterior probability of the selected scenario.</p>
      <p>For parameter estimation, we also relied on out‐of‐bag predictions to compute both global (i.e., prior) and local (i.e., posterior) accuracy measures, as detailed in the Appendix <xref rid="men13413-sup-0001" ref-type="supplementary-material">S3</xref> of Chapuis et al., <xref rid="men13413-bib-0012" ref-type="bibr">2020</xref>. Accuracy measures include: (i) both the global and local NMAE (i.e., the normalized mean absolute error which is the average absolute difference between the point estimate and the true simulated value divided by the true simulated value) with the mean or the median taken as point estimate; (ii) both the global and local MSE and NMSE (i.e., the mean square error which is the average squared difference between the point estimate and the true simulated value for MSE, divided by the true simulated value for NMSE), again with the mean or the median taken as point estimate; and (iii) several confidence interval measures, computed only at the global scale, including the 90% coverage (i.e., the proportion of true simulated values located between the estimated 5% and 95% quantiles), and the mean or the median of the 90% amplitude and relative 90% amplitude (i.e., the mean or median of the difference between the estimated 5% and 95% quantiles for the 90% amplitude, divided by the true simulated value for the relative 90% amplitude).</p>
    </sec>
    <sec id="men13413-sec-0008">
      <label>2.6</label>
      <title>Main technical features of the package DIYABC Random Forest v1.0</title>
      <p>The package DIYABC Random Forest v1.0 is composed of three parts: the data set simulator, the Random Forest inference engine and the graphical user interface. The whole is packaged as a standalone and user‐friendly application available at <ext-link xlink:href="https://diyabc.github.io" ext-link-type="uri">https://diyabc.github.io</ext-link>. The main technical features of the package (implementation, interface, outputs, memory space and computing time) are described in Appendix <xref rid="men13413-sup-0001" ref-type="supplementary-material">S1</xref>.</p>
    </sec>
    <sec id="men13413-sec-0009">
      <label>2.7</label>
      <title>Illustration using pseudo‐observed SNP data sets</title>
      <sec id="men13413-sec-0010">
        <label>2.7.1</label>
        <title>Compared scenarios and prior distributions</title>
        <p>We considered a case study where one wants to make inferences about the genetic origin of a population of interest (for example a recent invasive population) among a set of possible source populations (for which the topology is known; see Figure <xref rid="men13413-fig-0001" ref-type="fig">1</xref>). The target population (pop 4) has three possible single population sources (pop1, 2 or 3) and three possible admixed pairwise population sources (i.e., admixture between pop1 &amp; 2, pop1 &amp; 3 and pop2 &amp; 3). We hence formalized six competing scenarios that constitute two groups of scenarios when referring to the presence or absence of an admixture event when founding the target population 4: group 1 includes three scenarios including an admixture event (scenarios 1, 2 and 3) and group 2 three scenarios without any admixture event (scenarios 4, 5 and 6). Such grouping approach in scenario choice is relevant to disentangle in our analysis the level of confidence to make inferences about a given (or several) specific evolutionary event of interest, here the presence or absence of an admixed origin of population 4 (Chapuis et al., <xref rid="men13413-bib-0012" ref-type="bibr">2020</xref>; Estoup et al., <xref rid="men13413-bib-0018" ref-type="bibr">2018</xref>).</p>
        <fig position="float" fig-type="FIGURE" id="men13413-fig-0001">
          <label>FIGURE 1</label>
          <caption>
            <p>Evolutionary scenarios compared. The target population (pop 4) has three possible single (i.e., nonadmixed) population sources (pop 1, pop 2 or pop 3) composing a group of three scenarios without admixture (group 2 in the figure) and three possible admixed pairwise population sources (i.e., admixture between pop1&amp; pop2, pop 1&amp; pop3 and pop 2 &amp; pop3) composing a group of three scenarios with admixture (group 1 in the figure). Demographic and historical parameters include four effective population sizes <italic toggle="yes">N<sub>1</sub>
</italic>, <italic toggle="yes">N<sub>2</sub>
</italic>, <italic toggle="yes">N<sub>3</sub>
</italic> and <italic toggle="yes">N<sub>4</sub>
</italic> (for populations 1, 2, 3, and 4, respectively) and three divergence or admixture time events (<italic toggle="yes">t<sub>1</sub>
</italic>, <italic toggle="yes">t<sub>2</sub>
</italic> and <italic toggle="yes">t<sub>3</sub>
</italic>), For the scenarios with admixture, the parameter <italic toggle="yes">r<sub>a</sub>
</italic> corresponds to the proportion of genes of a given source population entering into the admixed population 4. See text for details about prior distribution of parameters</p>
          </caption>
          <graphic xlink:href="MEN-21-2598-g002" position="anchor" id="jats-graphic-1"/>
        </fig>
        <p>Demographic and historical parameters include four effective population sizes <italic toggle="yes">N</italic>
<sub>1</sub>, <italic toggle="yes">N</italic>
<sub>2</sub>, <italic toggle="yes">N</italic>
<sub>3</sub> and <italic toggle="yes">N</italic>
<sub>4</sub> (for pop 1, 2, 3, and 4, respectively) and three divergence or admixture time events (<italic toggle="yes">t</italic>
<sub>1</sub>, <italic toggle="yes">t</italic>
<sub>2</sub> and <italic toggle="yes">t</italic>
<sub>3</sub>), with <italic toggle="yes">t</italic>
<sub>1</sub> the divergence or admixture time of pop4, <italic toggle="yes">t</italic>
<sub>2</sub> the divergence time of pop3 from pop2, and <italic toggle="yes">t</italic>
<sub>3</sub> the divergence time of pop2 from pop1 (Figure <xref rid="men13413-fig-0001" ref-type="fig">1</xref>). For the three scenarios with admixture, the parameter <italic toggle="yes">r</italic>
<sub>a</sub> corresponds to the proportion of genes of a given source population entering into the admixed pop4. Prior values for time events (<italic toggle="yes">t</italic>
<sub>1</sub>, <italic toggle="yes">t</italic>
<sub>2</sub>, and <italic toggle="yes">t</italic>
<sub>3</sub>) were drawn from uniform distributions bounded between 10 and 1,000 generations, with <italic toggle="yes">t</italic>
<sub>3</sub>&gt;<italic toggle="yes">t</italic>
<sub>2</sub>&gt;<italic toggle="yes">t</italic>
<sub>1</sub>. We used uniform prior distributions bounded between 1 × 10<sup>2</sup> and 1 × 10<sup>4</sup> diploid individuals for each effective population sizes <italic toggle="yes">N</italic>
<sub>1</sub>, <italic toggle="yes">N</italic>
<sub>2</sub>, <italic toggle="yes">N</italic>
<sub>3</sub> and <italic toggle="yes">N</italic>
<sub>4</sub>. The admixture rate <italic toggle="yes">r</italic>
<sub>a</sub> was drawn from a uniform prior distribution bounded between 0.05 and 0.95.</p>
      </sec>
      <sec id="men13413-sec-0011">
        <label>2.7.2</label>
        <title>Pseudo‐observed data sets</title>
        <p>Our prediction targets correspond to four pseudo‐observed data sets that were generated using the “Synthetic data file generation” module of DIYABC Random Forest v1.0 under the (admixed) scenario 3 or the (nonadmixed) scenario 6 using the following parameter values: <italic toggle="yes">N</italic>
<sub>1</sub> = 7,000, <italic toggle="yes">N</italic>
<sub>2</sub> = 2,000, <italic toggle="yes">N</italic>
<sub>3</sub> = 4,000, <italic toggle="yes">N</italic>
<sub>4</sub> = 3,000, <italic toggle="yes">t</italic>
<sub>1</sub> = 200, <italic toggle="yes">t</italic>
<sub>2</sub> = 300, <italic toggle="yes">t</italic>
<sub>3</sub> = 500, and <italic toggle="yes">r<sub>a</sub>
</italic> = 0.3 for scenario 3. The short divergence times and large effective population sizes values correspond to a situation of low level of genetic differentiation among populations (cf. parwise <italic toggle="yes">F</italic>
<sub>ST</sub> values ranging from 3% to 7%) and hence to a difficult case study. The four pseudo‐observed data sets correspond to a PoolSeq read count data set and an IndSeq allele count data set generated under scenario 3 and under scenario 6, each with 30,000 SNPs. They represent similar sequencing efforts: a 100X coverage for each population of the PoolSeq data sets (with 100 individuals per population pool) and 10 individuals sequenced per population for the IndSeq data sets with a 10X coverage for each sequenced individual (the latter parameter being not explicitly indicated in the program as individual SNP genotypes are considered to be inferred without errors). Analyses were processed on a subset of 5,000 SNPs with a MRC = 5 for the PoolSeq data sets and a MAF = 5% for the IndSeq data sets.</p>
      </sec>
      <sec id="men13413-sec-0012">
        <label>2.7.3</label>
        <title>Scenario choice</title>
        <p>We processed scenario choice analyses grouping scenarios based on the presence or absence of an admixed origin of population 4, and then considered all six scenarios separately. The training sets which included a total of 12,000 simulated data sets (i.e., 2,000 per scenario) were generated using the “Training set simulation” module of DIYABC Random Forest v1.0, drawing parameter values into the prior distributions described above and summarizing SNP data using 130 statistics (see Table <xref rid="men13413-sup-0001" ref-type="supplementary-material">S1</xref>) plus one LDA axis or five LDA axes (i.e., the number of scenarios minus 1; see Pudlo et al., <xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>) computed when comparing the two groups of scenarios or individual scenarios, respectively. We then used the “Random Forest analyses” module of DIYABC Random Forest v1.0 to process RF treatments on the training sets. Following Pudlo et al. (<xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>), we checked that 12,000 data sets in the training set was sufficient by evaluating the stability of prior error rates and posterior probabilities estimations of the best scenario on subsets of 10,000, 11,000 and 12,000 data of the training set (results not shown). The number of trees in the constructed Random Forest was fixed to 1,000, as this number was large enough to ensure a stable estimation of the global error rate (Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S1</xref>). We predicted the best scenario and estimated its posterior probability, as well as the global and local error rates, over ten replicate RF analyses based on the same training set.</p>
        <p>For comparative purposes, we used the R package abc v2.1 to process scenario choice on the same data sets using two traditional ABC methods: the ABC rejection method and the ABC mnlog method based on a simple rejection and a multinomial regression algorithm, respectively (Blum, <xref rid="men13413-bib-0007" ref-type="bibr">2018</xref>; Csilléry et al., <xref rid="men13413-bib-0016" ref-type="bibr">2012</xref>). For all analyses, we used a tolerance rate of 5% and hence the 600 simulated data sets closest to the observed data set. The leave‐one‐out cross‐validation method implemented in abc v2.1 was used to compute global error rates from a sample of 10,000 data sets.</p>
      </sec>
      <sec id="men13413-sec-0013">
        <label>2.7.4</label>
        <title>Parameter estimation</title>
        <p>Following Raynal et al. (<xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>), we conducted independent RF treatments for each parameter of interest. For the sake of concision, we focused our estimations on four parameters involved in the admixture event in scenario 3 (i.e., the selected scenario after processing scenario choice for the pseudo‐observed data sets generated under the admixed scenario 3): the founding/admixture time for the target pop 4 (<italic toggle="yes">t<sub>1</sub>
</italic>), the admixture rate (<italic toggle="yes">r<sub>a</sub>
</italic> corresponding to the proportion of genes originating from pop 1), the effective population size of pop 4 (<italic toggle="yes">N<sub>4</sub>
</italic>), and the compound parameter corresponding to the ratio <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>. The same parameters, except <italic toggle="yes">r<sub>a</sub>
</italic>, were estimated for the pseudo‐observed data sets generated under the nonadmixed scenario 6. Considering ratios (or products) of parameters ‐ here the admixture time scaled by the effective population size as drift parameter ‐ allows reducing parameter identifiability issues of some scenarios (e.g., Beaumont, <xref rid="men13413-bib-0004" ref-type="bibr">2010</xref>). The training sets included 10,000 data sets simulated under scenario 3 or scenario 6, and summarized using the same 130 statistics (Table <xref rid="men13413-sup-0001" ref-type="supplementary-material">S1</xref>) plus 4 to 24 PLS axes depending on the parameter estimated and the training set analysed. For each parameter, we inferred point estimates and computed global and local accuracy metrics corresponding to global and local NMAE (with the mean and the median as point estimates), as well as the 90% coverage, using out‐of‐bag estimators from a sample of 10,000 data. We checked that 10,000 data sets in the training set were sufficient by evaluating the stability of the global accuracy metrics (i.e., NMAE using the mean as point estimates) on subsets of 8,000, 9,000 and 10,000 data of the training set (results not shown). The number of trees in the constructed random forest was fixed to 1,000, as this number wase large enough to ensure a stable estimation of the global accuracy metrics (Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S1</xref>). For each parameter, we conducted ten replicate RF analyses based on the same training set.</p>
        <p>For comparative purposes, we used the R package abc v2.1 to process parameter estimation on the same data sets using the ABC rejection method and the ABC logRidge method based on a simple rejection and a regression with a Ridge regulation algorithm, respectively (Blum, <xref rid="men13413-bib-0007" ref-type="bibr">2018</xref>; Csilléry et al., <xref rid="men13413-bib-0016" ref-type="bibr">2012</xref>). For all analyses, we used a tolerance rate of 5% and hence the 500 simulated data sets closest to the observed data set. We used an independent test data set including 1,000 data sets obtained from prior distributions to compute the global NMAE (with the mean and the median as point estimate) and the 90% coverage as accuracy metrics.</p>
      </sec>
    </sec>
    <sec id="men13413-sec-0014">
      <label>2.8</label>
      <title>Illustration using a real IndSeq SNP data set of human populations</title>
      <p>We analysed an IndSeq real data set including 5,000 SNP markers genotyped in four human populations by The 1000 Genomes Project Consortium (<xref rid="men13413-bib-0044" ref-type="bibr">2012</xref>). The four populations include Yoruba (Africa), Han (East Asia), British (Europe) and American individuals of African ancestry. Our intention is not to bring new insights into human population history, but to illustrate the potential of DIYABC Random Forest in this context. We compared six scenarios of evolution of the four human populations and focused on the estimation of the admixture rate associated to American individuals of African ancestry. The scenarios and prior distribution, the real and simulated IndSeq data sets, and the statistical methods used for inferences, including Random Forest and traditional ABC methods, were similar to those described for the analyses of the pseudo‐observed data sets in section <xref rid="men13413-sec-0009" ref-type="sec">2.7</xref> (Figures <xref rid="men13413-sup-0001" ref-type="supplementary-material">S3 and S4</xref>). See Appendix <xref rid="men13413-sup-0001" ref-type="supplementary-material">S2</xref> for details.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="men13413-sec-0015">
    <label>3</label>
    <title>RESULTS</title>
    <p>For both scenario choice and parameter estimation, we illustrate the inferential power and functionalities of DIYABC Random Forest v1.0 through the analysis of four pseudo‐observed SNP data sets corresponding to PoolSeq and IndSeq data. We first processed RF analyses grouping scenarios based on the presence or absence of an admixed origin of the target population 4, and then considered all six compared scenarios separately. We then estimated parameters of interests under the selected (best) scenario. We contrasted our inferential results with and without adding LDA axes (for scenario choice) or PLS axes (for parameter estimation) to the RF feature vector initially composed of 130 summary statistics. Finally, for comparative purposes, we present results obtained using two traditional ABC methods. In the following sections <xref rid="men13413-sec-0016" ref-type="sec">3.1</xref>, <xref rid="men13413-sec-0017" ref-type="sec">3.2</xref> and <xref rid="men13413-sec-0018" ref-type="sec">3.3</xref>, we detail results for the two pseudo‐observed data sets generated under the (admixed) scenario 3. Similar results were indeed obtained for the two pseudo‐observed data sets generated under the (nonadmixed) scenario 6 (see Tables <xref rid="men13413-sup-0001" ref-type="supplementary-material">S2 and S3</xref>).</p>
    <sec id="men13413-sec-0016">
      <label>3.1</label>
      <title>Scenario choice</title>
      <p>The projection of the data sets of the training set on a single (when analysing the two groups of scenarios) or on the first two LDA axes (when analysing the six scenarios considered separately) provides a first visual indication about our capacity to discriminate among the compared scenarios (Figure <xref rid="men13413-fig-0002" ref-type="fig">2</xref>). Simulations under the two groups of scenarios moderately overlapped suggesting a substantial power to discriminate among them. When considering the six scenarios individually, the projected points overlapped in a more marked way, at least for some of the scenarios, suggesting an overall lower power to discriminate among scenarios considered separately than when considering the two groups of scenarios. As a first inferential clue, the location of the observed data set (indicated by a vertical line and a star symbol in Figure <xref rid="men13413-fig-0002" ref-type="fig">2a,b</xref>, respectively) suggests, albeit without any formal quantification, a marked association with the scenario group 1 and with the scenario 3.</p>
      <fig position="float" fig-type="FIGURE" id="men13413-fig-0002">
        <label>FIGURE 2</label>
        <caption>
          <p>Projection of the PoolSeq data sets from the training set on a single LDA axis when analysing the two groups of scenarios (a) or on the first two LDA axes when analysing the six scenarios separately (b). The six compared scenarios and the two groups of scenarios are detailed in Figure <xref rid="men13413-fig-0001" ref-type="fig">1</xref>. The location of the PoolSeq pseudo‐observed data set in the LDA projection is indicated by a vertical line and a star symbol in panels a and b, respectively. The pseudo‐observed data sets was simulated under the (admixed) scenario 3 (belonging to the group 1) using the following parameter values: <italic toggle="yes">N</italic>
<sub>1</sub> = 7,000, <italic toggle="yes">N</italic>
<sub>2</sub> = 2,000, <italic toggle="yes">N</italic>
<sub>3</sub> = 4,000, <italic toggle="yes">N</italic>
<sub>4</sub> = 3,000, <italic toggle="yes">t</italic>
<sub>1</sub> = 200, <italic toggle="yes">r<sub>a</sub>
</italic> = 0.3, <italic toggle="yes">t</italic>
<sub>2</sub> = 300 and <italic toggle="yes">t</italic>
<sub>3</sub> = 500</p>
        </caption>
        <graphic xlink:href="MEN-21-2598-g001" position="anchor" id="jats-graphic-3"/>
      </fig>
      <p>The RF classification votes and posterior probabilities estimated for both the PoolSeq and IndSeq pseudo‐observed data sets (with or without adding LDA axes to the feature vector) were the highest for the scenario group 1, which includes an admixture event (Table <xref rid="men13413-tbl-0001" ref-type="table">1</xref>). When considering the six scenarios separately, the highest classification votes and posterior probabilities were for scenario 3, which congruently includes an admixture event between the pop 1 &amp; 3 as sources of the target pop 4. The posterior probabilities of scenario group 1 and scenario 3 were relatively high (from 0.657 to 0.891), which is satisfactory when considering the difficulty of the example case study (cf. low level of genetic differentiation among populations). We found that including LDA axes in the RF vector feature substantially improved scenario choice predictions (e.g., global prior error rates were 3% to 12% lower when including LDA axes; Table <xref rid="men13413-tbl-0001" ref-type="table">1</xref>). The levels of errors were considerably different at the global and local scales, with lower levels at the local scale for analyses of the PoolSeq data set, and a trend for higher levels at the local scale for analyses of the IndSeq data set.</p>
      <table-wrap position="float" id="men13413-tbl-0001" content-type="TABLE">
        <label>TABLE 1</label>
        <caption>
          <p>Results for scenario choice</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <thead valign="top">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="top" rowspan="1" colspan="1">Type of data set</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Type of treatment</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Global error rate</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Local error rate</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Vote scen. 1</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Vote scen. 2</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Vote scen. 3</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Vote scen. 4</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Vote scen. 5</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Vote scen. 6</th>
              <th align="left" valign="top" rowspan="1" colspan="1">Posterior probability</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="6" colspan="1">PoolSeq</td>
              <td align="left" colspan="10" rowspan="1">Groups of scenarios: with vs. without admixture</td>
            </tr>
            <tr>
              <td align="left" style="padding-left:10%" rowspan="1" colspan="1">RF with LDA</td>
              <td align="left" rowspan="1" colspan="1">0.172 (0.001)</td>
              <td align="left" rowspan="1" colspan="1">0.085 (0.009)</td>
              <td align="left" colspan="3" rowspan="1">925.1 (10.754)</td>
              <td align="left" colspan="3" rowspan="1">74.9 (10.754)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>.915 (group 1)</p>
                <p>(.009)</p>
              </td>
            </tr>
            <tr>
              <td align="left" style="padding-left:10%" rowspan="1" colspan="1">RF without LDA</td>
              <td align="left" rowspan="1" colspan="1">0.192 (0.001)</td>
              <td align="left" rowspan="1" colspan="1">0.162 (0.015)</td>
              <td align="left" colspan="3" rowspan="1">891.9 (13.585)</td>
              <td align="left" colspan="3" rowspan="1">108.1 (13.585)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>.838 [group 1]</p>
                <p>(.015)</p>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="10" rowspan="1">All scenarios considered separately</td>
            </tr>
            <tr>
              <td align="left" style="padding-left:10%" rowspan="1" colspan="1">RF with LDA</td>
              <td align="left" rowspan="1" colspan="1">0.196 (0.0008)</td>
              <td align="left" rowspan="1" colspan="1">0.135 (0.011)</td>
              <td align="left" rowspan="1" colspan="1">4.1 (1.297)</td>
              <td align="left" rowspan="1" colspan="1">65.3 (7.364)</td>
              <td align="left" rowspan="1" colspan="1">897.0 (8.056)</td>
              <td align="left" rowspan="1" colspan="1">8.5 (2.121)</td>
              <td align="left" rowspan="1" colspan="1">15.7 (4.808)</td>
              <td align="left" rowspan="1" colspan="1">9.4 (2.011)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>.865 [scen. 3]</p>
                <p>(.011)</p>
              </td>
            </tr>
            <tr>
              <td align="left" style="padding-left:10%" rowspan="1" colspan="1">RF without LDA</td>
              <td align="left" rowspan="1" colspan="1">0.220 (0.0009)</td>
              <td align="left" rowspan="1" colspan="1">0.202 (0.013)</td>
              <td align="left" rowspan="1" colspan="1">9.3 (3.020)</td>
              <td align="left" rowspan="1" colspan="1">98.5 (17.264)</td>
              <td align="left" rowspan="1" colspan="1">829.4 (20.304)</td>
              <td align="left" rowspan="1" colspan="1">8.3 (2.669)</td>
              <td align="left" rowspan="1" colspan="1">32.9 (3.381)</td>
              <td align="left" rowspan="1" colspan="1">21.6 (4.671)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>.798 [scen. 3]</p>
                <p>(.013)</p>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="6" colspan="1">IndSeq</td>
              <td align="left" colspan="10" rowspan="1">Groups of scenarios: with vs. without admixture</td>
            </tr>
            <tr>
              <td align="left" style="padding-left:10%" rowspan="1" colspan="1">RF with LDA</td>
              <td align="left" rowspan="1" colspan="1">0.212 (0.001)</td>
              <td align="left" rowspan="1" colspan="1">0.177 (0.016)</td>
              <td align="left" colspan="3" rowspan="1">840.6 (12.816)</td>
              <td align="left" colspan="3" rowspan="1">159.4 (12.816)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>.823 [group 1]</p>
                <p>(.016)</p>
              </td>
            </tr>
            <tr>
              <td align="left" style="padding-left:10%" rowspan="1" colspan="1">RF without LDA</td>
              <td align="left" rowspan="1" colspan="1">0.220 (0.002)</td>
              <td align="left" rowspan="1" colspan="1">0.270 (0.016)</td>
              <td align="left" colspan="3" rowspan="1">805.5 (18.940)</td>
              <td align="left" colspan="3" rowspan="1">194.5 (18.940)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>.730 [group 1]</p>
                <p>(.016)</p>
              </td>
            </tr>
            <tr>
              <td align="left" colspan="10" rowspan="1">All scenarios considered separately</td>
            </tr>
            <tr>
              <td align="left" style="padding-left:10%" rowspan="1" colspan="1">RF with LDA</td>
              <td align="left" rowspan="1" colspan="1">0.248 (0.001)</td>
              <td align="left" rowspan="1" colspan="1">0.268 (0.018)</td>
              <td align="left" rowspan="1" colspan="1">6.9 (3.107)</td>
              <td align="left" rowspan="1" colspan="1">105.9 (10.692)</td>
              <td align="left" rowspan="1" colspan="1">817.0 (13.021)</td>
              <td align="left" rowspan="1" colspan="1">12.7 (3.057)</td>
              <td align="left" rowspan="1" colspan="1">41.2 (5.159)</td>
              <td align="left" rowspan="1" colspan="1">16.3 (3.653)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>.732 [scen. 3]</p>
                <p>(.018)</p>
              </td>
            </tr>
            <tr>
              <td align="left" style="padding-left:10%" rowspan="1" colspan="1">RF without LDA</td>
              <td align="left" rowspan="1" colspan="1">0.262 (0.001)</td>
              <td align="left" rowspan="1" colspan="1">0.343 (0.0197)</td>
              <td align="left" rowspan="1" colspan="1">9.0 (2.981)</td>
              <td align="left" rowspan="1" colspan="1">123.5 (7.322)</td>
              <td align="left" rowspan="1" colspan="1">769.6 (12.366)</td>
              <td align="left" rowspan="1" colspan="1">15.8 (4.049)</td>
              <td align="left" rowspan="1" colspan="1">60.7 (8.982)</td>
              <td align="left" rowspan="1" colspan="1">21.4 (4.274)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>.657 [scen. 3]</p>
                <p>(.020)</p>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="men13413-ntgp-0002">
          <title>Note</title>
          <fn id="men13413-note-0002">
            <p>The six compared scenarios and the two groups of scenarios are detailed in Figure 1 Results are given for the two example pseudo‐observed data sets (PoolSeq and IndSeq) which were simulated under the (admixed) scenario 3 using the following parameter values: <italic toggle="yes">N</italic>
<sub>1</sub> = 7,000, <italic toggle="yes">N</italic>
<sub>2</sub> = 2,000, <italic toggle="yes">N</italic>
<sub>3</sub> = 4,000, <italic toggle="yes">N</italic>
<sub>4</sub> = 3,000, <italic toggle="yes">t</italic>
<sub>1</sub> = 200, <italic toggle="yes">r<sub>a</sub>
</italic> = 0.3, <italic toggle="yes">t</italic>
<sub>2</sub> = 300 and <italic toggle="yes">t</italic>
<sub>3</sub> = 500. In the RF with LDA treatments, five LDA axes were added to the set of 130 summary statistics composing the feature vector. Standard deviations over the 10 replicate analyses are given between brackets for each metrics, in addition to the means. See Table <xref rid="men13413-sup-0001" ref-type="supplementary-material">S2</xref> for a comparison with traditional ABC methods.</p>
          </fn>
        </table-wrap-foot>
        <permissions>
          <copyright-holder>John Wiley &amp; Sons, Ltd</copyright-holder>
        </permissions>
      </table-wrap>
      <p>Finally, we obtained better prediction levels (with or without LDA axes) for the PoolSeq data set than the IndSeq data set (e.g., global prior error rates were 14% to 27% lower for the PoolSeq data set; Table <xref rid="men13413-tbl-0001" ref-type="table">1</xref>). This indicates that, for a similar sequencing effort, a PoolSeq strategy is preferable to an IndSeq strategy, at least when a substantially large number of individual samples are available. This result, which might basically stem from a more accurate estimation of allele frequency when using PoolSeq data, echoes theoretical results in the comparative study by Gautier et al. (<xref rid="men13413-bib-0021" ref-type="bibr">2013</xref>).</p>
      <p>Traditional ABC methods provide qualitatively similar results, but precision metrics were poorer compared to those obtained using ABC Random Forest (Table <xref rid="men13413-sup-0001" ref-type="supplementary-material">S4</xref>).</p>
    </sec>
    <sec id="men13413-sec-0017">
      <label>3.2</label>
      <title>Parameter estimation</title>
      <p>NMAE values show that estimations were substantially more accurate both at the global and local scales for the admixture rate <italic toggle="yes">r<sub>a</sub>
</italic> and the compound parameter <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic> (cf. the low NMAE values for these parameters) than for <italic toggle="yes">t<sub>1</sub>
</italic> and <italic toggle="yes">N<sub>4</sub>
</italic> (Table <xref rid="men13413-tbl-0002" ref-type="table">2</xref>). This result is also illustrated for the two pseudo‐observed data sets by point estimates close to the true values and narrow 90% CI for <italic toggle="yes">r<sub>a</sub>
</italic> and <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>. NMAE values computed from median point estimates were systematically smaller (albeit sometimes only to a small extent) than those computed from mean point estimates, indicating that the median is globally a better point estimate of the parameter than the mean. As expected when considering point estimates for the two pseudo‐observed data sets, this trend did not translate for all parameters.</p>
      <table-wrap position="float" id="men13413-tbl-0002" content-type="TABLE">
        <label>TABLE 2</label>
        <caption>
          <p>Results for estimation of parameters of interest</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <col style="border-right:solid 1px #000000" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" rowspan="2" valign="bottom" colspan="1">Type of data set</th>
              <th align="left" rowspan="2" valign="bottom" colspan="1">Type of treatment</th>
              <th align="left" rowspan="2" valign="bottom" colspan="1">Parameter</th>
              <th align="left" colspan="3" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1">Posterior point estimates of</th>
              <th align="left" colspan="2" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1">Global (prior) NMAE computed from</th>
              <th align="left" colspan="2" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1">Local (posterior) NMAE computed from</th>
              <th align="left" rowspan="2" valign="bottom" colspan="1">90% coverage</th>
            </tr>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Mean</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Median</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">90% CI</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Mean</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Median</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Mean</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Median</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="9" colspan="1">PoolSeq</td>
              <td align="left" rowspan="4" colspan="1">RF with PLS</td>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">r<sub>a</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.346 (0.0018)</td>
              <td align="left" rowspan="1" colspan="1">0.352 (0.0030)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.248–0.422</p>
                <p>(0.0041) (0.0040)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.133 (0.0002)</td>
              <td align="left" rowspan="1" colspan="1">0.123 (0.0002)</td>
              <td align="left" rowspan="1" colspan="1">0.089 (0.0028)</td>
              <td align="left" rowspan="1" colspan="1">0.089 (0.0024)</td>
              <td align="left" rowspan="1" colspan="1">0.974 (0.0008)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">291.4 (3.366)</td>
              <td align="left" rowspan="1" colspan="1">300.5 (2.273)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>147.6–441.0</p>
                <p>(3.777) –(3.887)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.312 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">0.290 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">0.202 (0.0047)</td>
              <td align="left" rowspan="1" colspan="1">0.200 (0.0045)</td>
              <td align="left" rowspan="1" colspan="1">0.960 (0.0009)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">4040 (37.16)</td>
              <td align="left" rowspan="1" colspan="1">3658 (58.55)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>1861–7399</p>
                <p>(90.42) –(161.6)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.416 (0.0005)</td>
              <td align="left" rowspan="1" colspan="1">0.380 (0.0006)</td>
              <td align="left" rowspan="1" colspan="1">0.317 (0.0094)</td>
              <td align="left" rowspan="1" colspan="1">0.285 (0.0093)</td>
              <td align="left" rowspan="1" colspan="1">0.939 (0.0007)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.067 (0.0004)</td>
              <td align="left" rowspan="1" colspan="1">0.068 (0.0005)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.049–0.084</p>
                <p>(0.0010) (0.0006)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.217 (0.0008)</td>
              <td align="left" rowspan="1" colspan="1">0.178 (0.0002)</td>
              <td align="left" rowspan="1" colspan="1">0.079 (0.0020)</td>
              <td align="left" rowspan="1" colspan="1">0.077 (0.0016)</td>
              <td align="left" rowspan="1" colspan="1">0.979 (0.0004)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
RF without PLS
</td>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">r<sub>a</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.364 (0.0028)</td>
              <td align="left" rowspan="1" colspan="1">0.368 (0.0032)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.245–0.483</p>
                <p>(0.0072) (0.0055)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.143 (0.00026)</td>
              <td align="left" rowspan="1" colspan="1">0.130 (0.00028)</td>
              <td align="left" rowspan="1" colspan="1">0.010 (0.0032)</td>
              <td align="left" rowspan="1" colspan="1">0.098 (0.0025)</td>
              <td align="left" rowspan="1" colspan="1">0.973 (0.0004)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">288.2 (3.752)</td>
              <td align="left" rowspan="1" colspan="1">301.7 (2.540)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>134.4–443.0</p>
                <p>(8.044) (4.546)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.322 (0.00042)</td>
              <td align="left" rowspan="1" colspan="1">0.301 (0.00046)</td>
              <td align="left" rowspan="1" colspan="1">0.235 (0.0095)</td>
              <td align="left" rowspan="1" colspan="1">0.231 (0.0091)</td>
              <td align="left" rowspan="1" colspan="1">0.959 (0.0007)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <p>5517</p>
                <p>39.60</p>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <p>5539</p>
                <p>94.16</p>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <p>2319–8662</p>
                <p>(104.3) (133.8)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.456</p>
                <p>0.0006</p>
              </td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.421</p>
                <p>0.0006</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.324 (0.0116)</td>
              <td align="left" rowspan="1" colspan="1">0.297 (0.0091)</td>
              <td align="left" rowspan="1" colspan="1">0.936 (0.0011)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.068 (0.0004)</td>
              <td align="left" rowspan="1" colspan="1">0.068 (0.0006)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.050–0.085</p>
                <p>(0.0008) (0.0006)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.218 (0.0009)</td>
              <td align="left" rowspan="1" colspan="1">0.179 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">0.079 (0.0021)</td>
              <td align="left" rowspan="1" colspan="1">0.078 (0.0017)</td>
              <td align="left" rowspan="1" colspan="1">0.980 (00006)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.073</td>
              <td align="left" rowspan="1" colspan="1">0.073</td>
              <td align="left" rowspan="1" colspan="1">0.069–0.075</td>
              <td align="left" rowspan="1" colspan="1">0.203</td>
              <td align="left" rowspan="1" colspan="1">0.205</td>
              <td align="left" rowspan="1" colspan="1">NC</td>
              <td align="left" rowspan="1" colspan="1">NC</td>
              <td align="left" rowspan="1" colspan="1">0.702</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">IndSeq</td>
              <td align="left" rowspan="1" colspan="1">RF with PLS</td>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">r<sub>a</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.402 (0.0041)</td>
              <td align="left" rowspan="1" colspan="1">0.391 (0.0040)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.275–0.611</p>
                <p>(0.0041) (0.0096)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.172 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">0.154 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">0.161 (0.0021)</td>
              <td align="left" rowspan="1" colspan="1">0.150 (0.0020)</td>
              <td align="left" rowspan="1" colspan="1">0.963 (0.0011)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">400.5 (3.133)</td>
              <td align="left" rowspan="1" colspan="1">395.6 (2.875)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>231.5–574.1</p>
                <p>(4.478) (11.083)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.398 (0.0006)</td>
              <td align="left" rowspan="1" colspan="1">0.357 (0.0006)</td>
              <td align="left" rowspan="1" colspan="1">0.179 (0.0056)</td>
              <td align="left" rowspan="1" colspan="1">0.179 (0.0051)</td>
              <td align="left" rowspan="1" colspan="1">0.957 (0.0008)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">6608 (53.15)</td>
              <td align="left" rowspan="1" colspan="1">6796 (55.61)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>2861–9513</p>
                <p>(111.6) (148.7)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.476 (0.0006)</td>
              <td align="left" rowspan="1" colspan="1">0.442 (0.0007)</td>
              <td align="left" rowspan="1" colspan="1">0.249 (0.0117)</td>
              <td align="left" rowspan="1" colspan="1">0.249 (0.0105)</td>
              <td align="left" rowspan="1" colspan="1">0.927 (0.0008)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.061 (0.0004)</td>
              <td align="left" rowspan="1" colspan="1">0.061 (0.0004)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.044–0.077</p>
                <p>(0.0006) (0.0009)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.262 (0.0009)</td>
              <td align="left" rowspan="1" colspan="1">0.220 (0.0006)</td>
              <td align="left" rowspan="1" colspan="1">0.091 (0.0025)</td>
              <td align="left" rowspan="1" colspan="1">0.090 (0.0025)</td>
              <td align="left" rowspan="1" colspan="1">0.975 (0.0007)</td>
            </tr>
            <tr>
              <td align="left" rowspan="4" colspan="1">RF without PLS</td>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">r<sub>a</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.417 (0.0052)</td>
              <td align="left" rowspan="1" colspan="1">0.410 (0.0041)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.284–0.612</p>
                <p>(0.0050) (0.0143)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.173 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">0.155 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">0.162 (0.0055)</td>
              <td align="left" rowspan="1" colspan="1">0.153 (0.0048)</td>
              <td align="left" rowspan="1" colspan="1">0.963 (0.0007)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">399.0 (3.184)</td>
              <td align="left" rowspan="1" colspan="1">395.2 (3.370)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>227.9–591.9</p>
                <p>(4.653) (4.758)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.407 (0.0005)</td>
              <td align="left" rowspan="1" colspan="1">0.366 (0.0005)</td>
              <td align="left" rowspan="1" colspan="1">0.191 (0.0042)</td>
              <td align="left" rowspan="1" colspan="1">0.190 (0.0041)</td>
              <td align="left" rowspan="1" colspan="1">0.959 (0.0007)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">5837 (50.70)</td>
              <td align="left" rowspan="1" colspan="1">5978 (93.02)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>2524–9210</p>
                <p>(134.6) (166.8)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.499 (0.0006)</td>
              <td align="left" rowspan="1" colspan="1">0.467 (0.0006)</td>
              <td align="left" rowspan="1" colspan="1">0.295 (0.0053)</td>
              <td align="left" rowspan="1" colspan="1">0.292 (0.0051)</td>
              <td align="left" rowspan="1" colspan="1">0.926 (0.0007)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">0.061 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">0.061 (0.0003)</td>
              <td align="left" rowspan="1" colspan="1">
                <p>0.045–0.078</p>
                <p>(0.0005) (0.0008)</p>
              </td>
              <td align="left" rowspan="1" colspan="1">0.263 (0.0008)</td>
              <td align="left" rowspan="1" colspan="1">0.221 (0.0005)</td>
              <td align="left" rowspan="1" colspan="1">0.092 (0.0025)</td>
              <td align="left" rowspan="1" colspan="1">0.092 (0.0024)</td>
              <td align="left" rowspan="1" colspan="1">0.976 (0.0008)</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="men13413-ntgp-0003">
          <title>Note</title>
          <fn id="men13413-note-0003">
            <p>Results are given for the two example pseudo‐observed data sets (PoolSeq and IndSeq) which were simulated under the (admixed) scenario 3 using the following parameter values: <italic toggle="yes">r<sub>a</sub>
</italic> = 0.3, <italic toggle="yes">t</italic>
<sub>1</sub> = 200, <italic toggle="yes">N</italic>
<sub>4</sub> = 3,000 and <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic> = 0.067. In the RF with PLS treatments, the number of PLS axes which were added to the set of 130 summary statistics of the feature vector for the PoolSeq (IndSeq) data sets was equal to 12 (12), 18 (21), 23 (24), and 4 (4) for <italic toggle="yes">r<sub>a</sub>
</italic>, <italic toggle="yes">t<sub>1</sub>
</italic>, <italic toggle="yes">N<sub>4</sub>
</italic> and <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>, respectively. CI: credibility interval. 90% coverage: proportion of test parameter values comprise between the estimated 5% and the 95% quantile. Standard deviations over the 10 replicate analyses are given between brackets for each metrics, in addition to the means. See Table <xref rid="men13413-sup-0001" ref-type="supplementary-material">S3</xref> for a comparison with traditional ABC methods.</p>
          </fn>
        </table-wrap-foot>
        <permissions>
          <copyright-holder>John Wiley &amp; Sons, Ltd</copyright-holder>
        </permissions>
      </table-wrap>
      <p>We found that including PLS axes in the RF feature vector improved parameter estimation in a heterogeneous way. The accuracy gain of including PLS axes ranged from negligible (e.g., IndSeq global NMAE for <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic> based on median of 0.220 and 0.221 with and without PLS, respectively) to substantial (e.g., PoolSeq global NMAE for <italic toggle="yes">N<sub>4</sub>
</italic> based on median of 0.380 and 0.421 with and without PLS, respectively). The accuracy levels were always lower at the global than local scale, sometimes to a large extent (Table <xref rid="men13413-tbl-0002" ref-type="table">2</xref>). In the present case study, the pseudo‐observed data sets are hence located in a favourable part of the prior space. Finally, like scenario choice analyses, we obtained considerably higher accuracy (i.e., lower NMAE values with or without PLS axes) for the PoolSeq data set than the IndSeq data set. In accordance with this, point estimates for all parameters of the two pseudo‐observed data sets were closer to the true values with narrower ranges of 90% CI for PoolSeq than IndSeq data sets. This reinforces our previous conclusion that, for a similar sequencing effort, it is preferable to use a PoolSeq strategy than an IndSeq strategy when a large number of individual samples are available.</p>
      <p>Similar trends were observed when using traditional ABC methods, but the later type of methods were generally characterized by poorer accuracy metrics when compared to those obtained using ABC random forest (Table <xref rid="men13413-sup-0001" ref-type="supplementary-material">S5</xref>).</p>
    </sec>
    <sec id="men13413-sec-0018">
      <label>3.3</label>
      <title>Contribution to random forest inferences of components of the feature vector</title>
      <p>Learning more about how various summary statistics relate to scenarios or parameters would be useful for population genetics going forward. In the realm of traditional ABC methods, it is not clear which summary statistics are responsible for a signal. By contrast, many SML methods including RF allow direct measurement of the contribution of each component included in the feature vector. RF hence offer direct ways to assess which features of the input are driving inferences, information which can yield insights about the underlying processes. Figure <xref rid="men13413-fig-0003" ref-type="fig">3</xref> illustrates how RF automatically ranks the components of the feature vector according to their level of information when building trees of the forest. Figure <xref rid="men13413-fig-0003" ref-type="fig">3</xref> and Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S2</xref> show that informative statistics are different depending on the comparisons (individual scenarios or groups of scenarios) and the analysed parameter in a given scenario. Four‐ and three‐sample <italic toggle="yes">f</italic>‐statistics, as well as the related three‐sample coefficients of admixture (i.e., AML statistics), were among the most informative to discriminate scenarios (Figure <xref rid="men13413-fig-0003" ref-type="fig">3a</xref>). In accordance with this, such statistics are by construction highly sensitive to the topology connecting populations and including or not an admixture event (Estoup et al., <xref rid="men13413-bib-0018" ref-type="bibr">2018</xref>; Patterson et al., <xref rid="men13413-bib-0031" ref-type="bibr">2012</xref>). A typical feature of RF scenario choice is that one or several LDA axes always correspond to the best informative statistics.</p>
      <fig position="float" fig-type="FIGURE" id="men13413-fig-0003">
        <label>FIGURE 3</label>
        <caption>
          <p>Contributions for the PoolSeq data analyses of the 30 most informative statistics to the random forest when choosing among scenarios considered separately (a) and when estimating the parameter <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic> under scenario 3 (b). The variable importance of each statistics is computed as the mean decrease of impurity across the trees, where the impurity measure is the Gini index and the residual sum of squares for scenario choice and parameter inference, respectively. For each variable, the sum of the impurity decrease across every tree of the forest is accumulated every time that variable is chosen to split a node. The sum is divided by the number of trees in the forest to give an average. The scale is irrelevant: only the relative values matter. The variable importance was computed for each of the 130 summary statistics provided by DIYABC Random Forest, plus the LDA axes for scenario choice (denoted LD) or the PLS components for parameter estimation (denoted Comp.) that were added to the feature vector. The higher the variable importance the more informative is the statistic. Population index(s) are indicated at the end of each statistics and correspond to those in Figure <xref rid="men13413-fig-0001" ref-type="fig">1</xref>. More details about summary statistics can be found in Table <xref rid="men13413-sup-0001" ref-type="supplementary-material">S1</xref>. See Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S3</xref> for an illustration of the contributions of the most informative statistics when choosing among the two groups of scenarios and when estimating the parameters <italic toggle="yes">r<sub>a</sub>
</italic>, <italic toggle="yes">t<sub>1</sub>
</italic> and <italic toggle="yes">N<sub>4</sub>
</italic>
<sub>.</sub>
</p>
        </caption>
        <graphic xlink:href="MEN-21-2598-g003" position="anchor" id="jats-graphic-5"/>
      </fig>
      <p>For parameter estimation, the most informative summary statistics were different depending on the parameter of interest (Figure <xref rid="men13413-fig-0003" ref-type="fig">3b</xref> and Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S2</xref>). Figure <xref rid="men13413-fig-0003" ref-type="fig">3b</xref> shows that for the (well‐estimated) compound parameter <italic toggle="yes">t<sub>1</sub>/N<sub>4</sub>
</italic>, the most informative statistics included three‐sample <italic toggle="yes">f</italic>‐statistics and AML statistics with the pop 4 as target, the population‐specific <italic toggle="yes">F</italic>
<sub>ST</sub>, ML1p (proportion of monomorphic loci) and heterozygosity ‐ all for pop 4 ‐, and pairwise‐population statistics (<italic toggle="yes">F</italic>
<sub>ST</sub> and Nei's distance) that included pop 4. For other parameter values, the set of informative statistics differed among parameters, but always included a large number of four‐sample and three‐sample <italic toggle="yes">f</italic>‐statistics, as well as three‐sample AML statistics (Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S2</xref>). In contrast to LDA axes (used for scenario choice), only a subset of PLS components were ranked among the 30 most informative statistics and they were never ranked at first position</p>
      <p>We added five noise variables (corresponding to values randomly drawn into uniform distributions bounded between 0 and 1) to the feature vector processed by RF in order to evaluate the threshold of variable importance metrics below which components of the vector were not informative anymore. We found that for both scenario choice and parameter estimation, a substantial proportion of summary statistics was not informative. We found that 28% to 38% and 20% to 65% of the summary statistics were informative for scenario choice and parameter estimation, respectively. It is worth stressing that noninformative components of the feature vector are simply not or seldom chosen when constructing each individual trees of the forest, and hence do not alter RF inferences (Breiman, <xref rid="men13413-bib-0010" ref-type="bibr">2001</xref>; Marin et al., <xref rid="men13413-bib-0027" ref-type="bibr">2018</xref>; Raynal et al., <xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>). In agreement with this, removing noise variables from the feature vector did not impact the levels of errors in scenario choice and of accuracy in parameter estimation in the present case study (results not shown).</p>
    </sec>
    <sec id="men13413-sec-0019">
      <label>3.4</label>
      <title>Illustration using a real IndSeq SNP data set of human populations</title>
      <p>We analysed an IndSeq real data set including 5,000 SNP markers genotyped in four human populations, including Yoruba (Africa), Han (East Asia), British (Europe) and American individuals of African ancestry. We compared six scenarios of evolution of these populations and focused on the estimation of the admixture rate associated with American individuals of African ancestry (Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S3</xref>). The scenarios and prior distributions, the real and simulated IndSeq data sets, and the statistical methods used for inferences, including Random Forest and two standard ABC methods, are detailed in Appendix <xref rid="men13413-sup-0001" ref-type="supplementary-material">S2</xref>.</p>
      <p>Regarding scenario choice, ABC Random Forest using the LDA axes provides the best results. The RF algorithm selects (according to the number of votes) the group of scenarios including an admixture event and more specifically scenario 2 as the forecasted scenario, an answer suggested visually on the LDA projections of Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S5</xref> in Appendix <xref rid="men13413-sup-0001" ref-type="supplementary-material">S2</xref>. The posterior probability of the selected group of scenarios was 1.000 with LDA axes in the feature vector (global prior error rate = 0.0008) and 1.000 without LDA axes (global prior error rate = 0.0011). The posterior probability of the (admixed) scenario 2 was 0.997 with LDA axes (global prior error rate = 0.042) and 0.995 without LDA axes (global prior error rate = 0.061). Considering previous population genetics studies in the field, it is not surprising that scenario 2, which includes a single out‐of‐Africa colonization event giving an ancestral out‐of‐ Africa population with a secondary split into one European and one East Asian population lineage and a recent genetic admixture of Americans of African origin with their African ancestors and European individuals, was selected (e.g., Bryc et al., <xref rid="men13413-bib-0011" ref-type="bibr">2015</xref>). LDA axes, four‐sample and three‐sample <italic toggle="yes">f</italic>‐statistics, and three‐sample coefficients of admixture (i.e., AML statistics) were among the most informative statistics of the feature vector to discriminate scenarios (Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S6</xref>). Traditional ABC methods provided qualitatively similar results, but precision metrics were poorer compared to those obtained using ABC Random Forest. The posterior probability of the selected admixture group of scenarios was 0.663 with the ABC rejection method (global prior error rate = 0.162) and 1.000 with the ABC multinomial logistic method (global prior error rate = 0.016). The posterior probability of scenario 2 was 0.369 with the rejection method (global prior error rate = 0.321) and 1.000 with the multinomial logistic method (global prior error rate = 0.125). We observed substantial instability of the posterior probability of the best scenario as we found that, when using different threshold for selecting the closest simulated data sets, the posterior probability was always equal to 1.000 but was sometimes associated to a different scenario than scenario 2 (results not shown).</p>
      <p>We then focused on scenario 2 under which we estimated the admixture rate (<italic toggle="yes">r<sub>a</sub>
</italic>) associated to American individuals of African ancestry. Using DIYABC Random Forest and including two PLS axes, the estimations for <italic toggle="yes">r<sub>a</sub>
</italic> were equal to 0.230 (median) and 0.229 (mean), with 95% CIs of 0.201 and 0.261. Without PLS axes, similar estimations were obtained (median = 0.230, mean = 0.231, and 95% CIs [0.203, 0.264]). The latter estimates lay well within previous estimates of the mean proportion of genes of European ancestry within African American individuals, which typically ranged from 0.070 to 0.270 (with most estimates around 0.200), depending on individual exclusions, the population samples and sets of genetic markers considered, as well as the evolutionary models assumed and inferential methods used (reviewed in Bryc et al., <xref rid="men13413-bib-0011" ref-type="bibr">2015</xref>). Global (prior) NMAE values were equal to 0.025 for all types of ABC‐RF computation (i.e., with or without PLS axes and when computed on both median and mean). Local (posterior) NMAE were slightly smaller with PLS axes (0.030 and 0.032 with and without PLS axes, respectively). The 90% coverages were equal to 0.994 with and without PLS axes. The most informative statistics included the first PLS component, three‐sample AML statistics with the population ASW as target, the pairwise‐population statistics (<italic toggle="yes">F</italic>
<sub>ST</sub> and Nei's distance) including the population ASW, and MLp (proportion of monomorphic loci) statistics (Figure <xref rid="men13413-sup-0001" ref-type="supplementary-material">S6</xref>).</p>
      <p>Traditional ABC methods provided estimations of <italic toggle="yes">r<sub>a</sub>
</italic> close to those obtained with ABC random forest: (i) median = 0.240, mean = 0.253 and a large 95% CIs (0.078, 0.445) for ABC rejection, and (ii) median = mean = 0.241 and a very narrow 95% CIs = (0.240, 0.243) for ABC logRidge. NMAE values were large for ABC rejection (0.276 and 0.299 for NMAE on median and mean, respectively) and small for ABC logRidge (0.023 for both NMAE on median and mean). The 90% coverage was equal to 0.96 for ABC rejection and was particularly narrow (i.e., 0.71) for ABC logRidge.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="men13413-sec-0020">
    <label>4</label>
    <title>DISCUSSION</title>
    <p>Population genetics is now poised for an explosion in the use of SML approaches (Schrider &amp; Kern, <xref rid="men13413-bib-0039" ref-type="bibr">2018</xref>). In this context, any effort to create self‐contained, efficient, and user‐friendly software packages capable of performing the entire workflow associated to SML methods would streamline such methods and make them more accessible to researchers, especially for nonspecialist users. For this purpose, we developed the package DIYABC Random Forest v1.0 which integrates, within a user‐friendly interface, a set of methods to simulate training sets for various types of molecular data under custom evolutionary scenarios, encode both the simulated and observed (target) data as large size feature vectors (summary statistics), train RF algorithms, apply them on observed data point(s), and assess their performance in term of prediction (using various metrics to evaluate error and accuracy). We illustrate the main potentialities and functionalities of DIYABC Random Forest v1.0 through the treatments of pseudo‐observed and real data sets corresponding to PoolSeq and IndSeq SNP data sets. Our results indicate that SML methods such as RF show great promise in scenario selection and demographic estimation using genetic data and we argue that they may soon be the preferred choice over alternative methods based on traditional ABC.</p>
    <p>The first advantage of RF is that, given a pool of different metrics available (here various nonindependent summary statistics and their linear combinations), the method extracts the maximum information from the entire set of the proposed component of the feature vector. This avoids the arbitrary choice of a subset of components, which is often applied in ABC analyses. It also minimizes the curse of dimensionality whereby accuracy of inferences decreases as the number of summary statistics grows. As a matter of fact, SML methods such as RF can handle many statistics, even if they are strongly correlated and/or unnecessary (i.e., virtually noninformative), with a limited impact on the performance of the method (Marin et al., <xref rid="men13413-bib-0027" ref-type="bibr">2018</xref>; Raynal et al., <xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>). In practice, and in contrast to traditional ABC methods, SML methods perform better when the input data have a large number of features, in what is commonly called the “blessing of dimensionality” (e.g., Anderson et al., <xref rid="men13413-bib-0002" ref-type="bibr">2014</xref>; Breiman, <xref rid="men13413-bib-0010" ref-type="bibr">2001</xref>). In agreement with this, inputs that consist of thousands of variables have been used with great success (e.g., Amit &amp; Geman, <xref rid="men13413-bib-0001" ref-type="bibr">1997</xref>; Chen et al., <xref rid="men13413-bib-0013" ref-type="bibr">2013</xref>; and unpublished results obtained using feature vectors of &gt;10,000 summary statistics to treat SNP data sets under complex evolutionary scenarios with DIYABC Random Forest v1.0).</p>
    <p>Regarding the composition of the feature vector, defining informative statistics to be included in this vector remains an important issue of any SML method. We have implemented a new set of summary statistics to better extract the genetic information contained in the selectively neutral and independent SNP markers simulated in DIYABC Random Forest v1.0. For both scenario choice and parameter estimation, our results show, at least in the evolutionary contexts we explored, the high level of information content of four‐populations and three‐populations <italic toggle="yes">f</italic>‐statistics (Patterson et al., (<xref rid="men13413-bib-0031" ref-type="bibr">2012</xref>), as well as the related three‐sample AML statistics (Cornuet et al., <xref rid="men13413-bib-0014" ref-type="bibr">2014</xref>). We found that inferences were more accurate with this new set of SNP summary statistics than with the one previously proposed in DYABC v2.1.0 (Cornuet et al., <xref rid="men13413-bib-0014" ref-type="bibr">2014</xref>). For instance, comparative treatments based on the pseudo‐observed IndSeq data set generated under scenario 3, show that error levels were substantially lower and accuracy higher with the new set of SNP summary statistics (results not shown). The addition into the feature vector of linear combinations of statistics (LDA and PLS axes for scenario choice and parameter estimation, respectively) also globally improved our statistical inferences. While the inferential gain was systematic and substantial for LDA axes, we found that including PLS axes in the RF vector feature improved parameter estimation in a heterogeneous way, with a negligible gain in some cases.</p>
    <p>The second advantage of SML methods such as RF is that they naturally use all of the simulations to learn the mapping of data to scenarios and/or parameters. This contrasts to the rejection step of ABC methods which precludes an optimal use of the data sets that are not retained. This advantage remains although work has been done to retain more of the simulations in ABC, for instance by weighing their influence on parameter estimation according to their similarity to the observed data (e.g., Blum &amp; François, <xref rid="men13413-bib-0008" ref-type="bibr">2010</xref>). Consequently, the computing effort is considerably reduced for RF, as the method requires a substantially smaller training set compared to ABC methods (e.g., a few thousand simulated data sets versus hundreds of thousands of simulations per scenario for most ABC approaches; Blum &amp; François, <xref rid="men13413-bib-0008" ref-type="bibr">2010</xref>; Fraimout et al., <xref rid="men13413-bib-0020" ref-type="bibr">2017</xref>; Pudlo et al., <xref rid="men13413-bib-0033" ref-type="bibr">2016</xref>; Raynal et al., <xref rid="men13413-bib-0035" ref-type="bibr">2019</xref>). Given the ever‐increasing dimensionality of modern genetic data generated using NGS technologies, this is a particularly appealing property of SML methods. Moreover, it is worth noting that DIYABC Random Forest v1.0 relies on out‐of‐bag prediction to evaluate the error and accuracy of inferences, so that no additional potentially costly simulations of test data sets are necessary for this purpose.</p>
    <p>RF is often considered as a “tuning‐free” method in the sense that it does not require meticulous calibrations. This represents an important advantage of this method, especially for nonexpert users. On the opposite, ABC methods require calibration to optimize their use, such calibration being time consuming when different levels of tolerance are tested and/or used. In practice, we nevertheless advise users to consider several check points, before finalizing inferential treatments using DIYABC Random Forest v1.0. These are detailed in Appendix <xref rid="men13413-sup-0001" ref-type="supplementary-material">S3</xref>.</p>
    <p>Various SML methods have been recently developed (e.g., Schrider &amp; Kern, <xref rid="men13413-bib-0039" ref-type="bibr">2018</xref>; Wang et al., <xref rid="men13413-bib-0045" ref-type="bibr">2021</xref>). In particular, neural networks are machine learning methods which are used increasingly in population genetics, often under the term “deep learning” (Sheehan &amp; Song, <xref rid="men13413-bib-0041" ref-type="bibr">2016</xref>), and sometimes using an ABC framework (Mondal et al., <xref rid="men13413-bib-0029" ref-type="bibr">2019</xref>). Deep learning, with its incredibly flexible input and output structure, is expected to be an important area of future research in many different fields including population genetics (e.g., Angermueller et al., <xref rid="men13413-bib-0003" ref-type="bibr">2016</xref>; Flagel et al., <xref rid="men13413-bib-0019" ref-type="bibr">2018</xref>; Schrider &amp; Kern, <xref rid="men13413-bib-0039" ref-type="bibr">2018</xref>; Sheehan &amp; Song, <xref rid="men13413-bib-0041" ref-type="bibr">2016</xref>). In contrast to RF, deep learning methods are not tuning‐free and often require meticulous calibrations, including the specification of the number of layers composing the neural network, as well as thorough investigation of the regularization parameter of the cost function. Moreover, deep learning methods require data sets of larger size and substantially larger computing resources than RF. We hence believe that RF remains one of the most competitive SML methods when no tuning of parameters is desired. The RF method remains particularly attractive for nonexpert machine‐learning users, especially when it is embedded in an integrative user‐friendly interfaced program such as DIYABC Random Forest 1.0.</p>
    <p>In conclusion, although SML approaches are revolutionizing many fields, their use in population genetics inference is still in its infancy (Schrider &amp; Kern, <xref rid="men13413-bib-0039" ref-type="bibr">2018</xref>). However, the recent successes of SML approaches in the latter scientific field demonstrate that they have the potential to revolutionize the practice of population genetic data analysis. In particular, SML methods such as RF may soon be the preferred choice over ABC method in scenario selection and demographic estimation, especially when analysing multiple complex scenarios and large‐size data sets. In this context, DIYABC Random Forest v1.0 provides an integrative operational solution streamlining the entire workflow to applying RF methods to various types of population genetic data. We believe that because of the general properties of the implemented RF methods and the large set of summary statistics available for SNP data, DIYABC Random Forest v1.0 represents a useful resource to make efficient inferences about population genetic history from high dimensional genetic data sets, as typically obtained from NGS technologies.</p>
  </sec>
  <sec id="men13413-sec-0022">
    <title>AUTHOR CONTRIBUTIONS</title>
    <p>François‐David Collin, Louis Raynal, Jean‐Michel Marin and Arnaud Estoup were responsible for the conceptualization, F‐D.C., L.R., J‐M.M. and A.E.; François‐David Collin was responsible for the core program coding and Ghislain Durif for the interface coding; Mathieu Gautier, Renaud Vitalis and Arnaud Estoup provided the new SNP summary statistics: François‐David Collin, Eric Lombaert and Arnaud Estoup were responsible for program debugging and testing: Arnaud Estoup was responsible for the example data set analysis, Arnaud Estoup wrote the original draft of the manuscript; François‐David Collin, Louis Raynal, Ghislain Durif, Mathieu Gautier, Renaud Vitalis, Eric Lombaert, Jean‐Michel Marin and Arnaud Estoup wrote, reviewed and edited the manuscript, Jean‐Michel Marin and Arnaud Estoup were responsible for funding acquisition.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="men13413-sup-0001" position="float" content-type="local-data">
      <caption>
        <p>App S1</p>
      </caption>
      <media xlink:href="MEN-21-2598-s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="men13413-sec-0021">
    <title>ACKNOWLEDGEMENTS</title>
    <p>This work was supported by funds from the French Agence National pour la Recherche (projects SWING ANR‐16‐CE02‐0015‐01, GANDHI ANR‐20‐CE02‐0018 and ABSint ANR‐18‐CE40‐0034), and the LabEx NUMEV (NUMEV, ANR10‐LABX‐20). We thank Pierre Pudlo for useful discussions, and Jean‐Marie Cornuet and Alex Dehne Garcia for computer code expertise at the onset of the ABC Random Forest project.</p>
  </ack>
  <sec sec-type="data-availability" id="men13413-sec-0024">
    <title>DATA AVAILABILITY STATEMENT</title>
    <p>For the pseudo‐observed and real PoolSeq and IndSeq data sets used as examples in this paper, we provide the corresponding pseudo‐observed data sets (read numbers or genotype data and summary statistics: i.e., &lt;file_name.snp&gt;and statobsRF.txt), the headerRF files (headerRF.txt) and the training set files (reftableRF.bin) at <ext-link xlink:href="https://github.com/diyabc/MER_publication_materials/tree/main/MER_2021_DATASET_EXAMPLES" ext-link-type="uri" specific-use="software is-supplemented-by">https://github.com/diyabc/MER_publication_materials/tree/main/MER_2021_DATASET_EXAMPLES</ext-link>.</p>
  </sec>
  <ref-list content-type="cited-references" id="men13413-bibl-0001">
    <title>REFERENCES</title>
    <ref id="men13413-bib-0001">
      <mixed-citation publication-type="journal" id="men13413-cit-0001"><string-name><surname>Amit</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Geman</surname>, <given-names>D.</given-names></string-name> (<year>1997</year>). <article-title>Shape quantization and recognition with randomized trees</article-title>. <source>Neural Computation</source>, <volume>9</volume>, <fpage>1545</fpage>–<lpage>1588</lpage>.</mixed-citation>
    </ref>
    <ref id="men13413-bib-0002">
      <mixed-citation publication-type="book" id="men13413-cit-0002"><string-name><surname>Anderson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Belkin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Goyal</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Rademacher</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Voss</surname>, <given-names>J.</given-names></string-name> (<year>2014</year>). <part-title>The more, the merrier: the blessing of dimensionality for learning large Gaussian mixtures</part-title>. <source>Proceedings of The 27th Conference on Learning Theory</source> (pp. <fpage>1135</fpage>–<lpage>1164</lpage>). <publisher-name>PMLR 35</publisher-name>.</mixed-citation>
    </ref>
    <ref id="men13413-bib-0003">
      <mixed-citation publication-type="journal" id="men13413-cit-0003"><string-name><surname>Angermueller</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Parnamaa</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Parts</surname>, <given-names>L.</given-names></string-name>, &amp; <string-name><surname>Stegle</surname>, <given-names>O.</given-names></string-name> (<year>2016</year>). <article-title>Deep learning for computational biology</article-title>. <source>Molecular Systems Biology</source>, <volume>12</volume>(<issue>7</issue>), <fpage>878</fpage>. <pub-id pub-id-type="doi">10.15252/msb.20156651</pub-id>
<pub-id pub-id-type="pmid">27474269</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0004">
      <mixed-citation publication-type="journal" id="men13413-cit-0004"><string-name><surname>Beaumont</surname>, <given-names>M.</given-names></string-name> (<year>2010</year>). <article-title>Approximate Bayesian computation in evolution and ecology</article-title>. <source>Annual Review of Ecology, Evolution, and Systematics</source>, <volume>41</volume>(<issue>1</issue>), <fpage>379</fpage>–<lpage>406</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-ecolsys-102209-144621</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0005">
      <mixed-citation publication-type="journal" id="men13413-cit-0005"><string-name><surname>Beaumont</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>W.</given-names></string-name>, &amp; <string-name><surname>Balding</surname>, <given-names>D. J.</given-names></string-name> (<year>2002</year>). <article-title>Approximate Bayesian computation in population genetics</article-title>. <source>Genetics</source>, <volume>162</volume>(<issue>4</issue>), <fpage>2025</fpage>–<lpage>2035</lpage>.PMID: 12524368.<pub-id pub-id-type="pmid">12524368</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0006">
      <mixed-citation publication-type="journal" id="men13413-cit-0006"><string-name><surname>Bertorelle</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Benazzo</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Mona</surname>, <given-names>S.</given-names></string-name> (<year>2010</year>). <article-title>ABC as a flexible framework to estimate demography over space and time: Some cons, many pros</article-title>. <source>Molecular Ecology</source>, <volume>19</volume>(<issue>13</issue>), <fpage>2609</fpage>–<lpage>2625</lpage>. <pub-id pub-id-type="doi">10.1111/j.1365-294X.2010.04690.x</pub-id>
<pub-id pub-id-type="pmid">20561199</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0007">
      <mixed-citation publication-type="book" id="men13413-cit-0007"><string-name><surname>Blum</surname>, <given-names>M. G. B.</given-names></string-name> (<year>2018</year>). <part-title>Regression approaches for ABC</part-title>. In <person-group person-group-type="editor"><string-name><given-names>S.</given-names><surname>Sisson</surname></string-name></person-group>, <person-group person-group-type="editor"><string-name><given-names>Y.</given-names><surname>Fan</surname></string-name></person-group>, &amp; <person-group person-group-type="editor"><string-name><given-names>M.</given-names><surname>Beaumont</surname></string-name></person-group> (Eds.), <source>Handbook of Approximate Bayesian Computation</source>. <publisher-name>Chapman and Hall/CRC</publisher-name>. <pub-id pub-id-type="doi">10.1201/9781315117195</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0008">
      <mixed-citation publication-type="journal" id="men13413-cit-0008"><string-name><surname>Blum</surname>, <given-names>M. G. B.</given-names></string-name>, &amp; <string-name><surname>François</surname>, <given-names>O.</given-names></string-name> (<year>2010</year>). <article-title>Non‐linear regression models for approximate Bayesian computation</article-title>. <source>Statistics and Computing</source>, <volume>20</volume>(<issue>1</issue>), <fpage>63</fpage>–<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1007/s11222-009-9116-0</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0009">
      <mixed-citation publication-type="journal" id="men13413-cit-0009"><string-name><surname>Blum</surname>, <given-names>M. G. B.</given-names></string-name>, <string-name><surname>Nunes</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Prangle</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Sisson</surname>, <given-names>S. A.</given-names></string-name> (<year>2013</year>). <article-title>A comparative review of dimension reduction methods in approximate Bayesian computation</article-title>. <source>Statistical Science</source>, <volume>28</volume>(<issue>2</issue>), <fpage>189</fpage>–<lpage>208</lpage>. <pub-id pub-id-type="doi">10.1214/12-STS406</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0010">
      <mixed-citation publication-type="journal" id="men13413-cit-0010"><string-name><surname>Breiman</surname>, <given-names>L.</given-names></string-name> (<year>2001</year>). <article-title>Random forests</article-title>. <source>Machine Learning</source>, <volume>45</volume>(<issue>1</issue>), <fpage>5</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0011">
      <mixed-citation publication-type="journal" id="men13413-cit-0011"><string-name><surname>Bryc</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Durand</surname>, <given-names>E. Y.</given-names></string-name>, <string-name><surname>Macpherson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Reich</surname>, <given-names>D.</given-names></string-name>, &amp; <string-name><surname>Mountain</surname>, <given-names>J. L.</given-names></string-name> (<year>2015</year>). <article-title>The genetic ancestry of African Americans, Latinos, and European Americans across the United States</article-title>. <source>American Journal of Human Genetics</source>, <volume>96</volume>(<issue>1</issue>), <fpage>37</fpage>–<lpage>53</lpage>. <pub-id pub-id-type="doi">10.1016/j.ajhg.2014.11.010</pub-id>
<pub-id pub-id-type="pmid">25529636</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0012">
      <mixed-citation publication-type="journal" id="men13413-cit-0012"><string-name><surname>Chapuis</surname>, <given-names>M.‐P.‐ R.</given-names></string-name>, <string-name><surname>Raynal</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Plantamp</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Meynard</surname>, <given-names>C. N.</given-names></string-name>, <string-name><surname>Blondin</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marin</surname>, <given-names>J.‐M.</given-names></string-name>, &amp; <string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name> (<year>2020</year>). <article-title>A young age of subspecific divergence in the desert locust <italic toggle="yes">Schistocerca gregaria</italic>, inferred by ABC Random Forest</article-title>. <source>Molecular Ecology</source>, <volume>29</volume>(<issue>23</issue>), <fpage>4542</fpage>–<lpage>4558</lpage>. <pub-id pub-id-type="doi">10.1111/mec.15663</pub-id>. Previous version reviewed and recommended by Peer Community in Evolutionary Biology, bioRxiv, 671867. <pub-id pub-id-type="doi">10.24072/pci.evolbiol.100091</pub-id>
<pub-id pub-id-type="pmid">33000872</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0013">
      <mixed-citation publication-type="book" id="men13413-cit-0013"><string-name><surname>Chen</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Cao</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Wen</surname>, <given-names>F.</given-names></string-name>, &amp; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name> (<year>2013</year>). <part-title>Blessing of dimensionality: high‐dimensional feature and its efficient compression for face verification</part-title>. <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source> (pp. <fpage>3025</fpage>–<lpage>3032</lpage>). <publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="men13413-bib-0014">
      <mixed-citation publication-type="journal" id="men13413-cit-0014"><string-name><surname>Cornuet</surname>, <given-names>J.‐M.</given-names></string-name>, <string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Veyssier</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dehne‐Garcia</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gautier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Leblois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Marin</surname>, <given-names>J. M.</given-names></string-name>, &amp; <string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name> (<year>2014</year>). <article-title>DIYABC v2.0: a software to make approximate Bayesian computation inferences about population history using single nucleotide polymorphism, DNA sequence and microsatellite data</article-title>. <source>Bioinformatics</source>, <volume>30</volume>(<issue>8</issue>), <fpage>1187</fpage>–<lpage>1189</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btt763</pub-id>
<pub-id pub-id-type="pmid">24389659</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0015">
      <mixed-citation publication-type="journal" id="men13413-cit-0015"><string-name><surname>Csilléry</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Blum</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Gaggiotti</surname>, <given-names>O. E.</given-names></string-name>, &amp; <string-name><surname>François</surname>, <given-names>O.</given-names></string-name> (<year>2010</year>). <article-title>Approximate Bayesian Computation (ABC) in practice</article-title>. <source>Trends in Ecology &amp; Evolution</source>, <volume>25</volume>(<issue>7</issue>), <fpage>410</fpage>–<lpage>418</lpage>. <pub-id pub-id-type="doi">10.1016/j.tree.2010.04.001</pub-id>
<pub-id pub-id-type="pmid">20488578</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0016">
      <mixed-citation publication-type="journal" id="men13413-cit-0016"><string-name><surname>Csilléry</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>François</surname>, <given-names>O.</given-names></string-name>, &amp; <string-name><surname>Blum</surname>, <given-names>M. G.</given-names></string-name> (<year>2012</year>). <article-title>abc: an r package for approximate bayesiancomputation (abc)</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>3</volume>(<issue>3</issue>), <fpage>475</fpage>–<lpage>479</lpage>. <pub-id pub-id-type="doi">10.1111/j.2041-210X.2011.00179.x</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0017">
      <mixed-citation publication-type="journal" id="men13413-cit-0017"><string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lombaert</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Marin</surname>, <given-names>J.‐M.</given-names></string-name>, <string-name><surname>Robert</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Guillemaud</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Cornuet</surname>, <given-names>J.‐M.</given-names></string-name> (<year>2012</year>). <article-title>Estimation of demo‐genetic model probabilities with Approximate Bayesian Computation using linear discriminant analysis on summary statistics</article-title>. <source>Molecular Ecology Resources</source>, <volume>12</volume>(<issue>5</issue>), <fpage>846</fpage>–<lpage>855</lpage>. <pub-id pub-id-type="doi">10.1111/j.1755-0998.2012.03153.x</pub-id>
<pub-id pub-id-type="pmid">22571382</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0018">
      <mixed-citation publication-type="journal" id="men13413-cit-0018"><string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Raynal</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Verdu</surname>, <given-names>P.</given-names></string-name>, &amp; <string-name><surname>Marin</surname>, <given-names>J.‐M.</given-names></string-name> (<year>2018</year>). <article-title>Model choice using Approximate Bayesian Computation and Random Forests: Analyzes based on model grouping to make inferences about the genetic history of Pygmy human populations</article-title>. <source>Journal de la Société Française de Statistiques</source>, <volume>159</volume>(<issue>3</issue>), <fpage>167</fpage>–<lpage>190</lpage>.</mixed-citation>
    </ref>
    <ref id="men13413-bib-0019">
      <mixed-citation publication-type="journal" id="men13413-cit-0019"><string-name><surname>Flagel</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Brandvain</surname>, <given-names>Y.</given-names></string-name>, &amp; <string-name><surname>Schrider</surname>, <given-names>D. R.</given-names></string-name> (<year>2018</year>). <article-title>The unreasonable effectiveness of convolutional neural networks in population genetic inference</article-title>. <source>Molecular Biology and Evolution</source>, <volume>36</volume>(<issue>2</issue>), <fpage>220</fpage>–<lpage>238</lpage>. <pub-id pub-id-type="doi">10.1093/molbev/msy224</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0020">
      <mixed-citation publication-type="journal" id="men13413-cit-0020"><string-name><surname>Fraimout</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Debat</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Fellous</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hufbauer</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Foucaud</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Marin</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Price</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Cattel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Chen</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Depra</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Duyck</surname>, <given-names>P. F.</given-names></string-name>, <string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Guedot</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kenis</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kimura</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Loeb</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Loiseau</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <article-title>Deciphering the routes of invasion of <italic toggle="yes">Drosophila</italic>
<italic toggle="yes">suzukii</italic> by means of ABC Random Forest</article-title>. <source>Molecular Biology and Evolution</source>, <volume>34</volume>(<issue>4</issue>), <fpage>980</fpage>–<lpage>996</lpage>. <pub-id pub-id-type="doi">10.1093/molbev/msx050</pub-id>
<pub-id pub-id-type="pmid">28122970</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0021">
      <mixed-citation publication-type="journal" id="men13413-cit-0021"><string-name><surname>Gautier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Foucaud</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gharbi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Cézard</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Galan</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Loiseau</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Thomson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kerdelhué</surname>, <given-names>C.</given-names></string-name>, &amp; <string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name> (<year>2013</year>). <article-title>Estimation of population allele frequencies from next‐generation sequencing data: pool‐versus individual‐based genotyping</article-title>. <source>Molecular Ecology</source>, <volume>22</volume>(<issue>14</issue>), <fpage>3766</fpage>–<lpage>3779</lpage>. <pub-id pub-id-type="doi">10.1111/mec.12360</pub-id>
<pub-id pub-id-type="pmid">23730833</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0022">
      <mixed-citation publication-type="journal" id="men13413-cit-0022"><string-name><surname>Hivert</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Leblois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Petit</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Gautier</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Vitalis</surname>, <given-names>R.</given-names></string-name> (<year>2018</year>). <article-title>Measuring genetic differentiation from pool‐seq data</article-title>. <source>Genetics</source>, <volume>210</volume>(<issue>1</issue>), <fpage>31</fpage>–<lpage>330</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.118.300900</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0023">
      <mixed-citation publication-type="book" id="men13413-cit-0023"><string-name><surname>Hudson</surname>, <given-names>R. R.</given-names></string-name> (<year>1993</year>). <part-title>The how and why of generating gene genealogies</part-title>. In <person-group person-group-type="editor"><string-name><given-names>N.</given-names><surname>Takahata</surname></string-name></person-group>, &amp; <person-group person-group-type="editor"><string-name><given-names>A. G.</given-names><surname>Clark</surname></string-name></person-group> (Eds.), <source>Mechanisms of Molecular Evolution</source> (pp. <fpage>23</fpage>–<lpage>36</lpage>). <publisher-name>Sinauer Associates</publisher-name>.</mixed-citation>
    </ref>
    <ref id="men13413-bib-0024">
      <mixed-citation publication-type="journal" id="men13413-cit-0024"><string-name><surname>Hudson</surname>, <given-names>R.</given-names></string-name> (<year>2002</year>). <article-title>Generating samples under a Wright‐Fisher neutral model of genetic variation</article-title>. <source>Bioinformatics</source>, <volume>18</volume>(<issue>2</issue>), <fpage>337</fpage>–<lpage>338</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/18.2.337</pub-id>
<pub-id pub-id-type="pmid">11847089</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0025">
      <mixed-citation publication-type="journal" id="men13413-cit-0025"><string-name><surname>Leblois</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Gautier</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rohfritsch</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Foucaud</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Burban</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Galan</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Kerdelhué</surname>, <given-names>C.</given-names></string-name> (<year>2018</year>). <article-title>Deciphering the demographic history of allochronic differentiation in the pine processionary moth <italic toggle="yes">Thaumetopoea pityocampa</italic>
</article-title>. <source>Molecular Ecology</source>, <volume>27</volume>(<issue>1</issue>), <fpage>264</fpage>–<lpage>278</lpage>. <pub-id pub-id-type="doi">10.1111/mec.14411</pub-id>
<pub-id pub-id-type="pmid">29113013</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0026">
      <mixed-citation publication-type="journal" id="men13413-cit-0026"><string-name><surname>Libbrecht</surname>, <given-names>M. W.</given-names></string-name>, &amp; <string-name><surname>Noble</surname>, <given-names>W. S.</given-names></string-name> (<year>2015</year>). <article-title>Machine learning applications in genetics and genomics</article-title>. <source>Nature Reviews Genetics</source>, <volume>16</volume>(<issue>6</issue>), <fpage>321</fpage>–<lpage>332</lpage>. <pub-id pub-id-type="doi">10.1038/nrg3920</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0027">
      <mixed-citation publication-type="book" id="men13413-cit-0027"><string-name><surname>Marin</surname>, <given-names>J.‐M.</given-names></string-name>, <string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name>, &amp; <string-name><surname>Robert</surname>, <given-names>C. P.</given-names></string-name> (<year>2018</year>). <part-title>Likelihood‐free model choice</part-title>. In handbook of approximate Bayesian computation. In <person-group person-group-type="editor"><string-name><given-names>S.</given-names><surname>Sisson</surname></string-name></person-group>, <person-group person-group-type="editor"><string-name><given-names>Y.</given-names><surname>Fan</surname></string-name></person-group> &amp; <person-group person-group-type="editor"><string-name><given-names>M.</given-names><surname>Beaumont</surname></string-name></person-group> (Eds), <source>Handbook of Approximate Bayesian Computation</source>. <publisher-name>Chapman and Hall/CRC</publisher-name>. <pub-id pub-id-type="doi">10.1201/9781315117195</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0028">
      <mixed-citation publication-type="journal" id="men13413-cit-0028"><string-name><surname>Meinshausen</surname>, <given-names>N.</given-names></string-name> (<year>2006</year>). <article-title>Quantile regression forests</article-title>. <source>Journal of Machine Learning Research</source>, <volume>7</volume>, <fpage>983</fpage>–<lpage>999</lpage>. <pub-id pub-id-type="doi">10.5555/1248547.1248582</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0029">
      <mixed-citation publication-type="journal" id="men13413-cit-0029"><string-name><surname>Mondal</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bertranpetit</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Lao</surname>, <given-names>O.</given-names></string-name> (<year>2019</year>). <article-title>Approximate Bayesian Computation with deep learning supports a third archaic introgression in Asia and Oceania</article-title>. <source>Nature Communications</source>, <volume>10</volume>, <fpage>246</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-018-08089-7</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0030">
      <mixed-citation publication-type="journal" id="men13413-cit-0030"><string-name><surname>Nei</surname>, <given-names>M.</given-names></string-name> (<year>1972</year>). <article-title>Genetic distance between populations</article-title>. <source>American Naturalist</source>, <volume>106</volume>, <fpage>283</fpage>–<lpage>292</lpage>. <pub-id pub-id-type="doi">10.1086/282771</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0031">
      <mixed-citation publication-type="journal" id="men13413-cit-0031"><string-name><surname>Patterson</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Moorjani</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Mallick</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Rohland</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Zhan</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Genschoreck</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Webster</surname>, <given-names>T.</given-names></string-name>, &amp; <string-name><surname>Reich</surname>, <given-names>D.</given-names></string-name> (<year>2012</year>). <article-title>Ancient admixture in human history</article-title>. <source>Genetics</source>, <volume>192</volume>(<issue>3</issue>), <fpage>1065</fpage>–<lpage>1093</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.112.145037</pub-id>
<pub-id pub-id-type="pmid">22960212</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0033">
      <mixed-citation publication-type="journal" id="men13413-cit-0033"><string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Marin</surname>, <given-names>J.‐M.</given-names></string-name>, <string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cornuet</surname>, <given-names>J.‐M.</given-names></string-name>, <string-name><surname>Gautier</surname>, <given-names>M.</given-names></string-name>, &amp; <string-name><surname>Robert</surname>, <given-names>C. P.</given-names></string-name> (<year>2016</year>). <article-title>Reliable ABC model choice via random forests</article-title>. <source>Bioinformatics</source>, <volume>32</volume>(<issue>6</issue>), <fpage>859</fpage>–<lpage>866</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btv684</pub-id>
<pub-id pub-id-type="pmid">26589278</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0034">
      <mixed-citation publication-type="journal" id="men13413-cit-0034"><string-name><surname>Pybus</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Luisi</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Dall'Olio</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Uzkudun</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Laayouni</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Bertranpetit</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Engelken</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <article-title>Hierarchical boosting: A machine‐learning framework to detect and classify hard selective sweeps in human populations</article-title>. <source>Bioinformatics</source>, <volume>31</volume>(<issue>24</issue>), <fpage>3946</fpage>–<lpage>3952</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btv493</pub-id>
<pub-id pub-id-type="pmid">26315912</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0035">
      <mixed-citation publication-type="journal" id="men13413-cit-0035"><string-name><surname>Raynal</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Marin</surname>, <given-names>J.‐M.</given-names></string-name>, <string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Ribatet</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Robert</surname>, <given-names>C. P.</given-names></string-name>, &amp; <string-name><surname>Estoup</surname>, <given-names>A.</given-names></string-name> (<year>2019</year>). <article-title>ABC random forests for Bayesian parameter inference</article-title>. <source>Bioinformatics</source>, <volume>35</volume>(<issue>10</issue>), <fpage>1720</fpage>–<lpage>1728</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty867</pub-id>
<pub-id pub-id-type="pmid">30321307</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0036">
      <mixed-citation publication-type="journal" id="men13413-cit-0036"><string-name><surname>Schlötterer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Tobler</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Kofler</surname>, <given-names>R.</given-names></string-name>, &amp; <string-name><surname>Nolte</surname>, <given-names>V.</given-names></string-name> (<year>2014</year>). <article-title>Sequencing pools of individuals ‐ mining genome‐wide polymorphism data without big funding</article-title>. <source>Nature Reviews Genetics</source>, <volume>15</volume>(<issue>11</issue>), <fpage>749</fpage>–<lpage>763</lpage>. <pub-id pub-id-type="doi">10.1038/nrg3803</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0037">
      <mixed-citation publication-type="journal" id="men13413-cit-0037"><string-name><surname>Schrider</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Ayroles</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Matute</surname>, <given-names>D. R.</given-names></string-name>, &amp; <string-name><surname>Kern</surname>, <given-names>A. D.</given-names></string-name> (<year>2018</year>). <article-title>Supervised machine learning reveals introgressed loci in the genomes of <italic toggle="yes">Drosophila</italic>
<italic toggle="yes">simulans</italic> and <italic toggle="yes">D</italic>. <italic toggle="yes">sechellia</italic>
</article-title>. <source>PLoS Genetics</source>, <volume>14</volume>(<issue>4</issue>), <elocation-id>e1007341</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pgen.1007341</pub-id>
<pub-id pub-id-type="pmid">29684059</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0038">
      <mixed-citation publication-type="journal" id="men13413-cit-0038"><string-name><surname>Schrider</surname>, <given-names>D. R.</given-names></string-name>, &amp; <string-name><surname>Kern</surname>, <given-names>A. D.</given-names></string-name> (<year>2016</year>). <article-title>S/HIC: robust Identification of soft and hard sweeps using machine learning</article-title>. <source>PLoS Genetics</source>, <volume>12</volume>(<issue>3</issue>), <elocation-id>e1005928</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pgen.1005928</pub-id>.<pub-id pub-id-type="pmid">26977894</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0039">
      <mixed-citation publication-type="journal" id="men13413-cit-0039"><string-name><surname>Schrider</surname>, <given-names>D. R.</given-names></string-name>, &amp; <string-name><surname>Kern</surname>, <given-names>A. D.</given-names></string-name> (<year>2018</year>). <article-title>Supervised machine learning for population genetics: A new paradigm</article-title>. <source>Trends in Genetics</source>, <volume>34</volume>(<issue>4</issue>), <fpage>301</fpage>–<lpage>312</lpage>. <pub-id pub-id-type="doi">10.1016/j.tig.2017.12.005</pub-id>
<pub-id pub-id-type="pmid">29331490</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0040">
      <mixed-citation publication-type="journal" id="men13413-cit-0040"><string-name><surname>Sebastiani</surname>, <given-names>F.</given-names></string-name> (<year>2002</year>). <article-title>Machine learning in automated text categorization</article-title>. <source>ACM Computing Surveys</source>, <volume>34</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1145/505282.505283</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0041">
      <mixed-citation publication-type="journal" id="men13413-cit-0041"><string-name><surname>Sheehan</surname>, <given-names>S.</given-names></string-name>, &amp; <string-name><surname>Song</surname>, <given-names>Y. S.</given-names></string-name> (<year>2016</year>). <article-title>Deep learning for population genetic inference</article-title>. <source>PLoS Computional Biology</source>, <volume>12</volume>(<issue>3</issue>), <elocation-id>e1004845</elocation-id>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004845</pub-id>
</mixed-citation>
    </ref>
    <ref id="men13413-bib-0042">
      <mixed-citation publication-type="journal" id="men13413-cit-0042"><string-name><surname>Smith</surname>, <given-names>M. L.</given-names></string-name>, &amp; <string-name><surname>Carstens</surname>, <given-names>B. C.</given-names></string-name> (<year>2020</year>). <article-title>Process‐based species delimitation leads to identification of more biologically relevant species</article-title>. <source>Evolution</source>, <volume>74</volume>(<issue>2</issue>), <fpage>216</fpage>–<lpage>229</lpage>. <pub-id pub-id-type="doi">10.1111/evo.13878</pub-id>
<pub-id pub-id-type="pmid">31705650</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0043">
      <mixed-citation publication-type="journal" id="men13413-cit-0043"><string-name><surname>Smith</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Ruffley</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Espíndola</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Tank</surname>, <given-names>D. C.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>J.</given-names></string-name>, &amp; <string-name><surname>Carstens</surname>, <given-names>B. C.</given-names></string-name> (<year>2017</year>). <article-title>Demographic model selection using random forests and the site frequency spectrum</article-title>. <source>Molecular Ecology</source>, <volume>26</volume>(<issue>17</issue>), <fpage>4562</fpage>–<lpage>4573</lpage>. <pub-id pub-id-type="doi">10.1111/mec.14223</pub-id>
<pub-id pub-id-type="pmid">28665011</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0044">
      <mixed-citation publication-type="journal" id="men13413-cit-0044"><collab collab-type="authors">The 1000 Genomes Project Consortium</collab>
(<year>2012</year>). <article-title>An integrated map of genetic variation from 1,092 human genomes</article-title>. <source>Nature</source>, <volume>491</volume>(<issue>7422</issue>), <fpage>56</fpage>–<lpage>65</lpage>. <pub-id pub-id-type="doi">10.1038/nature11632</pub-id>
<pub-id pub-id-type="pmid">23128226</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0045">
      <mixed-citation publication-type="journal" id="men13413-cit-0045"><string-name><surname>Wang</surname>, <given-names>Z.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kourakos</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hoang</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>H. H.</given-names></string-name>, <string-name><surname>Mathieson</surname>, <given-names>I.</given-names></string-name>, &amp; <string-name><surname>Mathieson</surname>, <given-names>S.</given-names></string-name> (<year>2021</year>). <article-title>Automatic inference of demographic parameters using generative adversarial networks</article-title>. <source>Mol Ecol Resour.</source>, <fpage>1</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1101/2020.08.05.237834</pub-id>
<pub-id pub-id-type="pmid">33332771</pub-id></mixed-citation>
    </ref>
    <ref id="men13413-bib-0046">
      <mixed-citation publication-type="journal" id="men13413-cit-0046"><string-name><surname>Weir</surname>, <given-names>B. S.</given-names></string-name>, &amp; <string-name><surname>Goudet</surname>, <given-names>J.</given-names></string-name> (<year>2017</year>). <article-title>A unified characterization of population structure and relatedness</article-title>. <source>Genetics</source>, <volume>206</volume>(<issue>4</issue>), <fpage>2085</fpage>–<lpage>s2103</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.116.198424</pub-id>
<pub-id pub-id-type="pmid">28550018</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
