<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Hum Genomics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Hum. Genomics</journal-id>
    <journal-title-group>
      <journal-title>Human Genomics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1473-9542</issn>
    <issn pub-type="epub">1479-7364</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6805717</article-id>
    <article-id pub-id-type="publisher-id">225</article-id>
    <article-id pub-id-type="doi">10.1186/s40246-019-0225-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Human mitochondrial genome compression using machine learning techniques</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Rongjie</given-names>
        </name>
        <address>
          <email>rjwang.hit@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Zang</surname>
          <given-names>Tianyi</given-names>
        </name>
        <address>
          <email>tianyi.zang@hit.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wang</surname>
          <given-names>Yadong</given-names>
        </name>
        <address>
          <email>ydwang@hit.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label>Peng Cheng Laboratory, ShenZhen, China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0193 3564</institution-id><institution-id institution-id-type="GRID">grid.19373.3f</institution-id><institution>School of Computer Science and Technology, Harbin Institute of Technology, </institution></institution-wrap>Harbin, China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>22</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>22</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>13</volume>
    <issue>Suppl 1</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editor declares no competing interests.</issue-sponsor>
    <elocation-id>49</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">In recent years, with the development of high-throughput genome sequencing technologies, a large amount of genome data has been generated, which has caused widespread concern about data storage and transmission costs. However, how to effectively compression genome sequences data remains an unsolved problem.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we propose a compression method using machine learning techniques (DeepDNA), for compressing human mitochondrial genome data. The experimental results show the effectiveness of our proposed method compared with other on the human mitochondrial genome data.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">The compression method we proposed can be classified as non-reference based method, but the compression effect is comparable to that of reference based methods. Moreover, our method not only have a well compression results in the population genome with large redundancy, but also in the single genome with small redundancy. The codes of DeepDNA are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/rongjiewang/DeepDNA">https://github.com/rongjiewang/DeepDNA</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Compression</kwd>
      <kwd>Human mitochondrial genomes</kwd>
      <kwd>Machine learning</kwd>
    </kwd-group>
    <conference xlink:href="http://orienta.ugr.es/bibm2018/">
      <conf-name>IEEE International Conference on Bioinformatics and Biomedicine 2018</conf-name>
      <conf-loc>Madrid, Spain</conf-loc>
      <conf-date>3-6 December 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>The Human Genome Project (HGP) cost about $3 billion and took about 13 years, the completion of the Human Genome Project signs of the beginning of human genome research in the life sciences has entered a new era of genome [<xref ref-type="bibr" rid="CR1">1</xref>]. Since then, The amount of data in the genome is growing exponentially, even faster than Moore’s Law [<xref ref-type="bibr" rid="CR2">2</xref>]. Today’s high-throughput sequencing technology enables sequencing of individual genomes in a matter of hours, with sequencing costs less than $1,000. These advances have allowed researchers to increase the scope for scientific discovery through large amounts of data. However, the huge amount of genomic data presents new challenges for efficient storage and transmission.</p>
    <p>Genome sequences are generally stored in FASTA format [<xref ref-type="bibr" rid="CR3">3</xref>]. In this format, genome sequence characters are stored in ASCII-based, and represented by four diiferent symbols (called nucleotides or bases), namely (A) adenine, (C) cytosine, (T) thymine, (G) guanine. The problem we face is how to effectively compress strings of a certain length composed of these four elements.</p>
    <p>The existing genome compression methods were major based on the dictionary methods [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR7">7</xref>], and based on statistical methods [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. DeepZip [<xref ref-type="bibr" rid="CR10">10</xref>], as a machine learning compression method, compression the general context data at a online learning model. The parameters of compression is updated once after compressing each character, trying to learn the pattern of input data. The consequence of this approach is that a lot of time is spent updating model parameters during compression and decompression. Another minor defect is that when a pattern first appears in the input data, the compression and decompression model cannot be recognized it immediately. It needed to learn the pattern, and then, can be effectively compressed when these pattern were encountered again, which affects the compression result. As we know that genomes within the same species have a highly similarity, for example, genome similarity between two human individuals up to 99.9<italic>%</italic>. Even not in the same species, the genome similarity between humans and chimpanzees can be as high as 89% [<xref ref-type="bibr" rid="CR11">11</xref>]. The banana genome, which seems to have nothing to do with the human genome, has a similarity of 50% [<xref ref-type="bibr" rid="CR12">12</xref>]. In this work, we proposes a static machine learning compression method, use part of experiment data as a training set, optimal the compression model parameters. Then use the other part as test data to test the effect of compression model.</p>
    <p>In computer vision tasks, the deep learning model has achieved some good performance, such as using Convolutional Neural Network (CNN) and Long Short-Term Memory Networks (LSTM) models to solve text classification [<xref ref-type="bibr" rid="CR13">13</xref>], image caption generation [<xref ref-type="bibr" rid="CR14">14</xref>] and speech recognition [<xref ref-type="bibr" rid="CR15">15</xref>], and so on. However, in genome sequence data compression, for the first time, we tried to use a deep learning model to learn the sequence of patterns in the genome, and to predict the probabilities of the next base to be encoded, followed by arithmetic coding and output compressed data stream.</p>
    <p>We verified the validity of our proposed method in 1,000 human mitochondrial sequences, and randomly divided the data set into three parts in proportion (training set, verification set and test set). Then we trained our model with the training set, the verification set was used to select the optimal parameters, and the test set verified the effectiveness of the deep learning model.</p>
    <p>The remainder of this paper is organized as follows: Section II describes the DeepDNA method in detail, section III reports the experimental performance of DeepDNA, conclusion is drawn in Section IV.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Overview</title>
      <p>For a length of T sequence steam <italic>x</italic><sub>1:<italic>T</italic></sub>=<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>,…,<italic>x</italic><sub><italic>T</italic></sub>, where each variable <italic>x</italic><sub><italic>i</italic></sub>∈<italic>Σ</italic>,<italic>i</italic>∈[ 1,<italic>T</italic>], for genome sequence, <italic>Σ</italic>={<italic>A</italic>,<italic>C</italic>,<italic>G</italic>,<italic>T</italic>}. The probability of the entire sequence <italic>x</italic><sub>1:<italic>T</italic></sub>is: 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} p\left({x}_{1 : T}\right) &amp;=p\left(x_{1}\right) p\left(x_{2} |{x}_{1}\right) p\left(x_{3} | {x}_{1 : 2}\right) \cdots p\left(x_{T} | {x}_{1 :(T-1)}\right) \\ &amp;=\prod_{t=1}^{T} p\left(x_{t} | {x}_{1 :(t-1)}\right) \end{aligned}  $$ \end{document}</tex-math><mml:math id="M2"><mml:mtable><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>⋯</mml:mo><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∏</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="40246_2019_225_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Therefore, the probability density estimation problem of sequence data can be converted into a univariate conditional probability estimation, that means, the probability of a sequence can be viewed as the product of its probability of sub-sequence. The more likely a sequence is to occur, the lower its entropy value and the better compression result. In other words, the more accurate the prediction of conditional probability events of sub-sequence, the better the compression effect achieved. The conditional probability can be expressed as <italic>p</italic>(<italic>x</italic><sub><italic>t</italic></sub>|<italic>x</italic><sub>1:(<italic>t</italic>−1)</sub>) of <italic>x</italic><sub><italic>t</italic></sub> given <italic>x</italic><sub>1:(<italic>t</italic>−1)</sub>, which fed into the arithmetic encoding tool, to get the final compressed file.</p>
      <p>Given N sequences data, the sequence probability model needs to learn a model <italic>p</italic><sub><italic>θ</italic></sub>(<italic>x</italic><sub><italic>t</italic></sub>|<italic>x</italic><sub>1:(<italic>t</italic>−1)</sub>), to maximize the log likelihood function of the entire data set. 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \max_{\theta} \sum_{n=1}^{N} \log p_{\theta}\left({x}_{1 : T_{n}}^{(n)}\right)=\max_{\theta} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} \log p_{\theta}\left(x_{t}^{(n)} | {x}_{1:(t-1)}^{(n)}\right)  $$ \end{document}</tex-math><mml:math id="M4"><mml:mspace width="-14.0pt"/><mml:munder><mml:mrow><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>log</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mo>log</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="40246_2019_225_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>We can use the neural network model to estimate the conditional probability <italic>p</italic><sub><italic>θ</italic></sub>(<italic>x</italic><sub><italic>t</italic></sub>|<italic>x</italic><sub>1:(<italic>t</italic>−1)</sub>). Suppose a neural network <italic>f</italic>(<italic>θ</italic>), whose input is the historical information <italic>x</italic><sub>1:(<italic>t</italic>−1)</sub>=<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>,…,<italic>x</italic><sub><italic>t</italic>−1</sub>, the output is the occurrence of next base <italic>x</italic><sub><italic>t</italic></sub>, the output four nucleotides probabilities satisfy: 
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \sum_{x_{t} \in \{A, C, G, T\}} p_{\theta}\left(\mathbf{x_{t} | x_{1 :(t-1)}}\right)=1  $$ \end{document}</tex-math><mml:math id="M6"><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><graphic xlink:href="40246_2019_225_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Where <italic>θ</italic> represents the neural network parameter, the conditional probability <italic>p</italic><sub><italic>θ</italic></sub>(<italic>x</italic><sub><italic>t</italic></sub>|<italic>x</italic><sub>1:(<italic>t</italic>−1)</sub>) can be obtained from the output of the neural network.</p>
      <p>The architecture of our neural network model DeepDNA is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, the framework mainly has six layers: the first layer is the single one-hot representation, which convert the genome sequence nucleotides {<italic>A</italic>,<italic>C</italic>,<italic>G</italic>,<italic>T</italic>} to vectors. The second convolution layer extracts the context short-term correlation in the genome. The third layer is the pooling layer to remove the noise. The fourth layer, LSTM, extracted the long-term correlation in the genome. The fully connected layer and the last layer are used for outputting next nucleotides {<italic>A</italic>,<italic>C</italic>,<italic>G</italic>,<italic>T</italic>} probabilities.
<fig id="Fig1"><label>Fig. 1</label><caption><p>The architecture of the DeepDNA model. Firstly, the input genome sequence is transformed into one-hot 4-dimensions bit matrix; A convolution layer activated by a rectified linear units acts as a local feature extractor, its output is a matrix with column matrix of the convolution filter and the row matrix of the position in the input sequence; A max-pooling procedure is used to reduce the size of the output matrix and only preserve the main features; The subsequent Long Short-Term Memory network (LSTM) layer is considered as acting the role of capturing sequence long-term features; A flattened fully connected layer is to collect LSTM outputs; The last layer performs a sigmoid non-linear transformation to a vector that serves as probability predictions of the sequence base</p></caption><graphic xlink:href="40246_2019_225_Fig1_HTML" id="MO1"/></fig></p>
      <p>The following subsections describe how we apply CNN to extract genome sequences local features, LSTM to capture long-term dependencies over window features sequence and fed output layer to the arithmetic coder for getting the bit-stream respectively.</p>
    </sec>
    <sec id="Sec4">
      <title>Convolutional Neural Network (CNN)</title>
      <p>One-dimensional convolution operation corresponds a series of filters sliding over the genome sequence identify the sequence characteristics at different positions in the genome, it takes the one-hot encoding of the genome as input, where one can be defined as: 
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}}  \begin{array}{l} A \rightarrow [\!1, 0, 0, 0] \\ C \rightarrow [\!0, 1, 0, 0] \\ G \rightarrow [\!0, 0, 1, 0] \\ T \rightarrow [\!0, 0, 0, 1] \\ \end{array} \end{array} $$ \end{document}</tex-math><mml:math id="M8"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mtable><mml:mtr><mml:mtd><mml:mi>A</mml:mi><mml:mo>→</mml:mo><mml:mo>[</mml:mo><mml:mspace width="0.3em"/><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>C</mml:mi><mml:mo>→</mml:mo><mml:mo>[</mml:mo><mml:mspace width="0.3em"/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>G</mml:mi><mml:mo>→</mml:mo><mml:mo>[</mml:mo><mml:mspace width="0.3em"/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>]</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>T</mml:mi><mml:mo>→</mml:mo><mml:mo>[</mml:mo><mml:mspace width="0.3em"/><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:mtd></mml:mtr><mml:mtr/></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="40246_2019_225_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Let <inline-formula id="IEq1"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$x \in \mathbb {R}^{T\times 4}$\end{document}</tex-math><mml:math id="M10"><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq1.gif"/></alternatives></inline-formula> represents the genome sequence with input length of T, and <inline-formula id="IEq2"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$x_{i} \in \mathbb {R}^{4}$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq2.gif"/></alternatives></inline-formula> is the base vector representation of the <italic>i</italic>th position in the sequence. There are <italic>m</italic> filters in total, and the output of filters are <inline-formula id="IEq3"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$o \in \mathbb {R}^{(N-k+1)\times m}$\end{document}</tex-math><mml:math id="M14"><mml:mi>o</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq3.gif"/></alternatives></inline-formula>, the convolution operation of each element <italic>o</italic><sub><italic>i</italic>,<italic>j</italic></sub> is defined as follows: 
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  o_{i,j} = \delta (w_{j} \odot [x_{i},x_{i+1}, \dots, x_{i+k-1}] + b_{j})  $$ \end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><graphic xlink:href="40246_2019_225_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <inline-formula id="IEq4"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$w_{j} \in \mathbb {R}^{k\times 4}$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq4.gif"/></alternatives></inline-formula> is the <italic>j</italic>th filter vector, and <inline-formula id="IEq5"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$b_{j} \in \mathbb {R}$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq5.gif"/></alternatives></inline-formula> is a shared value for the bias for filter <italic>w</italic><sub><italic>j</italic></sub>, symbol ⊙ is a convolution operation, define as follow: 
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  w_{j} \odot [x_{i},x_{i+1}, \dots, x_{i+k-1}] = \sum_{n=0}^{k-1}{w_{nj}x_{i+n}}  $$ \end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">nj</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="40246_2019_225_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Symbol <italic>δ</italic> is the neural nonlinear activation function, we select the ReLU [<xref ref-type="bibr" rid="CR16">16</xref>] operation as the activation function, which outputs negative value to 0 and as defined below: 
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  ReLU(x) = \left\{ \begin{array}{rcl} x &amp; \text{if} &amp; x &gt; 0 \\ 0 &amp; \text{if} &amp; x \leq 0 \end{array} \right.  $$ \end{document}</tex-math><mml:math id="M24"><mml:mtext mathvariant="italic">ReLU</mml:mtext><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mtext>if</mml:mtext></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="40246_2019_225_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>We use the max-pooling as the convolution layer after the output processing for the output vector feature extraction. Max- pooling operation selects the maximum value in an unit area as the representative feature of the sequence, which is used to extract the sequence higher scale features in the next layer.</p>
      <p>To reduce over-fitting, a dropout layer is connected after the maximum pooling layer. The term “dropout” refers to the temporary deletion a apart of units in the neural network, deleting all the links associated with it. The probability of choosing which unit to be dropped is independent of the others, and with a fixed probability <italic>p</italic>.</p>
    </sec>
    <sec id="Sec5">
      <title>Long short-term memory networks (LSTM)</title>
      <p>Recurrent neural network (RNNs) propagates historical message through a concatenation network structure, but has the problem of long gradient disappearance. Long Short-Term Memory Networks (LSTM) [<xref ref-type="bibr" rid="CR17">17</xref>], as a special recurrent neural network, was proposed in 1997 to solve the problem that recurrent neural network (RNNs) cannot learn long-term dependent correlation. They are now widely used in various time series problems and have achieved a series of excellent results.</p>
      <p>LSTM hierarchical transformation function is defined as follows: 
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}}  f_{t} &amp;=&amp; \delta (W_{f} \cdot [\!x_{t}, h_{t-1}] + b_{f}) \\ i_{t} &amp;=&amp; \delta (W_{i} \cdot [\!x_{t}, h_{t-1}] + b_{i}) \\ o_{t} &amp;=&amp; \delta (W_{o} \cdot [\!x_{t}, h_{t-1}] + b_{o}) \\ c_{t} &amp;=&amp; f_{t} \odot c_{t-1} + i_{t} \odot tanh(W_{c} \cdot [x_{t}, h_{t-1}] + b_{c}) \\ h_{t} &amp;=&amp; o_{t} \odot tanh(c_{t})  \end{array} $$ \end{document}</tex-math><mml:math id="M26"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mo>[</mml:mo><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mtext/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mo>[</mml:mo><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mtext/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mo>[</mml:mo><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mtext mathvariant="italic">tanh</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mtext/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mtext mathvariant="italic">tanh</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="40246_2019_225_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Where <inline-formula id="IEq6"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f_{t} \in \mathbb {R}^{h}$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq6.gif"/></alternatives></inline-formula> is a forgetting gate to control which messages in the old memory unit will be discarded; <inline-formula id="IEq7"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$i_{t} \in \mathbb {R}^{h}$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq7.gif"/></alternatives></inline-formula> is an inputting gate to control how much new messages will be recorded in the current memory unit; <inline-formula id="IEq8"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$o_{t} \in \mathbb {R}^{h}$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq8.gif"/></alternatives></inline-formula> is an output gate to control output in the history unit. Through the joint control of the above three gates, the problem of long-term gradient disappearance is solved, and the dependence of long-term context can be excavated.</p>
      <p>The terms <italic>W</italic><sub><italic>f</italic></sub>, <italic>W</italic><sub><italic>i</italic></sub>, <italic>W</italic><sub><italic>o</italic></sub>, and <italic>W</italic><sub><italic>c</italic></sub> denote weight matrices for forgetting gate, inputting gate, outputting gate, and unit state connections. The terms <italic>b</italic><sub><italic>f</italic></sub>, <italic>b</italic><sub><italic>i</italic></sub>, <italic>b</italic><sub><italic>o</italic></sub>, and <italic>b</italic><sub><italic>c</italic></sub> denote the bias vectors of the forgetting gate, inputting gate, outputting gate, and unit state connections. <italic>x</italic><sub><italic>t</italic></sub>, <italic>h</italic><sub><italic>t</italic>−1</sub> is the input sequence data for the current time and state output for previous time separately.</p>
      <p>The symbol <italic>δ</italic> is function of logistic sigmoid, which limits output range to [ 0,1], defined as: 
<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}}  Sigmoid(x) = \frac{1}{1+e^{-x}} \end{array} $$ \end{document}</tex-math><mml:math id="M34"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mtext mathvariant="italic">Sigmoid</mml:mtext><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="40246_2019_225_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>The function of <italic>tanh</italic> limits output range to [ −1,1], which is expressed as hyperbolic tangent function and defined by: 
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}}  tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \end{array} $$ \end{document}</tex-math><mml:math id="M36"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mtext mathvariant="italic">tanh</mml:mtext><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="40246_2019_225_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>The symbol ⊙ represents the dot product operation of the element, as described in the formula <xref rid="Equ6" ref-type="">6</xref>.</p>
    </sec>
    <sec id="Sec6">
      <title>Arithmetic encoder</title>
      <p>The arithmetic encoder [<xref ref-type="bibr" rid="CR18">18</xref>] was proposed in 1976 by Rissanen and Pasco to solve the problem of infinite decimal precision, is a coding approach closest to information entropy when data distribution was fixed. Instead of encoding each character to an integer, the arithmetic encoder encrypts the sequence as a sufficiently precise numeric value in the interval (0,1), called sequence identifier or label. As the encoding progresses, the label interval becomes smaller and smaller, and the next interval range is fixed by the probability of encoding character. The decoding operation is similar to encode, given the character probabilities, the arithmetic decoder predict the probability to the corresponding character interval. As long as the decimal representation of the interval identifier or label is accurate enough, the decoder is able to recover the whole encoding sequence in lossless.</p>
      <p>Figure <xref rid="Fig2" ref-type="fig">2</xref> demonstrates the identifier determination procedure of arithmetic encoder, for example, coding a sequence ‘CGTA’, we assume that the probability values of each base are: <italic>p</italic>(<italic>A</italic>)=<italic>p</italic>(<italic>T</italic>)=0.2, <italic>p</italic>(<italic>C</italic>)=0.5, <italic>p</italic>(<italic>G</italic>)=0.1. At beginning, the initial interval is (0,1), then the first base ‘C’ limited the interval to (0.2,0.7), base ‘G’ limited the interval to (0.55,0.6), and so on ⋯. The latter interval is a subset of the former interval, so the range will be more and more smaller, lastly, the identifier is limited the interval to [0.59, 0.592]. We can choose any point within this interval, like a middle value 0.591, its binary value stream is the arithmetic coded description of the raw sequence ‘CGTA’.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Arithmetic encoding process. It illustrates the sequence label determination process when encoding a sequence ’CGTA’, assume that the probability values of each base: <italic>p</italic>(<italic>A</italic>)=<italic>p</italic>(<italic>T</italic>)=0.2, <italic>p</italic>(<italic>C</italic>)=0.5, <italic>p</italic>(<italic>G</italic>)=0.1</p></caption><graphic xlink:href="40246_2019_225_Fig2_HTML" id="MO2"/></fig></p>
      <p>The decoding process is similar to encoding. First, base ‘C’ is decoded according to 0.591 within the interval (0.2,0.7), while the next decoding process 0.591 within the interval (0.55,0.7), the base ‘G’ is decoded, and so on ⋯, until decoded the last base ‘A’, and the whole sequence ‘CGTA’ is decoded.</p>
      <p>Consider an input sequence <italic>x</italic><sub><italic>i</italic>−1</sub>,<italic>x</italic><sub><italic>i</italic>−2</sub>,…,<italic>x</italic><sub><italic>i</italic>−<italic>k</italic>+1</sub> for the compression model, the output for the model is the predict the next base <italic>x</italic><sub><italic>i</italic></sub>. The estimate probabilities <italic>p</italic>(<italic>x</italic><sub><italic>i</italic></sub>|(<italic>x</italic><sub><italic>i</italic>−1</sub>,<italic>x</italic><sub><italic>i</italic>−2</sub>,…,<italic>x</italic><sub><italic>i</italic>−<italic>k</italic>+1</sub>),<italic>h</italic><sub><italic>i</italic>−1</sub>) is provided into arithmetic encoder to obtain the final output bit-streams, where <italic>h</italic><sub><italic>i</italic>−1</sub> is the deep learning model former state. According to the theory of Shannon entropy [<xref ref-type="bibr" rid="CR19">19</xref>], the number of output bits for the base <italic>x</italic><sub><italic>i</italic></sub> is determined by: 
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}}  H = -{log}_{2}(p(x_{i}|(x_{i-1}, x_{i-2}, \dots, x_{i-k+1}), h_{i-1}) \end{array} $$ \end{document}</tex-math><mml:math id="M38"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="italic">log</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="40246_2019_225_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>That is, the more accurate the probability of our model estimation, the higher the probability of corresponding coding base, the smaller the output bit-tream, and the better the compression effect we get.</p>
    </sec>
    <sec id="Sec7">
      <title>Model setting &amp; training</title>
      <p>In the deep learning model, we make comprehensive use of the local feature capture ability of CNN and the long-term feature extraction ability of LSTM. Our deep learning model implementation uses the Keras [<xref ref-type="bibr" rid="CR20">20</xref>] library, which is an open resource for deep learning derived on the backend of Theano [<xref ref-type="bibr" rid="CR21">21</xref>].</p>
      <p>Comprehensive setting for the model structure are described as follows: 
<list list-type="bullet"><list-item><p>Input layer (Input nucleotides: 64 ×4)</p></list-item><list-item><p>Convolutional Neural Network (CNN) layer (Filters no.: 1024, window size: 24 ×4, stride: 1.)</p></list-item><list-item><p>Max-Pooling layer (Window size: 3 ×4, stride: 1.)</p></list-item><list-item><p>Dropout layer (Probability: 0.1)</p></list-item><list-item><p>Long Short-Term Memory networks (LSTM) layer (LSTM units: 256)</p></list-item><list-item><p>Dropout layer (Probability: 20%)</p></list-item><list-item><p>Fully connected layer (Units: 1024)</p></list-item><list-item><p>Sigmoid output layer (Units: 4)</p></list-item></list></p>
      <p>In the deep learning model, all parameters are initialized according to random and uniform distribution <italic>u</italic><italic>n</italic><italic>i</italic><italic>f</italic>(−0.05,0.05), and entire biases are initialized to 0. We use mini-batch training (default size: 64) to minimize the cross entropy loss function on the training data set. Validation losses were assessed at the end of each training epoch to monitor the convergence. We utilized about 10 epochs to complete the training, each of which took about ∼6 h.</p>
      <p>The loss function of cross-entropy is defined as: 
<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}}  L(y, \widehat{y}) = -\frac{1}{n}\sum_{t=1}^{n}\sum_{i=1}^{4}{y_{i}}^{(t)} log(\widehat{y_{i}}^{(t)}) \end{array} $$ \end{document}</tex-math><mml:math id="M40"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="false"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mtext mathvariant="italic">log</mml:mtext><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mover accent="false"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="40246_2019_225_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Where <inline-formula id="IEq9"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\widehat {y_{i}}^{(t)}$\end{document}</tex-math><mml:math id="M42"><mml:msup><mml:mrow><mml:mover accent="false"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq9.gif"/></alternatives></inline-formula> is the probability of prediction character at time <italic>t</italic> being nucleotide <italic>i</italic>, <italic>y</italic><sub><italic>i</italic></sub><sup>(<italic>t</italic>)</sup> is the one-hot vector represent the real nucleotide at time <italic>t</italic>, and mini-batches sample size is <italic>n</italic>. We exploited the adaptive learning rate RMSprop [<xref ref-type="bibr" rid="CR22">22</xref>] designed by Geoff Hinton as the learning rate of the model.</p>
      <p>Both in the encoding and decoding process, it calculated the nucleotide probability based on the same deep learning network parameters, so they get the same prediction probability value, therefore, the original sequence can be lossless reconstructed by arithmetic coding.</p>
    </sec>
  </sec>
  <sec id="Sec8" sec-type="results">
    <title>Results</title>
    <sec id="Sec9">
      <title>Dataset</title>
      <p>In order to validate the effectiveness of our proposed model, 1,000 complete human mitochondrial genome sequences were used as experimental data, and the average sequence length of human mitochondrial genome sequence is 16,500bp. All data were download from the MITOMAP [<xref ref-type="bibr" rid="CR23">23</xref>] database in the March 2019. We randomly selected 700 sequences being the training data set, 200 sequences being the verification data set, and 100 sequences being the test data set. In order to make the sequence consists of only 4 nucleotides (A, C, G, T), for simplicity, the fuzzy symbols was replaced with a fixed base “A” in the training process, and all lowercase nucleotides in the data set were converted to uppercase. In the compression process, we record the nucleotides not in the {<italic>A</italic>,<italic>C</italic>,<italic>G</italic>,<italic>T</italic>} and its position information, record whether the base is in lowercase or not, so that the original sequence can be reconstructed in lossless when decompressing.</p>
    </sec>
    <sec id="Sec10">
      <title>Results and model analysis</title>
      <p>We experimented DeepDNA method for training set, verification set and test set respectively. The training set was utilized for learning model parameters, the verification set was utilized to determine network structure and model parameters, and the test set was utilized to verify the performance of the final selection of model parameters.</p>
      <p>As we can be seen from Fig. <xref rid="Fig3" ref-type="fig">3</xref>, with the increase of the number of training mini-batches, the loss function gradually decreases and ultimately tends to converge. There was a trivial fluctuation point at the final of training, owing to a few genomics structural variation sites, which will affected the prediction performance. However, these structural changes accounted for less than 1% of the total sequence, that is, a few differences in the data sites of human mitochondrial genome did not have much impact on the final compression results.
<fig id="Fig3"><label>Fig. 3</label><caption><p>The training loss function values (bpb) as the number of training mini-batches for DeepDNA model. 700 human mitochondrial genome sequences were trained, and the input length of the base sequence was 64, and the output was the classification of the corresponding four nucleotides</p></caption><graphic xlink:href="40246_2019_225_Fig3_HTML" id="MO3"/></fig></p>
      <p>In order to verify the validity of our proposed DeepDNA model, we tested our method and the other four methods on the test set (100 human mitochondrial genome sequence data). Table <xref rid="Tab1" ref-type="table">1</xref> lists the compression results of the DeepDNA method, the Gzip [<xref ref-type="bibr" rid="CR24">24</xref>] method, MFCompress [<xref ref-type="bibr" rid="CR9">9</xref>] method, DMcompress [<xref ref-type="bibr" rid="CR25">25</xref>] method, which we proposed earlier, the unit of compression results is in bits per base (bpb). The compression result of DeepDNA corresponds to the average of the lengths of all base prediction probability output codes in the genome sequence, namely: 
<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{@{}rcl@{}}  DeepDNA (bpb) = -\frac{1}{T}\sum_{i=1}^{T} {log}_{2}(P(\widehat{y_{i}})) \end{array} $$ \end{document}</tex-math><mml:math id="M44"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mtext mathvariant="italic">DeepDNA</mml:mtext><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">bpb</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mtext mathvariant="italic">log</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mover accent="false"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="40246_2019_225_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Results for DeepDNA and the other methods compression for 100 human mitochondrial genomes</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">Total size</th><th align="left">Gzip</th><th align="left">MFCompress</th><th align="left">DMcompress</th><th align="left">DeepDNA</th></tr><tr><th align="left"/><th align="left">(nucleotides)</th><th align="left">(bpb)</th><th align="left">(bpb)</th><th align="left">(bpb)</th><th align="left">(bpb)</th></tr></thead><tbody><tr><td align="left">100 human</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Mitochondrial genomes</td><td align="left">1,656,779</td><td align="left">1.45</td><td align="left">0.07</td><td align="left">0.07</td><td align="left">0.03</td></tr></tbody></table><table-wrap-foot><p>The measure of space occupied is evaluated in bits per base (bpb)</p></table-wrap-foot></table-wrap></p>
      <p>Where T is the sequence length of compression genome, and <inline-formula id="IEq10"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$P(\widehat {y_{i}})$\end{document}</tex-math><mml:math id="M46"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mover accent="false"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="40246_2019_225_Article_IEq10.gif"/></alternatives></inline-formula> is the predicted probability value of the DeepDNA model output corresponding to the base <italic>y</italic><sub><italic>i</italic></sub> of the genome sequence at i-th position, which can be fed into arithmetic coding [<xref ref-type="bibr" rid="CR26">26</xref>] directly and got the compression bit-streams file.</p>
      <p>Table <xref rid="Tab1" ref-type="table">1</xref> shown that on the human mitochondrial genome test data set, the compression result of the normal text compression method Gzip is 1.45 bpb, and the DMcompress method proposed in our previous work and MFCompress method both are 0.07 bpb. The deep learning-based compression method DeepDNA proposed by us, has a compression result of 0.03 bpb. Our method DeepDNA has a better result to the other three methods in the human mitochondrial genome dataset.</p>
      <p>Table <xref rid="Tab1" ref-type="table">1</xref> lists the results of compression of all 100 human mitochondrial genomes as a group data-set. To verify their efficiency of compressing on individual genome sequence, we randomly selected 5 human mitochondrial genomes. The data is independently compressed and compared the results. The Table <xref rid="Tab2" ref-type="table">2</xref> lists the compression results of the 5 human mitochondrial genome sequences. It can be seen from the table that our proposed DeepDNA method achieves the best compression results on all five independent genomes. Comparing with the current normal text compression method Gzip, the finite context model based compression method MFCompress, and our earlier information entropy-based compression method DMcompress, the result of compression of DeepDNA is less than 0.05 bpb.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Detailed results for DeepDNA and the other methods on randomly selected five sequences from 100 human mitochondrial genome sequences</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Genome ID</th><th align="left">Gzip (bpb)</th><th align="left">MFCompress (bpb)</th><th align="left">DMcompress (bpb)</th><th align="left">DeepDNA (bpb)</th></tr></thead><tbody><tr><td align="left">KF162105.1</td><td align="justify">2.63</td><td align="justify">2.09</td><td align="justify">2.07</td><td align="justify">0.01</td></tr><tr><td align="left">MF058266.1</td><td align="justify">2.64</td><td align="justify">2.09</td><td align="justify">2.07</td><td align="justify">0.05</td></tr><tr><td align="left">KC911416.1</td><td align="justify">2.64</td><td align="justify">2.09</td><td align="justify">2.06</td><td align="justify">0.01</td></tr><tr><td align="left">AY339411.1</td><td align="justify">2.63</td><td align="justify">2.09</td><td align="justify">2.07</td><td align="justify">0.01</td></tr><tr><td align="left">JQ702777.1</td><td align="justify">2.64</td><td align="justify">2.08</td><td align="justify">2.06</td><td align="justify">0.04</td></tr></tbody></table><table-wrap-foot><p>The measure of space occupied is evaluated in bits per base (bpb)</p></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec11" sec-type="discussion">
    <title>Discussion</title>
    <p>The Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref> show that our proposed method can not only achieve compression effect on the multi-genomes, but also achieve good compression effect on individual genome data. The other three compression methods have better compression on the multi-genomes than on the individual genome, because the data redundancy on the multi-genomes is higher than the individual genome. The neural network model we proposed for lossless genome compression is not affected by this limitation.</p>
    <p>The neural network parameters, as part of the compression model, obtained through training and learning directly participate in the compression decompression process. Thus it avoided the process of continuing to update parameters during compression/decompression, saving a lot of time. Because of the consistency between genomes, we can train the compression model in advance, and then used directly for compression.</p>
  </sec>
  <sec id="Sec12" sec-type="conclusion">
    <title>Conclusions</title>
    <p>We designed a novel, machine learning method, DeepDNA, which integrates the convolutional neural network (CNN) and the long short-term memory network (LSTM) for compressing the genome sequences. Experiment on 1,000 complete human mitochondrial genome sequences have shown that our method can learn local features of sequences through convolution layer, and can learn advanced representations of long-term dependence of sequences through long short-term memory network (LSTM). We evaluated the performance of deep learning model on 100 human mitochondrial genome sequences compression task and obtained an acceptable result.</p>
    <p>Our model indicated the feasibility of compressing genome sequences via CNN and LSTM network models. This work will help to better explore the patterns and rules in genome sequences, assist in decoding the functional characteristics of sequences, and to help resolve the link between genes and disease. Because better sequence prediction model will be achieved better compression effect, and a better sequence prediction model can help solve all above problems.</p>
    <p>In addition, with the exploration of genome sequence characteristics in future biological analysis, the compression method can obtain more redundant information, and get a better improved compression performance. In the next work, we can explore methods of lossless compression of the entire human genome, making full use of as much background information as possible, for instance, mutations, tandem repeats, motifs, etc., to train the machine learning model for compression genome sequences.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
    <sec id="d29e3024">
      <title>About this supplement</title>
      <p>This article has been published as part of <italic>Human Genomics Volume 13 Supplement 1, 2019: Selected articles from the IEEE BIBM International Conference on Bioinformatics &amp; Biomedicine (BIBM) 2018: human genomics</italic>. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://humgenomics.biomedcentral.com/articles/supplements/volume-13-supplement-1">https://humgenomics.biomedcentral.com/articles/supplements/volume-13-supplement-1</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>RW designed the experiment and was a major contributor in writing the manuscript. TZ performed the reliability of the experiment. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Publication costs were funded by the National Key Research and Development Program of China [Grant No.: 2016YFC0901605, 2016YFC1201702-01], the National High-tech R&amp;D Program of China [Grant No.: 2015AA020108, 2012AA02A604].</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The dataset generated and analysed during the current study are available in the Mitomap repository, <ext-link ext-link-type="uri" xlink:href="https://www.mitomap.org/MITOMAP">https://www.mitomap.org/MITOMAP</ext-link></p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Collins</surname>
            <given-names>FS</given-names>
          </name>
          <name>
            <surname>Morgan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Patrinos</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>The human genome project: Lessons from large-scale biology</article-title>
        <source>Science</source>
        <year>2003</year>
        <volume>300</volume>
        <issue>5617</issue>
        <fpage>286</fpage>
        <lpage>90</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1084564</pub-id>
        <pub-id pub-id-type="pmid">12690187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schaller</surname>
            <given-names>RR</given-names>
          </name>
        </person-group>
        <article-title>Moore’s law: past, present and future</article-title>
        <source>IEEE Spectr</source>
        <year>1997</year>
        <volume>34</volume>
        <issue>6</issue>
        <fpage>52</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1109/6.591665</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pearson</surname>
            <given-names>WR</given-names>
          </name>
          <name>
            <surname>Lipman</surname>
            <given-names>DJ</given-names>
          </name>
        </person-group>
        <article-title>Improved tools for biological sequence comparison</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>1988</year>
        <volume>85</volume>
        <issue>8</issue>
        <fpage>2444</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.85.8.2444</pub-id>
        <pub-id pub-id-type="pmid">3162770</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <mixed-citation publication-type="other">Grumbach S, Tahi F. Compression of dna sequences. In: Data Compression Conference: 1993. p. 340–50.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <mixed-citation publication-type="other">Grumbach S, Tahi F. Inf Process Manag. 1994; 30(6):875–86.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Kwong</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>A compression algorithm for dna sequences and its applications in genome comparison</article-title>
        <source>Genome Inform Work Genome Inform</source>
        <year>2000</year>
        <volume>10</volume>
        <issue>4</issue>
        <fpage>51</fpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <mixed-citation publication-type="other">Li P, Wang S, Kim J, Xiong H, Ohnomachado L, Jiang X. Plos ONE. 2013; 8(11):80377.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <mixed-citation publication-type="other">Kaipa KK, Bopardikar AS, Abhilash S, Venkataraman P. Algorithm for dna sequence compression based on prediction of mismatch bases and repeat location. In: IEEE International Conference on Bioinformatics and Biomedicine Workshops: 2010. p. 851–2.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pinho</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Pratas</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Mfcompress: a compression tool for fasta and multi-fasta data</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>30</volume>
        <issue>1</issue>
        <fpage>117</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt594</pub-id>
        <pub-id pub-id-type="pmid">24132931</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <mixed-citation publication-type="other">Goyal M, Tatwawadi K, Chandak S, Ochoa I. Deepzip: Lossless data compression using recurrent neural networks. arXiv preprint. 2018. arXiv:1811.08162.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tomkins</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Genome-wide dna alignment similarity (identity) for 40,000 chimpanzee dna sequences queried against the human genome is 86-89%</article-title>
        <source>Answers Res J</source>
        <year>2011</year>
        <volume>4</volume>
        <issue>2011</issue>
        <fpage>233</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>D’hont</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Denoeud</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Aury</surname>
            <given-names>J-M</given-names>
          </name>
          <name>
            <surname>Baurens</surname>
            <given-names>F-C</given-names>
          </name>
          <name>
            <surname>Carreel</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Garsmeur</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Noel</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Bocs</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Droc</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rouard</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The banana (musa acuminata) genome and the evolution of monocotyledonous plants</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>488</volume>
        <issue>7410</issue>
        <fpage>213</fpage>
        <pub-id pub-id-type="doi">10.1038/nature11241</pub-id>
        <pub-id pub-id-type="pmid">22801500</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Zhou C, Sun C, Liu Z, Lau F. A c-lstm neural network for text classification. arXiv preprint. 2015. arXiv:1511.08630.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Xu K, Ba J, Kiros R, Cho K, Courville A, Salakhudinov R, Zemel R, Bengio Y. Show, attend and tell: Neural image caption generation with visual attention. In: International Conference on Machine Learning: 2015. p. 2048–57.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <mixed-citation publication-type="other">Sainath TN, Vinyals O, Senior A, Sak H. Convolutional, long short-term memory, fully connected deep neural networks. In: 2015 IEEE International Conference On Acoustics, Speech and Signal Processing (ICASSP). IEEE: 2015. p. 4580–4.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. In: Proceedings of the 27th international conference on machine learning (ICML-10). IEEE: 2010. p. 807–14.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <mixed-citation publication-type="other">Salomon D, Motta G. Handbook of Data Compression: Springer; 2010.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shannon</surname>
            <given-names>CE</given-names>
          </name>
        </person-group>
        <article-title>A mathematical theory of communication</article-title>
        <source>Bell Syst Tech J</source>
        <year>1948</year>
        <volume>27</volume>
        <issue>3</issue>
        <fpage>379</fpage>
        <lpage>423</lpage>
        <pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Chollet F, et al.Keras, GitHub. GitHub repository. 2015. <ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <mixed-citation publication-type="other">Bergstra J, Breuleux O, Bastien F, Lamblin P, Pascanu R, Desjardins G, Turian J, Warde-Farley D, Bengio Y. Theano: A CPU and GPU math compiler in Python. In: Proc. 9th Python in Science Conf. vol. 1. IEEE: 2010. p. 3–10.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <mixed-citation publication-type="other">Tieleman T, Hinton G. Lecture 6.5-rmsprop, coursera: Neural networks for machine learning: University of Toronto, Technical Report; 2012.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Mitomap. A human mitochondrial genome database. 2018. <ext-link ext-link-type="uri" xlink:href="http://www.mitomap.org">http://www.mitomap.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <mixed-citation publication-type="other">Deutsch P, Gailly J-L. Zlib compressed data format specification version 3.3. Tech Rep. 1996.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <mixed-citation publication-type="other">Wang R, Teng M, Bai Y, Zang T, Wang Y. Dmcompress: Dynamic markov models for bacterial genome compression. In: 2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE: 2016. p. 776–9.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Witten</surname>
            <given-names>IH</given-names>
          </name>
          <name>
            <surname>Neal</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Cleary</surname>
            <given-names>JG</given-names>
          </name>
        </person-group>
        <article-title>Arithmetic coding for data compression</article-title>
        <source>Commun ACM</source>
        <year>1987</year>
        <volume>30</volume>
        <issue>6</issue>
        <fpage>520</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1145/214762.214771</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
