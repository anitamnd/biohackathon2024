<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Cheminform</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Cheminform</journal-id>
    <journal-title-group>
      <journal-title>Journal of Cheminformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1758-2946</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5339264</article-id>
    <article-id pub-id-type="publisher-id">204</article-id>
    <article-id pub-id-type="doi">10.1186/s13321-017-0204-4</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Large-scale virtual screening on public cloud resources with Apache Spark</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4851-759X</contrib-id>
        <name>
          <surname>Capuccini</surname>
          <given-names>Marco</given-names>
        </name>
        <address>
          <email>marco.capuccini@it.uu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ahmed</surname>
          <given-names>Laeeq</given-names>
        </name>
        <address>
          <email>laeeq@kth.se</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schaal</surname>
          <given-names>Wesley</given-names>
        </name>
        <address>
          <email>wesley.schaal@farmbio.uu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Laure</surname>
          <given-names>Erwin</given-names>
        </name>
        <address>
          <email>erwin@kth.se</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Spjuth</surname>
          <given-names>Ola</given-names>
        </name>
        <address>
          <email>ola.spjuth@farmbio.uu.se</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 9457</institution-id><institution-id institution-id-type="GRID">grid.8993.b</institution-id><institution>Department of Information Technology, </institution><institution>Uppsala University, </institution></institution-wrap>Box 337, 75105 Uppsala, Sweden </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 9457</institution-id><institution-id institution-id-type="GRID">grid.8993.b</institution-id><institution>Department of Pharmaceutical Biosciences, </institution><institution>Uppsala University, </institution></institution-wrap>Box 591, 75124 Uppsala, Sweden </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000000121581746</institution-id><institution-id institution-id-type="GRID">grid.5037.1</institution-id><institution>Department of Computational Science and Technology, </institution><institution>Royal Institute of Technology (KTH), </institution></institution-wrap>Lindstedtsvägen 5, 10044 Stockholm, Sweden </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>6</day>
      <month>3</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>6</day>
      <month>3</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>9</volume>
    <elocation-id>15</elocation-id>
    <history>
      <date date-type="received">
        <day>1</day>
        <month>11</month>
        <year>2016</year>
      </date>
      <date date-type="accepted">
        <day>28</day>
        <month>2</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>
Structure-based virtual screening is an in-silico method to screen a target receptor against a virtual molecular library. Applying docking-based screening to large molecular libraries can be computationally expensive, however it constitutes a trivially parallelizable task. Most of the available parallel implementations are based on message passing interface, relying on low failure rate hardware and fast network connection. Google’s MapReduce revolutionized large-scale analysis, enabling the processing of massive datasets on commodity hardware and cloud resources, providing transparent scalability and fault tolerance at the software level. Open source implementations of MapReduce include Apache Hadoop and the more recent Apache Spark.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We developed a method to run existing docking-based screening software on distributed cloud resources, utilizing the MapReduce approach. We benchmarked our method, which is implemented in Apache Spark, docking a publicly available target receptor against <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim $$\end{document}</tex-math><mml:math id="M2"><mml:mo>∼</mml:mo></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq1.gif"/></alternatives></inline-formula>2.2 M compounds. The performance experiments show a good parallel efficiency (87%) when running in a public cloud environment.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>Our method enables parallel Structure-based virtual screening on public cloud resources or commodity computer clusters. The degree of scalability that we achieve allows for trying out our method on relatively small libraries first and then to scale to larger libraries. Our implementation is named Spark-VS and it is freely available as open source from GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/mcapuccini/spark-vs">https://github.com/mcapuccini/spark-vs</ext-link>).<fig position="anchor" id="Figa"><label>Graphical abstract</label><caption><p>.</p></caption><graphic position="anchor" xlink:href="13321_2017_204_Figa_HTML" id="MO1"/></fig>
</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Virtual screening</kwd>
      <kwd>Docking</kwd>
      <kwd>Cloud computing</kwd>
      <kwd>Apache Spark</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Identification of new drug leads is a key process in drug development. In the pharmaceutical industry, a widely established approach for this is High-Throughput Screening (HTS), where large molecular libraries are screened against a bioassay in fully automated environments [<xref ref-type="bibr" rid="CR1">1</xref>]. However, HTS is expensive and it has been shown to only produce a small number of hits or to have too many false positives and false negatives [<xref ref-type="bibr" rid="CR2">2</xref>]. Structure-based virtual screening (SBVS) is a complementary in silico method [<xref ref-type="bibr" rid="CR3">3</xref>] that has been successfully used to generate new drug leads [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>]. SBVS is cheaper and faster than HTS and it can be used in the early stages of drug development to predict if a chemical is likely to interact with certain targets. Typically, a docking-based SBVS workflow starts from a virtual molecular library and a target receptor structure. First, a preliminary preprocessing step, which is subject to the application scope, is performed. We don’t discuss the preprocessing phase in this paper as it depends on several parameters and because publicly-available, ready-to-use libraries exist (e.g., ZINC [<xref ref-type="bibr" rid="CR6">6</xref>]). After the preprocessing step, molecular docking software is used to dock each chemical in the library to the target receptor. For each molecule in the library, this step will produce a pose (which represents the orientation of the molecule in the target’s pocket) and a score. The higher the score, the more likely the predicted interaction is going to happen in reality. Hence, in the last SBVS phase, all of the poses are sorted by score and the desired number of top hits are returned.</p>
    <p>In recent decades, methods in high-throughput structural biology allowed the production of massive virtual molecular libraries. ZINC [<xref ref-type="bibr" rid="CR6">6</xref>] represents an excellent example, as it contains 20M commercially available molecules in ready-to-dock format. Using large datasets in SBVS is challenging, due to the computational cost. However, SBVS is trivially parallelizable and many tools (e.g., Multilevel Parallel Autodock 4.2 [<xref ref-type="bibr" rid="CR7">7</xref>] and OEDocking [<xref ref-type="bibr" rid="CR8">8</xref>]) parallelize it through message passing interface [<xref ref-type="bibr" rid="CR9">9</xref>] (MPI). Unfortunately, MPI implementations have some disadvantages. In fact, MPI just offers software developers an Application Programming Interface (API) for message passing. This means that, when writing MPI applications, the software developer has to write extra code to manage locality-aware scheduling, load balance and fault tolerance. As a result, many MPI-based applications must rely on fast network connections to provide scalability and in most cases they are not able to complete the computation in the event of a hardware fault. Therefore, to effectively run MPI-based applications, organizations need to have access to High Performance Computing (HPC) facilities.</p>
    <p>Google pioneered Big Data analytics on inexpensive hardware with its MapReduce (MR) programming model (and implementation) [<xref ref-type="bibr" rid="CR10">10</xref>]. In MR applications, problems like data distribution and locality-aware scheduling are managed by the underlying MR implementation, which is transparent to the software developer. In addition, the MR implementation takes care of possible hardware faults, so that the analysis can complete even if some cluster nodes die or if the network becomes temporary unavailable. These key features of MR make MR applications ready to be run in the cloud, where the virtual infrastructure is somewhat comparable to commodity hardware (in terms of performance and reliability), with almost no effort. Even if Google’s MapReduce implementation is not publicly available, some open source implementations exist (Apache Hadoop [<xref ref-type="bibr" rid="CR11">11</xref>] is probably the most widely used).</p>
    <p>Google’s MR has some limitations. MR is based on an acyclic data flow model, which penalizes many popular applications where the same dataset needs to be accessed in multiple iterations (e.g., machine learning and graph algorithms) [<xref ref-type="bibr" rid="CR12">12</xref>]. In fact, the lack of features like dataset caching, accumulators and broadcast variables, and native workflows support, makes it hard to develop scientific applications. Apache Spark is an open source cluster-computing framework for the processing of large-scale datasets, which overcomes the limitations of MR, while retaining scalability and fault tolerance [<xref ref-type="bibr" rid="CR12">12</xref>].</p>
    <p>Resilient Distributed Datasets (RDDs) represent the core component of Spark [<xref ref-type="bibr" rid="CR13">13</xref>]. An RDD is an abstraction of a dataset that is partitioned through the cluster and that can therefore be operated on in parallel. RDDs offer almost the same primitives of a standard Scala [<xref ref-type="bibr" rid="CR14">14</xref>] collection, adding transparent support for locality-aware scheduling, fault tolerance and in-memory dataset caching. Finally, RDDs can be loaded from any mountable distributed file system, Hadoop Distributed File System (HDFS) or any other Hadoop-compatible input source. However, when data is read from a distributed file system, locality-aware scheduling cannot be carried out since such file systems do not provide enough information to the Spark engine.</p>
    <p>The applicability of MR-oriented frameworks to virtual screening has already been investigated. For instance, Ahmed et al. [<xref ref-type="bibr" rid="CR15">15</xref>] implemented ligand-based virtual screening in Spark, showing good scalability in a cloud environment. Zhao et al. [<xref ref-type="bibr" rid="CR16">16</xref>] described a Hadoop-based infrastructure aimed to make the storage of massive molecular libraries and the docking procedure easier perform for chemists. AutoDockCloud by Ellingson and Baudry [<xref ref-type="bibr" rid="CR17">17</xref>] is another Hadoop molecular docking implementation. Nevertheless, in the two SBVS studies, the authors show performance metrics running their tools against only a few thousand molecules on bare-metal, high-performance clusters. Furthermore, the Hadoop nature of these projects limits their performance, since Spark-based applications are overall faster (as Shi et al. pointed out [<xref ref-type="bibr" rid="CR18">18</xref>]).</p>
    <p>In this paper we introduce a method for large-scale SBVS on public cloud resources. This represents an advancement over the earlier work by Ellingson and Baudry, as they acknowledged that further refinement in AutoDockCloud was needed in order to run their method on public cloud providers [<xref ref-type="bibr" rid="CR17">17</xref>]. Furthermore, to the best of our knowledge, in this paper we report for the first time a success story on scaling SBVS over millions of molecules, using public cloud resources. This achievement is relevant for organizations without access to an HPC system, since public cloud resources are readily available with a pay-per-use pricing model, without an upfront cost.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <p>We developed a method for parallel SBVS following the MR approach, which enables the screening of large molecular libraries on public cloud resources or on commodity hardware clusters. The method is implemented in Apache Spark and it is distributed as an open source library, named Spark-VS, along with some example ready-to-run SBVS pipelines.
</p>
    <p>Using Spark-VS, the user can define custom SBVS pipelines with a high level API (which is based on the Scala programming language [<xref ref-type="bibr" rid="CR14">14</xref>]). Despite the fact that the user needs to be familiar with some basic concepts in Spark and Scala, we believe this to be particularly convenient as SBVS applies to many use cases and the user may want to fine-tune the workflows. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows an example pipeline that was defined using Spark-VS. Once the pipeline has being defined, it can be packaged like a standard Spark application to be submitted to a Spark cluster. When using public cloud resources, the effort of starting a Spark cluster boils down to the execution of a setup script.</p>
    <fig id="Fig1">
      <label>Fig. 1</label>
      <caption>
        <p>SBVS pipeline in Spark-VS. This example pipeline reads a molecular library in SDF format, docks it against a target receptor and returns the 10 top-scoring molecules. The dock primitive takes as parameters a receptor structure in the OEDocking TK binary format, a scoring method and a search resolution for the underlying docking software. In addition, the <italic>saveAsTextFile</italic> primitive is used to checkpoint all of the poses after the docking phase. This is a best practice as docking is time consuming</p>
      </caption>
      <graphic xlink:href="13321_2017_204_Fig1_HTML" id="MO2"/>
    </fig>
    <sec id="Sec3">
      <title>Data input</title>
      <p>When loading a text file to an RDD, Spark normally splits it line-by-line. Nevertheless, most molecular 3D representations consist of multiple lines. The Structure Data File (SDF) is a handy, multiline format that can be used to store 3D molecular structures along with some metadata (e.g., identifier and docking score) [<xref ref-type="bibr" rid="CR19">19</xref>]. Spark-VS can read SDF files and properly split them across the cluster thanks to a custom record reader that we defined. Since each instance of the docking program takes some time to initialize, it is good to feed each docking process with multiple molecules. On the other hand, if too many are passed together, load balancing becomes harder. For this reason, the custom SDF input format takes a <italic>chunk size</italic> parameter (default: 30) that controls the number of molecules to be loaded into a single RDD record.</p>
    </sec>
    <sec id="Sec4">
      <title>Parallel screening</title>
      <p>OEDocking TK [<xref ref-type="bibr" rid="CR20">20</xref>] was used as the underlying docking software. Even if this is a commercial software, free academic licenses are available. OEDocking TK provides a C++ API that we used to implement a light-weight docking executable, which takes a chunk of molecules from the standard input, docks them to a particular receptor (available on each node through the <italic>addFile</italic> primitive) and produces a chunk of poses (with the relative score) in the standard output. Using the standard input and the standard output in this way is very convenient. In fact, Spark-VS uses the <italic>map</italic> RDD primitive to pipe each RDD record to a docking executable instance via standard input and takes the results back via standard output. This contrasts to the implementations by Zhao et al. [<xref ref-type="bibr" rid="CR16">16</xref>] and by Ellingson and Baudry [<xref ref-type="bibr" rid="CR17">17</xref>] where the input chunks are passed to the docking program and read back to the Hadoop framework using local files. This impacts performance, since unlike pipes (that are fully operated in memory), files are stored to the disk.</p>
      <p>RDDs offer some convenient primitives to do the post processing part of SBVS. For instance, Spark-VS uses the <italic>saveAsTextFile</italic> primitive to handle persistence on a distributed storage. However, the built-in <italic>sortBy</italic> primitive is not suitable for SBVS. The Spark primitive assumes small RDD records so shuffling the relatively large SDF representations through the network would lead to a large overhead. Fortunately, we figured out a simple workaround to avoid this. Instead of performing distributed sorting, Spark-VS collects ID/Score tuples for each pose and efficiently sorts them serially. Then, using the RDD <italic>filter</italic> primitive, Spark-VS retrieves the top scoring molecules by ID.</p>
      <p>With the aim of testing our parallel implementation, we ran OEDocking TK serially over 1000 molecules that were randomly drawn from the benchmark dataset. We observed that the output for the parallel implementation did not differ from the one that we got in the serial execution. We have provided this validation procedure along with Spark-VS and it can be easily reproduced by running in local mode.</p>
    </sec>
    <sec id="Sec5">
      <title>Experiments</title>
      <sec id="Sec6">
        <title>Experimental settings</title>
        <p>We deployed a standalone Spark cluster, along with HDFS, on the CityCloud [<xref ref-type="bibr" rid="CR21">21</xref>] public cloud provider using SparkNow [<xref ref-type="bibr" rid="CR22">22</xref>] for host cloud and virtual machine provisioning. More specifically, we setup 21 nodes with 4 virtual CPUs (vCPUs), 8 GB of RAM, 20 GB of ephemeral storage and 40 GB of block storage each. This is a fully virtualized environment that, in terms of resources, resembles a commodity computers cluster. Since in a Spark cluster, one node (namely, the master node) acts as a controller, the maximum level of parallelism was 80 in our case.</p>
      </sec>
      <sec id="Sec7">
        <title>Benchmark</title>
        <p>We benchmarked our method and the Spark-VS implementation, screening HIV-1 protease receptor against the whole SureChEMBL library [<xref ref-type="bibr" rid="CR23">23</xref>] downloaded in ready-to-dock format from ZINC. The raw SureChEMBL dataset contains 17M compounds, retrieved from patent documentation. However, the ready-to-dock version contains only <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim $$\end{document}</tex-math><mml:math id="M4"><mml:mo>∼</mml:mo></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq2.gif"/></alternatives></inline-formula>2.2 M molecules, as ZINC applies some filtering rules in its preparation protocol. The dataset, in SDF format, was made available to the worker nodes via HDFS (using block size 64 Mb and block redundance 3). It is interesting to observe that this dataset is relatively small in terms of disk space (<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim $$\end{document}</tex-math><mml:math id="M6"><mml:mo>∼</mml:mo></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq3.gif"/></alternatives></inline-formula>8 GB), if compared to more orthodox MR benchmarks. In fact, Google claims to use MR for the processing of petabytes of data [<xref ref-type="bibr" rid="CR10">10</xref>]. Nevertheless, molecular docking is compute intensive, which contrasts to traditional MR applications and it justifies the applicability of Spark to such use case.</p>
        <p>Finally, it is worth mentioning that the target receptor was converted in the OEDocking TK format starting from an HIV-1 protease receptor representation available in the literature [<xref ref-type="bibr" rid="CR24">24</xref>].</p>
      </sec>
      <sec id="Sec8">
        <title>Performance metrics</title>
        <p>First, we studied the scaling efficiency of Spark-VS, pointing out the portion of processing units that are actually used during the computation. The scaling efficiency is a very important matter when using cloud resources since they are typically pay-per-use. In order to give a resource usage estimation, we repeatedly ran Spark-VS over <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1/4, 2/4 \ldots 4/4$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>4</mml:mn><mml:mo>…</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq4.gif"/></alternatives></inline-formula> of the benchmark dataset, restricting the vCPUs usage to 20 in the first run, and allowing 20 additional vCPUs each time the input increased. Then, for each run we computed the Weak Scaling Efficiency (WSE), that is the running time for one processing element (20 cores in our case) to process one work unit (1/4 of the dataset), divided by the running time for <italic>N</italic> processing elements to process <italic>N</italic> work units (<inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=1,2 \ldots 4$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>…</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq5.gif"/></alternatives></inline-formula> in our case). Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the WSE for each run. An important finding is that, when using full resources, we are able to show a scaling efficiency of 87%.<fig id="Fig2"><label>Fig. 2</label><caption><p>Weak Scaling Efficiency plot. Each bar represents a different run and it shows how efficiently the respective vCPUs were used. The input size was increased by a work unit, along with the number of vCPUs, in each consecutive run. The trend curve was computed by 2nd degree polynomial interpolation</p></caption><graphic xlink:href="13321_2017_204_Fig2_HTML" id="MO3"/></fig>
</p>
        <p>It took 8.9 h to run the complete analysis on 80 vCPUs. The speedup is an interesting metric that compares the parallel running time, to the single core running time. Given the single core running time <inline-formula id="IEq7"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_1$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq7.gif"/></alternatives></inline-formula>, and the parallel running time <inline-formula id="IEq8"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_N$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq8.gif"/></alternatives></inline-formula>, where <italic>N</italic> is the parallelism level, the speedup is defined as <inline-formula id="IEq9"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_1/T_N$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">/</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq9.gif"/></alternatives></inline-formula>. In other words, the speedup tells us how much faster the computation get completed, using a certain level of parallelism. In order to compute <italic>T</italic>1 when running the analysis, we annotated each pose with the time that it took to produce it on the assigned vCPU. The histogram in Fig. <xref rid="Fig3" ref-type="fig">3</xref> shows the 2.2 M running times, in equally spaced bins. The serial running time, for each of the molecules in the benchmark, sums up to <inline-formula id="IEq10"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim $$\end{document}</tex-math><mml:math id="M18"><mml:mo>∼</mml:mo></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq10.gif"/></alternatives></inline-formula>635.7 h. Hence, we got a speedup of <inline-formula id="IEq11"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim $$\end{document}</tex-math><mml:math id="M20"><mml:mo>∼</mml:mo></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq11.gif"/></alternatives></inline-formula>71 when using 80 cores.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Discussion</title>
    <p>From the experiments, it emerged that our method scales well. In fact, we got a scaling efficiency of 87% and a speedup of 71, when using 80 vCPUs. These two metrics are strictly related, since a good speedup can be obtained only when the resources are efficiently exploited. The benchmark experiments showed that the WSE levels off when increasing from 60 to 80 vCPUs, and the running time allows for running the analysis over night. Hence, we believe that the costs for more massive runs are not justified.</p>
    <p>Molecular docking is compute intensive, which contrasts to most of the MR applications, where a quick operation is applied to each input record. In Fig. <xref rid="Fig3" ref-type="fig">3</xref>, we observe that the docking time varies between 0.75 and 1.50 for most of the molecules. This is an important remark, as Spark does not have enough information to predict how long the processing of each record will take, hence it assigns molecules to processing units randomly. Therefore, we believe that the scaling efficiency of the method can be improved by tuning the Spark cluster configuration, for better load balancing. We leave this as future work, since it is not the focus of this study.</p>
    <fig id="Fig3">
      <label>Fig. 3</label>
      <caption>
        <p>Docking time per molecule. The histogram shows the serial docking time for each molecule in the benchmark dataset (<inline-formula id="IEq6"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sim}2.2\,M$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>2.2</mml:mn><mml:mspace width="0.166667em"/><mml:mi>M</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq6.gif"/></alternatives></inline-formula>) divided into equally spaced bins. Note that in this plot the <italic>number of molecules</italic> is on logarithmic scale</p>
      </caption>
      <graphic xlink:href="13321_2017_204_Fig3_HTML" id="MO4"/>
    </fig>
    <p>Ellingson and Baudry reported a speedup of 450 for AutoDockCloud, when they ran it on a high-performance bare-metal Hadoop installation [<xref ref-type="bibr" rid="CR17">17</xref>]. Comparing the speedup that we got for Spark-VS is unfair, as we ran in a fully virtualized environment with a much lower parallelism level. Nowadays, the biggest cloud providers (e.g., Amazon Web Services [<xref ref-type="bibr" rid="CR25">25</xref>] and Google Cloud Platform [<xref ref-type="bibr" rid="CR26">26</xref>]) sell their resources as virtual infrastructure, providing handy tools to deploy Spark and Hadoop clusters. This is why we believe that MR applications should always be tested on virtual environments. Fortunately, Ellingson and Baudry provide enough information to estimate their scaling efficiency. AutoDockCloud was tested on 2637 input molecules, with a parallelism level of 570. Therefore, we can derive a work unit of <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sim}5$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq12.gif"/></alternatives></inline-formula> molecules. The total serial running time for 2637 molecules was <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\sim}69$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>69</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13321_2017_204_Article_IEq13.gif"/></alternatives></inline-formula> h. Hence, each molecule was docked in 94 s on average, leading to a average work unit time of 470 s. The parallel execution of AutoDockCloud took 550 s, therefore we can estimate a WSE of 85%, which is slightly lower than the one we show for Spark-VS (87%).</p>
  </sec>
  <sec id="Sec10">
    <title>Conclusion</title>
    <p>Our method provides the means to run SBVS on public cloud resources or on commodity computers clusters. The performance experiments showed good scalability on a virtual environment that resembles an inexpensive hardware cluster. To the best of our knowledge, this is the first time that a method has been shown to scale SBVS over millions of molecules on a fully virtualized environment. This is a very important point, because public cloud providers, such as Amazon or Google, only sell virtual resources. Public cloud is pay-per-use and it can be allocated and deallocated on-demand. Furthermore, cloud providers offer handy tools and interfaces that provide the means to setup Hadoop and Spark clusters, relieving the scientists from tedious configurations. Hence, being able to scale SBVS in these kinds of environments constitutes an important advancement. In fact, organizations that want to approach SBVS can now try it out on their favorite cloud provider, benchmarking it on relatively small libraries (at lower costs), and then scale to larger libraries. Furthermore, being relieved form the up-front investments, hardware configuration and maintenance costs is certainly an advantage too.</p>
    <p>The method, along with its source code and unit tests, is free to use and publicly available on GitHub [<xref ref-type="bibr" rid="CR27">27</xref>]. Even though our implementation uses a commercial molecular docking software by default, which is free only for academics, any other docking software can be used with minor adaption.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Authors' contributions</title>
    <p>MC and OS conceived the project. MC and LA implemented the method and carried out experiments. WS contributed with expertise in modeling. EL contributed with expertise in HPC. All authors read and approved the final manuscript.</p>
    <sec id="FPar1">
      <title>Acknowledgements</title>
      <p>This project was supported by the Swedish strategic research programs eSSENCE and Swedish e-Science Research Center (SeRC). Cloud resources were kindly provided by CityCloud [<xref ref-type="bibr" rid="CR21">21</xref>].</p>
    </sec>
    <sec id="FPar2">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fox</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Farr-Jones</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sopchak</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Boggs</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Nicely</surname>
            <given-names>HW</given-names>
          </name>
          <name>
            <surname>Khoury</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Biros</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>High-throughput screening: update on practices and success</article-title>
        <source>J Biomol Screen</source>
        <year>2006</year>
        <volume>11</volume>
        <issue>7</issue>
        <fpage>864</fpage>
        <lpage>869</lpage>
        <pub-id pub-id-type="doi">10.1177/1087057106292473</pub-id>
        <pub-id pub-id-type="pmid">16973922</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hughes</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Rees</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kalindjian</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Philpott</surname>
            <given-names>KL</given-names>
          </name>
        </person-group>
        <article-title>Principles of early drug discovery</article-title>
        <source>Br J Pharmacol</source>
        <year>2011</year>
        <volume>162</volume>
        <issue>6</issue>
        <fpage>1239</fpage>
        <lpage>1249</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1476-5381.2010.01127.x</pub-id>
        <pub-id pub-id-type="pmid">21091654</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bryant</surname>
            <given-names>SH</given-names>
          </name>
        </person-group>
        <article-title>Structure-based virtual screening for drug discovery: a problem-centric review</article-title>
        <source>AAPS J</source>
        <year>2012</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>133</fpage>
        <lpage>141</lpage>
        <pub-id pub-id-type="doi">10.1208/s12248-012-9322-0</pub-id>
        <pub-id pub-id-type="pmid">22281989</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seifert</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Lang</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Essential factors for successful virtual screening</article-title>
        <source>Mini Rev Med Chem</source>
        <year>2008</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>63</fpage>
        <lpage>72</lpage>
        <pub-id pub-id-type="doi">10.2174/138955708783331540</pub-id>
        <pub-id pub-id-type="pmid">18220986</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Villoutreix</surname>
            <given-names>BO</given-names>
          </name>
          <name>
            <surname>Eudes</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Miteva</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Structure-based virtual ligand screening: recent success stories</article-title>
        <source>Comb Chem High Throughput Screen</source>
        <year>2009</year>
        <volume>12</volume>
        <issue>10</issue>
        <fpage>1000</fpage>
        <lpage>1016</lpage>
        <pub-id pub-id-type="doi">10.2174/138620709789824682</pub-id>
        <pub-id pub-id-type="pmid">20025565</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Irwin</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Sterling</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Mysinger</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Bolstad</surname>
            <given-names>ES</given-names>
          </name>
          <name>
            <surname>Coleman</surname>
            <given-names>RG</given-names>
          </name>
        </person-group>
        <article-title>ZINC: a free tool to discover chemistry for biology</article-title>
        <source>J Chem Inf Model</source>
        <year>2012</year>
        <volume>52</volume>
        <issue>7</issue>
        <fpage>1757</fpage>
        <lpage>1768</lpage>
        <pub-id pub-id-type="doi">10.1021/ci3001277</pub-id>
        <pub-id pub-id-type="pmid">22587354</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Norgan</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Coffman</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Kocher</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Katzmann</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Sosa</surname>
            <given-names>CP</given-names>
          </name>
        </person-group>
        <article-title>Multilevel parallelization of Auto Dock 4.2</article-title>
        <source>J Cheminform</source>
        <year>2011</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>12</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-3-12</pub-id>
        <pub-id pub-id-type="pmid">21527034</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">OEDocking. <ext-link ext-link-type="uri" xlink:href="http://www.eyesopen.com/oedocking-v3.2-released">http://www.eyesopen.com/oedocking-v3.2-released</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Forum</surname>
            <given-names>MPI</given-names>
          </name>
        </person-group>
        <article-title>MPI: a message-passing interface standard</article-title>
        <source>Int J Supercomput Appl</source>
        <year>1994</year>
        <volume>8</volume>
        <issue>3/4</issue>
        <fpage>165</fpage>
        <lpage>414</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ghemawat</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>MapReduce: simplified data processing on large clusters</article-title>
        <source>Commun ACM</source>
        <year>2008</year>
        <volume>51</volume>
        <issue>1</issue>
        <fpage>107</fpage>
        <lpage>113</lpage>
        <pub-id pub-id-type="doi">10.1145/1327452.1327492</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Apache Hadoop. <ext-link ext-link-type="uri" xlink:href="https://hadoop.apache.org">https://hadoop.apache.org</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Zaharia M, Chowdhury M, Franklin MJ, Shenker S, Stoica I (2010) Spark: cluster computing with working sets. In: Proceedings of the 2Nd USENIX conference on hot topics in cloud computing., HotCloud’10USENIX Association, Berkeley, CA, USA, pp 10–10</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin MJ, Shenker S, Stoica I (2012) Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing. In: Proceedings of the 9th USENIX conference on networked systems design and implementation., NSDI’12USENIX Association, Berkeley, CA, USA, pp 2–2</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Scala. <ext-link ext-link-type="uri" xlink:href="http://scala-lang.org">http://scala-lang.org</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Ahmed L, Edlund Å, Laure E, Spjuth O (2013) Using iterative MapReduce for parallel virtual screening. In: IEEE 5th international conference on cloud computing technology and science, CloudCom 2013, Bristol, United Kingdom, vol 2, pp 27–32</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Zhao J, Zhang R, Zhao Z, Chen D, Hou L (2012) Hadoop mapreduce framework to implement molecular docking of large-scale virtual screening. In: Proceedings of the 2012 IEEE Asia-Pacific services computing conference)., APSCC ’12IEEE Computer Society, Washington, DC, USA, pp 350–353</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Ellingson SR, Baudry J (2011) High-throughput virtual molecular docking: Hadoop implementation of autodock4 on a private cloud. In: Proceedings of the second international workshop on emerging computational methods for the life sciences., ECMLS ’11ACM, New York, NY, USA, pp 33–38</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Minhas</surname>
            <given-names>UF</given-names>
          </name>
          <name>
            <surname>Jiao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Reinwald</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Özcan</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Clash of the titans: mapreduce vs. spark for large scale data analytics</article-title>
        <source>Proc. VLDB Endow.</source>
        <year>2015</year>
        <volume>8</volume>
        <issue>13</issue>
        <fpage>2110</fpage>
        <lpage>2121</lpage>
        <pub-id pub-id-type="doi">10.14778/2831360.2831365</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dalby</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Nourse</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Hounshell</surname>
            <given-names>WD</given-names>
          </name>
          <name>
            <surname>Gushurst</surname>
            <given-names>AKI</given-names>
          </name>
          <name>
            <surname>Grier</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Leland</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Laufer</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Description of several chemical structure file formats used by computer programs developed at molecular design limited</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>1992</year>
        <volume>32</volume>
        <issue>3</issue>
        <fpage>244</fpage>
        <lpage>255</lpage>
        <pub-id pub-id-type="doi">10.1021/ci00007a012</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">OEDocking TK. <ext-link ext-link-type="uri" xlink:href="http://www.eyesopen.com/oedocking-tk">http://www.eyesopen.com/oedocking-tk</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">City Cloud. <ext-link ext-link-type="uri" xlink:href="https://www.citycloud.com">https://www.citycloud.com</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">SparkNow. <ext-link ext-link-type="uri" xlink:href="https://github.com/mcapuccini/SparkNow">https://github.com/mcapuccini/SparkNow</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Papadatos</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Davies</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dedman</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Chambers</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gaulton</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Siddle</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Koks</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Irvine</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Pettersson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Goncharoff</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hersey</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Overington</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>SureChEMBL: a large-scale, chemically annotated patent document database</article-title>
        <source>Nucleic Acids Res.</source>
        <year>2016</year>
        <volume>44</volume>
        <issue>D1</issue>
        <fpage>1220</fpage>
        <lpage>1228</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv1253</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bäckbro</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Löwgren</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Österlund</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Atepo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Unge</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Hultén</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bonham</surname>
            <given-names>NM</given-names>
          </name>
          <name>
            <surname>Schaal</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Karlén</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hallberg</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Unexpected binding mode of a cyclic sulfamide HIV-1 protease inhibitor</article-title>
        <source>J Med Chem</source>
        <year>1997</year>
        <volume>40</volume>
        <issue>6</issue>
        <fpage>898</fpage>
        <lpage>902</lpage>
        <pub-id pub-id-type="doi">10.1021/jm960588d</pub-id>
        <pub-id pub-id-type="pmid">9083478</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Amazon Web Services. <ext-link ext-link-type="uri" xlink:href="https://aws.amazon.com">https://aws.amazon.com</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Google Cloud Platform. <ext-link ext-link-type="uri" xlink:href="https://cloud.google.com">https://cloud.google.com</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Spark-VS GitHub Page. <ext-link ext-link-type="uri" xlink:href="https://github.com/mcapuccini/spark-vs">https://github.com/mcapuccini/spark-vs</ext-link>. Accessed 13 July 2016</mixed-citation>
    </ref>
  </ref-list>
</back>
