<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ1813 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEgr8 jpg ?>
<?FILEgr9 jpg ?>
<?FILEga1 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?FILEsi4 svg ?>
<?FILEsi5 svg ?>
<?FILEsi6 svg ?>
<?FILEsi7 svg ?>
<?FILEsi8 svg ?>
<?FILEsi9 svg ?>
<?FILEsi10 svg ?>
<?FILEsi11 svg ?>
<?FILEsi12 svg ?>
<?FILEsi13 svg ?>
<?FILEsi14 svg ?>
<?FILEsi15 svg ?>
<?FILEsi16 svg ?>
<?FILEsi17 svg ?>
<?FILEsi18 svg ?>
<?FILEsi19 svg ?>
<?FILEsi20 svg ?>
<?FILEsi21 svg ?>
<?FILEsi22 svg ?>
<?FILEsi23 svg ?>
<?FILEsi24 svg ?>
<?FILEsi25 svg ?>
<?FILEsi26 svg ?>
<?FILEsi27 svg ?>
<?FILEsi28 svg ?>
<?FILEsi29 svg ?>
<?FILEsi30 svg ?>
<?FILEsi31 svg ?>
<?FILEsi32 svg ?>
<?FILEsi33 svg ?>
<?FILEsi34 svg ?>
<?FILEsi35 svg ?>
<?FILEsi36 svg ?>
<?FILEsi37 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9546503</article-id>
    <article-id pub-id-type="pii">S2001-0370(22)00459-7</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2022.10.012</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Cross-attention PHV: Prediction of human and virus protein-protein interactions using cross-attention–based neural networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Tsukiyama</surname>
          <given-names>Sho</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>Kurata</surname>
          <given-names>Hiroyuki</given-names>
        </name>
        <email>kurata@bio.kyutech.ac.jp</email>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="af005">Department of Bioscience and Bioinformatics, Kyushu Institute of Technology, 680-4 Kawazu, Iizuka, Fukuoka 820-8502, Japan</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author. <email>kurata@bio.kyutech.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>08</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>20</volume>
    <fpage>5564</fpage>
    <lpage>5573</lpage>
    <history>
      <date date-type="received">
        <day>9</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>5</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>5</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Author(s)</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="graphical" id="ab005">
      <title>Graphical abstract</title>
      <fig id="f0050" position="anchor">
        <graphic xlink:href="ga1"/>
      </fig>
    </abstract>
    <abstract abstract-type="author-highlights" id="ab010">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="l0005">
          <list-item id="o0005">
            <label>•</label>
            <p id="p0005">Cross-attention PHV implements two key technologies: cross-attention mechanism and 1D-CNN.</p>
          </list-item>
          <list-item id="o0010">
            <label>•</label>
            <p id="p0010">It accurately predicts PPIs between human and unknown influenza viruses/SARS-CoV-2.</p>
          </list-item>
          <list-item id="o0015">
            <label>•</label>
            <p id="p0015">It extracts critical taxonomic and evolutionary differences responsible for PPI prediction.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract id="ab015">
      <p>Viral infections represent a major health concern worldwide. The alarming rate at which SARS-CoV-2 spreads, for example, led to a worldwide pandemic. Viruses incorporate genetic material into the host genome to hijack host cell functions such as the cell cycle and apoptosis. In these viral processes, protein–protein interactions (PPIs) play critical roles. Therefore, the identification of PPIs between humans and viruses is crucial for understanding the infection mechanism and host immune responses to viral infections and for discovering effective drugs. Experimental methods including mass spectrometry-based proteomics and yeast two-hybrid assays are widely used to identify human-virus PPIs, but these experimental methods are time-consuming, expensive, and laborious. To overcome this problem, we developed a novel computational predictor, named cross-attention PHV, by implementing two key technologies of the cross-attention mechanism and a one-dimensional convolutional neural network (1D-CNN). The cross-attention mechanisms were very effective in enhancing prediction and generalization abilities. Application of 1D-CNN to the word2vec-generated feature matrices reduced computational costs, thus extending the allowable length of protein sequences to 9000 amino acid residues. Cross-attention PHV outperformed existing state-of-the-art models using a benchmark dataset and accurately predicted PPIs for unknown viruses. Cross-attention PHV also predicted human–SARS-CoV-2 PPIs with area under the curve values &gt;0.95. The Cross-attention PHV web server and source codes are freely available at <ext-link ext-link-type="uri" xlink:href="https://kurata35.bio.kyutech.ac.jp/Cross-attention_PHV/" id="ir005">https://kurata35.bio.kyutech.ac.jp/Cross-attention_PHV/</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/kuratahiroyuki/Cross-Attention_PHV" id="ir010">https://github.com/kuratahiroyuki/Cross-Attention_PHV</ext-link>, respectively.</p>
    </abstract>
    <kwd-group id="kg005">
      <title>Keywords</title>
      <kwd>Human</kwd>
      <kwd>Virus</kwd>
      <kwd>Protein–protein interaction</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>Word2vec</kwd>
      <kwd>SARS-CoV-2</kwd>
    </kwd-group>
    <kwd-group id="pc_QKkoCvkVcy4bU5tN5yyKJsvLPazTbUPL">
      <title>Abbreviations</title>
      <kwd>SARS-CoV-2, Severe acute respiratory syndrome coronavirus 2</kwd>
      <kwd>PPIs, Protein-protein interactions</kwd>
      <kwd>HV-PPIs, Human-virus PPIs</kwd>
      <kwd>CNN, Convolutional neural network</kwd>
      <kwd>1D-CNN, One-dimensional-CNN</kwd>
      <kwd>HuV-PPI, Human–unknown virus PPI</kwd>
      <kwd>AC, Accuracy</kwd>
      <kwd>AUC, Area under the curve</kwd>
      <kwd>SN, Sensitivity</kwd>
      <kwd>SP, Specificity</kwd>
      <kwd>MCC, Matthews correlation coefficient</kwd>
      <kwd>F1, F1-score</kwd>
      <kwd>T-SNE, T-distributed stochastic neighbor embedding</kwd>
      <kwd>DT, Decision tree</kwd>
      <kwd>LR, Linear regression</kwd>
      <kwd>RF, Random forest</kwd>
      <kwd>SVM, Support vector machine</kwd>
      <kwd>W2V, Word2vec</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0020">Viral infections represent a major health concern worldwide. The alarming rate at which severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spreads, for example, led to a worldwide pandemic. According to the World Health Organization, more than 280 million people have been infected with SARS-CoV-2, and 5 million people had died by December 2021 <xref rid="b0005" ref-type="bibr">[1]</xref>.</p>
    <p id="p0025">Viruses enter host cells by interacting with receptors on the plasma membrane or by inducing endocytosis or membrane fusion <xref rid="b0010" ref-type="bibr">[2]</xref>, <xref rid="b0015" ref-type="bibr">[3]</xref>, <xref rid="b0020" ref-type="bibr">[4]</xref>. To create an environment that promotes clone proliferation, viruses then incorporate its genetic material into the host genome to hijack host cell functions such as the cell cycle and apoptosis <xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0030" ref-type="bibr">[6]</xref>, <xref rid="b0035" ref-type="bibr">[7]</xref>. In these viral processes, protein–protein interactions (PPIs) play critical roles. Therefore, the identification of human and virus PPIs is crucial for understanding the infection mechanism and host immune responses and for discovering effective drugs. Experimental methods including mass spectrometry-based proteomics and yeast two-hybrid assays are widely used to identify human-virus PPIs (HV-PPIs). Specifically, high-throughput methods provided a large amount data related to PPIs <xref rid="b0040" ref-type="bibr">[8]</xref>. However, these experimental methods are not suitable for measuring all protein pairs because they are time-consuming, expensive, and laborious. To complement these existing experimental methods, various computational approaches have been adapted. Many protein structure–based PPI predictors have been proposed <xref rid="b0045" ref-type="bibr">[9]</xref>, <xref rid="b0050" ref-type="bibr">[10]</xref>, <xref rid="b0055" ref-type="bibr">[11]</xref>, but these tools are limited to predicting proteins for which the structure is known. As amino acid sequences are abundant and accessible, sequence-based prediction methods using machine and deep learning approaches have attracted attention.</p>
    <p id="p0030">Eid et al. proposed a sequence-based HV-PPI predictor, Denovo, which exhibited biological soundness and robust predictions by using two important methods <xref rid="b0060" ref-type="bibr">[12]</xref>. First, they constructed test datasets that included PPIs of viruses that are taxonomically far from viruses involved in PPIs of the training dataset according to rank in a taxonomy tree. Second, they employed a “dissimilarity-based negative sampling” method. Generally, negative data are essential for training models, but databases including non-interacting protein pairs do not exist, as far as we know. Therefore, negative samples need to be generated artificially. Many studies randomly sampled negative PPI pairs without any experimentally verification <xref rid="b0065" ref-type="bibr">[13]</xref>, but this resulted in the generation of numerous false-negative results <xref rid="b0070" ref-type="bibr">[14]</xref>. To deal with this issue, a dissimilarity-based negative sampling method was used.</p>
    <p id="p0035">In the prediction of PPIs using machine learning models, amino acid sequences were encoded based on physicochemical properties, domain profiles, and sequence composition <xref rid="b0075" ref-type="bibr">[15]</xref>, <xref rid="b0080" ref-type="bibr">[16]</xref>. Zhou et al. and Alguwaizani et al. generated feature vectors using compositional information to build SVM models <xref rid="b0085" ref-type="bibr">[17]</xref>, <xref rid="b0090" ref-type="bibr">[18]</xref>. Their methods, which were evaluated with Denovo's datasets, predicted HV-PPIs with an accuracy (AC) of around 85 %. Furthermore, to enable predictions involving unknown viruses, the models were evaluated with a test dataset that excluded the virus species employed in the training dataset.</p>
    <p id="p0040">It is difficult for classical machine learning methods to extract local sequence patterns because they do not directly encode amino acid sequence-order information. Deep learning–based models have overcome such problems. For example, Yang et al. embedded local features such as binding motifs into feature matrices and captured their patterns using a convolutional neural network (CNN) <xref rid="b0095" ref-type="bibr">[19]</xref>. They applied two different transfer learning methods to improve the generalizability of the model. Liu-Wei et al. developed a CNN-based HV-PPI predictor called DeepViral <xref rid="b0100" ref-type="bibr">[20]</xref> by using not only sequence data but also disease phenotypes such as signs and symptoms.</p>
    <p id="p0045">Recently, the word2vec <xref rid="b0105" ref-type="bibr">[21]</xref> and doc2vec <xref rid="b0110" ref-type="bibr">[22]</xref> methods, which were invented in the field of natural language processing, have been applied for various biological predictions, including DNA N6-methyladenine site prediction <xref rid="b0115" ref-type="bibr">[23]</xref>, bitter peptide prediction <xref rid="b0120" ref-type="bibr">[24]</xref>, anti-virus peptide prediction <xref rid="b0125" ref-type="bibr">[25]</xref>, and prediction of compound-protein interactions <xref rid="b0130" ref-type="bibr">[26]</xref>. These methods employ unsupervised embedding techniques to vectorize documents and words. For the prediction of HV-PPIs, Yang et al. used doc2vec-based embedding to extract contextual information from amino acid sequences <xref rid="b0135" ref-type="bibr">[27]</xref>. We developed the long short-term memory LSTM-PHV <xref rid="b0140" ref-type="bibr">[28]</xref> by adopting word2vec to consider consecutive 4-mers of amino acid sequences as words and captured the represented contextual information using an LSTM-based neural network. We demonstrated that LSTM-PHV accurately predicted HV-PPIs, whereas LSTM-PHV exhibited high computational and memory costs due to the recurrent LSTM computations. These methods exhibited good PPI prediction performance, but there is still room for improvement in predicting PPIs of unknown virus species.</p>
    <p id="p0050">To overcome such problems, we developed a novel sequence-based HV-PPI prediction model named cross-attention PHV. Neural networks have led to breakthroughs in various fields including natural language processing <xref rid="b0145" ref-type="bibr">[29]</xref>, <xref rid="b0150" ref-type="bibr">[30]</xref> and image processing <xref rid="b0155" ref-type="bibr">[31]</xref>, <xref rid="b0160" ref-type="bibr">[32]</xref>. We adopted the cross-attention PHV, an attention-based neural network, because the attention mechanisms directly capture the complicated relationship among local sequence features than CNN-type and RNN-type neural networks. Furthermore, we proposed the cross-attention network which extracts some relationships between local sequences in the human and virus proteins by simultaneously inputting the human and virus sequences-related information into a single attention-based neural network. It should be noted that this is the first application of cross-attention to PPI prediction, to the best of our knowledge. Furthermore, we applied a one-dimensional-CNN (1D-CNN) approach to increase the calculation speed, which resulted in extending the allowable length of protein sequences for training to 9000 residues, whereas recent neural network-based methods target proteins with sequence less than 2000 residues. The proposed method outperformed state-of-the-art models on Denovo's datasets and accurately predicted unknown viral HV-PPIs.</p>
  </sec>
  <sec id="s0010">
    <label>2</label>
    <title>Materials and methods</title>
    <sec id="s0015">
      <label>2.1</label>
      <title>Dataset construction</title>
      <p id="p0055">We employed a benchmark dataset constructed by Eid et al. to compare the cross-attention PHV with previous models and constructed two datasets to assess the generalizability and robustness of models without overestimating <xref rid="b0060" ref-type="bibr">[12]</xref>. The numbers of long and short proteins were presented in the <xref rid="s0135" ref-type="sec">Table S1</xref>.</p>
      <sec id="s0020">
        <label>2.1.1</label>
        <title>Denovo's dataset</title>
        <p id="p0060">Firstly, we used Denovo’s dataset which was used in many previous studies for HV-PPIs prediction. It consists of a training dataset with 5020 positive and 4734 negative samples and an independent test dataset with 425 positive and 425 negative samples <xref rid="b0060" ref-type="bibr">[12]</xref>. We removed from the training dataset HV-PPIs that involved proteins with non-standard amino acids. The resultant training dataset included 5016 positive samples and 4732 negative samples.</p>
      </sec>
      <sec id="s0025">
        <label>2.1.2</label>
        <title>Human–unknown virus PPI dataset</title>
        <p id="p0065">Secondly, to verify whether our proposed model predicts PPIs involving unknown viruses, we constructed a human–unknown virus PPI (HuV-PPI) dataset consisting of datasets of three influenza viruses, H1N1, H3N2, and H5N1. The HuV-PPI datasets were composed of training datasets without any samples of influenza viruses and the independent test datasets that included only samples of the influenza viruses, as shown in <xref rid="t0005" ref-type="table">Table 1</xref>. In each dataset, to reduce the bias depending on the sample sizes in positive and negative samples, we randomly collected the negative samples so that the number of negative samples was equal to that of positive samples. Each dataset was divided into the training and validation data at a ratio of 4:1.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>Statistical features of the HuV-PPI dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>Virus in training data</th><th>Virus in test data</th><th>Training samples</th><th>Test samples</th></tr></thead><tbody><tr><td>H1N1</td><td>Viruses other than H1N1</td><td>H1N1</td><td>64,934</td><td>18,136</td></tr><tr><td>H3N2</td><td>Viruses other than H3N2</td><td>H3N2</td><td>81,554</td><td>1516</td></tr><tr><td>H5N1</td><td>Viruses other than H5N1</td><td>H5N1</td><td>81,868</td><td>1170</td></tr></tbody></table></table-wrap></p>
        <p id="p0070">Details regarding construction of the HuV-PPI datasets are described as follows. The PPIs and protein sequences were downloaded from the HVIDB <xref rid="b0165" ref-type="bibr">[33]</xref> and UniProtKB (Swiss-Prot and TrEMBL) databases <xref rid="b0170" ref-type="bibr">[34]</xref>, respectively. We then removed the HV-PPIs involving proteins with a length of less than 30 or greater than 9000 residues and included non-standard amino acids. Negative samples were constructed using the dissimilarity-based negative sampling method reported by Eid et al. <xref rid="b0060" ref-type="bibr">[12]</xref> with the same parameters as in our previous study <xref rid="b0140" ref-type="bibr">[28]</xref>. Eid et al. hypothesized that the virus proteins with similar sequences could interact with the many common host proteins. This method uses a sequence similarity measure to search the protein pairs that are unlikely to interact with each other. When virus protein A interacts with human protein B, the method assumed that virus protein C, which exhibits less sequence similarity to virus protein A at an identity threshold of <italic>T</italic>, does not interact with human protein B. The pair of human protein B and virus protein C is thus a candidate negative sample. According to this approach, we compiled the negative PPI samples as follows. We combined human proteins registered in the UniProtKB/Swiss-Prot database with virus proteins included among the positive PPI samples. From the resulting human and virus protein pairs, we removed the pairs of the positive PPIs and further deleted the pairs of the human proteins and virus proteins that exhibited higher similarity (<italic>T</italic> &gt; 0.2) to the human protein-interacting virus proteins. We computed the sequence similarities for all the pairs of virus proteins included in the positive PPI samples by using the Needleman-Wunsch algorithm with BLOSUM62 <xref rid="b0175" ref-type="bibr">[35]</xref>.</p>
      </sec>
      <sec id="s0030">
        <label>2.1.3</label>
        <title>Human–SARS-CoV-2 PPI dataset construction</title>
        <p id="p0075">Thirdly, to investigate the usefulness of prediction in human-SARS-CoV-2 PPIs, we constructed the human-SARS-CoV-2 PPI dataset. PPIs between human and SARS-CoV-2 were downloaded from the BioGRID database (COVID-19 Coronavirus Project 4.4.205) <xref rid="b0180" ref-type="bibr">[36]</xref>. The sequence of each protein was retrieved from the UniProtKB database <xref rid="b0170" ref-type="bibr">[34]</xref>. We removed PPIs involving proteins with a length of greater than 9000 residues or less than 30 residues and those that included non-standard amino acids. The remaining 14,218 PPIs were used as positive samples. Negative samples were generated by applying the dissimilarity-based negative sampling method to human proteins retrieved from the UniProtKB/Swiss-Prot database <xref rid="b0170" ref-type="bibr">[34]</xref>. The identity threshold was set to 0.2. The resultant dataset was named the human–SARS-CoV-2 PPI dataset. To build the balanced and imbalanced datasets, negative samples were randomly selected so that the ratios of positive to negative samples were 1:1 and 1:5, respectively. While non-interacting protein pairs are extremely abundant than interacting ones, we included 5 times more negative samples than positive samples in the data in terms of sample size and model training time. As shown in <xref rid="t0010" ref-type="table">Table 2</xref>, the balanced and imbalanced datasets consisted of 28,436 and 85,308 samples, respectively. The resultant datasets were divided into training and test datasets at a ratio of 4:1.<table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>Statistical features of the SARS-CoV-2-PPI dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>All samples</th><th>Positive samples</th><th>Negative samples</th><th>Human proteins</th><th>Virus proteins</th></tr></thead><tbody><tr><td>Balanced</td><td>28,436</td><td>14,218</td><td>14,218</td><td>14,426</td><td>14</td></tr><tr><td>Imbalanced</td><td>85,308</td><td>14,218</td><td>71,090</td><td>20,192</td><td>14</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="s0035">
      <label>2.2</label>
      <title>Feature encoding methods</title>
      <p id="p0080">The query sequences of human and virus proteins were encoded into feature matrices using word2vec, which generates distributed representations of words through a task that predicts a target word from its surrounding words (Continuous Bag-of-Words Model; CBOW) or predicts surrounding words from a target word (Continuous Skip-Gram Model; Skip-Gram). Although CBOW was employed in our previous study due to low computational cost <xref rid="b0140" ref-type="bibr">[28]</xref>, Skip-Gram was used in the present study because of its greater capacity to learn contextual information than CBOW <xref rid="b0105" ref-type="bibr">[21]</xref>. Protein sequences were tokenized into consecutive k-mer amino acids (<xref rid="f0005" ref-type="fig">Fig. 1</xref>A).<fig id="f0005"><label>Fig. 1</label><caption><p>Workflow of word2vec-based encoding. (A) Amino acid sequences were converted into arrangements of consecutive 4-mers. (B) Amino acid sequences in the UniProtKB/Swiss-Prot database were converted as representations of 4-mers and used for training the word2vec model. (C) Each 4-mer in the amino acid sequence was converted into a feature vector using the trained word2vec model. The resultant feature vectors were concatenated into a feature matrix.</p></caption><graphic xlink:href="gr1"/></fig></p>
      <p id="p0085">To train the word2vec model, we used the protein sequences in the UniProtKB/Swiss-Prot database <xref rid="b0170" ref-type="bibr">[34]</xref> (<xref rid="f0005" ref-type="fig">Fig. 1</xref>B). Sequences with non-standard amino acids and those with a length greater than 9000 residues were excluded, and redundant sequences were then removed using CD-HIT with a threshold of 0.9 <xref rid="b0185" ref-type="bibr">[37]</xref>. The remaining sequences were used to train the word2vec model. The maximum distance between a target k-mer and its surrounding k-mers (window size) and training iteration were set to 5 and 100, respectively.</p>
      <p id="p0090">Consecutive k-mer amino acids of protein sequences were transformed by the trained word2vec model into 128-dimensional feature vectors. These vectors were concatenated in the order of the sequences (<xref rid="f0005" ref-type="fig">Fig. 1</xref>C) to arrange the feature matrixes with a shape of (9000 − k + 1) × 128, where zero-padding was applied so that the maximum sequence size was 9000. We constructed the word2vec model using Genism (version: 3.8.3) in the Python package (version: 3.8.0).</p>
    </sec>
    <sec id="s0040">
      <label>2.3</label>
      <title>Neural networks</title>
      <sec id="s0045">
        <label>2.3.1</label>
        <title>1D-CNN with max-pooling layer</title>
        <p id="p0095">To extract the features of local sequences such as binding motifs, 1-D convolutional layers were used. In these layers, input matrix <italic>X</italic> with <italic>n</italic> length and <italic>s</italic> channels was converted to feature matrix <italic>C</italic> with <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">-</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> length and <inline-formula><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> channels using sliding with shift width <inline-formula><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M4" altimg="si2.svg"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> filters with size <inline-formula><mml:math id="M5" altimg="si4.svg"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula>. The <italic>i</italic>-th element <inline-formula><mml:math id="M6" altimg="si5.svg"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of the matrix generated by the <italic>k</italic>-th filter <inline-formula><mml:math id="M7" altimg="si6.svg"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is given by:<disp-formula id="e0005"><label>(1)</label><mml:math id="M8" altimg="si7.svg"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>w</mml:mi></mml:msubsup><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo stretchy="false">/</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p id="p0100">The pooling layer was placed at the position following the convolutional layers to suppress overfitting and increase generalization ability. The max-pooling layer samples the maximum values from the certain area of the input as follows:<disp-formula id="e0010"><label>(2)</label><mml:math id="M9" altimg="si8.svg"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M10" altimg="si9.svg"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> represents the size of the pooling window. Zero-padding was applied to the input matrix so that the lengths of the input and output of the pooling layer were the same. A global max-pooling layer generated a vector by sampling the maximum value from each channel of the output as follows:<disp-formula id="e0015"><label>(3)</label><mml:math id="M11" altimg="si10.svg"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfenced><mml:mo stretchy="false">/</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      </sec>
      <sec id="s0050">
        <label>2.3.2</label>
        <title>Attention mechanism</title>
        <p id="p0105">Attention mechanism has been an important contributor to the remarkable advances that have occurred in neural network development, and it has been incorporated in recent neural network models such as BERT <xref rid="b0190" ref-type="bibr">[38]</xref> and Transformer <xref rid="b0195" ref-type="bibr">[39]</xref>. In the attention mechanism, output feature <inline-formula><mml:math id="M12" altimg="si11.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is generated by updating pre-updated feature <inline-formula><mml:math id="M13" altimg="si12.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> with information-giving feature <inline-formula><mml:math id="M14" altimg="si13.svg"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>. For the update, three representations, known as Query, Key, and Value, are generated by applying three different learnable weights, <inline-formula><mml:math id="M15" altimg="si14.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>Q</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M16" altimg="si15.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M17" altimg="si16.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, to these features, as follows:<disp-formula id="e0020"><label>(4)</label><mml:math id="M18" altimg="si17.svg"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>Q</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="e0025"><label>(5)</label><mml:math id="M19" altimg="si18.svg"><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="e0030"><label>(6)</label><mml:math id="M20" altimg="si19.svg"><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="M21" altimg="si20.svg"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M22" altimg="si21.svg"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> indicate the <italic>i</italic>-th feature vectors of <inline-formula><mml:math id="M23" altimg="si12.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M24" altimg="si13.svg"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula>, respectively, and <inline-formula><mml:math id="M25" altimg="si22.svg"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mo>∙</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M26" altimg="si23.svg"><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mo>∙</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M27" altimg="si24.svg"><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mo>∙</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> represent the transformation functions for calculating Query, Key, and Value, respectively. Next, attention weight <inline-formula><mml:math id="M28" altimg="si25.svg"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which determines the degree of influence of <inline-formula><mml:math id="M29" altimg="si26.svg"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on the calculation of updated vector <inline-formula><mml:math id="M30" altimg="si27.svg"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> of <inline-formula><mml:math id="M31" altimg="si20.svg"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, is given by scaling the dot-product between Key and Query with dimension <inline-formula><mml:math id="M32" altimg="si28.svg"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="italic">key</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of Key and by applying the masking and softmax functions to the scaled dot-product, as follows:<disp-formula id="e0035"><label>(7)</label><mml:math id="M33" altimg="si29.svg"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>M</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:msup><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="italic">key</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0110">In the masking, the elements of padding position are set to minus infinity. Consequently, the effect of zero-padding can be neglected after applying the softmax function. To selectively extract information from the Value, depending on the relationship between Key and Query, the weighted sum of Value is calculated with the attention weights, as follows:<disp-formula id="e0040"><label>(8)</label><mml:math id="M34" altimg="si30.svg"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">Attention</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="p0115">Next, the feature <inline-formula><mml:math id="M35" altimg="si11.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">out</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is updated as follows:<disp-formula id="e0045"><label>(9)</label><mml:math id="M36" altimg="si31.svg"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi mathvariant="italic">out</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">Attention</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo linebreak="badbreak">+</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula>where learnable weight <inline-formula><mml:math id="M37" altimg="si32.svg"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is applied to the weighted sum. In the multi-head attention layers, the weighted sums are calculated in parallel in each “head”, concatenated, and applied by <inline-formula><mml:math id="M38" altimg="si32.svg"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
    </sec>
    <sec id="s0055">
      <label>2.4</label>
      <title>Cross-attention PHV</title>
      <p id="p0120">As shown in <xref rid="f0010" ref-type="fig">Fig. 2</xref>, cross-attention PHV is composed of three sub-networks: (1) convolutional embedding modules, (2) a cross-attention network module, and (3) a feature integration network. The key technologies are to use 1D-CNN, which effectively extracts the features of long-length sequences of human and virus proteins and to develop a cross-attention module that extracts some feature interactions between human and virus proteins as the core of the learning method. Importantly, the attention modules represent human and virus proteins to capture global information regarding the amino acid sequences. We crossed the two attention modules to mutually consider the features of human and virus proteins.<fig id="f0010"><label>Fig. 2</label><caption><p>Structure of cross-attention PHV. Cross-attention PHV is composed of three sub-networks. The word2vec (W2V)-based feature matrices of humans and viruses were input into the convolutional embedding module. To extract interaction features between two protein sequences, multi-head attention layers were employed in the cross-attention module. Finally, the feature vectors generated by the global max-pooling layer were concatenated to compute a final score through three linear layers.</p></caption><graphic xlink:href="gr2"/></fig></p>
      <p id="p0125">In the convolutional embedding modules, the word2vec-generated matrices of human and virus proteins were filtered by 1D-CNN layers with 128 filters and by max-pooling layers with a pooling window of 3. The size and shift width of the filters were set to 20 and 10, respectively. Based on a previous study <xref rid="b0200" ref-type="bibr">[40]</xref>, we inserted a dropout layer with a ratio of 0.5 between the 1D-CNN and the pooling layers. This transformation provides two advantages. One advantage is that it significantly decreases computational cost and memory usage for the next layers due to the reduced dimension of the feature matrix. Another advantage is that each vector of these feature matrices is generated from consecutive k-mer amino acids, which makes it possible to learn the dependencies among local patterns such as motifs.</p>
      <p id="p0130">In the cross-attention network module, the filtered matrices of human and virus proteins in the first sub-network were input into cross-attention modules consisting of two multi-head attention modules. We crossed the two attention modules to extract the features of human and virus proteins while referring to virus and human protein features, respectively. Specifically, one attention module applied the Query from the human feature matrix to the Keys from the virus feature matrix to calculate the attention weights, generating the attention of the Values of the virus feature matrix. This attention was used to extract virus protein features related to the Query from human protein features. In the same manner, the other attention module extracted the human protein features related to the virus protein features. The number of heads and the dimension of feature representations (Query, Key, Value) were set to 4 and 32, respectively.</p>
      <p id="p0135">In the feature integration network, the feature matrices processed by the cross-attention modules were transformed into feature vectors by global max-pooling layers having a dropout layer with a dropout ratio of 0.5. The feature vectors were then concatenated and transferred to the three fully connected layers to compute the final output. The hidden vectors from the first and second fully connected layers were dropped out at a ratio of 0.3. The vector dimensions from the first and second fully connected layers were set to 64 and 16, respectively. We constructed the whole neural network model using PyTorch in the Python package (version: 3.8.0).</p>
    </sec>
    <sec id="s0060">
      <label>2.5</label>
      <title>Training and testing</title>
      <p id="p0140">In the training scheme, loss was calculated using a binary cross-entropy function for each mini-batch of size 32. Optimization was executed using the Adam optimizer with a learning rate of 0.0001. To prevent over-learning, the training was stopped (early stopping) when the maximum area under the curve (AUC) was not improved for 20 consecutive epochs.</p>
    </sec>
    <sec id="s0065">
      <label>2.6</label>
      <title>Measures</title>
      <p id="p0145">Six statistical measures were employed to evaluate the trained model: sensitivity (SN; recall), specificity (SP), accuracy (AC), Matthews correlation coefficient (MCC), F1-score (F1), and AUC. The formulas for calculating the measures other than AUC are given by:<disp-formula id="e0050"><label>(10)</label><mml:math id="M39" altimg="si33.svg"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="e0055"><label>(11)</label><mml:math id="M40" altimg="si34.svg"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="e0060"><label>(12)</label><mml:math id="M41" altimg="si35.svg"><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="e0065"><label>(13)</label><mml:math id="M42" altimg="si36.svg"><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:msqrt><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="e0070"><label>(14)</label><mml:math id="M43" altimg="si37.svg"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where TP, FP, TN, and FN indicate the numbers of true-positive, false-positive, true-negative, and false-negative samples, respectively. The threshold responsible for determining whether PPIs occur was set to 0.5. In addition to the standard measurements of SN, SP, AC, and AUC, to evaluate the performances on imbalanced datasets, Matthews correlation coefficient (MCC) and F1-score (F1) were used. MCC is regarded as a good measure that considers all four entries of the confusion matrix. F1 is a reasonable accuracy metric given by the precision and recall that take into account class imbalance. F1 scores ignore the count of true negatives and is highly influenced by which class is labeled as positive. These measures were computed by using scikit-learn of the Python package <xref rid="b0205" ref-type="bibr">[41]</xref>.</p>
    </sec>
    <sec id="s0070">
      <label>2.7</label>
      <title>Visualization of features</title>
      <p id="p0150">To visualize feature vectors, we used <italic>t</italic>-distributed stochastic neighbor embedding (<italic>t</italic>-SNE) <xref rid="b0210" ref-type="bibr">[42]</xref>. Compared with classical linear mapping methods such as principal component analysis and multiple discriminant analysis, <italic>t</italic>-SNE precisely projects both local and global structures of high-dimensional vectors into low-dimensional representations and is suitable for visualization of nonlinear data. The perplexity in <italic>t</italic>-SNE was set to 50.</p>
    </sec>
  </sec>
  <sec id="s0075">
    <label>3</label>
    <title>Results and discussion</title>
    <sec id="s0080">
      <label>3.1</label>
      <title>Optimization of cross-attention PHV</title>
      <p id="p0155">We encoded the protein sequences into the feature matrices using the word2vec model and trained cross-attention PHV using Denovo’s training dataset. To achieve the best model, we optimized the k-mer value between 2 and 4 via 5-fold cross-validation, where the all k-mers in sequences were covered by the training dataset of word2vec. The training data were divided into 5 subsets, and then 4 subsets were used for training the model; the remaining subset was used for validation. Cross-attention PHV presented AUCs &gt; 0.97 on average, and the 4-mer model provided the highest values in terms of AC, MCC, AUC, and F1 (<xref rid="f0015" ref-type="fig">Fig. 3</xref>). Thus, we set the k-mer value to 4.<fig id="f0015"><label>Fig. 3</label><caption><p>Prediction performance of word2vec-based cross-attention PHV with respect to k-mer value. The models were evaluated via 5-fold cross-validation on Denovo's training dataset.</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0160">To demonstrate the superiority of word2vec, we used a binary encoding as the reference to construct the binary encoding–based cross-attention PHV. The binary encoding concatenated the one-hot vectors of each amino acid in the order of the protein sequence. The word2vec-based and binary encoding–based models were trained via 5-fold cross-validation on Denovo’s training dataset and evaluated using Denovo's test dataset. To compare the two encoding methods, a two-sample <italic>t</italic>-test was applied to the AUC and AC values. As shown in <xref rid="f0020" ref-type="fig">Fig. 4</xref>, the word2vec-based model (cross-attention PHV) provided significantly better performance than the binary encoding–based model (AUC; <italic>p</italic>-value &lt; 0.01, AC; <italic>p</italic>-value &lt; 0.05), suggesting that word2vec efficiently represents protein sequence contextual information.<fig id="f0020"><label>Fig. 4</label><caption><p>Comparison of performance between the word2vec-based and binary encodings in cross-attention PHV. Models trained via 5-fold cross-validation were evaluated with Denovo's test dataset.</p></caption><graphic xlink:href="gr4"/></fig></p>
      <p id="p0165">The cross-attention modules were expected to extract features of human and virus proteins by considering various relationships between local patterns in the two protein sequences. We compared cross-attention PHV with a self-attention–based neural network in which the features of human and virus proteins were input separately to multi-head attention modules without any interactions (<xref rid="s0135" ref-type="sec">Fig. S1</xref>). Both the cross-attention PHV and the self-attention–based neural network were trained via 5-fold cross-validation with Denovo's training dataset. The cross-attention PHV presents a little higher values in 5 out of 6 measurements than the self-attention network on the independent test (<xref rid="f0025" ref-type="fig">Fig. 5</xref>). Particularly, the AUCs of the cross-attention PHV was significantly higher than that of the self-attention-based neural network (one-sided two-sample paired <italic>t</italic>-test; p-value &lt; 0.05). AUC is independent of the threshold value and measures the abilities of model’s discriminability in binary classification. We suggest that the extraction of relationship between local sequences in human and virus proteins is effective for the human-virus PPI prediction.<fig id="f0025"><label>Fig. 5</label><caption><p>Comparison of performance between cross-attention–based and self-attention–based neural networks. Models trained via 5-fold cross-validation were evaluated with Denovo's test dataset.</p></caption><graphic xlink:href="gr5"/></fig></p>
    </sec>
    <sec id="s0085">
      <label>3.2</label>
      <title>Prediction of cross-attention PHV for long proteins</title>
      <p id="p0170">To validate the performances of cross-attention PHV in PPIs with long proteins, we compared the prediction performances of the PPIs composed of long human or virus proteins with those composed of short human and virus proteins in the independent test with Denovo's dataset, where the length of long proteins is more than 2000 and that of short proteins is less than 2000. As shown in <xref rid="s0135" ref-type="sec">Table S2</xref>, in prediction for long proteins, cross-attention PHV presented the AUC of more than 0.96 and accuracies of more than 0.94 which are slightly low or comparable measure values as that for short proteins, respectively, suggesting the usefulness of cross-attention PHV for the prediction of PPIs with long proteins.</p>
    </sec>
    <sec id="s0090">
      <label>3.3</label>
      <title>Comparison of state-of-the-art methods</title>
      <p id="p0175">We compared the performance of cross-attention PHV with seven state-of-the-art methods, including Denovo <xref rid="b0060" ref-type="bibr">[12]</xref>, Zhou et al.’s SVM-based method <xref rid="b0085" ref-type="bibr">[17]</xref>, Alguwaizani et al.’s SVM-based method <xref rid="b0090" ref-type="bibr">[18]</xref>, Yang et al.’s random forest–based and Doc2vec-based method <xref rid="b0135" ref-type="bibr">[27]</xref>, DeepViral <xref rid="b0100" ref-type="bibr">[20]</xref>, and Yang et al.’s CNN-based method <xref rid="b0095" ref-type="bibr">[19]</xref>, using Denovo’s test dataset. As shown in <xref rid="t0015" ref-type="table">Table 3</xref>, cross-attention PHV predicted the PPIs with an AC value &gt;0.95 and outperformed the state-of-the-art models in five metrics, including SN, AC, AUC, MCC, and F1, demonstrating the superiority of cross-attention PHV. Generally, conventional descriptors including amino acid composition, physicochemical properties and evolutionary information have difficulty in capturing contextual information of sequences because they do not hold the information regarding the order of amino acid residues. On the other hand, the word2vec, which is employed by cross-attention PHV, captures the contextual information because it learns the distributed representation of words. Compared with the CNN-type and RNN-type neural networks, attention-based neural networks directly extract some relationships between local sequence features. We consider that the high prediction performance of cross-attention PHV is attributed to capturing of contextual, interrelated information between the two feature matrices of human and virus protein sequences, which are critically important for the PPI prediction.<table-wrap position="float" id="t0015"><label>Table 3</label><caption><p>Comparison of the performance of cross-attention PHV with existing state-of-the-art models on Denovo's test dataset. Data regarding the performance of existing models were obtained from the respective papers. Bold values indicate the highest value for each measurement.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>SN</th><th>SP</th><th>AC</th><th>MCC</th><th>AUC</th><th>F1</th></tr></thead><tbody><tr><td>Denovo [2015]</td><td align="char">0.807</td><td align="char">0.831</td><td align="char">0.819</td><td>NA</td><td>NA</td><td>NA</td></tr><tr><td>Zhou et al.'s model [2018]</td><td align="char">0.800</td><td align="char">0.889</td><td align="char">0.845</td><td>0.692</td><td>0.897</td><td>NA</td></tr><tr><td>Alguwaizani et al.'s model [2018]</td><td align="char">0.864</td><td align="char">0.866</td><td align="char">0.865</td><td>0.729</td><td>0.926</td><td>NA</td></tr><tr><td>Yang et al's model (Doc2vec + RF) [2020]</td><td align="char">0.903</td><td align="char">0.962</td><td align="char">0.932</td><td>0.866</td><td>0.981</td><td>0.931</td></tr><tr><td>DeepViral (seq) [2021]</td><td align="char">0.894</td><td align="char">0.969</td><td align="char">0.931</td><td>0.865</td><td>0.960</td><td>0.929</td></tr><tr><td>DeepViral (joint)[2021]</td><td align="char">0.903</td><td align="char"><bold>0.976</bold></td><td align="char">0.939</td><td>0.881</td><td>0.976</td><td>0.937</td></tr><tr><td>Yang et al.'s model (CNN) [2021]</td><td align="char">0.908</td><td align="char">0.974</td><td align="char">0.941</td><td>NA</td><td>NA</td><td>0.939</td></tr><tr><td>Cross Attention-PHV</td><td align="char"><bold>0.944</bold></td><td align="char">0.967</td><td align="char"><bold>0.956</bold></td><td><bold>0.912</bold></td><td><bold>0.988</bold></td><td><bold>0.955</bold></td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="s0095">
      <label>3.4</label>
      <title>Performance in predicting PPIs between humans and unknown virus species</title>
      <p id="p0180">To evaluate the PPI prediction performance between humans and unknown viruses, we employed the HuV-PPI (H1N1, H3N2, and H5N1) datasets. It should be noted that the training dataset excluded the PPIs of the influenza species employed in the independent test dataset. Twenty percent of the training data were used for validation of early stopping. We compared cross-attention PHV with LSTM-PHV <xref rid="b0140" ref-type="bibr">[28]</xref>, which exhibited the best performance in the year 2021. In the training of LSTM-PHV, proteins with a sequence length greater than 1000 residues were removed in the same manner <xref rid="b0140" ref-type="bibr">[28]</xref> because the method has a significant memory and time cost. As shown in <xref rid="f0030" ref-type="fig">Fig. 6</xref>, cross-attention PHV exhibited AC and AUC values &gt;0.91 and &gt;0.96, respectively, on the independent datasets. Cross-attention PHV outperformed LSTM-PHV, demonstrating the high generalizability of cross-attention PHV. As the encoding method (word2vec) used in cross-attention PHV is the same as that of LSTM-PHV, the cross-attention–based network was found to be more predictive than the LSTM-based network. Furthermore, cross-attention PHV does not require the recursive calculations employed by LSTM-PHV, which greatly reduces computation time. This is also major advantage of cross-attention PHV, as it enables us to process long sequences for the training scheme.<fig id="f0030"><label>Fig. 6</label><caption><p>Comparison of the performance of cross-attention PHV and LSTM-PHV in predicting PPIs for unknown viruses. (A) Performance on the H1N1 dataset, which regards H1N1 as an unknown virus. (B) Performance on the H3N2 dataset, which regards H3N2 as an unknown virus. (C) Performance on the H5N1 dataset, which regards H5N1 as an unknown virus.</p></caption><graphic xlink:href="gr6"/></fig></p>
    </sec>
    <sec id="s0100">
      <label>3.5</label>
      <title>Performance in predicting PPIs between humans and SARS-CoV-2</title>
      <p id="p0185">We also investigated whether cross-attention PHV can predict PPIs between humans and SARS-CoV-2 using the human–SARS-CoV-2 PPI dataset. The models were trained via 5-fold cross-validation with the training dataset and evaluated with its independent test dataset. All measures were averaged over the five models. We characterized cross-attention PHV in comparison with LSTM-PHV and machine learning methods including decision tree (DT), linear regression (LR), random forest (RF), and support vector machine (SVM) with parameters as show in <xref rid="s0135" ref-type="sec">Table S3</xref>. To train the LSTM-PHV, we removed PPIs involving proteins with a sequence length greater than 1000 residues to minimize the computational cost. In those machine learning methods, each amino acid in the human and virus protein sequences was transformed into a feature vector by one-hot encoding and word2vec, and each element of the vectors was averaged in the sequence direction. As shown in <xref rid="f0035" ref-type="fig">Fig. 7</xref>, cross-attention PHV exhibited AUCs &gt; 0.95 with both the balanced and imbalanced datasets. Furthermore, cross-attention PHV exhibited better performance than LSTM-PHV for all measures except SN (<xref rid="f0035" ref-type="fig">Fig. 7</xref>) and machine learning-based methods for all measures except SP (Tables S4-S5). In particular, when the model was trained on a limited number of negative samples in the balanced dataset, cross-attention PHV demonstrated higher generalization ability than LSTM-PHV.<fig id="f0035"><label>Fig. 7</label><caption><p>Comparison of the performance of cross-attention PHV with LSTM-PHV in predicting human–SARS-CoV-2 PPIs. (A) Performance on a balanced dataset (positive:negative = 1:1). (B) Performance on an imbalanced dataset (positive:negative = 1:5).</p></caption><graphic xlink:href="gr7"/></fig></p>
    </sec>
    <sec id="s0105">
      <label>3.6</label>
      <title>Visualization and analysis of feature vectors and matrices</title>
      <p id="p0190">To investigate how each subnetwork of cross-attention PHV contributes to the prediction of PPIs between humans and unknown viruses, <italic>t</italic>-SNE was used to visualize the three features during testing of the HuV-PPI datasets: the word2vec-based feature matrices, hidden feature matrices, and feature vectors (<xref rid="f0010" ref-type="fig">Fig. 2</xref>). Before visualization, the word2vec-based feature matrices and hidden feature matrices were transformed into vectors by sampling the maximum values of each feature. The feature vectors for humans and viruses were then concatenated. As shown in <xref rid="f0040" ref-type="fig">Fig. 8</xref>, for all the three datasets, the distributions of positive and negative PPI samples became clearly separated during testing, suggesting that the convolutional embedding and cross-attention modules extract important features responsible for the prediction. Furthermore, we visualized the concatenated human and virus feature vectors. Interestingly, the shapes of the feature vector distributions differed between humans and viruses (<xref rid="f0045" ref-type="fig">Fig. 9</xref>), reflecting the evolutionary or taxonomic differences between human and virus proteins.<fig id="f0040"><label>Fig. 8</label><caption><p><italic>t</italic>-SNE–based visualization of features generated during prediction of PPIs using the HuV-PPI test datasets. The word2vec-based feature matrices, hidden feature matrices, and feature vectors were retrieved from the neural networks. The feature matrices were transformed into vectors by sampling the maximum values of each feature. The human and virus feature vectors were then concatenated. The <italic>t</italic>-SNE maps for the H1N1, H3N2, and H5N1 datasets are shown at the left, center, and right, respectively. Blue, yellow, green, and red marks indicate false-positive, false-negative, true-negative, and true-positive samples, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr8"/></fig><fig id="f0045"><label>Fig. 9</label><caption><p><italic>t</italic>-SNE–visualized map of the respective human and virus feature vectors on the HuV-PPI datasets.</p></caption><graphic xlink:href="gr9"/></fig></p>
    </sec>
    <sec id="s0110">
      <label>3.7</label>
      <title>Limitation</title>
      <p id="p0195">Our proposed neural network was able to increase the allowable length of protein sequences to 9000 residues because a 1D-CNN was adopted to reduce the dimension of the sequences. However, such a feature extraction method makes it difficult for the attention weight to identify k-mer amino acid residues responsible for interactions. In future work, we hope to propose a methodology to identify amino acids important for interactions.</p>
    </sec>
    <sec id="s0115">
      <label>3.8</label>
      <title>Webserver construction</title>
      <p id="p0200">To facilitate access for the research community, we built a webserver application of the HV-PPI prediction tool using Apache (2.4.18) and Flask (1.1.2). Users can access the server from <ext-link ext-link-type="uri" xlink:href="https://kurata35.bio.kyutech.ac.jp/Cross-attention_PHV/" id="ir015">https://kurata35.bio.kyutech.ac.jp/Cross-attention_PHV/</ext-link> to input or upload human and virus sequences of interest. For other overviews, refer to the help page of the website.</p>
    </sec>
  </sec>
  <sec id="s0120">
    <label>4</label>
    <title>Conclusions</title>
    <p id="p0205">To construct the cross-attention PHV predictor for PPIs between humans and viruses, we applied two key technologies, a cross-attention mechanism and a 1D-CNN. The cross-attention mechanism was very effective in achieving enhanced prediction and generalization to unknown virus species. Application of the 1D-CNN to word2vec-generated feature matrices extended the allowable length of protein sequences to 9000 residues for training scheme. We employed the word2vec model to embed the protein sequences and optimized the k-mer value of the word2vec model. Cross-attention PHV outperformed the state-of-the-art models for the five measures of SN, AC, MCC, AUC, and F1 using Denovo's benchmark dataset. Furthermore, cross-attention PHV outperformed the best model of the year 2021 (LSTM-PHV) in predicting PPIs for unknown viruses. Finally, we demonstrated that cross-attention PHV captures the features responsible for virus infection–related proteins and distinguishes taxonomic and evolutionary differences between human and virus proteins. While we employed a linguistic approach using the word2vec and attention mechanism (Transformer encoder) in this study, we did not consider any conventional descriptors including physicochemical, evolutionary and structural properties. We can combine the linguistic approach and conventional descriptors for enhanced prediction performance. Furthermore, since recently deep learning methods have made a breakthrough in constructing 3D structures in proteins <xref rid="b0215" ref-type="bibr">[43]</xref>, <xref rid="b0220" ref-type="bibr">[44]</xref>, it would be interesting if we integrate the recent structural approaches into the linguistic approaches.</p>
  </sec>
  <sec id="s0125">
    <title>CRediT authorship contribution statement</title>
    <p id="p0210"><bold>Sho Tsukiyama:</bold> Conceptualization, Methodology, Software, Formal analysis, Writing – original draft, Writing – review &amp; editing. <bold>Hiroyuki Kurata:</bold> Conceptualization, Methodology, Writing – original draft, Writing – review &amp; editing.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0215">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <label>1</label>
      <mixed-citation publication-type="other" id="h0005">World Health Organization et al. Coronavirus disease (covid-19) situation dashboard. https://covid19.who.int/ (December 29 2021, date last accessed).</mixed-citation>
    </ref>
    <ref id="b0010">
      <label>2</label>
      <mixed-citation publication-type="other" id="h0010">Burckhardt CJ, Greber UF. Virus movements on the plasma membrane support infection and transmission between cells, PLoS Pathogens 2009;5:e1000621-e1000621.</mixed-citation>
    </ref>
    <ref id="b0015">
      <label>3</label>
      <element-citation publication-type="journal" id="h0015">
        <person-group person-group-type="author">
          <name>
            <surname>Grove</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Marsh</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>The cell biology of receptor-mediated virus entry</article-title>
        <source>J Cell Biol</source>
        <volume>195</volume>
        <year>2011</year>
        <fpage>1071</fpage>
        <lpage>1082</lpage>
        <pub-id pub-id-type="pmid">22123832</pub-id>
      </element-citation>
    </ref>
    <ref id="b0020">
      <label>4</label>
      <element-citation publication-type="journal" id="h0020">
        <person-group person-group-type="author">
          <name>
            <surname>Maginnis</surname>
            <given-names>M.S.</given-names>
          </name>
        </person-group>
        <article-title>Virus-receptor interactions: the key to cellular invasion</article-title>
        <source>J Mol Biol</source>
        <volume>430</volume>
        <year>2018</year>
        <fpage>2590</fpage>
        <lpage>2611</lpage>
        <pub-id pub-id-type="pmid">29924965</pub-id>
      </element-citation>
    </ref>
    <ref id="b0025">
      <label>5</label>
      <element-citation publication-type="journal" id="h0025">
        <person-group person-group-type="author">
          <name>
            <surname>Parnell</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>McLean</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Booth</surname>
            <given-names>D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Aberrant cell cycle and apoptotic changes characterise severe influenza A infection–a meta-analysis of genomic signatures in circulating leukocytes</article-title>
        <source>PLoS ONE</source>
        <volume>6</volume>
        <year>2011</year>
        <fpage>e17186</fpage>
        <lpage>e</lpage>
        <pub-id pub-id-type="pmid">21408152</pub-id>
      </element-citation>
    </ref>
    <ref id="b0030">
      <label>6</label>
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>F.Q.</given-names>
          </name>
          <name>
            <surname>Tam</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>D.X.</given-names>
          </name>
        </person-group>
        <article-title>Cell cycle arrest and apoptosis induced by the coronavirus infectious bronchitis virus in the absence of p53</article-title>
        <source>Virology</source>
        <volume>365</volume>
        <year>2007</year>
        <fpage>435</fpage>
        <lpage>445</lpage>
        <pub-id pub-id-type="pmid">17493653</pub-id>
      </element-citation>
    </ref>
    <ref id="b0035">
      <label>7</label>
      <element-citation publication-type="journal" id="h0035">
        <person-group person-group-type="author">
          <name>
            <surname>Gioti</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Kottaridi</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Voyiatzaki</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Animal coronaviruses induced apoptosis</article-title>
        <source>Life</source>
        <volume>11</volume>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b0040">
      <label>8</label>
      <element-citation publication-type="journal" id="h0040">
        <person-group person-group-type="author">
          <name>
            <surname>Mir</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Naghibzadeh</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Saadati</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>INDEX: Incremental depth extension approach for protein–protein interaction networks alignment</article-title>
        <source>Biosystems</source>
        <volume>162</volume>
        <year>2017</year>
        <fpage>24</fpage>
        <lpage>34</lpage>
        <pub-id pub-id-type="pmid">28860070</pub-id>
      </element-citation>
    </ref>
    <ref id="b0045">
      <label>9</label>
      <element-citation publication-type="journal" id="h0045">
        <person-group person-group-type="author">
          <name>
            <surname>Hayashi</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Matsuzaki</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yanagisawa</surname>
            <given-names>K.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MEGADOCK-Web: an integrated database of high-throughput structure-based protein-protein interaction predictions</article-title>
        <source>BMC Bioinf</source>
        <volume>19</volume>
        <year>2018</year>
        <fpage>62</fpage>
      </element-citation>
    </ref>
    <ref id="b0050">
      <label>10</label>
      <mixed-citation publication-type="other" id="h0050">Singh R, Park D, Xu J et al. Struct2Net: a web service to predict protein-protein interactions using a structure-based approach, Nucleic Acids Res 2010;38:W508-515.</mixed-citation>
    </ref>
    <ref id="b0055">
      <label>11</label>
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Q.C.</given-names>
          </name>
          <name>
            <surname>Petrey</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Structure-based prediction of protein–protein interactions on a genome-wide scale</article-title>
        <source>Nature</source>
        <volume>490</volume>
        <year>2012</year>
        <fpage>556</fpage>
        <lpage>560</lpage>
        <pub-id pub-id-type="pmid">23023127</pub-id>
      </element-citation>
    </ref>
    <ref id="b0060">
      <label>12</label>
      <element-citation publication-type="journal" id="h0060">
        <person-group person-group-type="author">
          <name>
            <surname>Eid</surname>
            <given-names>F.E.</given-names>
          </name>
          <name>
            <surname>ElHefnawi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Heath</surname>
            <given-names>L.S.</given-names>
          </name>
        </person-group>
        <article-title>DeNovo: virus-host sequence-based protein-protein interaction prediction</article-title>
        <source>Bioinformatics</source>
        <volume>32</volume>
        <year>2016</year>
        <fpage>1144</fpage>
        <lpage>1150</lpage>
        <pub-id pub-id-type="pmid">26677965</pub-id>
      </element-citation>
    </ref>
    <ref id="b0065">
      <label>13</label>
      <mixed-citation publication-type="other" id="h0065">Barman RK, Saha S, Das S. Prediction of interactions between viral and host proteins using supervised machine learning methods, PLoS One 2014;9:e112034.</mixed-citation>
    </ref>
    <ref id="b0070">
      <label>14</label>
      <element-citation publication-type="journal" id="h0070">
        <person-group person-group-type="author">
          <name>
            <surname>Dey</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Chakraborty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Mukhopadhyay</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Machine learning techniques for sequence-based prediction of viral-host interactions between SARS-CoV-2 and human proteins</article-title>
        <source>Biomed J</source>
        <volume>43</volume>
        <year>2020</year>
        <fpage>438</fpage>
        <lpage>450</lpage>
        <pub-id pub-id-type="pmid">33036956</pub-id>
      </element-citation>
    </ref>
    <ref id="b0075">
      <label>15</label>
      <element-citation publication-type="journal" id="h0075">
        <person-group person-group-type="author">
          <name>
            <surname>Khorsand</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Savadi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zahiri</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Alpha influenza virus infiltration prediction using virus-human protein-protein interaction network</article-title>
        <source>Mathem Biosci Eng</source>
        <volume>17</volume>
        <year>2020</year>
        <fpage>3109</fpage>
        <lpage>3129</lpage>
      </element-citation>
    </ref>
    <ref id="b0080">
      <label>16</label>
      <element-citation publication-type="journal" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>Hu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Ling</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning frameworks for protein–protein interaction prediction</article-title>
        <source>Comput Struct Biotechnol J</source>
        <volume>20</volume>
        <year>2022</year>
        <fpage>3223</fpage>
        <lpage>3233</lpage>
        <pub-id pub-id-type="pmid">35832624</pub-id>
      </element-citation>
    </ref>
    <ref id="b0085">
      <label>17</label>
      <element-citation publication-type="journal" id="h0085">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A generalized approach to predicting protein-protein interactions between virus and host</article-title>
        <source>BMC Genomics</source>
        <volume>19</volume>
        <year>2018</year>
        <fpage>568</fpage>
        <pub-id pub-id-type="pmid">30367586</pub-id>
      </element-citation>
    </ref>
    <ref id="b0090">
      <label>18</label>
      <element-citation publication-type="journal" id="h0090">
        <person-group person-group-type="author">
          <name>
            <surname>Alguwaizani</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Park</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting interactions between virus and host proteins using repeat patterns and composition of amino acids</article-title>
        <source>J Healthcare Eng</source>
        <volume>2018</volume>
        <year>2018</year>
        <fpage>1391265</fpage>
      </element-citation>
    </ref>
    <ref id="b0095">
      <label>19</label>
      <element-citation publication-type="journal" id="h0095">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Transfer learning via multi-scale convolutional neural layers for human–virus protein–protein interaction prediction</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <year>2021</year>
        <fpage>4771</fpage>
        <lpage>4778</lpage>
      </element-citation>
    </ref>
    <ref id="b0100">
      <label>20</label>
      <element-citation publication-type="journal" id="h0100">
        <person-group person-group-type="author">
          <name>
            <surname>Liu-Wei</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Kafkas</surname>
            <given-names>Ş.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepViral: prediction of novel virus–host interactions from protein sequences and infectious disease phenotypes</article-title>
        <source>Bioinformatics</source>
        <volume>37</volume>
        <year>2021</year>
        <fpage>2722</fpage>
        <lpage>2729</lpage>
      </element-citation>
    </ref>
    <ref id="b0105">
      <label>21</label>
      <mixed-citation publication-type="other" id="h0105">Mikolov T, Chen K, Corrado G et al. Efficient estimation of word representations in vector space. 2013, arXiv:1301.3781.</mixed-citation>
    </ref>
    <ref id="b0110">
      <label>22</label>
      <mixed-citation publication-type="other" id="h0110">Mikolov T, Sutskever I, Chen K et al. Distributed representations of words and phrases and their compositionality. 2013, arXiv:1310.4546.</mixed-citation>
    </ref>
    <ref id="b0115">
      <label>23</label>
      <mixed-citation publication-type="other" id="h0115">Tsukiyama S, Hasan MM, Deng H-W et al. BERT6mA: prediction of DNA N6-methyladenine site using deep learning-based approaches, Briefings in Bioinformatics 2022;23:bbac053.</mixed-citation>
    </ref>
    <ref id="b0120">
      <label>24</label>
      <mixed-citation publication-type="other" id="h0120">Charoenkwan P, Nantasenamat C, Hasan MM et al. BERT4Bitter: a bidirectional encoder representations from transformers (BERT)-based model for improving the prediction of bitter peptides, Bioinformatics 2021:btab133.</mixed-citation>
    </ref>
    <ref id="b0125">
      <label>25</label>
      <element-citation publication-type="journal" id="h0125">
        <person-group person-group-type="author">
          <name>
            <surname>Kurata</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Tsukiyama</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Manavalan</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>iACVP: markedly enhanced identification of anti-coronavirus peptides using a dataset-specific word2vec model</article-title>
        <source>Briefings Bioinf</source>
        <volume>23</volume>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="b0130">
      <label>26</label>
      <element-citation publication-type="journal" id="h0130">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y.F.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Kaushik</surname>
            <given-names>A.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SPVec: A Word2vec-inspired feature representation method for drug-target interaction prediction</article-title>
        <source>Front Chem</source>
        <volume>7</volume>
        <year>2019</year>
        <fpage>895</fpage>
        <pub-id pub-id-type="pmid">31998687</pub-id>
      </element-citation>
    </ref>
    <ref id="b0135">
      <label>27</label>
      <element-citation publication-type="journal" id="h0135">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prediction of human-virus protein-protein interactions through a sequence embedding-based machine learning method</article-title>
        <source>Comput Struct Biotechnol J</source>
        <volume>18</volume>
        <year>2020</year>
        <fpage>153</fpage>
        <lpage>161</lpage>
        <pub-id pub-id-type="pmid">31969974</pub-id>
      </element-citation>
    </ref>
    <ref id="b0140">
      <label>28</label>
      <mixed-citation publication-type="other" id="h0140">Tsukiyama S, Hasan MM, Fujii S et al. LSTM-PHV: prediction of human-virus protein–protein interactions by LSTM with word2vec, Briefings in Bioinform 2021;22:bbab228.</mixed-citation>
    </ref>
    <ref id="b0145">
      <label>29</label>
      <mixed-citation publication-type="other" id="h0145">Chen X, Cheng Y, Wang S et al. EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets. 2020, arXiv:2101.00063.</mixed-citation>
    </ref>
    <ref id="b0150">
      <label>30</label>
      <mixed-citation publication-type="other" id="h0150">Lei T. When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute. 2021, arXiv:2102.12459.</mixed-citation>
    </ref>
    <ref id="b0155">
      <label>31</label>
      <mixed-citation publication-type="other" id="h0155">Mildenhall B, Srinivasan PP, Tancik M et al. NeRF: Representing scenes as neural radiance fields for view synthesis. 2020, arXiv:2003.08934.</mixed-citation>
    </ref>
    <ref id="b0160">
      <label>32</label>
      <mixed-citation publication-type="other" id="h0160">Tolstikhin I, Houlsby N, Kolesnikov A et al. MLP-Mixer: An all-MLP Architecture for Vision. 2021, arXiv:2105.01601.</mixed-citation>
    </ref>
    <ref id="b0165">
      <label>33</label>
      <element-citation publication-type="journal" id="h0165">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>HVIDB: a comprehensive database for human-virus protein-protein interactions</article-title>
        <source>Brief Bioinform</source>
        <volume>22</volume>
        <year>2021</year>
        <fpage>832</fpage>
        <lpage>844</lpage>
        <pub-id pub-id-type="pmid">33515030</pub-id>
      </element-citation>
    </ref>
    <ref id="b0170">
      <label>34</label>
      <mixed-citation publication-type="other" id="h0170">The UniProt Consortium. UniProt: the universal protein knowledgebase, Nucleic Acids Res 2017;45:D158-D169.</mixed-citation>
    </ref>
    <ref id="b0175">
      <label>35</label>
      <element-citation publication-type="journal" id="h0175">
        <person-group person-group-type="author">
          <name>
            <surname>Henikoff</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Henikoff</surname>
            <given-names>J.G.</given-names>
          </name>
        </person-group>
        <article-title>Amino acid substitution matrices from protein blocks</article-title>
        <source>PNAS</source>
        <volume>89</volume>
        <year>1992</year>
        <fpage>10915</fpage>
        <lpage>10919</lpage>
        <pub-id pub-id-type="pmid">1438297</pub-id>
      </element-citation>
    </ref>
    <ref id="b0180">
      <label>36</label>
      <element-citation publication-type="journal" id="h0180">
        <person-group person-group-type="author">
          <name>
            <surname>Oughtred</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Rust</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The BioGRID database: A comprehensive biomedical resource of curated protein, genetic, and chemical interactions</article-title>
        <source>Protein Sci</source>
        <volume>30</volume>
        <year>2021</year>
        <fpage>187</fpage>
        <lpage>200</lpage>
        <pub-id pub-id-type="pmid">33070389</pub-id>
      </element-citation>
    </ref>
    <ref id="b0185">
      <label>37</label>
      <element-citation publication-type="journal" id="h0185">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CD-HIT: accelerated for clustering the next-generation sequencing data</article-title>
        <source>Bioinformatics</source>
        <volume>28</volume>
        <year>2012</year>
        <fpage>3150</fpage>
        <lpage>3152</lpage>
        <pub-id pub-id-type="pmid">23060610</pub-id>
      </element-citation>
    </ref>
    <ref id="b0190">
      <label>38</label>
      <mixed-citation publication-type="other" id="h0190">Devlin J, Chang M-W, Lee K et al. BERT: Pre-training of deep bidirectional transformers for language understanding. 2018, arXiv:1810.04805.</mixed-citation>
    </ref>
    <ref id="b0195">
      <label>39</label>
      <mixed-citation publication-type="other" id="h0195">Vaswani A, Shazeer N, Parmar N et al. Attention is all you need. 2017, arXiv:1706.03762.</mixed-citation>
    </ref>
    <ref id="b0200">
      <label>40</label>
      <mixed-citation publication-type="other" id="h0200">Wu H, Gu X. Max-pooling dropout for regularization of convolutional neural networks. 2015, arXiv:1512.01400.</mixed-citation>
    </ref>
    <ref id="b0205">
      <label>41</label>
      <mixed-citation publication-type="other" id="h0205">Pedregosa F, Varoquaux G, Gramfort A et al. Scikit-learn: machine learning in python. 2012, arXiv:1201.0490.</mixed-citation>
    </ref>
    <ref id="b0210">
      <label>42</label>
      <element-citation publication-type="journal" id="h0210">
        <person-group person-group-type="author">
          <name>
            <surname>van der Maaten</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Visualizing data using t-SNE</article-title>
        <source>J Machine Learn Res</source>
        <volume>9</volume>
        <year>2008</year>
        <fpage>2579</fpage>
        <lpage>2605</lpage>
      </element-citation>
    </ref>
    <ref id="b0215">
      <label>43</label>
      <element-citation publication-type="journal" id="h0215">
        <person-group person-group-type="author">
          <name>
            <surname>Jumper</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Pritzel</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>
        <source>Nature</source>
        <volume>596</volume>
        <year>2021</year>
        <fpage>583</fpage>
        <lpage>589</lpage>
        <pub-id pub-id-type="pmid">34265844</pub-id>
      </element-citation>
    </ref>
    <ref id="b0220">
      <label>44</label>
      <mixed-citation publication-type="other" id="h0220">Wu R, Ding F, Wang R et al. High-resolution de novo structure prediction from primary sequence, bioRxiv 2022:2022.2007.2021.500999.</mixed-citation>
    </ref>
  </ref-list>
  <sec id="s0135" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary data</title>
    <p id="p0230">The following are the Supplementary data to this article:<supplementary-material content-type="local-data" id="m0005"><caption><title>Supplementary data 1</title></caption><media xlink:href="mmc1.pdf"/></supplementary-material></p>
  </sec>
  <ack id="ak005">
    <title>Acknowledgements</title>
    <p id="p0220">This work was supported by a Grant-in-Aid for Scientific Research (B) (22H03688) and partially supported by a Grant-in-Aid for JSPS Research Fellows (22J22706) from the Japan Society for the Promotion of Science (JSPS).</p>
  </ack>
  <fn-group>
    <fn id="s0130" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0225">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csbj.2022.10.012" id="ir020">https://doi.org/10.1016/j.csbj.2022.10.012</ext-link>.</p>
    </fn>
  </fn-group>
</back>
