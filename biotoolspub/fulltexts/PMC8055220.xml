<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8055220</article-id>
    <article-id pub-id-type="pmid">32702108</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa662</article-id>
    <article-id pub-id-type="publisher-id">btaa662</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Notes</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Data and Text Mining</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><italic>treeheatr</italic>: an R package for interpretable decision tree visualizations</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3737-6565</contrib-id>
        <name>
          <surname>Le</surname>
          <given-names>Trang T</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa662-cor1"/>
        <!--<email>ttle@pennmedicine.upenn.edu</email>-->
        <xref ref-type="aff" rid="btaa662-aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Moore</surname>
          <given-names>Jason H</given-names>
        </name>
        <xref ref-type="corresp" rid="btaa662-cor1"/>
        <xref ref-type="aff" rid="btaa662-aff1"/>
        <!--<email>jhmoore@upenn.edu</email>-->
      </contrib>
    </contrib-group>
    <aff id="btaa662-aff1"><institution>Department of Biostatistics, Epidemiology and Informatics, Institute for Biomedical Informatics, University of Pennsylvania</institution>, Philadelphia, PA 19104, <country country="GB">USA</country></aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Wren</surname>
          <given-names>Jonathan</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btaa662-cor1">To whom correspondence should be addressed. <email>ttle@pennmedicine.upenn.edu</email> or <email>jhmoore@upenn.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-07-23">
      <day>23</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>37</volume>
    <issue>2</issue>
    <fpage>282</fpage>
    <lpage>284</lpage>
    <history>
      <date date-type="received">
        <day>03</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>09</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="editorial-decision">
        <day>14</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>7</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa662.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Summary</title>
        <p><italic>treeheatr</italic> is an R package for creating interpretable decision tree visualizations with the data represented as a heatmap at the tree’s leaf nodes. The integrated presentation of the tree structure along with an overview of the data efficiently illustrates how the tree nodes split up the feature space and how well the tree model performs. This visualization can also be examined in depth to uncover the correlation structure in the data and importance of each feature in predicting the outcome. Implemented in an easily installed package with a detailed vignette, <italic>treeheatr</italic> can be a useful teaching tool to enhance students’ understanding of a simple decision tree model before diving into more complex tree-based machine learning methods.</p>
      </sec>
      <sec id="s2">
        <title>Availability and implementation</title>
        <p>The <italic>treeheatr</italic> package is freely available under the permissive MIT license at <ext-link ext-link-type="uri" xlink:href="https://trang1618.github.io/treeheatr">https://trang1618.github.io/treeheatr</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=treeheatr">https://cran.r-project.org/package=treeheatr</ext-link>. It comes with a detailed vignette that is automatically built with GitHub Actions continuous integration.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>LM010098</award-id>
        <award-id>AI116794</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Decision tree models comprise a set of machine learning algorithms widely used for predicting an outcome from a set of predictors or features. For specific problems, a single decision tree can provide predictions at desirable accuracy while remaining easy to understand and interpret (<xref rid="btaa662-B8" ref-type="bibr">Yan <italic>et al.</italic>, 2020</xref>). These models are also important building blocks of more complex tree-based structures such as random forests and gradient-boosted trees.</p>
    <p>The simplicity of decision tree models allows for clear visualizations that can be incorporated with rich additional information such as the feature space. However, existing software frequently treats all nodes in a decision tree similarly, leaving limited options for improving information presentation at the leaf nodes. Specifically, the R library <italic>rpart.plot</italic> displays at each node its characteristics including the number of observations falling in that node, the proportion of those observations in each class and the node’s majority vote. Despite being potentially helpful, these statistics may not immediately convey important information about the tree such as its overall performance. Function <bold><monospace>visTree</monospace></bold>() from the R package <italic>visNetwork</italic> draws trees that are aesthetically pleasing but lack general information about the data and are difficult to interpret. The state-of-the-art Python’s <italic>dtreeviz</italic> produces decision trees with detailed histograms at inner nodes but still draw pie chart of different classes at leaf nodes. <italic>ggparty</italic> is a flexible R package that allows the user to have full control of the representation of each node. However, this library fixes the leaf node widths, which limits its ability to show more collective visualizations. We have developed the <italic>treeheatr</italic> package to incorporate the functionality of <italic>ggparty</italic> but also utilize the leaf node space to display the data as a heatmap, a popular visualization that uncovers groups of samples and features in a dataset (<xref rid="btaa662-B1" ref-type="bibr">Galili <italic>et al.</italic>, 2018</xref>; <xref rid="btaa662-B7" ref-type="bibr">Wilkinson <italic>et al.</italic>, 2009</xref>). A heatmap also displays a useful general view of the dataset, e.g. how large it is or whether it contains any outliers. Integrated with a decision tree, the samples in each leaf node are ordered based on an efficient seriation method.</p>
    <p>After simple installation, the user can apply <italic>treeheatr</italic> on their classification or regression tree with a single function:</p>
    <p>
      <disp-quote content-type="extract">
        <p>
          <bold>
            <monospace>heat_tree</monospace>
          </bold>
          <monospace>(x, target_lab = ‘Outcome’)</monospace>
        </p>
      </disp-quote>
    </p>
    <p>This one line of code above will produce a decision tree-heatmap as a <italic>ggplot</italic> object that can be viewed in RStudio’s viewer pane, saved to a graphic file, or embedded in an RMarkdown document. This example assumes a classification problem, but one can also apply <italic>treeheatr</italic> on a regression problem by setting <monospace>task = ‘regression’</monospace>.</p>
    <p>This article is organized as follows. In Section 2, we present an example <italic>treeheatr</italic> application using its functions on a real-world clinical dataset from a study of COVID-19 patient outcome in Wuhan, China (<xref rid="btaa662-B8" ref-type="bibr">Yan <italic>et al.</italic>, 2020</xref>). In Section 3, we describe in detail the important functions and corresponding arguments in <italic>treeheatr</italic>. We demonstrate the flexibility that the user has in tweaking these arguments to enhance understanding of the tree-based models applied on their dataset. Finally, we discuss general guidelines for creating effective decision tree-heatmap visualization.</p>
  </sec>
  <sec>
    <title>2 A simple example</title>
    <p>This example visualizes the conditional inference tree model built to predict whether or not a patient survived from COVID-19 in Wuhan, China (<xref rid="btaa662-B8" ref-type="bibr">Yan <italic>et al.</italic>, 2020</xref>). The dataset contains blood samples of 351 patients admitted to Tongji hospital between January 10 and February 18, 2020. Three features were selected based on their importance score from a multi-tree XGBoost model, including lactic dehydrogenase (LDH), lymphocyte levels and high-sensitivity C-reactive protein (hs_CRP). Detailed characteristics of the samples can be found in the original publication (<xref rid="btaa662-B8" ref-type="bibr">Yan <italic>et al.</italic>, 2020</xref>).</p>
    <p>The following lines of code compute and visualize the conditional decision tree along with the heatmap containing features that are important for constructing this model (<xref ref-type="fig" rid="btaa662-F1">Fig. 1</xref>):</p>
    <p>
      <disp-quote content-type="extract">
        <p>
          <bold>
            <monospace>heat_tree</monospace>
          </bold>
          <monospace>(</monospace>
        </p>
        <p>
          <monospace>x =</monospace>
          <bold>
            <monospace>ctree</monospace>
          </bold>
          <monospace>(Outcome ∼ ., data = covid),</monospace>
        </p>
        <p>
          <bold>
            <monospace>label_map</monospace>
          </bold>
          <monospace>=</monospace>
          <bold>
            <monospace>c</monospace>
          </bold>
          <monospace>(`0`= ‘Survived’, `1`= ‘Deceased’)</monospace>
        </p>
        <p>
          <monospace>)</monospace>
        </p>
      </disp-quote>
    </p>
    <fig id="btaa662-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>A decision tree-heatmap for predicting whether or not a patient survived from COVID-19 in Wuhan, China. The heatmap colors present the relative value of a sample compared to the rest of the group on each feature</p>
      </caption>
      <graphic xlink:href="btaa662f1"/>
    </fig>
    <p>The <bold><monospace>heat_tree</monospace></bold>() function takes a party or partynode object representing the decision tree and other optional arguments such as the outcome label mapping. If instead of a tree object, x is a data.frame representing a dataset, <bold><monospace>heat_tree</monospace></bold>() automatically computes a conditional tree for visualization, given that an argument specifying the column name associated with the phenotype/outcome, <monospace>target_lab</monospace>, is provided.</p>
    <p>In the decision tree, the leaf nodes are labeled based on their majority votes and colored to correlate with the true outcome. On the right split of hs_CRP (hs_CRP ≤ 52.5 and hs_CRP &gt; 52.5), although individuals of both branches are all predicted to survive by majority voting, the leaf nodes have different purity, indicating different confidence levels the model has in classifying samples in the two nodes. These seemingly non-beneficial splits present an opportunity to teach machine learning novices the different measures of node impurity such as the Gini index or cross-entropy (Hastie <italic>et al.</italic>, 2009).</p>
    <p>In the heatmap, each (very thin) column is a sample, and each row represents a feature or the outcome. For a specific feature, the color shows the relative value of a sample compared to the rest of the group on that feature; higher values are associated with lighter colors. Within the heatmap, similar color patterns between LDH and hs_CRP suggest a positive correlation between these two features, which is expected because they are both systemic inflammation markers.</p>
    <p>Together, the tree and heatmap give us an approximation of the proportion of samples per leaf and the model’s confidence in its classification of samples in each leaf. Three main blocks of different lymphocyte levels in the heatmap illustrate its importance as a determining factor in predicting patient outcome. When this value is below 12.7 but larger than 5.5 (observations with dark green lymphocyte value), hs_CRP helps further distinguish the group that survived from the other. Here, if we focus on the hs_CRP &gt; 35.5 branch, we notice that the corresponding hs_CRP colors range from light green to yellow (&gt;0.5), illustrating that the individuals in this branch have higher hs_CRP than the median of the group. This connection is immediate with the two components visualized together but would not have been possible with the tree model alone. In summary, the tree and heatmap integration provides a comprehensive view of the data along with key characteristics of the decision tree.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <p>When the first argument x is a data.frame object representing the dataset instead of the decision tree, <italic>treeheatr</italic> automatically computes a conditional tree with default parameters for visualization. Conditional decision trees (<xref rid="btaa662-B4" ref-type="bibr">Hothorn <italic>et al.</italic>, 2006</xref>) are non-parametric models performing recursive binary partitioning with well-defined theoretical background. Conditional trees support unbiased selection among covariates and produce competitive prediction accuracy for many problems (<xref rid="btaa662-B4" ref-type="bibr">Hothorn <italic>et al.</italic>, 2006</xref>). The default parameter setting often results in smaller trees that are less prone to overfit. <italic>treeheatr</italic> utilizes the <italic>partykit</italic> R package to fit the conditional tree and <italic>ggparty</italic> R package to compute its edge and node information.</p>
    <p>While <italic>ggparty</italic> assumes fixed leaf node widths, <italic>treeheatr</italic> uses a flexible node layout to accommodate the different number of samples shown in the heatmap at each leaf node. This new node layout structure supports various leaf node widths, prevents crossings of different tree branches and generalizes as the trees grow in size. This new layout weighs the <italic>x</italic>-coordinate of the parent node according to the levels of the child nodes to avoid branch crossing. This relative weight can be adjusted with the <monospace>lev_fac</monospace> parameter in <bold><monospace>heat_tree</monospace></bold>(). <monospace>lev_fac = 1</monospace> sets the parent node’s <italic>x</italic>-coordinate perfectly in the middle of those of its child nodes. The default <monospace>level_fac = 1.3</monospace> seems to provide optimal node layout independent of the tree size. The user can define a customized layout for a specific set of nodes and combine that layout with the automatic layout for the remaining nodes.</p>
    <p>By default, heatmap samples (columns) are automatically reordered within each leaf node using a <italic>seriation</italic> method (<xref rid="btaa662-B3" ref-type="bibr">Hahsler <italic>et al.</italic>, 2008</xref>) using all features and outcome label, unless <monospace>clust_target = FALSE</monospace>. <italic>treeheatr</italic> uses the <bold><monospace>daisy</monospace></bold>() function in the <italic>cluster</italic> R package with the Gower metric (<xref rid="btaa662-B2" ref-type="bibr">Gower, 1971</xref>) to compute the dissimilarity matrix of a dataset that may have both continuous and nominal categorical feature types. Heatmap features (rows) are ordered in a similar manner. We note that, while there is no definitive guideline for proper weighting of features of different types, the goal of the seriation step is to reduce the amount of stochasticity in the heatmap and not to make precise inference about each grouping.</p>
    <p>In a visualization, it is difficult to strike the balance between enhancing understanding and overloading information. We believe showing a heatmap at the leaf node space provides additional information of the data in an elegant way that is not overwhelming and may even simplify the model’s interpretation. We left it for the user to decide what type of information to be displayed at the inner nodes via different <italic>geom</italic> objects (e.g. <monospace>geom_node_plot</monospace>, <monospace>geom_edge_label</monospace>, etc.) in the <italic>ggparty</italic> package. For example, one may choose to show at these decision nodes the distribution of the features or their corresponding Bonferroni-adjusted <italic>P</italic>-values computed in the conditional tree algorithm (<xref rid="btaa662-B4" ref-type="bibr">Hothorn <italic>et al.</italic>, 2006</xref>).</p>
    <p>Striving for simplicity, <italic>treeheatr</italic> utilizes direct labeling to avoid unnecessary legends. For example, in classification, the leaf node labels have colors corresponding with different classes, e.g. purple for Deceased and green for Survived in the COVID-19 dataset (<xref ref-type="fig" rid="btaa662-F1">Fig. 1</xref>). As for feature values, by default, the color scale ranges from 0 to 1 and indicates the relative value of a sample compared to the rest of the group on each feature. Linking the color values of a particular feature to the corresponding edge labels can reveal additional information that is not available with the decision tree alone.</p>
    <p>In addition to the main dataset, the user can supply to <bold><monospace>heat_tree</monospace></bold>() a validation dataset via the <monospace>data_test</monospace> argument. As a result, <bold><monospace>heat_tree</monospace></bold>() will train the conditional tree on the original training dataset, draw the decision tree-heatmap on the testing dataset, and, if desired, print next to the tree its performance on the test set according to specified metrics (e.g. balanced accuracy for classification or root mean-squared error for regression problem).</p>
    <p>The integration of heatmap nicely complements the current techniques of visualizing decision trees. Node purity, a metric measuring the tree’s performance, can be visualized from the distribution of true outcome labels at each leaf node in the first row. Comparing these values with the leaf node label gives a visual estimate of how accurate the tree predictions are. Further, without explicitly choosing two features to show in a 2-D scatter plot, we can still infer correlation structures among features in the heatmap. The additional seriation may also reveal sub-structures within a leaf node.</p>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>In this article, we presented a new type of integrated visualization of decision trees and heatmaps, which provides a comprehensive data overview as well as model interpretation. We demonstrated that this integration uncovers meaningful patterns among the predictive features and highlights the important elements of decision trees including feature splits and several leaf node characteristics such as prediction value, impurity and number of leaf samples. Its detailed vignette makes <italic>treeheatr</italic> a useful teaching tool to enhance students’ understanding of this fundamental model before diving into more complex tree-based machine learning methods. <italic>treeheatr</italic> is scalable to large datasets. For example, <bold><monospace>heat_tree</monospace></bold>() runtime on the <italic>waveform</italic> dataset with 5000 observations and 40 features was approximately 80 s on a machine with a 2.2 GHz Intel Core i7 processor and 8 GB of RAM. However, as with other visualization tools, the tree’s interpretation becomes more difficult as the feature space expands. Thus, for high-dimensional datasets, it is potentially beneficial to perform feature selection to reduce the number of features or random sampling to reduce the number of observations prior to plotting the tree. Moreover, when the single tree does not perform well and the average node purity is low, it can be challenging to interpret the heatmap because clear signal cannot emerge if the features have low predictability.</p>
    <p>Future work on <italic>treeheatr</italic> includes enhancements such as support for left-to-right orientation and highlighting the tree branches that point to a specific sample. We will also investigate other data preprocess and seriation options that might result in more robust models and informative visualizations.</p>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The <italic>treeheatr</italic> package was made possible by leveraging integral R packages including <italic>ggplot2</italic> (<xref rid="btaa662-B6" ref-type="bibr">Wickham, 2009</xref>), <italic>partykit</italic> (<xref rid="btaa662-B5" ref-type="bibr">Hothorn <italic>et al.</italic>, 2015</xref>), <italic>ggparty</italic> and many others. The authors thank Daniel Himmelstein for his helpful comments on the package’s licensing and continuous integration configuration. Finally, the authors thank two anonymous reviewers whose helpful feedback helped improve the package and clarify this manuscript.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the National Institutes of Health Grant Nos. LM010098 and AI116794.</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa662-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galili</surname><given-names>T.</given-names></string-name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>heatmaply: an R package for creating interactive cluster heatmaps for online publishing</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>1600</fpage>–<lpage>1602</lpage>.<pub-id pub-id-type="pmid">29069305</pub-id></mixed-citation>
    </ref>
    <ref id="btaa662-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gower</surname><given-names>J.C.</given-names></string-name></person-group> (<year>1971</year>) 
<article-title>A general coefficient of similarity and some of its properties</article-title>. <source>Biometrics</source>, <volume>27</volume>, <fpage>857</fpage>.</mixed-citation>
    </ref>
    <ref id="btaa662-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hahsler</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Getting things in order: an introduction to the <italic>R</italic> package seriation</article-title>. <source>J. Stat. Softw</source>., <volume>25, 1-34</volume>.</mixed-citation>
    </ref>
    <ref id="btaa662-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Hastie</surname><given-names>T.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) <source>The elements of statistical learning: data mining, inference, and prediction</source>, <edition>2nd edn</edition>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>New York, NY</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btaa662-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hothorn</surname><given-names>T.</given-names></string-name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>Unbiased recursive partitioning: a conditional inference framework</article-title>. <source>J. Comput. Graph. Stat</source>., <volume>15</volume>, <fpage>651</fpage>–<lpage>674</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa662-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hothorn</surname><given-names>T.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>partykit: a modular toolkit for recursive partytioning in R</article-title>. <source>J. Mach. Learn. Res</source>., <volume>16</volume>, <fpage>3905</fpage>–<lpage>3909</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa662-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Wickham</surname><given-names>H.</given-names></string-name></person-group> (<year>2009</year>) <source>Ggplot2: elegant graphics for data analysis</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>New York</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btaa662-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilkinson</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>The history of the cluster heat map</article-title>. <source>Am. Stat</source>., <volume>63</volume>, <fpage>179</fpage>–<lpage>184</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa662-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yan</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) 
<article-title>An interpretable mortality prediction model for COVID-19 patients</article-title>. <source>Nat. Mach. Intell</source>., <volume>2</volume>, <fpage>283</fpage>–<lpage>288</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
