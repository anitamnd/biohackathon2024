<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nat Commun</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id>
    <journal-title-group>
      <journal-title>Nature Communications</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2041-1723</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8501087</article-id>
    <article-id pub-id-type="publisher-id">26216</article-id>
    <article-id pub-id-type="doi">10.1038/s41467-021-26216-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Annotation-efficient deep learning for automatic medical image segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0575-6523</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Shanshan</given-names>
        </name>
        <address>
          <email>ss.wang@siat.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5400-2093</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Cheng</given-names>
        </name>
        <address>
          <email>cheng.li6@siat.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Wang</surname>
          <given-names>Rongpin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3296-9759</contrib-id>
        <name>
          <surname>Liu</surname>
          <given-names>Zaiyi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4454-5005</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Meiyun</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tan</surname>
          <given-names>Hongna</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wu</surname>
          <given-names>Yaping</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Xinfeng</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sun</surname>
          <given-names>Hui</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Rui</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Xin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Jie</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Huihui</given-names>
        </name>
        <xref ref-type="aff" rid="Aff9">9</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ben Ayed</surname>
          <given-names>Ismail</given-names>
        </name>
        <xref ref-type="aff" rid="Aff10">10</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8558-5102</contrib-id>
        <name>
          <surname>Zheng</surname>
          <given-names>Hairong</given-names>
        </name>
        <address>
          <email>hr.zheng@siat.ac.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.9227.e</institution-id><institution-id institution-id-type="ISNI">0000000119573309</institution-id><institution>Paul C. Lauterbur Research Center for Biomedical Imaging, Shenzhen Institutes of Advanced Technology, </institution><institution>Chinese Academy of Sciences, </institution></institution-wrap>Shenzhen, Guangdong China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.508161.b</institution-id><institution>Peng Cheng Laboratory, </institution></institution-wrap>Shenzhen, Guangdong China </aff>
      <aff id="Aff3"><label>3</label>Pazhou Laboratory, Guangzhou, Guangdong China </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.459540.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1791 4503</institution-id><institution>Department of Medical Imaging, </institution><institution>Guizhou Provincial People’s Hospital, </institution></institution-wrap>Guiyang, Guizhou China </aff>
      <aff id="Aff5"><label>5</label>Department of Medical Imaging, Guangdong General Hospital, Guangdong Academy of Medical Sciences, Guangzhou, Guangdong China </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.414011.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 1808 090X</institution-id><institution>Department of Medical Imaging, </institution><institution>Henan Provincial People’s Hospital &amp; the People’s Hospital of Zhengzhou University, </institution></institution-wrap>Zhengzhou, Henan China </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.412632.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 1758 2270</institution-id><institution>Department of Urology, </institution><institution>Renmin Hospital of Wuhan University, </institution></institution-wrap>Wuhan, Hubei China </aff>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.11135.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 2256 9319</institution-id><institution>School of Electronic and Computer Engineering, </institution><institution>Shenzhen Graduate School, Peking University, </institution></institution-wrap>Shenzhen, Guangdong China </aff>
      <aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.9227.e</institution-id><institution-id institution-id-type="ISNI">0000000119573309</institution-id><institution>Brain Cognition and Brain Disease Institute, Shenzhen Institutes of Advanced Technology, </institution><institution>Chinese Academy of Sciences, </institution></institution-wrap>Shenzhen, Guangdong China </aff>
      <aff id="Aff10"><label>10</label>ETS Montreal, Montreal, Canada </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>8</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>8</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>5915</elocation-id>
    <history>
      <date date-type="received">
        <day>31</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Automatic medical image segmentation plays a critical role in scientific research and medical care. Existing high-performance deep learning methods typically rely on large training datasets with high-quality manual annotations, which are difficult to obtain in many clinical applications. Here, we introduce Annotation-effIcient Deep lEarning (AIDE), an open-source framework to handle imperfect training datasets. Methodological analyses and empirical evaluations are conducted, and we demonstrate that AIDE surpasses conventional fully-supervised models by presenting better performance on open datasets possessing scarce or noisy annotations. We further test AIDE in a real-life case study for breast tumor segmentation. Three datasets containing 11,852 breast images from three medical centers are employed, and AIDE, utilizing 10% training annotations, consistently produces segmentation maps comparable to those generated by fully-supervised counterparts or provided by independent radiologists. The 10-fold enhanced efficiency in utilizing expert labels has the potential to promote a wide range of biomedical applications.</p>
    </abstract>
    <abstract id="Abs2" abstract-type="web-summary">
      <p id="Par2">Existing high-performance deep learning methods typically rely on large training datasets with high-quality manual annotations, which are difficult to obtain in many clinical applications. Here, the authors introduce an open-source framework to handle imperfect training datasets.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Image processing</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Biomedical engineering</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China (National Science Foundation of China)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>61871371, 81830056</award-id>
        <award-id>81720108021</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wang</surname>
            <given-names>Shanshan</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Meiyun</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100004739</institution-id>
            <institution>Youth Innovation Promotion Association of the Chinese Academy of Sciences (Youth Innovation Promotion Association CAS)</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2019351</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wang</surname>
            <given-names>Shanshan</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Scientific and Technical Innovation 2030-“New Generation Artificial Intelligence” Project (2020AAA0104100, 2020AAA0104105) Key-Area Research and Development Program of Guangdong Province (2018B010109009) the Basic Research Program of Shenzhen (JCYJ20180507182400762)</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Key R&amp;D Program of China (2017YFE0103600) Zhongyuan Thousand Talents Plan Project (ZYQR201810117) Zhengzhou Collaborative Innovation Major Project (20XTZX05015)</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1" sec-type="introduction">
    <title>Introduction</title>
    <p id="Par3">Medical imaging contributes significantly to progress in scientific discoveries and medicine<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Semantic segmentation partitions raw image data into structured and meaningful regions and thus enables further image analysis and quantification, which are critical for various applications, including anatomy research, disease diagnosis, treatment planning, and prognosis monitoring<sup><xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR5">5</xref></sup>. With the global expansion of medical imaging and the advancement of imaging techniques, the volume of acquired medical image data is increasing at a pace much faster than available human experts can interpret. Thus, automated segmentation algorithms are needed to assist physicians in realizing accurate and timely imaging-based diagnosis<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>.</p>
    <p id="Par4">In the past decade, deep learning has made considerable progress in automatic medical image segmentation by demonstrating promising performance in various breakthrough studies<sup><xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR13">13</xref></sup>. Nevertheless, the applicability of deep-learning methods to clinical practice is limited because of the heavy reliance on training data, especially training annotations<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>. Large curated datasets are necessary, but annotating medical images is a time-consuming, labor-intensive, and expensive process. Depending on the complexity of the regions of interest to segment and the local anatomical structures, minutes to hours may be required to annotate a single image. Furthermore, label noise is inevitable in real-world applications of deep-learning models<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. Such noise can result from systematic errors of the annotator, as well as inter-annotator variation. More than three domain experts are typically needed to generate trustworthy annotations<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Any biases in the data can be transferred to the outcomes of the learned models<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Consequently, the lack of large and high-quality labeled datasets has been identified as the primary limitation of the application of supervised deep learning for medical imaging tasks<sup><xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref></sup>. Learning with imperfect datasets having limited annotations (semi-supervised learning, SSL), lacking target domain annotations (unsupervised domain adaptation, UDA), or containing noisy annotations (noisy label learning, NLL) are three of the most frequently encountered challenges in clinical applications<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.</p>
    <p id="Par5">Co-training is one of the most prevalent methods for SSL<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup> that works by training two classifiers for two complementary views using the labeled data, generating pseudo-labels for unlabeled data by enforcing agreement between the classifier predictions, and combining the labeled and pseudo-labeled data for further training. Co-training has been employed mainly to semi-supervised classification tasks. Only recently has co-training been extended to semi-supervised image segmentation<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> and UDA of segmentation models<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Despite the achieved inspiring performance, the direct employment of co-training methods to NLL is problematic, as they do not possess the ability to distinguish between accurate and noisy labels<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. Co-teaching was developed based on co-training to specifically address the challenge of NLL by dropping suspected highly noisy samples during network optimization<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. Nevertheless, data filtering by dropping samples is an inefficient approach that might lead to model learning from a pseudo-realistic data distribution; thus, co-teaching methods are more applicable to natural image classification tasks when sufficient large datasets are available to cover the full range of different situations, even after data dropping.</p>
    <p id="Par6">In this study, we develop an annotation-efficient deep-learning framework for medical image segmentation, which we call AIDE, to handle different types of imperfect datasets. AIDE is designed to address all three challenges of SSL, UDA, and NLL. With AIDE, SSL and UDA are transformed into NLL by generating low-quality noisy labels for the unlabeled training data utilizing models trained either on the limited annotated data (SSL) or on the annotated source domain data (UDA). A cross-model self-correction method is proposed to achieve annotation-efficient network learning. Specifically, cross-model co-optimization learning is realized by training two networks in parallel and conducting cross-model information exchange. With the exchanged information, self-label filtering and correction of inexpensive noisy labels proceed with cascaded local and global steps progressively in an elaborately designed schedule according to an observed small loss criterion. The framework is flexible regarding the deep neural network (DNN) models to be utilized.</p>
    <p id="Par7">Methodological analyses are performed to evaluate the effectiveness of AIDE for handling imperfect training datasets. In order to fairly evaluate the method without severe ground-truth label biases, we conduct extensive experiments on a variety of public datasets which have widely accepted data and labels. Specifically, extensive experiments on open datasets with limited, no target domain, and noisy annotations are performed. Better results are achieved with AIDE compared to the respective fully supervised baselines optimized with the available noisy annotations or model-generated low-quality labels. Furthermore, to test the applicability of our method in real-world applications, three datasets for breast tumor segmentation (11,852 image samples of 872 patients) collected from three medical centers are experimented with. The annotations of these data are carefully curated by three experienced radiologists with more than 10 years of experience in breast MR image interpretation. Comparable segmentation results to those obtained by fully supervised models with access to 100% training data annotations and those provided by independent radiologists are achieved with AIDE by utilizing only 10% of the annotations. Our results indicate that DNNs are capable of exploring the image contents of large datasets under proper guidance without the necessity of high-quality annotations. We believe that our proposed framework has the potential to improve the medical image diagnosis workflow in an efficient manner and at a low cost.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>AIDE: a deep-learning framework to handle imperfect training datasets</title>
      <p id="Par8">AIDE is a deep-learning framework that achieves accurate image segmentation with imperfect training datasets. A cross-model self-label correction mechanism is proposed to efficiently exploit inexpensive noisy training data labels, which are either generated by pretrained low-performance deep-learning models or provided by individual annotators without quality controls.</p>
      <p id="Par9">An overview of AIDE and example images of the datasets we utilized are depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. AIDE is proposed to address three challenging tasks caused by imperfect training datasets (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). The first is SSL with limited annotated training data. Proper utilization of the relatively abundant unlabeled data is very important in this case. The second is UDA, where large discrepancies may exist between the target and source domains. The third is NLL that considers the variations of annotations provided by different observers. By means of task standardization by generating low-quality noisy labels for SSL and UDA (for SSL, models are pretrained with the available limited annotated training data, and low-quality labels are generated for the remaining unlabeled training data; for UDA, models are pretrained with the source domain labeled training data, and low-quality labels are generated for the target domain unlabeled training data), all three challenging tasks are addressed in one framework that targets model optimization by datasets containing problematic labels (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><title>Overview of AIDE.</title><p><bold>a</bold> The three challenges (semi-supervised learning (SSL), unsupervised domain adaptation (UDA), and noisy label learning (NLL)) that AIDE addresses and the proposed task standardization method. <bold>b</bold> The overall framework of AIDE, which comprises three major elements: local label filtering, global label correction, and cross-model co-optimization. <bold>c</bold> Example images of the open datasets. The top left two images are from the CHAOS dataset. The top right three images are from the three domains of prostate datasets. The bottom images are from the QUBIQ datasets, where the first four correspond to the four subtasks and the last one is an enlarged view of the fourth image. Color lines indicate the target regions. <bold>d</bold> Example images of the breast datasets. From left to right, the three columns correspond to the images collected from the three medical centers. Red color regions show the breast tumors.</p></caption><graphic xlink:href="41467_2021_26216_Fig1_HTML" id="d32e583"/></fig></p>
      <p id="Par10">For AIDE, two networks are trained in parallel to conduct cross-model co-optimization (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). In each iteration, those samples in an input batch that are suspected to have noisy labels are filtered out and are subjected to data augmentation (random rotation and flipping), and corresponding pseudo-labels are generated by distilling the predictions of the augmented inputs (local label filtering in Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). These pseudo-labels, together with the high-quality labels, are utilized to train the network. After each epoch, the labels of the whole training set are analyzed, and those that have low similarities (smaller Dice scores) with the network’s predictions are updated if an elaborately designed updating criterion is met (global label correction in Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). With AIDE, the networks are forced to concentrate on the image contents instead of extracting image features guided by only the annotations. The exact segmentation network we employ has a classical encoder−decoder structure with multiple streams to extract image features from different modalities<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>, should multimodal inputs be available.</p>
      <p id="Par11">Multiple evaluation metrics are reported to characterize the segmentation performance, namely, the Dice score (Dice similarity coefficient, DSC), relative area/volume difference (RAVD), average symmetric surface distance (ASSD), and maximum symmetric surface distance (MSSD). Higher DSC values and lower RAVD, ASSD, and MSSD indicate more accurate segmentation results.</p>
    </sec>
    <sec id="Sec4">
      <title>Enhancement of AIDE compared to conventional fully supervised learning</title>
      <p id="Par12">The effectiveness of AIDE in handling low-quality annotated data can be attributed to the following advantages. First, the local label-filtering step in each iteration enforces a constraint on suspected low-quality annotated samples to generate predictions that are consistent with those of the augmented inputs, which is a form of data distillation that enforces a transformation-consistent constraint on the model predictions<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. This design can prevent the negative effects of low-quality annotations and exploit as much useful information as possible from these inputs instead of dropping them. Compared to conventional fully supervised learning, which trains the network only by minimizing the discrepancies between the predictions and the ground-truth labels, the consistency constraint of AIDE can force the network to learn latent features from inputs that are transformation invariant; in this way, the network concentrates more on the image contents instead of focusing on only the regression of ground-truth labels. Second, the global label-correction step after each epoch progressively shifts the networks towards predicting consistent results at different time points. In other words, the networks are expected to have small variations, and those samples that lead to large variations are considered low-quality annotated samples since low-quality labels, to a certain extent, contradict the information provided in the images. These suspected low-quality annotated samples should be corrected. Direct label correction to replace the old labels, which are predicted by the framework in previous steps, aims to find useful information from the framework’s own outputs; thus, it offers the framework a self-evolving capability. Since in most cases, medical images of the same region appear roughly similar among different patients, we believe that this evolving capability of updating a portion of suspected low-quality annotations is reasonable and applicable. The remaining non-updated samples can guarantee the stable training of the network. Third, the proposed cross-model co-optimization learning can prevent error propagation and accumulation in one network. By building two networks and letting them exchange information, we reduce the risk of network over-fitting to its own predicted pseudo-labels.</p>
    </sec>
    <sec id="Sec5">
      <title>Semi-supervised learning with severely limited training samples</title>
      <p id="Par13">The CHAOS dataset, which is built for liver segmentation, is introduced to investigate the effectiveness of AIDE for SSL (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). Different sizes of high-quality annotated image samples are utilized to train the networks. As expected, a greater number of training samples results in improved segmentation results (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). Compared to that of networks trained with all ten labeled cases (331 image samples, DSC: 87.9%, RAVD: 10.4%, ASSD: 4.65 mm, and MSSD: 65.1 mm), the segmentation performance of networks trained with only one labeled case (30 image samples) is significantly worse (DSC: 70.1%, RAVD: 16.1%, ASSD: 16.1 mm, and MSSD: 176.3 mm). Thus, when training deep-learning models utilizing only the labeled samples in a fully supervised optimization manner, large datasets are required to obtain satisfactory results.</p>
      <p id="Par14">To enlarge the training dataset, low-quality noisy labels are generated by models trained with the minimal annotated training data (1 case with 30 image samples). Then, networks are trained with the enlarged dataset. In this work, low-quality labels or noisy labels are counted in an image-based manner. Different levels of noisy labels are experimented with (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). For the fully supervised learning baseline, noisy labels already affect their performance when the noise level is over 20% (<italic>P</italic> = 0.0133 from a two-sided paired <italic>t</italic> test between the DSCs of Exp. 6 and Exp. 7 in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). Under the experimental settings of over 90% noisy labels (S05_30_301_F_NP and S09_30_954_F_NP in Table <xref rid="Tab1" ref-type="table">1</xref>), the segmentation performance of the baseline method deteriorates dramatically. Our proposed AIDE is effective when limited annotations are available and low-quality noisy labels are utilized (Table <xref rid="Tab1" ref-type="table">1</xref>). It achieves much better results (S12_30_954_A_YP in Table <xref rid="Tab1" ref-type="table">1</xref>) compared to those achieved by the baseline method (S10_30_954_F_YP in Table <xref rid="Tab1" ref-type="table">1</xref>) and those obtained by existing methods (pseudo-label<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and co-teaching<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> in Table <xref rid="Tab1" ref-type="table">1</xref>) when the same quantity of labeled training data is utilized (the pseudo-label method is different from our setting of S10_30_954_F_YP in that the low-quality labels are updated during model optimization for the pseudo-label method, whereas for S10_30_954_F_YP, the low-quality labels remain unchanged once generated). Promising performance is achieved by our method with high noise levels such as the 97% noise level experimented with (S12_30_954_A_YP in Table <xref rid="Tab1" ref-type="table">1</xref>). Particularly, with 30 labeled and 954 unlabeled training image samples (noise level of 97%), our method generates comparable results (DSC: 86.9%, RAVD: 10.0%, ASSD: 4.17 mm, and MSSD: 44.6 mm) to those of models trained with 331 high-quality annotated samples (DSC: 88.5%, RAVD: 10.8%, ASSD: 3.64 mm, and MSSD: 43.8 mm) (no significant difference between the DSCs, with <italic>P</italic> = 0.0791 from a two-sided paired <italic>t</italic> test).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Segmentation results of networks under different SSL settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Settings</th><th>Train HQA</th><th>Train LQA</th><th>AIDE</th><th>PP</th><th>DSC (%)</th><th>RAVD (%)</th><th>ASSD (mm)</th><th>MSSD (mm)</th></tr></thead><tbody><tr><td>S01_30_0_F_NP</td><td>30</td><td>0</td><td>No</td><td>No</td><td>70.1</td><td>42.0</td><td>16.1</td><td>176.3</td></tr><tr><td>S02_30_0_F_YP</td><td>30</td><td>0</td><td>No</td><td>Yes</td><td>75.6</td><td>22.0</td><td>7.68</td><td>54.8</td></tr><tr><td>S03_331_0_F_NP</td><td>331</td><td>0</td><td>No</td><td>No</td><td>87.9</td><td>10.4</td><td>4.65</td><td>65.1</td></tr><tr><td>S04_331_0_F_YP</td><td>331</td><td>0</td><td>No</td><td>Yes</td><td>88.5</td><td>10.8</td><td>3.64</td><td>43.8</td></tr><tr><td>S05_30_301_F_NP</td><td>30</td><td>301</td><td>No</td><td>No</td><td>78.4</td><td>19.0</td><td>7.92</td><td>95.3</td></tr><tr><td>S06_30_301_F_YP</td><td>30</td><td>301</td><td>No</td><td>Yes</td><td>79.7</td><td>21.1</td><td>6.10</td><td>53.4</td></tr><tr><td>S07_30_301_A_NP</td><td>30</td><td>301</td><td>Yes</td><td>No</td><td>79.8</td><td>18.5</td><td>10.8</td><td>116.3</td></tr><tr><td>S08_30_301_A_YP</td><td>30</td><td>301</td><td>Yes</td><td>Yes</td><td>82.9</td><td>16.9</td><td>5.43</td><td>49.8</td></tr><tr><td>S09_30_954_F_NP</td><td>30</td><td>954</td><td>No</td><td>No</td><td>79.5</td><td>19.7</td><td>8.43</td><td>104.2</td></tr><tr><td>S10_30_954_F_YP</td><td>30</td><td>954</td><td>No</td><td>Yes</td><td>80.2</td><td>19.1</td><td>6.47</td><td>53.8</td></tr><tr><td>S11_30_954_A_NP</td><td>30</td><td>954</td><td>Yes</td><td>No</td><td>86.1</td><td>10.2</td><td>5.49</td><td>75.8</td></tr><tr><td>S12_30_954_A_YP</td><td>30</td><td>954</td><td>Yes</td><td>Yes</td><td>86.9</td><td>10.0</td><td>4.17</td><td>44.6</td></tr><tr><td>Pseudo-label<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td>30</td><td>954</td><td>No</td><td>Yes</td><td>81.4</td><td>19.7</td><td>5.99</td><td>52.9</td></tr><tr><td>Co-teaching<sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td>30</td><td>954</td><td>No</td><td>Yes</td><td>82.4</td><td>15.8</td><td>5.50</td><td>49.2</td></tr></tbody></table><table-wrap-foot><p>HQA and LQA indicate high-quality and low-quality annotations. LQAs are generated by the model trained using the data provided with HQAs. PP refers to post-processing, which is the process of keeping the largest connected components. Context is included in the notation of the experimental setting. For the setting S05_30_301_F_NP, 30 refers to 30 training samples with HQAs, 301 means 301 training samples with LQAs, F indicates training with the conventional fully supervised learning approach, and NP means no post-processing.</p></table-wrap-foot></table-wrap></p>
      <p id="Par15">Different numbers of labeled training image samples have been experimented with, and the results confirm that our method is always effective (Exp. 14 to Exp. 19 in Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>). In addition, as the number of unlabeled training samples increases, the performance of our method (compared that of S12_30_954_A_YP with S08_30_301_A_YP in Table <xref rid="Tab1" ref-type="table">1</xref>) continues to improve. This property is especially important for clinical applications, as unlabeled data are much easier to collect. Visualizations of the self-corrected labels and model outputs are shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref> (more results can be found in Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>). Overall, the labels after correction are closer to the high-quality annotations (Fig. <xref rid="Fig2" ref-type="fig">2a, b</xref>), and our AIDE generates more accurate segmentations than do the corresponding baseline networks (Fig. <xref rid="Fig2" ref-type="fig">2c, d</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><title>Visualizations of label correction and segmentation results for SSL.</title><p><bold>a</bold>, <bold>b</bold> Example results of training data label correction. The red regions in the four images from left to right correspond to the high-quality label, the low-quality label utilized to train the model, and the self-corrected labels of the two networks, respectively. <bold>c</bold>, <bold>d</bold> Example segmentation results. The first columns are the corresponding high-quality labels. The second to the last columns are the results achieved under settings S02, S04, S06, S08, S10, and S12 in Table <xref rid="Tab1" ref-type="table">1</xref>. The numbers are the DSC values (%). In each subfigure, the first row (white background) shows the segmentation results in 3D rendering, and the second row (black background) gives the results of a single selected slice in 2D.</p></caption><graphic xlink:href="41467_2021_26216_Fig2_HTML" id="d32e1169"/></fig></p>
      <p id="Par16">Results generated by AIDE, which is trained with ten training cases (one labeled case and nine unlabeled cases), are submitted for evaluation online, and an average DSC of 83.1% is achieved on the test data. This DSC value is within the range of the values achieved by fully supervised methods trained on 20 labeled cases ([63%, 95%] as reported in the summary paper of the CHAOS challenge<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>). Considering the small labeled image data utilized, our method achieves a reasonably good performance compared to these fully supervised ones.</p>
    </sec>
    <sec id="Sec6">
      <title>Unsupervised domain adaptation with large domain discrepancies</title>
      <p id="Par17">Three domains (domains 1, 2, and 3) of prostate datasets with different image acquisition protocols are utilized to inspect the framework performance when no target domain annotations are provided during model training. Domain 3 is a combined dataset with different image acquisition parameters. When training with a single domain dataset, the networks become biased to the domain properties, and the performance on data from other domains is compromised. Table <xref rid="Tab2" ref-type="table">2</xref> presents the results of networks tested on the same domain or different domains from the training set. Obvious reductions in segmentation accuracy are observed when cross-domain testing (especially for models optimized in domain 1 or domain 2) is performed. Meanwhile, when labeled training data from a different domain are included during model training, the model performance improves slightly (Supplementary Table <xref rid="MOESM1" ref-type="media">3</xref>).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Segmentation results of networks trained and tested with prostate datasets of different domains.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Training dataset</th><th>Testing dataset</th><th>DSC (%)</th><th>RAVD (%)</th><th>ASSD (mm)</th><th>MSSD (mm)</th></tr></thead><tbody><tr><td>Domain 1</td><td>Domain 1</td><td>88.9</td><td>11.1</td><td>1.49</td><td>8.27</td></tr><tr><td>Domain 1</td><td>Domain 2</td><td>45.8</td><td>65.6</td><td>5.42</td><td>17.8</td></tr><tr><td>Domain 1</td><td>Domain 3</td><td>40.0</td><td>67.8</td><td>9.24</td><td>24.3</td></tr><tr><td>Domain 2</td><td>Domain 1</td><td>69.8</td><td>35.5</td><td>3.43</td><td>12.9</td></tr><tr><td>Domain 2</td><td>Domain 2</td><td>87.3</td><td>16.1</td><td>1.36</td><td>7.89</td></tr><tr><td>Domain 2</td><td>Domain 3</td><td>55.1</td><td>53.0</td><td>8.32</td><td>21.3</td></tr><tr><td>Domain 3</td><td>Domain 1</td><td>80.9</td><td>8.61</td><td>2.43</td><td>14.9</td></tr><tr><td>Domain 3</td><td>Domain 2</td><td>86.1</td><td>12.3</td><td>1.58</td><td>8.27</td></tr><tr><td>Domain 3</td><td>Domain 3</td><td>86.7</td><td>8.77</td><td>1.55</td><td>9.20</td></tr></tbody></table></table-wrap></p>
      <p id="Par18">Similar to the strategies adopted for SSL, low-quality noisy labels for the target domain training data are generated by the models trained with the source domain labeled data, and model training from the scratch with the combined dataset is conducted to facilitate the model adaptation to new domains. Figure <xref rid="Fig3" ref-type="fig">3</xref> presents the prostate segmentation results of models optimized without target domain high-quality training annotations, with or without the proposed AIDE framework (more results can be found in Supplementary Figs. <xref rid="MOESM1" ref-type="media">3</xref>–<xref rid="MOESM1" ref-type="media">5</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">3</xref>). AIDE successfully improves the segmentation performance, as indicated by the large increase in DSC (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>) and the notable decrease in RAVD (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). As a special case, when transferring models from domain 1 to domain 2, direct model training with the combined dataset utilizing high-quality annotations for domain 1 and model-generated low-quality labels for domain 2 produces worse results (DSC: 33.7%, RAVD: 71.5%, ASSD: 5.94 mm, and MSSD: 20.7 mm) than those achieved by direct testing of domain 1 optimized models on domain 2 data (DSC: 45.8%, RAVD: 65.6%, ASSD: 5.42 mm, and MSSD: 17.8 mm). On the other hand, AIDE increases DSC by more than 30% (from 45.8 to 80.0%). However, the distance metrics (ASSD in Fig. <xref rid="Fig3" ref-type="fig">3c</xref> and MSSD in Fig. <xref rid="Fig3" ref-type="fig">3d</xref>) are not largely improved. In addition, even with AIDE, the performance on domain 3 is worse than that on domain 1 and domain 2. Since domain 3 is a combination of data from different sources, we speculate that adapting models to a combined diverse dataset is more difficult. Nevertheless, the performance is acceptable, and the DSC values fall in the range of reported results (from 0.71 to 0.90)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Besides, better performance is achieved by AIDE than that of the two existing methods, pseudo-label<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and co-teaching<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> (Supplementary Table <xref rid="MOESM1" ref-type="media">4</xref>). Overall, our experiments validate that the proposed framework achieves promising results for UDA.<fig id="Fig3"><label>Fig. 3</label><caption><title>Results of prostate segmentation for UDA.</title><p><bold>a</bold>–<bold>d</bold>, The four evaluation metrics, DSC (%), RAVD (%), ASSD (mm), and MSSD (mm). In each subfigure, the left and right mappings indicate that the networks are trained without and with the proposed AIDE. D1, D2, and D3 refer to domains 1, 2, and 3. In each mapping, the vertical axis indicates the dataset utilized to train the models and the horizontal axis indicates the dataset utilized to test the models. <bold>e</bold>–<bold>f</bold> Example segmentation results when transferring models between domains. GT stands for ground truth, referring to the high-quality annotations. Conven is conventional, indicating that the results are generated by a fully supervised optimization method utilizing the combined dataset (high-quality labels for the source domain training data and model-generated low-quality labels for the target domain training data).</p></caption><graphic xlink:href="41467_2021_26216_Fig3_HTML" id="d32e1457"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Noisy label learning with annotations provided by different annotators</title>
      <p id="Par19">Four subtasks, including prostate segmentation, brain growth segmentation, brain tumor segmentation, and kidney segmentation, are investigated using the QUBIQ datasets when multiple annotations are provided. Variations exist between annotations provided by different experts, especially for small target objects (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). We treat each set of annotations provided by respective annotators as noisy labels. Overall, testing with different thresholds shows that AIDE can generate segmentation result distributions that are consistent with the annotators (Tables <xref rid="Tab3" ref-type="table">3</xref>–<xref rid="Tab5" ref-type="table">5</xref> and Supplementary Figs. <xref rid="MOESM1" ref-type="media">6</xref> and <xref rid="MOESM1" ref-type="media">7</xref>). Although the improvements on relatively large region segmentation tasks (Task1_Prostate Segmentation in Table <xref rid="Tab3" ref-type="table">3</xref> and Task2_Brain Growth Segmentation in Table <xref rid="Tab4" ref-type="table">4</xref>) are minor, AIDE achieves better performance on the more challenging small object segmentation tasks (Task3_Brain Tumor Segmentation and Task4_Kidney Segmentation in Table <xref rid="Tab5" ref-type="table">5</xref>). Online server evaluation gives average DSCs of 93.2%, 86.6%, 93.7%, and 89.8% on the four tasks for our method optimized with only a single set of annotations from one annotator for each task. These results are comparable to those achieved by methods utilizing the annotations provided by multiple annotators.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Segmentation performance (DSC: %) of Task1_Prostate Segmentation on the QUBIQ datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Anno1</th><th>Anno2</th><th>Anno3</th><th>Anno4</th><th>Anno5</th><th>Anno6</th></tr></thead><tbody><tr><td>Conventional</td><td>89.2</td><td>87.9</td><td>84.3</td><td>89.3</td><td>86.4</td><td>90.0</td></tr><tr><td>AIDE</td><td>91.4</td><td>89.2</td><td>85.2</td><td>90.4</td><td>88.1</td><td>90.3</td></tr></tbody></table><table-wrap-foot><p>Conventional method refers to the fully supervised method utilizing labels of the respective annotations (e.g., Anno1). Anno1 to Anno7 indicate the annotations utilized to train the model (e.g., Anno1 means annotations from annotator 1 are used).</p></table-wrap-foot></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Segmentation performance (DSC: %) of Task2_Brain Growth Segmentation on the QUBIQ datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Anno1</th><th>Anno2</th><th>Anno3</th><th>Anno4</th><th>Anno5</th><th>Anno6</th><th>Anno7</th></tr></thead><tbody><tr><td>Conventional</td><td>70.7</td><td>73.2</td><td>73.8</td><td>73.2</td><td>71.6</td><td>72.1</td><td>71.8</td></tr><tr><td>AIDE</td><td>71.1</td><td>75.0</td><td>74.8</td><td>73.5</td><td>71.6</td><td>72.7</td><td>72.0</td></tr></tbody></table></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Segmentation performance (DSC: %) of Task3_Brain Tumor Segmentation and Task4_Kidney Segmentation on the QUBIQ datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Anno1</th><th>Anno2</th><th>Anno3</th></tr></thead><tbody><tr><td rowspan="1" colspan="3">Task3_Brain Tumor Segmentation</td><td/></tr><tr><td> Conventional</td><td>83.9</td><td>83.1</td><td>84.2</td></tr><tr><td> AIDE</td><td>92.6</td><td>91.8</td><td>92.7</td></tr><tr><td rowspan="1" colspan="3">Task4_Kidney Segmentation</td><td/></tr><tr><td> Conventional</td><td>69.8</td><td>68.9</td><td>70.7</td></tr><tr><td> AIDE</td><td>85.4</td><td>83.8</td><td>89.9</td></tr></tbody></table></table-wrap></p>
      <p id="Par20">DNNs possess a high capacity to fit training data, even in the presence of noise. Two or even three annotators may not be sufficient to cover the full range of observer variability<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. To force the model to learn from the real data distributions instead of learning possible biases introduced by annotators, for conventional fully supervised deep-learning models, trustworthy labels obtained by effectively combining the annotations provided by different observers are needed. With our proposed method, only noisy labels are utilized during the whole training process, and the model can self-adjust and update the training labels via the proposed local label-filtering and global label-correction steps to correct possible observer biases. More satisfactory segmentation results can then be generated. Therefore, compared to the fully supervised learning approach, AIDE may relax the demand for multiple annotators.</p>
    </sec>
    <sec id="Sec8">
      <title>Real-life case study for breast tumor segmentation</title>
      <p id="Par21">Further evaluations on three breast tumor segmentation datasets (GGH dataset from Guangdong General Hospital, GPPH dataset from Guizhou Provincial People’s Hospital, and HPPH dataset from Henan Provincial People’s Hospital) are conducted to investigate the feasibility of the proposed framework for processing raw clinical data. Dynamic contrast-enhanced MR (DCE-MR) images are acquired. The experimental results validate the effectiveness of AIDE in analyzing clinical samples with decreased manual efforts (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). Under the same experimental settings, AIDE generates much better results than the respective baselines, increasing the DSC values by more than 7% (7.7% absolute increase for GGH dataset between LQA200 and LQA200_Ours in Fig. <xref rid="Fig4" ref-type="fig">4a</xref>, 18.2% absolute increase for GPPH dataset between LQA100 and LQA100_Ours in Fig. <xref rid="Fig4" ref-type="fig">4b</xref>, and 10.0% absolute increase for HPPH dataset between LAQ272 and LQA272_Ours in Fig. <xref rid="Fig4" ref-type="fig">4c</xref>). In addition, despite the small number of training annotations being utilized (10% of those utilized by fully supervised method for GGH and GPPH, and 9.2% for HPPH), AIDE can always achieve segmentation performance similar to that of the corresponding fully supervised models. Specifically, for the GGH dataset, AIDE achieves an average DSC of <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.690\pm 0.251$$\end{document}</tex-math><mml:math id="M2"><mml:mn>0.690</mml:mn><mml:mo>±</mml:mo><mml:mn>0.251</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq1.gif"/></alternatives></inline-formula>, whereas the fully supervised model achieves <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.722\pm 0.208$$\end{document}</tex-math><mml:math id="M4"><mml:mn>0.722</mml:mn><mml:mo>±</mml:mo><mml:mn>0.208</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq2.gif"/></alternatives></inline-formula> (<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.0608$$\end{document}</tex-math><mml:math id="M6"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0608</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq3.gif"/></alternatives></inline-formula>) (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>). For GPPH, AIDE obtains <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.654\pm 0.221$$\end{document}</tex-math><mml:math id="M8"><mml:mn>0.654</mml:mn><mml:mo>±</mml:mo><mml:mn>0.221</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq4.gif"/></alternatives></inline-formula>, and the fully supervised model obtains <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.678\pm 0.260$$\end{document}</tex-math><mml:math id="M10"><mml:mn>0.678</mml:mn><mml:mo>±</mml:mo><mml:mn>0.260</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq5.gif"/></alternatives></inline-formula> (<inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.2927$$\end{document}</tex-math><mml:math id="M12"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2927</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq6.gif"/></alternatives></inline-formula>) (Fig. <xref rid="Fig4" ref-type="fig">4b</xref>). For HPPH, AIDE obtains <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.731\pm 0.196$$\end{document}</tex-math><mml:math id="M14"><mml:mn>0.731</mml:mn><mml:mo>±</mml:mo><mml:mn>0.196</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq7.gif"/></alternatives></inline-formula>, and the fully supervised model obtains <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.738\pm 0.227$$\end{document}</tex-math><mml:math id="M16"><mml:mn>0.738</mml:mn><mml:mo>±</mml:mo><mml:mn>0.227</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq8.gif"/></alternatives></inline-formula> (<inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.6545$$\end{document}</tex-math><mml:math id="M18"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6545</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq9.gif"/></alternatives></inline-formula>) (Fig. <xref rid="Fig4" ref-type="fig">4c</xref>). The visual results confirm the effectiveness of AIDE in generating segmentation contours that are similar to the ground-truth labels (Fig. <xref rid="Fig4" ref-type="fig">4d–f</xref>). We further observe that AIDE performs better with larger datasets (comparing Fig. <xref rid="Fig4" ref-type="fig">4c</xref> to Fig. <xref rid="Fig4" ref-type="fig">4b</xref>), which can be very useful in real clinical applications considering the large quantities of unannotated images accumulated every day.<fig id="Fig4"><label>Fig. 4</label><caption><title>Results of breast tumor segmentation.</title><p><bold>a</bold>−<bold>c</bold> Results on GGH, GPPH, and HPPH datasets, respectively. LQA and HQA indicate low- and high-quality annotations. The numbers after HQA refer to the annotations utilized to train the models. For LQA, the numbers indicate that we utilize the respective fewest HQA data with the remaining annotations generated by the pretrained models. For example, LQA200 in (<bold>a</bold>) means 20 high-quality labeled and 180 low-quality labeled data are utilized. Data are represented as box plots. The central red lines indicate median DSC values, green triangles the average DSC values, boxes the interquartile range, whiskers the smallest and largest values, and data points (<inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$+$$\end{document}</tex-math><mml:math id="M20"><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq10.gif"/></alternatives></inline-formula>) outliers. * indicates a significant difference between the corresponding experiments, with ***<inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P\le 0.001$$\end{document}</tex-math><mml:math id="M22"><mml:mi>P</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.001</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq11.gif"/></alternatives></inline-formula>, **<inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P\le 0.005$$\end{document}</tex-math><mml:math id="M24"><mml:mi>P</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.005</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq12.gif"/></alternatives></inline-formula>, and *<inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P\le 0.05$$\end{document}</tex-math><mml:math id="M26"><mml:mi>P</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq13.gif"/></alternatives></inline-formula> (two-sided paired <italic>t</italic> test, <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n=100$$\end{document}</tex-math><mml:math id="M28"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq14.gif"/></alternatives></inline-formula> independent patient cases). <bold>a</bold> Between HQA20 and LQA200_Ours, <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.0002$$\end{document}</tex-math><mml:math id="M30"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0002</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq15.gif"/></alternatives></inline-formula>; between HQA50 and LQA200_Ours, <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.0139$$\end{document}</tex-math><mml:math id="M32"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0139</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq16.gif"/></alternatives></inline-formula>; between HQA100 and LQA200_Ours, <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.3119$$\end{document}</tex-math><mml:math id="M34"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3119</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq17.gif"/></alternatives></inline-formula>; between HQA200 and LQA200_Ours, <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.0608$$\end{document}</tex-math><mml:math id="M36"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0608</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq18.gif"/></alternatives></inline-formula>; between LQA200 and LQA200_Ours, <inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.0017$$\end{document}</tex-math><mml:math id="M38"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0017</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq19.gif"/></alternatives></inline-formula>. <bold>b</bold> Between HQA10 and LQA100_Ours, <inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P &lt; 0.0001$$\end{document}</tex-math><mml:math id="M40"><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq20.gif"/></alternatives></inline-formula>; between HQA25 and LQA100_Ours, <inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.0002$$\end{document}</tex-math><mml:math id="M42"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0002</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq21.gif"/></alternatives></inline-formula>; between HQA50 and LQA100_Ours, <inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.7602$$\end{document}</tex-math><mml:math id="M44"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.7602</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq22.gif"/></alternatives></inline-formula>; between HQA100 and LQA100_Ours, <inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.2927$$\end{document}</tex-math><mml:math id="M46"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2927</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq23.gif"/></alternatives></inline-formula>; between LQA100 and LQA100_Ours, <inline-formula id="IEq24"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P &lt; 0.0001$$\end{document}</tex-math><mml:math id="M48"><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq24.gif"/></alternatives></inline-formula>. <bold>c</bold> Between HQA25 and LQA272_Ours, <inline-formula id="IEq25"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P \, &lt; \, 0.0001$$\end{document}</tex-math><mml:math id="M50"><mml:mi>P</mml:mi><mml:mspace width="0.25em"/><mml:mo>&lt;</mml:mo><mml:mspace width="0.25em"/><mml:mn>0.0001</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq25.gif"/></alternatives></inline-formula>; between HQA50 and LQA272_Ours, <inline-formula id="IEq26"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P &lt; 0.0001$$\end{document}</tex-math><mml:math id="M52"><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq26.gif"/></alternatives></inline-formula>; between HQA100 and LQA272_Ours, <inline-formula id="IEq27"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.0075$$\end{document}</tex-math><mml:math id="M54"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0075</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq27.gif"/></alternatives></inline-formula>; between HQA200 and LQA272_Ours, <inline-formula id="IEq28"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.2925$$\end{document}</tex-math><mml:math id="M56"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2925</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq28.gif"/></alternatives></inline-formula>; between HQA272 and LQA272_Ours, <inline-formula id="IEq29"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.6545$$\end{document}</tex-math><mml:math id="M58"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.6545</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq29.gif"/></alternatives></inline-formula>; between LQA272 and LQA272_Ours, <inline-formula id="IEq30"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P &lt; 0.0001$$\end{document}</tex-math><mml:math id="M60"><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.0001</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq30.gif"/></alternatives></inline-formula>. <bold>d</bold>−<bold>f</bold> Visualizations of segmentation maps on the three datasets. The three columns correspond to the results of LQA, LQA_Ours, and independent radiologists. Red contours indicate the high-quality annotations. Magenta, green, and yellow contours are the results of LQA, LQA_Ours, and the independent radiologists.</p></caption><graphic xlink:href="41467_2021_26216_Fig4_HTML" id="d32e2258"/></fig></p>
      <p id="Par22">Additional radiologists were employed to segment the breast tumors in the central slices of the three test sets (Fig. <xref rid="Fig4" ref-type="fig">4d–f</xref> and Supplementary Figs. <xref rid="MOESM1" ref-type="media">8</xref>–<xref rid="MOESM1" ref-type="media">10</xref>). For the GGH dataset, the manual annotations achieve an average DSC of <inline-formula id="IEq31"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.621\pm 0.155$$\end{document}</tex-math><mml:math id="M62"><mml:mn>0.621</mml:mn><mml:mo>±</mml:mo><mml:mn>0.155</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq31.gif"/></alternatives></inline-formula>, which is worse than that of AIDE (<inline-formula id="IEq32"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.690\pm 0.251$$\end{document}</tex-math><mml:math id="M64"><mml:mn>0.690</mml:mn><mml:mo>±</mml:mo><mml:mn>0.251</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq32.gif"/></alternatives></inline-formula>), with <inline-formula id="IEq33"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.0098$$\end{document}</tex-math><mml:math id="M66"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0098</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq33.gif"/></alternatives></inline-formula>. For GPPH, the manual annotations obtain <inline-formula id="IEq34"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.861\pm 0.086$$\end{document}</tex-math><mml:math id="M68"><mml:mn>0.861</mml:mn><mml:mo>±</mml:mo><mml:mn>0.086</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq34.gif"/></alternatives></inline-formula> and AIDE obtains <inline-formula id="IEq35"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.846\pm 0.118$$\end{document}</tex-math><mml:math id="M70"><mml:mn>0.846</mml:mn><mml:mo>±</mml:mo><mml:mn>0.118</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq35.gif"/></alternatives></inline-formula>. No significant difference is found (<inline-formula id="IEq36"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.3317$$\end{document}</tex-math><mml:math id="M72"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3317</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq36.gif"/></alternatives></inline-formula>). For HPPH, the manual annotations obtain <inline-formula id="IEq37"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.735\pm 0.225$$\end{document}</tex-math><mml:math id="M74"><mml:mn>0.735</mml:mn><mml:mo>±</mml:mo><mml:mn>0.225</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq37.gif"/></alternatives></inline-formula> and AIDE obtains <inline-formula id="IEq38"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$0.761\pm 0.234$$\end{document}</tex-math><mml:math id="M76"><mml:mn>0.761</mml:mn><mml:mo>±</mml:mo><mml:mn>0.234</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq38.gif"/></alternatives></inline-formula> with no significant difference observed (<inline-formula id="IEq39"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=0.3079$$\end{document}</tex-math><mml:math id="M78"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3079</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq39.gif"/></alternatives></inline-formula>). Overall, our proposed AIDE can generate comparable or even better segmentation results when compared with those of radiologists.</p>
      <p id="Par23">Breast tumor segmentation in DCE-MR images is a challenging task due to the severe class imbalance issue (very small tumor regions compared to the whole breast images) and the high confounding background signals (organs in the chest and dense glandular tissues). As a result, the DSC values obtained for breast tumor segmentation are not as high as those for other tasks (such as prostate segmentation). Nevertheless, our method is not intended to replace radiologists in the disease diagnosis or treatment planning workflow but serves as an automated computer-aided system. Recently, Zhang et al.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> achieved a mean DSC of 72% for breast tumor segmentation in DCE-MR images with a hierarchical convolutional neural network framework, and Qiao et al.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> obtained a value of 78% utilizing 3D U-Net and single-phase DCE-MR images. Our results (73.1% on the HPPH dataset) are comparable to these literature-reported values achieved by fully supervised learning with hundreds of patient cases. Considering the small number of training annotations utilized (25 cases for the HPPH dataset), our proposed method can be a valuable tool in clinical practice to assist radiologists in achieving fast and reliable breast tumor segmentation.</p>
    </sec>
  </sec>
  <sec id="Sec9" sec-type="discussion">
    <title>Discussion</title>
    <p id="Par24">Image segmentation plays an important role in medical imaging applications. In recent studies, DNNs have been widely employed to automate and accelerate the segmentation process<sup><xref ref-type="bibr" rid="CR37">37</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup>. Despite the reported successful applications, the performance of DNNs depends heavily on the quality of the utilized training datasets. Frameworks that can optimize DNNs without strict reliance on large and high-quality annotated training datasets can significantly narrow the gap between research and clinical applications. In this study, we propose an annotation-efficient deep-learning framework, AIDE, for medical image segmentation network learning with imperfect datasets to address three challenges: SSL, UDA, and NLL. Our target is not to build a more sophisticated model for fully supervised learning, but to build a framework that can work properly without sufficient labeled data, so as to alleviate the reliance on the time-consuming and expensive manual annotations when applying AI to medical imaging. Extensive experiments have been conducted with open datasets. The results show that AIDE performs better than conventional fully supervised models under the same training conditions and even comparable to models trained with corresponding perfect datasets, thereby validating the effectiveness of AIDE. Additional experiments on clinical breast tumor segmentation datasets from three medical centers further prove the robustness and generalization ability of AIDE when processing real clinical data. On all three independent datasets, AIDE generates satisfactory segmentation results by utilizing only 10% of the training annotations, which indicates that AIDE can alleviate radiologists’ manual efforts compared to conventional fully supervised DNNs.</p>
    <p id="Par25">Recent deep-learning works have presented encouraging performance in handling certain types of imperfect datasets in isolation<sup><xref ref-type="bibr" rid="CR41">41</xref>–<xref ref-type="bibr" rid="CR45">45</xref></sup> but have not yet shown general applicability by addressing all three types. Methods such as data augmentation<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>, transfer learning<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>, semi-supervised learning<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup>, and self-supervised learning<sup><xref ref-type="bibr" rid="CR48">48</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup> have been extensively investigated to handle cases with limited training annotations or no target domain annotations. By contrast, much less attention has been given to noisy label learning in medical imaging<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup>. Most existing studies concentrate on designing new loss weighting strategies<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup> or new loss functions<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. Since various issues with the datasets may exist in reality, the effectiveness of these methods is compromised<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. The available methods most closely related to ours are pseudo-labels<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and co-teaching<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Compared to existing pseudo-label studies that update the pseudo-labels of all unlabeled data simultaneously during network learning<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR54">54</xref>,<xref ref-type="bibr" rid="CR55">55</xref></sup>, label updating in AIDE is conducted in an orderly (label updating is conducted according to the calculated similarities between the temporal network predictions and noisy labels in defined training epochs) and selective (only a defined percentage of noisy labels are updated) manner according to an observed small loss criterion, which has also been noted and confirmed for natural images<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. Moreover, AIDE trains the model with the segmentation loss on filtered labels to avoid the negative effects of highly noisy labels, and an additional consistency loss is introduced to enforce consistent predictions with augmented inputs for the remaining suspected highly noisy data. AIDE also differs from the two-model co-teaching method<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>: AIDE generates pseudo-labels for suspected low-quality annotated data in an effective and progressive manner. During network training, a hyper-parameter that increases from 0 to 1 within defined initial training epochs is introduced. This hyper-parameter controls the contribution of the consistency loss; thus, the consistency loss becomes increasingly important for network optimization in the training process. Additionally, the local label-filtering and global label-correction design progressively places more emphasis on pseudo-labels. These properties of AIDE contribute to the full utilization of valuable medical image data and improve the model performance.</p>
    <p id="Par26">Constructing large datasets with high-quality annotations is particularly challenging in medical imaging. The process of medical image annotation, especially dense annotation for image segmentation, is highly resource intensive. Different from natural images, only domain experts have the knowledge to annotate medical images. In some cases, such as our brain tumor segmentation task, large variations exist between experts (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>), which raises the necessity of multiple annotators to achieve consensus annotations<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. Employing a large number of domain experts to annotate large medical image datasets requires massive financial and logistical resources that are difficult to obtain in many applications. Our proposed AIDE has the potential to solve these issues effectively. It achieves promising performance by utilizing only 10% of the training annotations used by its fully supervised counterparts. Nevertheless, label-free or unsupervised learning that can eliminate manual annotation entirely is more appealing<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>. We are still working on improving the methodology to further reduce the reliance on annotations. In our following study, we will seek to address the challenge of unsupervised deep learning for large-scale and automatic medical image segmentation. Furthermore, although only medical image segmentation is considered in this work, AIDE might be applicable to other medical image analysis tasks, for example, image classification. The flexibility of AIDE in this perspective will also be evaluated in the future work.</p>
    <p id="Par27">In summary, we have analyzed and empirically demonstrated that our proposed framework, AIDE, can achieve accurate medical image segmentation to address the three challenges of semi-supervised learning, unsupervised domain adaptation, and noisy label learning. Therefore, AIDE provides another perspective for DNNs to handle imperfect training datasets. In the real-life case study, compared to conventional DNN training, AIDE can save almost 90% of the manual effort required to annotate the training data. With further development and clinical trials, such a ten-fold improvement in efficiency in utilizing expert labels is expected to promote a wide range of biomedical applications. Our proposed framework has the potential to improve the medical image diagnosis workflow in an efficient manner and at a low cost.</p>
  </sec>
  <sec id="Sec10">
    <title>Methods</title>
    <sec id="Sec11">
      <title>Public datasets</title>
      <p id="Par28">Abdomen MR images from the CHAOS challenge are adopted<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR59">59</xref></sup> (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>). These images were acquired with a 1.5 T Philips MRI system. The image matrix size is 256 × 256, with an inplane resolution from 1.36 to 1.89 mm. The slice thickness is between 5.5 and 9 mm. One 3D image contains 26−50 slices. Although multi-parametric MR images are provided, they are not registered and are thus difficult to utilize as multimodal inputs for the segmentation task. We utilize the T1-DUAL images. For each patient, there is one inphase image and one outphase image, which can be treated as multimodal inputs. In total, the challenge provides data from 40 patients, 20 cases (647 T1-DUAL images, 647 samples) with high-quality annotations and 20 cases (653 T1-DUAL images, 653 samples) without annotations. According to the challenge, image annotation was performed by two teams. Each team included a radiology expert and an experienced medical image processing scientist. Then, a third radiology expert and another medical imaging scientist inspected the labels, which were further fine-tuned according to discussions between annotators and controllers. For our setting of SSL, only the label of one randomly selected case (30 labeled image samples) is utilized, and pseudo-labels for the remaining data (29 cases including 9 randomly selected cases from the provided labeled cases and 20 unlabeled cases) are generated by a network trained with the labeled samples. Model testing is performed on the remaining ten labeled cases.</p>
      <p id="Par29">T2-weighted MR images of the prostate from two challenges (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>), NCI-ISBI 2013 <sup><xref ref-type="bibr" rid="CR60">60</xref></sup> and PROMISE12 <sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, are utilized. NCI-ISBI 2013 consists of three groups of data: the training set (60 cases), the test set (10 cases), and the leaderboard set (10 cases). All are provided with high-quality annotations. Annotation was performed by two groups separately, one with two experts and the other with three experts. We combine the test set and leaderboard set to form our enlarged test set (20 cases) and divide the dataset into two datasets according to the data acquisition sites to form two domain samples. The domain 1 dataset contains training and testing data collected with 1.5 T MRI systems from Boston Medical Center (30 training cases and 10 test cases), and the domain 2 dataset contains data collected with 3.0 T MRI systems from Radboud University Nijmegen Medical Centre (30 training cases and 10 test cases). PROMISE12 provides data from 50 patients with high-quality annotations. Annotation was conducted by an experienced reader and then checked by a second expert. These samples were collected from different centers using different MRI machines with different acquisition protocols. Deleting 13 cases that are the same as those in domain 2, 37 cases collected from Haukeland University Hospital (12 cases), Beth Israel Deaconess Medical Center (12 cases), and University College London (13 cases) are obtained. Therefore, PROMISE12 can be treated as a combined dataset from different domains and is referred to as the domain 3 dataset in our experiments. Of the 37 cases, 27 are randomly selected as the training set, and the remaining 10 form the test set. UDA is constructed via model learning with labeled training data from the source domain and unlabeled training data from the target domain. Pseudo-labels are generated for the target domain training data, and the high-quality labeled source domain training data and low-quality noisily labeled target domain training data form the combined dataset to facilitate the domain transfer of the models. Performance is evaluated on the target domain testing data.</p>
      <p id="Par30">The QUBIQ challenge provides four datasets with annotations from multiple experts, including a prostate image dataset with 55 cases (6 sets of annotations), a brain growth image dataset with 39 cases (7 sets of annotations), a brain tumor image dataset with 32 cases (3 sets of annotations), and a kidney image dataset with 24 cases (3 sets of annotations). In total, there are seven binary segmentation tasks: two for prostate segmentation, one for brain growth segmentation, three for brain tumor segmentation, and one for kidney segmentation. In this study, we conduct four tasks (Fig. <xref rid="Fig1" ref-type="fig">1c</xref>), one for each dataset, to investigate the effects of our proposed AIDE framework. Following the challenge, 48 cases of prostate data, 34 cases of brain growth data, 28 cases of brain tumor data, and 20 cases of kidney data are utilized as the training data. The remaining are used as testing data. Each set of annotations is considered a noisy label set for model training. As defined by the challenge, performance characterization of models is conducted by comparing the predictions (continuous values in [0, 1]) to the continuous ground-truth labels (obtained by averaging multiple experts’ annotations) by thresholding the continuous labels at different probability levels (0.1, 0.2, … 0.8, 0.9). Then, the DSCs for all thresholds are averaged to obtain the final metrics.</p>
    </sec>
    <sec id="Sec12">
      <title>Clinical datasets</title>
      <p id="Par31">This retrospective study was approved by the institutional review board of each participating hospital with the written informed consent requirement waived. All patient records were de-identified before analysis and were reviewed by the institutional review boards to guarantee no potential risk to patients. The researchers who conducted the segmentation tasks have no link to the patients to prevent any possible breach of confidentiality.</p>
      <p id="Par32">Dynamic contrast-enhanced breast MR data from three medical centers, Guangdong General Hospital (GGH), Guizhou Provincial People’s Hospital (GPPH), and Henan Provincial People’s Hospital (HPPH), were investigated (Fig. <xref rid="Fig1" ref-type="fig">1d</xref>). FDA-approved fully features PACS viewer was used to collect the breast image data. GGH provided data for 300 patients, GPPH provided data for 200 patients, and HPPH provided data for 372 patients. For each dataset, three experienced radiologists with more than 10 years of experience provided the image annotations. Two radiologists delineated the breast tumor regions independently, and a third radiologist (the most experienced radiologist) checked the two sets of annotations and made the final decision. For the GGH dataset, breast tumor regions in central slices are annotated, leading to 300 image samples. For the GPPH dataset and HPPH dataset, 3D breast tumor annotations are provided. The GPPH dataset has 4902 annotated image samples, and the HPPH dataset contains 6650 image samples, resulting in a total of 872 MR data points (11,852 image samples) for our experiments. For each dataset, we randomly select 100 annotated patient cases as the respective hold-out test set and use the remaining cases as the training set.</p>
    </sec>
    <sec id="Sec13">
      <title>AIDE framework</title>
      <p id="Par33">Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates the overall AIDE framework. Task standardization is performed to transform SSL and UDA into NLL. For SSL, models are pretrained with the available limited annotated training data, and low-quality labels are generated for the remaining unlabeled training data. For UDA, models are pretrained with the source domain labeled training data, and low-quality labels are generated for the target domain unlabeled training data. Thus, the remaining issue becomes learning with possibly noisy annotations (NLL).</p>
      <p id="Par34">Details of the proposed self-correcting algorithm are presented in Algorithm 1 (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). Two networks are learned in parallel to achieve cross-model self-correction of low-quality labels of imperfect datasets (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). The whole process can be divided into two major steps. The first is local label filtering. In each iteration, a defined percentage of training samples with relatively large segmentation losses is selected by the counterpart model as suspicious samples that have low-quality labels. For these samples, the final loss function is calculated as a weighted summation of the segmentation loss and consistency loss. We choose the commonly utilized combination of Dice loss and cross-entropy loss as our segmentation loss in this work (Eq. (<xref rid="Equ1" ref-type="">1</xref>)). The consistency loss is introduced to the network as a consistency regularization. Specifically, the means of the outputs of <italic>K</italic> augmented inputs are calculated and are regarded as pseudo-labels after temperature sharpening<sup><xref ref-type="bibr" rid="CR61">61</xref>,<xref ref-type="bibr" rid="CR62">62</xref></sup>. Consistency loss, which is implemented as the mean square error (MSE) loss (Eq. (<xref rid="Equ2" ref-type="">2</xref>)), is calculated between the network outputs of suspected noisy samples and the corresponding pseudo-labels. The samples with smaller segmentation losses are expected to be accurately labeled samples, and only the segmentation loss is calculated. The network parameters are updated according to the respective losses.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{{seg}}(y,\,y^{\prime} )= \;{L}_{{Dice}}(y,\,y^{\prime} )+\alpha \cdot {L}_{{CE}}(y,\,y^{\prime} )=\left(1-\frac{2\cdot {\sum }_{i=1}^{N}{y}_{i}^{{\prime} }\cdot {y}_{i}+\varepsilon }{{\sum }_{i=1}^{N}{y}_{i}^{{\prime} }+{\sum }_{i=1}^{N}{y}_{i}+\varepsilon }\right)\\ -\frac{\alpha }{N}\left(\mathop{\sum }\limits_{i=1}^{N}{y}_{i}\,\log ({y}_{i}^{{\prime} })+(1-{y}_{i})\log (1-{y}_{i}^{{\prime} })\right)$$\end{document}</tex-math><mml:math id="M80"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.16em"/><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mrow><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2021_26216_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{{cor}}(\hat{y},\,y^{\prime} )=\frac{1}{2N}\mathop{\sum }\limits_{i=1}^{N}{\Vert}{\hat{y}}_{i}-{y^{\prime}_{i}}{\Vert}^{2}$$\end{document}</tex-math><mml:math id="M82"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><graphic xlink:href="41467_2021_26216_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>y</italic>′ is the prediction of the network, <italic>y</italic> is the reference, <inline-formula id="IEq40"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><mml:math id="M84"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq40.gif"/></alternatives></inline-formula> is the temperature-sharpened local pseudo-labels generated from the augmented inputs, <italic>N</italic> refers to the total number of pixels in the image, <italic>α</italic> is a constant to balance the different losses (we set <inline-formula id="IEq41"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{\rm{\alpha }}}}}}=1$$\end{document}</tex-math><mml:math id="M86"><mml:mi mathvariant="normal">α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq41.gif"/></alternatives></inline-formula> in all experiments unless otherwise specified), and <inline-formula id="IEq42"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varepsilon$$\end{document}</tex-math><mml:math id="M88"><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq42.gif"/></alternatives></inline-formula> is a constant to ensure numerical stability (we set <inline-formula id="IEq43"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varepsilon =1.0$$\end{document}</tex-math><mml:math id="M90"><mml:mi>ε</mml:mi><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq43.gif"/></alternatives></inline-formula>).<fig id="Fig5"><label>Fig. 5</label><caption><title>Pseudo-code of AIDE.</title><p>The inputs required for the model training, the overall training process with the proposed cross-model self-correcting mechanism, and the optimized model to be saved are included.</p></caption><graphic xlink:href="41467_2021_26216_Fig5_HTML" id="d32e3092"/></fig></p>
      <p id="Par35">The second step is global label correction. After each epoch, the DSCs of the whole training set are calculated and ranked. A defined percentage of samples (25% in our experiments) with the smallest DSCs are selected, and their labels are updated when certain criteria are met. In this work, we update the labels if the training epoch number is smaller than the defined warm-up epoch number and every ten epochs thereafter.</p>
      <p id="Par36">The rationale for our designed filtering and correction steps is related to the frequently observed network memorization pattern for natural image analysis: deep neural networks tend to learn simple patterns before memorizing all samples and real data examples are easier to fit than examples with noisy labels<sup><xref ref-type="bibr" rid="CR56">56</xref>,<xref ref-type="bibr" rid="CR63">63</xref>,<xref ref-type="bibr" rid="CR64">64</xref></sup>. Here, we find a similar network memorization pattern for medical image segmentation. By training a neural network utilizing the conventional fully supervised learning method with noisy labels, an overall positive correlation between the DSCs calculated by comparing network outputs to the noisy labels and the DSCs calculated by comparing the noisy labels to the high-quality labels exists for the training data during the initial training epochs (Supplementary Fig. <xref rid="MOESM1" ref-type="media">11a</xref>). In other words, within the considered training period, the network cannot successfully memorize those samples that contain large label noise but can learn the patterns of the samples that contain low label noise. On the other hand, all samples are well memorized when the network converges (Supplementary Fig. <xref rid="MOESM1" ref-type="media">11b</xref>). The noisy label updating schedule is designed accordingly—the suspected noisy labels are updated if the training epoch number is smaller than the defined warm-up epoch number and every ten epochs thereafter. The first criterion is based on the abovementioned observed network memorization behavior, and ablation studies indicate that the model performance is insensitive to the value of the warm-up epoch number in a large range (Supplementary Table <xref rid="MOESM1" ref-type="media">2</xref>). The second criterion is implemented because after the initial training epochs, the performances of the networks become relatively stable, and there is no need to update the labels frequently. Therefore, in our experiments, we update the labels every ten training epochs.</p>
    </sec>
    <sec id="Sec14">
      <title>Segmentation network architecture</title>
      <p id="Par37">A simple, general, and robust network architecture (U-Net) is adopted to process the different image inputs<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The architecture employs a classical encoder−decoder structure with an encoder stream to downsample the image resolution and extract image features and a decoder stream to recover the image resolution and generate segmentation outputs<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Skip connections are included to improve the localization accuracy. Since the depth resolution is much lower than the inplane resolution for the liver and prostate segmentation datasets and the noisy datasets contain only 2D samples, all our experiments proceed in a 2D fashion. The 3D extension is straightforward but requires considerably more effort to construct suitable implementation datasets, which is out of the scope of this study.</p>
      <p id="Par38">Similar encoder−decoder network architectures are utilized to handle single- and multimodal image datasets. During encoding, input images pass through five downsampling blocks sequentially with four max-pooling operations in between to extract multilayer image features. For multimodal inputs, multiple downsampling streams are utilized to extract features from the different modalities<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR65">65</xref></sup>. The extracted features are combined in a multilayer fusion manner. The features from all downsampling blocks are correspondingly concatenated to fuse information from different modalities. During decoding, the extracted image features pass through four upsampling blocks, each followed by a bilinear upsampling operation, to generate segmentation outputs with the same size as the inputs. Downsampling and upsampling blocks have similar components: two convolutional layers with a 3 × 3 convolution, batch normalization, and ReLU activation. The low-level features of the encoder are introduced to the decoder through skip connections and feature concatenation. Finally, a 1 × 1 convolution is appended to generate two features corresponding to the background and target segmentation maps, and a softmax activation is included to generate the segmentation probability maps.</p>
    </sec>
    <sec id="Sec15">
      <title>Evaluation metrics</title>
      <p id="Par39">Different metrics can be used to characterize the segmentation results. For our experiments, we choose the commonly utilized Dice score (Dice similarity coefficient, DSC), relative area/volume difference (RAVD), average symmetric surface distance (ASSD), and maximum symmetric surface distance (MSSD).<disp-formula id="Equ03"><label>3</label><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{\rm{DSC}}}}}}=\frac{2{{TP}}}{2{{TP}}+{{FP}}+{{FN}}}$$\end{document}</tex-math><mml:math id="M92"><mml:mi mathvariant="normal">DSC</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41467_2021_26216_Article_Equ03.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>4</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{\rm{RAVD}}}}}}=\frac{{{FP}}-{{FN}}}{{{TP}}+{{FN}}}$$\end{document}</tex-math><mml:math id="M94"><mml:mi mathvariant="normal">RAVD</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41467_2021_26216_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>5</label><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{\rm{ASSD}}}}}}=\frac{1}{|S({y}^{\prime} )|+|S(y)|}\left(\mathop{\sum }\limits _{a\in S({y}^{\prime} )}\mathop{\min }\limits_{b\in S(y)}||a-b||+\mathop{\sum }\limits _{b\in S(y)}\mathop{\min }\limits_{a\in S({y}^{\prime} )}||b-a||\right)$$\end{document}</tex-math><mml:math id="M96"><mml:mi mathvariant="normal">ASSD</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∣</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:mo>+</mml:mo><mml:mo>∣</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∣</mml:mo></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mrow><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2021_26216_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>6</label><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{\rm{MSSD}}}}}}=\,{{\max }}\,\Big(\mathop{\max }\limits_{a\in S({y}^{\prime} )}\mathop{\min }\limits_{b\in S(y)}||a-b||,\mathop{\max }\limits_{b\in S(y)}\mathop{\min }\limits_{a\in S({y}^{\prime} )}||b-a||\Big)$$\end{document}</tex-math><mml:math id="M98"><mml:mi mathvariant="normal">MSSD</mml:mi><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mi>max</mml:mi><mml:mspace width="0.25em"/><mml:mstyle mathsize="1.61em"><mml:mfenced open="("><mml:mrow/></mml:mfenced></mml:mstyle><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:mi>a</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:mo>,</mml:mo><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:mi>b</mml:mi><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:mstyle mathsize="1.61em"><mml:mfenced open=")"><mml:mrow/></mml:mfenced></mml:mstyle></mml:math><graphic xlink:href="41467_2021_26216_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where TP, FP, and FN refer to true positive predictions, false positive predictions, and false negative predictions, respectively. <italic>S</italic>(<italic>yʹ</italic>) and <italic>S</italic>(<italic>y</italic>) indicate the boundary points on the predicted segmentations and reference segmentations.</p>
      <p id="Par40">Significant differences between the different experiments and between the model results and human annotations are determined using two-sided paired <italic>t</italic> tests, with<inline-formula id="IEq44"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,P\le 0.05$$\end{document}</tex-math><mml:math id="M100"><mml:mspace width="0.25em"/><mml:mi>P</mml:mi><mml:mo>≤</mml:mo><mml:mn>0.05</mml:mn></mml:math><inline-graphic xlink:href="41467_2021_26216_Article_IEq44.gif"/></alternatives></inline-formula>.</p>
    </sec>
    <sec id="Sec16">
      <title>Statistics and reproducibility</title>
      <p id="Par41">The code used for training the deep-learning models are made publicly available for the reproducibility purpose. Statistical analysis has been given as well. Specifically, we run the code three times with different random initializations for the CHAOS dataset. For the domain adaptation task on prostate segmentation, six independent experiments were performed. On the QUBIQ datasets, we repeated 6, 7, 3, and 3 times respectively for the four different subtasks according to their dataset properties. For our breast datasets, data from three hospitals were utilized. So the experiments were performed independently for three times.</p>
    </sec>
    <sec id="Sec17">
      <title>Reporting summary</title>
      <p id="Par42">Further information on research design is available in the <xref rid="MOESM2" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec18">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41467_2021_26216_MOESM1_ESM.pdf">
            <caption>
              <p>Supplementary Information</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41467_2021_26216_MOESM2_ESM.pdf">
            <caption>
              <p>Reporting Summary</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41467_2021_26216_MOESM3_ESM.pdf">
            <caption>
              <p>Peer Review File</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec19">
        <title>Source data</title>
        <p id="Par45">
          <media position="anchor" xlink:href="41467_2021_26216_MOESM4_ESM.xlsx" id="MOESM4">
            <caption>
              <p>Source Data</p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p><bold>Peer review information</bold><italic>Nature Communications</italic> thanks the anonymous reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.</p>
    </fn>
    <fn>
      <p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors contributed equally: Shanshan Wang, Cheng Li, Rongpin Wang.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p>The online version contains supplementary material available at 10.1038/s41467-021-26216-9.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was partly supported by Scientific and Technical Innovation 2030-“New Generation Artificial Intelligence” Project (2020AAA0104100, 2020AAA0104105), the National Natural Science Foundation of China (61871371, 81830056), Key-Area Research and Development Program of Guangdong Province (2018B010109009), the Basic Research Program of Shenzhen (JCYJ20180507182400762), Youth Innovation Promotion Association Program of the Chinese Academy of Sciences (2019351), National Key R&amp;D Program of China (2017YFE0103600), National Natural Science Foundation of China (81720108021), Zhongyuan Thousand Talents Plan Project (ZYQR201810117), Zhengzhou Collaborative Innovation Major Project (20XTZX05015). We thank all the participants and staff of the four challenges, CHAOS, NCI-ISBI 2013, PROMISE12, and QUBIQ. We thank Professor H. Peng for discussions.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>S.W. and C.L. proposed the idea and initialized the project and the collaboration. C.L. and S.W. developed and implemented the framework. R.W., Z.L., M.W., H.T., and Xinfeng L. collected the breast data. R.W., Z.L., M.W., Y.W., and R.Y. provided the manual annotations. R.W., Z.L., M.W., H.T., Xinfeng L., Y.W., R.Y., and Xin L. contributed clinical expertise. C.L. and H.S. analyzed the results and plotted the figures. C.L., H.S., J.C., H. Zhou, I.B.A., and S.W. wrote the manuscript. S.W. and H. Zheng supervised the project. All authors read and contributed to revision and approved the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The raw image data and relevant information of the utilized open datasets are accessible from the respective official websites of the challenges (CHAOS: <ext-link ext-link-type="uri" xlink:href="https://chaos.grand-challenge.org/">https://chaos.grand-challenge.org/</ext-link>, NCI-ISBI 2013: <ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagingarchive.net/display/Public/NCI-ISBI+2013+Challenge+-+Automated+Segmentation+of+Prostate+Structures">https://wiki.cancerimagingarchive.net/display/Public/NCI-ISBI+2013+Challenge+-+Automated+Segmentation+of+Prostate+Structures</ext-link>, PROMISE12: <ext-link ext-link-type="uri" xlink:href="https://promise12.grand-challenge.org/">https://promise12.grand-challenge.org/</ext-link>, and QUBIQ: <ext-link ext-link-type="uri" xlink:href="https://qubiq.grand-challenge.org/">https://qubiq.grand-challenge.org/</ext-link>) through standard procedures. The clinical breast data were collected by the hospitals in de-identified format. Owing to patient-privacy considerations, they are not publicly available. All requests for academic use of in-house raw and analyzed data can be addressed to the corresponding authors. All requests will be promptly reviewed within 10 working days to determine whether the request is subject to any intellectual property or patient-confidentiality obligations, will be processed in concordance with institutional and departmental guidelines, and will require a material transfer agreement (available at <ext-link ext-link-type="uri" xlink:href="https://github.com/lich0031/AIDE">https://github.com/lich0031/AIDE</ext-link>). <xref rid="Sec19" ref-type="sec">Source data</xref> are provided with this paper.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The code used for training the deep-learning models and models used in this study are made publicly available<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. Implementation of our work is based on PyTorch with necessary publicly available packages, including numpy, pandas, PIL, skimage, and SimpleITK.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par43">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hollon</surname>
            <given-names>TC</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Near real-time intraoperative brain tumor diagnosis using stimulated Raman histology and deep neural networks</article-title>
        <source>Nat. Med.</source>
        <year>2020</year>
        <volume>26</volume>
        <fpage>52</fpage>
        <lpage>58</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-019-0715-9</pub-id>
        <pub-id pub-id-type="pmid">31907460</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bai</surname>
            <given-names>W</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A population-based phenome-wide association study of cardiac and aortic structure and function</article-title>
        <source>Nat. Med.</source>
        <year>2020</year>
        <volume>26</volume>
        <fpage>1654</fpage>
        <lpage>1662</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-020-1009-y</pub-id>
        <pub-id pub-id-type="pmid">32839619</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mei</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Artificial intelligence-enabled rapid diagnosis of patients with COVID-19</article-title>
        <source>Nat. Med.</source>
        <year>2020</year>
        <volume>26</volume>
        <fpage>1224</fpage>
        <lpage>1228</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-020-0931-3</pub-id>
        <pub-id pub-id-type="pmid">32427924</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kickingereder</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated quantitative tumour response assessment of MRI in neuro-oncology with artificial neural networks: a multicentre, retrospective study</article-title>
        <source>Lancet Oncol.</source>
        <year>2019</year>
        <volume>20</volume>
        <fpage>728</fpage>
        <lpage>740</lpage>
        <pub-id pub-id-type="doi">10.1016/S1470-2045(19)30098-1</pub-id>
        <pub-id pub-id-type="pmid">30952559</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Benchmark on automatic 6-month-old infant brain segmentation algorithms: the iSeg-2017 challenge</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2019</year>
        <volume>38</volume>
        <fpage>2219</fpage>
        <lpage>2230</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2901712</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Clinically applicable deep learning framework for organs at risk delineation in CT images</article-title>
        <source>Nat. Mach. Intell.</source>
        <year>2019</year>
        <volume>1</volume>
        <fpage>480</fpage>
        <lpage>491</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-019-0099-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khened</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kollerathu</surname>
            <given-names>VA</given-names>
          </name>
          <name>
            <surname>Krishnamurthi</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Fully convolutional multi-scale residual DenseNets for cardiac segmentation and automated cardiac diagnosis using ensemble of classifiers</article-title>
        <source>Med. Image Anal.</source>
        <year>2019</year>
        <volume>51</volume>
        <fpage>21</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2018.10.004</pub-id>
        <pub-id pub-id-type="pmid">30390512</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Litjens</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A survey on deep learning in medical image analysis</article-title>
        <source>Med. Image Anal.</source>
        <year>2017</year>
        <volume>42</volume>
        <fpage>60</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id>
        <pub-id pub-id-type="pmid">28778026</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Suk</surname>
            <given-names>H-I</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in medical image analysis</article-title>
        <source>Annu. Rev. Biomed. Eng.</source>
        <year>2017</year>
        <volume>19</volume>
        <fpage>221</fpage>
        <lpage>248</lpage>
        <pub-id pub-id-type="doi">10.1146/annurev-bioeng-071516-044442</pub-id>
        <pub-id pub-id-type="pmid">28301734</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hesamian</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Kennedy</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Deep learning techniques for medical image segmentation: achievements and challenges</article-title>
        <source>J. Digit. Imaging</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>582</fpage>
        <lpage>596</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-019-00227-x</pub-id>
        <pub-id pub-id-type="pmid">31144149</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>H-DenseUNet: Hybrid densely connected UNet for liver and tumor segmentation from CT volumes</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>2663</fpage>
        <lpage>2674</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2845918</pub-id>
        <pub-id pub-id-type="pmid">29994201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dolz</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>HyperDense-Net: a hyper-densely connected CNN for multi-modal image segmentation</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2019</year>
        <volume>38</volume>
        <fpage>1116</fpage>
        <lpage>1126</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2878669</pub-id>
        <pub-id pub-id-type="pmid">30387726</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haberl</surname>
            <given-names>MG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CDeep3M—Plug-and-Play cloud-based deep learning for image segmentation</article-title>
        <source>Nat. Methods</source>
        <year>2018</year>
        <volume>15</volume>
        <fpage>677</fpage>
        <lpage>680</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0106-z</pub-id>
        <pub-id pub-id-type="pmid">30171236</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lecun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Webb</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Deep learning for biology</article-title>
        <source>Nature</source>
        <year>2018</year>
        <volume>554</volume>
        <fpage>555</fpage>
        <lpage>557</lpage>
        <pub-id pub-id-type="doi">10.1038/d41586-018-02174-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karimi</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Warfield</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Gholipour</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deep learning with noisy labels: exploring techniques and remedies in medical image analysis</article-title>
        <source>Med. Image Anal.</source>
        <year>2020</year>
        <volume>65</volume>
        <fpage>101759</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2020.101759</pub-id>
        <pub-id pub-id-type="pmid">32623277</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Joskowicz</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Caplan</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sosna</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Inter-observer variability of manual contour delineation of structures in CT</article-title>
        <source>Eur. Radiol.</source>
        <year>2019</year>
        <volume>29</volume>
        <fpage>1391</fpage>
        <lpage>1399</lpage>
        <pub-id pub-id-type="doi">10.1007/s00330-018-5695-5</pub-id>
        <pub-id pub-id-type="pmid">30194472</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Towards trustable machine learning. <italic>Nat. Biomed. Eng</italic>. <bold>2</bold>, 709–710 (2018).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lutnick</surname>
            <given-names>B</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An integrated iterative annotation technique for easing neural network training in medical image analysis</article-title>
        <source>Nat. Mach. Intell.</source>
        <year>2019</year>
        <volume>1</volume>
        <fpage>112</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-019-0018-3</pub-id>
        <pub-id pub-id-type="pmid">31187088</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Esteva</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A guide to deep learning in healthcare</article-title>
        <source>Nat. Med.</source>
        <year>2019</year>
        <volume>25</volume>
        <fpage>24</fpage>
        <lpage>29</lpage>
        <pub-id pub-id-type="doi">10.1038/s41591-018-0316-z</pub-id>
        <pub-id pub-id-type="pmid">30617335</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holmberg</surname>
            <given-names>OG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Self-supervised retinal thickness prediction enables deep learning from unlabelled data to boost classification of diabetic retinopathy</article-title>
        <source>Nat. Mach. Intell.</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>719</fpage>
        <lpage>726</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-00247-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tajbakhsh</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Embracing imperfect datasets: a review of deep learning solutions for medical image segmentation</article-title>
        <source>Med. Image Anal.</source>
        <year>2020</year>
        <volume>63</volume>
        <fpage>101693</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2020.101693</pub-id>
        <pub-id pub-id-type="pmid">32289663</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Blum, A. &amp; Mitchell, T. Combining labeled and unlabeled data with co-training. In <italic>Conference on Computational Learning Theory</italic> (eds Bartlett, P. L. &amp; Mansour, Y.) 92–100 (ACM, 1998).</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Qiao, S., Shen, W., Zhang, Z., Wang, B. &amp; Yuille, A. Deep co-training for semi-supervised image recognition. In <italic>European Conference on Computer Vision (ECCV)</italic> (eds Ferrari, V., Hebert, M., Sminchisescu, C. &amp; Weiss, Y.) 135–152 (Springer, Cham, 2018).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Estrada</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pedersoli</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Desrosiers</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Deep co-training for semi-supervised image segmentation</article-title>
        <source>Pattern Recogn</source>
        <year>2020</year>
        <volume>107</volume>
        <fpage>107269</fpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2020.107269</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xia</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Uncertainty-aware multi-view co-training for semi-supervised medical image segmentation and domain adaptation</article-title>
        <source>Med. Image Anal.</source>
        <year>2020</year>
        <volume>65</volume>
        <fpage>101766</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2020.101766</pub-id>
        <pub-id pub-id-type="pmid">32623276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Han, B. et al. Co-teaching: robust training of deep neural networks with extremely noisy labels. In <italic>Conference on Neural Information Processing Systems (NeurIPS)</italic> (Montreal, Canada, 2018).</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Yu, X. et al. How does disagreement help generalization against label corruption? In <italic>International Conference on Machine Learning (ICML)</italic> (Long Beach, California, USA, 2019).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Falk</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>U-Net: deep learning for cell counting, detection, and morphometry</article-title>
        <source>Nat. Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>67</fpage>
        <lpage>70</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0261-2</pub-id>
        <pub-id pub-id-type="pmid">30559429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Hazirbas, C. &amp; Ma, L. FuseNet: incorporating depth into semantic segmentation via fusion-based CNN architecture. In <italic>Asian Conference on Computer Vision (ACCV)</italic> (Taipei, Taiwan, China, 2016).</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Vohra, Y. et al. Data distillation: towards omni-supervised learning. In <italic>IEEE Conference on Computer Vision and Pattern Recognition</italic> (<italic>CVPR</italic>) 4119–4128 (IEEE, 2018).</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Lee, D.-H. Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks. <italic>ICML2013 Workshop on Challenges in Representational Learning</italic>, Vol. 3 (Atlanta, USA, 2013).</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kavur</surname>
            <given-names>AE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CHAOS Challenge—combined (CT-MR) healthy abdominal organ segmentation</article-title>
        <source>Med. Image Anal.</source>
        <year>2021</year>
        <volume>69</volume>
        <fpage>101950</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2020.101950</pub-id>
        <pub-id pub-id-type="pmid">33421920</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Litjens</surname>
            <given-names>G</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Evaluation of prostate segmentation algorithms for MRI: the PROMISE12 challenge</article-title>
        <source>Med. Image Anal.</source>
        <year>2014</year>
        <volume>18</volume>
        <fpage>359</fpage>
        <lpage>373</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2013.12.002</pub-id>
        <pub-id pub-id-type="pmid">24418598</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Saha</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Mazurowski</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Hierarchical convolutional neural networks for segmentation of breast tumors in MRI with application to radiogenomics</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2019</year>
        <volume>38</volume>
        <fpage>435</fpage>
        <lpage>447</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2865671</pub-id>
        <pub-id pub-id-type="pmid">30130181</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qiao</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Three-dimensional breast tumor segmentation on DCE-MRI with a multilabel attention-guided joint-phaselearning network</article-title>
        <source>Comput. Med. Imaging Graph.</source>
        <year>2021</year>
        <volume>90</volume>
        <fpage>101909</fpage>
        <pub-id pub-id-type="doi">10.1016/j.compmedimag.2021.101909</pub-id>
        <pub-id pub-id-type="pmid">33845432</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>AUNet: attention-guided dense-upsampling networks for breast mass segmentation in whole mammograms</article-title>
        <source>Phys. Med. Biol.</source>
        <year>2020</year>
        <volume>65</volume>
        <fpage>055005</fpage>
        <pub-id pub-id-type="doi">10.1088/1361-6560/ab5745</pub-id>
        <pub-id pub-id-type="pmid">31722327</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>D-UNet: a dimension-fusion U shape network for chronic stroke lesion segmentation</article-title>
        <source>IEEE/ACM Trans. Comput. Biol. Bioinform.</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>940</fpage>
        <lpage>950</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2019.2939522</pub-id>
        <pub-id pub-id-type="pmid">31502985</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Yang, H. et al. CLCI-Net: Cross-level fusion and context inference networks for lesion segmentation of chronic stroke. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11766, 266–274 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Qi, K. et al. X-Net: brain stroke lesion segmentation based on depthwise separable convolution and long-range dependencies. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11766, 247–255 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Zhang, Z., Yang, L. &amp; Zheng, Y. Translating and segmenting multimodal medical volumes with cycle- and shape-consistency generative adversarial network. In <italic>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic> 9242–9251 (IEEE, 2018).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>PA</given-names>
          </name>
        </person-group>
        <article-title>Unsupervised bidirectional cross-modality adaptation via deeply synergistic image and feature alignment for medical image segmentation</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
        <volume>39</volume>
        <fpage>2494</fpage>
        <lpage>2505</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2020.2972701</pub-id>
        <pub-id pub-id-type="pmid">32054572</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Augmenting vascular disease diagnosis by vasculature-aware unsupervised learning</article-title>
        <source>Nat. Mach. Intell.</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>337</fpage>
        <lpage>346</lpage>
        <pub-id pub-id-type="doi">10.1038/s42256-020-0188-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Nie, D., Gao, Y., Wang, L. &amp; Shen, D. ASDNet: attention based semi-supervised deep networks for medical image segmentation. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Frangi, A., Schnabel, J., Davatzikos, C., Alberola-López, C. &amp; Fichtinger, G.) Vol. 11073, 370–378 (Springer, Cham, 2018).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets</article-title>
        <source>Nat. Biomed. Eng.</source>
        <year>2019</year>
        <volume>3</volume>
        <fpage>173</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1038/s41551-018-0324-9</pub-id>
        <pub-id pub-id-type="pmid">30948806</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">Li, Z., Kamnitsas, K. &amp; Glocker, B. Overfitting of neural nets under class imbalance: analysis and improvements for segmentation. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11766, 402–410 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ren</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Interleaved 3D-CNNs for joint segmentation of small-volume structures in head and neck CT images</article-title>
        <source>Med. Phys.</source>
        <year>2018</year>
        <volume>45</volume>
        <fpage>2063</fpage>
        <lpage>2075</lpage>
        <pub-id pub-id-type="doi">10.1002/mp.12837</pub-id>
        <pub-id pub-id-type="pmid">29480928</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Bai, W. et al. Self-supervised learning for cardiac MR image segmentation by anatomical position prediction. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11765, 541–549 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Zhou, Z. et al. Models genesis: generic autodidactic models for 3d medical image analysis. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11767, 384–393 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Hardt, M. Understanding deep learning requires rethinking generalization. In <italic>International Conference on Learning Representations (ICLR)</italic> (Toulon, France, 2017).</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Le, H. et al. Pancreatic cancer detection in whole slide images using noisy label annotations. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11764, 541–549 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Zhu, H., Shi, J. &amp; Wu, J. Pick-and-learn: automatic quality evaluation for noisy-labeled image segmentation. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11769, 576–584 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Shu, Y., Wu, X. &amp; Li, W. LVC-Net: Medical image segmentation with noisy label based on local visual cues. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11769, 558–566 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Bai, W. et al. Semi-supervised learning for network-based cardiac MR image segmentation. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Descoteaux, M., Maier-Hein, L., Franz, A.,Jannin, P., Collins, D. &amp; Duchesne, S.) Vol. 10434, 253–260 (Springer, Cham, 2017).</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Xia, Y. et al. 3D semi-supervised learning with uncertainty-aware multi-view co-training. In <italic>Winter Conference on Applications of Computer Vision (WACV)</italic> 3646–3655 (IEEE, 2020).</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Arplt, D. et al. A closer look at memorization in deep networks. In <italic>International Conference on Machine Learning (ICML)</italic> (Sydney, Australia, 2017).</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Greenspan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ginneken</surname>
            <given-names>Bvan</given-names>
          </name>
          <name>
            <surname>Summers</surname>
            <given-names>RM</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in medical imaging: overview and future promise of an exciting new technique</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1153</fpage>
        <lpage>1159</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2016.2553401</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kavur</surname>
            <given-names>AE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comparison of semi-automatic and deep learning-based automatic methods for liver segmentation in living liver transplant donors</article-title>
        <source>Diagnostic Interv. Radiol.</source>
        <year>2020</year>
        <volume>26</volume>
        <fpage>11</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.5152/dir.2019.19025</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <mixed-citation publication-type="other">Kavur, A. E., Selver, M. A., Dicle, O., Barış, M. &amp; Gezer, N. S. <italic>CHAOS—Combined (CT-MR) Healthy Abdominal Organ Segmentation Challenge Data</italic> (Zenodo, 2019).</mixed-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <mixed-citation publication-type="other">Bloch, N. et al. NCI-ISBI 2013 challenge: automated segmentation of prostate structures. In <italic>International Symposium on Biomedical Imaging (ISBI)</italic>, (San Francisco, California, USA, 2013).</mixed-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <mixed-citation publication-type="other">Hinton, G., Vinyals, O. &amp; Dean, J. Distilling the knowledge in a neural network. In <italic>Conference on Neural Information Processing Systems (NeurIPS)</italic> (Montreal, Canada, 2015).</mixed-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <mixed-citation publication-type="other">Berthelot, D. et al. MixMatch: a holistic approach to semi-supervised learning. In <italic>Conference on Neural Information Processing Systems (NeurIPS)</italic> (Vancouver, Canada, 2019).</mixed-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <mixed-citation publication-type="other">Jiang, L., Zhou, Z., Leung, T., Li, L.-J. &amp; Li, F.-F. MentorNet: learning data-driven curriculum for very deep neural networks on corrupted labels. In <italic>International Conference on Machine Learning (ICML)</italic> (Stockholm, Sweden, 2018).</mixed-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Chen, P., Liao, B., Chen, G. &amp; Zhang, S. Understanding and utilizing deep neural networks trained with noisy labels. In <italic>International Conference on Machine Learning (ICML)</italic> (Long Beach, California, USA, 2019).</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <mixed-citation publication-type="other">Li, C. et al. Learning cross-modal deep representations for multi-modal MR image segmentation. In <italic>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</italic> (eds Shen, D. et al.) Vol. 11765, 57–65 (Springer, Cham, 2019).</mixed-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Wang, S. et al. AIDE: annotation-efficient deep learning for automatic medical image segmentation, AIDE-v1.0.0 10.5281/zenodo.5511736 (2021).</mixed-citation>
    </ref>
  </ref-list>
</back>
